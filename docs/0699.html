<html>
<head>
<title>NLP News Cypher | 07.19.20</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP新闻密码| 07.19.20</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/nlp-news-cypher-07-19-20-75c1a3f7f57b?source=collection_archive---------3-----------------------#2020-07-19">https://pub.towardsai.net/nlp-news-cypher-07-19-20-75c1a3f7f57b?source=collection_archive---------3-----------------------#2020-07-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8fe77e7ebb4bd8dcb03a3733339a44e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fMOV8oM474vdGXuU"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">穆罕默德·阿里扎德在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h2 id="20d8" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph">自然语言处理每周时事通讯</h2><div class=""/><div class=""><h2 id="7233" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">模块性</h2></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="136d" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">Twitter的帮助台度过了富有成效的一周。生产力如何？👇</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="ml mm l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">非常多产</figcaption></figure><p id="0548" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">如果你错过了，名人的推特账户在一个比特币庞氏骗局中被黑。随着Twitter和co .争相灭火，他们停用了所有蓝色支票账户。矩阵中的一个小故障！</p><p id="a596" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">几天后，选定的Cloudflare服务器陷入黑暗，因为他们将插件服务🧐(sorry不和谐归咎于糟糕的路由。退一步说，科技股度过了艰难的一周。</p><p id="64fb" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">但这并没有阻止我随机浏览暗网，以了解更多关于最近黑客攻击的信息。我什么也没找到！耶！然而，我确实发现美国特勤局从数字货币交易所比特币基地购买了一份为期4年的加密软件合同。是的，它甚至出现在美国政府的公开文件中:</p><div class="ip iq gp gr ir mn"><a href="https://beta.sam.gov/awards/90905932%2BAWARD#general-information" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">贝塔。SAM.gov</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">编辑描述</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">beta.sam.gov</p></div></div></div></a></div><p id="bf27" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">这有什么关系？根据Benzinga的说法，“比特币基地还收集私人用户数据，作为其平台反洗钱要求的一部分。”🙈</p><p id="a0a2" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">还有，ICML发生了！这里有一些很棒的论文:</p><div class="ip iq gp gr ir mn"><a href="http://ai.stanford.edu/blog/icml-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">2020年ICML斯坦福人工智能实验室论文和演讲</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">2020年机器学习国际会议(ICML)将于7月13日至7月18日举行…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">ai.stanford.edu</p></div></div><div class="mw l"><div class="mx l my mz na mw nb ix mn"/></div></div></a></div><p id="d9b8" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">亮点:<a class="ae jd" href="https://arxiv.org/pdf/2005.10636.pdf" rel="noopener ugc nofollow" target="_blank">根据诊断反馈进行基于图形的自我监督程序修复</a></p><div class="ip iq gp gr ir mn"><a href="https://ai.googleblog.com/2020/07/google-at-icml-2020.html" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">谷歌在ICML 2020</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">机器学习是谷歌的一个关键战略重点，高度活跃的团队几乎在所有领域都开展研究</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">ai.googleblog.com</p></div></div><div class="mw l"><div class="nc l my mz na mw nb ix mn"/></div></div></a></div><p id="7825" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">亮点:<a class="ae jd" href="https://arxiv.org/pdf/2002.08909.pdf" rel="noopener ugc nofollow" target="_blank">领域:检索-增强语言模型预训练</a></p><div class="ip iq gp gr ir mn"><a href="https://blog.ml.cmu.edu/2020/07/13/carnegie-mellon-university-at-icml-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">卡耐基梅隆大学ICML分校2020</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">卡耐基梅隆大学很荣幸在第37届机器学习国际会议(ICML)上提交44篇论文</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">blog.ml.cmu.edu</p></div></div><div class="mw l"><div class="nd l my mz na mw nb ix mn"/></div></div></a></div><p id="2b21" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">亮点:<a class="ae jd" href="https://arxiv.org/pdf/2003.11080.pdf" rel="noopener ugc nofollow" target="_blank"> XTREME:一个用于评估跨语言泛化的大型多语言多任务基准</a></p><div class="ip iq gp gr ir mn"><a href="https://ai.facebook.com/blog/facebook-research-at-icml-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">ICML 2020的脸书研究</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">来自世界各地的机器学习专家正在为2020年国际机器大会虚拟地聚集在一起…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">ai.facebook.com</p></div></div><div class="mw l"><div class="ne l my mz na mw nb ix mn"/></div></div></a></div><p id="447d" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">亮点:<a class="ae jd" href="https://arxiv.org/pdf/2004.01655.pdf" rel="noopener ugc nofollow" target="_blank">非自回归机器翻译的对齐交叉熵</a></p><p id="fe99" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">研讨会亮点:强化学习中的语言:</p><div class="ip iq gp gr ir mn"><a href="https://larel-ws.github.io/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">拉雷尔2020</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">语言是人类最令人印象深刻的成就之一，被认为是我们学习能力的核心…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">larel-ws.github.io</p></div></div><div class="mw l"><div class="nf l my mz na mw nb ix mn"/></div></div></a></div><p id="7c43" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><a class="ae jd" href="https://grlplus.github.io/papers/" rel="noopener ugc nofollow" target="_blank">荣誉奖</a></p><p id="d099" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">⚡ <strong class="ln jq">超级骗子NLP回购</strong> ⚡</p><p id="8366" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">仅供参考:又增加了52台笔记本电脑，使我们的NLP Colabs总数达到233台。感谢您的贡献:Manu Romero、Abhishek Mishra、Nikhil Narayan、Oleksii Trekhleb、Chris Tran、Prasanna Kumar和Cristiano De Nobili。</p><div class="ip iq gp gr ir mn"><a href="https://notebooks.quantumstat.com" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">超级骗子NLP回购</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">适用于NLP中各种任务的Colab笔记本</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">notebooks.quantumstat.com</p></div></div><div class="mw l"><div class="ng l my mz na mw nb ix mn"/></div></div></a></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="35bb" class="nh ni jg bd nj nk nl nm nn no np nq nr kv ns kw nt ky nu kz nv lb nw lc nx ny bi translated">本周</h1><blockquote class="nz oa ob"><p id="8814" class="ll lm oc ln b lo lp kq lq lr ls kt lt od lv lw lx oe lz ma mb of md me mf mg ij bi translated">适配器、适配器集线器和模块化(带有绝密采访)</p><p id="3aa5" class="ll lm oc ln b lo lp kq lq lr ls kt lt od lv lw lx oe lz ma mb of md me mf mg ij bi translated">GPT-3余波</p><p id="0832" class="ll lm oc ln b lo lp kq lq lr ls kt lt od lv lw lx oe lz ma mb of md me mf mg ij bi translated">使用简单变压器的超参数优化</p><p id="97aa" class="ll lm oc ln b lo lp kq lq lr ls kt lt od lv lw lx oe lz ma mb of md me mf mg ij bi translated">机器学习原型的用户界面</p><p id="16b2" class="ll lm oc ln b lo lp kq lq lr ls kt lt od lv lw lx oe lz ma mb of md me mf mg ij bi translated">可视化:设置为眩晕</p><p id="f030" class="ll lm oc ln b lo lp kq lq lr ls kt lt od lv lw lx oe lz ma mb of md me mf mg ij bi translated">基于图的深度学习报告</p><p id="9cdc" class="ll lm oc ln b lo lp kq lq lr ls kt lt od lv lw lx oe lz ma mb of md me mf mg ij bi translated">开放域对话式人工智能</p><p id="6afe" class="ll lm oc ln b lo lp kq lq lr ls kt lt od lv lw lx oe lz ma mb of md me mf mg ij bi translated">本周数据集:关键角色龙与地下城数据集(CRD3)</p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="26e5" class="nh ni jg bd nj nk nl nm nn no np nq nr kv ns kw nt ky nu kz nv lb nw lc nx ny bi translated">适配器、适配器Hub和模块化</h1><p id="3cb6" class="pw-post-body-paragraph ll lm jg ln b lo og kq lq lr oh kt lt lu oi lw lx ly oj ma mb mc ok me mf mg ij bi translated">偶尔会有一些很酷的事情发生，在过去的一周里，AdapterHub框架被弃用了。在NLP迁移学习的下一个发展中，适配器提供了一个新的(更模块化的)架构。</p><p id="a927" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><a class="ae jd" href="https://arxiv.org/pdf/2007.07779.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a>(易读)| <a class="ae jd" href="https://github.com/Adapter-Hub/adapter-transformers" rel="noopener ugc nofollow" target="_blank"> Github </a></p><p id="2ffe" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">轮毂:</strong></p><div class="ip iq gp gr ir mn"><a href="https://adapterhub.ml/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">适配器Hub - 175适配器，适用于21种文本任务和32种语言</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">从我们的存储库中加载现有的适配器就像增加一行代码一样简单:model =…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">adapterhub.ml</p></div></div><div class="mw l"><div class="ol l my mz na mw nb ix mn"/></div></div></a></div><p id="f541" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">哦，我们假设你们大多数人会说“WTF是适配器？!"因此，我们非常兴奋地与AdapterHub的作者Jonas Pfeiffer交谈，让我们了解适配器及其框架的所有方面:👇</p><figure class="mh mi mj mk gt is gh gi paragraph-image"><div class="gh gi om"><img src="../Images/46974583292e1e867e512e1066d3694e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*0_MsW6k0tp9qRzgc.jpg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">😎</figcaption></figure><ol class=""><li id="bc85" class="on oo jg ln b lo lp lr ls lu op ly oq mc or mg os ot ou ov bi translated"><strong class="ln jq">嗨，Jonas，祝贺你的新的和令人敬畏的框架适配器Hub！对于那些在循环之外的人，你将如何简单地描述适配器？</strong></li></ol><p id="c2ad" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">适配器是封装在转换器模型的每一层中的小型模块化单元，它学习存储特定于任务或语言的信息。这是通过训练*仅*新引入的适配器权重，同时保持预训练模型的其余部分固定来实现的。关于适配器最吸引人的概念是它们的模块化，这为组合来自许多适配器的知识提供了许多可能性，这些适配器接受了多种任务的训练。为了使训练适配器和随后尽可能容易地共享它们，我们提出了AdapterHub框架。</p><p id="54f3" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">2.<strong class="ln jq">与预训练模型的传统微调相比，适配器有哪些优势？</strong></p><p id="69f4" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">“对于工业界的NLP工程师和研究人员来说，都有许多优势。对于从业者来说，最有趣的概念可能是适配器的小存储空间。<strong class="ln jq">适配器仅需要3.5Mb(在所有任务中共享&gt; 99%的参数)，并且仍能实现一流的性能。这意味着，为了在一个设备上存储125个适配器型号，您需要多达2个完全微调的BERT型号的空间。</strong>适配器提供的最大优势是它们的模块化。通过冻结预先训练的模型权重，传统的多任务学习问题如任务间的灾难性遗忘和灾难性干扰不再存在。因此，适配器可以针对各种任务进行单独训练，随后进行组合或堆叠，以组合存储在其中的信息。”</p><p id="4030" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">3.<strong class="ln jq">在训练适配器时，其训练时间与传统微调相比如何？</strong></p><p id="cdea" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">“到目前为止，我们已经观察到训练适配器通常比完全微调更快。对于一些设置，我们可以看到执行一个训练步骤所需的时间提高了30%。这是因为我们不需要对整个模型(如BERT嵌入矩阵)进行反向传递，也是因为PyTorch优化策略。不幸的是，对于较小的数据集，由于随机权重初始化，适配器需要比完全微调更多的步骤。我们认为效率是与许多实际应用相关的重要属性。这就是为什么我们目前正在更大范围内调查不同架构的计算效率，包括几个训练和推理场景。”</p><p id="fde4" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">4.<strong class="ln jq">您已经为社区创建了AdapterHub来查找、培训和/或使用适配器；在哪里可以找到更多关于如何提供帮助的信息？</strong></p><p id="f937" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">“适配器最近才推出，所以研究领域相当新。我们试图在与AdapterHub框架一起发布的论文中总结我们对适配器的看法。对我们来说，AdapterHub是一个长期项目，我们希望NLP社区能够利用它来开发新的研究方向，以适配器及其惊人的模块化功能为基础。”</p><p id="9c92" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">5.<strong class="ln jq">你认为适配器是NLP迁移学习的下一个重要步骤吗？</strong></p><p id="e25e" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">“跨任务共享信息在机器学习中有着悠久的历史，其中多任务学习可以说受到了最多的关注，伴随着许多问题。通过首先将存储的信息封装在冻结的参数中，然后将其组合起来，我们能够缓解其中的一些问题。知识组件的模块化可以随时随意组合，这是非常可取和有效的。因此，从我的角度来看，适配器是迁移学习的一个非常重要和有前途的方向，我坚信它们有能力加快这一领域的研究。”</p><p id="6213" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><em class="oc">鳍</em>👀</p><p id="34e2" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">从乔纳斯的回答中可以看出，这是迁移学习和模型架构的显著进步。AdapterHub框架建立在Hugging Face库的基础上，只需要1-2行代码(在您习惯于Transformers库中的常用代码之上)就可以初始化一个适配器。</p><p id="bc8b" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">为了展示入门(使用推理)是多么容易，我们创建了一个Colab，其中BERT与SST-2适配器(二进制情感分析)堆叠在一起。试一试吧，别忘了检查AdapterHub并训练那些适配器！感谢乔纳斯的精彩介绍！</p><h2 id="d0d8" class="ow ni jg bd nj ox oy dn nn oz pa dp nr lu pb pc nt ly pd pe nv mc pf pg nx jm bi translated">本周可乐🤟</h2><div class="ip iq gp gr ir mn"><a href="https://colab.research.google.com/drive/1iEymIGmkX9_EjirgQpcKoVc-0wuv-DXN?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">谷歌联合实验室</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">编辑描述</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">colab.research.google.com</p></div></div><div class="mw l"><div class="ph l my mz na mw nb ix mn"/></div></div></a></div><h1 id="4def" class="nh ni jg bd nj nk pi nm nn no pj nq nr kv pk kw nt ky pl kz nv lb pm lc nx ny bi translated">GPT-3余波</h1><p id="133d" class="pw-post-body-paragraph ll lm jg ln b lo og kq lq lr oh kt lt lu oi lw lx ly oj ma mb mc ok me mf mg ij bi translated">GPT 3号本周收到了很多反馈。在他的博客上，Max Woolf评论了GPT-3令人印象深刻的能力和语言模型的不足之处。</p><p id="e854" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">TL；博士:</strong></p><ul class=""><li id="157e" class="on oo jg ln b lo lp lr ls lu op ly oq mc or mg pn ot ou ov bi translated">黑盒问题依然存在。</li><li id="c6a4" class="on oo jg ln b lo po lr pp lu pq ly pr mc ps mg pn ot ou ov bi translated">模型很慢。</li><li id="91bf" class="on oo jg ln b lo po lr pp lu pq ly pr mc ps mg pn ot ou ov bi translated">模型输出仍然需要精选，但要比GPT-2更好。</li><li id="374b" class="on oo jg ln b lo po lr pp lu pq ly pr mc ps mg pn ot ou ov bi translated">输出不灵敏仍然是个问题。</li></ul><p id="4aa1" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">博客:</strong></p><div class="ip iq gp gr ir mn"><a href="https://minimaxir.com/2020/07/gpt3-expectations/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">降低对GPT-3和open ai API的期望</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">5月29日，OpenAI发布了一篇关于GPT-3的论文，这是他们基于变形金刚的文本生成神经系统的下一次迭代</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">minimaxir.com</p></div></div><div class="mw l"><div class="pt l my mz na mw nb ix mn"/></div></div></a></div><p id="685b" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">值得一提的是，约夫·戈德堡与GPT 3号的互动也值得在他的推特上查看:<a class="ae jd" href="https://twitter.com/yoavgo" rel="noopener ugc nofollow" target="_blank">https://twitter.com/yoavgo</a></p><h1 id="6c4d" class="nh ni jg bd nj nk pi nm nn no pj nq nr kv pk kw nt ky pl kz nv lb pm lc nx ny bi translated">使用简单变压器的超参数优化</h1><p id="af15" class="pw-post-body-paragraph ll lm jg ln b lo og kq lq lr oh kt lt lu oi lw lx ly oj ma mb mc ok me mf mg ij bi translated">Thilina Rajapakse是Simple Transformers library的作者，她在<a class="ae jd" href="https://aclweb.org/aclwiki/Recognizing_Textual_Entailment" rel="noopener ugc nofollow" target="_blank">识别文本蕴涵(RTE) </a>任务中探索了超参数优化。一个直观的一步一步的指南(包括代码)，此外还有集成在他的库中的来自W &amp; B Sweeps的可视化。</p><div class="ip iq gp gr ir mn"><a href="https://towardsdatascience.com/hyperparameter-optimization-for-optimum-transformer-models-b95a32b70949" rel="noopener follow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">最佳变压器模型的超参数优化</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">如何使用简单的转换器调整超参数，以实现更好的自然语言处理。</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">towardsdatascience.com</p></div></div><div class="mw l"><div class="pu l my mz na mw nb ix mn"/></div></div></a></div><h1 id="80d6" class="nh ni jg bd nj nk pi nm nn no pj nq nr kv pk kw nt ky pl kz nv lb pm lc nx ny bi translated">机器学习原型的用户界面</h1><p id="5ffa" class="pw-post-body-paragraph ll lm jg ln b lo og kq lq lr oh kt lt lu oi lw lx ly oj ma mb mc ok me mf mg ij bi translated">想要添加一个快速UI来可视化您的transformer模型吗？向格拉迪奥问好。该库包括Colab/Jupyter支持，因此您可以将您的推断从Colab直接传输到浏览器。它包括TensorFlow和PyTorch支持，可以用于CV和NLP演示。</p><p id="c266" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">仅供参考，2 Gradio笔记本包含在最新更新的<a class="ae jd" href="https://notebooks.quantumstat.com" rel="noopener ugc nofollow" target="_blank"> Super Duper NLP Repo </a>中！前往那里(或<a class="ae jd" href="https://www.gradio.app/" rel="noopener ugc nofollow" target="_blank">这里</a>)快速体验它的功能。</p><div class="ip iq gp gr ir mn"><a href="https://github.com/gradio-app/gradio" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">gradio-app/gradio</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">围绕TensorFlow或PyTorch模型，甚至任意Python函数，快速创建可定制的UI组件…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">github.com</p></div></div><div class="mw l"><div class="pv l my mz na mw nb ix mn"/></div></div></a></div><p id="9c16" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">论文</strong>:</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="pw mm l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://arxiv.org/pdf/1906.02569v1.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="3913" class="nh ni jg bd nj nk pi nm nn no pj nq nr kv pk kw nt ky pl kz nv lb pm lc nx ny bi translated">可视化:设置为眩晕</h1><p id="c1ef" class="pw-post-body-paragraph ll lm jg ln b lo og kq lq lr oh kt lt lu oi lw lx ly oj ma mb mc ok me mf mg ij bi translated">一个新的python可视化库问世了。光学效果令人印象深刻。如果你想走出matplotlib书呆子的世界，看看它令人惊叹的视觉效果。</p><p id="c68d" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><em class="oc">绘制一个简单的文本可视化只需要10行代码:</em></p><ol class=""><li id="2a9b" class="on oo jg ln b lo lp lr ls lu op ly oq mc or mg os ot ou ov bi translated"><em class="oc">第3行导入matplotlib，复用和可视化样式；</em></li><li id="99ba" class="on oo jg ln b lo po lr pp lu pq ly pr mc ps mg os ot ou ov bi translated"><em class="oc">第3行设置可视化对象，加载数据，设置样式；</em></li><li id="c6b7" class="on oo jg ln b lo po lr pp lu pq ly pr mc ps mg os ot ou ov bi translated"><em class="oc"> 4行绘制和显示可视化，包括标题和题注。</em></li></ol><div class="ip iq gp gr ir mn"><a href="https://github.com/NicholasMamo/multiplex-plot" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">Nicholas mamo/复合图</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">可视化应该讲述一个故事，并以一种美丽的方式讲述。Multiplex是Python的可视化库…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">github.com</p></div></div><div class="mw l"><div class="px l my mz na mw nb ix mn"/></div></div></a></div><h1 id="f0fe" class="nh ni jg bd nj nk pi nm nn no pj nq nr kv pk kw nt ky pl kz nv lb pm lc nx ny bi translated">基于图的深度学习报告</h1><p id="b8b5" class="pw-post-body-paragraph ll lm jg ln b lo og kq lq lr oh kt lt lu oi lw lx ly oj ma mb mc ok me mf mg ij bi translated">如果你想了解图表和深度学习的内幕，这是一个很方便的资源。本报告包含按年份和会议索引的研究文献/调查综述。👀</p><div class="ip iq gp gr ir mn"><a href="https://github.com/naganandy/graph-based-deep-learning-literature" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">nagan Andy/基于图形的深度学习文献</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">该存储库包含基于图形的深度学习中的链接。会议出版物的链接排列在…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">github.com</p></div></div><div class="mw l"><div class="py l my mz na mw nb ix mn"/></div></div></a></div><h1 id="5799" class="nh ni jg bd nj nk pi nm nn no pj nq nr kv pk kw nt ky pl kz nv lb pm lc nx ny bi translated">开放域对话式人工智能</h1><p id="279e" class="pw-post-body-paragraph ll lm jg ln b lo og kq lq lr oh kt lt lu oi lw lx ly oj ma mb mc ok me mf mg ij bi translated">脸书人工智能，其ParlAI库在开放域对话代理方面是世界一流的(还记得Blender)。他们发布了一个说明性的概述，介绍了如何构建优秀的对话代理、当前的研究和未来的方向。</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="pw mm l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://arxiv.org/pdf/2006.12442.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="3499" class="nh ni jg bd nj nk pi nm nn no pj nq nr kv pk kw nt ky pl kz nv lb pm lc nx ny bi translated">本周数据集:关键角色龙与地下城数据集(CRD3)</h1><h2 id="0ae7" class="ow ni jg bd nj ox oy dn nn oz pa dp nr lu pb pc nt ly pd pe nv mc pf pg nx jm bi translated">这是什么？</h2><p id="ab3d" class="pw-post-body-paragraph ll lm jg ln b lo og kq lq lr oh kt lt lu oi lw lx ly oj ma mb mc ok me mf mg ij bi translated">数据集从转录成文本对话的159个关键角色DD片段中收集，包括398，682个话轮。它还包括从Fandom wiki收集的相应抽象摘要。</p><h2 id="6792" class="ow ni jg bd nj ox oy dn nn oz pa dp nr lu pb pc nt ly pd pe nv mc pf pg nx jm bi translated">样品</h2><figure class="mh mi mj mk gt is gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/a34042155aace017d98363ca7374ce53.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/0*DCutgUxq5n89d_Qh.PNG"/></div></figure><h2 id="b6b5" class="ow ni jg bd nj ox oy dn nn oz pa dp nr lu pb pc nt ly pd pe nv mc pf pg nx jm bi translated">它在哪里？</h2><div class="ip iq gp gr ir mn"><a href="https://github.com/RevanthRameshkumar/CRD3" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jq gy z fp ms fr fs mt fu fw jp bi translated">RevanthRameshkumar/CRD3</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">本文描述了关键角色龙与地下城数据集(CRD3)和相关分析。关键角色是一个…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">github.com</p></div></div><div class="mw l"><div class="qa l my mz na mw nb ix mn"/></div></div></a></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><blockquote class="qb"><p id="9bab" class="qc qd jg bd qe qf qg qh qi qj qk mg dk translated"><em class="ql">每周日，我们都会对来自世界各地研究人员的NLP新闻和代码进行一次每周综述。</em></p><p id="9116" class="qc qd jg bd qe qf qg qh qi qj qk mg dk translated">如果您喜欢这篇文章，请帮助我们并与朋友分享！</p><p id="a4c0" class="qc qd jg bd qe qf qg qh qi qj qk mg dk translated"><em class="ql">如需完整报道，请关注我们的推特:</em><a class="ae jd" href="http://twitter.com/Quantum_Stat" rel="noopener ugc nofollow" target="_blank"><em class="ql">@ Quantum _ Stat</em></a></p></blockquote><figure class="qn qo qp qq qr is gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/c0644a2851c6647c167d1811c2157502.png" data-original-src="https://miro.medium.com/v2/resize:fit:108/0*CoFlcBLeiQ7kqlgW"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="http://www.quantumstat.com/" rel="noopener ugc nofollow" target="_blank">www.quantumstat.com</a></figcaption></figure></div></div>    
</body>
</html>