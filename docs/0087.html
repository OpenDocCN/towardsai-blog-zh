<html>
<head>
<title>Near-Optimal Representation Learning for Hierarchical Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于分层强化学习的近似最优表示学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/near-optimal-representation-learning-for-hierarchical-reinforcement-learning-37e01c24a299?source=collection_archive---------0-----------------------#2019-06-25">https://pub.towardsai.net/near-optimal-representation-learning-for-hierarchical-reinforcement-learning-37e01c24a299?source=collection_archive---------0-----------------------#2019-06-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e6ef" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="d64d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">超越分层强化学习与偏离政策的纠正(HIRO)</h2></div><h1 id="da04" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">介绍</h1><p id="1fb4" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">这是该系列的第二篇文章，在这篇文章中，我们将讨论一种新的分层强化学习，它建立在我们在<a class="ae mf" href="https://towardsdatascience.com/data-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80?source=friends_link&amp;sk=9a800a0ccd8f074bbfec7dd09e9835a2" rel="noopener" target="_blank">上一篇文章</a>中讨论过的<strong class="ll jd">嗨</strong>分层<strong class="ll jd"> R </strong>强化学习和<strong class="ll jd">O</strong>ff-策略修正(HIRO)的基础上。</p><p id="7aa3" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">这个员额由两部分组成。在第一部分，我们首先比较了HRL和HIRO的表征学习体系；然后，我们从论文中的权利要求4开始，看看如何学习导致有界次优的良好表示，以及如何定义低级策略的内在回报；我们将在本节末尾提供算法的伪代码。在部分讨论中，我们将为算法带来一些见解，并将低级策略连接到概率图形模型，以建立一些直觉。</p><p id="3ad7" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">为了更好地阅读数学表达式和最新更新，你可能想参考<a class="ae mf" href="https://xlnwel.github.io/blog/reinforcement%20learning/NORL-HRL/" rel="noopener ugc nofollow" target="_blank">我的个人博客</a>。</p><h2 id="6624" class="ml ks it bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh iz bi translated">结构</h2><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi mw"><img src="../Images/c46816c00b568ebdbc27bbd846255aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wBESP3pkpLn0Ql6b5xaJzA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">摘自O . f . nach um等人的《用于分层强化学习的近似最佳表示学习》</figcaption></figure><p id="eada" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">不同于HIRO，其中目标用作当前状态和期望状态之间的相异度的度量，这里的目标用于结合当前状态直接产生较低级别的政策。形式上，我们使用较高级别的策略对目标<em class="nm"> gₜ∼π^{high}(g|sₜ) </em>进行采样，并将其转换为较低级别的策略<em class="nm">πₜˡᵒʷ=ψ(sₜ,gₜ)</em>，后者用于对<em class="nm"> a_{t+k}∼πₜˡᵒʷ(a|s_{t+k}、k) </em>的行动进行采样。然后从<em class="nm"> s_{t+c} </em>开始重复该过程。(请注意，上面的低策略描述是一个理论模型，它不同于我们稍后将讨论的实践中的模型)</p><p id="45d0" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">映射<em class="nm">ψ</em>通常表示为策略空间上RL优化的结果</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nn"><img src="../Images/2228b749b176aad5fba88bc3e5738829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ndC9ht6RV2E8ZBmOn6d4VQ.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">等式1理论上期望的低级策略</figcaption></figure><p id="2c3f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中<em class="nm"> P_π(s_{t+k}|sₜ) </em>表示从<em class="nm"> sₜ </em>开始跟随<em class="nm">π</em>k步后处于状态<em class="nm"> s_{t+k} </em>的概率，<em class="nm"> f </em>表示将状态编码为低维表示的表示函数，<em class="nm"> D </em>是距离函数(例如<a class="ae mf" href="https://en.wikipedia.org/wiki/Huber_loss" rel="noopener ugc nofollow" target="_blank">胡伯函数</a>，其负值表示固有的低水平奖励。注意这里的<em class="nm"> D </em>与HIRO定义的主要有两点不同:1 .由于<em class="nm"> gₜ </em>不再是相异度的度量，因此不涉及<em class="nm">sₜ</em>—<em class="nm">gₜ</em>现在更像是<em class="nm"> s_{t+c} </em>在目标空间的投影。2.现在在目标空间(通常是低维空间)而不是原始状态空间上测量距离。</p><h2 id="8ece" class="ml ks it bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh iz bi translated">好的表示导致有界的次优</h2><p id="a4ae" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">等式1让我们看到了期望的低级策略看起来像什么，但是它没有提供关于应该如何定义表示学习函数<em class="nm"> f </em>的任何洞察。论文中的权利要求4表明，如果我们将<em class="nm">ψ</em>定义为等式1中给出的传统目标的轻微修改，那么我们可以将<em class="nm">ψ</em>的次优性转化为<em class="nm"> f </em>的实际表示学习目标。(不要被符号和术语吓到，很快会有解释)</p><blockquote class="no np nq"><p id="766c" class="lj lk nm ll b lm mg kd lo lp mh kg lr nr mi lu lv ns mj ly lz nt mk mc md me im bi translated"><strong class="ll jd">权利要求4 </strong>:设ρ(s)是状态空间s上的先验，设f和φ为</p></blockquote><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nu"><img src="../Images/e87b358672640ae4afc530ea5e9d11b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_HfAOKUz6xNJVaZ5VFbLmw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">等式2代表目标</figcaption></figure><blockquote class="no np nq"><p id="b56e" class="lj lk nm ll b lm mg kd lo lp mh kg lr nr mi lu lv ns mj ly lz nt mk mc md me im bi translated">如果低级目标被定义为</p></blockquote><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nv"><img src="../Images/76e2de7cfd3b17359ad36b9fa453f506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3k992-iSX2yX5i56qc7ow.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">方程式3低水平政策目标</figcaption></figure><blockquote class="no np nq"><p id="8b84" class="lj lk nm ll b lm mg kd lo lp mh kg lr nr mi lu lv ns mj ly lz nt mk mc md me im bi translated">那么ψ的次优性被Cϵ所有界</p></blockquote><p id="c3e9" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们没有在这里证明这种说法，因为论文提供的详细证明相当简洁，但仍然需要两页以上。在这里，我们将解决上面使用的那些符号和术语，稍后我们将对此声明提出一些见解:</p><ul class=""><li id="43f3" class="nw nx it ll b lm mg lp mh ls ny lw nz ma oa me ob oc od oe bi translated"><em class="nm"> f </em>是我们在<em class="nm">之前讨论过的表征学习函数。φ </em>是一个辅助的逆目标模型，旨在预测哪个目标会导致<em class="nm">ψ</em>产生一个策略<em class="nm">\ tildeπ=ψ(s，g) </em>诱导后续分配<em class="nm">p _ { \ tildeπ}(s_{t+k}|sₜ)</em>类似于<em class="nm"> P_π(s_{t+k}|sₜ) </em>对于<em class="nm">k∈【1，c】</em>。正如我们将很快看到的，f 和φ<em class="nm">都通过最小化等式2来优化。</em></li><li id="1d2c" class="nw nx it ll b lm of lp og ls oh lw oi ma oj me ob oc od oe bi translated">我们通过权重<em class="nm"> wₖ=1 </em>对<em class="nm"> k &lt; c </em>和wₖ=(1-γ)^{-1} fo<em class="nm">r k</em>=<em class="nm">c .</em>对分布之间的KL散度进行加权。我们进一步将<em class="nm"> \bar w </em>表示为在<em class="nm"> c </em>步<em class="nm"> ∑w_{1:c} </em>上贴现的<em class="nm"> w </em>之和，这将所有权重归一化，以便它们被加和为一。</li><li id="2172" class="nw nx it ll b lm of lp og ls oh lw oi ma oj me ob oc od oe bi translated">作者将<em class="nm"> K </em>解释为联合分布<em class="nm">P(state = s ')P(repr = Z | state = s ')=ρ(s ')(exp(-D(f(s ')，z))/Z) </em>的条件<em class="nm">P(state = s ')<em class="nm">Z</em>。这样，我们可以把<em class="nm"> P(repr=z|stat=s') </em>看成一个输入logits <em class="nm"> -D(f(s ')，z)的玻尔兹曼分布。</em>正如我们将在后面看到的，通过以这种方式设计<em class="nm"> K </em>，我们可以将表示学习与<a class="ae mf" href="https://xlnwel.github.io/blog/representation%20learning/MINE/" rel="noopener ugc nofollow" target="_blank"> MINE </a>对齐，并且基于<a class="ae mf" href="https://xlnwel.github.io/blog/reinforcement%20learning/PGM/" rel="noopener ugc nofollow" target="_blank">概率图形模型</a>给出更好的解释。</em></li><li id="86ca" class="nw nx it ll b lm of lp og ls oh lw oi ma oj me ob oc od oe bi translated"><em class="nm">ψ</em>的次最优性根据状态值测量使用策略<em class="nm">ψ</em>相对于最优策略的损失。形式上定义为最优策略π*与π^{hier}通过<em class="nm">ψ</em>学习到的分层策略之间的最大值差，即，</li></ul><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/1f3b5d0cf25b74435dc4ceae606066f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*Ne_PnFi2yQ65tSY1S0w2CA.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">次优，定义为最优策略和分级策略之间的最大值差</figcaption></figure><ul class=""><li id="1b24" class="nw nx it ll b lm mg lp mh ls ny lw nz ma oa me ob oc od oe bi translated">低级目标也可以转换成KL</li></ul><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ol"><img src="../Images/032de3df89c9b9b50e6af7f49e401e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nzx8cd2zmA7-sJ3Q7nhQTw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">等式4 KL版本的低层次政策目标</figcaption></figure><p id="0ef9" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中公式3中的<em class="nm"> g </em>替换为<em class="nm"> h φ(sₜ,π) </em>。其实情商。4等同于等式2的LHS，这为我们带来了策略优化和表征学习之间的相关性的很好的解释。我们将在分组讨论中回到这一点。</p><h2 id="ebe1" class="ml ks it bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh iz bi translated">学问</h2><p id="ddc2" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><strong class="ll jd">表象学习</strong></p><p id="ae83" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们先用<em class="nm"> θ </em>参数化表征学习函数<em class="nm"> f </em>和逆目标模型<em class="nm"> φ </em>。等式2中的上确界表示<em class="nm"> f </em>和<em class="nm"> φ </em>应该最小化上确界的内部。然而，实际上，我们无法访问策略表示<em class="nm"> π </em>。因此，作者建议选择从重放缓冲区均匀采样的<em class="nm"> sₜ </em>，并使用随后的<em class="nm"> c </em>动作a_{t:t+c-1}作为策略的表示。因此，我们的表示学习目标是</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi om"><img src="../Images/c7ebf5cb582163e3b1634291e835c018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BK_QyeSu8yvQCluXMvd_9w.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">公式5表示学习目标</figcaption></figure><p id="cc14" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">为了在下面的讨论中简化符号，我们将缩写如下</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi on"><img src="../Images/cc880d000f214de68810788c29fb709a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sFJnW31qa8JIsDi9fjx3hw.png"/></div></div></figure><p id="d7ad" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">同样，回想一下等式2</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi oo"><img src="../Images/08c6ccc5a676bfa54b1bd44d3f4e50fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1o0MJnGP8MqLWXGFGSH-OA.png"/></div></div></figure><p id="e912" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">现在我们扩展等式5中的目标</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi op"><img src="../Images/06961701a81286e99cef5ab4ec47f4bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j39EMt6WW8rkyuLxNGwiCA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">等式6表示学习目标</figcaption></figure><p id="6f04" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中尽可能省略与<em class="nm"> θ </em>无关的术语。注意，等式6中的内容正是地雷估计器的目标。这表明我们的表征学习目标实际上是最大化<em class="nm"> π </em>和<em class="nm"> s_{t+k} </em>之间的互信息，随着<em class="nm"> k </em>增加而减少。正如在我的文章中，括号中第二项的梯度将引入偏倚，作者建议将第二项替换为</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/aa1411be4188e8a6c267d612aceb9ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*mYFCq8Emyj0Mj5vsp6D1ZQ.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">等式7对数分区函数的替换</figcaption></figure><p id="72fe" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中使用从重放缓冲器采样的附加小批量状态来近似分母，并且没有通过分母反向传播的梯度。</p><p id="4e1d" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">由于等式6本质上是对于<em class="nm"> k ∈ [1，c] </em>最大化<em class="nm"> π </em>和<em class="nm"> s_{t+k} </em>之间的互信息，因此可以进一步使用诸如<a class="ae mf" href="https://xlnwel.github.io/blog/representation%20learning/DIM/" rel="noopener ugc nofollow" target="_blank"> DIM </a>的其他方法来提高性能。</p><p id="6beb" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><strong class="ll jd">低级策略学习</strong></p><p id="96c8" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">等式3建议为每个<em class="nm"> sₜ，g </em>优化一个策略<em class="nm"> π_{sₜ，g}(a|s_{t+k}，k) </em>。这相当于最大化参数化<em class="nm"> π^{low}(a|sₜ，g，s_{t+k}，k) </em>，这是目标条件分层设计中的标准。可以采用标准RL算法来最大化由以下等式暗示的低级别奖励</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi or"><img src="../Images/b38c9bb4657f85371af3ef59f5751505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C1YtIPmYhAWa36A3iGveZA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">低级奖励功能</figcaption></figure><p id="6659" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中括号中的第一项计算起来很简单，但其余项通常是未知的。为了解决这个问题，我们将<em class="nm"> P_π(s_{t+k}|sₜ) </em>替换为<em class="nm"> K_θ(s_{t+k}|s_t，π) </em>(因为等式5)，最后得到</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi os"><img src="../Images/4fa5bd92ed7dd0369ac43384cf361aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p1AxzIdgzKAQ_CAc08sknA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">等式8低级奖励函数</figcaption></figure><p id="d60b" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中π近似为<em class="nm"> a_{t:t+c-1} </em>如前。现在我们有了可以从等式8计算出来的低级奖励，低级策略可以像我们在HIRO那样用一些非策略方法来学习。</p><h2 id="6fb6" class="ml ks it bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh iz bi translated">算法</h2><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ot"><img src="../Images/00905e704d06aeea7613f1b5fa37e5a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7AsHm3Tu_asplZJ7jpx_Q.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">O fir na chum等人的伪代码，用于分层强化学习的近似最佳表示学习</figcaption></figure></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><h2 id="77b3" class="ml ks it bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh iz bi translated">讨论</h2><p id="a066" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在这一节中，我谦恭地讨论一下阅读这篇论文时的一些个人想法。</p><p id="cea1" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><strong class="ll jd">对<em class="nm">的不同解读【k(s_{t+k}|sₜπ】</em></strong></p><p id="3c40" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">如果我们将<em class="nm">【-d(f(s_{t+k}),φ(sₜ,π】)</em>作为奖励函数<em class="nm"> r(sₜ，π，s_{t+k}) </em>，并将<em class="nm"> K </em>的定义与我们在<a class="ae mf" href="https://xlnwel.github.io/blog/reinforcement%20learning/PGM/" rel="noopener ugc nofollow" target="_blank">这篇</a>文章中讨论过的概率图模型(PGM)联系起来，我们可以得到如下PGM</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi pb"><img src="../Images/6a580b84806efa66cf18150a7789e29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87PxZ_UJkvllR0WhhueUXw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">K的概率图形视图</figcaption></figure><p id="eca4" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">那么我们有</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi pc"><img src="../Images/06b6a1fb184bfac1a736e582c8b687d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yz7sIJdW65MnheeNXW8-ng.png"/></div></div></figure><p id="4ffc" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">现在我们可以将<em class="nm"> K(s_{t+k}|sₜ,π) </em>解释为给定当前状态<em class="nm"> sₜ </em>和策略<em class="nm">T7】π，状态<em class="nm"> s_{t+k} </em>最优的概率。</em></p><p id="fa06" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><strong class="ll jd">低级策略优化和表征学习之间的相关性</strong></p><p id="1f39" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">如等式所示。如图4和5所示，如果我们忽略所有权重和折扣因子，并且为了简单起见仅考虑单个时间戳，则低级策略优化和表示学习本质上都最小化KL发散，如下所示</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi pd"><img src="../Images/db99229da015341b765cbdaa209add19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Z5yZZokwLQNpPqvqOhFVQ.png"/></div></div></figure><p id="6da4" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中<em class="nm"> φ_θ(sₜ、π) </em>和<em class="nm"> g </em>可以互换。从这个KL发散中我们可以看出:底层策略优化绘制<em class="nm"> P_{π'} </em>接近<em class="nm"> K_θ </em>，而表征学习绘制<em class="nm"> K_θ </em>接近<em class="nm"> P_{π'} </em>。直观上，我们在优化低级策略时，希望<em class="nm"> s_{t+k} </em>在该策略下的分布等于<em class="nm"> s_{t+k} </em>最优的概率。另一方面，我们在做表征学习时，最大化<em class="nm"> s_{t+k} </em>和π之间的互信息，使得底层策略更容易优化。</p><p id="e97f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们是否应该像在HIRO那样，在这里重新标注目标？</p><p id="a1a3" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">这个问题困扰了我一段时间，因为它没有在论文中提到。到目前为止，我个人的答案是，也许我们也应该重新标记，但它不像在HIRO那样紧迫。HIRO要求重新标记目标的相同原因在这里仍然有效:随着低级策略的发展，以前收集的转换元组可能不再对高级策略有效。然而，在某种意义上，这个问题可能会因为我们在较低的维度中表示目标而得到缓解。</p><p id="3410" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">好吧，也许论文以一种非常含蓄的方式提到了这一点……用于训练高级策略的经验包括状态和低级动作序列，这仅对于偏离策略的纠正有用。</p><p id="5bca" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">感谢O fr nach um在回复中澄清了这个问题:-)是的，目标确实像在HIRO那样被重新标记。</p><h2 id="d252" class="ml ks it bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh iz bi translated">结束</h2><p id="5975" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">嗯，这是一个漫长的旅程。希望你喜欢它。如果你遇到任何困惑，欢迎留言。</p><p id="6322" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">感谢人工智能团队帮助我们接触奥弗尔纳丘姆:-)</p><h2 id="4565" class="ml ks it bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh iz bi translated">参考</h2><p id="a222" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">Mohamed Ishmael Belghazi等.互信息神经估计</p><p id="518f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">用于分层强化学习的近似最佳表示学习</p></div></div>    
</body>
</html>