<html>
<head>
<title>A Visual Journey in What Vision-Transformers See</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视觉变形者眼中的视觉之旅</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=collection_archive---------3-----------------------#2022-12-22">https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=collection_archive---------3-----------------------#2022-12-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a007" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一些大模特如何看待这个世界</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/628267e469af4e1521c6add37b1c7eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1P7Z2q28DUEK0t3UjCkbTA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自原文章:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="0118" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可视化CNN让我们了解了更多关于这些模型是如何工作的。既然《视觉变形金刚》正走上舞台，一篇新文章解释了我们如何能看到这些广义模型眼中的世界。</p><h1 id="e213" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">想象视觉变形金刚</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/f00b4f175bcb5714d38b9424900623b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5X-0KhvFcPKqUa3wx-Q6LA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自文章原文:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="7dc3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">自从</strong> <a class="ae kv" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">卷积神经网络</strong> </a> <strong class="ky ir"> (CNN)成为计算机视觉中的一个获奖模型以来，不同的研究小组一直专注于了解这些模型学习什么。</strong></p><p id="4379" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一方面，神经网络已经出现在几个领域(从语言分析到计算机视觉)，但一直被认为是“黑箱”。与许多其他算法相比，它们更难解释。事实上，模型变得越强大(参数数量的增长)，就越难以理解内部的情况。</p><p id="8875" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，已经开发了几种方法来可视化<a class="ae kv" href="https://cs231n.github.io/understanding-cnn/" rel="noopener ugc nofollow" target="_blank">卷积神经网络学习的内容</a>。一些最常用的:</p><ul class=""><li id="d098" class="ml mm iq ky b kz la lc ld lf mn lj mo ln mp lr mq mr ms mt bi translated">可视化过滤器(或可视化权重)。</li><li id="fada" class="ml mm iq ky b kz mu lc mv lf mw lj mx ln my lr mq mr ms mt bi translated">可视化层激活</li><li id="1335" class="ml mm iq ky b kz mu lc mv lf mw lj mx ln my lr mq mr ms mt bi translated">检索最大程度激活神经元的图像</li><li id="df37" class="ml mm iq ky b kz mu lc mv lf mw lj mx ln my lr mq mr ms mt bi translated">用t-SNE嵌入特征向量。</li><li id="3543" class="ml mm iq ky b kz mu lc mv lf mw lj mx ln my lr mq mr ms mt bi translated">GradCAM，显著图。</li></ul><p id="6e53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2016年<a class="ae kv" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener ugc nofollow" target="_blank">变形金刚</a>登场。这些基于自我关注的广泛模型已经被证明在NLP(机器翻译、语言分类等)中实现了更好的性能。很快，它们成为了NLP的标准，随着视觉变形器的引入，它们也被应用于计算机视觉。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/333edd7f3b2d445f53957bccb63de726.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*PpM_2mheZE7u-bnA40OeHQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来自原变压器篇:<a class="ae kv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="4d80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，不同的研究人员试图将 <a class="ae kv" href="https://en.wikipedia.org/wiki/Vision_transformer" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">视觉变形金刚</strong> </a> <strong class="ky ir"> (ViTs)学到的东西可视化。</strong>vit已被证明更加难以分析，而且到目前为止，所用的方法已显示出局限性。<strong class="ky ir">理解这些模型的内部运作可能有助于解释它们的成功和潜在的困境。</strong></p><p id="bdd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以前的工作集中在观察自我注意层的键、查询和值的激活，但结果是不成功的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/9bd35d63af234656aaf6441392a3e363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*RevsRYWCfGKpckidKUbhAg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">视觉化自我关注的重量并不会带来深刻的视觉化。原文章的标题和图片:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="2690" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">纽约大学和马里兰大学<strong class="ky ir">的研究人员最近发表了一篇论文</strong>，更好地理解了模型</strong>内部发生的事情(无论它们是视觉变形金刚还是像<a class="ae kv" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> CLIP </a>这样的模型)。</p><p id="2397" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在文章中，研究人员总结了他们的贡献:</p><ol class=""><li id="618a" class="ml mm iq ky b kz la lc ld lf mn lj mo ln mp lr nb mr ms mt bi translated">虽然标准方法会导致无法解释的结果(特别是在应用于键、查询和值时)，但通过将相同的技术应用于同一变压器模块的下一个前馈层，可以获得信息丰富的可视化效果(他们使用不同的模型演示了这一点:ViTs、DeiT、CoaT、ConViT、PiT、Swin和Twin transformers)。</li><li id="3d64" class="ml mm iq ky b kz mu lc mv lf mw lj mx ln my lr nb mr ms mt bi translated">ViT特征的逐片图像激活模式的行为类似于显著图，表明该模型保持了片之间的位置关系(并且在训练期间学习这一点)。</li><li id="0a1f" class="ml mm iq ky b kz mu lc mv lf mw lj mx ln my lr nb mr ms mt bi translated">CNN和ViTs构建了一个复杂的渐进式表示(在CNN中，第一层表示边缘和纹理，而后面的层学习更复杂的模式，作者表明在ViTs中也会发生同样的情况)。与CNN相比，vit能够更好地利用背景信息。</li><li id="8175" class="ml mm iq ky b kz mu lc mv lf mw lj mx ln my lr nb mr ms mt bi translated">作者还将他们的方法应用于使用语言监督的模型(如CLIP)，并表明可以从这些模型中提取与字幕文本相关的特征(如介词、形容词和概念类别)。</li></ol><p id="6f67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者将vit与卷积网络进行了比较，并注意到表示的复杂性随着模式的增加而增加(更早的层学习更简单的结构，而更高级的层学习更复杂的模式)。<strong class="ky ir">实际上，CNN和ViTs都有一个共同的特点，那就是所谓的渐进式专业化。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/4a386a18181f67514684a6b58ea222e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_u6LYCG9tTg2oTu8sjxYQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">维生素B-32可视化特征的进展。早期图层的特征捕捉一般的边缘和纹理。移动到更深的层，特征进化到捕捉更专业的图像成分，并最终捕捉具体的对象。”原文章的标题和图片:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/2acd47ae7b21903449fdd964ca00501e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BsV7tdRXHURwOF5srBQJzg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">“ViT B-32特性的复杂性与深度。可视化表明，vit类似于CNN，因为它们显示了从纹理到零件到对象的特征进展，正如我们从浅到深的特征进展一样。”原文的说明和图片:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="2b0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也有不同之处。作者研究了vit和CNN对背景和前景图像特征的依赖性(使用ImageNet上的边界框)。<strong class="ky ir"> ViTs能够检测图像中的背景信息</strong>(例如图像中的草地和雪地)。此外，通过掩盖图像中的背景或前景，研究人员表明<strong class="ky ir">vit不仅可以更好地利用背景信息，而且受其移除的影响也更小。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/8ae84392fca8e997be09508efa171093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ZK6bMDcn7FDa6EzmIQxPg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">“维特-B16检测背景特征。左:图像优化，最大限度地激活第6层的功能。中心:来自ImageNet的相应最大激活示例。右图:图像的逐片激活图。(b):原始图像和被遮罩的前景和背景的示例原文章的标题和图片:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><blockquote class="nf ng nh"><p id="a0d2" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">我们发现令人惊讶的是，即使每个小块可以影响每个其他小块的表示，这些表示仍然是局部的，甚至对于网络深层的单个通道也是如此。虽然对CNNs(其神经元可能具有有限的感受野)的类似发现并不令人惊讶，但即使在ViT的第一层中的神经元也具有完整的感受野。换句话说，vit学会保存空间信息，尽管缺乏CNN的归纳偏向。-来源:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></blockquote><p id="cbbe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">换句话说，在训练期间，模型学习如何保存空间信息。</strong>此外，最后一层反而具有统一的激活模式，并学习如何对图像进行分类(根据作者的说法，最后一层具有全球化信息的功能)。</p><blockquote class="nf ng nh"><p id="0b1a" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">基于空间信息在斑块中的保存，我们假设CLS令牌在整个网络中扮演相对次要的角色，并且直到最后一层才用于全球化。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/209cff717e3352f067ee97af83464ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-n1fBNTNFfR7dwNWsoe2w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">“来自ViT前馈层的特征可视化示例。左:图像优化，以最大限度地激活第5层的功能。中心:对应最大化激活ImageNet示例。右图:图像的逐片激活图。(b):购物车最常激活的最后一层中的一个要素原文章的标题和图片:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="bb6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">近年来，视觉转换器模型已经用语言监督和对比学习技术进行了训练。其中一个例子是剪辑。因为这些模型的使用越来越多，竞争越来越激烈，所以作者也分析了CLIP。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/1018f8c926fd45d4e36ded5dfa57b5ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rPNDQE8-yYSvQOUv-hYNqQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">左:功能优化显示清晰的边界，最大化激活ImageNet示例包含不同的相邻图像。中图:功能优化和最大化激活ImageNet照片都是从一个升高的有利位置显示图像。正确:功能优化显示了一群人，但最大限度地激活图像表明对象的重复比对象的类型更相关。原文的说明和图片:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="0c26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型显示了与推测相关的特征，例如“之前和之后”或“从上面”换句话说，有一些特征代表了概念类别，并且是清晰可辨的:</p><blockquote class="nf ng nh"><p id="af94" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">来自数据集的相应的七个高度活跃的图像包括其他不同的对象，如带血的武器、僵尸和骨骼。从严格的视觉角度来看，这些类别具有非常不同的属性，表明该特征可能负责检测与发病率广泛相关的图像成分。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/32aeb3fbd7282c639fe96a582706055e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2rWJGrjjAdTEozGdu8pJCw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">“使用与发病率类别相关的CLIP训练的ViT特征。每个类别中的左上方图像:优化图像以最大程度地激活第10层的功能。其余:最能激活该功能的十个ImageNet图像中的七个。原文章的说明和图片:<a class="ae kv" href="https://arxiv.org/pdf/2212.06727.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="6fae" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="8624" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">要明白，眼见为实总是更好。近年来，人们越来越重视模型的可解释性。虽然在中枢神经系统上有许多有效的方法，但是能够可视化ViTs的特征是不可能的。</p><p id="c83c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者不仅确定了一种能够做到这一点的方法(他们表明必须使用前馈层，而不是自我关注层)，而且还分析了这些特征的属性。他们展示了模型如何能够在训练期间学习空间关系，以及另一方面，最后一层如何不参与这种空间表示。</p><p id="96cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，虽然vit类似于卷积网络，但作者的成功部分源于他们如何更好地利用背景相关信息。他们还表明，当vit在语言模型监督下接受d训练时，他们学习的是更多的语义和概念特征，而不是特定于对象的视觉特征。</p><p id="6c2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">代号</strong>:此处<a class="ae kv" href="https://github.com/hamidkazemi22/vit-visualization" rel="noopener ugc nofollow" target="_blank"/>，<strong class="ky ir">条</strong> : <a class="ae kv" href="https://arxiv.org/abs/2212.06727" rel="noopener ugc nofollow" target="_blank">此处</a></p><h1 id="5787" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如果你觉得有趣:</h1><p id="7dd9" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">你可以寻找我的其他文章，你也可以<a class="ae kv" href="https://salvatore-raieli.medium.com/subscribe" rel="noopener"> <strong class="ky ir">订阅</strong> </a>在我发表文章时得到通知，你也可以在<strong class="ky ir"/><a class="ae kv" href="https://www.linkedin.com/in/salvatore-raieli/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">LinkedIn</strong></a><strong class="ky ir">上连接或联系我。</strong>感谢您的支持！</p><p id="2b68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我的GitHub知识库的链接，我计划在这里收集代码和许多与机器学习、人工智能等相关的资源。</p><div class="nu nv gp gr nw nx"><a href="https://github.com/SalvatoreRa/tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">GitHub - SalvatoreRa/tutorial:关于机器学习、人工智能、数据科学的教程…</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">关于机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码(python…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">github.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol kp nx"/></div></div></a></div><p id="2deb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者随意查看我在Medium上的其他文章:</p><div class="nu nv gp gr nw nx"><a rel="noopener  ugc nofollow" target="_blank" href="/the-rise-of-ai-a-look-at-the-2022-landscape-956e7e3f1839"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">人工智能的崛起:2022年展望</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">创新与颠覆:2022年人工智能的展望</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">pub.towardsai.net</p></div></div><div class="og l"><div class="om l oi oj ok og ol kp nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://medium.com/mlearning-ai/twitters-acquisition-raises-red-flags-for-scientific-community-9b865fb2f694" rel="noopener follow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">Twitter的收购给科学界敲响了警钟</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">为什么科学家和数据科学家会关注</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">medium.com</p></div></div><div class="og l"><div class="on l oi oj ok og ol kp nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://medium.com/mlearning-ai/unleashing-the-power-of-generative-ai-the-definitive-list-13988d422c16" rel="noopener follow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">释放生成人工智能的力量:最终列表</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">探索人工智能技术的最新进展，以及它们如何让你受益</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">medium.com</p></div></div><div class="og l"><div class="oo l oi oj ok og ol kp nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a href="https://medium.com/mlearning-ai/can-an-ai-be-a-data-scientist-2d4d9b6c5d5" rel="noopener follow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd ir gy z fp oc fr fs od fu fw ip bi translated">人工智能可以成为数据科学家吗？</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">OpenAI的ChatGPT让数据科学家们大吃一惊。它会偷走他们的工作吗？</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">medium.com</p></div></div><div class="og l"><div class="op l oi oj ok og ol kp nx"/></div></div></a></div></div></div>    
</body>
</html>