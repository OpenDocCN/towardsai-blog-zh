<html>
<head>
<title>K-Fold Cross Validation Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了k倍交叉验证</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/k-fold-cross-validation-explained-ec99bec8e2b6?source=collection_archive---------3-----------------------#2022-04-25">https://pub.towardsai.net/k-fold-cross-validation-explained-ec99bec8e2b6?source=collection_archive---------3-----------------------#2022-04-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6769" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用SciKit-Learn和Yellowbrick库</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/5b2390c9f4c480d0e336b202f52801c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*IOwNOMoEMtU0NnuVHesP1A.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated">k倍交叉验证</figcaption></figure><h2 id="70f0" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">过度拟合问题</h2><p id="e116" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">机器学习最大的问题之一是过度拟合的风险。但是什么是<strong class="ls iu">过拟合</strong>？</p><p id="7692" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">当我们的模型的结果(准确性)在训练数据中非常好，但模型在测试或验证数据中未能获得相同的结果时，就会发生过度拟合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/939d2aa765389bd020660d0fc34142fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQ__HwyhiVEiNqRyGZtVqA.png"/></div></figure><p id="9201" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">在上面的图像中，我们可以看到训练集中的分数是85%，但是在测试集中，它只有81%。虽然差别不大，但这是一个过拟合的例子。测试集是专门创建的，用于检查我们的机器创建的模型是否过拟合。最常见的方法是为此使用部分数据，即，例如，80%的数据用于训练模型，20%的数据用于测试过拟合，并更好地了解模型的实际性能。功能</p><h2 id="3de9" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">训练/测试分割</h2><p id="e444" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">Scikit-Learn的<code class="fe mp mq mr ms b"><a class="ae mt" href="https://scikit-learn.org/0.17/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split" rel="noopener ugc nofollow" target="_blank"><strong class="ls iu">train_test_split</strong></a><strong class="ls iu"> </strong></code>函数用于对两组数据进行随机划分，从而保持随机性，如有必要，还保持每组结果的比例。</p><p id="2986" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">然而，在机器学习中，自动生成的模型需要研究人员进行一些优化(如SVM算法中的C参数)。对于这种优化，算法需要“看到”测试集中的一些数据，以便知道如何优化模型。</p><p id="c484" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">从模型“看到”测试集数据的那一刻起，它就开始了解它们，我们再次面临过度拟合的风险。在有大量数据的许多情况下，解决方案是创建第三个集合，即验证集。因此，模型的最终评估在验证集中进行。</p><h2 id="8207" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">但是如果你的数据很少怎么办呢？</h2><p id="1c6a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">如果可用数据量不是很大，那么将数据库分成三个集合会产生相反的效果，因为训练数据会急剧减少，这会损害模型的性能。</p><h2 id="defa" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">K倍解</h2><p id="efe2" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly ld lz ma mb lh mc md me ll mf mg mh mi im bi translated">交叉验证是一种在数据有限时使用的技术，它将数据重新用于训练和测试集。一种交叉验证方法中的K-Fold，它将数据分成随机选择的K个子样本，然后使用k-1个样本进行训练，1个样本进行测试。例如，如果我们选择5倍，在有100个条目的数据集中，我们的机器将生成5个子样本，每个子样本有20个条目。姑且称之为:</p><pre class="kj kk kl km gt mu ms mv mw aw mx bi"><span id="7264" class="ku kv it ms b gy my mz l na nb">Dataset n=100</span><span id="fc08" class="ku kv it ms b gy nc mz l na nb"><strong class="ms iu">#Generate 5-fold samples:</strong></span><span id="d6b4" class="ku kv it ms b gy nc mz l na nb">S1 (n=20)<br/>S2 (n=20)<br/>S3 (n=20)<br/>S4 (n=20)<br/>S5 (n=20)</span><span id="0d3b" class="ku kv it ms b gy nc mz l na nb"><strong class="ms iu">#Train and evaluate model:</strong></span><span id="fabd" class="ku kv it ms b gy nc mz l na nb">K1:<br/>Train with: S2+S3+S4+S5 (n=80)<br/>Test: S1 (n=20)<br/>K2:<br/>Train with: S1+S3+S4+S5 (n=80)<br/>Test: S2 (n=20)<br/>K3:<br/>Train with: S1+S2+S4+S5 (n=80)<br/>Test: S3 (n=20)<br/>K4:<br/>Train with: S1+S2+S3+S5 (n=80)<br/>Test: S4 (n=20)<br/>K5:<br/>Train with: S1+S2+S3+S4 (n=80)<br/>Test: S5 (n=20)</span><span id="3e6e" class="ku kv it ms b gy nc mz l na nb"><strong class="ms iu">#Overall accuracy:</strong><br/>Test accuracy: (test <strong class="ms iu">S1</strong> + test <strong class="ms iu">S2</strong> + test <strong class="ms iu">S3</strong> + test <strong class="ms iu">S4</strong> + test <strong class="ms iu">S5</strong>)<strong class="ms iu">/5</strong></span></pre><p id="364c" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">对于我所解释的，一个直观的解释是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/5b2390c9f4c480d0e336b202f52801c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*IOwNOMoEMtU0NnuVHesP1A.png"/></div></figure><p id="d927" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">现在我们来玩一些代码和数据。我将使用我在上一篇文章中已经使用过的数据集。你可以在 这里找到<a class="ae mt" rel="noopener ugc nofollow" target="_blank" href="/support-vector-machine-svm-for-binary-and-multiclass-classification-hands-on-with-scikit-learn-29cdbe5cb90e"> <strong class="ls iu">的下载和探索性数据分析的链接。</strong></a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/776297acfbbde58c8a2bef274aa758be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FzkiU_u1RIRhaAYEPI16CQ.png"/></div></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae mt" rel="noopener ugc nofollow" target="_blank" href="/support-vector-machine-svm-for-binary-and-multiclass-classification-hands-on-with-scikit-learn-29cdbe5cb90e">https://pub . toward sai . net/support-vector-machine-SVM-for-binary-and-multi-class-class-class-class ification-hands-on-with-sci kit-learn-29 cdbe 5 CB 90 e</a></figcaption></figure><p id="d111" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">与任何其他项目一样，我们从导入必要的库开始:</p><pre class="kj kk kl km gt mu ms mv mw aw mx bi"><span id="69ce" class="ku kv it ms b gy my mz l na nb">import numpy as np<br/>import pandas as pd</span><span id="c0d7" class="ku kv it ms b gy nc mz l na nb">from sklearn.model_selection import StratifiedKFold<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn.model_selection import cross_val_score</span><span id="7804" class="ku kv it ms b gy nc mz l na nb">from yellowbrick.datasets import load_occupancy<br/>from yellowbrick.model_selection import CVScores</span></pre><p id="246a" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">接下来，我们将导入、读取和检查我们的数据库。</p><pre class="kj kk kl km gt mu ms mv mw aw mx bi"><span id="4eb4" class="ku kv it ms b gy my mz l na nb">df = pd.read_csv("/content/weather.csv")<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ni"><img src="../Images/79ad2bdcad9ab4eda394d9982ee6fa69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZxdzNbwl3CWl2_7v5H2nMw.png"/></div></div></figure><p id="6923" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">我们的目标是使用除“<strong class="ls iu">温度_温度</strong>”之外的所有其他参数，对结果“<strong class="ls iu">描述</strong>”进行二元分类。因此，让我们创建X和y变量，排除这两个参数:</p><pre class="kj kk kl km gt mu ms mv mw aw mx bi"><span id="4f52" class="ku kv it ms b gy my mz l na nb">X = df.drop('Description', axis=1)<br/>X = X.drop('Temperature_c', axis=1)<br/>y = df['Description']</span></pre><p id="2db0" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">现在是时候选择我们的交叉验证策略了，这里我们要用k倍法，k=10。</p><pre class="kj kk kl km gt mu ms mv mw aw mx bi"><span id="2c20" class="ku kv it ms b gy my mz l na nb">cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)</span></pre><p id="3b4b" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">在这个例子中，我们将使用多项式朴素贝叶斯作为机器学习方法。我们需要定义和构建我们的模型。</p><pre class="kj kk kl km gt mu ms mv mw aw mx bi"><span id="d7e8" class="ku kv it ms b gy my mz l na nb">model = MultinomialNB()<br/>clf = model.fit(X,y)</span></pre><p id="bada" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">现在我们可以得到所有10次折叠的分数(准确度):</p><pre class="kj kk kl km gt mu ms mv mw aw mx bi"><span id="3528" class="ku kv it ms b gy my mz l na nb">scores = cross_val_score(clf, X, y, cv=cv)<br/>scores</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/879458b103650015c0333a41b902af9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*5NB6uWrW8-gRa8-L26rQtA.png"/></div></figure><p id="3e68" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">如果我们愿意，我们还可以构建一个图形可视化:</p><pre class="kj kk kl km gt mu ms mv mw aw mx bi"><span id="6eb8" class="ku kv it ms b gy my mz l na nb">visualizer = CVScores(model, cv=cv, scoring='f1_weighted')<br/>visualizer.fit(X, y)        <br/>visualizer.show()           </span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d028b9986e7bd3027db44411e3a18684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*auRbYKU4rkM3TPsGhIZwug.png"/></div></figure><p id="e039" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">现在我们可以看到平均分是0.402。这不是一个好的精度，但这是一个例子，现在你知道如何做到这一点，以及如何应用到你的数据！</p><p id="a88d" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">感谢您的阅读！如果你有建议要添加到这个列表中，请告诉我，不要忘记订阅以接收关于我未来出版物的通知。</p><p id="2c75" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">如果:你喜欢这篇文章，别忘了关注我，这样你就能收到所有关于新出版物的更新。</p><p id="aebb" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">否则如果:你想了解更多，你可以通过<a class="ae mt" href="https://cdanielaam.medium.com/membership" rel="noopener">我的推荐链接</a>订阅媒体会员。它不会花你更多的钱，但会支付我一杯咖啡。</p><p id="5249" class="pw-post-body-paragraph lq lr it ls b lt mj ju lv lw mk jx ly ld ml ma mb lh mm md me ll mn mg mh mi im bi translated">Else:谢谢！</p></div></div>    
</body>
</html>