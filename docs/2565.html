<html>
<head>
<title>KNN Algorithm for Classification and Regression: Hands-On With Scikit- Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于分类和回归的KNN算法:Scikit- Learn实践</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/knn-algorithm-for-classification-and-regression-hands-on-with-scikit-learn-4c5ec558cdba?source=collection_archive---------0-----------------------#2022-02-22">https://pub.towardsai.net/knn-algorithm-for-classification-and-regression-hands-on-with-scikit-learn-4c5ec558cdba?source=collection_archive---------0-----------------------#2022-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="74ae" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="0b96" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Google Colab和Python</h2></div><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="7ec7" class="la lb it kw b gy lc ld l le lf">Table of Contents:</span><span id="c974" class="la lb it kw b gy lg ld l le lf">A — KNN for classification<br/><strong class="kw jd"><em class="lh">1. Import Libraries<br/>2. Build a dataset<br/>3. Explore Dataset<br/>4. Setting variables for ML<br/>5. Split data into train and test<br/></em>6. Building the model<br/><em class="lh">7. Finding best k value<br/>8. Model Complexity<br/>9. Plotting decision boundaries</em></strong></span><span id="ab63" class="la lb it kw b gy lg ld l le lf">B — KNN for regression<br/><strong class="kw jd"><em class="lh">1. Import Libraries</em></strong><br/>...<br/><strong class="kw jd">6. Building the model<br/><em class="lh">7. Finding best k value<br/>8. Model Complexity</em></strong></span><span id="760f" class="la lb it kw b gy lg ld l le lf">C — Advantages and Disadvantages of KNN<br/><strong class="kw jd">Advantages<em class="lh"><br/></em>Disadvantages</strong></span></pre></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/6e3f5a584928fa004158daa403b8e0d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*aAfMi2d2_2d8sRxo-sVPaA.png"/></div></figure><p id="1c5e" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">k近邻算法是机器学习中最简单的算法之一。这是一种为输入数据集中最近邻的聚类分配一个点的聚类算法。该算法可用于将k-最近邻问题近似为分类任务，其中k是最近邻的数量。它还可以用于回归任务，通过找到相关邻居的平均差异。</p><h2 id="9718" class="la lb it bd mp mq mr dn ms mt mu dp mv mc mw mx my mg mz na nb mk nc nd ne iz bi translated">A — KNN分类:</h2><p id="e004" class="pw-post-body-paragraph lt lu it lv b lw nf kd ly lz ng kg mb mc nh me mf mg ni mi mj mk nj mm mn mo im bi translated">当使用KNN进行分类任务，并且k=1时，我们的模型简单地从数据集中搜索更接近我们想要分类的值的数据点。如果我们选择k=3、k=5或K= <em class="lh"> n </em>，模型将搜索更接近我们想要分类的值的3、5或<em class="lh"> n </em>个值。然后，我们的值被分配给具有更多值的类。</p><p id="cb30" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh">例1: </em> </strong>对于k=7，模型发现，从与我们的值比较接近的7个点中，有3个来自类0，4个来自类1。在这种情况下，我们的值将被分配给类1。</p><p id="9de8" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh">例2: </em> </strong>对于k=6，模型发现，在最接近我们值的6个点中，有3个来自类0，3个来自类1。在这种情况下，模型将根据我们的值和每个类的三个点计算中值欧氏距离。我们的值将被分配给具有较低欧几里德距离的类。</p><h2 id="86f1" class="la lb it bd mp mq mr dn ms mt mu dp mv mc mw mx my mg mz na nb mk nc nd ne iz bi translated"><strong class="ak">动手:</strong></h2><p id="9f99" class="pw-post-body-paragraph lt lu it lv b lw nf kd ly lz ng kg mb mc nh me mf mg ni mi mj mk nj mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 1。导入库:</em> </strong></p><p id="400e" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">正如机器学习中的任何其他项目一样，第一件事是导入必要的库:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="7e21" class="la lb it kw b gy lc ld l le lf">#Import Libraries:</span><span id="b435" class="la lb it kw b gy lg ld l le lf">from random import random<br/>from random import randint<br/>import numpy as np<br/>import pandas as pd</span><span id="077d" class="la lb it kw b gy lg ld l le lf">import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="d3d7" class="la lb it kw b gy lg ld l le lf">from sklearn.model_selection import train_test_split<br/>from sklearn.neighbors import KNeighborsClassifier</span><span id="81fa" class="la lb it kw b gy lg ld l le lf">from mlxtend.plotting import plot_decision_regions</span></pre><p id="0dc8" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 2。建立数据集:</em> </strong></p><p id="9a50" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">接下来，我将创建一个假数据集用于此任务。您可以出于学习目的进行同样的操作，或者如果您正在处理现实世界中的问题，也可以使用您的数据。</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="3230" class="la lb it kw b gy lc ld l le lf">#Fabricating variables:</span><span id="4c45" class="la lb it kw b gy lg ld l le lf">#Creating values for FeNO with 3 classes:<br/>FeNO_0 = np.random.randint(1,20, 100)<br/>FeNO_1 = np.random.randint(25,55, 100)<br/>FeNO_2 = np.random.randint(45, 100, 100)</span><span id="996b" class="la lb it kw b gy lg ld l le lf">#Creating values for FEV1 with 3 classes:<br/>FEV1_0 = [np.random.uniform(3.12,4.50) for _ in range(100)]<br/>FEV1_1 = [np.random.uniform(2.98,4.25) for _ in range(100)]<br/>FEV1_2 = [np.random.uniform(2.55,3.99) for _ in range(100)]</span><span id="2f11" class="la lb it kw b gy lg ld l le lf">#Creating values for Bronco Dilation with 3 classes:<br/>BD_0 = [np.random.uniform(0,0.180) for _ in range(100)]<br/>BD_1 = [np.random.uniform(0.160,0.250) for _ in range(100)]<br/>BD_2 = [np.random.uniform(0.200,0.700) for _ in range(100)]</span><span id="7b4b" class="la lb it kw b gy lg ld l le lf">#Creating labels variable with two classes (1)Disease (0)No disease:<br/>not_asma = np.zeros((150,), dtype=int)<br/>asma = np.ones((150,), dtype=int)</span></pre><p id="bd09" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">请注意，我已经用三个类创建了三个变量，而label变量只有两个类。这是有目的的，通过添加一些具有两个类特征的条目，在数据集中制造一些复杂性。</p><p id="7268" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">现在是时候将这三类变量连接成一个变量了:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="075b" class="la lb it kw b gy lc ld l le lf">#Concatenate classes into one variable:<br/>FeNO = np.concatenate([FeNO_0, FeNO_1, FeNO_2])<br/>FEV1 = np.concatenate([FEV1_0, FEV1_1, FEV1_2])<br/>BD = np.concatenate([BD_0, BD_1, BD_2])<br/>dx = np.concatenate([not_asma, asma])</span></pre><p id="585b" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">现在创建一个数据帧并将变量添加到数据帧中:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="83b1" class="la lb it kw b gy lc ld l le lf">#Create DataFrame:<br/>df = pd.DataFrame()</span><span id="bd81" class="la lb it kw b gy lg ld l le lf">#Add variables to DataFrame:<br/>df['FeNO'] = FeNO.tolist()<br/>df['FEV1'] = FEV1.tolist()<br/>df['BD'] = BD.tolist()<br/>df['dx'] = dx.tolist()</span></pre><p id="087b" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">要检查数据帧，只需键入:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="e30d" class="la lb it kw b gy lc ld l le lf">df</span></pre><p id="744e" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">您将看到一个名为df的数据帧，有300行和4列:</p><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/484d06eca09f7e4430aad62128023b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*qksrKVOIoLWiBVMu1hykCQ.png"/></div></figure><p id="b596" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 3。探索数据集:</em> </strong></p><p id="25d6" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">探索我们的数据集和变量之间的关系:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="499a" class="la lb it kw b gy lc ld l le lf">#Exploring dataset:</span><span id="196e" class="la lb it kw b gy lg ld l le lf"># left<br/>sns.pairplot(df, kind="scatter", hue="dx")<br/>plt.show()</span><span id="5c7a" class="la lb it kw b gy lg ld l le lf"># right: you can give other arguments with plot_kws.<br/>sns.pairplot(df, kind="scatter", hue="dx")<br/>plt.show()</span></pre><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/6e3f5a584928fa004158daa403b8e0d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*aAfMi2d2_2d8sRxo-sVPaA.png"/></div></figure><p id="bd84" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 4。设置变量:</em> </strong></p><p id="2c3e" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">在机器学习项目中，我们通常将包含特征的数据集称为<strong class="lv jd"> <em class="lh"> X </em> </strong>，将标签数组称为<strong class="lv jd"> <em class="lh"> y </em> </strong> <em class="lh"> </em>。创建我们的<strong class="lv jd"> <em class="lh"> X </em> </strong>和<strong class="lv jd"> <em class="lh"> y </em> </strong>变量:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="19b8" class="la lb it kw b gy lc ld l le lf">X = df.drop('dx', axis=1)<br/>y = df['dx']</span></pre><p id="5229" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">我们可以通过键入<strong class="lv jd"> <em class="lh"> X </em> </strong>来检查是否一切正常:</p><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/6b62982887d179ccb921ed479cc711ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*n1oKBz7osxdd7sFzgF4C2w.png"/></div></figure><p id="79ca" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">和<strong class="lv jd"> <em class="lh"> y </em> </strong>:</p><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/2a91fa344f2ceec2c65945f04b074ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*X618DXIG4D9sg1jA1HKXiA.png"/></div></figure><p id="c66c" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 5。将数据分成训练和测试:</em> </strong></p><p id="5eef" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">现在，我们将把数据分为训练和测试两部分。这很重要，因为我们将用一个训练子集来训练我们的模型，并用一个测试子集来测试模型的准确性。Scikit-Lear有一个我们将使用的<strong class="lv jd"> train_test_split </strong>函数。我们将80%的数据用于训练，20%用于测试(用test_size=0.20表示):</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="6f25" class="la lb it kw b gy lc ld l le lf">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)</span></pre><p id="3e38" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> 6。建立模型:</strong></p><p id="e354" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">现在是时候构建模型并将数据传递给模型了。我们从k=1开始。</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="64d9" class="la lb it kw b gy lc ld l le lf">clf = KNeighborsClassifier(n_neighbors=1)<br/>clf.fit(X_train, y_train)</span></pre><p id="09e0" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">我们可以从模型中看到一些预测:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="a81d" class="la lb it kw b gy lc ld l le lf">clf.predict(X_test)</span></pre><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/60ba3c4c47a9bb17418c8c54802ded30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*e2oTJ5ss6QeiDO5vw8-ZjA.png"/></div></figure><p id="b6b6" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">要评估模型:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="300f" class="la lb it kw b gy lc ld l le lf">clf.score(X_test, y_test)</span></pre><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8f02d37491ac1b8545f9edd728e9c201.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*jWsafXaeb8L0I_FcwEegvg.png"/></div></figure><p id="e4c8" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 7。寻找最佳k: </em> </strong></p><p id="7a3a" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">我们可以看到我们的模型精度为0.80。不错，但我们可以尝试不同的k数。为了寻找最佳的k数，我们尝试不同的随机k，或者我们可以建立一个图表，绘制不同的k值和模型精度。在这种情况下，我将尝试1到30之间的k值。</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="3192" class="la lb it kw b gy lc ld l le lf">training_accuracy = []<br/>test_accuracy = []</span><span id="d5ea" class="la lb it kw b gy lg ld l le lf"># try n_neighbors from 1 to 30.<br/>neighbors_settings = range(1, 31)</span><span id="d1ff" class="la lb it kw b gy lg ld l le lf">for n_neighbors in neighbors_settings:<br/>    # build the model<br/>    clf = KNeighborsClassifier(n_neighbors=n_neighbors)<br/>    clf.fit(X_train, y_train)<br/>    # record training set accuracy<br/>    training_accuracy.append(clf.score(X_train, y_train))<br/>    # record generalization accuracy<br/>    test_accuracy.append(clf.score(X_test, y_test))</span><span id="66cf" class="la lb it kw b gy lg ld l le lf">plt.plot(neighbors_settings, training_accuracy, label="training accuracy")<br/>plt.plot(neighbors_settings, test_accuracy, label="test accuracy")<br/>plt.legend()</span></pre><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi np"><img src="../Images/798d4e87311a3e75a3a88db9ef25936b.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*IxU-HP6tUVaUctk_yM7IIw.png"/></div></figure><p id="6d7c" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">通过图形检查，k=20似乎具有最好的测试精度。所以我们建立了k=20的模型:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="927e" class="la lb it kw b gy lc ld l le lf">clf = KNeighborsClassifier(n_neighbors=20)<br/>clf.fit(X_train, y_train)<br/>clf.predict(X_test)<br/>clf.score(X_test, y_test)</span></pre><p id="7d4d" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">现在我们的分数是0.82，略高于0.8。</p><p id="e47f" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd">T39】8。模型复杂度: </strong></p><p id="2c93" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">使用k=20而不是k=1增加了我们模型的准确性。KNN的另一个重要方面是，k值较高的模型复杂度较低。因此，通过使用更高的k，我们不仅提高了模型的准确性，而且降低了复杂性。</p><p id="cd11" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 9。绘制决策边界:</em> </strong></p><p id="a5f3" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">决策边界图最适用于只有两个要素的数据。我们的数据有三个特征，但我们仍然可以通过选择使用哪些特征来绘制决策边界。</p><p id="4142" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">第一步是将我们的数据集存储为CSV格式，并使用pandas再次导入它:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="0205" class="la lb it kw b gy lc ld l le lf">df.to_csv('data.csv', index = False)<br/>data = pd.read_csv('data.csv')</span></pre><p id="1a89" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">现在，我们将构建一个函数来创建另一个模型并绘制决策边界。有了这三个变量，我们可以构建三个不同的图。对于这个例子，我将FEV1和BD作为特征:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="7929" class="la lb it kw b gy lc ld l le lf">def knn_comparison(data,k):<br/>    x = data[['FEV1','BD',]].values<br/>    y = data['dx'].astype(int).values<br/>    clf = KNeighborsClassifier(n_neighbors=k)<br/>    clf.fit(x,y)<br/>    print(clf.score(x,y))</span><span id="4b59" class="la lb it kw b gy lg ld l le lf">    #Plot decision region:<br/>    plot_decision_regions(x,y,clf=clf, legend=2)</span><span id="1f3d" class="la lb it kw b gy lg ld l le lf">    #Adding axes annotations:<br/>    plt.xlabel('X_train')<br/>    plt.ylabel('y_train')<br/>    plt.title('KNN with k='+str(k))<br/>    plt.show()</span></pre><p id="f03d" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">要构建模型和情节，只需调用函数:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="a7f9" class="la lb it kw b gy lc ld l le lf">knn_comparison(data,20)</span></pre><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/5ab12980a7a268dd71ae9ca07d10cb82.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*g_bK0CzgC7xnD86LGBGwQQ.png"/></div></figure><h2 id="5388" class="la lb it bd mp mq mr dn ms mt mu dp mv mc mw mx my mg mz na nb mk nc nd ne iz bi translated">B —用于回归的KNN:</h2><p id="d0e3" class="pw-post-body-paragraph lt lu it lv b lw nf kd ly lz ng kg mb mc nh me mf mg ni mi mj mk nj mm mn mo im bi translated">KNN算法也可以用于回归。如果K=1，回归值将是数据集中最近点的值。如果k=n且n&gt;1，则回归值将是我们的值与数据集中n个点之间的欧氏距离的平均值。</p><p id="6575" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 1。导入库:</em> </strong></p><p id="c9cf" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">我们还需要一个:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="0c0b" class="la lb it kw b gy lc ld l le lf">from sklearn.neighbors import KNeighborsRegressor</span></pre><p id="064b" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">由于我们将使用相同的数据集，我们可以直接跳到第6步。</p><p id="d947" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 6。</em>建筑模型:</strong></p><p id="64f2" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">为了构建模型，我们将遵循与分类中非常相似的步骤，从k=1开始:</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="68ea" class="la lb it kw b gy lc ld l le lf">reg = KNeighborsRegressor(n_neighbors=1)<br/>reg.fit(X_train, y_train)<br/>reg.predict(X_test)<br/>reg.score(X_test, y_test)</span></pre><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nr"><img src="../Images/4e88cf295b56b4e9fd491589ad2ba490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ccmAq3bhEU7JJlIcwxYLQ.png"/></div></div></figure><p id="a3c6" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">我们模特的分数是0.40左右，很差。我们将努力找到最佳的k值，并改进我们的模型预测。</p><p id="2443" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd">第七期<em class="lh">。寻找最佳k值:</em> </strong></p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="ace6" class="la lb it kw b gy lc ld l le lf">training_accuracy = []<br/>test_accuracy = []</span><span id="6371" class="la lb it kw b gy lg ld l le lf"># try n_neighbors from 1 to 30.<br/>neighbors_settings = range(1, 31)</span><span id="1b26" class="la lb it kw b gy lg ld l le lf">for n_neighbors in neighbors_settings:<br/>    # build the model<br/>    reg = KNeighborsRegressor(n_neighbors=n_neighbors)<br/>    reg.fit(X_train, y_train)<br/>    # record training set accuracy<br/>    training_accuracy.append(reg.score(X_train, y_train))  <br/>    # record generalization accuracy<br/>    test_accuracy.append(reg.score(X_test, y_test))</span><span id="24fd" class="la lb it kw b gy lg ld l le lf">plt.plot(neighbors_settings, training_accuracy, label="training accuracy")<br/>plt.plot(neighbors_settings, test_accuracy, label="test accuracy")<br/>plt.legend()</span></pre><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/518a9178f39ab3cf4507419a3425cf8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*B_7R-k60gH3D7EMSx3TJ8A.png"/></div></figure><p id="4b78" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">分析图表，k=22似乎是一个最佳值。</p><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="fc39" class="la lb it kw b gy lc ld l le lf">reg = KNeighborsRegressor(n_neighbors=22)<br/>reg.fit(X_train, y_train)<br/>reg.predict(X_test)<br/>reg.score(X_test, y_test)</span></pre><figure class="kr ks kt ku gt lq gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f0f11c48a8524876fd92cbda9ad283d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*1SwgMoVPhywn487BOiRc4A.png"/></div></figure><p id="fe50" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">正如所料，增加k值，增加了模型得分，现在大约为0.63，比0.39好得多。</p><p id="e78b" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh"> 8。模型复杂度:</em> </strong></p><p id="809b" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">与分类类似，增加回归模型中的k数会降低模型的复杂性。考虑更多的邻居，至少是令人窒息的预测。</p><h2 id="b5da" class="la lb it bd mp mq mr dn ms mt mu dp mv mc mw mx my mg mz na nb mk nc nd ne iz bi translated">KNN的优势和劣势</h2><p id="ae2c" class="pw-post-body-paragraph lt lu it lv b lw nf kd ly lz ng kg mb mc nh me mf mg ni mi mj mk nj mm mn mo im bi translated">和所有其他算法一样，KNN算法有优点也有缺点。其中一些是:</p><p id="0b88" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh">优点:</em></strong><br/>——简单的ML模型；<br/>-易于理解并向非技术人员解释；<br/>-易于工程化(仅针对k进行调整)；<br/>-在尝试更复杂的模型之前最好考虑一下。</p><p id="22a0" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd"> <em class="lh">缺点:</em></strong><br/>——对于非常大的数据集执行速度较慢；无法处理和表现许多特征。</p><h2 id="9fec" class="la lb it bd mp mq mr dn ms mt mu dp mv mc mw mx my mg mz na nb mk nc nd ne iz bi translated">结论</h2><p id="76c3" class="pw-post-body-paragraph lt lu it lv b lw nf kd ly lz ng kg mb mc nh me mf mg ni mi mj mk nj mm mn mo im bi translated">在机器学习项目中，KNN是一种简单易行的算法，尤其是当我们有一小组特征要分析，并且我们的数据被分成固定数量的类别时。在本文中，您将找到完成项目所需的全部内容，包括构建、评估和设计模型的python代码，以及在需要时构建图形可视化。</p><p id="78b3" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">感谢您的阅读！</p><p id="3bb1" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">如果你喜欢这篇文章，别忘了关注我，这样你就能收到关于新出版物的所有更新。</p><p id="3a8d" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd">其他如果:</strong>想了解更多，可以通过<a class="ae ny" href="https://cdanielaam.medium.com/membership" rel="noopener">我的推荐链接</a>订阅Medium会员。它不会花你更多的钱，但会支付我一杯咖啡。</p><p id="661b" class="pw-post-body-paragraph lt lu it lv b lw lx kd ly lz ma kg mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><strong class="lv jd">其他:</strong>感谢阅读！</p></div></div>    
</body>
</html>