<html>
<head>
<title>All About Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于随机森林的一切</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/all-about-random-forest-84daf291eb04?source=collection_archive---------1-----------------------#2022-04-24">https://pub.towardsai.net/all-about-random-forest-84daf291eb04?source=collection_archive---------1-----------------------#2022-04-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/b75e8fa59e5a6b99529284e5f6b02d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J2Jql6cmA30xR_Zg.jpg"/></div></div></figure><p id="c88c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本文中，我们将通过回答以下问题来理解随机森林:</p><ol class=""><li id="e9b9" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">什么是随机森林？</li><li id="1fb7" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">为什么我们使用随机森林？</li><li id="302f" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">随机森林是如何工作的？</li><li id="e0eb" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">随机森林的优缺点是什么？</li><li id="9fdb" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">如何使用python实现随机森林？</li></ol><h1 id="db79" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><strong class="ak">什么是随机森林？</strong></h1><p id="d9a2" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">随机森林是一种有监督的机器学习算法。这种算法非常流行，因为它既可以处理分类问题，也可以处理回归问题。该算法的主要思想是训练多个决策树，收集它们的预测，并根据预测的最终输出，在分类的情况下使用多数计数或在回归的情况下取平均值。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/4ce27bfa251f1ceaf7418fce90f9a1b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kKGYp-tlAQrvo2va.png"/></div></div></figure><p id="ee88" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">随机森林可能看起来非常类似于Bagging Ensemble技术，但是在随机森林中存在两个主要区别。第一个区别是，与bagging技术不同，random forest只使用一个决策树作为它的基本算法。第二个区别是随机森林在将数据输入基本模型之前增加了数据的随机性。</p><h1 id="e8c4" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><strong class="ak">为什么我们使用随机森林？</strong></h1><p id="68ee" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">要回答上述问题，首先，我们必须了解决策树的缺点。正如我们所知，决策树本身是一种非常强大的监督机器学习算法。但是它有在训练数据集上过度拟合的趋势，这导致对新数据点的不良预测。简而言之，一个完全成长的决策树通常提供一个低偏差和高方差的模型。这里随机森林进入画面。随机森林通过同时训练多个决策树，将低偏差、高方差模型转换为低偏差、低方差模型。随机森林中的每个决策树获得训练数据集的子集，并相应地预测结果。之后，随机森林收集这些结果，并执行不同的操作，最终得出最终预测。</p><blockquote class="ms mt mu"><p id="d45a" class="jy jz mv ka b kb kc kd ke kf kg kh ki mw kk kl km mx ko kp kq my ks kt ku kv ij bi translated">要了解决策树，请单击下面的链接</p></blockquote><div class="mz na gp gr nb nc"><a rel="noopener  ugc nofollow" target="_blank" href="/all-about-decision-tree-c252e0612812"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd ir gy z fp nh fr fs ni fu fw ip bi translated">关于决策树的一切</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">在本文中，我们将通过回答以下问题来理解决策树:</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">pub.towardsai.net</p></div></div><div class="nl l"><div class="nm l nn no np nl nq jw nc"/></div></div></a></div><p id="5caa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是这个过程也可以通过bagging系综技术来完成。所以，问题又来了，为什么是随机森林？</p><p id="4545" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用随机森林而不是bagging的主要原因是随机森林给训练数据增加了更多的随机性。在随机森林中，每当决策树中的节点分裂时，都会对训练数据进行采样，但在bagging中，采样发生在树形成的开始。</p><h1 id="e3a9" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><strong class="ak">随机森林是如何工作的？</strong></h1><p id="5138" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">随机森林的工作可以分为三个主要步骤。第一步是对随机森林中每个决策树的数据集进行采样。取样大致有三种方式，即:</p><ul class=""><li id="8e56" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv nr lc ld le bi translated">拔靴带</li><li id="6021" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv nr lc ld le bi translated">涂</li><li id="9f24" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv nr lc ld le bi translated">随机子空间</li></ul><p id="ca43" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> Bootstrapping: </strong>在这种情况下，通过选择随机的行/元组来创建训练数据集的子集，替换意味着数据集的行可以重复。</p><p id="9a34" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">粘贴:</strong>在这种情况下，通过挑选随机的行/元组来创建训练数据集的子集，而无需替换，这意味着数据集的行不能重复。</p><p id="e47f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">随机子空间:</strong>在这里，通过挑选随机特征/列来创建训练数据集的子集。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d82088adb21dd0ad36e4ec092f227b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*BnyjzqlewPm-8RRK0BUpLg.png"/></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk translated">并行训练决策树</figcaption></figure><p id="e01a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">采样之后，第二步是并行训练各个决策树模型，而不相互依赖。现在，随机森林已经训练好了，我们可以输入数据了。每棵树都将根据提供的样本数据得出自己的预测。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/04dfb55c204ec859c820ffd37c2e78f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ni6XiPnx1BQu23HYfqtp_g.png"/></div></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk translated">随机森林分类</figcaption></figure><p id="4685" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，第三步也是最后一步是聚合每个决策树模型的预测，并根据问题陈述执行数学运算。对于回归问题陈述，我们通常取预测的平均值，给出最终输出。对于分类问题语句，我们通常采用多数计数并给出最终输出。</p><blockquote class="ms mt mu"><p id="f8e9" class="jy jz mv ka b kb kc kd ke kf kg kh ki mw kk kl km mx ko kp kq my ks kt ku kv ij bi translated">要了解合奏技巧，请点击下面的链接</p></blockquote><div class="mz na gp gr nb nc"><a rel="noopener  ugc nofollow" target="_blank" href="/all-about-ensemble-techniques-821a8957fab2"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd ir gy z fp nh fr fs ni fu fw ip bi translated">关于合奏技巧的一切</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">在本文中，我们将通过回答以下问题来尝试理解机器学习环境中的集成思想</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">pub.towardsai.net</p></div></div><div class="nl l"><div class="ny l nn no np nl nq jw nc"/></div></div></a></div><h1 id="a83c" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><strong class="ak">随机森林有什么优缺点？</strong></h1><h2 id="8115" class="nz ll iq bd lm oa ob dn lq oc od dp lu kj oe of ly kn og oh mc kr oi oj mg ok bi translated">优势:</h2><ul class=""><li id="1222" class="kw kx iq ka b kb mi kf mj kj ol kn om kr on kv nr lc ld le bi translated">使用随机森林的主要优点是它减少了模型的方差，而不影响偏差。简而言之，它将低偏差高方差模型转换为低偏差低方差模型。</li><li id="240a" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv nr lc ld le bi translated">随机森林可用于分类和回归问题。</li><li id="b25b" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv nr lc ld le bi translated">与单一决策树相比，随机森林更加稳定。</li><li id="2b91" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv nr lc ld le bi translated">随机森林可以处理缺失值，并且对异常值具有鲁棒性。</li></ul><h2 id="92e0" class="nz ll iq bd lm oa ob dn lq oc od dp lu kj oe of ly kn og oh mc kr oi oj mg ok bi translated">缺点:</h2><ul class=""><li id="6f9b" class="kw kx iq ka b kb mi kf mj kj ol kn om kr on kv nr lc ld le bi translated">与决策树相比，随机森林需要更多的计算能力，因为随机森林中有100到500棵树在并行训练。</li><li id="77c9" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv nr lc ld le bi translated">我们可以可视化单个决策树，但是我们不能监控随机的森林，因为树的数量太多了。</li></ul><h1 id="6a40" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">如何使用python实现随机森林？</h1><p id="8c45" class="pw-post-body-paragraph jy jz iq ka b kb mi kd ke kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv ij bi translated">用python实现随机森林非常简单，我们只需导入scikit learn模块。所以让我们来看看如何实现它。</p><p id="3f47" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们将从python导入重要的库。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/5dd188fbac28cd1e67efc80bd6c076c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*JNJsavjfpUQKuKtS2KNurw.png"/></div></figure><p id="fc6c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，我们将创建一个虚拟数据集，看看算法是如何工作的。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ab4b71db58bae340447f76385516d5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*ZY-xmny8LYecyHIdxq8X3A.png"/></div></figure><p id="6a66" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们使用seaborn库来可视化数据集。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/106a01127d10ca1da8781cbe44b4beb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*bTBUufNB5kZVkudOLE79lw.png"/></div></figure><p id="e276" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以观察到有两类“橙色”和“蓝色”。我们必须对此进行分类。现在下一步是将数据分成训练集和测试集。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi or"><img src="../Images/bb7e3324d7c50e5aa8ed16b1c93f730a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*T5xu8oUlA4kfvH99cc2Zqg.png"/></div></figure><p id="0341" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，我们将尝试使用决策树进行分类，以比较随机森林如何比决策树更好地工作。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi os"><img src="../Images/c28d57601231cb5fe184a18ad6f7db7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*OU0SApjyxwUMi7wB3_QYWg.png"/></div></figure><p id="f6cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以观察到决策树过度拟合数据，这导致了高方差并降低了准确性。我们来计算一下决策树的准确率。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/1d1317230527b10b090f4264c344385a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*-uEOP-cqqdmiC8jgt2X2fg.png"/></div></figure><p id="7526" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">准确率80%。现在，我们将训练随机森林算法。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/86bb884a16c0a19f131ec04b4f745854.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*T70bJfIrx_MzBuzeXDh2rg.png"/></div></figure><p id="9601" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们比较两种算法的决策边界，我们将观察到随机森林的决策边界具有平滑的轮廓，并且它减少了过拟合问题。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/25a1443ec1310441c92c5d99fe1ece92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*KZZ1f2lvpLNCpXv9kvyLDQ.png"/></div></figure><p id="b507" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，准确率提高了4%,总共提高了84%。</p><p id="941f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您想在编码部分进行更多探索，或者想直观地了解随机森林算法如何适应数据集。然后，请点击下面的Github资源库链接。</p><div class="mz na gp gr nb nc"><a href="https://github.com/Akashdawari/Articles_Blogs_Content/blob/main/All%20About%20Random%20Forest.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd ir gy z fp nh fr fs ni fu fw ip bi translated">Articles _ Blogs _ Content/All About Random forest . ipynb at main Akashdawari/Articles _ Blogs _ Content</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">这个知识库包含了jupyter关于发表在博客上的文章的笔记本。-文章_博客_内容/全部…</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">github.com</p></div></div><div class="nl l"><div class="ow l nn no np nl nq jw nc"/></div></div></a></div><p id="7111" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">喜欢并分享如果你觉得这篇文章有帮助。还有，关注我的medium，了解更多机器学习和深度学习相关的内容。</p></div></div>    
</body>
</html>