<html>
<head>
<title>Introduction to Deep Learning with TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow深度学习简介</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/introduction-to-deep-learning-with-tensorflow-f7d2c53f93c8?source=collection_archive---------2-----------------------#2020-08-26">https://pub.towardsai.net/introduction-to-deep-learning-with-tensorflow-f7d2c53f93c8?source=collection_archive---------2-----------------------#2020-08-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9bf6" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>、<a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="34f1" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">温和地介绍和实施前三种类型的人工神经网络；使用张量流的前馈神经网络、RNNs和CNN</h2></div><p id="d9c5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">随着当前数据分析和数据科学的发展，许多组织开始实施基于人工神经网络(ANN)的解决方案来自动预测各种业务问题的结果。</p><p id="e567" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有各种类型的人工神经网络，例如；前馈神经网络(人工神经元)、递归神经网络(RNN)、卷积神经网络(CNN)等。在本文中，我们将讨论所有这三种类型的人工神经网络。对于这里显示的所有示例，我都使用了2.3.0版的<strong class="kq ja"> <em class="lk"> TensorFlow </em> </strong>。某些命令和方法可能在以前的版本中不可用。</p><h1 id="a162" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated">前馈神经网络</h1><p id="5d6e" class="pw-post-body-paragraph ko kp iq kq b kr md ka kt ku me kd kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">这是最简单的人工神经网络形式之一，其中输入仅在一个方向上传播。数据通过输入层，在输出层输出。它可能有隐藏层，也可能没有。顾名思义，它只有前向传播，没有后向传播。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mi"><img src="../Images/1236b7919a1c5907add6ee50656b4921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fmWBy3ktIHiKih9BcTvSYg.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">具有一个隐藏层的前馈神经网络的简单视图</figcaption></figure><p id="6954" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Tensorflow的实现可以通过使用<code class="fe my mz na nb b">dense</code>层的sequential从下面的代码片段中完成。</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="7d37" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以通过调用<code class="fe my mz na nb b">model.get_weights()</code>得到神经元的权重和偏差。第一个是针对该层中16个神经元的5个特征输入的权重，第二个数组表示每16个神经元的偏差。第三和第四个数组表示来自前一层(16个神经元)的输出的权重和输出神经元的偏差。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/bb11db20476d10afa5ebff885628b223.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*UWKW_oO_bc727hN8dW3nbQ.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">model.get_weights()的结果</figcaption></figure><p id="eecb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，通过可视化训练日志和预测误差，我们可以了解关于模型学习过程(历史)和模型性能的更多信息。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nf"><img src="../Images/954956896adea09b151748e58b300fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJjLCrQCXeB3gTkJV3VN8Q.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">模型性能可视化</figcaption></figure><h1 id="2019" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated">RNN递归神经网络</h1><p id="3c96" class="pw-post-body-paragraph ko kp iq kq b kr md ka kt ku me kd kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">递归神经网络不同于传统的前馈神经网络。递归神经网络旨在更好地处理顺序信息。它们引入状态变量来存储过去的信息，结合当前的输入来确定当前的输出。RNN的一些常见示例基于文本数据和时间序列数据。</p><p id="c00b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们想象隐藏状态的RNN。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ng"><img src="../Images/3a41f1b1c2ac49b7f6973f28fbced9f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6KrB_NE7KEP3X8M9wM7PXA.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">隐藏状态的RNN</figcaption></figure><p id="b456" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以看到，当前时间步的隐变量的计算是由当前时间步的输入和前一个时间步的隐变量共同决定的。这些激活被存储在网络的内部状态中，该网络可以保存长期的时间上下文信息。该上下文信息用于将输入序列转换成输出序列。因此，<strong class="kq ja">上下文是递归神经网络的关键</strong>。</p><p id="1ce2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，RNN不足以解决今天的序列学习问题，例如；梯度计算过程中的数值不稳定性。</p><p id="0fc1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如上所述，网络具有可以表示上下文信息的内部状态(存储)。存储器也可以由另一个网络或图形来代替。这种受控状态是指门控状态或门控记忆，是<em class="lk">长短期记忆</em>网络(<em class="lk">lstm</em>)和<em class="lk">门控循环单元(GRUs) </em>的一部分。该网络也可称为<em class="lk">反馈神经网络</em>。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nh"><img src="../Images/5384397d484fe40001b7f446cca50e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XaDxNtZZHnsFoUG0x3f9HA.jpeg"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">GRU(左)和LSTM(右)</figcaption></figure><p id="e607" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">rnn可以是单向和双向的。如果我们相信数据周围有一个环境，不仅仅是数据出现在它之前。我们应该使用双向rnn，例如，上下文出现在单词前后的文本数据。关于RNN的更多解释，我建议观看视频<a class="ae ni" href="https://www.youtube.com/watch?v=UNmqTiOnRfg" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="039f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">GRUs和LSTMs都消耗[批量大小、时间步长、特征]的输入形状。</p><ul class=""><li id="f469" class="nj nk iq kq b kr ks ku kv kx nl lb nm lf nn lj no np nq nr bi translated"><strong class="kq ja"> <em class="lk">批量</em> </strong>是神经网络在调整权重之前要查看的样本数。小批量会降低训练速度，但如果批量太大，模型将无法进行归纳，并消耗更多内存(以保持每批的样本数量)</li><li id="cfb0" class="nj nk iq kq b kr ns ku nt kx nu lb nv lf nw lj no np nq nr bi translated"><strong class="kq ja"> <em class="lk">时间步长</em> </strong>是神经网络查看的时间回溯单位(即时间步长= 30可以表示我们回溯并输入30天的数据作为一次观察的输入</li><li id="4b89" class="nj nk iq kq b kr ns ku nt kx nu lb nv lf nw lj no np nq nr bi translated"><strong class="kq ja"> <em class="lk">特征</em> </strong>是每个时间步中使用的属性数量。例如，如果我们仅使用60天时间步长的股票收盘价，特征大小将仅为1</li></ul><p id="575b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下面是同时实现GRUs和LSTMs来预测Google股价的例子。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nx"><img src="../Images/149448e69963be9d9b3aba863824eae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IB9HuEGRfOlj5bjjjJN7DQ.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">2006年至2018年谷歌股价</figcaption></figure><p id="7172" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通常，当我们使用递归神经网络来预测数值时，我们需要对输入值进行标准化。</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">预测股票价格的GRU模型的代码片段</figcaption></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ny"><img src="../Images/b7f4c008f5e513afbffaadfed481be4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*seLp8EymqIyYboQwmJWDMg.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">每个时期的性能(左)和实际与预测之间的可视化(右)</figcaption></figure><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">预测股票价格的LSTM模型的代码片段</figcaption></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nz"><img src="../Images/0fe2ec8cbc9a792d469d94fb9ebf302d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*07jtQoX-cHuwgIafg863jw.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">每个时期的性能(左)和实际与预测之间的可视化(右)</figcaption></figure><p id="bb3f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正如我们所看到的，LSTM可以察觉到这一趋势，但仍有很大的改进空间。然而，在这篇文章中，我将只关注用法。</p><p id="0e65" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总之，gru是:</p><ul class=""><li id="e9f6" class="nj nk iq kq b kr ks ku kv kx nl lb nm lf nn lj no np nq nr bi translated">更擅长捕捉具有大时间步长距离的时间序列的相关性</li><li id="3558" class="nj nk iq kq b kr ns ku nt kx nu lb nv lf nw lj no np nq nr bi translated">重置门有助于捕捉短期相关性，而更新门有助于捕捉时间序列数据中的长期相关性</li></ul><p id="cc11" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">LSTMs有；</p><ul class=""><li id="6313" class="nj nk iq kq b kr ks ku kv kx nl lb nm lf nn lj no np nq nr bi translated">控制信息流的三种门(输入门、遗忘门和输出门)</li><li id="345b" class="nj nk iq kq b kr ns ku nt kx nu lb nv lf nw lj no np nq nr bi translated">LSTM的隐藏层输出包括隐藏状态和存储单元。隐藏状态被传递到输出层，但是存储单元完全是内部的</li><li id="67bd" class="nj nk iq kq b kr ns ku nt kx nu lb nv lf nw lj no np nq nr bi translated">它可以处理消失和爆炸梯度</li></ul><h1 id="0031" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated">卷积神经网络</h1><p id="2401" class="pw-post-body-paragraph ko kp iq kq b kr md ka kt ku me kd kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">CNN像神经网络一样，由具有可学习的权重和偏差的神经元组成。然而，它们是为计算机视觉设计的强大的神经网络家族。细胞神经网络的应用涉及图像识别、目标检测、语义分割等。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oa"><img src="../Images/ffcfb21b57a48669090bb05b7cecbe48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OMpW2v2hvGt4sMQaLFn1mA.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">神经网络(左)和卷积神经网络(右)之间的比较</figcaption></figure><p id="4970" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">CNN往往计算效率高，因为它们比全连接架构需要更少的参数，也因为它们易于跨GPU并行化。有了这些，CNN被用于一维序列结构(即音频、文本和时间序列数据)。此外，一些人在图形结构数据和推荐系统中采用了CNN。我建议阅读卷积的每个功能的细节，更详细的池层<a class="ae ni" href="https://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ob"><img src="../Images/bb8afd584f5fcf4dc5ef66f9720f161e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLMjOkO5Q1uS0pIFMbSUaw.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">LeNet-5架构(参考:<a class="ae ni" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank">http://vision . Stanford . edu/cs 598 _ spring 07/papers/le Cun 98 . pdf</a>)</figcaption></figure><p id="2057" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通常对于计算机视觉任务，CNN接受形状的输入(图像高度、图像宽度、颜色通道)，忽略批量大小。颜色通道或深度指的是(R，G，B)配色方案。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/de0ac4b2d630d104f6f29c9553dd4c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*qtPsQ_fzZTlqIM2gNkzu3Q.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">张量输入形状</figcaption></figure><p id="8a47" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下面是我们如何实现一个基本的CNN模型来对图像进行分类。</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi od"><img src="../Images/855a51aba740593dfccca96492fade4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQ8Q5QOvFoZLKFd-jkOmHg.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">CNN模型在每个历元的性能</figcaption></figure><p id="2325" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以看到，该模型变得与训练数据过度拟合，并且在验证数据上的性能变得最差。训练数据的模型性能为93.1%，而测试数据为76.4%。</p><p id="e946" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，我们可以通过添加一个<strong class="kq ja">数据扩充</strong>过程来扩展训练数据集，从而帮助模型更好地泛化，从而改善模型结果。</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nc nd l"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">前几行显示我们实现了数据扩充，并直接从文件夹中输入图像</figcaption></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi oe"><img src="../Images/b29f53ea7f5414579c2e2d6ef07f5b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3J4L1Aq-vF3uZPHxn6SkGw.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">相同CNN结构的性能，但增加了数据</figcaption></figure><p id="35aa" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用这种简单的技术，模型对数据更加一般化。对训练数据的准确度降低到85.1%，但是测试准确度现在是81.7%。</p><p id="ebfb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">综上所述；</p><ul class=""><li id="e4d2" class="nj nk iq kq b kr ks ku kv kx nl lb nm lf nn lj no np nq nr bi translated">卷积层从输入数据中提取特征</li><li id="69a3" class="nj nk iq kq b kr ns ku nt kx nu lb nv lf nw lj no np nq nr bi translated">卷积层的排列使得它们逐渐降低表示的空间分辨率，同时增加通道的数量</li><li id="0dc0" class="nj nk iq kq b kr ns ku nt kx nu lb nv lf nw lj no np nq nr bi translated">它通常由卷积、非线性和通常的池操作组成</li><li id="5631" class="nj nk iq kq b kr ns ku nt kx nu lb nv lf nw lj no np nq nr bi translated">在网络的末端，它在计算输出之前连接到一个或多个完全连接的层</li></ul><h1 id="8377" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated">尾注</h1><p id="4ea6" class="pw-post-body-paragraph ko kp iq kq b kr md ka kt ku me kd kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">我介绍了最常用的网络类型及其使用python和TensorFlow的实现。关于本文中使用的完整代码，请参考GitHub资源库或单击此处阅读笔记本。</p><p id="96d2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一旦你完全理解了每一种网络类型，你就可以把它们结合起来解决更复杂的问题，例如；并行LSTM解决实时预测中的流数据，卷积LSTM解决视频问题(想象它是一个图像序列)。代码和数据请参考我下面的GitHub库。</p><h1 id="9d93" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated">附加阅读和Github知识库</h1><div class="of og gp gr oh oi"><a href="https://cs231n.github.io/convolutional-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd ja gy z fp on fr fs oo fu fw iz bi translated">用于视觉识别的CS231n卷积神经网络</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">目录:卷积神经网络非常类似于以前的普通神经网络…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">cs231n.github.io</p></div></div><div class="or l"><div class="os l ot ou ov or ow ms oi"/></div></div></a></div><div class="of og gp gr oh oi"><a href="https://d2l.ai/chapter_recurrent-neural-networks/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd ja gy z fp on fr fs oo fu fw iz bi translated">8.递归神经网络-深入学习0.14.3文档</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">到目前为止，我们遇到了两种类型的数据:一般矢量和图像。对于后者，我们设计了专门的层来…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">d2l.ai</p></div></div></div></a></div><div class="of og gp gr oh oi"><a href="https://github.com/netsatsawat/Introduction_to_ANN_tensorflow" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd ja gy z fp on fr fs oo fu fw iz bi translated">netsatsawat/Introduction _ to _ ANN _ tensor flow</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">有许多类型的人工神经网络，但最流行的类型，我们已经使用和实施，是前馈神经…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">github.com</p></div></div><div class="or l"><div class="ox l ot ou ov or ow ms oi"/></div></div></a></div></div></div>    
</body>
</html>