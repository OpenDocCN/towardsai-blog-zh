<html>
<head>
<title>Cracking Open Bitcoin with Artificial Intelligence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用人工智能破解开放比特币</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/cracking-open-bitcoin-with-artificial-intelligence-4a196cdc604?source=collection_archive---------0-----------------------#2021-03-02">https://pub.towardsai.net/cracking-open-bitcoin-with-artificial-intelligence-4a196cdc604?source=collection_archive---------0-----------------------#2021-03-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="98a6" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/cryptography" rel="noopener ugc nofollow" target="_blank">密码术</a></h2><div class=""/><div class=""><h2 id="9c5a" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">可视化SHA256</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/5030e67fe0dfe0042c71e3d5f46ba33b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yheHvtFVgxdOwV_Xfs_Xrw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:<a class="ae le" href="https://commons.wikimedia.org/wiki/File:Bitcoinas.svg" rel="noopener ugc nofollow" target="_blank">维基共享</a>，根据<a class="ae le" href="https://en.wikipedia.org/wiki/en:Creative_Commons" rel="noopener ugc nofollow" target="_blank">知识共享</a> <a class="ae le" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" rel="noopener ugc nofollow" target="_blank">署名-分享4.0国际</a>许可授权。</figcaption></figure><p id="dce5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在比特币挖矿中，区块、私钥、公钥都可以找到一些和某处提到的SHA256的联系。这使得研究SHA256很有趣。在本文中，我们将重点关注SHA256。我们将深入SHA256的代码，同时研究加密哈希函数的语义。我们还将把SHA256分解成它的基本组件，并做一些有趣的机器学习。本文的目的是对SHA256有一个大致的了解，sha 256是比特币和整个互联网中一个重要的加密哈希函数。</p><p id="3e4c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">希望到本文结束时，读者会发现一些与SHA256相关的有趣研究。这篇文章也将为那些对机器学习感兴趣的人提供一个方向。这篇文章没有强调破解比特币，而是强调打开比特币看看里面有什么。SHA256是比特币内部发生的事情的重要组成部分。SHA256在本文中也被称为“哈希”。让我们先看看单词“hello”的散列，然后再看看单词“Hello”。我们可以通过简单地导入SHA256，然后编写一个小函数来做到这一点</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="a23d" class="mg mh iq mc b gy mi mj l mk ml"><strong class="mc ja">from</strong> hashlib <strong class="mc ja">import</strong> sha256<br/>def hash(text):<br/>  word= (text)<br/>  hex=(sha256(word.encode('utf-8')).hexdigest())<br/>  return(hex)</span></pre><p id="39fe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以看到散列函数接受一个字符串值，并输出一个十六进制值。仔细检查下面的两个散列结果，我们可以看到该函数是区分大小写的。这意味着，即使字母的大小写稍有变化，加密哈希函数也会有不同的响应。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="c04d" class="mg mh iq mc b gy mi mj l mk ml">&gt;&gt;&gt; <strong class="mc ja">print</strong>(hash("Hello"))<br/>185f8db32271fe25f561a6fc938b2e264306ec304eda518007d1764826381969</span><span id="8904" class="mg mh iq mc b gy mm mj l mk ml">&gt;&gt;&gt; <strong class="mc ja">print</strong>(hash("hello"))<br/>2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824</span></pre><p id="37b8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里出现了一个有趣的问题，密码哈希函数和单词的含义有什么关系吗？我们从上面的散列中看到，散列的形态学并不与原始单词保持关系。所以，单词的散列值会随着单词的细微变化而变化，但是意思相似的单词在它们对应的散列值之间有什么模式吗？为了分析这一点，我们将使用一些向量，这些向量是根据twitter数据计算出来的。这些向量来自研究人员称之为“手套”的算法。手套向量有几个维度；我们将简单地下载并可视化一些十六进制散列(链接<a class="ae le" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mn"><img src="../Images/42df3e4b65f39f88b7259253a57bdbea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GdBj_WtIANyXp_B-36CKHg.gif"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图片由作者提供(完整的可视化可在<a class="ae le" href="https://projector.tensorflow.org/?config=https%3A%2F%2Fgist.githubusercontent.com%2Fseanjudelyons%2F63e6bdb8174599e295d39bd92e94116d%2Fraw%2F09a3a8ba63f061124367a7cbec9e2c04e30ed1e1%2Fgistfile1.txt&amp;fbclid=IwAR2RB6p2gUZ_a8PUtuz5XIl3QiT65l50tsULgD7I_hRwSWY8ZR0bJ60TYJU" rel="noopener ugc nofollow" target="_blank">这里</a>获得)</figcaption></figure><p id="361d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过分析一些语义相似的词，如“狗”和“猫”或“澳大利亚”和“加拿大”，我们可以看到SHA256的词法与原始字符串没有语义相似性。这意味着意思相似的单词有完全不同的哈希值，这种怀疑现在得到了这种观想的经验支持。好了，让我们深入一点，看看SHA256加密哈希是如何创建的。</p><p id="f751" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一位理论物理学家用python从头开始编写了加密哈希函数的代码。这个git要点的链接可以在<a class="ae le" href="https://gist.github.com/Nikolaj-K/388e643d1f5e6989072a21e469d10a48" rel="noopener ugc nofollow" target="_blank">这里</a>找到。代码是在知识共享许可下发布的，所以让我们写一些评论并想象一下这个算法。我们可以按照S-BERT的类似技术来可视化算法。关于算法可视化器的文章可以在<a class="ae le" href="https://medium.datadriveninvestor.com/visualise-algorithm-bd5c6d1634d9" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mn"><img src="../Images/111d9e4451c7c625fad8df9190bd13da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7NEVHM02gsVga3TNEt9tHg.gif"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">作者提供的图片(可在此<a class="ae le" href="https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/seanjudelyons/f9bd72959cedfd900819517347868f15/raw/04ab168d5503d5e5ae9d09ec076777195c7ae019/gistfile1.txt" rel="noopener ugc nofollow" target="_blank">获得完整的可视化效果</a>)</figcaption></figure><p id="b359" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在检查算法时，我们可以看到一些重要的函数，它们将十六进制转换为比特，反之亦然(见下图)。这一点很重要，因为算法使用布尔逻辑来运行它的过程。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mo"><img src="../Images/19158ebcf5f5fbeb056825354a95d00c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GfkM3Wo8fH3AvzS5lpPAeg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</figcaption></figure><p id="dede" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们稍微跑题一下，讨论一下布尔逻辑。也许对读者最友好的布尔逻辑和二进制位的阐述是莱布尼茨写的</p><blockquote class="mp mq mr"><p id="55f3" class="lf lg ms lh b li lj ka lk ll lm kd ln mt lp lq lr mu lt lu lv mv lx ly lz ma ij bi translated">“但我多年来一直使用最简单的级数，即两个一组的级数，而不是十个一组的级数，因为我发现这有助于完善数字科学。因此，除了0和1，我没有使用其他字符，当达到2时，我重新开始”(链接<a class="ae le" href="http://www.leibniz-translations.com/binary.htm" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p></blockquote><p id="ea37" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">莱布尼茨发现所有的字符都可以用0和1来表示。这实质上是二进制背后的思想，其中“bini”是拉丁语中的“二”。接下来，我们可以用“0”和“1”来表示单词中的每个字符。这样做的过程将包括找到ASCII值并除以2，如下图所示。余数从下到上以字母“a”的二进制表示形式读取(ASSCI表示“a”= 97)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mw"><img src="../Images/317e5ddc0783786d0e93a2926f646e1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BboAiE8SYeMlKzRgXQexQQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</figcaption></figure><p id="95df" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">ASCII值可以简单地认为是数字标准。ASCII代表美国信息交换标准代码。' a '的ASCII值是97，' b '是98，' c '是99，' d '是100，依此类推。为了将ASCII值转换成二进制，我们遵循如上所示的方法，其中有两个基数(0和1)。在除以2形成数字的二进制数之后，余数从底部向上读取。接下来我们可以写一个小函数，将单词转换成二进制并返回它-</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="07ab" class="mg mh iq mc b gy mi mj l mk ml">def ascitobin(n):<br/>    l=[]<br/>    for i in n:        <br/>        if i == 'a':<br/>            l.append('01100001')<br/>        elif i == 'b':<br/>            l.append('01100010')<br/>        elif i == 'c':<br/>            l.append('01100011')<br/>        elif i == 'd':<br/>            l.append('01100100')<br/>#...and so on</span></pre><p id="db1b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">按照类似的计算，我们可以编写另一个函数，将SHA256中的十六进制值转换为二进制数字。随着文章的深入，这样做的原因会变得更加清楚。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="ccf0" class="mg mh iq mc b gy mi mj l mk ml">def HexToBin(hexdec):<br/>    l=[]<br/>    for i in hexdec:        <br/>        if i == '0':<br/>            l.append('0'+'0'+'0'+'0')<br/>        elif i == '1':<br/>            l.append('0'+'0'+'0'+'1')<br/>        elif i == '2':<br/>            l.append('0'+'0'+'1'+'0')<br/>        elif i == '3':<br/>            l.append('0'+'0'+'1'+'1')<br/>        elif i == '4':<br/>            l.append('0'+'1'+'0'+'0')<br/>#...and so on</span></pre><p id="ba2f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们继续讨论机器学习。为了简单起见，我们将测试并观察一个简单的神经网络是否能够识别关于SHA256的任何信息。为此，我们将使用梯度下降优化网络，同时使用sigmoid函数激活它。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="a56f" class="mg mh iq mc b gy mi mj l mk ml">model = tf.keras.Sequential([tf.keras.layers.Dense(256, activation='sigmoid')])</span><span id="99ee" class="mg mh iq mc b gy mm mj l mk ml">model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])</span></pre><p id="3ab9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们在这里看到，这个神经网络只有256个单元。这些单位将对应于SHA256算法中的256位。我们现在可以编写一个小函数，按照SHA256预处理协议的一些概念对字符串进行预处理。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="18d3" class="mg mh iq mc b gy mi mj l mk ml">def shapre(word):<br/>    wordbinlist=ascitobin(word)<br/>    wordbin=""<br/>    for j in wordbinlist:<br/>      wordbin=wordbin+j<br/>    c=0<br/>    wordarr = ['0' for _ in range(256)]<br/>    wordarr[255]=len(word)<br/>    wordarr[len(wordbin)]='1'<br/>    c=0<br/>    for i in wordbin:<br/>      wordarr[c]=i<br/>      c=c+1<br/>    return wordarr</span></pre><p id="f9ed" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">编写这个函数的原因将在本文的后面部分看到并解释，但一般来说，我们将把一个字符串分解成相应的位(如前面的函数所示)。然后，我们将在数组末尾追加一个“1”，用“0”填充数组。最后，在数组的末尾，我们将附上字符串的长度。让我们考虑单词“a”的例子(见下文)。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="663a" class="mg mh iq mc b gy mi mj l mk ml">0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.</span></pre><p id="f7db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">上述数据将形成一个主要的数据条目。输入模型的另一个主要数据条目是SHA256，它被分解成二进制数字。所以如果我们考虑同一个例子a-</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="6443" class="mg mh iq mc b gy mi mj l mk ml">word='a'<br/>SHA256=ca978112ca1bbdcafac231b39a23dc4da786eff8147c4e72b9807785afee48bbca978112ca1bbdcafac231b39a23dc4da786eff8147c4e72b9807785afee48bb</span><span id="6664" class="mg mh iq mc b gy mm mj l mk ml">Binary<br/>1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1.  1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.  0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0.  1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1.  0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1.  0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.  0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0.  0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.  1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0.  0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0.  0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.</span></pre><p id="e864" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了训练我们的模型，我们将使用来自Github的大约50万个单词。这些文字是免费的，可以在开放许可下获得(链接<a class="ae le" href="https://github.com/dwyl/english-words" rel="noopener ugc nofollow" target="_blank">此处</a>)。这个想法很简单。我们将把SHA256分解成它的基本组件(“1”&amp;“0”)，然后沿着原始字符串训练一个模型。原始字符串将遵循前面讨论的预处理。在字符串准备好应用SHA256算法之前，必须对其进行预处理。在这种情况下，我们只是遵循一些SHA256先决条件来测试和分析机器学习(参见下面为机器学习准备的数据)。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="600d" class="mg mh iq mc b gy mi mj l mk ml">data shape=(416296, 256)<br/>word="abaddon"</span><span id="3d90" class="mg mh iq mc b gy mm mj l mk ml">SHA256 tf.Tensor( [1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1.  1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.  0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0.  1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1.  0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1.  0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.  0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0.  0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.  1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0.  0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0.  0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.], shape=(256,), dtype=float32)</span><span id="72f6" class="mg mh iq mc b gy mm mj l mk ml">String in Binary tf.Tensor( [0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1.  0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.  0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7.], shape=(256,), dtype=float32) </span></pre><p id="85aa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们只训练几个时期来测试和观察模型的损失和准确性的变化。正如我们在下图中看到的，损耗在逐渐减少，而在第一次循环后精度达到大约100%。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="2814" class="mg mh iq mc b gy mi mj l mk ml">history=model.fit(x, y, epochs=10)</span><span id="3bb9" class="mg mh iq mc b gy mm mj l mk ml">Epoch 1/10 13010/13010 [==============================] - 24s 2ms/step - loss: 0.4527 - accuracy: 0.9159 Epoch 2/10 13010/13010 [==============================] - 22s 2ms/step - loss: 0.3709 - accuracy: 0.9999 Epoch 3/10 13010/13010 [==============================] - 22s 2ms/step - loss: 0.3684 - accuracy: 1.0000 Epoch 4/10 13010/13010 [==============================] - 22s 2ms/step - loss: 0.3664 - accuracy: 1.0000 Epoch 5/10 13010/13010 [==============================] - 22s 2ms/step - loss: 0.3664 - accuracy: 0.9999 Epoch 6/10 13010/13010 [==============================] - 22s 2ms/step - loss: 0.3659 - accuracy: 0.9999 Epoch 7/10 13010/13010 [==============================] - 21s 2ms/step - loss: 0.3656 - accuracy: 0.9999 Epoch 8/10 13010/13010 [==============================] - 22s 2ms/step - loss: 0.3649 - accuracy: 0.9999 Epoch 9/10 13010/13010 [==============================] - 22s 2ms/step - loss: 0.3644 - accuracy: 0.9999 Epoch 10/10 13010/13010 [==============================] - 22s 2ms/step - loss: 0.3655 - accuracy: 0.9999</span></pre><p id="70a9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有几个原因可以让我们对这99%的准确性持怀疑态度。主要原因是模型的大小。该型号只有256台。所以，我们不会把注意力放在机器学习和架构上。相反，让我们看看这种方法是否有任何模式识别。现在，如果我们只使用<code class="fe mx my mz mc b">predict</code>方法，我们将获得任何给定散列的一系列256个浮点值。因此，我们将使用<code class="fe mx my mz mc b">np.where</code>进行舍入，并查看单词“a”的散列的输出模式。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="7549" class="mg mh iq mc b gy mi mj l mk ml">word='a'<br/>hash_1=hash(word)<br/>ca978112ca1bbdcafac231b39a23dc4da786eff8147c4e72b9807785afee48bb</span><span id="5917" class="mg mh iq mc b gy mm mj l mk ml">vecs=model.predict(hash_1)<br/>predict=np.where(vecs &gt; 0.4, 1, 0)<br/>[[0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0   0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0   0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   0 0 0 1]]</span></pre><p id="fca0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">总的来说，模型预测了一个有趣的模式。该模式在某种程度上对应于“a”的原始预处理字符串。然而，这个预测没有一点是准确的，甚至长度也不准确，因为<code class="fe mx my mz mc b">np.where</code>实际上是对大约10的浮点小数进行舍入。但是我认为这个实验是成功的。这是因为模型被输入了“0”和“1”的任意散列，并且它仍然设法输出某种模式，而这种模式对于其输入来说并不是任意的。这是这个实验的目标，即，看看机器学习在学习关于SHA256的任何东西时是否有任何潜力。</p><h1 id="c364" class="na mh iq bd nb nc nd ne nf ng nh ni nj kf nk kg nl ki nm kj nn kl no km np nq bi translated">总结</h1><p id="5cc5" class="pw-post-body-paragraph lf lg iq lh b li nr ka lk ll ns kd ln lo nt lq lr ls nu lu lv lw nv ly lz ma ij bi translated">在撰写这篇文章的过程中，我学到了三个要点。</p><ol class=""><li id="4e4b" class="nw nx iq lh b li lj ll lm lo ny ls nz lw oa ma ob oc od oe bi translated">我怀疑SHA256是可以预测的。</li><li id="9d3f" class="nw nx iq lh b li of ll og lo oh ls oi lw oj ma ob oc od oe bi translated">SHA256是语言问题。</li><li id="7758" class="nw nx iq lh b li of ll og lo oh ls oi lw oj ma ob oc od oe bi translated">如果我不得不重写这篇文章，我会做不同的事情。</li></ol><p id="9239" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于第一点，一旦一个字符串被散列，就没有办法回到原来的字符串。然而，模型可以在高维空间中将语义相似的向量并置在一起。这就是TensorFlow嵌入式投影仪擅长的，即可视化机器学习。</p><p id="2b53" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于第2点，SHA256可以作为一个语言问题被机器学习。想象一个名叫“约翰·多伊”的人在一张纸上写下签名。现在将SHA256视为该签名的代表。这意味着该名称具有唯一的SHA256哈希，它链接并标识原始名称。约翰·多伊签名的这一特性意味着签名中有一些与姓名相关的信息。</p><p id="060a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于第三点，对我来说，语言模型成功背后的秘密是对沃尔夫密码类型的识别。隐型是一个深层的意义，它不是在一个词的形态学中给出的。换句话说，它是一个在散列中看不到或识别不到的意义，但它存在于将原始字符串连接到散列的过程中。这种意义可以并且已经被具有语义向量的机器学习成功捕获。</p><p id="2155" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它是在手套中用它们的共现矩阵学习的沃尔夫隐型，在我看来它也是在伯特中用掩蔽技术学习的。意义不能被解释或明确指出，但它可以通过并列来对比和阐明。读者可能会有兴趣将此作为一个语言问题来处理，所以让我们来看看我会如何以不同的方式做事情。</p><p id="aea0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，我会重写预处理函数，把字符串的长度变成二进制。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="2705" class="mg mh iq mc b gy mi mj l mk ml">def shapre(word):<br/>    wordbinlist=ascitobin(word)<br/>    wordbin=""<br/>    for j in wordbinlist:<br/>      wordbin=wordbin+j<br/>    c=0<br/>    wordarr = ['0' for _ in range(256)]<br/>    a=[]<br/>    x=bin(len(word))<br/>    for i in x[2:]:<br/>      a.append(i)<br/>    wordarr[(256-len(a)):256]=a<br/>    wordarr[len(wordbin)]='1'<br/>    c=0<br/>    for i in wordbin:<br/>      wordarr[c]=i<br/>      c=c+1<br/>    return wordarr</span></pre><p id="407e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第二，我对摆弄LSTMs感兴趣。</p><pre class="kp kq kr ks gt mb mc md me aw mf bi"><span id="321a" class="mg mh iq mc b gy mi mj l mk ml">vocab_size=416296<br/>embedding_dim=256</span><span id="2485" class="mg mh iq mc b gy mm mj l mk ml">model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=256),<br/>    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),<br/>    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(124, return_sequences=True)),<br/>    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.8)),<br/>    tf.keras.layers.Dense(256)<br/>])</span></pre><p id="44fd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是一个有趣的实验，我没有尝试过，但我认为它仍然不会产生太多关于SHA256的任何东西。我觉得用TensorFlow和PyTorch探索变形金刚会很有意思。但是我想我将把探索ML架构的工作留给读者。我写这篇文章的目的是让读者对比特币、ML和加密哈希函数的未来有所了解。本文的代码可以在Apache 2.0许可下获得，可以在这里找到<a class="ae le" href="https://github.com/seanjudelyons/SHA256/blob/main/SGD_SIGMOID_(SHA256)_SingleDense.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p></div></div>    
</body>
</html>