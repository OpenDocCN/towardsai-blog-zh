<html>
<head>
<title>Neural Networks from Scratch with Python Code and Math in Detail— I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络从零开始，详细介绍Python代码和数学——I</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf?source=collection_archive---------0-----------------------#2020-06-20">https://pub.towardsai.net/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf?source=collection_archive---------0-----------------------#2020-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/056c31860a81a69a6dd29f2628c35ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IRsMu32DQYRmaVuxQTKEjg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://pixabay.com/photos/neural-networks-brain-5321301/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><h2 id="1fb0" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>、<a class="ae ep" href="https://towardsai.net/p/category/scholarly" rel="noopener ugc nofollow" target="_blank">学者型</a>、<a class="ae ep" href="https://towardsai.net/p/category/tutorial" rel="noopener ugc nofollow" target="_blank">教程型</a></h2><div class=""/><div class=""><h2 id="fdb4" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">从头开始构建神经网络。从它们背后的数学到用Python和Google Colab一步一步实现编码示例</h2></div><p id="c46e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">作者:</strong> <a class="ae jg" href="https://www.linkedin.com/in/pratik-shukla28/" rel="noopener ugc nofollow" target="_blank">普拉蒂克·舒克拉</a>，<a class="ae jg" href="https://mktg.best/vguzs" rel="noopener ugc nofollow" target="_blank">罗伯特·伊里翁多</a></p><p id="3275" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后更新于2021年12月1日</p><div class="is it gp gr iu md"><a href="https://members.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">加入人工智能，成为会员，你将不仅支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><p id="143f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> <em class="ms">注:</em> </strong> <em class="ms">在我们的第二篇</em> <a class="ae jg" href="https://towardsai.net/p/machine-learning/building-neural-networks-with-python-code-and-math-in-detail-ii-bbe8accbf3d1" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> <em class="ms">神经网络教程</em> </strong> </a> <em class="ms">中，我们深入探究了使用神经网络的局限性和优势。我们展示了如何实现具有隐藏层的神经网络，以及这些如何导致我们的预测具有更高的准确率，以及在Google Colab上用Python实现的示例。</em></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/40c315a0303e97dd294aa4f90ea1f488.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*YNueo-SwLiNqPo50jV8Cpg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:神经网络在人工智能、机器学习和深度学习中的位置。</figcaption></figure><h1 id="e997" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">什么是神经网络？</h1><p id="7b25" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">神经网络形成了深度学习的基础，这是<a class="ae jg" href="https://mld.ai/mldcmu" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">机器学习</strong> </a>的一个子领域，其中人脑的结构激发了算法。神经网络获取输入数据，训练自己识别数据中的模式，然后预测一组新的类似数据的输出。因此，神经网络可以被认为是深度学习的功能单元，它模仿人脑的行为来解决复杂的数据驱动问题。</p><p id="ebcc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当我们想到“<strong class="lj jt">神经网络</strong>”时，我们首先想到的是生物学，的确，神经网络是受我们大脑的启发。</p><blockquote class="nv"><p id="cf1e" class="nw nx jj bd ny nz oa ob oc od oe mc dk translated">📚查看我们对<a class="ae jg" href="https://towardsai.net/p/machine-learning/best-machine-learning-books-free-and-paid-ml-book-recommendations-40c9ab30b0c" rel="noopener ugc nofollow" target="_blank">最佳机器学习书籍</a>的编辑推荐。📚</p></blockquote><p id="13e8" class="pw-post-body-paragraph lh li jj lj b lk of kt lm ln og kw lp lq oh ls lt lu oi lw lx ly oj ma mb mc im bi translated">让我们试着去理解它们:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/d43b199b559a19d05ce58ad738b92479.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxPHt27i8ewAQNg3lerMWw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:代表生物神经元的图像|来源:维基百科[ <a class="ae jg" href="https://en.wikipedia.org/wiki/Biological_neuron_model" rel="noopener ugc nofollow" target="_blank"> 1 </a></figcaption></figure><p id="9e65" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在机器学习中，神经元的树突称为输入，细胞核处理数据，并通过轴突转发计算的输出。在生物神经网络中，树突的宽度(厚度)定义了与其相关的权重。</p><h2 id="7494" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">索引:</h2><ol class=""><li id="ace0" class="ow ox jj lj b lk nq ln nr lq oy lu oz ly pa mc pb pc pd pe bi translated"><a class="ae jg" href="#3a44" rel="noopener ugc nofollow">什么是人工神经网络？</a></li><li id="f703" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#a78a" rel="noopener ugc nofollow">人工神经网络的应用</a></li><li id="7dfd" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#478a" rel="noopener ugc nofollow">人工神经网络(ANN)的一般结构</a></li><li id="948c" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#3997" rel="noopener ugc nofollow">什么是感知器？</a></li><li id="9193" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#8c37" rel="noopener ugc nofollow">感知器简单例子</a></li><li id="bf00" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#efe7" rel="noopener ugc nofollow"> Sigmoid函数(神经网络的激活函数)</a></li><li id="5d47" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#48c4" rel="noopener ugc nofollow">神经网络从零开始实现</a></li><li id="d855" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#344b" rel="noopener ugc nofollow">什么是梯度下降？</a></li><li id="5a68" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#cc41" rel="noopener ugc nofollow">神经网络所用公式的推导</a></li><li id="6002" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#0ea5" rel="noopener ugc nofollow">神经网络的Python实现</a></li><li id="4555" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#0700" rel="noopener ugc nofollow">我们为什么要加偏倚？</a></li><li id="a152" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated"><a class="ae jg" href="#a748" rel="noopener ugc nofollow">案例研究:用Python开发的神经网络预测病毒收缩</a></li></ol><blockquote class="nv"><p id="3eca" class="nw nx jj bd ny nz pk pl pm pn po mc dk translated">📚查看我们的教程<a class="ae jg" href="https://towardsai.net/p/machine-learning/monte-carlo-simulation-an-in-depth-tutorial-with-python-bcf6eb7856c8" rel="noopener ugc nofollow" target="_blank">蒙特卡罗模拟</a>和Python中的例子📚</p></blockquote><h1 id="3a44" class="my mz jj bd na nb nc nd ne nf ng nh ni ky pp kz nk lb pq lc nm le pr lf no np bi translated">1.什么是人工神经网络？</h1><p id="4e55" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">简而言之，人工神经网络代表相互连接的输入和输出单元，其中每个连接都有一个相关的权重。在学习阶段，网络通过调整这些权重进行学习，以便能够预测输入数据的正确类别。</p><p id="cd5e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:</p><p id="edd6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们遇到自己处于深度睡眠状态，突然我们的环境开始颤抖。紧接着，我们的大脑意识到这是一场地震。我们立刻想到对我们来说最有价值的东西:</p><ul class=""><li id="4736" class="ow ox jj lj b lk ll ln lo lq ps lu pt ly pu mc pv pc pd pe bi translated">我们深爱的人。</li><li id="d91d" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pv pc pd pe bi translated">基本文件。</li><li id="dd86" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pv pc pd pe bi translated">珠宝。</li><li id="5e29" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pv pc pd pe bi translated">笔记本电脑。</li><li id="c4b1" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pv pc pd pe bi translated">一支铅笔。</li></ul><p id="1e32" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们只有几分钟的时间走出房子，我们只能挽救一些东西。在这种情况下，我们的首要任务是什么？</p><p id="aaa5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">也许，我们是要先去救我们心爱的人，然后如果时间允许，我们可以想其他的事情。我们在这里做的是，给我们的贵重物品分配一个重量。在那一刻，每一个有价值的东西都是一个输入，优先级是我们赋予它的权重。</p><p id="2338" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">神经网络也是如此。我们给不同的值分配权重，并预测它们的输出。然而，在这种情况下，我们不知道每个输入的相关权重，因此我们制定了一个算法，通过处理大量输入数据来计算与它们相关的权重。</p><h1 id="a78a" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">2.人工神经网络的应用:</h1><h2 id="eaed" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> a .数据分类:</strong></h2><p id="2bd0" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">基于一组数据，我们训练好的神经网络预测它是狗还是猫？</p><h2 id="5bb3" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> b .异常检测:</strong></h2><p id="526c" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">给出一个人的交易细节，就可以说这个交易是不是欺诈。</p><h2 id="5c7f" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> c .语音识别:</strong></h2><p id="0265" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">我们可以训练我们的神经网络来识别语音模式。比如:Siri，Alexa，谷歌助手。</p><h2 id="8c6e" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> d .音频生成:</strong></h2><p id="7edc" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">给定作为音频文件的输入，它可以基于各种因素生成新的音乐，如流派、歌手等。</p><h2 id="04af" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> e .时间序列分析:</strong></h2><p id="2179" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">训练有素的神经网络可以预测股价。</p><h2 id="3ead" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> f .拼写检查:</strong></h2><p id="34fb" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">我们可以训练一个神经网络来检测拼错的拼写，也可以为单词提供类似的意思。例子:语法上</p><h2 id="e352" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> g .字符识别:</strong></h2><p id="e0ab" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">训练有素的神经网络可以检测手写字符。</p><h2 id="f69a" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> h .机器翻译:</strong></h2><p id="a4e2" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">我们可以开发一种神经网络，将一种语言翻译成另一种语言。</p><h2 id="8afa" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">一、图像处理:</strong></h2><p id="62ad" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">我们可以训练神经网络来处理图像，并从中提取信息。</p><h1 id="478a" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">3.人工神经网络(ANN)的一般结构:</h1><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/6e268b7b0fb129a7f087367fec98c2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/0*eI2BIlzkKfzef_yU.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3:人工神经网络</figcaption></figure><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi px"><img src="../Images/e6bfdf052408dca0b8802e75404558d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*Bqpea_57RtV-kJ6D.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:具有3层的人工神经网络</figcaption></figure><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/20e05cfbf432b7b3e5267058672a4411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WvPonYqOoZ-z6TosgBhEEg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图Frank Rosenblatt的感知机|来源:<a class="ae jg" href="https://mld.ai/mldcmu" rel="noopener ugc nofollow" target="_blank">卡内基梅隆大学机器学习系</a></figcaption></figure><h1 id="3997" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">4.什么是感知器？</h1><p id="ff03" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">感知器是一个没有任何隐藏层的神经网络。感知器只有一个输入层和一个输出层。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/b43933d444d001b8df7755724cb8fd5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/0*k4xSNASqLjbMSGON.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图6:感知器</figcaption></figure><h2 id="a668" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">我们可以使用感知器。</h2><p id="1b1d" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">感知器的用途在于许多情况下。虽然感知器主要用于简单的决策，但它们也可以在更大的计算机程序中一起解决更复杂的问题。</p><p id="9a8a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:</p><ol class=""><li id="146c" class="ow ox jj lj b lk ll ln lo lq ps lu pt ly pu mc pb pc pd pe bi translated">如果某人是教职员工，则允许访问；如果某人是学生，则拒绝访问。</li><li id="df19" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">只允许人类进入。</li><li id="fed8" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">逻辑门的实现<a class="ae jg" href="https://en.wikipedia.org/wiki/Logic_gate" rel="noopener ugc nofollow" target="_blank"> 2 </a>。</li></ol><h2 id="f730" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">实施神经网络的步骤包括:</h2><p id="e819" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">神经网络分两步执行:</p><h2 id="6b1b" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 1。前馈:</strong></h2><p id="329d" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">在前向神经网络中，我们有一组输入特征和一些随机权重。请注意，在这种情况下，我们采用随机权重，并使用反向传播进行优化。</p><h2 id="e63e" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 2。反向传播:</strong></h2><p id="e8c2" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">在反向传播期间，我们计算预测输出和目标输出之间的误差，然后使用算法(梯度下降)来更新权重值。</p><h2 id="d66d" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">为什么我们需要反向传播？</strong></h2><p id="e96a" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">在设计神经网络时，首先，我们需要训练一个模型，并为每个输入分配特定的权重。这个权重决定了这个特征对我们的预测有多重要。权重越高，重要性越大。然而，最初，我们不知道这些输入所需的具体重量。所以我们要做的是，给我们的输入分配一些随机权重，然后我们的模型计算预测的误差。此后，我们更新我们的权重值并重新运行代码(反向传播)。在单独迭代之后，我们可以获得更低的误差值和更高的精度。</p><h2 id="9df5" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">概述人工神经网络:</h2><ol class=""><li id="8666" class="ow ox jj lj b lk nq ln nr lq oy lu oz ly pa mc pb pc pd pe bi translated">接受输入</li><li id="285c" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">添加偏差(如果需要)</li><li id="7977" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">为输入要素分配随机权重</li><li id="0a9b" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">运行代码进行培训。</li><li id="86c4" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">找出预测中的错误。</li><li id="32c4" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">通过梯度下降算法更新权重。</li><li id="8c79" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">用更新的重量重复训练阶段。</li><li id="aa6e" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">做预测。</li></ol><h2 id="fb41" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">简单神经网络的流程图:</h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/8d3ac1646bd2f6facdf0280594e01d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*ZJtto33Yo-gc4xPa.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7:人工神经网络(ANN)基本流程图</figcaption></figure><h2 id="e7a9" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">神经网络的训练阶段:</h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qb"><img src="../Images/6e9cf682fa6547b6d36e673147d4f799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FXlZ_3zWqsULq68r.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:神经网络的训练阶段</figcaption></figure><h1 id="8c37" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">5.感知器示例:</h1><p id="4db5" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">下面是一个简单的感知器模型，有四个输入和一个输出。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/fef03cdba9dff201b841829752af0882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/0*87ax_yzYZp-OUkFL.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:一个简单的感知机</figcaption></figure><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/70ccd6c9a0099eac28536742fd918ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/0*STlyBIvRqyewqRV2.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:一组数据</figcaption></figure><p id="555b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里我们有输入值和它们对应的目标输出值。所以我们要做的是，给输入分配一些权重，然后计算它们的预测输出值。</p><p id="697e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本例中，我们将通过以下公式计算输出:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/488fb6364e4c25646372bdbd9afe1e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/0*yD_zwiYpMJCJ3sov.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:计算神经网络输出的公式</figcaption></figure><p id="1613" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这个例子中，为了简化计算，我们将偏差值设为0。</p><p id="f6d1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> a. </strong>我们取W = 3，检查预测输出。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/91c125f6c24866c5173cd918f119dc2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/0*fO15qRrcxOuukGwu.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12:W = 3时的输出</figcaption></figure><p id="4788" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> b. </strong>在我们找到W=3的预测输出值后，我们将把它与我们的目标输出进行比较，通过这样做，我们可以找到预测模型中的误差。请记住，我们的目标是为我们的模型实现最小的误差和最大的准确性。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qg"><img src="../Images/13a98027d3d2c4b34060161bfd22f1e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pkDzx8UWu7vEnOES.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图13:W = 3时的误差</figcaption></figure><p id="68f6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> c. </strong>注意，在上面的计算中，4个预测中有3个有误差。因此，我们必须改变我们的权重参数值设置为低。现在我们有两个选择:</p><ol class=""><li id="340a" class="ow ox jj lj b lk ll ln lo lq ps lu pt ly pu mc pb pc pd pe bi translated">增加重量</li><li id="11db" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">减轻重量</li></ol><p id="4239" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，我们将增加权重的值，并检查它是否会导致更高的错误率或更低的错误率。这里我们将权重值增加1，并将其更改为W = 4。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/c2610d2f7dda41f7d05ed6448316a3d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/0*lRruKXzrRMozL_mI.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图14:W = 4时的输出</figcaption></figure><p id="e984" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> d. </strong>从上图我们可以看到，就是预测的误差在增加。所以现在我们可以得出结论，增加权重值并不能帮助我们减少预测中的误差。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/ae4d18f4f97ffd0c63c9accac4d6b363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/0*Zmh9TDpZWphyzBNE.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图15:W = 4时的误差</figcaption></figure><p id="94f5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e. </strong>在我们增加权重值失败后，我们将为其降低权重值。此外，通过这样做，我们可以看到它是否有帮助。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/5e5f0ec73b6663b876df4fcb49c826e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/0*8WmbNsR8mY8wtSII.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图16:W = 2时的输出</figcaption></figure><p id="82f5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> f. </strong>计算预测中的误差。这里我们可以看到，我们已经达到了全球最低水平。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/edbe437de57678f36b58a68265843d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/0*COTKWaMsDt2DgdYf.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图17:W = 2时的误差</figcaption></figure><p id="2762" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图17中，我们可以看到预测没有错误。</p><p id="e3dc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们在这里做的是:</p><ol class=""><li id="d9d6" class="ow ox jj lj b lk ll ln lo lq ps lu pt ly pu mc pb pc pd pe bi translated">首先，我们有输入值和目标输出。</li><li id="ed6c" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">然后我们初始化一些随机值为W，然后我们继续。</li><li id="6e0d" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pb pc pd pe bi translated">最后，我们计算了该权重值的预测误差。之后，我们更新了权重并预测了输出。经过几次反复试验，我们可以减少预测中的误差。</li></ol><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/18a8107c4e297eab1f5005fd625dba55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*o4258202yMrg7Kdc.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图18:演示我们的功能</figcaption></figure><p id="2fb8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，我们试图得到重量值，使误差最小。我们需要弄清楚是否需要增加或减少权重值。一旦我们知道了，我们继续在那个方向更新权重值，直到误差变得最小。我们可能会达到这样一个点，如果权重发生进一步的更新，误差将会增加。这时候，我们需要停下来，那就是我们最终的重量值。</p><p id="ae6b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在现实生活的数据中，情况可能会更复杂一些。在上面的例子中，我们看到我们可以尝试不同的权重值，并手动获得最小误差。但是，在实际数据中，权重值通常是小数(非整数)。因此，我们将使用具有低学习率的梯度下降算法，以便我们可以尝试不同的权重值，并从我们的模型中获得最佳预测。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/17a3aede611dc992f8fe868131173493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/0*YQxmeju_ZNDvOh_R.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图19:表示最终</figcaption></figure><h1 id="efe7" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">6.Sigmoid函数:</h1><p id="1a2d" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">在我们的神经网络训练中，sigmoid函数充当<strong class="lj jt">激活函数。我们通常使用神经网络进行分类。在二元分类中，我们有两种类型。然而，正如我们所看到的，我们的输出值可以是我们使用的等式中的任何可能的数字。为了解决这个问题，我们使用一个sigmoid函数。现在为了分类，我们希望输出值为0或1。所以为了得到0到1之间的值，我们使用sigmoid函数。sigmoid函数在0和1之间转换我们的输出值。</strong></p><p id="bb1d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们来看看:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/ec51764dbefe0d4ce9cbd3d5c0060cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*85x58ShMCwJVfehZ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图20: Sigmoid函数</figcaption></figure><p id="19ae" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们用Python可视化我们的sigmoid函数:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/c999eb5ccafb53fd19b52f0f4a12bf4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/0*Xc3aWE_hffumnes5.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图sigmoid函数的Python代码</figcaption></figure><p id="15be" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">输出:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/944ab72a5f9b2b958d356e850835c884.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/0*nNmpIZag9_WI1Edk.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图22: Sigmoid函数图</figcaption></figure><p id="0b9f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">说明:</strong></p><p id="9418" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图21和22中，对于任何输入值，sigmoid函数的值将始终介于0和1之间。请注意，对于负数，sigmoid函数的输出≤0.5，或者我们可以说更接近于零，对于正数，输出将大于0.5，或者我们可以说更接近于1。</p><h1 id="48c4" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">7.从头开始实现神经网络:</h1><p id="4240" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">我们要做的是用感知器实现“或”逻辑门。请记住，这里我们不会使用任何隐藏层。</p><h2 id="7259" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">什么是逻辑或门？</h2><p id="53fb" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">直截了当地说，当其中一个输入为1时,“或”门的输出也将为1。这意味着只有当两个输入都为0时，输出才为0。</p><h2 id="cf82" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">代表性:</h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/ec4bf19736203f4b779a6cae1e3d8581.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/0*xdOl1O8iRpdo0hy6.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图23:或门</figcaption></figure><h2 id="a945" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">or门的真值表:</h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/cdf298b4aa063cbc23b8cc0591884b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/0*FCHVfcoBGuUdOaAN.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图24:OR门的真值表数据集</figcaption></figure><h2 id="c002" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">或门的感知器；</h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/d52ce854a2ebd6e5a6dd8abf9b5f5345.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/0*WZ3ejB1QgidBrUak.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图25:一个感知机</figcaption></figure><p id="aa86" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们将为每个输入值分配一些权重并进行计算。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/32e0cd4a4212316dd45ef78c9c4efd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/0*loux0m-d__XpgCAj.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图26: A加权感知器</figcaption></figure><h2 id="e7da" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">示例:(手动计算)</h2><p id="78b0" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated"><strong class="lj jt"> a .计算o1的输入:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/403905ac2127c18261e3d159039195f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/0*fzWIa0YUrnAb-wyu.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图27:计算o1输入的公式</figcaption></figure><p id="ac81" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> b .计算输出值:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qt"><img src="../Images/aa1cb1adb89f00c09a42af50b41d9a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/0*GhEZJ9nPQ8JMQbEs.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图28:计算输出值的公式</figcaption></figure><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qu"><img src="../Images/c3fd93a667ba0684685d43accc08452d.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/0*HK75lIcOEUs77kzW.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图29:结果输出值</figcaption></figure><p id="d57d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意，从我们的真值表中，我们可以看到我们想要的输出是1，但是我们在这里得到的是0.68997。现在我们需要计算误差，然后反向传播，然后更新权重值。</p><p id="8e8f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> c .误差计算:</strong></p><p id="087a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们将使用均方差来计算误差:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/41455b4fc1b8bc2dbd176ed168475440.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/0*sh1oNhH4moIxoWFK.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图30:均方误差公式</figcaption></figure><p id="7f3b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">求和符号(适马符号)意味着我们必须将所有输入集的误差相加。在这里，我们将看到它是如何只对一个输入集起作用的。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qw"><img src="../Images/7ca35e26dc06fb5b30a542457e4ca48e.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/0*lfP2f0U6iOnasMjQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图31:MSE的结果</figcaption></figure><p id="6c87" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们必须对所有剩余的输入进行同样的操作。既然我们已经发现了误差，我们必须更新权重的值以使误差最小。为了更新权重值，我们将使用梯度下降算法。</p><h1 id="344b" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">8.什么是梯度下降？</h1><p id="c579" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">梯度下降是一种机器学习算法，它迭代地寻找其参数的最佳值。它考虑了用户定义的学习率和初始参数值。</p><p id="47a1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">工作:(迭代)</p><p id="3f74" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">1.从初始值开始。</p><p id="f2db" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">2.计算成本。</p><p id="fc6d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">3.使用Update函数更新值。</p><p id="92a4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">4.返回成本函数的最小成本</p><h2 id="a54b" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">我们为什么需要它？</h2><p id="1ce6" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">一般来说，我们要做的是，找到给出参数最优值的公式。但是，在这个算法中，它是自己找值的！</p><blockquote class="nv"><p id="6ab3" class="nw nx jj bd ny nz oa ob oc od oe mc dk translated">很有趣，不是吗？</p></blockquote><figure class="qy qz ra rb rc iv gh gi paragraph-image"><div class="gh gi qx"><img src="../Images/243d5e01b1c92a8e42ea398e1ea33de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*PKC5zgTemM4VUG9p.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图32；梯度下降算法的公式</figcaption></figure><p id="f4dd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将用这个算法来更新我们的权重。首先，我们需要求f(X)的导数。</p><h1 id="cc41" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">9.神经网络中所用公式的推导</h1><p id="8d84" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">接下来，我们想要找到的是特定的权重值如何影响误差。发现我们要应用链式法则。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rd"><img src="../Images/c2a0155d2d92e131f9e7b1ef946d6f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/0*i2GFerIxFMfuQd_7.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图33:寻找导数</figcaption></figure><p id="e791" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">之后，我们要做的是找到这三个导数的值。</p><p id="aee2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在下面的图片中，我们试图展示这些导数的导数，以展示梯度下降背后的数学原理。</p><p id="3a2a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> d .计算衍生品:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi re"><img src="../Images/1e37a668bc383808611ccb2265cada12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/0*F9FMqgZwe4VNkNIG.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图34:计算导数</figcaption></figure><p id="944a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们的案例中:</p><p id="4f0e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">输出= 0.68997 <br/>目标= 1</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rf"><img src="../Images/b68582631a734fb196d3bf0794105b45.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/0*6h1riYbukzXWr24m.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图35:寻找一阶导数</figcaption></figure><p id="2995" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e .求导数的第二部分:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rg"><img src="../Images/20fdd95d203b09dadf63dadb4d40ea81.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*zuNRVzU4UiwmyQaT.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图36:计算第二部分</figcaption></figure><p id="30ea" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">要逐步理解它:</p><p id="70d9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">outo 1的e.a .值:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rh"><img src="../Images/d088e5adccf099ab7630317666a66dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/0*Ebmm6yov2nyfWhh1.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图37:outo 1的值</figcaption></figure><p id="b358" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.b .求关于ino1的导数:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ri"><img src="../Images/015984b8287b0ceaacb1363a8b8b2f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/0*vQHkgM75zevAE1ui.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图38:outo 1相对于ino1的导数</figcaption></figure><p id="1691" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.c .稍微简化一下，很容易找到导数:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rj"><img src="../Images/a48b0132a49d9790dc67678c7bc27377.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/0*vQKp7YK0-tf6UHlp.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图39:简化</figcaption></figure><p id="777b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用链式法则和幂法则:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rk"><img src="../Images/e39c74fd050b1d61b9e5363c6fb31267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*pV2jMa2NO-qRbSMD.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图40:应用链式法则和幂法则</figcaption></figure><p id="e545" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.e .应用求和规则:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rl"><img src="../Images/833bfcf57bf9e7ba982afb1cdc577b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*05RXws-9j-WWTVFI.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图41:针对ino1对outo1应用求和规则</figcaption></figure><p id="8432" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.f .常数的导数为零:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rm"><img src="../Images/d89a8ba0428671815108542179e5f3b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/0*3UZA0GzzSQn5L9nu.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图42:常数的导数为零</figcaption></figure><p id="5c5f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">例如应用指数法则和链式法则:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rn"><img src="../Images/b39b713c498fc24f088eae5db67392a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/0*0dQOe8qKivar7f9S.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图42:应用指数规则和链式规则</figcaption></figure><p id="d48f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.h .稍微简化一下:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ro"><img src="../Images/954653999068b53474d022dec3e3d6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/0*3JVJMZZRF-o4MRmK.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图43:简化导数</figcaption></figure><p id="c910" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">即将两个负号相乘:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rp"><img src="../Images/8f283a202fbb81fdd49362f38498c25f.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/0*FuC8qBB0cDpZ0NET.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图44:两个负数的乘法</figcaption></figure><p id="7ae3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.j .把负幂放在分母:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rq"><img src="../Images/f614681147210f0c2abcb450d04b2609.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/0*YcSkl5Vm5SqQGsTx.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图45:将负幂移动到分母</figcaption></figure><p id="1332" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">就是这样。然而，我们需要简化它，因为对于我们的机器学习算法来说，处理大量输入有点复杂。</p><p id="b0c1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.k .简化它:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rr"><img src="../Images/207fbb83ac294fdbbad7a3f280f170d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*X-LmFQgphyYCLdge.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图46:简化算法</figcaption></figure><p id="36a2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.l .进一步简化:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rs"><img src="../Images/178283ffe314d435d7c0fe335395452d.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/0*X71FrR_OjNwcYS4L.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图47:简化的第二步</figcaption></figure><p id="7414" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.k .添加+1–1:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rt"><img src="../Images/fcf407a97f84717661cebb5012f065f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/0*xqZ8PHss3guRdCMX.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图48:添加值</figcaption></figure><p id="d14c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.l .分离零件:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ru"><img src="../Images/b7f8c83d8a80ab1cc48b497f9f8a4b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/0*akdKdHzbmBvZhmYT.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图49:分离算法</figcaption></figure><p id="075c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.m .简化:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rv"><img src="../Images/2b242c054cf84a8a13b86864a2523677.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*lD6ve6F54wjk4_pY.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图50:简化分离</figcaption></figure><p id="5b70" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.n .现在我们都从等式1知道了outo1的值:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rh"><img src="../Images/f90155d3f2808a7a9c021a83999b77a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/0*7JwL6i6KSSWBd-ha.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图outo1的值</figcaption></figure><p id="2e8f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由此我们可以推导出下面的最终导数</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rw"><img src="../Images/89ab21e57a8b486799b8202430394971.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/0*Zymcf8kDZzi-U1by.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图52:推导最终导数</figcaption></figure><p id="e738" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e.p .计算我们输入的值:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rx"><img src="../Images/54e0fb56941499c138f9f50971b5ddc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/0*JleapOZ654ZiRJhX.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图53:输出的最终计算</figcaption></figure><p id="36e5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> f .寻找导数的第三部分:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ry"><img src="../Images/a9a4654443abeab9d3fadc0980489eb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/0*GLyWTC8WINNksuCo.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图54:计算三阶导数的公式</figcaption></figure><p id="3aef" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"/><code class="fe rz sa sb sc b"><strong class="lj jt">ino</strong></code><strong class="lj jt">的f.a值:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sd"><img src="../Images/34e0258e8f595a34c66e2002ac9036ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/0*b3yXCxBBuTfp8pRs.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图55:ino的价值</figcaption></figure><p id="9712" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> f.b .寻找衍生工具:</strong></p><p id="9629" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">除w2之外的所有其他值在这里都被认为是常数。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi se"><img src="../Images/435b153b83f48289525d1e6387e066b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*v5SOnOqtJmdCPQpc.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图56:寻找导数</figcaption></figure><p id="3df9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> f.c .为我们的输入计算两个值:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sf"><img src="../Images/9ffcfc53f7f6bf56ba571c99ad195426.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/0*0x1Wmlgi7fQvSA-A.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图57:计算输入的两个值</figcaption></figure><p id="bf50" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">消防部门汇总:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sg"><img src="../Images/d88f52802691e77538df8ca01a986795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/0*4aZ6llLOxSJYvRRw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图58:整体计算</figcaption></figure><p id="a51c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">将它放入我们的主方程:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sh"><img src="../Images/1863acc0605ba4297b7a455924cc2c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/0*5UcqO3vguoeGPGHG.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图59:把它放在主方程上</figcaption></figure><p id="29c8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> f.f我们可以计算:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi si"><img src="../Images/b00d748675be19b558378a6e5ac94757.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*fbEfSzDxwU2iliQm.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图60:第二重量的计算</figcaption></figure><p id="5664" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意这里的权重值增加了。我们可以用这种方法计算所有的值，但是正如我们所看到的，这将是一个漫长的过程。所以现在我们要用Python实现所有的步骤。</p></div><div class="ab cl sj sk hx sl" role="separator"><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so"/></div><div class="im in io ip iq"><h2 id="59f8" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">人工实现神经网络的概要:</h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sq"><img src="../Images/6eb6811992d71b7b02a0b2e391806887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/0*Yjrlsu__vO3y08zx.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图61:记住ino =输入值*权重| outo =应用sigmoid函数后的输出值</figcaption></figure><p id="2753" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> a .感知器的输入:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sr"><img src="../Images/74fa57d697b1aef26d452373c1431c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/0*1jHa89eAni9t0FTO.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图62:感知器的输入值</figcaption></figure><p id="4be3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> b .将sigmoid函数应用于预测输出:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ss"><img src="../Images/17e69df2928dabca586f92d6c537461b.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/0*4lTazX9HP9c7aawp.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图63:对预测输出应用sigmoid函数</figcaption></figure><p id="908a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> c .计算误差:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/6afe329908c9bd00b109b192fd25bb28.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/0*yuD_uNCSYoevSiMY.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图64:计算误差</figcaption></figure><p id="127a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> d .根据梯度下降公式改变权重值:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi st"><img src="../Images/74f737a5694d4eb15e2b7afbcf01f862.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/0*TFnbyO8o3BLYbwWD.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图65:基于梯度下降改变权重值</figcaption></figure><p id="176b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> e .计算导数:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi su"><img src="../Images/51254aee3266ca6a18c154b933392bf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/0*sLxzY6TFEf-Cz-y7.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图66:计算导数</figcaption></figure><p id="1d41" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> f .个别衍生产品:</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sv"><img src="../Images/4bc0e02dbc559a0af1bbb2818ae83540.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/0*xtpvuLnbBTey67ST.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图67:一阶导数</figcaption></figure><p id="604c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">来源:图片由作者创作。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sw"><img src="../Images/ff49b9ad48bbcf91548a0a10ca571d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/0*fsnZtcWYU42tPfwL.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图68:二阶导数</figcaption></figure><p id="3f16" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">来源:图片由作者创作。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sx"><img src="../Images/455a972581f15e201bf53f0a378670ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/0*igteaxfY9yYkaqzQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图69:三阶导数</figcaption></figure><p id="e7d3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，我们使用更新的权重值运行相同的代码。</p><blockquote class="nv"><p id="71df" class="nw nx jj bd ny nz oa ob oc od oe mc dk translated">让我们编码:</p></blockquote><h1 id="0ea5" class="my mz jj bd na nb nc nd ne nf ng nh ni ky pp kz nk lb pq lc nm le pr lf no np bi translated">10.用Python实现神经网络；</h1><h2 id="473a" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 10.1导入所需库:</strong></h2><p id="1576" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">首先，我们要导入Python库。我们使用NumPy进行计算:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sy"><img src="../Images/b672d834856810b9789349b6bab4b48e.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/0*_DzeheykHF1QomhD.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图70:导入数字</figcaption></figure><h2 id="c0fb" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 10.2分配输入值:</strong></h2><p id="bbe9" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">接下来，我们将获取我们想要训练神经网络的输入值。这里我们可以看到，我们采用了两个输入特征。在实际数据集中，输入要素的值通常很高。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi sz"><img src="../Images/15d88129174faab10ea8d76d84963c82.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/0*Sciz4hWMAPR1XWAh.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图71:分配输入值来训练我们的神经网络</figcaption></figure><h2 id="65ad" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 10.3目标产量:</strong></h2><p id="9129" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">对于输入特性，我们希望特定的输入特性有特定的输出。它被称为目标输出。我们将训练为我们的输入特征提供目标输出的模型。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ta"><img src="../Images/dc868f872622521497591985a67eb302.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/0*Q9AybSXAfCklzuOe.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图72:定义目标输出</figcaption></figure><h2 id="0aac" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 10.3分配权重:</strong></h2><p id="a01f" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">接下来，我们将为输入要素分配随机权重。请注意，我们的模型会将这些权重值修改为最佳值。此时，我们随机取这些值。这里我们有两个输入要素，所以我们要取两个权重值。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi tb"><img src="../Images/ff5f9817d1d2b0003abf457ce7978fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/0*RPLlUfWWcDa9LlGC.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图73:为我们的输入特征分配随机权重</figcaption></figure><h2 id="fbf9" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 10.4添加偏差值并指定学习率:</strong></h2><p id="c4ca" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">现在，我们将添加偏差值。bias = 1的值。然而，分配给它的权重最初是随机的，我们的模型将针对我们的目标输出优化它。</p><p id="050c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另一个参数叫做学习率(LR)。我们将使用梯度下降算法中的学习率来更新权重值。一般来说，我们保持尽可能低的学习率，这样我们就可以达到最小的错误率。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi tc"><img src="../Images/0832147211e3f4cdfc92f7c351ef976f.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/0*92yjeOTsbWfIBuCD.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图74:添加偏差值并分配学习率(LR)</figcaption></figure><h2 id="7e76" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 10.5应用Sigmoid函数:</strong></h2><p id="ba94" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">一旦我们有了权重值和输入特征，我们将把它发送给预测输出的主函数。现在请注意，我们的输入要素和权重值可以是任何值，但是这里我们要对数据进行分类，因此我们需要0到1之间的输出。为此，我们要去一个sigmoid函数。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi td"><img src="../Images/1aa46e743aa8a5d1bc2bca5abc88045d.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/0*7Wdoc52nVsZ7Pizf.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图75:将sigmoid函数应用于我们的神经网络</figcaption></figure><h2 id="f1ea" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">10.6 sigmoid函数的导数:</strong></h2><p id="dd12" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">在梯度下降算法中，我们需要sigmoid函数的导数。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi te"><img src="../Images/b40e3c027f128f61d6376d0aad609316.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/0*vmgA7nb21IHnRLkG.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图76:计算sigmoid函数的导数</figcaption></figure><h2 id="24ec" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 10.7预测输出和更新权重值的主要逻辑:</strong></h2><p id="05b9" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">我们将逐步解释下面的代码。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi tf"><img src="../Images/55b90eee33fddf4bebad8d44239b04b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/0*xVrxL5fDz_MRmwRy.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图77:点10.1到10.7的源代码</figcaption></figure><h2 id="abe5" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">它是如何工作的？</h2><p id="44cc" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">首先，上面的代码需要运行大约10，000次。请记住，如果我们只运行这段代码几次，那么我们可能会有更高的错误率。因此，简而言之，我们可以说我们将更新权重值10，000次，以达到可能的最佳值。</p><p id="4315" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们需要做的是将输入特征与相应的权重值相乘，我们要提供给感知器的值可以用矩阵的形式表示。</p><p id="8c84" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rz sa sb sc b">in_o</code>代表<code class="fe rz sa sb sc b">input_features</code>和<code class="fe rz sa sb sc b">weight</code>的点积。请注意，第一个矩阵(输入要素)的大小为(4*2)，第二个矩阵(权重)的大小为(2*1)。乘法之后，结果矩阵的大小为(4*1)。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi tg"><img src="../Images/c293e9104e48594a34c1b7f800c5f4f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/0*_CK2wHr-MBdkHUSe.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图78:每个方框代表一个值</figcaption></figure><p id="16c2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在上面的表示中，每个盒子代表一个值。</p><p id="c81b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在在我们的公式中，我们也有偏差值。我们用简单的矩阵表示来理解吧。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi sr"><img src="../Images/555d5ef7ae946a9661d8c8e534f4e15a.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/0*uHsQEa0uxIFZ0rS3.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图79:值的矩阵表示，带有额外的偏置值</figcaption></figure><p id="67da" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们将添加偏差值。矩阵中的加法运算很容易理解。这就是sigmoid函数的输入。之后，我们将对输入值应用sigmoid函数，这将给出介于0和1之间的预测输出值。</p><p id="1675" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们要计算预测的误差。为此，我们通常使用<strong class="lj jt">均方误差(MSE) </strong>，但是为了计算的简单，这里我们只使用简单的误差函数。最后，我们将添加所有四个输入的误差。</p><p id="6a72" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们的最终目标是最小化误差。为了最小化误差，我们可以更新我们的权重值。为了更新权重值，我们将使用梯度下降算法。</p><p id="62e7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了找到导数，我们需要梯度下降算法的一些导数的值。正如我们已经讨论过的，我们将找到3个单独的导数值，然后乘以它。</p><p id="5da8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一阶导数是:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi th"><img src="../Images/12b24b60566f9856079a564f1cb30239.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/0*7CAaPidwmeaWjgFS.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图80:一阶导数</figcaption></figure><p id="18c2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">二阶导数是:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ti"><img src="../Images/8d9bd0f6cee14474be9cde60fd507ca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/0*AmaCWLj3N_wo9pmw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图81:二阶导数</figcaption></figure><p id="d138" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">三阶导数是:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi tj"><img src="../Images/6db8930082f0bf40bb37cdaa53e7afac.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/0*0OxjDRNl4phnEgm6.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图82:三阶导数</figcaption></figure><p id="16d2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意，我们可以很容易地找到前两个导数的值，因为它们不依赖于输入。接下来，我们将前两个导数的乘积值存储在<code class="fe rz sa sb sc b">deriv</code>变量中。现在这些导数的值必须和权重的大小一样。权重的大小是(2*1)。</p><p id="fa96" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了找到最终的导数，我们需要找到我们的<code class="fe rz sa sb sc b">input_features </code>的转置，然后我们将它乘以我们的<code class="fe rz sa sb sc b">deriv</code>变量，它基本上是其他两个导数的乘积。</p><p id="3db9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们来看看运算的矩阵表示。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rw"><img src="../Images/d8c645137722642da737a53c3a6c778b.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/0*G9dM-1h6ERY6x5hc.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图83:操作的矩阵表示</figcaption></figure><p id="203b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图83上，第一个矩阵是<code class="fe rz sa sb sc b">input_features</code>的转置矩阵。第二个矩阵存储其他两个导数的乘积值。现在看到我们已经将这些值存储在一个名为<code class="fe rz sa sb sc b">deriv_final</code>的矩阵中。请注意，<code class="fe rz sa sb sc b">deriv_final</code>的大小是(2*1)，这与我们的权重矩阵的大小(2*1)相同。</p><p id="5138" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">之后，我们更新权重值，注意我们有了更新权重所需的所有值。我们将使用以下公式来更新权重值。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi tk"><img src="../Images/172406d50e3bc202c5eb5e9af4c877de.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/0*WI-WDysda5RWYump.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图84:更新我们体重值的公式</figcaption></figure><p id="e6d7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后，我们需要更新偏差值。如果我们记得这个图表，我们可能已经注意到偏差权重的值不依赖于输入。所以要单独更新。在这种情况下，我们需要<code class="fe rz sa sb sc b">deriv</code>值，因为它不依赖于输入值。为了更新偏差值，我们通过for循环在每次迭代中更新每个输入的值。</p></div><div class="ab cl sj sk hx sl" role="separator"><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so"/></div><div class="im in io ip iq"><h2 id="ef0d" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> 10.8检查重量和偏差的值:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi tl"><img src="../Images/7797583347d4507508ca6e5dd0689c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/0*fLTm6tyjTY4H6hg1.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图85:注意我们的权重和值是如何从随机分配的值变化的</figcaption></figure><p id="ffff" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图85中，注意我们的权重和偏差值已经从我们随机分配的值改变了。</p><p id="be39" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> 10.9预测值:</strong></p><p id="e9cd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">既然我们已经训练了我们的模型，我们可以开始根据它进行预测。</p><p id="a8a5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> 10.9.1对(1，0)的预测:</strong></p><p id="33ec" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标值= 1</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi tm"><img src="../Images/6e3b2227a886986f34605a27fbd6cdd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/0*SzWVHkTTZvUn_Fxi.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图86:目标值= 1时，预测输出约为1</figcaption></figure><p id="a5fb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图86中，我们可以看到预测输出非常接近1。</p><p id="c274" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">10 . 9 . 2(1，1)的预测:</strong></p><p id="ff02" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标产量= 1</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi tn"><img src="../Images/7bfb4bcfa54ecc20068bc4b43c01672d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*M4s5VySR-AQuhkux.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图87:目标输出= 1时，预测值接近1</figcaption></figure><p id="3c91" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图87中，我们可以看到预测输出非常接近1。</p><p id="8e93" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">10 . 9 . 3(0，0)的预测:</strong></p><p id="2c93" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标输出= 0</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi to"><img src="../Images/4485f49246cf28656f2e0d7783949b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/0*a8qF8mywqrNPGIPW.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图88:目标输出= 0时，预测值接近0</figcaption></figure><p id="244a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图88中，我们可以看到预测输出非常接近0。</p></div><div class="ab cl sj sk hx sl" role="separator"><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so"/></div><div class="im in io ip iq"><h1 id="4499" class="my mz jj bd na nb tp nd ne nf tq nh ni ky tr kz nk lb ts lc nm le tt lf no np bi translated">将所有这些放在一起:</h1><pre class="mu mv mw mx gt tu sc tv tw aw tx bi"><span id="0633" class="ol mz jj sc b gy ty tz l ua ub"># Import required libraries:<br/>import numpy as np# Define input features:<br/>input_features = np.array([[0,0],[0,1],[1,0],[1,1]])<br/>print (input_features.shape)<br/>print (input_features)# Define target output:target_output = np.array([[0,1,1,1]])# Reshaping our target output into vector:<br/>target_output = target_output.reshape(4,1)<br/>print(target_output.shape)<br/>print (target_output)# Define weights:<br/>weights = np.array([[0.1],[0.2]])<br/>print(weights.shape)<br/>print (weights)# Bias weight:<br/>bias = 0.3# Learning Rate:<br/>lr = 0.05# Sigmoid function:<br/>def sigmoid(x):<br/>return 1/(1+np.exp(-x))# Derivative of sigmoid function:<br/>def sigmoid_der(x):<br/>return sigmoid(x)*(1-sigmoid(x))# Main logic for neural network:</span><span id="5647" class="ol mz jj sc b gy uc tz l ua ub"># Running our code 10000 times:for epoch in range(10000):<br/>inputs = input_features#Feedforward input:<br/>in_o = np.dot(inputs, weights) + bias #Feedforward output:<br/>out_o = sigmoid(in_o) #Backpropogation</span><span id="a0c4" class="ol mz jj sc b gy uc tz l ua ub">#Calculating error<br/>error = out_o - target_output</span><span id="44d5" class="ol mz jj sc b gy uc tz l ua ub">#Going with the formula:<br/>x = error.sum()<br/>print(x)</span><span id="37fa" class="ol mz jj sc b gy uc tz l ua ub">#Calculating derivative:<br/>derror_douto = error<br/>douto_dino = sigmoid_der(out_o)</span><span id="ab1a" class="ol mz jj sc b gy uc tz l ua ub">#Multiplying individual derivatives:<br/>deriv = derror_douto * douto_dino #Multiplying with the 3rd individual derivative:<br/>#Finding the transpose of input_features:<br/>inputs = input_features.T<br/>deriv_final = np.dot(inputs,deriv)</span><span id="05ae" class="ol mz jj sc b gy uc tz l ua ub">#Updating the weights values:<br/>weights -= lr * deriv_final #Updating the bias weight value:<br/>for i in deriv:<br/> bias -= lr * i #Check the final values for weight and biasprint (weights)</span><span id="3e8d" class="ol mz jj sc b gy uc tz l ua ub">print (bias) #Taking inputs:<br/>single_point = np.array([1,0]) #1st step:<br/>result1 = np.dot(single_point, weights) + bias #2nd step:<br/>result2 = sigmoid(result1) #Print final result<br/>print(result2) #Taking inputs:<br/>single_point = np.array([1,1]) #1st step:<br/>result1 = np.dot(single_point, weights) + bias #2nd step:<br/>result2 = sigmoid(result1) #Print final result<br/>print(result2) #Taking inputs:<br/>single_point = np.array([0,0]) #1st step:<br/>result1 = np.dot(single_point, weights) + bias #2nd step:<br/>result2 = sigmoid(result1) #Print final result<br/>print(result2)</span></pre><p id="fdf0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">在Google Colab上发布:</strong></p><div class="is it gp gr iu md"><a href="https://colab.research.google.com/drive/16PsMib3ejn_TmV7D339nm6qdik9tWs6P#scrollTo=TfbSw80eVNCS&amp;line=6&amp;uniqifier=1" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">谷歌联合实验室</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">神经网络在Python—https://towardsai.net/neural-networks-with-python中的实现</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">colab.research.google.com</p></div></div><div class="mm l"><div class="ud l mo mp mq mm mr ja md"/></div></div></a></div><h1 id="0700" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">为什么我们要加入偏见？</h1><p id="e894" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">假设我们有输入值(0，0)，输入节点和权重的乘积之和总是为零。在这种情况下，无论我们如何训练我们的模型，输出将始终为零。为了解决这个问题并做出可靠的预测，我们使用了偏差项。简而言之，我们可以说，偏置项是构成一个健壮的神经网络所必需的。</p><p id="94d6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，<strong class="lj jt">bias的值如何影响我们sigmoid函数的形状？</strong>我们用一些例子来形象化一下。</p><p id="a6e8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了改变s形曲线的陡度，我们可以相应地调整权重。</p><p id="e6cb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ue"><img src="../Images/1e65086e978f0413d22b1ed409774d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/0*EFdGtDwm_nQjqD3a.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图89: Python代码类似于我们实现的没有偏差值的神经网络</figcaption></figure><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi uf"><img src="../Images/cd3a9ee85a881aed09d9863aecda3418.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/0*FbMhTbvENQrGf7VT.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图90:我们神经网络的数据可视化</figcaption></figure><p id="548b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从输出中，我们可以很快注意到，对于负值，sigmoid函数的输出将≤0.5。此外，对于正值，输出将大于0.5。</p><p id="ad4d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从图中(红色曲线)，你可以看到，如果我们减少权重的值，它会减少陡度的值，如果我们增加权重的值(绿色曲线)，它会增加陡度的值。然而，对于所有三条曲线，如果输入为负，输出将始终≤0.5。对于正数，输出总是大于0.5。</p><h2 id="7bd9" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">如果我们想改变这种模式呢？</strong></h2><p id="f777" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">对于这种情况，我们使用偏差值。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ug"><img src="../Images/a6313468da379eb14866b37a502d4b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/0*AlbUhpLEymw1vrcS.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图91:</figcaption></figure><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi uh"><img src="../Images/a634f59e5907399eeb822e8dc1e53318.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/0*TZQQ3LjbhOnFqzwS.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图92:</figcaption></figure><p id="5047" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从输出中，我们可以注意到，我们可以移动x轴上的曲线，这有助于我们改变上一个示例中显示的模式。</p><h2 id="2d46" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">总结:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ui"><img src="../Images/7ddc461a8667efd927b4e4fd45442edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/0*tmA_YYt1fqGW346c.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图93:模式变化的移位曲线阈值总结</figcaption></figure><p id="e76b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在神经网络中:</p><ul class=""><li id="bcca" class="ow ox jj lj b lk ll ln lo lq ps lu pt ly pu mc pv pc pd pe bi translated">我们可以将偏差视为激活的阈值。</li><li id="e614" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pv pc pd pe bi translated">偏差增加了模型的灵活性</li><li id="3659" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pv pc pd pe bi translated">偏差值允许我们向右或向左移动激活功能。</li><li id="7353" class="ow ox jj lj b lk pf ln pg lq ph lu pi ly pj mc pv pc pd pe bi translated">当输入全为零(0，0)时，偏置值最有用。</li></ul><p id="0899" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们试着用之前看到的同一个例子来理解它。然而，这里我们不打算添加偏差值。模型训练完成后，我们将尝试预测(0，0)的值。理想情况下，应该接近于零。现在让我们看看下面的例子。</p><h1 id="57b2" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">没有偏置值的实现:</h1><h2 id="64ba" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> a .导入所需库:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi tg"><img src="../Images/1ae16b24647613cdc91a82a8cf8c669f.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/0*wyxE_C8G4YTkkyaP.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图94:用Python导入NumPy</figcaption></figure><h2 id="9e71" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> b .输入特征:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi uj"><img src="../Images/680b2d73bfb52b9066f08a0f52f7b2f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/0*5vpdlH81-vfr6S71.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图95:用Python定义我们的输入特征</figcaption></figure><h2 id="92a3" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> c .目标产量:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi uk"><img src="../Images/46d17a8737e51ea5b77c1b1658a6bb8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/0*TOqIiOyaEV5H0CQG.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图96:定义我们的目标输出，并将我们的目标输出整形为一个向量</figcaption></figure><h2 id="34fa" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> d .定义输入权重:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ul"><img src="../Images/054442fa268819b9f145b4386582c6a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/0*NQd_nn69uJDH3ldb.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图97:定义我们的输入权重</figcaption></figure><h2 id="9f83" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> e .定义学习率:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi um"><img src="../Images/4918b8eb3edad1a1186893da05814ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/0*LBddxkD_U7JOyVMY.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图98:定义我们的神经网络的学习率</figcaption></figure><h2 id="c5ed" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> f .激活功能:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi un"><img src="../Images/6173a222e5825dc626cb5ca1dcb96f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/0*-y_Lr23gtCS1DGlr.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图99:定义我们的sigmoid函数</figcaption></figure><h2 id="9153" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">g . sigmoid函数的导数:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rg"><img src="../Images/713f9565f60f235ed9b3a7382a47f963.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*GL1goMGYU83UwI4J.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图100:对我们的sigmoid函数进行求导</figcaption></figure><h2 id="c899" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> h .训练我们模型的主要逻辑:</strong></h2><p id="fbbe" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">请注意，我们不会在任何地方使用偏差值。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi uo"><img src="../Images/2aadc2df44288d483154129ac5de621a.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/0*Vaf4x7Kf9HokYcFI.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图101:注意我们不会在神经网络实现中使用偏见</figcaption></figure><h2 id="08e5" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">一、预测:</strong></h2><p id="16c4" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated"><strong class="lj jt">(1，0)的预测:</strong></p><p id="fdbb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标产量= 1</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi up"><img src="../Images/03196dce46bd7d2cf36d68641a7b102a.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*A4cOUzPQ9r_ut4t5.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图102:处理目标产量= 1的近似预测</figcaption></figure><p id="9dd5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从预测的输出我们可以看到它接近1。</p><p id="b911" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">(0，0)的预测:</strong></p><p id="623e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标输出= 0</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi uq"><img src="../Images/1059533f7cf84f1347fcc01af1e0b4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/0*UStRMzBoG6fuazYo.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图103:处理目标产量= 0的近似预测</figcaption></figure><p id="26a8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里我们可以看到它离0很远。所以我们可以说我们的模型没能预测到它。这就是添加偏差值的原因。</p><p id="1660" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">(1，1)的内部控制预测:</strong></p><p id="e764" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标产量= 1</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi up"><img src="../Images/729502fb1943d7b70c13c97494050c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*P9zvdPV7thc2zR29.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图104:处理目标产量= 1的近似预测</figcaption></figure><p id="7a86" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以看到它接近1。</p><h1 id="d4c7" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">将所有这些放在一起:</h1><pre class="mu mv mw mx gt tu sc tv tw aw tx bi"><span id="1f0a" class="ol mz jj sc b gy ty tz l ua ub"># Import required libraries :<br/>import numpy as np# Define input features :<br/>input_features = np.array([[0,0],[0,1],[1,0],[1,1]])<br/>print (input_features.shape)<br/>print (input_features)# Define target output :<br/>target_output = np.array([[0,1,1,1]])# Reshaping our target output into vector :<br/>target_output = target_output.reshape(4,1)<br/>print(target_output.shape)<br/>print (target_output)# Define weights :<br/>weights = np.array([[0.1],[0.2]])<br/>print(weights.shape)<br/>print (weights)# Define learning rate :<br/>lr = 0.05# Sigmoid function :<br/>def sigmoid(x):<br/> return 1/(1+np.exp(-x))# Derivative of sigmoid function :<br/>def sigmoid_der(x):<br/> return sigmoid(x)*(1-sigmoid(x))# Main logic for neural network :<br/># Running our code 10000 times :for epoch in range(10000):<br/> inputs = input_features#Feedforward input :<br/> pred_in = np.dot(inputs, weights)#Feedforward output :<br/> pred_out = sigmoid(pred_in)#Backpropogation <br/> #Calculating error<br/> error = pred_out — target_output<br/> x = error.sum()<br/> <br/> #Going with the formula :<br/> print(x)<br/> <br/> #Calculating derivative :<br/> dcost_dpred = error<br/> dpred_dz = sigmoid_der(pred_out)<br/> <br/> #Multiplying individual derivatives :<br/> z_delta = dcost_dpred * dpred_dz#Multiplying with the 3rd individual derivative :<br/> inputs = input_features.T<br/> weights -= lr * np.dot(inputs, z_delta)<br/> <br/> <br/>#Taking inputs :<br/>single_point = np.array([1,0])#1st step :<br/>result1 = np.dot(single_point, weights)#2nd step :<br/>result2 = sigmoid(result1)#Print final result<br/>print(result2)#Taking inputs :<br/>single_point = np.array([0,0])#1st step :<br/>result1 = np.dot(single_point, weights)#2nd step :<br/>result2 = sigmoid(result1)#Print final result<br/>print(result2)#Taking inputs :<br/>single_point = np.array([1,1])#1st step :<br/>result1 = np.dot(single_point, weights)#2nd step :<br/>result2 = sigmoid(result1)#Print final result<br/>print(result2)</span></pre></div><div class="ab cl sj sk hx sl" role="separator"><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so"/></div><div class="im in io ip iq"><p id="976a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">在Google Colab上发布:</strong></p><div class="is it gp gr iu md"><a href="https://colab.research.google.com/drive/16PsMib3ejn_TmV7D339nm6qdik9tWs6P#scrollTo=YrRCooGFcb0f&amp;line=51&amp;uniqifier=1" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">谷歌联合实验室</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">神经网络在Python中的实现无偏向值—https://towardsai.net/neural-networks-with-python</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">colab.research.google.com</p></div></div><div class="mm l"><div class="ur l mo mp mq mm mr ja md"/></div></div></a></div></div><div class="ab cl sj sk hx sl" role="separator"><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so"/></div><div class="im in io ip iq"><blockquote class="nv"><p id="af9e" class="nw nx jj bd ny nz oa ob oc od oe mc dk translated">📚查看<a class="ae jg" href="https://towardsai.net/machine-learning-algorithms" rel="noopener ugc nofollow" target="_blank">机器学习算法的概述</a>，为初学者提供Python代码示例📚</p></blockquote><h1 id="a748" class="my mz jj bd na nb nc nd ne nf ng nh ni ky pp kz nk lb pq lc nm le pr lf no np bi translated">案例研究:用Python语言的神经网络预测病毒收缩</h1><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi us"><img src="../Images/1ec8aff106a52436289c22632f61781a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/0*I2nC-S1adZyKw6ne.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图105:感知器</figcaption></figure><h2 id="c7e7" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">数据集:</h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ut"><img src="../Images/421081089bbca3b7c3ad02e2689f2087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/0*UO53b57-u8yCAKlR.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图106:我们案例研究的数据集</figcaption></figure><p id="6c2e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于这个例子，我们的目标是根据给定的输入特征预测一个人是否对病毒呈阳性。这里1代表“是”，0代表“否”。</p><blockquote class="nv"><p id="3dc5" class="nw nx jj bd ny nz oa ob oc od oe mc dk translated">让我们编码:</p></blockquote><h2 id="1c8d" class="ol mz jj bd na om uu dn ne oo uv dp ni lq uw or nk lu ux ot nm ly uy ov no jp bi translated"><strong class="ak"> a .导入所需的库:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi uz"><img src="../Images/13704c0aee9c969a187f310322aaf454.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/0*FqfzXNeS4JlYG4DK.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图107:用Python导入NumPy</figcaption></figure><p id="3da6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">来源:图片由作者创作。</p><h2 id="8a1d" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> b .输入特性:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi rx"><img src="../Images/d7328483f4247f61bcd4cf90c17f2f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/0*ZpS9KPGlcmnj0Cz0.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图108:定义我们的输入特征</figcaption></figure><h2 id="5c67" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> c .目标输出:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi va"><img src="../Images/a162ec1197d0eb829767cade9e4e5ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/0*3X42uX9kfN_T1m9S.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图108:定义我们的目标输出</figcaption></figure><h2 id="d66c" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> d .定义权重:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi vb"><img src="../Images/9d3a05af9a6412cd39828c2e8ae1b724.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/0*Nk4IKY2wxyaFZLOW.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图109:定义我们的权重</figcaption></figure><h2 id="fc01" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> e .偏差值和学习率:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi vc"><img src="../Images/8e0d545eca3365fe678168e48df67d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/0*y2Av5-BqPiXxS7Ot.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图110:定义我们的偏差值和学习率</figcaption></figure><h2 id="514d" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">乙状结肠功能:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi vd"><img src="../Images/32ab89cc773b65cb48e7dd5f1f03d1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/0*M8V2o1y2YKT0sxTs.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图111:应用sigmoid函数</figcaption></figure><h2 id="f412" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">g . sigmoid函数的导数:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi ve"><img src="../Images/cf1bb998df0ed3503e6953571317c359.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/0*IB0tuH6IRVYw4XBL.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图112:和以前一样，我们导出了sigmoid函数</figcaption></figure><h2 id="7b5f" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> h .培训模型的主要逻辑:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi vf"><img src="../Images/e28bfd59a0b9a1e10d890fbf7726a970.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/0*mX0cD624VfEqfpeO.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图113:我们案例研究的训练模型的主要逻辑</figcaption></figure><h2 id="87d4" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak">一、预测:</strong></h2><p id="7372" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated"><strong class="lj jt">例如，受检者呈病毒阳性。</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi vg"><img src="../Images/654e91e041ff8866b770bb351effd20b.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/0*6DT1fXguXnsQc3gf.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图114:预测受试者对病毒呈阳性的大致时间</figcaption></figure><p id="f25f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> i.b .受检者病毒检测呈阴性。</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/2832564bd0a29eb8063ff860c5e840ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/0*RTttZW732spQN3YF.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图115:预测受试者对病毒呈阴性的大致时间</figcaption></figure><p id="8da1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> i.c .被检测者呈病毒阳性。</strong></p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi vh"><img src="../Images/118dbf66ca1d636de3ebd584755a3862.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/0*E1Omy0rnmw8S9KnD.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图116:预测受试者对病毒呈阳性的大致时间</figcaption></figure><h2 id="34df" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated"><strong class="ak"> j .最终重量和偏差值:</strong></h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi vi"><img src="../Images/ee88b1482a78d1fe064161275fec1136.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/0*JSaTJhvPwbTG3knr.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图117:检查我们的最终权重和偏差值</figcaption></figure><p id="a0c0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本例中，我们可以注意到输入特征“嗅觉丧失”对输出的影响最大。如果这是真的，那么在大多数情况下，这个人的病毒检测呈阳性。我们也可以从重量值中得出这个结论。请记住，权重值越高，对输出的影响越大。输入特征“体重减轻”对输出没有太大影响，因此我们可以在对更大的数据集进行预测时将其排除。</p><h1 id="623a" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">将所有这些放在一起:</h1><pre class="mu mv mw mx gt tu sc tv tw aw tx bi"><span id="f19e" class="ol mz jj sc b gy ty tz l ua ub"># Import required libraries :<br/>import numpy as np# Define input features :<br/>input_features = np.array([[1,0,0,1],[1,0,0,0],[0,0,1,1],<br/> [0,1,0,0],[1,1,0,0],[0,0,1,1],<br/> [0,0,0,1],[0,0,1,0]])<br/>print (input_features.shape)<br/>print (input_features)# Define target output :<br/>target_output = np.array([[1,1,0,0,1,1,0,0]])# Reshaping our target output into vector :<br/>target_output = target_output.reshape(8,1)<br/>print(target_output.shape)<br/>print (target_output)# Define weights :<br/>weights = np.array([[0.1],[0.2],[0.3],[0.4]])<br/>print(weights.shape)<br/>print (weights)# Bias weight :<br/>bias = 0.3# Learning Rate :<br/>lr = 0.05# Sigmoid function :<br/>def sigmoid(x):<br/> return 1/(1+np.exp(-x))# Derivative of sigmoid function :<br/>def sigmoid_der(x):<br/> return sigmoid(x)*(1-sigmoid(x))# Main logic for neural network :<br/># Running our code 10000 times :for epoch in range(10000):<br/> inputs = input_features#Feedforward input :<br/> pred_in = np.dot(inputs, weights) + bias#Feedforward output :<br/> pred_out = sigmoid(pred_in)#Backpropogation <br/> #Calculating error<br/> error = pred_out — target_output<br/> <br/> #Going with the formula :<br/> x = error.sum()<br/> print(x)<br/> <br/> #Calculating derivative :<br/> dcost_dpred = error<br/> dpred_dz = sigmoid_der(pred_out)<br/> <br/> #Multiplying individual derivatives :<br/> z_delta = dcost_dpred * dpred_dz#Multiplying with the 3rd individual derivative :<br/> inputs = input_features.T<br/> weights -= lr * np.dot(inputs, z_delta)#Updating the bias weight value :<br/> for i in z_delta:<br/>  bias -= lr * i#Printing final weights: </span><span id="55e5" class="ol mz jj sc b gy uc tz l ua ub">print (weights)<br/>print (“\n\n”)<br/>print (bias)#Taking inputs :<br/>single_point = np.array([1,0,0,1])#1st step :<br/>result1 = np.dot(single_point, weights) + bias#2nd step :<br/>result2 = sigmoid(result1)#Print final result<br/>print(result2)#Taking inputs :<br/>single_point = np.array([0,0,1,0])#1st step :<br/>result1 = np.dot(single_point, weights) + bias#2nd step :<br/>result2 = sigmoid(result1)#Print final result<br/>print(result2)#Taking inputs :<br/>single_point = np.array([1,0,1,0])#1st step :<br/>result1 = np.dot(single_point, weights) + bias#2nd step :<br/>result2 = sigmoid(result1)#Print final result<br/>print(result2)</span></pre><p id="f45d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在Google Colab上发布:</p><div class="is it gp gr iu md"><a href="https://colab.research.google.com/drive/16PsMib3ejn_TmV7D339nm6qdik9tWs6P#scrollTo=ES5UHf2ufWXc&amp;line=8&amp;uniqifier=1" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">谷歌联合实验室</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">病毒检测预测神经网络—https://towardsai.net/neural-networks-with-python</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">colab.research.google.com</p></div></div><div class="mm l"><div class="vj l mo mp mq mm mr ja md"/></div></div></a></div></div><div class="ab cl sj sk hx sl" role="separator"><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so"/></div><div class="im in io ip iq"><p id="c030" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在上面的例子中，我们没有使用任何隐藏层进行计算。注意，在上面的例子中，我们的数据是线性可分的。例如:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi qw"><img src="../Images/236a42478fa150595fa1f0507ca5487b.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/0*Mxl5DICHQffh4Qet.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图118:显示可线性分离的输入值的数据集</figcaption></figure><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi vk"><img src="../Images/2c190a13a15e0350a6103d334964771d.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/0*Jti_QKi670rPEeq9.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图119:线性可分数据的可视化</figcaption></figure><p id="bf52" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以看到红线可以把黄点(值= 1)和绿点(值= 0)分开。</p><h2 id="f644" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">感知器模型的局限性(没有隐藏层):</h2><p id="4ea0" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">1.单层感知器无法对非线性可分离数据点进行分类。</p><p id="c0db" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">2.涉及许多参数的复杂问题无法用单层感知器解决。</p><p id="cc9a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，在一些情况下，数据不是线性可分的。在这种情况下，我们的感知器模型(没有隐藏层)无法做出准确的预测。为了做出准确的预测，我们需要添加一个或多个隐藏层。<br/>非线性可分数据的可视化表示:</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div class="gh gi vl"><img src="../Images/b9bd2a9130ad90fa4cd18e6cd02616f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/0*vZ-zCjannuJrkZCl.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图120:非线性可分离数据的可视化表示</figcaption></figure></div><div class="ab cl sj sk hx sl" role="separator"><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so"/></div><div class="im in io ip iq"><figure class="mu mv mw mx gt iv gh gi paragraph-image"><a href="https://www.buymeacoffee.com/pratu"><div class="gh gi vm"><img src="../Images/2093d0f16d94a8942508624035f676b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QCQqlZr6doDP-cszzpaSpw.png"/></div></a><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">给普拉蒂克买杯咖啡！</figcaption></figure><p id="00b8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">免责声明:</strong>本文表达的观点仅代表作者个人观点，不代表卡内基梅隆大学或其他(直接或间接)与作者相关的公司的观点。这些文章并不打算成为最终产品，而是当前思想的反映，同时也是讨论和改进的催化剂。</p><p id="b415" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过<a class="ae jg" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向艾</a>发布</p></div><div class="ab cl sj sk hx sl" role="separator"><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so"/></div><div class="im in io ip iq"><h1 id="79a7" class="my mz jj bd na nb tp nd ne nf tq nh ni ky tr kz nk lb ts lc nm le tt lf no np bi translated">推荐文章</h1><p id="27e7" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">一、<a class="ae jg" href="https://towardsai.net/p/machine-learning/best-datasets-for-machine-learning-and-data-science-d80e9f030279" rel="noopener ugc nofollow" target="_blank">机器学习和数据科学最佳数据集</a> <br/>二。<a class="ae jg" href="http://towardsai.net/ai-salaries" rel="noopener ugc nofollow" target="_blank">艾薪资冲天</a>三世<br/>。<a class="ae jg" href="https://towardsai.net/p/machine-learning/what-is-machine-learning-ml-b58162f97ec7" rel="noopener ugc nofollow" target="_blank">什么是机器学习？</a> <br/>四世。<a class="ae jg" href="https://towardsai.net/ml-masters" rel="noopener ugc nofollow" target="_blank">2020年最佳机器学习硕士项目</a> <br/>五、<a class="ae jg" href="https://towardsai.net/ml-phd" rel="noopener ugc nofollow" target="_blank">2020年最佳机器学习博士项目</a> <br/>六、<a class="ae jg" href="https://towardsai.net/p/machine-learning/best-machine-learning-blogs-6730ea2df3bd" rel="noopener ugc nofollow" target="_blank">最佳机器学习博客</a> <br/>七。<a class="ae jg" href="https://towardsai.net/p/machine-learning/key-machine-learning-ml-definitions-43e837ec6add" rel="noopener ugc nofollow" target="_blank">关键机器学习定义</a> <br/>八。<a class="ae jg" href="https://towardsai.net/ml-captcha" rel="noopener ugc nofollow" target="_blank">用机器学习在0.05秒内破解验证码</a> <br/>九。<a class="ae jg" href="https://towardsai.net/p/machine-learning/machine-learning-vs-ai-important-differences-between-them/robiriondo/3432/" rel="noopener ugc nofollow" target="_blank">机器学习vs. AI及其重要区别</a> <br/>十.<a class="ae jg" href="https://towardsai.net/p/machine-learning/moocs-vs-academia-ensuring-success-starting-in-a-machine-learning-ml-career-304b2e42315e" rel="noopener ugc nofollow" target="_blank">确保成功开创机器学习事业(ML) </a> <br/> XI。<a class="ae jg" href="https://towardsai.net/p/machine-learning/machine-learning-algorithms-for-beginners-with-python-code-examples-ml-19c6afd60daa" rel="noopener ugc nofollow" target="_blank">机器学习算法初学者</a> <br/>十二。<a class="ae jg" href="https://towardsai.net/neural-networks-with-python" rel="noopener ugc nofollow" target="_blank">神经网络从零开始详细用Python代码和数学</a> <br/> XIII。<a class="ae jg" href="https://towardsai.net/p/machine-learning/building-neural-networks-with-python-code-and-math-in-detail-ii-bbe8accbf3d1" rel="noopener ugc nofollow" target="_blank">用Python构建神经网络</a> <br/> XIV。<a class="ae jg" href="https://towardsai.net/p/machine-learning/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e" rel="noopener ugc nofollow" target="_blank">神经网络的主要类型</a> <br/>十五。<a class="ae jg" href="https://towardsai.net/p/machine-learning/monte-carlo-simulation-an-in-depth-tutorial-with-python-bcf6eb7856c8" rel="noopener ugc nofollow" target="_blank">用Python编写的蒙特卡洛模拟教程</a> <br/> XVI。<a class="ae jg" href="https://towardsai.net/p/nlp/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0" rel="noopener ugc nofollow" target="_blank">Python自然语言处理教程</a></p></div><div class="ab cl sj sk hx sl" role="separator"><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so sp"/><span class="sm bw bk sn so"/></div><div class="im in io ip iq"><h1 id="b2ac" class="my mz jj bd na nb tp nd ne nf tq nh ni ky tr kz nk lb ts lc nm le tt lf no np bi translated">引用</h1><p id="c75f" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">对于学术背景下的归属，请引用该工作为:</p><pre class="mu mv mw mx gt tu sc tv tw aw tx bi"><span id="be51" class="ol mz jj sc b gy ty tz l ua ub">Shukla, et al., “Neural Networks from Scratch with Python Code and Math in Detail — I”, Towards AI, 2020</span></pre><h1 id="c129" class="my mz jj bd na nb nc nd ne nf ng nh ni ky nj kz nk lb nl lc nm le nn lf no np bi translated">BibTex引文:</h1><pre class="mu mv mw mx gt tu sc tv tw aw tx bi"><span id="0433" class="ol mz jj sc b gy ty tz l ua ub">@article{pratik_iriondo_2020, <br/> title={Neural Networks from Scratch with Python Code and Math in Detail — I}, <br/> url={<a class="ae jg" href="https://towardsai.net/neural-networks-with-python" rel="noopener ugc nofollow" target="_blank">https://towardsai.net/neural-networks-with-python</a>}, <br/> journal={Towards AI}, <br/> publisher={Towards AI Co.}, <br/> author={Pratik, Shukla and Iriondo, <br/> Roberto},  <br/> year={2020}, <br/> month={Jun}<br/>}</span></pre><h2 id="0523" class="ol mz jj bd na om on dn ne oo op dp ni lq oq or nk lu os ot nm ly ou ov no jp bi translated">参考资料:</h2><p id="a68e" class="pw-post-body-paragraph lh li jj lj b lk nq kt lm ln nr kw lp lq ns ls lt lu nt lw lx ly nu ma mb mc im bi translated">[1]生物神经元模型，维基百科，<a class="ae jg" href="https://en.wikipedia.org/wiki/Biological_neuron_model" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Biological_neuron_model</a></p><p id="fbcc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]逻辑门，维基百科，【https://en.wikipedia.org/wiki/Logic_gate T2】</p></div></div>    
</body>
</html>