<html>
<head>
<title>Paper Review: Summarization using Reinforcement Learning From Human Feedback</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文综述:利用人类反馈的强化学习进行摘要</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/paper-review-summarization-using-reinforcement-learning-from-human-feedback-e000a66404ff?source=collection_archive---------1-----------------------#2022-12-21">https://pub.towardsai.net/paper-review-summarization-using-reinforcement-learning-from-human-feedback-e000a66404ff?source=collection_archive---------1-----------------------#2022-12-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c556" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">人工智能校准、从人类反馈中强化学习、近似策略优化(PPO)</h2></div><h1 id="e286" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="d687" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">OpenAI的ChatGPT是镇上新的酷人工智能，并席卷了世界。我们都见过无数的Twitter帖子，媒体文章等等。，强调了ChatGPT的不同用法<a class="ae lt" href="https://cookup.ai/chatgpt/usecases/" rel="noopener ugc nofollow" target="_blank"/>。一些开发者已经开始构建<a class="ae lt" href="https://twitter.com/jordnb/status/1599880721077633024" rel="noopener ugc nofollow" target="_blank">应用</a>、插件、服务等。，利用ChatGPT。</p><p id="de24" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">虽然ChatGPT的确切工作方式尚不清楚，因为OpenAI尚未发布论文或开源他们的代码。我们确实知道他们利用了来自人类反馈的<strong class="kz ir">强化学习(RLHF) </strong>的想法</p><blockquote class="lz ma mb"><p id="d718" class="kx ky mc kz b la lu jr lc ld lv ju lf md lw li lj me lx lm ln mf ly lq lr ls ij bi translated">我们使用来自人类反馈的强化学习(RLHF)来训练这个模型，使用与InstructGPT相同的方法，但在数据收集设置上略有不同。</p></blockquote><p id="ee3b" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">在今天的文章中，我们将深入到OpenAI之前的<a class="ae lt" href="https://arxiv.org/pdf/2009.01325.pdf" rel="noopener ugc nofollow" target="_blank">工作</a>中，他们使用RLHF来学习总结文档。本文将由以下几个部分组成。</p><ul class=""><li id="f301" class="mg mh iq kz b la lu ld lv lg mi lk mj lo mk ls ml mm mn mo bi translated"><strong class="kz ir">数据集创建</strong></li><li id="eb14" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated"><strong class="kz ir">监督微调</strong></li><li id="70df" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated"><strong class="kz ir">培训奖励模型</strong></li><li id="3cef" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated"><strong class="kz ir">强化学习</strong></li><li id="8dd4" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated"><strong class="kz ir">局部策略优化</strong></li></ul><h1 id="739f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">什么是数据集？</h1><p id="66eb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">OpenAI通过从Reddit <a class="ae lt" href="https://webis.de/data/webis-tldr-17.html" rel="noopener ugc nofollow" target="_blank"> TL采样来创建数据集；DR </a>数据集。此数据集由后接TL的帖子组成；帖子的创建者写的博士。TL；DR被当作原帖的总结。</p><p id="d43a" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">原TL；灾难恢复数据集包含300万篇帖子。然而，OpenAI运行了一些质量过滤器，还过滤了长度在24到48个令牌之间的摘要。长度过滤器的原因是限制摘要的长度对确定其质量的影响。帖子过滤，他们的数据集包含123，169个帖子，其中5%用作验证集。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ef9b914e7435497976b58a6c34dc9554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*fNomM4H07Jj2Wcp6J1tr3g.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">表格来自论文:<a class="ae lt" href="https://arxiv.org/pdf/2009.01325.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.01325.pdf</a></figcaption></figure><p id="2e9d" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">在上图中，你可以看到文章的分布和它们所属的子编辑。OpenAI还在这里公开了数据集<a class="ae lt" href="https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/tldr" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="997e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">方法学</h1><p id="c144" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">概括地说，该方法包括以下三个主要步骤:</p><ol class=""><li id="495a" class="mg mh iq kz b la lu ld lv lg mi lk mj lo mk ls ng mm mn mo bi translated"><strong class="kz ir">监督微调(SFT) </strong></li><li id="436c" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ng mm mn mo bi translated"><strong class="kz ir">创建奖励模型</strong></li><li id="f762" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ng mm mn mo bi translated"><strong class="kz ir">训练一个利用奖励模型的强化学习模型。</strong></li></ol><h2 id="4a34" class="nh kg iq bd kh ni nj dn kl nk nl dp kp lg nm nn kr lk no np kt lo nq nr kv ns bi translated"><strong class="ak">监督微调(SFT) </strong></h2><p id="9631" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这个步骤类似于任何预先训练的transformer模型的微调步骤，以便它可以进行特定于任务的预测。<strong class="kz ir">在这种特殊情况下，作者训练模型来预测/生成给定帖子的摘要。</strong></p><p id="d43f" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">他们没有使用现成的预训练变压器模型，<strong class="kz ir">而是复制标准变压器架构，并对其进行预训练，以预测下一个令牌</strong>。他们使用以下语料库进行预培训:</p><ul class=""><li id="7268" class="mg mh iq kz b la lu ld lv lg mi lk mj lo mk ls ml mm mn mo bi translated">普通爬行</li><li id="d806" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated">网络文本</li><li id="2682" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated">书</li><li id="0246" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated">维基百科(一个基于wiki技术的多语言的百科全书协作计划ˌ也是一部用不同语言写成的网络百科全书ˌ 其目标及宗旨是为全人类提供自由的百科全书)ˌ开放性的百科全书</li></ul><p id="403a" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">发送到变压器模型用于预测汇总的输入遵循如下所示的模板:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi nt"><img src="../Images/b09f34fa33f5099899bf688aedead7d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8gzyvPAlDEROutXOp0isng.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">表格来自论文:<a class="ae lt" href="https://arxiv.org/pdf/2009.01325.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.01325.pdf</a></figcaption></figure><h1 id="1f26" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">创建奖励模型</h1><h2 id="8961" class="nh kg iq bd kh ni nj dn kl nk nl dp kp lg nm nn kr lk no np kt lo nq nr kv ns bi translated"><strong class="ak">强化学习的基础</strong></h2><p id="5877" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">强化学习(RL)背后的思想是，一个主体从它与它所活动的环境/世界的交互中学习。代理从RL学习的一个例子是像吃豆人这样的游戏。代理可以选择在特定的方向上移动它所采取的每一个动作。当代理做出某些动作时，它可以从环境接收一些反馈。反馈可以是点数增加、被杀死等。</p><p id="22e2" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">收到的反馈被称为<strong class="kz ir">奖励</strong>，代理可以感知和利用来决定下一步采取什么行动的所有信息被称为<strong class="kz ir">观察状态</strong>。在RL中，代理人<strong class="kz ir">的目标是最大化其可以收到的</strong>的报酬。代理采取的单个动作被称为<strong class="kz ir">步骤、</strong>，导致游戏结束的一系列步骤被称为<strong class="kz ir">情节</strong>。</p><p id="c603" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">在摘要的上下文中，观察到的状态是帖子的文本内容，代理可以采取的操作是逐个(按步骤)输出一个令牌，并为每集生成一个摘要。然而，我们需要找到一种在一集结束时奖励代理人的方法。奖励必须是一个标量值。在RLHF中，我们训练一个奖励模型，它判断我们代理的输出，并提供一个分数作为奖励。</p><h2 id="7c13" class="nh kg iq bd kh ni nj dn kl nk nl dp kp lg nm nn kr lk no np kt lo nq nr kv ns bi translated">为奖励模型收集数据</h2><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/3c3ffef4b5a51a22dba69a66ae667a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*bv8xeSJUmAcPyCDr3BA7dg.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">图片来自论文:<a class="ae lt" href="https://arxiv.org/pdf/2009.01325.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.01325.pdf</a></figcaption></figure><p id="a8ef" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">为了训练一个奖励模型，作者从他们的数据集中抽取一个帖子，然后获得一串摘要。他们获取给定帖子摘要的一些方法有:</p><ul class=""><li id="a112" class="mg mh iq kz b la lu ld lv lg mi lk mj lo mk ls ml mm mn mo bi translated">SFT模型的样本</li><li id="e256" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated">通过传递一些上下文中的摘要示例，从没有SFT的预训练模型中抽取样本。</li><li id="f345" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated">使用帖子创建者写的地面真相总结。</li></ul><p id="02eb" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">一个新的任务是这样创建的:为一个给定的帖子取成对的摘要，并要求签约的贴标签者在这两个摘要中选择他们更喜欢的一个。</p><h2 id="fd14" class="nh kg iq bd kh ni nj dn kl nk nl dp kp lg nm nn kr lk no np kt lo nq nr kv ns bi translated"><strong class="ak">培训奖励模型</strong></h2><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/88f5760bc504450f67a7b00511be16c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*xKZI1T6zaeHnJ03KNaABsw.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">图片来自论文:【https://arxiv.org/pdf/2009.01325.pdf T2】</figcaption></figure><p id="1d66" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">为了获得奖励模型，<strong class="kz ir">从SFT得到的模型被用作起点</strong>，并且用于预测代币的层<strong class="kz ir">被替换为新的头部</strong>(线性层)以获得发送到奖励模型的post +摘要文档的标量分数。与上面创建的数据集保持一致，给定帖子的两个<strong class="kz ir">候选摘要都被单独传递给奖励模型，以获得每个摘要的分数。</strong></p><p id="2b2e" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">该模型被训练成最大化优选概要和其他候选概要之间的分数差<strong class="kz ir">，或者换句话说，最小化分数的负差。这可以在下面所示的损失函数中观察到，其中<strong class="kz ir"> <em class="mc"> y_i </em> </strong>对应于<strong class="kz ir">优选人类</strong>摘要，<strong class="kz ir"><em class="mc">y _(1-I)</em></strong><em class="mc"/>对应于其他候选摘要<em class="mc"> </em>，<strong class="kz ir"> <em class="mc"> x </em> </strong>对应于帖子，<strong class="kz ir"> <em class="mc"> r </em> </strong>对应于我们的奖励模型</strong></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi oa"><img src="../Images/13b476ba3446c0c62c63a213565762e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T__USUa_TevbhMJBICLu8A.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">图片来自论文:<a class="ae lt" href="https://arxiv.org/pdf/2009.01325.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.01325.pdf</a></figcaption></figure><p id="cfb9" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">很好，现在我们有了一个奖励模型，好的总结应该得到高分，坏的总结应该得到低分<strong class="kz ir"/>！下一步是在RL环境中利用奖励模型。</p><h1 id="e2bd" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">使用近似策略优化的强化学习</h1><h2 id="65ad" class="nh kg iq bd kh ni nj dn kl nk nl dp kp lg nm nn kr lk no np kt lo nq nr kv ns bi translated">最近策略优化的基础</h2><p id="5e9f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">R1中的一个<strong class="kz ir">策略</strong>决定了给定一个状态<strong class="kz ir"> ( <em class="mc"> s </em> ) </strong>，代理应该采取什么动作<strong class="kz ir"> ( <em class="mc"> a </em> ) </strong>。一个策略通常用符号<strong class="kz ir"> <em class="mc"> π </em> </strong> <em class="mc">来表示。</em>在本文的上下文中，我们的<strong class="kz ir">初始策略是生成摘要的转换器模型</strong>。</p><p id="781a" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">在RL，我们试图学习最好的政策，以便我们的代理人可以获得最高的回报。我们通过在每集结束时更新我们的总结模型的权重来更新我们的策略。本文上下文中的一个事件是我们的策略为其生成摘要的一批帖子。<strong class="kz ir"> PPO是一种专注于如何更新与我们的策略相对应的参数的算法。</strong>策略模型的损失可由以下三个等式表示:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi ob"><img src="../Images/0f998e7f93d411c13d3c3aab6c681207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9r793P6kJm1R3FJGEvXo7A.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">本文作者图片</figcaption></figure><p id="c369" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">基于等式1，我们想要找到基于我们当前策略对给定状态<em class="mc"> s </em>采取<strong class="kz ir">某个动作<em class="mc"> a </em>的概率与根据<strong class="kz ir">我们先前/旧策略</strong>对同一状态<em class="mc"> s </em> </strong>采取相同动作<em class="mc"> a </em>的概率之间的比率。</p><p id="99ab" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">最近策略优化(PPO) <strong class="kz ir">背后的想法是，我们希望避免在一个步骤中彻底改变策略</strong>。过于彻底地改变策略的危险在于，代理可能会忘记它到目前为止所学习的内容，并最终进入一个性能非常低的空间，并且重新学习它所忘记的内容可能会花费很多时间。从等式1计算出的比率让我们了解了政策变化的幅度。</p><p id="e593" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">等式2中描绘的优势函数是在状态<strong class="kz ir"><em class="mc"/></strong>下采取行动<strong class="kz ir"><em class="mc"/></strong>比在该状态下预期的平均回报好或差多少的度量。本质上，这是一种判断我们是否在采取好的行动的方式。如果你对Q值和价值函数不熟悉，我建议你深入学习由Huggingface创建的RL课程，链接在参考资料部分。</p><p id="2b3b" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">在等式3中，clip函数绑定范围1-<em class="mc">ε</em>和1+<em class="mc">ε</em>，<em class="mc">ε</em>之间的比值，通常是范围[0.1–0.2]内的常数值。损失函数<em class="mc"> L_ppo </em>确保梯度为0，当:</p><ol class=""><li id="f6a8" class="mg mh iq kz b la lu ld lv lg mi lk mj lo mk ls ng mm mn mo bi translated">比率&lt; 1-<em class="mc">ε</em>和<em class="mc">优势</em>0</li><li id="8d80" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ng mm mn mo bi translated">比值&gt; 1+<em class="mc">ε</em>和<em class="mc">优势</em>0</li></ol><p id="49c9" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">总结这两种情况，如果我们已经意识到我们正在采取的行动相当好或相当坏，并且在所有其他情况下更新它，我们不想再更新我们的政策。如果你不清楚为什么上述两种情况下梯度为零，请参考<a class="ae lt" href="https://huggingface.co/blog/deep-rl-ppo" rel="noopener ugc nofollow" target="_blank">这篇</a>文章。</p><h2 id="ee38" class="nh kg iq bd kh ni nj dn kl nk nl dp kp lg nm nn kr lk no np kt lo nq nr kv ns bi translated">更新的奖励方程式</h2><p id="8e00" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">该论文的作者还增加了一个KL散度惩罚，以确保更新后的政策不会偏离原始的SFT模型太远。这是为了确保模型不会通过欺骗和产生不连贯的总结来找到最大化奖励函数的方法。它还确保了新策略生成的输出摘要不会与奖励模型所依据的太不相似。</p><p id="9794" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">奖励的最终等式是</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi oc"><img src="../Images/4146cbf8f93e738133d3ff609e07831c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rigo7CNIsBKwTzJBp0Mu1Q.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">图片来自论文:【https://arxiv.org/pdf/2009.01325.pdf T2】</figcaption></figure><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi od"><img src="../Images/541136a18a83b52dcd384ba53eb0a082.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*QpZbhUmcao86c_pNEKKs4g.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">图片来自论文:【https://arxiv.org/pdf/2009.01325.pdf T4】</figcaption></figure><p id="e9a2" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">PPO-Clip算法可以遵循基于<a class="ae lt" href="https://huggingface.co/blog/deep-rl-a2c" rel="noopener ugc nofollow" target="_blank">演员-评论家</a>的训练范例。评论家负责给我们Q值(<em class="mc"> Q(s，a) </em>)，也称为价值函数。价值函数由另一个用与回报模型相同的权重初始化的变换器模型来表示。</p><p id="19c6" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">由于作者没有提到关于价值函数的任何其他信息，我们假设他们通过最小化价值函数的预测回报和回报函数提供的实际回报之间的差异来优化它。</p><p id="c70b" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">总而言之，我们有三种不同的模型:</p><ol class=""><li id="f7ac" class="mg mh iq kz b la lu ld lv lg mi lk mj lo mk ls ng mm mn mo bi translated">策略模型</li><li id="425b" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ng mm mn mo bi translated">奖励模型</li><li id="ce50" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ng mm mn mo bi translated">价值函数模型</li></ol><p id="8b7d" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">这三种模型都基于transformer架构。</p><h1 id="7154" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">结论</strong></h1><p id="18d6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">论文作者根据实验得出以下结论:</p><ol class=""><li id="b8d2" class="mg mh iq kz b la lu ld lv lg mi lk mj lo mk ls ng mm mn mo bi translated"><em class="mc">“使用人类反馈的训练明显优于非常强的英语摘要基线。”</em>:这是基于贴标机运行的评估。</li><li id="c9f9" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ng mm mn mo bi translated"><em class="mc">“人类反馈模型比监督模型更好地推广到新领域”</em>:这是基于在TL上训练的模型的性能；DR数据集对照<a class="ae lt" href="https://huggingface.co/datasets/cnn_dailymail" rel="noopener ugc nofollow" target="_blank"> CNN/DailyMail </a>数据集。</li></ol><p id="83d4" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">在今天的文章中，我们讨论了如何在NLP环境中使用来自人类反馈的强化学习来总结文档。如果您对本文有任何疑问或想法，请留言。</p><h2 id="e8c1" class="nh kg iq bd kh ni nj dn kl nk nl dp kp lg nm nn kr lk no np kt lo nq nr kv ns bi translated">参考</h2><ul class=""><li id="3442" class="mg mh iq kz b la lb ld le lg oe lk of lo og ls ml mm mn mo bi translated">Huggingface <a class="ae lt" href="https://github.com/huggingface/deep-rl-class" rel="noopener ugc nofollow" target="_blank"> DeepRL类</a></li><li id="8cf1" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated">最近的政策优化<a class="ae lt" href="https://huggingface.co/blog/deep-rl-ppo" rel="noopener ugc nofollow" target="_blank"> (PPO) </a></li><li id="487a" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated"><a class="ae lt" href="https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6" rel="noopener" target="_blank">https://towards data science . com/proximity-policy-optimization-tutorial-part-1-actor-critic-method-d 53 f 9 affffb f 6</a></li><li id="d76d" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated"><a class="ae lt" href="https://github.com/openai/summarize-from-feedback" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/summarize-from-feedback</a></li><li id="9303" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated"><a class="ae lt" href="https://openai.com/blog/learning-to-summarize-with-human-feedback/" rel="noopener ugc nofollow" target="_blank">https://open ai . com/blog/learning-to-summary-with-human-feedback/</a></li><li id="f8e9" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated"><a class="ae lt" href="https://keras.io/examples/rl/ppo_cartpole/" rel="noopener ugc nofollow" target="_blank">https://keras.io/examples/rl/ppo_cartpole/</a></li><li id="e66e" class="mg mh iq kz b la mp ld mq lg mr lk ms lo mt ls ml mm mn mo bi translated"><a class="ae lt" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html" rel="noopener ugc nofollow" target="_blank">https://spinningup.openai.com/en/latest/algorithms/ppo.html</a></li></ul></div></div>    
</body>
</html>