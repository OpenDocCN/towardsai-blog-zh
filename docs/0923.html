<html>
<head>
<title>Sentence Embeddings with sentence-transformers library</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用句子变形库嵌入句子</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/sentence-embeddings-with-sentence-transformers-library-7420fc6e3815?source=collection_archive---------0-----------------------#2020-09-14">https://pub.towardsai.net/sentence-embeddings-with-sentence-transformers-library-7420fc6e3815?source=collection_archive---------0-----------------------#2020-09-14</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="117b" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="9b79" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">使用伯特/罗伯塔/XLM-罗伯塔公司和PyTorch的多语言句子嵌入</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/a755c1a7d85b8ef0c7fedcbb8768b658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*t7NIlLZb39SlBJng"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">照片由<a class="ae li" href="https://www.pexels.com/@ken123films" rel="noopener ugc nofollow" target="_blank">肯德尔·胡普斯</a>在<a class="ae li" href="https://www.pexels.com/photo/silhouette-photography-of-people-2901134/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure></div><div class="ab cl lj lk hy ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="in io ip iq ir"><p id="86f9" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated">当我最近致力于实现语义搜索功能时，我偶然发现了这个简单易用的库。作为其中的一部分，我必须将每个文档的密集向量表示索引到Elasticsearch中，以便语义搜索能够工作。有了这个库，我能够快速有效地实现这个功能。我希望这篇文章对你有所帮助。</p><p id="3da2" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated">这篇文章需要嵌入的知识(单词嵌入或句子嵌入)。你可以参考<a class="ae li" href="https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2" rel="noopener" target="_blank">这篇</a>文章，快速刷新你的记忆。如果你已经了解了嵌入，你可以继续阅读。</p><h1 id="d43d" class="mq mr iu bd ms mt mu mv mw mx my mz na kj nb kk nc km nd kn ne kp nf kq ng nh bi translated">装置</h1><pre class="kt ku kv kw gu ni mp nj nk aw nl bi"><span id="50b5" class="nm mr iu mp b gz nn no l np nq">pip install -U sentence-transformers</span></pre><h1 id="3ee1" class="mq mr iu bd ms mt mu mv mw mx my mz na kj nb kk nc km nd kn ne kp nf kq ng nh bi translated">使用</h1><h2 id="a640" class="nm mr iu bd ms nr ns dn mw nt nu dp na lz nv nw nc md nx ny ne mh nz oa ng ja bi translated">1.句子嵌入</h2><pre class="kt ku kv kw gu ni mp nj nk aw nl bi"><span id="93ed" class="nm mr iu mp b gz nn no l np nq">from sentence_transformers import SentenceTransformer<br/>model = SentenceTransformer('model_name_or_path')</span></pre><p id="48b5" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated">在下面的例子中，我们将一个预先训练好的模型<code class="fe mm mn mo mp b">distilbert-base-nli-stsb-mean-tokens</code>传递给<code class="fe mm mn mo mp b">SentenceTransformer </code>来计算句子嵌入。预训练模型的完整列表见<a class="ae li" href="https://www.sbert.net/docs/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">此处</a>。请注意，没有一个嵌入可以适用于所有的任务，因此我们应该尝试这些模型中的一些，并选择最有效的一个。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ob oc l"/></div></figure><blockquote class="od oe of"><p id="41f3" class="lq lr og ls b lt lu ke lv lw lx kh ly oh ma mb mc oi me mf mg oj mi mj mk ml in bi translated"><strong class="ls je">注意</strong> : <code class="fe mm mn mo mp b">sentence-transformers</code>模型也托管在Huggingface存储库中。所以我们可以直接使用Hugginface的<code class="fe mm mn mo mp b">Transformers</code>库来生成句子嵌入，而不需要安装<code class="fe mm mn mo mp b">sentence-transformers</code>库。这里给出了<a class="ae li" href="https://www.sbert.net/docs/usage/computing_sentence_embeddings.html#sentence-embeddings-with-transformers" rel="noopener ugc nofollow" target="_blank">的示例代码</a>。</p></blockquote><h2 id="531f" class="nm mr iu bd ms nr ns dn mw nt nu dp na lz nv nw nc md nx ny ne mh nz oa ng ja bi translated">2.语义文本相似度</h2><p id="b35c" class="pw-post-body-paragraph lq lr iu ls b lt ok ke lv lw ol kh ly lz om mb mc md on mf mg mh oo mj mk ml in bi translated">现在我们已经了解了如何生成句子嵌入，下一步是比较句子的语义文本相似性，并根据余弦相似性对它们进行排序。</p><p id="4b29" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated">下面列出了句子相似度的推荐模型。这些模型在<a class="ae li" href="https://www.sbert.net/docs/pretrained-models/nli-models.html" rel="noopener ugc nofollow" target="_blank"/>&amp;<a class="ae li" href="https://www.sbert.net/docs/pretrained-models/sts-models.html" rel="noopener ugc nofollow" target="_blank">STS</a>数据上训练，在STSbenchmark数据集上评估。作者推荐型号<code class="fe mm mn mo mp b">distilbert-base-nli-stsb-mean-tokens</code>，因为它在速度和性能之间达到了完美的平衡。</p><p id="d196" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated"><strong class="ls je">Roberta-large-nli-STSb-mean-tokens</strong>—STSb性能:86.39<br/><strong class="ls je">Roberta-base-nli-STSb-mean-tokens</strong>—STSb性能:85.44<br/>—T12】Bert-large-nli-STSb-mean-tokens—STSb性能:85.29<br/><strong class="ls je">distilbert-base-nli-STSb-mean-tokens</strong></p><p id="ebd2" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated">让我们看一个我们在前面的例子中使用的句子之间的余弦相似性的例子:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ob oc l"/></div></figure><p id="a821" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated">该方法使用强力方法来寻找得分最高的对，其具有二次复杂度。对于较长的句子，这种方法不可行。下面讨论的释义挖掘是最佳方法。</p><h2 id="d762" class="nm mr iu bd ms nr ns dn mw nt nu dp na lz nv nw nc md nx ny ne mh nz oa ng ja bi translated">3.释义挖掘</h2><p id="f2ab" class="pw-post-body-paragraph lq lr iu ls b lt ok ke lv lw ol kh ly lz om mb mc md on mf mg mh oo mj mk ml in bi translated">当我们需要处理大量的句子集合(10，000个或更多)时，会使用复述挖掘。在这里可以找到<a class="ae li" href="https://www.sbert.net/docs/usage/paraphrase_mining.html#paraphrase-mining" rel="noopener ugc nofollow" target="_blank">对释义挖掘的更详细的解释。</a></p><p id="d7ec" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated">让我们看一个使用释义挖掘的例子:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ob oc l"/></div></figure><h2 id="d4c4" class="nm mr iu bd ms nr ns dn mw nt nu dp na lz nv nw nc md nx ny ne mh nz oa ng ja bi translated">4.语义搜索</h2><p id="2f72" class="pw-post-body-paragraph lq lr iu ls b lt ok ke lv lw ol kh ly lz om mb mc md on mf mg mh oo mj mk ml in bi translated">传统的搜索引擎是为基于词汇的搜索而设计的，但是使用语义搜索，我们可以基于同义词来查找文档。使用我们上面学到的技术，我们可以实现语义搜索功能。语义搜索试图通过理解搜索查询的内容来提高搜索的准确性。</p><p id="63dc" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated">语义搜索最常用于Elasticsearch等搜索引擎。如果你对Elasticsearch有一个基本的了解，并通过这个<a class="ae li" href="https://www.sbert.net/docs/usage/semantic_search.html#elasticsearch" rel="noopener ugc nofollow" target="_blank">链接</a>了解语义搜索是如何实现的。</p><h1 id="dc4b" class="mq mr iu bd ms mt mu mv mw mx my mz na kj nb kk nc km nd kn ne kp nf kq ng nh bi translated">结论</h1><p id="28da" class="pw-post-body-paragraph lq lr iu ls b lt ok ke lv lw ol kh ly lz om mb mc md on mf mg mh oo mj mk ml in bi translated">希望你已经理解了如何使用<code class="fe mm mn mo mp b">sentence-transformers</code>库来计算句子嵌入，如何获得句子之间的相似度，以及最后我们如何确定句子嵌入来实现语义搜索。</p><p id="48fc" class="pw-post-body-paragraph lq lr iu ls b lt lu ke lv lw lx kh ly lz ma mb mc md me mf mg mh mi mj mk ml in bi translated"><em class="og">阅读更多关于Python和数据科学的此类有趣文章，</em> <a class="ae li" href="https://pythonsimplified.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ls je"> <em class="og">订阅</em> </strong> </a> <em class="og">到我的博客</em><a class="ae li" href="http://www.pythonsimplified.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ls je"><em class="og">【www.pythonsimplified.com】</em></strong></a><strong class="ls je"><em class="og">。</em> </strong>你也可以通过<a class="ae li" href="https://www.linkedin.com/in/chetanambi/" rel="noopener ugc nofollow" target="_blank"> <strong class="ls je"> LinkedIn </strong> </a>联系我。</p></div><div class="ab cl lj lk hy ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="in io ip iq ir"><h1 id="a20f" class="mq mr iu bd ms mt op mv mw mx oq mz na kj or kk nc km os kn ne kp ot kq ng nh bi translated">参考</h1><div class="ou ov gq gs ow ox"><a href="https://github.com/UKPLab/sentence-transformers" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fp"><div class="oz ab pa cl cj pb"><h2 class="bd je gz z fq pc fs ft pd fv fx jd bi translated">uk plab/句子-变形金刚</h2><div class="pe l"><h3 class="bd b gz z fq pc fs ft pd fv fx dk translated">这个框架提供了一个简单的方法来计算句子和段落的密集向量表示(也称为…</h3></div><div class="pf l"><p class="bd b dl z fq pc fs ft pd fv fx dk translated">github.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl lc ox"/></div></div></a></div><div class="ou ov gq gs ow ox"><a href="https://www.sbert.net/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fp"><div class="oz ab pa cl cj pb"><h2 class="bd je gz z fq pc fs ft pd fv fx jd bi translated">句子变压器文件-句子-变压器文件</h2><div class="pe l"><h3 class="bd b gz z fq pc fs ft pd fv fx dk translated">编辑描述</h3></div><div class="pf l"><p class="bd b dl z fq pc fs ft pd fv fx dk translated">www.sbert.net</p></div></div></div></a></div></div></div>    
</body>
</html>