<html>
<head>
<title>Practical Guide to Support Vector Machines In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python支持向量机实用指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/practical-guide-to-support-vector-machines-in-python-dc0e628d50bc?source=collection_archive---------1-----------------------#2022-10-11">https://pub.towardsai.net/practical-guide-to-support-vector-machines-in-python-dc0e628d50bc?source=collection_archive---------1-----------------------#2022-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e5c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ko translated">支持向量机(SVM)是一种强大的机器学习算法，被数据科学家和机器学习实践者广泛使用。原因是它的线性和非线性分类和回归能力。它可以用于不同的数据集，如文本和图像。<strong class="js iu">支持向量机非常适合处理中小型数据集的分类问题。</strong>这其中的主要原因</p><p id="0014" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就是它训练复杂度高，会需要很高的训练时间和精力。因此，它只对小型和中型数据集具有计算效率。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi kx"><img src="../Images/4dd710e7677fbcae2e0a15a8ab2090ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*483rJNORr4QuW7uLtqPgMQ.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">支持向量机</figcaption></figure><h1 id="40bf" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">目录:</h1><ol class=""><li id="57a1" class="ml mm it js b jt mn jx mo kb mp kf mq kj mr kn ms mt mu mv bi translated">线性SVM分类<br/> 1.1。硬利润与软利润分类<br/> 1.2。实践中的线性SVM</li><li id="a726" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">非线性SVM分类<br/> 2.1。多项式内核<br/> 2.2。相似特征</li><li id="b8e7" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">SVM回归</li><li id="bebe" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn ms mt mu mv bi translated">参考</li></ol><p id="1171" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以在GitHub资源库中找到本文中使用的代码:</p><div class="nb nc gp gr nd ne"><a href="https://github.com/youssefHosni/Machine-Learning-Practical-Guide" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">GitHub-youssefHosni/机器学习-实用指南</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">github.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns lh ne"/></div></div></a></div></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><p id="6699" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">如果你想免费学习数据科学和机器学习，看看这些资源:</strong></p><ul class=""><li id="73ea" class="ml mm it js b jt ju jx jy kb oa kf ob kj oc kn od mt mu mv bi translated">免费互动路线图，自学数据科学和机器学习。从这里开始:<a class="ae oe" href="https://aigents.co/learn/roadmaps/intro" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn/roadmaps/intro</a></li><li id="04fc" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn od mt mu mv bi translated">数据科学学习资源搜索引擎(免费)。将你最喜欢的资源加入书签，将文章标记为完整，并添加学习笔记。<a class="ae oe" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></li><li id="0fb9" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn od mt mu mv bi translated">想要在导师和学习社区的支持下从头开始学习数据科学吗？免费加入这个学习圈:<a class="ae oe" href="https://community.aigents.co/spaces/9010170/" rel="noopener ugc nofollow" target="_blank">https://community.aigents.co/spaces/9010170/</a></li></ul><p id="c335" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你想在数据科学&amp;人工智能领域开始职业生涯，但你不知道如何开始。我提供数据科学指导课程和长期职业指导:</p><ul class=""><li id="e675" class="ml mm it js b jt ju jx jy kb oa kf ob kj oc kn od mt mu mv bi translated">长期指导:<a class="ae oe" href="https://lnkd.in/dtdUYBrM" rel="noopener ugc nofollow" target="_blank">https://lnkd.in/dtdUYBrM</a></li><li id="b1f0" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn od mt mu mv bi translated">辅导会议:<a class="ae oe" href="https://lnkd.in/dXeg3KPW" rel="noopener ugc nofollow" target="_blank">https://lnkd.in/dXeg3KPW</a></li></ul><p id="2fa6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="of">加入</em> </strong> <a class="ae oe" href="https://youssefraafat57.medium.com/membership" rel="noopener"> <strong class="js iu"> <em class="of">中等会员</em> </strong> </a> <strong class="js iu"> <em class="of">计划，只需5美元，继续无限制学习。如果你使用下面的链接，我会收到一小部分会员费，不需要你额外付费。</em>T13】</strong></p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="725c" class="og lo it bd lp oh oi dn lt oj ok dp lx kb ol om mb kf on oo mf kj op oq mj or bi translated">1.线性SVM分类</h2><p id="888b" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated">SVM分类器背后的基本思想是在类之间拟合尽可能宽的街道或边界。这就是所谓的<strong class="js iu">大幅度分类</strong>。考虑下图，有两个类，SVM分类器的主要目标是分类数据的决策边界或超平面以及具有最大余量的支持向量。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/5d45a072a763ac67aa0c9c9752d50319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*TxAz4VR3Liecnnjnh5vXQA.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图一。SVM决策界限</figcaption></figure><p id="7a7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">值得一提的是<strong class="js iu">支持向量机分类器对特征的尺度很敏感。</strong>如果特征不是同一尺度，这将导致优化的边距会偏向最大的特征。因此，在训练模型之前对数据进行标准化非常重要。这可以在下图中看到，当要素具有不同的比例时，我们可以看到决策边界和支持向量只对X1要素进行分类，而没有考虑X0要素，但是在将数据缩放到相同的比例后，决策边界和支持向量看起来更好，并且模型考虑了这两个要素。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ow"><img src="../Images/35c95a4f493c6acad47d9edbcd474ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kVMCbxrfMS7yyu2eU0iz5Q.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图二。SVM分类器决策边界和支持向量的非缩放与缩放特征。</figcaption></figure><h2 id="6e50" class="og lo it bd lp oh oi dn lt oj ok dp lx kb ol om mb kf on oo mf kj op oq mj or bi translated"><strong class="ak"> 1.1。硬利润与软利润分类</strong></h2><p id="9580" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated">有两种类型的分类硬分类和软分类余量。硬分类页边距是这样一种页边距，其中所有实例都必须在街道之外和右侧。硬分类余量有两个主要问题:</p><ul class=""><li id="d0f0" class="ml mm it js b jt ju jx jy kb oa kf ob kj oc kn od mt mu mv bi translated">仅对于线性可分的数据:如果数据不是线性可分的，我们将无法找到硬边界。</li><li id="4b87" class="ml mm it js b jt mw jx mx kb my kf mz kj na kn od mt mu mv bi translated">对异常值敏感:如果数据有异常值，这将影响差值，并且很难找到类别之间的硬差值。</li></ul><p id="dcc5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了克服这些问题，我们使用软边界分类，在这种分类中，我们试图在尽可能拥有最宽的街道或边界与限制边界违规的数量(例如在错误的一侧拥有实例)之间进行平衡。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="262e" class="og lo it bd lp oh oi dn lt oj ok dp lx kb ol om mb kf on oo mf kj op oq mj or bi translated">1.2.实践中的线性SVM</h2><p id="f89d" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated">让我们看看如何在实践中使用SVM，为此我们将使用<strong class="js iu"> scikit learn </strong>库。在下面的示例中，我们将首先加载iris数据集，缩放要素，然后训练线性SVM来检测Iris virginica花朵:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="7fc1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面的Scikit-Learn代码缩放这些特征，然后训练一个线性SVM模型(使用C = 1的LinearSVC类和铰链损失函数，简要描述)来检测Iris-Virginica花朵。最终的模型如图3所示。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi oz"><img src="../Images/975ae8a894bd35b36ffa0a97ef075ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dI0DEP_FW_Mhjg-T_spQgw.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图3。缩放后的虹膜数据上的决策边界。</figcaption></figure><p id="e6fd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> LinearSVC </strong>类正则化了偏差项，因此您应该首先通过减去其平均值来集中训练集。如果您使用<strong class="js iu">标准缩放器</strong>缩放数据，这是自动的，如上面的代码所示。此外，确保将<strong class="js iu">损耗</strong>超参数设置为<strong class="js iu">“铰链”</strong>，因为它不是默认值。最后，为了获得更好的性能，应该将dual超参数设置为False，除非特征多于训练实例。</p><p id="8c4c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您也可以使用SVC类，使用<strong class="js iu"> SVC(kernel="linear "，C=1) </strong>，但是它会慢得多，特别是对于大型训练集，所以不推荐使用。另一种选择是使用<strong class="js iu"> SGDClassifier类</strong>，带SGDClassifier(loss="hinge "，alpha=1/(m*C))。这应用规则随机梯度下降来训练线性SVM分类器。<strong class="js iu">它没有LinearSVC类收敛得快，但是它可以用于处理不适合内存的大型数据集。</strong>最后，重要的是要记住，如果你的SVM模型过度拟合，你可以通过减少c来调整它</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="1c73" class="ln lo it bd lp lq pa ls lt lu pb lw lx ly pc ma mb mc pd me mf mg pe mi mj mk bi translated">2.非线性SVM分类</h1><p id="14f3" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated">尽管线性SVM分类器非常高效，并且在许多情况下效果惊人，但许多数据集并不接近线性可分。处理非线性可分离数据集的一种方法是添加更多的要素，例如多项式要素。在某些情况下，这可能会产生线性可分离数据集。考虑图4中左边的图:它表示一个只有一个特征x1的简单数据集。如你所见，这个数据集不是线性可分的。但是如果你添加第二个特征x2 = (x1)2，得到的2D数据集是完全线性可分的。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi pf"><img src="../Images/3fc1503a3b2b904cda39fd121e6b85d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-8AyHT5MDjnpBM0ZSgj7g.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图4。添加要素将使数据集可线性分离。</figcaption></figure><p id="4364" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了使用Scikit-Learn实现这一想法，您可以创建一个包含<strong class="js iu">polynomial features</strong>transformer的管道来创建Iris数据集的多项式特征，然后，它将跟随一个<strong class="js iu"> StandardScaler </strong>和一个<strong class="js iu"> LinearSVC </strong>。让我们在moons数据集上对此进行测试:这是一个用于二进制分类的玩具数据集，其中数据点被成形为两个交错的半圆，如图5所示。以下代码将生成月球数据集，创建多项式要素，对其进行缩放，在其上训练线性SVM，然后绘制决策边界。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/a48c1a62a376df75daf0aaff309b5a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*vbQzDia-imL7VL3UTzPPbA.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图5。使用多项式特征的线性SVM分类器</figcaption></figure></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="0472" class="og lo it bd lp oh oi dn lt oj ok dp lx kb ol om mb kf on oo mf kj op oq mj or bi translated"><strong class="ak"> 2.1。多项式内核</strong></h2><p id="5c89" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated">如前一示例中那样添加多项式要素实现起来很简单，并且可以很好地用于各种机器学习算法(不仅仅是SVM)，但是在低多项式次数下，无法处理非常复杂的数据集，而在高多项式次数下，会创建大量的要素，使得模型太慢。</p><p id="21af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">幸运的是，在使用支持向量机时，您可以应用一种几乎不可思议的数学技术，称为核技巧，稍后将对其进行解释。它可以获得与添加许多多项式特征相同的结果，即使多项式的次数非常高，实际上也不必添加它们。因此，功能的数量不会增加，因为我们实际上没有添加任何功能。这个技巧是由SVC类实现的。让我们在moons数据集上测试它:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="8e68" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此代码使用三次多项式内核训练SVM分类器。图6显示了这一点，它是使用以下函数绘制的:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/1f86d61a6495843c8294ed1e0f329d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*zPKGmg9bVH8b4fdb2RxBZA.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图6。具有次数= 3且系数0 = 1的多项式核的SVM分类器</figcaption></figure><p id="9b43" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图7显示了使用10次多项式内核的另一个SVM分类器的决策边界。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/3e1098ecc89ced8b1c6a998c1ae400d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*-PHbiG4zfCviDsnqm8LgvQ.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图7。具有次数= 10且系数0 = 10的多项式核的SVM分类器。</figcaption></figure><p id="8c5d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">显然，如果您的模型过拟合，您可能希望降低多项式次数。相反，如果它不合适，你可以尝试增加它。超参数<strong class="js iu"> coef0 </strong>控制模型受高次多项式和低次多项式的影响程度。找到正确的超参数值的一个常用方法是使用<strong class="js iu">网格搜索</strong>。首先进行非常粗略的网格搜索，然后围绕找到的最佳值进行更精细的网格搜索，通常会更快。很好地理解每个超参数的实际作用可以帮助您搜索超参数空间的正确部分。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="cfa9" class="og lo it bd lp oh oi dn lt oj ok dp lx kb ol om mb kf on oo mf kj op oq mj or bi translated"><strong class="ak"> 2.2。相似特征</strong></h2><p id="6768" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated">另一种处理非线性问题的技术是添加使用相似性函数计算的特征，该函数测量每个实例与特定地标的相似程度。例如，让我们以前面讨论过的图4所示的一维数据集为例，在x1 =–2和x1 = 1处添加两个界标，如图8(左)所示。</p><p id="4922" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，让我们将相似性函数定义为γ = 0.3的高斯径向基函数(RBF)，如下式所示:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/83000c014342f3e705325c6243046301.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*K8bF7Vs-WyWbXSYLh-SIDQ.png"/></div></figure><p id="8c15" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它是一个钟形函数，从0(离标志非常远)到1(在标志处)变化。现在我们准备计算新的特征。例如，让我们看看实例x1 =–1:它位于距离第一个地标1和第二个地标2的位置。因此，它的新特性是x2 = exp(–0.3×12)≈0.74，x3 = exp(–0.3×22)≈0.30。图8(右)显示了没有原始要素的变换数据集。如你所见，它现在是线性可分的。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi pi"><img src="../Images/463088ae4d992b20d000dace8483b21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C41-Y5s2e9XDOveHC1K4Cg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图8。使用高斯径向基函数的特征相似性。</figcaption></figure><p id="79b3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可能想知道如何选择地标。最简单的方法是在数据集中每个实例的位置创建一个地标。这产生了许多维度，从而增加了变换后的训练集是线性可分的机会。缺点是，具有m个实例和n个特征的训练集被转换为具有m个实例和m个特征的训练集(假设您删除了原始特征)。如果你的训练集非常大，你最终会得到同样多的特征。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="e1cd" class="og lo it bd lp oh oi dn lt oj ok dp lx kb ol om mb kf on oo mf kj op oq mj or bi translated"><strong class="ak"> 2.3。高斯径向基函数核</strong></h2><p id="973d" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated">就像多项式特征方法一样，相似性特征方法对于任何机器学习算法都是有用的，但是计算所有附加特征在计算上可能是昂贵的，尤其是在大型训练集上。然而，内核技巧又一次施展了它的SVM魔法:它使得获得类似的结果成为可能，就好像你已经添加了许多类似的特性，而实际上并没有添加它们。</p><p id="eca7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们使用SVC类来尝试高斯RBF核:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="4343" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该模型的输出如下图所示:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/bedbb1fa95ad19b7c80c8e541387adb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*LRWxZglWbGwFCLK9F5HNow.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图9。使用RBF核特征γ=5和c=0.001的SVM分类器。</figcaption></figure><p id="e067" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们还可以尝试使用超参数γ(γ)和C的其他值来训练模型。让我们首先将C增加到1000，并像以前一样保持γ = 5。这个模型的输出如图10所示。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/9c2397ea9ee9625ff4cb16c7cabd79b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*a96dGyJXz_ucJBs4qAXCwA.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图10。使用RBF核特征γ=5和c=1000的SVM分类器。</figcaption></figure><p id="5e7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们将γ减小到0.1，并尝试C = 0.001和C =1000。结果如图11和12所示:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/b8cc6b57dbe051f415b54cfd514abe95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*ZGbnrZUDG_DUzvJzAgbNKw.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图11。使用RBF核特征γ=0.1和c=0.001的SVM分类器。</figcaption></figure><p id="aca8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后我们用γ = 0.1，C=1000来训练模型:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/aae84880723bf6fb02231919db3a7ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*CUG-zMZVOMaKzViLbyd9Rg.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图12。使用RBF核特征γ=0.1和c=1000的SVM分类器。</figcaption></figure><p id="4808" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">增加gamma会使钟形曲线变得更窄(参见图8的左图)，因此，每个实例的影响范围更小:决策边界最终变得更加不规则，在各个实例周围摆动(图9、10)。相反，较小的gamma值会使钟形曲线变得更宽，因此实例的影响范围更大，决策边界最终会更平滑(图11、12)。所以γ的作用就像一个正则化超参数:如果你的模型过拟合，你就应该减少它，如果过拟合，你就应该增加它(类似于C超参数)。</p><blockquote class="pj pk pl"><p id="64b2" class="jq jr of js b jt ju jv jw jx jy jz ka pm kc kd ke pn kg kh ki po kk kl km kn im bi translated">其他内核也存在，但很少使用。例如，一些内核专用于特定的数据结构。当分类文本文档或DNA序列时，有时使用串核(例如，使用基于Levenshtein距离的串子序列核)。</p></blockquote><p id="fa3e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有这么多内核可供选择，您如何决定使用哪一个呢？作为一个经验法则，你应该总是首先尝试线性内核(记住LinearSVC比sklearn中的SVC(kernel = "<strong class="js iu">linear</strong>")<strong class="js iu">快得多，尤其是如果训练集非常大或者如果它有很多特性</strong>。如果训练集不是太大，也要试试高斯RBF核；它在大多数情况下工作良好。然后，如果您有空闲时间和计算能力，您还可以使用交叉验证和网格搜索来试验一些其他内核，尤其是如果有专门针对您的训练集的数据结构的内核。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="ce72" class="og lo it bd lp oh oi dn lt oj ok dp lx kb ol om mb kf on oo mf kj op oq mj or bi translated">2.4.计算的复杂性</h2><p id="15b1" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated"><strong class="js iu"> LinearSVC </strong>类基于<strong class="js iu"> liblinear </strong>库，该库实现了线性SVM的优化算法。它不支持内核技巧，但它几乎与训练实例的数量和特征的数量成线性比例关系:它的训练时间复杂度大致为O(m × n)。如果您要求非常高的精度，该算法需要更长的时间。这由公差超参数ϵ控制(在Scikit-Learn中称为<strong class="js iu"> tol </strong>)。在大多数分类任务中，默认容差是合适的。</p><p id="fdce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> SVC </strong>类基于<strong class="js iu"> libsvm </strong>库，它实现了一个支持内核技巧的算法。训练时间复杂度通常在O(m2 × n)到O(m3 × n)之间。不幸的是，这意味着当训练实例的数量变大时(例如，几十万个实例)，它会变得非常慢。该算法非常适合复杂但规模较小或中等的训练集。<strong class="js iu">然而，它与特征的数量成比例，尤其是稀疏特征(即，当每个实例几乎没有非零特征时)。在这种情况下，该算法大致与每个实例中非零要素的平均数量成比例。</strong></p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="c4ec" class="ln lo it bd lp lq pa ls lt lu pb lw lx ly pc ma mb mc pd me mf mg pe mi mj mk bi translated">3.SVM回归</h1><p id="dae4" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated">SVM算法非常通用:它不仅支持线性和非线性分类，还支持线性和非线性回归。诀窍是颠倒目标:SVM回归不是试图在两个类之间拟合最大可能的街道，同时限制边界违规，而是试图在街道上拟合尽可能多的实例，同时限制边界违规(即街道外的实例)。街道的宽度由超参数ϵ.控制</p><p id="0e64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们首先创建随机数据:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="f3b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，让我们训练两个SVM分类器，一个具有ϵ =1.5，ϵ =0.5:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="94bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，让我们绘制两个基于随机线性数据训练的线性SVM回归模型，一个具有大幅度(ϵ = 1.5)，另一个具有小幅度(ϵ =0.5)。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/6dcde6493824cc6473c41dac4ae79fc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*dmh33DtOLmRaL2n0rpTgnQ.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图13。不同ϵ = 1.5和0.5的SVM回归。</figcaption></figure><p id="0c5e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在容限内添加更多的定型实例不会影响模型的预测；因此，这个模型被称为ϵ-insensitive.您可以使用Scikit-Learn的<strong class="js iu"> LinearSVR </strong>类来执行线性SVM回归。</p><p id="3cbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了处理非线性回归任务，您可以使用内核化的SVM模型。首先在下面的代码中，我们将生成一个随机二次训练集:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="47c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们将使用不同的C值的二次多项式核来训练SVM回归器。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="2ed6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们将使用下面的代码绘制这两个模型的曲线图:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/73bcbb77f7b62e4504dce79c4458ceab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*UAUG3Wdg5Al3flqpB4MJ0Q.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图14。使用二次多项式核的SVM回归。</figcaption></figure><p id="3abb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们在图14中看到的，左边的图(具有大C值的模型)上几乎没有正则化，而右边的图(具有小C值的模型)上有更多的正则化。</p><p id="92fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用Scikit-Learn的<strong class="js iu"> SVR类</strong>(支持内核技巧)生成图14中的相同模型。<strong class="js iu"> SVR类</strong>是<strong class="js iu"> SVC类</strong>的回归等价物，<strong class="js iu"> LinearSVR类</strong>是<strong class="js iu"> LinearSVC类</strong>的回归等价物。<strong class="js iu"> LinearSVR类</strong>随着训练集的大小线性扩展(就像LinearSVC类)，而当训练集变大时，SVR类变得太慢(就像SVC类)。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ox oy l"/></div></figure></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="a72d" class="ln lo it bd lp lq pa ls lt lu pb lw lx ly pc ma mb mc pd me mf mg pe mi mj mk bi translated">4.参考</h1><p id="d169" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb os kd ke kf ot kh ki kj ou kl km kn im bi translated">[1].使用Scikit-Learn、Keras和TensorFlow进行机器实践学习，第二版</p><p id="53a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2].您可以在GitHub资源库中找到本文中使用的代码:</p><div class="nb nc gp gr nd ne"><a href="https://github.com/youssefHosni/Machine-Learning-Practical-Guide" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">GitHub—youssefHosni/机器学习实用指南</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">github.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns lh ne"/></div></div></a></div></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><div class="ky kz la lb gt ne"><a href="https://youssefraafat57.medium.com/membership" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">加入我的介绍链接媒体-优素福胡斯尼</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">阅读Youssef Hosni(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">youssefraafat57.medium.com</p></div></div><div class="nn l"><div class="pr l np nq nr nn ns lh ne"/></div></div></a></div><p id="cff8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="of">感谢阅读！如果你喜欢这篇文章，一定要鼓掌(高达50！)并在</em><a class="ae oe" href="https://www.linkedin.com/in/youssef-hosni-b2960b135/" rel="noopener ugc nofollow" target="_blank"><em class="of">LinkedIn</em></a><em class="of">上与我联系，并在</em><a class="ae oe" href="https://youssefraafat57.medium.com/" rel="noopener"><em class="of">Medium</em></a><em class="of">上关注我的新文章</em></p></div></div>    
</body>
</html>