<html>
<head>
<title>Everything on Hierarchical Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于层次聚类的一切</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/everything-on-hierarchical-clustering-60bf613377a2?source=collection_archive---------0-----------------------#2021-06-26">https://pub.towardsai.net/everything-on-hierarchical-clustering-60bf613377a2?source=collection_archive---------0-----------------------#2021-06-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ba7f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="cb56" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一种无监督聚类算法，用于将具有共同特征的数据分层聚类成不同的组</h2></div><p id="b98b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，您将了解到。</p><ul class=""><li id="c248" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">什么是层次聚类，它用在什么地方？</li><li id="992f" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">两种不同类型的层次聚类——凝聚聚类和分裂聚类</li><li id="1823" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">了解不同的联系和度量后，层次聚类算法如何工作？</li><li id="b255" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">什么是树状图？</li><li id="7924" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">从树状图中寻找最佳聚类数</li><li id="166e" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">使用python实现分层聚类</li></ul><p id="f545" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">聚类是对无标签数据进行无监督学习的最常见形式，用于根据距离度量将具有共同特征的对象聚类成离散的簇。</p><p id="c0cf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">常见的聚类算法有</strong></p><ul class=""><li id="6e2f" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><strong class="kt jd">基于质心的聚类</strong>像<a class="ae mb" rel="noopener ugc nofollow" target="_blank" href="/k-means-clustering-techniques-to-find-the-optimal-clusters-7eea5431a4fb">k意味着</a>是高效的，但是对初始条件和离群值敏感</li><li id="6971" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">基于密度的聚类</strong>如<a class="ae mb" href="https://medium.com/swlh/dbscan-density-based-spatial-clustering-for-applications-with-noise-476d95c1f14a" rel="noopener"> DBSCAN </a>将数据聚类到由低密度区域分隔的高密度区域。</li><li id="7b73" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">基于分布的聚类</strong>类似于使用期望最大化(EM)的高斯混合模型，这是一种生成概率模型，试图找到最能模拟数据集的高斯概率分布</li><li id="0136" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">层次聚类</strong></li></ul><blockquote class="mc"><p id="5cbc" class="md me it bd mf mg mh mi mj mk ml lm dk translated">层次聚类构建聚类的层次结构，而无需根据相似性得分预先指定聚类的数量</p></blockquote><p id="58fe" class="pw-post-body-paragraph kr ks it kt b ku mm kd kw kx mn kg kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">分层聚类有助于</p><ul class=""><li id="5c14" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><strong class="kt jd">客户细分</strong>可以使用人口统计、收入或购买模式对客户进行细分</li><li id="9552" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">社交网络分析</strong>根据个人和团体的兴趣和对可用信息的访问来了解他们的动态。</li><li id="0726" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">根据共同特征将基因组数据</strong>整理成有意义的生物结构</li><li id="19ee" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">城市规划集群</strong>确保商业区不在工业区内，或者住宅区不在工业区内。</li></ul><p id="4e08" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">层次聚类或者是自下而上的，称为聚集聚类，或者是分裂聚类，使用自上而下的方法。</p><h2 id="5b06" class="mr ms it bd mt mu mv dn mw mx my dp mz la na nb nc le nd ne nf li ng nh ni iz bi translated"><strong class="ak">凝聚聚类</strong></h2><p id="cd7b" class="pw-post-body-paragraph kr ks it kt b ku nj kd kw kx nk kg kz la nl lc ld le nm lg lh li nn lk ll lm im bi translated">一种<strong class="kt jd">自下而上的方法，其中每个数据点在开始</strong>、<strong class="kt jd">被认为是单个聚类，聚类基于相似性</strong>被迭代合并，直到所有数据点都被合并到一个聚类中。</p><p id="489d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">聚集聚类<strong class="kt jd">基于使用距离度量计算的最大相似性聚集成对的聚类，以获得新的聚类</strong>，从而减少每次迭代的聚类数量。他们<strong class="kt jd">只考虑本地模式</strong>而不考虑全球分布。</p><h2 id="51f0" class="mr ms it bd mt mu mv dn mw mx my dp mz la na nb nc le nd ne nf li ng nh ni iz bi translated">分裂聚类</h2><p id="0e23" class="pw-post-body-paragraph kr ks it kt b ku nj kd kw kx nk kg kz la nl lc ld le nm lg lh li nn lk ll lm im bi translated">一种<strong class="kt jd">自上而下的方法</strong>，与凝聚型集群的自下而上的方法相反。分裂聚类<strong class="kt jd">从包含来自数据集的所有数据点</strong>的一个聚类开始。然后<strong class="kt jd">使用类似KMeans的平面聚类算法</strong>迭代地分割聚类，直到每个数据点都属于一个单独的聚类。<strong class="kt jd">分裂聚类比自底向上的聚集聚类产生更精确的层次结构</strong>。分裂聚类<strong class="kt jd">通过查看数据集中存在的完整信息来解释全局模式</strong>。</p><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8cc8022711a6f9100d80a884360691e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*qwZurf5TdzD_GMqVrmTErw.png"/></div></figure><h2 id="59f6" class="mr ms it bd mt mu mv dn mw mx my dp mz la na nb nc le nd ne nf li ng nh ni iz bi translated">深入研究聚集聚类的工作原理</h2><p id="bb21" class="pw-post-body-paragraph kr ks it kt b ku nj kd kw kx nk kg kz la nl lc ld le nm lg lh li nn lk ll lm im bi translated"><strong class="kt jd">第一步</strong> : <strong class="kt jd">所有的数据点被指定为一个单点簇</strong>。如果数据集中有<em class="nw"> m </em>个观察值，每个点被分配到一个聚类，我们将有<em class="nw"> m </em>个聚类。</p><p id="2262" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第二步:找到壁橱或最相似的一对集群，将它们合并成一个集群。层次聚类使用相似性度量来组合最相似的聚类对。<strong class="kt jd">使用欧几里德距离、曼哈顿距离(城市街区距离)、闵可夫斯基距离或余弦相似性来测量聚类之间的相似性。</strong></p><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/a7cd37dd4a1424ef0078bc316307cf3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZWAOYhcMFsfyTL7szmtCxw.png"/></div></div></figure><p id="1666" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">第三步:在确定两个最近的聚类之后，使用链接方法来确定如何合并这两个聚类</strong>。几种联动方式有<strong class="kt jd">单联动、完全联动、平均联动、质心法或沃德法。</strong></p><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oc"><img src="../Images/dcc0abbc9ba76cbf480d7d9e38b1f859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vv_0OE7Wwr3_CujLmKyKPA.png"/></div></div><figcaption class="od oe gj gh gi of og bd b be z dk translated">不同的链接合并两个最接近或相似的集群</figcaption></figure><p id="dd0e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">重复第2步和第3步，直到所有观察结果都聚集在一起。整合成一个集群</strong></p><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oh"><img src="../Images/c13ad34db3326a8e3c17bd80a2ec04b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPFcvJnckpIlJPBrwJ4QSA.png"/></div></div></figure><h2 id="347a" class="mr ms it bd mt mu mv dn mw mx my dp mz la na nb nc le nd ne nf li ng nh ni iz bi translated">系统树图</h2><p id="5b1a" class="pw-post-body-paragraph kr ks it kt b ku nj kd kw kx nk kg kz la nl lc ld le nm lg lh li nn lk ll lm im bi translated"><strong class="kt jd">层次聚类通常使用树状图<em class="nw">来可视化。</em>树状图<em class="nw">是基于相似性或不相似性度量的点的</em>树状表示。树状图具有一个轴的数据项和沿着另一个轴的距离，其中两个合并点或聚类之间的距离单调增加。</strong></p><p id="0ac3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">计算每一对点之间的距离分数</p><p id="b4ff" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">基于不同连锁方法的树状图</strong></p><pre class="np nq nr ns gt oi oj ok ol aw om bi"><span id="3ae3" class="mr ms it oj b gy on oo l op oq"><strong class="oj jd">from scipy.cluster.hierarchy import dendrogram, linkage<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>plt.figure(figsize=(15,5))<br/>data= [[i] for i in [9, 3, 6, 4, 11]]<br/>linked_s = linkage(data, 'single', metric='euclidean')<br/>plt.subplot(2, 5, 1)</strong></span><span id="33bd" class="mr ms it oj b gy or oo l op oq"><strong class="oj jd">dendrogram(linked , labels=data)<br/>plt.ylabel('Distance')<br/>plt.title("single linkage")</strong></span><span id="ec3c" class="mr ms it oj b gy or oo l op oq"><strong class="oj jd">linked_s = linkage(data, 'complete', metric='euclidean')<br/>plt.subplot(2, 5,2)<br/>dendrogram(linked_s , labels=data)<br/>plt.title("complete linkage")</strong></span><span id="ae57" class="mr ms it oj b gy or oo l op oq"><strong class="oj jd">linked_s = linkage(data, 'average', metric='euclidean')<br/>plt.subplot(2, 5,3)<br/>dendrogram(linked_s , labels=data)<br/>plt.title("average linkage")</strong></span><span id="29f0" class="mr ms it oj b gy or oo l op oq"><strong class="oj jd">linked_s = linkage(data, 'ward', metric='euclidean')<br/>plt.subplot(2, 5,4)<br/>dendrogram(linked_s , labels=data)<br/>plt.title("ward method")</strong></span><span id="d9b9" class="mr ms it oj b gy or oo l op oq"><strong class="oj jd">linked_s = linkage(data, 'centroid', metric='euclidean')<br/>plt.subplot(2, 5,5)<br/>dendrogram(linked_s , labels=data)<br/>plt.title("centroid method")<br/>plt.show()</strong></span></pre><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi os"><img src="../Images/541c50c6017f11776fd34009d7c95256.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6MQp6nIwOMcT0Cyy7ar7yw.png"/></div></div></figure><p id="626c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">凝聚式聚类实现</strong></p><p id="43ae" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">数据集- <a class="ae mb" href="https://www.kaggle.com/shwetabh123/mall-customers" rel="noopener ugc nofollow" target="_blank">商城客户数据集</a></p><pre class="np nq nr ns gt oi oj ok ol aw om bi"><span id="55ac" class="mr ms it oj b gy on oo l op oq"><strong class="oj jd">import pandas as pd</strong><br/># Read the dataset into a dataframe<br/><strong class="oj jd">dataset = pd.read_csv('Mall_Customers.csv',index_col='CustomerID')</strong><br/># Drop duplicates<br/><strong class="oj jd">dataset.drop_duplicates(inplace=True)<br/>plt.figure(figsize=(10,5))</strong><br/># Creating the input variable<br/><strong class="oj jd">X= dataset.iloc[:, [1,2,3]].values<br/>linked_s = linkage(X, 'complete', metric='CityBlock')<br/>dendrogram(linked , labels=X)<br/>plt.ylabel('Distance')<br/>plt.axhline(y=1.5, color='orange')<br/>plt.title("Mall Customer HCA")<br/>plt.tight_layout(pad=3.0)<br/>plt.show()</strong></span></pre><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/dd6c1dc9f06172772cf1883c0e8f26b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*8Qz-GpbXOzkhYd4B6hY5Uw.png"/></div></figure><p id="447d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">从树状图中寻找最佳聚类数</strong></p><p id="b2b6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">找出上面蓝色箭头所示的垂直线中的最大高度差。选择阈值的方式是，它切割最高的垂直线。</p><p id="01cf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">聚类的数量将是与使用阈值绘制的线相交的垂直线的数量。</strong></p><p id="a200" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在上面的例子中，聚类的最佳数量是4</p><p id="1d4c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">运行凝聚聚类</strong></p><pre class="np nq nr ns gt oi oj ok ol aw om bi"><span id="e0dc" class="mr ms it oj b gy on oo l op oq"><strong class="oj jd">from sklearn.cluster import AgglomerativeClustering<br/>import seaborn as sns<br/>agg_cluster = AgglomerativeClustering(n_clusters=4).fit_predict(X)</strong></span><span id="85a3" class="mr ms it oj b gy or oo l op oq">#Visualising the clusters<br/><strong class="oj jd">plt.figure(figsize=(15,7))<br/>sns.scatterplot(X[agg_cluster == 0, 2], X[agg_cluster == 0, 1], color = 'yellow', label = 'Cluster 1',s=50)<br/>sns.scatterplot(X[agg_cluster == 1, 2], X[agg_cluster == 1, 1], color = 'blue', label = 'Cluster 2',s=50)<br/>sns.scatterplot(X[agg_cluster == 2, 2], X[agg_cluster == 2, 1], color = 'green', label = 'Cluster 3',s=50)<br/>sns.scatterplot(X[agg_cluster == 3, 2], X[agg_cluster == 3, 1], color = 'grey', label = 'Cluster 4',s=50)<br/>plt.grid(False)<br/>plt.title('Clusters of customers')<br/>plt.xlabel('Annual Income')<br/>plt.ylabel('Spending Score')<br/>plt.legend()<br/>plt.show()</strong></span></pre><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi ou"><img src="../Images/ab1d42255f538dd5101764b3ce0f5e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lEs6u7OFJtHt9szoAa2uJw.png"/></div></div></figure><p id="85de" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">关于分层集群的其他知识</strong></p><p id="c19d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">分层聚类对异常值很敏感，并且不能处理缺失数据</p><p id="1d94" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">分层聚类在处理较小的数据集时特别有效。当聚类完成时，随着更多的数据点被考虑，它在计算上变得更加昂贵。</p><h2 id="dc9c" class="mr ms it bd mt mu mv dn mw mx my dp mz la na nb nc le nd ne nf li ng nh ni iz bi translated">结论:</h2><p id="1338" class="pw-post-body-paragraph kr ks it kt b ku nj kd kw kx nk kg kz la nl lc ld le nm lg lh li nn lk ll lm im bi translated">层次聚类是一种无监督聚类方法，根据距离度量将具有共同特征的对象聚类成离散的簇。分层算法通过连续合并或拆分聚类来构建聚类，而无需预先指定聚类的数量。相似性得分使用不同的方法计算，如欧几里德距离或城市街区距离。当我们连续合并聚类时，这是凝聚聚类，而当我们连续分裂时，这种聚类被称为分裂聚类。使用树状图来可视化层次聚类。</p><h2 id="78f8" class="mr ms it bd mt mu mv dn mw mx my dp mz la na nb nc le nd ne nf li ng nh ni iz bi translated">参考资料:</h2><p id="ceb6" class="pw-post-body-paragraph kr ks it kt b ku nj kd kw kx nk kg kz la nl lc ld le nm lg lh li nn lk ll lm im bi translated"><a class="ae mb" href="https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html" rel="noopener ugc nofollow" target="_blank">层次凝聚聚类(stanford.edu)</a></p><div class="ov ow gp gr ox oy"><a href="https://www.mygreatlearning.com/blog/hierarchical-clustering" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">什么是层次聚类？层次聚类简介</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">什么是层次聚类聚类是用于创建同构群的流行技术之一…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">www.mygreatlearning.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm nu oy"/></div></div></a></div><p id="f474" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae mb" href="https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/hierarchical-clustering.pdf" rel="noopener ugc nofollow" target="_blank">princeton.edu hierarchical-clustering.pdf</a></p><p id="2c58" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae mb" href="https://developers.google.com/machine-learning/clustering" rel="noopener ugc nofollow" target="_blank">https://developers.google.com/machine-learning/clustering</a></p></div></div>    
</body>
</html>