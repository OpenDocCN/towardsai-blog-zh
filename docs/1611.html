<html>
<head>
<title>Google’s Model Search: An Open Source Platform for Finding Optimal ML Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Google的模型搜索:一个寻找最佳ML模型的开源平台</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/googles-model-search-an-open-source-platform-for-finding-optimal-ml-models-d2f422864a93?source=collection_archive---------2-----------------------#2021-03-04">https://pub.towardsai.net/googles-model-search-an-open-source-platform-for-finding-optimal-ml-models-d2f422864a93?source=collection_archive---------2-----------------------#2021-03-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="5669" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/optimization" rel="noopener ugc nofollow" target="_blank">优化</a></h2><div class=""/><div class=""><h2 id="5ce2" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从A到Z自动化关于模型架构的一切</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ec519007a164ba4f5e6fe94aa656a319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YvxTcE2r6JXtxEqM"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@lucabravo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卢卡·布拉沃</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><blockquote class="li lj lk"><p id="71f7" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">对于给定的问题，合适的神经网络是什么样的？应该有多深？应该使用哪种类型的层？lstm<a class="ae lh" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">是否足够或者变压器</a><a class="ae lh" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank">层是否更好？或者两者兼而有之？</a><a class="ae lh" href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="noopener ugc nofollow" target="_blank">组装</a>或<a class="ae lh" href="https://en.wikipedia.org/wiki/Knowledge_distillation" rel="noopener ugc nofollow" target="_blank">蒸馏</a>会提升性能吗？当考虑机器学习(ML)领域时，这些棘手的问题变得更加具有挑战性，在这些领域中可能存在比其他领域更好的直觉和更深的理解。</p></blockquote><p id="4703" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">来源:<a class="ae lh" href="https://ai.googleblog.com/2021/02/introducing-model-search-open-source.html" rel="noopener ugc nofollow" target="_blank">谷歌</a></p><p id="67ea" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">以上问题相当棘手。作为数据科学家，目前的方法只是尝试更有意义的可能性，评估，做出另一个选择&amp;重复。这个过程相当耗时，网络架构搜索(NAS)已经存在一段时间了。NAS的概念是让网络/软件为给定的任务选择最合适的架构，这样你就不必在那些冗长的实验上浪费时间。</p><p id="9746" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">几天前，谷歌发布了它的NAS框架“模型搜索”。在他们的框架中，他们试图优化NAS框架的现有问题，例如[1]:</p><ol class=""><li id="d76d" class="ml mm it lo b lp lq ls lt mi mn mj mo mk mp mh mq mr ms mt bi translated">昂贵的计算成本</li><li id="2c36" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh mq mr ms mt bi translated">需要包含大量的先验信息</li><li id="8574" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh mq mr ms mt bi translated">低概括能力</li></ol><p id="2fdf" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">让我们从理论的角度来解释它是如何工作的。然后我们可以转到编码方面，给出一个快速教程。</p><p id="fc90" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">那么它是如何工作的呢？</p><p id="f610" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我不会深究细节，因为这不是本文的重点，你可以查看谷歌的帖子。但是，基本上，该系统运行几次试验，并且在每个周期的开始，<a class="ae lh" href="https://en.wikipedia.org/wiki/Beam_search" rel="noopener ugc nofollow" target="_blank">波束搜索算法</a>被用于基于所提供的度量来决定接下来要尝试什么。该算法是一种计算机科学启发式图搜索算法，它在这里工作得很好，因为张量流依赖于图。</p><p id="1610" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">该系统由多名训练员和一个评估系统组成，以便平行进行试验，并将所有模型保存到数据库中。</p><blockquote class="li lj lk"><p id="65e0" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在模型搜索中实现的搜索算法是自适应的、<a class="ae lh" href="https://en.wikipedia.org/wiki/Greedy_algorithm" rel="noopener ugc nofollow" target="_blank">贪婪的</a>和增量的，这使得它们比RL算法收敛得更快。然而，它们确实模仿了RL算法的“<a class="ae lh" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank">探索&amp;利用</a>”本质，通过分离对良好候选的搜索(探索步骤)，并通过集合被发现的良好候选来提高准确性(利用步骤)。在对架构或训练技术应用随机改变(例如，使架构更深)之后，主搜索算法自适应地修改前<em class="it"> k </em>个执行实验中的一个(其中<em class="it"> k </em>可以由用户指定)。</p></blockquote><p id="c682" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">来源:<a class="ae lh" href="https://ai.googleblog.com/2021/02/introducing-model-search-open-source.html" rel="noopener ugc nofollow" target="_blank">谷歌</a></p><p id="f3bf" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">真正有趣的是，搜索算法建立在强化学习策略的基础上，它们试图平衡“探索和利用”策略，以优化模型，同时探索最佳的潜在模型。</p><p id="ba32" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">选择的最后一点(也取决于试验)是将这些模型集成到一个模型中，还是将较大的模型提取为较小的模型。</p><p id="b68a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我希望文章的下一部分更多的是关于编码，而不是理论。然而，无论如何，如果你是一个好奇的数据科学家(像我一样)，我建议看看谷歌的这篇论文，我相信很多理论都是建立在它的基础上的:</p><p id="8f29" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><a class="ae lh" href="https://pdfs.semanticscholar.org/1bca/d4cdfbc01fbb60a815660d034e561843d67a.pdf" rel="noopener ugc nofollow" target="_blank">通过大规模的神经架构搜索改进关键词定位和语言识别</a></p><p id="baff" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">本文深入探讨了有趣的NAS，详细介绍了塔式搜索算法、何时集合、语言识别等等。它还深入探讨了NAS背后的数学原理。</p><h2 id="4987" class="mz na it bd nb nc nd dn ne nf ng dp nh mi ni nj nk mj nl nm nn mk no np nq iz bi translated">让我们把它付诸实践</h2><p id="34d9" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">他们在<a class="ae lh" href="https://github.com/google/model_search" rel="noopener ugc nofollow" target="_blank"> Github </a>上提供了框架，它目前只支持分类问题(遗憾的是)，但我认为你可以很容易地将它用于回归问题。他们将2个用例分成CSV数据和非CSV数据。</p><h2 id="201c" class="mz na it bd nb nc nd dn ne nf ng dp nh mi ni nj nk mj nl nm nn mk no np nq iz bi translated">对于第一种情况:CSV数据</h2><p id="5e58" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">让我们从进口开始:</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="3fcf" class="mz na it nx b gy ob oc l od oe">import model_search<br/>from model_search import constants<br/>from model_search import single_trainer<br/>from model_search.data import csv_data</span></pre><p id="986b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">来源:<a class="ae lh" href="https://github.com/google/model_search" rel="noopener ugc nofollow" target="_blank"> Github </a></p><p id="6827" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">然后我们实例化SingleTrainer和csv_data实例</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="c910" class="mz na it nx b gy ob oc l od oe">trainer = single_trainer.SingleTrainer(<br/>    data=csv_data.Provider(<br/>        label_index=0,<br/>        logits_dimension=2, # the number of outputs, 2 is for binary<br/>        record_defaults=[0, 0, 0, 0],<br/>        filename="model_search/data/testdata/csv_random_data.csv"),<br/>        spec=constants.DEFAULT_DNN)</span></pre><p id="28a0" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">来源:<a class="ae lh" href="https://github.com/google/model_search" rel="noopener ugc nofollow" target="_blank"> Github </a></p><p id="3166" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">“spec”参数指向存储库中您可以更改的配置。我研究了默认的DNN配置，它指定了最常用的DNN层选择:</p><ul class=""><li id="c37d" class="ml mm it lo b lp lq ls lt mi mn mj mo mk mp mh of mr ms mt bi translated">深度</li><li id="1436" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh of mr ms mt bi translated">完全连接的层</li><li id="5511" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh of mr ms mt bi translated">指数式衰减</li><li id="11f0" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh of mr ms mt bi translated">规范化</li><li id="ace0" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh of mr ms mt bi translated">宽度</li><li id="369d" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh of mr ms mt bi translated">组装和蒸馏</li></ul><p id="c09a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">最后，进行试验:</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="f282" class="mz na it nx b gy ob oc l od oe">trainer.try_models(<br/>    number_models=200,<br/>    train_steps=1000,<br/>    eval_steps=100,<br/>    root_dir="/tmp/run_example",<br/>    batch_size=32,<br/>    experiment_name="example",<br/>    experiment_owner="model_search_user")</span></pre><p id="8d26" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">来源:<a class="ae lh" href="https://github.com/google/model_search" rel="noopener ugc nofollow" target="_blank"> Github </a></p><h2 id="a6e2" class="mz na it bd nb nc nd dn ne nf ng dp nh mi ni nj nk mj nl nm nn mk no np nq iz bi translated">对于第二种情况:非CSV数据</h2><p id="2098" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">这个用例比前一个稍微难一点。与Pytorch加载器一样，您必须设计自己的类并从它们的抽象类<code class="fe og oh oi nx b">model_search.data.Provider.</code>继承，这包括定义输入函数和任务</p><p id="390a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">最终想法和要点</strong></p><p id="5d2a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我希望这篇文章能让你对Google的模型搜索有一个大致的了解。如果你有兴趣了解更多关于它的特性，我还没有介绍他们的Github库，比如:</p><ul class=""><li id="40f1" class="ml mm it lo b lp lq ls lt mi mn mj mo mk mp mh of mr ms mt bi translated">向搜索空间添加模型和架构</li><li id="a455" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh of mr ms mt bi translated">创建定型独立二进制文件，而不编写主</li><li id="6d52" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh of mr ms mt bi translated">分布式运行</li><li id="ef77" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh of mr ms mt bi translated">云汽车</li></ul><p id="7ea4" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">看看这个框架是否会在诸如Kaggle比赛这样的挑战中实际使用，或者它是否不会被社区使用，它只是被浪费掉，这将是很有趣的。(希望不会)。我想已经有很多关于NAS的论文了，但是大部分都没有做成框架。</p><p id="0a8d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">参考文献:</strong></p><p id="7829" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">[1]<a class="ae lh" href="https://ai.googleblog.com/2021/02/introducing-model-search-open-source.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2021/02/introducing-model-search-open-source . html</a></p></div></div>    
</body>
</html>