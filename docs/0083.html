<html>
<head>
<title>The Basics of Recurrent Neural Networks (RNNs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络的基础</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/whirlwind-tour-of-rnns-a11effb7808f?source=collection_archive---------0-----------------------#2019-06-24">https://pub.towardsai.net/whirlwind-tour-of-rnns-a11effb7808f?source=collection_archive---------0-----------------------#2019-06-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1aa7" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><h1 id="11ca" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">目录</h1><ul class=""><li id="bce5" class="ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">rnn是用来做什么的？</li><li id="e37b" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh li lj lk ll bi translated">什么是rnn，它们是如何工作的？</li><li id="9b95" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh li lj lk ll bi translated">一个简单的例子——正向传播，反向传播</li><li id="ef85" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh li lj lk ll bi translated">一个主要问题是:渐变消失</li></ul><h1 id="43f4" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">rnn是用来做什么的？</h1><p id="0bcd" class="pw-post-body-paragraph lr ls iq kw b kx ky lt lu kz la lv lw lb lx ly lz ld ma mb mc lf md me mf lh ij bi translated">递归神经网络(RNNs)广泛用于具有某种顺序结构的数据。例如，时序数据具有基于时间的内在排序。句子也是连续的，“我喜欢狗”和“我喜欢的狗”有不同的意思简单地说，如果你的数据的<em class="mg">语义</em>被随机排列改变了，你就有了一个连续的数据集，RNNs可能会被用于你的问题！为了帮助巩固rnn可以解决的问题类型，这里有一个常见应用的<strong class="kw ja">列表:</strong></p><ul class=""><li id="e9b7" class="ku kv iq kw b kx mh kz mi lb mj ld mk lf ml lh li lj lk ll bi translated">语音识别</li><li id="31c8" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh li lj lk ll bi translated">情感分类</li><li id="bfc0" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh li lj lk ll bi translated">机器翻译(即中文翻译成英文)</li><li id="c113" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh li lj lk ll bi translated">视频活动识别</li><li id="123f" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh li lj lk ll bi translated">名称实体识别—(即识别句子中的名称)</li></ul><p id="112f" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">太好了！我们知道可以应用RNNs解决的问题类型，现在…</p><h1 id="2ea3" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">什么是rnn，它们是如何工作的？</h1><p id="a992" class="pw-post-body-paragraph lr ls iq kw b kx ky lt lu kz la lv lw lb lx ly lz ld ma mb mc lf md me mf lh ij bi translated">rnn不同于经典的多层感知器(MLP)网络，因为两个主要原因:1)它们考虑了之前发生的事情(2)它们共享参数/权重。</p><p id="4c95" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated"><strong class="kw ja">RNN的建筑</strong></p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mp"><img src="../Images/7e27cd0654187bb9b0cd7893995422c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ccHxugJhQo7VH4GAAZt3Sg.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated"><strong class="bd jy">左</strong>:常用于RNNs的简写符号，<strong class="bd jy">右</strong>:RNNs的展开符号</figcaption></figure><p id="5a07" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">如果这没有意义，请不要担心，我们将分解所有变量，稍后将进行正向传播和反向传播！第一眼只关注变量的流动。</p><p id="4c69" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated"><strong class="kw ja">架构分解</strong></p><p id="3f1d" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">绿色方块被称为<strong class="kw ja"> <em class="mg">隐藏状态</em> </strong>。由<strong class="kw ja">向量</strong> <strong class="kw ja"> <em class="mg">定义的蓝色圆圈，一个</em> </strong> <em class="mg"> </em>在每个块内，被称为<strong class="kw ja"> <em class="mg">隐藏节点</em> </strong> <em class="mg">或</em> <strong class="kw ja"> <em class="mg">隐藏单元</em> </strong> <em class="mg"> </em>其中节点的数量由超参数<strong class="kw ja"> <em class="mg"> d </em> </strong> <em class="mg">决定。</em>类似于MLPs中的激活，将每个绿色块视为作用于每个蓝色节点的激活函数。在本文的正向传播部分，我们将讨论隐藏状态中的计算。</p><p id="aebe" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated"><strong class="kw ja">向量<em class="mg"> h </em> </strong> <em class="mg"> — </em>是隐藏节点应用激活函数后隐藏状态的输出。如您在时间<em class="mg"> t、</em>所见，该架构通过包括来自<em class="mg">先前</em>隐藏状态的<em class="mg"> h </em>以及时间<em class="mg"> t </em>的输入<em class="mg"> x </em>来考虑在<em class="mg"> t-1 </em>发生的事情。这允许网络考虑来自顺序在当前输入之后的先前输入的信息。值得注意的是，第零个<em class="mg"> h </em>向量将总是以0的向量开始，因为该算法在序列中的第一个元素之前没有任何信息。</p><div class="mq mr ms mt gt ab cb"><figure class="nf mu ng nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/00f26810ff7e3243edb48cf7fda7b691.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div></figure><figure class="nf mu nl nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/fae913c74047addef22cbe518aaec4a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*IKUumcHgEWs8kw7w-_LFQw.png"/></div></figure><figure class="nf mu ng nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/00f26810ff7e3243edb48cf7fda7b691.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk nm di nn no translated">在t=2时的隐藏状态将t-1和x在t时的输出作为输入。</figcaption></figure></div><p id="5320" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated"><strong class="kw ja">矩阵<em class="mg"> Wx，Wy，Wh </em> </strong> —是整个网络中<em class="mg">共享的</em>RNN架构的权重。<em class="mg"> Wx </em>在<em class="mg"> t=1 </em>处的模型权重与<em class="mg"> Wx </em>在<em class="mg"> t=2 </em>处以及每隔一个时间步的权重完全相同。</p><p id="0fe8" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated"><strong class="kw ja">向量<em class="mg"> xᵢ </em> </strong> <em class="mg"> — </em>是输入序列中每个元素的每个隐藏状态的输入，其中i=1，2，…，n。回想一下，文本必须编码成数值。例如，单词“dogs”中的每个字母都是一个一维编码向量，维数为<em class="mg"> (4x1)。</em>同样，x也可以是单词嵌入或其他数值表示。</p><div class="mq mr ms mt gt ab cb"><figure class="nf mu nq nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/f96c7adad5c223c8eca2469a4702d6b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div></figure><figure class="nf mu nr nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/d774f9ea0432bce8334639410a7f2f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*qyiBXcnMwynHGD7pssXJFw.png"/></div></figure><figure class="nf mu nq nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/f96c7adad5c223c8eca2469a4702d6b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk ns di nt no translated">“狗”这个词的一键编码</figcaption></figure></div><p id="3bf4" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated"><strong class="kw ja"> RNN方程</strong></p><p id="bd59" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">现在我们知道了所有的变量，这是我们进行RNN计算需要的所有方程:</p><div class="mq mr ms mt gt ab cb"><figure class="nf mu nu nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/579e85c298c8b5888700a140fa12920a.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div></figure><figure class="nf mu nv nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/b08371344c0364888008b454c4bebcdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*jLWB_Dute-qB43DXqe8G3Q.png"/></div></figure><figure class="nf mu nu nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/579e85c298c8b5888700a140fa12920a.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div></figure></div><p id="20ad" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">这是我们唯一需要的三个方程，非常好！<strong class="kw ja">隐藏节点</strong>是由权重矩阵<em class="mg"> Wh </em>加权的前一状态的输出和由权重矩阵<em class="mg"> Wx加权的输入<em class="mg"> x </em>的串联。</em><em class="mg">tanh</em>功能就是我们之前提到的<strong class="kw ja">激活功能</strong>，用绿色方块表示。隐藏状态的<strong class="kw ja">输出是应用于隐藏节点的激活函数。<strong class="kw ja"> </strong>为了进行<strong class="kw ja">预测</strong>，我们从当前隐藏状态中提取输出，并通过softmax激活的权重矩阵<em class="mg"> Wy </em>对其进行加权。</strong></p><p id="b081" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">理解所有浮动变量的维度也很重要。一般来说，预测一个序列:</p><div class="mq mr ms mt gt ab cb"><figure class="nf mu nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/4c4fe51e01626e19e2628c5e6b1e9dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div></figure><figure class="nf mu nx nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/0bc2b350758ef2d2937bd8b68e087f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*92EFlFFWnLRSz6gYxeGx8w.png"/></div></figure><figure class="nf mu nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/4c4fe51e01626e19e2628c5e6b1e9dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div></figure></div><p id="b501" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">在哪里</p><ul class=""><li id="bcac" class="ku kv iq kw b kx mh kz mi lb mj ld mk lf ml lh li lj lk ll bi translated"><strong class="kw ja"> k </strong>是输入向量的维数<em class="mg"> xᵢ </em></li><li id="d53a" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh li lj lk ll bi translated"><strong class="kw ja"> d </strong>是隐藏节点的数量</li></ul><p id="cdef" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">现在，我们准备浏览一个示例！</p><h1 id="6c40" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">一个微不足道的例子</strong></h1><p id="03be" class="pw-post-body-paragraph lr ls iq kw b kx ky lt lu kz la lv lw lb lx ly lz ld ma mb mc lf md me mf lh ij bi translated">以单词<strong class="kw ja">、</strong>为例，在给定字母<strong class="kw ja">、【d】、【o】、【g】、</strong>的情况下，我们希望训练一个RNN来预测字母<strong class="kw ja">、【s】、</strong>。上面的体系结构如下所示:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ny"><img src="../Images/42fa8cf4b7ede2ce30e7a0e8cd303bf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dyfwJJuGT2Svy10iYAVDEQ.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">RNN建筑预测“狗”中的字母“s”</figcaption></figure><p id="39c5" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">为了使这个例子简单，我们将在RNN <em class="mg"> (d=3) </em>中使用3个隐藏节点。我们每个变量的维度如下:</p><div class="mq mr ms mt gt ab cb"><figure class="nf mu nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/4c4fe51e01626e19e2628c5e6b1e9dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div></figure><figure class="nf mu nx nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/2c9d3c50922aab13445c6a58387379db.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*CtC31tBxLqN4xO49ttoR4A.png"/></div></figure><figure class="nf mu nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/4c4fe51e01626e19e2628c5e6b1e9dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*JzXJ4bIf_PzkmNZOpVxhiQ.png"/></div></figure></div><p id="0a67" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">其中k = 4，因为我们的输入<em class="mg"> x </em>是“dogs”中字母的一个4维hot向量。<em class="mg"/></p><h2 id="0d6e" class="nz jx iq bd jy oa ob dn kc oc od dp kg lb oe of kk ld og oh ko lf oi oj ks iw bi translated">正向传播</h2><p id="49a8" class="pw-post-body-paragraph lr ls iq kw b kx ky lt lu kz la lv lw lb lx ly lz ld ma mb mc lf md me mf lh ij bi translated">让我们看看在时间<em class="mg"> t=1时正向传播是如何工作的。</em>首先我们要计算隐藏节点<strong class="kw ja"> <em class="mg"> a </em> </strong>，然后应用激活函数得到<strong class="kw ja"> <em class="mg"> h </em> </strong>，最后计算<strong class="kw ja">预测</strong>。轻松点。</p><p id="304e" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">在t=1时</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ok"><img src="../Images/244fca284bc9e5c319e9c252426f2652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*whk9i5s_eOyNecN9ChVLbA.png"/></div></div></figure><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ol"><img src="../Images/cb5f788324d2e7a14535fc190c5695df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-mMcu-obKSmrgXD0Kgzfw.png"/></div></div></figure><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi om"><img src="../Images/7a0fb4875938f82d5867a18fd277d97a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IHEaJk8AHDTot5HANg49WQ.png"/></div></div></figure><p id="23ee" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">为了使这个例子具体化，我已经为矩阵<strong class="kw ja"> <em class="mg"> Wx，Wy，</em> </strong>和<em class="mg"> </em> <strong class="kw ja"> Wh </strong>初始化了随机权重，以提供一个带有数字的例子。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi on"><img src="../Images/6d50254b8f1acf1cc5e7cb18fb4b3e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6zRiOkJuhjzKfs2r_UB8iw.png"/></div></div></figure><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oo"><img src="../Images/3c15b95fd7c3ac1535c2bf6941272cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esee_GisAgQ1Fctmm6dR5w.png"/></div></div></figure><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi op"><img src="../Images/47156b839712c9f374ca0a6ef3b39ab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xdkZbBzYrKI0DTrQOGb5g.png"/></div></div></figure><p id="3c40" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">在t=1时，给定输入“d”，我们的RNN将预测字母“d”。这没有意义，但没关系，因为我们已经使用了未经训练的随机权重。这只是为了展示RNN向前传球的工作流程。在t=2和t=3时，除了来自<em class="mg"> t-1 </em>的<em class="mg">向量h </em>不再是0的向量，而是基于时间<em class="mg"> t之前的输入的非零向量之外，工作流程将是类似的。(提醒一下，对于t=1、2和3，权重矩阵Wx、Wh和Wy保持不变。</em>)</p><p id="9ede" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">值得注意的是，虽然RNN <em class="mg">可以</em>在每一个时间步输出一个预测，但这不是必须的。如果我们只对输入“dog”后面的字母感兴趣，我们可以只取t=3时的输出，而忽略其他的。</p><p id="a0ce" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">现在我们已经了解了如何用rnn进行预测，让我们来探索rnn如何学习进行<em class="mg">正确预测</em>。</p><h2 id="5e01" class="nz jx iq bd jy oa ob dn kc oc od dp kg lb oe of kk ld og oh ko lf oi oj ks iw bi translated">穿越时间的反向传播</h2><p id="a744" class="pw-post-body-paragraph lr ls iq kw b kx ky lt lu kz la lv lw lb lx ly lz ld ma mb mc lf md me mf lh ij bi translated">像它们的经典对应物(MLP)一样，rnn使用反向传播方法从序列训练数据中学习。由于权重的<strong class="kw ja"> <em class="mg">递归性质及其对时间跨度损失的影响，使用RNNs进行反向传播更具挑战性。</em> </strong>我们一会儿就明白这是什么意思了。</p><p id="7533" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">为了具体理解反向传播是如何工作的，让我们展示一下一般的工作流程:</p><ol class=""><li id="dae1" class="ku kv iq kw b kx mh kz mi lb mj ld mk lf ml lh oq lj lk ll bi translated">随机初始化权重矩阵<em class="mg"> Wx，Wy，Wh </em></li><li id="e86d" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh oq lj lk ll bi translated">向前传播以计算预测</li><li id="3f72" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh oq lj lk ll bi translated">计算损失</li><li id="5e29" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh oq lj lk ll bi translated">反向传播计算梯度</li><li id="1b51" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh oq lj lk ll bi translated">基于梯度更新权重</li><li id="511e" class="ku kv iq kw b kx lm kz ln lb lo ld lp lf lq lh oq lj lk ll bi translated">重复步骤2–5</li></ol><p id="3a9d" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated"><strong class="kw ja"/></p><p id="2438" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">因为这个例子是一个分类问题，我们试图预测四个可能的字母(“d-o-g-s”)，所以使用<strong class="kw ja">多类交叉熵损失函数</strong>是有意义的:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi or"><img src="../Images/e3c590ca8063dde1d1ab8e3e7b068479.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BdpRyrGzngk2Jlc8GF8h-Q.png"/></div></div></figure><p id="ec40" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">考虑到所有时间步骤，<strong class="kw ja">总损失为</strong>:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi or"><img src="../Images/8eaf89912dafc1e6ea79a22c2796a83b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RVv8Z2PoNYPN8hSaZyNFQA.png"/></div></div></figure><p id="2509" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">从视觉上，这可以看做:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi os"><img src="../Images/104f4fc945278844df559381ef992390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ytrqbNfSquJYVRGOmHHwA.png"/></div></div></figure><p id="56ff" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">给定损失函数，我们需要计算三个权重矩阵<em class="mg"> Wx、Wy、Wh、</em>的梯度，并用学习率η更新它们<em class="mg"> </em>。类似于正常的反向传播，梯度给我们一种损失如何相对于每个权重参数变化的感觉。我们用下面的等式更新权重以最小化损失:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ot"><img src="../Images/68495c02c68a3d0ea7e8205dca71d7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SrgOZCwVZ2LnN2wF94ATSA.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">其中i = x、y和h是3个权重矩阵的简写</figcaption></figure><p id="4f07" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">现在棘手的部分来了，计算Wx，Wy和Wh的梯度。我们将从计算<em class="mg"> Wy </em>的梯度开始，因为这是最简单的。如前所述，权重对损失的影响会随着时间而变化。<em class="mg"> Wy </em>的重量梯度如下:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ou"><img src="../Images/4bf0b1287ea837103a491a3f14e00f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sLvhzDZNWVDLn3e_lm_R5A.png"/></div></div></figure><p id="7ad1" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">这就是<em class="mg"> Wy </em>的渐变计算。希望如此，非常简单明了，<strong class="kw ja">主要思想是链式法则，并考虑每个时间步的损失。</strong></p><p id="ba7e" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">权重矩阵<em class="mg"> Wx </em>和W <em class="mg"> h </em>彼此类似，因此我们将只查看<em class="mg"> Wx </em>的梯度，并将<em class="mg"> Wh </em>留给您。计算<em class="mg"> Wx </em>最棘手的部分之一是对先前状态的递归依赖，如下图第(2)行所示。我们需要考虑当前误差相对于每个先前状态的导数，这在(3)中完成。最后，我们再次需要考虑每个时间步(4)的损失。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ov"><img src="../Images/73f075ae467223c3ed427a1c19cb533d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZLEhMi0tq46Lsyqfu39RQ.png"/></div></div></figure><p id="a432" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">这就是反向传播！一旦我们有了<em class="mg"> Wx、Wh和Wy </em>的梯度，我们照常更新它们，并继续反向传播工作流程。现在你知道了rnn是如何学习和预测的，让我们来看看一个主要的缺陷，然后结束这篇文章。</p><p id="62e0" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated"><strong class="kw ja"/></p><h1 id="ef37" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">一个主要问题是:渐变消失</h1><p id="f1fc" class="pw-post-body-paragraph lr ls iq kw b kx ky lt lu kz la lv lw lb lx ly lz ld ma mb mc lf md me mf lh ij bi translated">RNNs面临的一个问题，也是其他深度神经网络普遍存在的问题，就是<strong class="kw ja">消失梯度问题</strong>。消失梯度使得模型难以学习长期依赖性。例如，如果给一个RNN人这样一句话:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ow"><img src="../Images/fc71e619697ff86fe4dca5ccced20e92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RLVikpHXtJrqOGqYLo6uNg.png"/></div></div></figure><p id="218e" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">并且必须预测最后两个单词“german”和“shepherd”，RNN需要考虑输入“brown”、“black”和“dog”，这是描述德国牧羊犬的名词和形容词。然而，“棕色”这个词与“牧羊人”这个词相差甚远从前面看到的<em class="mg"> Wx </em>的梯度计算，我们可以把“shepherd”这个词的反向传播误差分解回“brown”，看看是什么样子的:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ox"><img src="../Images/6b628f24e0db93e6ec28c434c7254a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAsAcrO7fGyv2AnHL-ZdNw.png"/></div></div></figure><p id="3e95" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">对应于输入“shepherd”的状态相对于状态“brown”的偏导数本身实际上是一个链式法则，导致:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oy"><img src="../Images/e369d5d5cc20f21a857a04c4fb7f6327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqyG3rPFIJGt9l3Bnd8DGg.png"/></div></div></figure><p id="7ad2" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">那可是一大堆链式法则啊！这些梯度链很麻烦，因为如果小于1，它们会导致单词shepherd相对于单词brown的损失接近0，从而<em class="mg">消失</em>。这使得权重很难考虑出现在长序列开头的单词。因此，当进行前向传播时，单词“brown”可能对“shepherd”的预测没有任何影响，因为权重由于消失梯度而没有更新。这是RNNs的主要缺点之一。</p><p id="c6ba" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">然而，在rnn方面已经取得了进展，例如门控循环单元(GRUs)和长短期记忆(LSTMs ),它们已经能够处理消失梯度的问题。我们不会在这篇博文中涉及它们，但在将来，我会写关于GRUs和LSTMs以及它们如何处理渐变消失的问题。</p><p id="c98d" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">这篇博文到此为止。如果您有任何问题、意见或反馈，欢迎在下面评论。我希望你觉得这有用，感谢阅读！</p><h1 id="b2c1" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">参考</h1><p id="1591" class="pw-post-body-paragraph lr ls iq kw b kx ky lt lu kz la lv lw lb lx ly lz ld ma mb mc lf md me mf lh ij bi translated">[1]:吴恩达。<em class="mg">为什么序列模式</em>。<a class="ae oz" href="https://www.coursera.org/learn/nlp-sequence-models/lecture/0h7gT/why-sequence-models" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/NLP-sequence-models/lecture/0h7gT/why-sequence-models</a></p><p id="0fd6" class="pw-post-body-paragraph lr ls iq kw b kx mh lt lu kz mi lv lw lb mm ly lz ld mn mb mc lf mo me mf lh ij bi translated">[2]:陈刚。<em class="mg">带误差反向传播的递归神经网络温柔教程</em>。<a class="ae oz" href="https://arxiv.org/pdf/1610.02583.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1610.02583.pdf</a></p></div></div>    
</body>
</html>