<html>
<head>
<title>Understanding Abstractive Text Summarization from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始理解抽象文本摘要</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understanding-abstractive-text-summarization-from-scratch-baaf83d446b3?source=collection_archive---------0-----------------------#2020-08-16">https://pub.towardsai.net/understanding-abstractive-text-summarization-from-scratch-baaf83d446b3?source=collection_archive---------0-----------------------#2020-08-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c54f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="d846" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">CNN新闻摘要案例研究</h2></div><p id="c989" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Linkedin <a class="ae lk" href="http://linkedin.com/in/gundluru-chandrasekhar-499129196" rel="noopener ugc nofollow" target="_blank">简介</a> &amp; Github <a class="ae lk" href="https://github.com/Chandugundluru?tab=repositories" rel="noopener ugc nofollow" target="_blank">简介</a>代码实现。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/d921219d7302ee4a1bf7b1996bd32c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5y7Z6Xp55uXzA2Zc.jpeg"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">照片由<a class="ae lk" href="https://unsplash.com/@haydenwalker?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">海登·沃克</a>在<a class="ae lk" href="https://unsplash.com/s/photos/newspaper?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="bca2" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">目录:</h1><ol class=""><li id="59ab" class="mt mu iq kq b kr mv ku mw kx mx lb my lf mz lj na nb nc nd bi translated">文本摘要介绍</li><li id="2648" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">了解CNN数据集</li><li id="952d" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">数据清理</li><li id="5c9f" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">文本数据分析</li><li id="6fa2" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">评估指标</li><li id="a6f5" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">数据预处理</li><li id="41b8" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">基线模型</li><li id="0656" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">注意力层</li><li id="0647" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">覆盖机制</li><li id="1701" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">预训练伯特</li><li id="bb4a" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">微调T5</li><li id="7c94" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">模型分析</li><li id="eb74" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">结论</li><li id="9475" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">参考</li></ol></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="e79d" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated"><strong class="ak"> 1。文本摘要介绍。</strong></h2><blockquote class="ob oc od"><p id="ed44" class="ko kp oe kq b kr ks ka kt ku kv kd kw of ky kz la og lc ld le oh lg lh li lj ij bi translated"><em class="iq">自动摘要定义:自动摘要是通过计算缩短一组数据的过程，以创建代表原始内容中最重要或最相关信息的子集。</em></p></blockquote><p id="bf14" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">文本摘要分为两种类型——<strong class="kq ja">抽取式</strong>和<strong class="kq ja">抽象式摘要</strong>。</p><ol class=""><li id="f092" class="mt mu iq kq b kr ks ku kv kx oi lb oj lf ok lj na nb nc nd bi translated"><strong class="kq ja">提取摘要:</strong>提取文本摘要过程提取文本的要点，而不对这些要点进行任何修改，并重新排列这些要点的顺序和语法，以从摘要中获取精华。</li><li id="3cd4" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">抽象概括:抽象方法使用先进的技术得到一个全新的概括。该摘要的某些部分甚至可能不会出现在原文中。</li></ol></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="0c47" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated"><strong class="ak"> 2。了解CNN数据集</strong></h2><p id="2c04" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated"><a class="ae lk" href="https://cs.nyu.edu/~kcho/DMQA/" rel="noopener ugc nofollow" target="_blank">(来源:https://cs.nyu.edu/~kcho/DMQA/)</a></p><p id="b321" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该数据集包含来自CNN新闻文章的文档。大约有9万份文件。每篇文章都包含一个故事及其亮点。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="f5e3" class="nq mc iq op b gy ot ou l ov ow">file = open('Data/cnn/stories' + '/' + '000c835555db62e319854d9f8912061cdca1893e.story', encoding=utf-8)text = file.read() <br/>file.close() <br/>text</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/bad7ce9230f82c3e1ddcc404bbc4f34a.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*sBz-vNy677SKc00xpfaQfw.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">原始文章</figcaption></figure></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="6f56" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated">3.<strong class="ak">数据清理</strong></h2><p id="0784" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated">我们得到的故事是完全混乱和不干净的文本，有许多不想要的字符和符号。所以我们必须清理数据，并从故事文章中提取摘要</p><p id="f2aa" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> a)从文章中提取摘要:</em> </strong></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="eaca" class="nq mc iq op b gy ot ou l ov ow">def loading_articles(file_name): <br/>    file = open(file_name, encoding='utf-8')  <br/>    text = file.read()  <br/>    file.close()  <br/>    return textdef split_story(doc):  <br/>    index = doc.find('@highlight')<br/>    story, highlights = doc[:index],<br/>    doc[index:].split('@highlight') <br/>    highlights = [h.strip() for h in highlights if len(h) &gt; 0]         <br/>    return story, highlights</span></pre><p id="e244" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总结在文章中标注在“@highlight”之后。上述功能将从文章中提取一个摘要，还将故事文章和摘要分开</p><p id="893b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> b)扩展收缩词:</em> </strong></p><p id="ebaf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">缩略词是由两个词缩短或组合而成的词。像不可以(可以+不可以)、不要(做+不可以)、我已经(我+有)这些词都是缩写。作为清理步骤，我们用下面的函数扩展所有这些收缩。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="0363" class="nq mc iq op b gy ot ou l ov ow">def decontracted(phrase): <br/>    phrase = re.sub(r"won't", "will not", phrase) <br/>    phrase = re.sub(r"can\'t", "can not", phrase)  <br/>    phrase = re.sub(r"n\'t", " not", phrase)  <br/>    phrase = re.sub(r"\'re", " are", phrase)  <br/>    phrase = re.sub(r"\'s", " is", phrase)  <br/>    phrase = re.sub(r"\'d", " would", phrase)  <br/>    phrase = re.sub(r"\'ll", " will", phrase)  <br/>    phrase = re.sub(r"\'t", " not", phrase)  <br/>    phrase = re.sub(r"\'ve", " have", phrase)  <br/>    phrase = re.sub(r"\'m", " am", phrase)  <br/>    return phrase</span></pre><p id="7a09" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> c)去除不想要的符号:</em> </strong></p><p id="70c9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Token '(CNN)'出现在每篇毫无意义的文章开头。以便从文档中移除标记以及'$%^&amp;*#'符号。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="8eb2" class="nq mc iq op b gy ot ou l ov ow">article_text=[]for i in CNN.article.values:tt=re.sub(r'\n',' ', i)<br/>    tt=re.sub(r"([?!¿])", r" \1 ", tt)<br/>    tt=decontracted(tt)<br/>    tt = re.sub('[^A-Za-z0-9.,]+', ' ', tt)<br/>    tt = tt.lower()<br/>article_text.append(tt)</span></pre><p id="80fc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在清除所有的文本后，它被转换成小写。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="efd4" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated"><strong class="ak"> 4。文本数据分析</strong></h2><p id="7b70" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> a)文章长度:</em> </strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/eab024c5cd1808125b67b8ab75dc5005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/0*y1XESn0djBrUJErI.png"/></div></figure><p id="19bd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">文章长度分布</p><p id="de82" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">大多数文章的长度为800，分布看起来是右偏的，让我们检查90–99之间的长度百分位值</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="5b66" class="nq mc iq op b gy ot ou l ov ow">import numpy as np <br/>b = [i for i in range(90,100)] <br/>for i in b:  <br/>    print(i,'th percentile is ', np.percentile(art_len, i))</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/33662a5fb6aaf0826bf9194a1267bf58.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/0*mZsu4Fy6VXdf6L6A.png"/></div></figure><h2 id="3396" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated">第95百分位文章长度置信区间</h2><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="e012" class="nq mc iq op b gy ot ou l ov ow">percentiles_95=[]<br/>for i in range(0,200):<br/>    samples=sample(summ_len,500)<br/>    k=np.percentile(samples, 95)<br/>    percentiles_95.append(k)<br/>    mean = np.round(mean(percentiles_95),3)<br/>    std = np.round(stdev(percentiles_95),3)<br/>    left_limit = np.round(mean - 2*(std/np.sqrt(sample_size)), 3)     <br/>    right_limit = np.round(mean + 2*(std/np.sqrt(sample_size)), 3)   print("95% of CI for MSE=",[left_limit,right_limit])</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/213ae0beeb8efeda329efa2905746882.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*lV-2C5nrFzF8neNo"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/8beaf3f575ec3d166631452c57c8fecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/0*OGnwOgKWpRtvCnEQ.png"/></div></figure><p id="61a4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> b)汇总长度:</em> </strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/93ce6e2d24ceff6bf8aa377f9df8418a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/0*0xnl0-cJtq5ywO8k.png"/></div></figure><p id="59ad" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">概要长度分布</p><p id="fb2f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">看起来大部分摘要都有35-55个单词。看起来很有趣..让我们探索百分位值。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/4905f6cda60961259e7d165911e29fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/0*Xm_jN1fMKuIPhojd.png"/></div></figure><h2 id="fbb2" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated">第95百分位汇总长度置信区间</h2><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="9973" class="nq mc iq op b gy ot ou l ov ow">percentiles_95=[]<br/>for i in range(0,200):<br/>    samples=sample(summ_len,500)<br/>    k=np.percentile(samples, 95)<br/>    percentiles_95.append(k)<br/>    mean = np.round(mean(percentiles_95),3)<br/>    std = np.round(stdev(percentiles_95),3)<br/>    left_limit = np.round(mean - 2*(std/np.sqrt(sample_size)), 3)     <br/>    right_limit = np.round(mean + 2*(std/np.sqrt(sample_size)), 3)   print("95% of CI for MSE=",[left_limit,right_limit])</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/db652715c0dc73d3b6450c89f8e14afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*52rSm35gTC_BmQVz"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/1e95aa828a653536ec2012fc117307a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/0*rVHLGBvwwcwpz1gj.png"/></div></figure><p id="144f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们再深入挖掘一下，找出哪些词在总结中出现的频率更高。</p><p id="c4b7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> c)词云为总结词:</em> </strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/291bc7363afbba8948732ad8aeffb465.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/0*CxxxR_PJyfxEVlYa.png"/></div></figure><p id="877a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里每个单词的大小表明了它的频率或重要性。重要的文本数据点可以使用单词云突出显示。</p><p id="2e6b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们在这里可以观察到一个有趣的现象，就是像‘say’，‘said’，‘will’，‘may’这样的动词在总结中出现的频率更高。</p><p id="af33" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于CNN的文章是以美国新闻为基础的，像“美国”、“纽约”、“美国”这样的词在云中有着重要的意义。</p><p id="020e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> d)词性标注到摘要和文章:</em> </strong></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="3f64" class="nq mc iq op b gy ot ou l ov ow">import spacy<br/>sum_pos=[]<br/>nlp = spacy.load("en_core_web_lg")<br/>for i in tqdm(data_cleaned.Summary.values):<br/>    pos_tag=[]<br/>    doc = nlp(i)<br/>    for token in doc:<br/>        pos_tag.append(token.pos_)<br/>    sum_pos.append(pos_tag)</span></pre><p id="819a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="oe">名词百分比分布</em></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="3cf0" class="nq mc iq op b gy ot ou l ov ow">noun_percent=[]<br/>for i in sum_pos:<br/>    lent = i.count('NOUN')<br/>    lentl = i.count('PROPN')<br/>    noun=((lent+lentl)/len(i))*100<br/>    noun_percent.append(noun)plt.figure(figsize=(20,8))sns.distplot(noun_percent,color='red');</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/b8f59a454e5625c57e91b251bd7b9fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*Tkb5HMQSk6N9PDKs.png"/></div></figure><p id="4238" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">名词百分比分布汇总和文章</p><p id="a048" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="oe">动词百分比分布</em></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="74e8" class="nq mc iq op b gy ot ou l ov ow">verb_percent=[]for i in sum_pos:<br/>    lent = i.count('VERB')<br/>    verb=(lent/len(i))*100<br/>    verb_percent.append(verb)<br/>plt.figure(figsize=(20,8))<br/>sns.distplot(verb_percent,color='red');</span></pre><div class="lm ln lo lp gt ab cb"><figure class="ph lq pi pj pk pl pm paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/b8a1f78ac3dfc1f1fd80b4869b7b3efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/0*mJWS4Rh_uAS1WPzT.png"/></div></figure><figure class="ph lq pn pj pk pl pm paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/103903f1697af20f8780621a5663247d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/0*Tl7YjgtktoW0rvoX.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk po di pp pq translated">动词在摘要和文章中的百分比分布</figcaption></figure></div><p id="954e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> e)命名_实体标注:</em> </strong></p><p id="9fa5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">命名实体是真实世界的对象，例如人、地点、组织、产品等。，可以用一个合适的名字来表示。它可以是抽象的，也可以是有形的存在。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="a2de" class="nq mc iq op b gy ot ou l ov ow">import spacy<br/>from spacy import displacy<br/>from IPython.core.display import display, HTML<br/>nlp = spacy.load("en_core_web_lg")<br/>doc2 = nlp(data_cleaned.Article.values[2])<br/>displacy.render(doc2, style="ent", jupyter=True)</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi pr"><img src="../Images/f979a51df96a3e7d0e9f55f86511d5f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YzMsQgW3eGzR2OMc.png"/></div></div></figure><p id="e2ce" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">带有命名实体的文章</p><p id="f679" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> f)从分析中观察:</em> </strong></p><p id="1ad7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">1)每个摘要平均有30%的名词和15%的动词。</p><p id="e389" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2)从命名实体的可视化来看，我们可以说有很大概率会把一个包含ORG实体的句子当作摘要。由于文章只有两个“组织”实体，因此在摘要中选择了“GSA”组织实体。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="164e" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated"><strong class="ak"> 5。评估指标</strong></h2><p id="4dd0" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated">摘要是一个棘手的问题，因为系统必须理解文本的要点。这需要使用单词知识对内容进行语义分析和分组。</p><p id="54c3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">用于总结结果的两个通用评估指标是<strong class="kq ja"> BLEU </strong>和<strong class="kq ja"> ROGUE </strong></p><p id="128d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> a) BLEU Score </strong>是<strong class="kq ja"> </strong>用来发现从一种语言预测到另一种语言的文本质量。</p><blockquote class="ps"><p id="27b9" class="pt pu iq bd pv pw px py pz qa qb lj dk translated">BLEU代表双语评估替角</p></blockquote><p id="8a43" class="pw-post-body-paragraph ko kp iq kq b kr qc ka kt ku qd kd kw kx qe kz la lb qf ld le lf qg lh li lj ij bi translated">BLEU分数范围从0到1。如果预测和原始文本是一个相似的分数接近1，反之亦然。</p><p id="1016" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">胭脂是什么？</strong></p><p id="f667" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了评估生成的摘要的好坏，文本摘要空间中的常用度量被称为Rouge score。</p><blockquote class="ps"><p id="3adf" class="pt pu iq bd pv pw px py pz qa qb lj dk translated">ROUGE代表面向回忆的替角，用于Gisting评估。</p></blockquote><p id="9be4" class="pw-post-body-paragraph ko kp iq kq b kr qc ka kt ku qd kd kw kx qe kz la lb qf ld le lf qg lh li lj ij bi translated">它通过将自动生成的摘要或翻译与一组参考摘要(通常是人工生成的)进行比较来工作。它通过匹配生成的和参考摘要的n元语法的重叠来工作。</p><p id="e3f8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> ROUGE-1 </strong>指系统和参考摘要之间的单字(每个单词)重叠。</p><p id="cdb5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> ROUGE-2 </strong>指系统和参考汇总之间二元模型的重叠。</p><p id="1c57" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> ROUGE-L </strong>:基于最长公共子序列(LCS)的统计。最长公共子序列问题自然地考虑了句子级结构的相似性，并自动识别序列n元文法中最长的共现。</p><ul class=""><li id="ce7d" class="mt mu iq kq b kr ks ku kv kx oi lb oj lf ok lj qh nb nc nd bi translated">ROUGE-n recall=40%意味着<em class="oe">引用</em>摘要中40%的n元文法也存在于<em class="oe">生成的</em>摘要中。</li></ul><p id="8f42" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于这个模型，我使用rogue分数作为评估指标</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="9811" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated">6。数据预处理</h2><p id="bd89" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> a)替换文本中的实体:</em> </strong></p><p id="a98d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">命名实体是真实世界的对象，例如人、地点、组织、产品等。，可以用一个合适的名字来表示。它可以是抽象的，也可以是有形的存在。如果我们认为他们是词汇感知的，他们中的大多数被归类为生僻字。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="5119" class="nq mc iq op b gy ot ou l ov ow">for i in range(len(cleaned_text)):<br/>    doc1 = nlp(cleaned_text[i])<br/>    c=(" ".join([t.text if not t.ent_type_ else t.ent_type_ for t in    doc1]))<br/>    c=c.lower()<br/>    short_text.append(c)doc2 = nlp(cleaned_summary[i])<br/>    k=(" ".join([t.text if not t.ent_type_ else t.ent_type_ for t in    doc2]))<br/>    k=k.lower()<br/>    short_summary.append(k)</span></pre><p id="9625" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设有一篇关于人名“张克帆·诺兰”的文章，如果我们在其他文章中没有再看到这个名字，那么文章模型可以假设这些名字是罕见的单词。但是这个词概括起来很重要。所以我们正在替换文本中的实体。</p><blockquote class="ps"><p id="7fa1" class="pt pu iq bd pv pw px py pz qa qb lj dk translated">例如:“张克帆·诺兰在网飞执导的网络系列”转换为“gpe中由个人执导的网络系列”</p></blockquote><p id="9cc0" class="pw-post-body-paragraph ko kp iq kq b kr qc ka kt ku qd kd kw kx qe kz la lb qf ld le lf qg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> b)处理OOV的话:</em> </strong></p><p id="d8e9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">出现在训练数据中而在测试数据中缺失的单词被认为是词汇表之外的单词。</p><blockquote class="ps"><p id="182d" class="pt pu iq bd pv pw px py pz qa qb lj dk translated">Keras Tokenizer(oov_token='ukn ')怎么了？</p></blockquote><p id="5dd3" class="pw-post-body-paragraph ko kp iq kq b kr qc ka kt ku qd kd kw kx qe kz la lb qf ld le lf qg lh li lj ij bi translated">如果我们在tokenizer中使用oov_token='ukn ',它将替换训练vocab中缺少的所有测试令牌。测试时，我们给未训练的模型嵌入，因为“unk”没有被训练。这将显著影响模型性能。</p><p id="a839" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，我们必须在训练中包含“ukn”标记，因为我正在考虑将训练词汇中的生僻字作为“ukn”。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="6c7b" class="nq mc iq op b gy ot ou l ov ow">thresh=2<br/>rare_word=[]for key,value in y_tokenizer.word_counts.items():<br/>    if(value&lt;thresh):<br/>    rare_word.append(key)</span></pre><p id="ec73" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里我认为我的阈值是2。这表明如果语料库中的词频小于2，则将这些词作为稀有词追加。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="88eb" class="nq mc iq op b gy ot ou l ov ow">tokenrare=[]<br/>for i in range(len(rare_word)):<br/>    tokenrare.append('ukn')<br/>dictionary_1 = dict(zip(rare_word,tokenrare))<br/>y_trunk=[]<br/>for i in y_train:<br/>    for word in i.split():<br/>        if word.lower() in dictionary_1:<br/>            i = i.replace(word, dictionary_1[word.lower()])<br/>     y_trunk.append(i)</span></pre><p id="c5f8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通过使用这个代码片段，我用“unk”替换了train文档中的生僻字。这个过程有助于更好地嵌入训练数据中的生僻字以及测试数据中的“未知”字符。</p><p id="d51f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> c)分词和填充:</em> </strong></p><p id="9636" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们使用Keras分词器来分词。创建标记器后，我们将它用于训练数据，然后我们将使用它来拟合测试数据。</p><p id="2b65" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，我们需要做填充，因为文本中的每个句子都没有相同的字数，我们也可以定义每个句子的最大字数，如果一个句子较长，那么我们可以删除一些单词。下面是填充线，如下图所示:</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="8f55" class="nq mc iq op b gy ot ou l ov ow">y_tokenizer = Tokenizer(oov_token='ukn')<br/>y_tokenizer.fit_on_texts(list(y_trunk)) <br/>y_tr_seq = y_tokenizer.texts_to_sequences(y_trunk)y_val_seq = y_tokenizer.texts_to_sequences(y_validatioin)zero upto maximum lengthy_tr = pad_sequences(y_tr_seq,maxlen=max_summary_len, padding='post')y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')y_voc = len(y_tokenizer.word_index) +1</span></pre><p id="ab6f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe"> d)字嵌入:</em> </strong></p><p id="c40d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">单词嵌入是将单个单词解释为预定义向量空间中的实值向量的技术。每个单词被映射到一个向量，向量值以类似于神经网络的方式被学习。</p><p id="95a9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这些单词嵌入是对文本的学习表示，其中具有相同含义的单词具有相似的表示。正是这种表示单词和文档的方法可以被认为是深度学习在挑战自然语言处理问题上的关键突破之一。</p><p id="3472" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这种方法的关键是对每个单词使用密集分布的表示法。</p><p id="0ff5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里我使用预先训练的100个手套向量。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="b43c" class="nq mc iq op b gy ot ou l ov ow">embeddings_dictionary = dict()glove_file = open("/content/gdrive/My Drive/glove.6B.100d.txt", encoding="utf8")for line in glove_file:  <br/>    records = line.split()  <br/>    word = records[0]  <br/>    vector_dimensions = np.asarray(records[1:], dtype='float32')               <br/>    embeddings_dictionary [word] = vector_dimensions   glove_file.close()</span></pre><h2 id="6fab" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated"><strong class="ak"> 7。基准模型</strong></h2><p id="bc18" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated">我使用了一个简单的<strong class="kq ja">编码器-解码器</strong>模型作为基线模型，这是我使用的架构。</p><div class="lm ln lo lp gt ab cb"><figure class="ph lq qi pj pk pl pm paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/c658412ce742ec26abace51150bac710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*029qI1Fp3gvr5_m2.png"/></div></figure><figure class="ph lq qj pj pk pl pm paragraph-image"><img src="../Images/bb74f7404a0416372a736f0b099a5127.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/0*FQfPMwzxfVFO8RwH.png"/><figcaption class="lx ly gj gh gi lz ma bd b be z dk qk di ql pq translated">来自<a class="ae lk" href="http://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">应用人工智能课程</a></figcaption></figure></div><p id="7c77" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe">编码器</em> </strong></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="c938" class="nq mc iq op b gy ot ou l ov ow">encoder_inputs = Input(shape=(max_text_len,))enc_emb = Embedding(x_voc+1, 50,mask_zero=True, weights=[embedding_matrix_x] ,trainable=True)encoder = Bidirectional(LSTM(64, return_state=True))encoder_outputs, forward_h, forward_c, backward_h,  backward_c=encoder(enc_emb)state_h = Concatenate()([forward_h, backward_h])state_c = Concatenate()([forward_c, backward_c])</span></pre><ul class=""><li id="5f42" class="mt mu iq kq b kr ks ku kv kx oi lb oj lf ok lj qh nb nc nd bi translated">对于编码器，我们可以使用LSTM/GRU细胞在这里我使用双向LSTM作为编码器。它返回编码器输出、前向h、前向c、后向h、后向c。</li><li id="362a" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj qh nb nc nd bi translated">然后连接前向和后向状态以及将作为初始状态提供给解码器单元输出。</li><li id="9b26" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj qh nb nc nd bi translated">编码器接收输入序列，并将信息封装为内部状态向量。</li></ul><p id="7d7d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe">解码器</em> </strong></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="524a" class="nq mc iq op b gy ot ou l ov ow">decoder_inputs = Input(shape=(max_sum_len,)) #embedding layerdec_emb_layer = Embedding(y_voc+1, 50,mask_zero=True, weights=[embedding_matrix_y])dec_emb = dec_emb_layer(decoder_inputs)decoder_lstm = LSTM(128, return_sequences=True, return_state=True)decoder_output,decoder_state_h, decoder_state_c = decoder_lstm(dec_emb,initial_state=[state_h,state_c])dense layer decoder_dense = TimeDistributed(Dense(y_voc+1, activation='softmax'))decoder_outputs = decoder_dense(decoder_output)model1 = Model([encoder_inputs, decoder_inputs], decoder_outputs)</span></pre><p id="db90" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于解码器，我们给出摘要作为输入，并使用教师强制技术来训练模型。</p><p id="e789" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如解码器结构所示，我们的第一个时间戳输入是解码器LSTM单元的“开始”(y1)输出，给定大小摘要vocab的密集层。</p><p id="4f1a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后将softmax应用于其结果以给出vocab的概率分布。我们可以挑选概率最高的词作为摘要中的下一个词(比如y2)。这个y2是LSTM的下一个时间戳的输入。</p><p id="84c2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe">预测汇总:</em> </strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/9f89106a14a4493fe1cd01a30901fb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/0*fa5_Ec-vcd-spD-Z.png"/></div></figure><p id="58fc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">很明显，该模型不能清楚地预测摘要和重复相同的单词。现在让我们通过添加注意力层来检查这个模型。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="11ab" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated"><strong class="ak"> 8。关注层</strong></h2><blockquote class="ob oc od"><p id="db04" class="ko kp oe kq b kr ks ka kt ku kv kd kw of ky kz la og lc ld le oh lg lh li lj ij bi translated"><strong class="kq ja"> <em class="iq">为什么关注？</em>T9】</strong></p></blockquote><p id="bdc2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">标准的seq2seq模型通常不能准确地处理长输入序列，因为只有编码器RNN的最后隐藏状态被用作解码器的上下文向量。另一方面，注意力机制直接解决了这个问题，因为它在解码过程中保留并利用了输入序列的所有隐藏状态。这是通过在解码器输出的每个时间步长与所有编码器隐藏状态之间创建唯一映射来实现的。这意味着，对于解码器产生的每个输出，它可以访问整个输入序列，并可以有选择地从该序列中挑选出特定的元素来产生输出。</p><p id="34a9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，该机制允许模型根据需要将更多的“注意力”集中在输入序列的相关部分上。</p><p id="82b2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="oe">理解注意机制</em> </strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qn"><img src="../Images/a338abeee9cc350a9eb92be010f03c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H2iMhWA7IDzCzcyF.png"/></div></div></figure><p id="1617" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当我们想到英文单词“Attention”时，我们知道它的意思是将你的注意力集中在某件事情上，并给予更多的注意。深度学习中的注意机制是基于这种引导你的焦点的概念，它在处理数据时更加关注某些因素。</p><p id="778b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们将使用基线模型中相同的编码器来为每个传入的输入产生隐藏状态/输出。我们将把编码器产生的所有隐藏状态带到下一步，而不是在最后一个时间步只使用隐藏状态。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="58d9" class="nq mc iq op b gy ot ou l ov ow">class additiveAttention(tf.keras.layers.AdditiveAttention): <br/>    def __init__(self,units):  <br/>    super(additiveAttention,self).__init__()  <br/>    self.units = units   <br/>    self.W1 = Dense(units)  <br/>    self.W2 = Dense(units)  <br/>    self.V = Dense(1)  <br/>    @tf.function  <br/>    <br/>    def call(self, keys):   <br/>        query=keys[0]  <br/>        values=keys[1]  <br/>        ht_with_time_axis = tf.expand_dims(values, axis=1)score = self.V(tf.nn.tanh(self.W1(ht_with_time_axis) +      self.W2(query)))  <br/>        attention_weights = tf.nn.softmax(score, axis=1)    <br/>        context_vector = attention_weights * query  <br/>        context_vector = tf.reduce_sum(context_vector, axis=1)return context_vector, attention_weights</span></pre><h1 id="28d9" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">计算校准分数:</h1><p id="2d5b" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated">为了计算解码器时间步长t处的对齐分数，我们将考虑(t-1)的解码器隐藏状态和编码器隐藏状态。</p><p id="0584" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从上面的代码</p><p id="19d9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">查询形状(编码器输出)==(批量大小，最大长度，隐藏大小)值形状(解码器隐藏在(t-1)) ==(批量大小，隐藏大小)</p><blockquote class="ps"><p id="1520" class="pt pu iq bd pv pw px py pz qa qb lj dk translated">scorealignment=wcombined⋅tanh(wdecoder⋅hdecoder+wencoder⋅hencoder)</p></blockquote><h1 id="cb15" class="mb mc iq bd md me mf mg mh mi mj mk ml kf qo kg mn ki qp kj mp kl qq km mr ms bi translated">上下文向量:</h1><blockquote class="ps"><p id="0f5f" class="pt pu iq bd pv pw px py pz qa qb lj dk translated">Attention_weights=softmax(得分对齐)</p><p id="c531" class="pt pu iq bd pv pw px py pz qa qb lj dk translated">context _ vector = Attention _ weights * query</p></blockquote><p id="c639" class="pw-post-body-paragraph ko kp iq kq b kr qc ka kt ku qd kd kw kx qe kz la lb qf ld le lf qg lh li lj ij bi translated">我们产生的上下文向量然后将与先前的解码器输出连接。然后，它被送入解码器RNN单元，以产生一个新的隐藏状态，并重复这一过程。时间步长的最终输出是通过将新的隐藏状态传递到密集层来获得的，密集层充当分类器来给出下一个预测单词的概率分数。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qr"><img src="../Images/5fd4ae1d99c6ca80df53cdca583f18e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DXsE2xYXkPpzVtxM.png"/></div></div></figure><p id="9b9d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们的注意力模型试图预测单词，但整体预测的摘要没有任何意义，许多单词都在重复。为了避免单词重复，我在我的模型中添加了覆盖机制。让我们来看看它是如何工作的。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="b18c" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated"><strong class="ak"> 9。覆盖机制</strong></h2><p id="5274" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated">重复是序列到序列模型的常见问题，在生成多句子文本时尤其明显。</p><p id="77e7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这种覆盖机制中，我们使用覆盖向量ct，它是所有先前解码器时间步长上的注意力分布的总和。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/2ecb020413bf271ca7b716995b5b9253.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/0*EzBSR4UMv-hawPEI.png"/></div></figure><p id="df64" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">直观上，ct是源文档单词上的分布，它表示这些单词迄今为止从注意机制接收到的覆盖程度。注意，c0是一个零向量，因为在第一个时间步长上，没有覆盖任何源文档</p><p id="9d95" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们将这个覆盖向量添加到注意力机制中，同时找到对齐分数，正如我们在上一节中所看到的。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi qt"><img src="../Images/dc7b70f032c198828544e1879382b685.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/0*PbcKdPTgj-iEKFhB.png"/></div></figure><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="efc1" class="nq mc iq op b gy ot ou l ov ow">class additiveAttention(tf.keras.layers.AdditiveAttention):  <br/>    def __init__(self, hidden_units,is_coverage=False):   <br/>        super().__init__()   <br/>        self.Wh = tf.keras.layers.Dense(hidden_units) <br/>        self.Ws = tf.keras.layers.Dense(hidden_units)<br/>        self.wc = tf.keras.layers.Dense(1)<br/>        self.V = tf.keras.layers.Dense(1)  <br/>        self.coverage = is_coverage  <br/>        <br/>        if self.coverage is False:  <br/>             self.wc.trainable = Falsedef call(self,keys):    <br/>        value=keys[0]  <br/>        query=keys[1]  <br/>        ct=keys[2]    <br/>        value = tf.expand_dims(value, 1)<br/>        ct = tf.expand_dims(ct, 1)score = self.V(tf.nn.tanh(  self.Wh(query) +  self.Ws(value) +  self.wc(ct)  ))attention_weights = tf.nn.softmax(score, axis=1)ct = tf.squeeze(ct,1) <br/>        if self.coverage is True:  <br/>            ct+=tf.squeeze(attention_weights)context_vector = attention_weights * query<br/>        context_vector = tf.reduce_sum(context_vector, axis=1)<br/> <br/>        return context_vector, attention_weights, ct</span></pre><p id="5bf7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中，wc是一个长度与v相同的可学习参数向量。这确保了注意力机制的当前决策(选择接下来在哪里出席)被其先前决策的提醒所通知(在c . t中总结)。这将使注意力机制更容易避免重复注意相同的位置，从而避免产生重复的文本。</p><p id="0697" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">重复去同一个地方也有保险损失。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi qu"><img src="../Images/f6a6761802a110e640f1a0b487307493.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/0*p3477VHOaDhCnI8X.png"/></div></figure><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="89c7" class="nq mc iq op b gy ot ou l ov ow">def coverage_loss(attention_weights,coverage_vector,target): <br/>    mask = tf.math.logical_not(tf.math.equal(target, 0))  <br/>    coverage_vector = tf.expand_dims(coverage_vector,axis=2)ct_min=tf.reduce_min(tf.concat<br/>    ([attention_weights,coverage_vector],axis=2),axis=2)cov_loss = tf.reduce_sum(ct_min,axis=1)  <br/>    mask = tf.cast(mask, dtype=cov_loss.dtype)  <br/>    cov_loss *= mask     <br/>    <br/>    return cov_loss</span></pre><p id="43a3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们检查一下我们的摘要是如何生成的</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/0a7136dbeeebb497f8af148e578d3527.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/0*XFAPmjQqOmgYXdJB.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">来自覆盖模型的摘要</figcaption></figure><p id="856b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这些结果比由序列到序列和注意机制生成的摘要更有意义。</p><p id="45e8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">流氓评分与序列序列+关注+覆盖机制</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi qw"><img src="../Images/25fbd388367f55850a4c753a07ae233d.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/0*1qCuDGJPzQ7y7zIY.png"/></div></figure></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="93fa" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated"><strong class="ak"> 10。预训练的BERT </strong></h2><p id="e360" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated">来自变压器的双向编码器表示(BERT)推进了广泛的自然语言处理任务。在描述该模型的论文发布后不久，该团队还开源了该模型的代码，并提供了已经在大规模数据集上进行了预训练的模型下载版本。这是一个重大的发展，因为它使任何人都能够建立一个涉及语言处理的机器学习模型，以此作为一个随时可用的组件，节省了从头训练语言处理模型所需的时间、精力、知识和资源，现在它可以有效地应用于提取和抽象模型的文本摘要。</p><p id="afc8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">关于伯特的更多细节，请查看这篇写得很漂亮的博客。这里我假设你熟悉伯特。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi qx"><img src="../Images/b75c6c241c00f5b15ebb5fa4b640bece.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nwOZP5NAmymVEAA9.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">https://arxiv.org/pdf/1908.08345.pdf<a class="ae lk" href="https://arxiv.org/pdf/1908.08345.pdf" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="46f2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">伯特是如何接受总结训练的</strong></p><blockquote class="ob oc od"><p id="9672" class="ko kp oe kq b kr ks ka kt ku kv kd kw of ky kz la og lc ld le oh lg lh li lj ij bi translated"><strong class="kq ja"> <em class="iq">伯特森:</em> </strong></p></blockquote><p id="7505" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们知道摘要是从文档中选择重要的句子。考虑在包含句子[sent1，sent2，...，sentm]的文档1上实现摘要的任务。</p><p id="f2e7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们可以假设抽取摘要是二元分类的任务，给每个句子分配一个标签，不管该句子是否应该包括在摘要中。假设摘要句子代表了文档中最重要的内容。</p><p id="0a61" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">来自顶层的[CLS]符号的向量可以用作句子的表示。</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="bf15" class="nq mc iq op b gy ot ou l ov ow">yˆi = σ(W h + bo)</span></pre><p id="eeed" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中‘h’是来自变换器顶层的句子的[CLS]向量。实验中发现L = 1，2，3的变压器，发现L = 2的变压器表现最好。模型的损失是预测yˇI对真实标签yi的二元分类熵。这款车型被命名为<strong class="kq ja"> BERTSUM。</strong></p><blockquote class="ob oc od"><p id="b625" class="ko kp oe kq b kr ks ka kt ku kv kd kw of ky kz la og lc ld le oh lg lh li lj ij bi translated"><strong class="kq ja"><em class="iq"/></strong></p></blockquote><p id="e537" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">BERTSUMABS被训练使用标准的编码器-解码器框架进行抽象概括。这里，编码器是预先训练的BERTSUM，解码器是从头开始训练的6层变换器。</p><p id="46f0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">可以相信，编码器和解码器之间存在不匹配，因为BERTSUM是预先训练的，而解码器必须从头开始训练。这会使微调变得不确定。编码器可能会过拟合数据，而解码器则欠拟合，反之亦然。</p><p id="8a76" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了避免这种情况，BERTSUMABS为编码器和解码器使用了两个独立的优化器。</p><p id="a5bf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">除了这两种策略之外，还有一种两阶段微调方法，其中<strong class="kq ja"> BERTSUMEXTABS </strong>首先在提取摘要任务上微调编码器，然后在抽象摘要任务上微调它。因为使用提取意图可以提高抽象概括的性能。</p><p id="274e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">下载CNN每日邮报数据上的预训练模型</strong></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="81f3" class="nq mc iq op b gy ot ou l ov ow">! gdown <a class="ae lk" href="https://drive.google.com/uc?id=1-IKVCtc4Q-BdZpjXc4s70_fRsWnjtYLr&amp;export=download" rel="noopener ugc nofollow" target="_blank">https://drive.google.com/uc?id=1-IKVCtc4Q-BdZpjXc4s70_fRsWnjtYLr&amp;export=download</a> #CNN/DM Abstractive model_step_148000.pt</span></pre><p id="19af" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">克隆git回购</strong></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="1a29" class="nq mc iq op b gy ot ou l ov ow">!git clone <a class="ae lk" href="https://github.com/mingchen62/PreSumm.git" rel="noopener ugc nofollow" target="_blank">https://github.com/mingchen62/PreSumm.git</a></span></pre><p id="e50d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">生成汇总</strong></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="2e05" class="nq mc iq op b gy ot ou l ov ow">!python summarizer.py -task abs -mode test  -test_from models/CNN_DailyMail_Abstractive/model_step_148000.pt \  -batch_size 6 -test_batch_size 6 -bert_data_path bert_data/cnndm \  -log_file $log_file -report_rouge False \  -sep_optim true -use_interval true \  -visible_gpus -1 -max_pos 512 \  -max_src_nsents 100 -max_length 200 \  -alpha 0.95 -min_length 50 \  -result_path $result_path \</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi qy"><img src="../Images/ea86cbd8ee2b80b946c6193dda15a3e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*cMu_E4SBZr2YV62m.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">伯特的总结</figcaption></figure></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="5107" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated">11。微调T5 </h2><p id="7f33" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated"><strong class="kq ja"> T5:文本到文本转换转换器</strong></p><p id="fed3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从拥抱脸下载T5-小变压器用于条件生成</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="fef0" class="nq mc iq op b gy ot ou l ov ow">tokenizer = T5Tokenizer.from_pretrained('t5-small') model = TFT5ForConditionalGeneration.from_pretrained('t5-small') task_specific_params = model.config.task_specific_params if task_specific_params is not None:     <br/>    model.config.update(task_specific_params.get("summarization", {}))  <br/>    <br/>pad_token_id = tokenizer.pad_token_id</span></pre><p id="9d6e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">为模型准备数据</strong></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="8a40" class="nq mc iq op b gy ot ou l ov ow">def normalize_text(text):   <br/>    text = tf.strings.lower(text)  <br/>    text = tf.strings.regex_replace(text,"'(.*)'", r"\1")  <br/>    return text.numpy().decode('UTF-8') def tokenize_articles(text):  <br/>    text = normalize_text(text)  <br/>    <br/>    ids = tokenizer.encode_plus((model.config.prefix + text),      <br/>    return_tensors="tf", max_length=350)<br/>    <br/>    return tf.squeeze(ids['input_ids']),    tf.squeeze(ids['attention_mask']) def tokenize_highlights(text):  <br/>    text = normalize_text(text)  <br/>    ids = tokenizer.encode(text, return_tensors="tf", max_length=50)         return tf.squeeze(ids) def map_func(x, y):      article_ids, attention_mask = tf.py_function(tokenize_articles, inp=[x], Tout=(tf.int32, tf.int32))  <br/>    highlights_ids = tf.py_function(tokenize_highlights, inp=[y], Tout=tf.int32)      return article_ids, attention_mask, highlights_ids</span></pre><p id="1e5c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">检查此<a class="ae lk" href="https://github.com/Chandugundluru/Understanding-Abstractive-Summarization-from-scratch" rel="noopener ugc nofollow" target="_blank"> GitHub </a> repo用于模型训练</p><p id="9fdf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">模型预测:</strong></p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="d8c4" class="nq mc iq op b gy ot ou l ov ow">from tqdm import tqdm <br/>predictions = [] <br/>reference=[] <br/>for i, (input_ids, input_mask, y) in (enumerate(test_ds)):      summaries = model.generate(input_ids=input_ids,max_length=45  ,attention_mask=input_mask)      pred = [tokenizer.decode(g, skip_special_tokens=True,   clean_up_tokenization_spaces=False) for g in summaries]      real = [tokenizer.decode(g, skip_special_tokens=True,  clean_up_tokenization_spaces=False) for g in y]       predictions.append(pred)      reference.append(real)</span></pre><p id="c1df" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">生成汇总</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi qz"><img src="../Images/5f0e4dee292efb086dcaab73fedff808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/0*AXeOVsMpRwBTNl3K.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">微调T5模型的总结</figcaption></figure><p id="7725" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> Rogue_score </strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ra"><img src="../Images/619662c16d759dbc0710bb8ee8f3d4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/0*hiZROsr_W6X5Z4V5.png"/></div></figure><h2 id="c395" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated"><strong class="ak"> 12。模型分析</strong></h2><p id="06f1" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated"><strong class="kq ja">带有纪元的覆盖模型分析:</strong></p><p id="87c9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们检查一下覆盖向量及其损失是如何阻止预测中单词的重复的。在这里，我打印3、5、10、25个时期后的摘要输出</p><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="bef8" class="nq mc iq op b gy ot ou l ov ow">print('length_of_article : ',len(x_train[12].split( ))) <br/>print('\n') print('orginal_summary : ',orgsummepoch3[12]) <br/>print('\n') <br/>print('predictes summary after 3rd epoch:',predsummepoch3[12]) print('\n') <br/>print('predictes summary after 5th epoch:',predsummepoch5[12]) print('\n') <br/>print('predictes summary after 10th epoch:',predsummepoch10[12]) print('\n') <br/>print('predictes summary after 25th epoch:',predsummepoch25[12])</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi rb"><img src="../Images/1490d9265cde32ddcc17c84594ff249a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*lCeIKdmpP_QLsqUU.png"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi rc"><img src="../Images/37dddf540123021a23097d6a37669be6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/0*dUOPtZE4JlLD8sDN.png"/></div></figure><p id="187b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在<strong class="kq ja">历元-3 </strong>之后生成的摘要类似于我们通过关注机制得到的输出，但是在<strong class="kq ja">历元-10 </strong>之后，事情发生了变化，我们的覆盖机制启动，并且在<strong class="kq ja">25历元</strong>之后，我们得到了精确的摘要</p><p id="c318" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">让我们分析一下我们的模型失败在哪里</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi rd"><img src="../Images/5fcd0eebd7d75adb0c4b4894f5222f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/0*TypE0QX3SthT-846.png"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi re"><img src="../Images/4dea7de4703f8ce89d653f23b563d2bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/0*2ro4cGTBy6PwEJJE.png"/></div></figure><p id="587c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在分析模型时发现的一个普遍现象是输入文章的长度很长，一些摘要是没有意义的。看了图片的第二个总结，觉得很可笑…..作为一个预测总结，他出生于2010年，但在org summary中说他在2009年死于未经治疗的细菌性肺炎，这没有任何意义</p><p id="8ed7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">微调后的T5车型有多好？</strong></p><p id="509b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了检查这一点，我根据文章长度将我的数据分为3组</p><ol class=""><li id="fe21" class="mt mu iq kq b kr ks ku kv kx oi lb oj lf ok lj na nb nc nd bi translated">长度小于200的文章</li><li id="00b3" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">文章长度在200-500之间</li><li id="d165" class="mt mu iq kq b kr ne ku nf kx ng lb nh lf ni lj na nb nc nd bi translated">文章长度超过700</li></ol><pre class="lm ln lo lp gt oo op oq or aw os bi"><span id="0442" class="nq mc iq op b gy ot ou l ov ow">max_art_len=500 <br/>min_art_len=200 cleaned_text = np.array(data_cleaned['text']) <br/>cleaned_summary = np.array(data_cleaned['summary'])<br/>short_text = [] <br/>short_summary = [] <br/>for i in range(len(cleaned_text)):          if(len(cleaned_text[i].split())&gt;=min_art_len and<br/>        len(cleaned_text[i].split())&lt;=max_art_len)        short_text.append(cleaned_text[i])        <br/> <br/>        short_summary.append(cleaned_summary[i]) data1=pd.DataFrame({'text':short_text,'summary':short_summary})</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi rf"><img src="../Images/2f311b0bfa0cbfd1218232309950ccc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*aCCyD4sJ_YGFZ6hY.png"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi rg"><img src="../Images/f45379b6cb127546b68f7ec7bd894b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/0*GMrCAzwTXUz1TlSx.png"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi rh"><img src="../Images/be94d4dede4cbf1f837bf97b182a20aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/0*DTwpXYsekZfb_wFr.png"/></div></figure><p id="ed00" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以上分别是上述文章和模型的不良分数，即使输入文本很长，它们也做得很好</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="d995" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated">13。结论</h2><p id="b9fc" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated">所有这些都是从简单的seq-seq模型开始，以最先进的技术T5结束。我希望你们能从这个博客中找到一些有用的信息。</p><p id="037f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">查看Linkedin <a class="ae lk" href="http://linkedin.com/in/gundluru-chandrasekhar-499129196" rel="noopener ugc nofollow" target="_blank">简介</a> &amp; Github <a class="ae lk" href="https://github.com/Chandugundluru?tab=repositories" rel="noopener ugc nofollow" target="_blank">简介</a>查看完整代码实现。</p><h2 id="eac3" class="nq mc iq bd md nr ns dn mh nt nu dp ml kx nv nw mn lb nx ny mp lf nz oa mr iw bi translated">14。参考文献</h2><p id="c699" class="pw-post-body-paragraph ko kp iq kq b kr mv ka kt ku mw kd kw kx ol kz la lb om ld le lf on lh li lj ij bi translated"><a class="ae lk" href="http://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">www.appliedaicourse.com</a></p><p id="3a52" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://github.com/nlpyang/PreSumm" rel="noopener ugc nofollow" target="_blank">nlpyang/proposum</a></p><p id="e61a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://arxiv.org/abs/1908.08345" rel="noopener ugc nofollow" target="_blank">带预训练编码器的文本摘要</a></p><p id="4a83" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://arxiv.org/abs/1908.08345" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1908.08345</a></p><p id="5725" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://huggingface.co/transformers/model_doc/t5.html#tft5forconditionalgeneration" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/transformers/model _ doc/t5 . html # tft 5 for conditional generation</a></p><p id="2ddc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" rel="noopener ugc nofollow" target="_blank">http://www . abigailsee . com/2017/04/16/taming-rnns-for-better-summarying . html</a></p></div></div>    
</body>
</html>