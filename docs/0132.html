<html>
<head>
<title>DIAYN: Diversity Is All You Need</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DIAYN:多样性是你所需要的</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/diayn-diversity-is-all-you-need-23aaa6532e84?source=collection_archive---------0-----------------------#2019-08-11">https://pub.towardsai.net/diayn-diversity-is-all-you-need-23aaa6532e84?source=collection_archive---------0-----------------------#2019-08-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="fd9e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">向着AI 潜入DIAYN | <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank"/></h2><div class=""/><div class=""><h2 id="6b5e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一种无监督的基于信息的学习不同技能的方法</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/5bf59334f8a6ed97e9837efaf9f84a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*UTgCy-ZZTLsV6piUYsV6ug.gif"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">DIAYN在没有任何外在奖励信号的情况下学习了不同的技能。来源:https://sites.google.com/view/diayn<a class="ae le" href="https://sites.google.com/view/diayn" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><h1 id="3ce5" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">介绍</h1><p id="f7cd" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我们讨论了一种基于信息的强化学习方法，该方法通过学习不同的技能来探索环境，而无需外部奖励的监督。简而言之，该方法，即DIAYN(多样性是你所需要的)，通过信息论目标建立技能的多样性，并使用最大熵强化学习(MaxEnt RL)算法(例如，SAC)对其进行优化。尽管它很简单，但这种方法已经被证明能够在各种模拟机器人任务中学习不同的技能，如行走和跳跃。此外，它能够解决许多RL基准任务，即使没有收到任何任务奖励。更多有趣的实验结果，请参考他们的<a class="ae le" href="https://sites.google.com/view/diayn" rel="noopener ugc nofollow" target="_blank">项目网站</a>。</p><h1 id="caa2" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">预赛</h1><p id="46f3" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我们用元组<em class="mt"> (S，A，P，r，γ) </em>定义一个马尔可夫决策过程(MDP)，其中<em class="mt"> S </em>是状态的集合，<em class="mt"> A </em>是动作的集合，<em class="mt"> P: S × A → S </em>是转移函数，<em class="mt"> r </em>是奖励函数，<em class="mt"> γ </em>是贴现因子。RL算法旨在最大化预期奖励的贴现总额，定义如下</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/3ae39280e2fd6f405958138501a84814.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*5ePlVLUdFYgDzWQkydYMFA.png"/></div></figure><p id="8c78" class="pw-post-body-paragraph lx ly iq lz b ma mv ka mc md mw kd mf mg mx mi mj mk my mm mn mo mz mq mr ms ij bi translated">在DIAYN，我们不考虑来自环境的奖励信号。相反，我们根据信息论来定义任务无关的奖励，我们很快就会看到。</p><h1 id="f367" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">多样性是你所需要的</h1><p id="7041" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在DIAYN中，我们将技能定义为以一致的方式改变环境状态的潜在条件政策。数学上，一项技能用条件策略<em class="mt"> p(a|s，z) </em>来表示，其中<em class="mt"> z </em>是从某个分布<em class="mt"> p(z) </em>中采样的潜在变量。该方法主要基于三个想法(对于那些不熟悉互信息概念的人，我建议参考本文的第二部分以获得一些直觉):</p><ol class=""><li id="aefd" class="na nb iq lz b ma mv md mw mg nc mk nd mo ne ms nf ng nh ni bi translated">为了使技能有用，我们希望技能能够指示代理访问的状态。不同的技能应该访问不同的状态，因此是可以区分的。为了实现这一点，我们最大化互信息I(S；Z) 状态<em class="mt"> S </em>和技能<em class="mt"> Z </em>之间。</li><li id="3dea" class="na nb iq lz b ma nj md nk mg nl mk nm mo nn ms nf ng nh ni bi translated">我们想用状态而不是动作来区分技能，因为不影响环境的动作对于外部观察者来说是不可见的。这是通过最小化互信息<em class="mt">I(A；动作<em class="mt"> A </em>和技能<em class="mt"> Z </em>之间</em>给定状态<em class="mt"> S </em>。</li><li id="3daa" class="na nb iq lz b ma nj md nk mg nl mk nm mo nn ms nf ng nh ni bi translated">我们鼓励探索，并通过学习尽可能随机的技能来激励技能尽可能多样化。正如在最大熵强化学习中所做的，这是通过最大化策略熵<em class="mt"> H(A|S) </em>来实现的。</li></ol><p id="cc27" class="pw-post-body-paragraph lx ly iq lz b ma mv ka mc md mw kd mf mg mx mi mj mk my mm mn mo mz mq mr ms ij bi translated">如果我们把这三个目标放在一起，我们会得到</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/e3235deb986cece596f7eaca19a99f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AjQ_WnfExMsZkIhuycdpMw.png"/></div></div></figure><p id="62a1" class="pw-post-body-paragraph lx ly iq lz b ma mv ka mc md mw kd mf mg mx mi mj mk my mm mn mo mz mq mr ms ij bi translated">我们现在对每一项都有了一些直觉。第一项鼓励我们的先验分布<em class="mt"> p(z) </em>具有高熵。对于一组固定的技能，我们将<em class="mt"> p(z) </em>设为保证其具有最大熵的<a class="ae le" href="https://github.com/ben-eysenbach/sac/blob/2116fc394749ca745f093a36635a9b253da8170d/sac/algos/diayn.py#L92" rel="noopener ugc nofollow" target="_blank">离散均匀分布</a>。最小化第二项表明，应该很容易从当前状态推断技能。第三项表示每个技能应该尽可能随机行动。</p><p id="25ff" class="pw-post-body-paragraph lx ly iq lz b ma mv ka mc md mw kd mf mg mx mi mj mk my mm mn mo mz mq mr ms ij bi translated">我们可以通过一些MaxEnt RL方法轻松地最大化第三项(例如，他们的实验中使用的温度为<em class="mt"> 0.1 </em>的<a class="ae le" href="https://arxiv.org/abs/1801.01290" rel="noopener ugc nofollow" target="_blank"> SAC </a>)。至于前两项，作者建议将它们合并到一个伪奖励中:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/1c4eddd067817f94788d879e59f44e2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*ifLDJL-PWTwTAIIx92uCFg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">伪奖励</figcaption></figure><p id="4024" class="pw-post-body-paragraph lx ly iq lz b ma mv ka mc md mw kd mf mg mx mi mj mk my mm mn mo mz mq mr ms ij bi translated">其中，一个已学习的鉴别器<em class="mt"> q_ϕ(z|s) </em>用于近似<em class="mt"> p(z|s) </em>，这是有效的，因为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/d0038a55314393099c55e7359027d27d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EBCT3QD-jTfpl3JIST_S4A.png"/></div></div></figure><p id="0011" class="pw-post-body-paragraph lx ly iq lz b ma mv ka mc md mw kd mf mg mx mi mj mk my mm mn mo mz mq mr ms ij bi translated">请注意，如果<em class="mt"> q_ϕ(z|s) ≥ p(z) </em>，奖励函数中的常量<em class="mt"> log p(z) </em>有助于鼓励代理保持存活，当代理成功学习技能<em class="mt"> p(a|s，z) </em>时，应始终保持该常量。另一方面，移除<em class="mt"> log p(z) </em>会导致负回报，这会诱使代理尽快结束剧集。</p><h2 id="9fee" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">算法</h2><p id="f5e7" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">到目前为止，我们已经定义了无监督MDP，并指定了强化学习方法，因此很容易理解整个算法:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/2fef2b2ab0ca06f14980268c8e66b7e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gz-rQZhnhYCwsrchuBQIng.png"/></div></div></figure><h1 id="9522" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">将DIAYN结合到分层强化学习中</h1><p id="9c89" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">DIAYN学习的网络可以用来初始化特定任务的代理，这为初始探索提供了一个很好的方法。DIAYN的另一个有趣的应用是将学到的技能用作分层强化学习(HRL)算法的低级策略。为此，我们进一步学习元控制器，它为接下来的<em class="mt"> k </em>步骤选择要执行的技能。元控制器具有与技能相同的观察空间，并以任务报酬最大化为目标。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/9ea4dcc9f4af654b3aa2f11ff571d668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MKgZP47RmIpkYyEkoFKI1A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:多样性是你所需要的</figcaption></figure><p id="51b8" class="pw-post-body-paragraph lx ly iq lz b ma mv ka mc md mw kd mf mg mx mi mj mk my mm mn mo mz mq mr ms ij bi translated">作者在两个具有挑战性的模拟机器人环境中对HRL算法进行了实验。在猎豹跨栏任务中，代理人会因为跳过障碍而获得奖励，而在蚂蚁导航任务中，代理人必须按照特定的顺序走到一组5个路点，在到达每个路点时只能获得少量奖励。下图展示了DIAYN如何优于一些最先进的RL方法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/d353b7940cba24a63a47afa57b4866c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0CHt3nOEXcNb3HesjLs0zg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:多样性是你所需要的</figcaption></figure><p id="383a" class="pw-post-body-paragraph lx ly iq lz b ma mv ka mc md mw kd mf mg mx mi mj mk my mm mn mo mz mq mr ms ij bi translated">值得注意的是，平原DIAYN像其他人一样在蚂蚁导航任务中挣扎。这可以通过在鉴别器中加入一些先验知识来弥补。具体地说，鉴别器取计算代理质心的输入<em class="mt"> f(s) </em>，HRL方法保持不变。“DIAYN+prior”表明对鉴别器的这种简单修改显著提高了性能。</p><h1 id="6231" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参考</h1><p id="7827" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated"><a class="ae le" href="https://openreview.net/profile?email=beysenba%40cs.cmu.edu" rel="noopener ugc nofollow" target="_blank"> <em class="mt">本杰明·艾森巴赫</em></a><em class="mt"/><a class="ae le" href="https://openreview.net/profile?email=abhigupta%40berkeley.edu" rel="noopener ugc nofollow" target="_blank"><em class="mt">阿布舍克·古普塔</em></a><em class="mt"/><a class="ae le" href="https://openreview.net/profile?email=julianibarz%40google.com" rel="noopener ugc nofollow" target="_blank"><em class="mt">朱利安·伊巴兹</em></a><em class="mt"/><a class="ae le" href="https://openreview.net/profile?email=svlevine%40eecs.berkeley.edu" rel="noopener ugc nofollow" target="_blank"><em class="mt">谢尔盖·莱文</em> </a>。多样性是你所需要的:学习没有奖励功能的技能。于2019年在ICLR展出。</p></div></div>    
</body>
</html>