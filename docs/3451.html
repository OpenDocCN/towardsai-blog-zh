<html>
<head>
<title>Lazypredict: Run All Sklearn Algorithms With a Line Of Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Lazypredict:用一行代码运行所有Sklearn算法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/lazypredict-run-all-sklearn-algorithms-with-a-line-of-code-29d73d82499c?source=collection_archive---------1-----------------------#2022-12-26">https://pub.towardsai.net/lazypredict-run-all-sklearn-algorithms-with-a-line-of-code-29d73d82499c?source=collection_archive---------1-----------------------#2022-12-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="50a6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何(以及为什么不应该)使用它</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e200bcf03aeb2b2462d2a67512584dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*89pCa2pFuQkquWTp"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">lazypredict的输出。</figcaption></figure><p id="521d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是数据科学家的两个痛点:</p><h2 id="eb1e" class="lr ls iq bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">难点1:数据科学生命周期的时间有限</h2><p id="6662" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">数据科学家必须分清主次。这可能意味着花更多的时间来理解业务问题并确定最合适的方法，而不是仅仅专注于开发机器学习算法。</p><h2 id="053f" class="lr ls iq bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">痛点2:机器学习建模可能很耗时</h2><p id="cbbd" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">微调机器学习算法包括找到这些超参数的最佳值，这可能是一个反复试验的过程。这需要很长时间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/723d6d2331d0c59e62e24e6d1b911204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*6XHbJCKzUAhPkU7kKN73Pw.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">自动化机器学习可以极大地帮助数据科学家。通过稳定扩散成像。</figcaption></figure><h1 id="f073" class="mq ls iq bd lt mr ms mt lw mu mv mw lz jw mx jx mc jz my ka mf kc mz kd mi na bi translated">AutoML拯救世界</h1><p id="3cfb" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">AutoML可以解决这些问题。一个新生的图书馆是lazypredict。在这篇文章中，我浏览了以下内容:</p><ul class=""><li id="382e" class="nb nc iq kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">什么是lazypredict</li><li id="cf9d" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">安装lazypredict</li><li id="5aad" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">如何使用它来自动拟合scikit-learn回归算法</li><li id="02bd" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">如何将它用于自动拟合分类算法</li><li id="7991" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">为什么不应该使用它(以及还可以使用什么)</li></ul><p id="1d79" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="np">注意:我不隶属于lazypredict。</em></p><h1 id="6fd9" class="mq ls iq bd lt mr ms mt lw mu mv mw lz jw mx jx mc jz my ka mf kc mz kd mi na bi translated">什么是Lazypredict</h1><p id="03e2" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">Lazypredict是一个Python包，旨在自动化机器学习建模过程。它对回归和分类任务都有效。</p><p id="c3a8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它的关键特征是能够自动训练和评估机器学习模型。它提供了一个简单的界面来定义一系列超参数，然后使用这些超参数的各种不同组合来训练和评估模型。</p><h1 id="752e" class="mq ls iq bd lt mr ms mt lw mu mv mw lz jw mx jx mc jz my ka mf kc mz kd mi na bi translated">安装Lazypredict</h1><p id="fb13" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在您的终端上，运行以下命令</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="09c4" class="nv ls iq nr b be nw nx l ny nz">pip install lazypredict</span></pre><p id="fec1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，您可能需要手动安装lazypredict的一些依赖项。如果遇到需要安装scikit-learn、xgboost或lightgbm的问题，可以运行pip install来安装必要的库。</p><p id="b6d0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">就我个人而言，我通过下面的<code class="fe oa ob oc nr b">requirements.txt</code>让它在<code class="fe oa ob oc nr b">python 3.9.13</code>上工作</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="ffd8" class="nv ls iq nr b be nw nx l ny nz">pandas==1.4.4<br/>numpy==1.21.5<br/>scikit-learn==1.0.2<br/>lazypredict==0.2.12</span></pre><p id="ca82" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我通过在终端上运行这个命令安装了以下库:<code class="fe oa ob oc nr b">pip install -r requirements.txt</code>。</p><p id="7873" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种情况下使用虚拟环境就更好了。</p><h1 id="4cf2" class="mq ls iq bd lt mr ms mt lw mu mv mw lz jw mx jx mc jz my ka mf kc mz kd mi na bi translated">使用Lazypredict进行回归</h1><p id="7764" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">让我们浏览一下代码。(如果你只是想要完整的代码，在本文中搜索“完整代码”。)</p><p id="7b78" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将首先导入必要的库。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="2ac9" class="nv ls iq nr b be nw nx l ny nz">from lazypredict.Supervised import LazyRegressor<br/>from sklearn import datasets<br/>from sklearn.utils import shuffle<br/>import numpy as np<br/></span></pre><p id="14c5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，我们将导入糖尿病数据集。</p><blockquote class="od oe of"><p id="1d2c" class="kv kw np kx b ky kz jr la lb lc ju ld og lf lg lh oh lj lk ll oi ln lo lp lq ij bi translated">对每名n = 442的糖尿病患者获得10个基线变量，年龄、性别、体重指数、平均血压和6个血清测量值，以及感兴趣的反应，即基线后一年疾病进展的定量测量。</p></blockquote><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="cb1c" class="nv ls iq nr b be nw nx l ny nz"># Import the Diabetes Dataset<br/>diabetes = datasets.load_diabetes()</span></pre><p id="98d7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们打乱数据集，以便我们可以将它们分成训练测试集。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="b4d0" class="nv ls iq nr b be nw nx l ny nz"># Shuffle the dataset <br/>X, y = shuffle(diabetes.data, diabetes.target, random_state=13)<br/><br/># Cast the numerical values into a numpy float.<br/>X = X.astype(np.float32)<br/><br/># Split the dataset into 90% and 10%.<br/>offset = int(X.shape[0] * 0.9)<br/><br/># Split into train and test<br/>X_train, y_train = X[:offset], y[:offset]<br/>X_test, y_test = X[offset:], y[offset:]<br/></span></pre><p id="a452" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们初始化LazyRegressor对象。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="a1e0" class="nv ls iq nr b be nw nx l ny nz"># Running the Lazypredict library and fit multiple regression libraries<br/># for the same dataset<br/>reg = LazyRegressor(verbose=0, <br/>                    ignore_warnings=False, <br/>                    custom_metric=None,<br/>                    predictions=False,<br/>                    random_state = 13)<br/><br/># Parameters<br/># ----------<br/># verbose : int, optional (default=0)<br/>#       For the liblinear and lbfgs solvers set verbose to any positive<br/>#       number for verbosity.<br/># ignore_warnings : bool, optional (default=True)<br/>#       When set to True, the warning related to algorigms that are not able<br/>#       to run are ignored.<br/># custom_metric : function, optional (default=None)<br/>#       When function is provided, models are evaluated based on the custom <br/>#       evaluation metric provided.<br/># prediction : bool, optional (default=False)<br/>#       When set to True, the predictions of all the models models are <br/>#       returned as dataframe.<br/># regressors : list, optional (default="all")<br/>#       When function is provided, trains the chosen regressor(s).<br/></span></pre><p id="0827" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们将使用lazypredict库<code class="fe oa ob oc nr b">fit</code>多重回归算法。这一步总共花了3秒钟。</p><p id="ada6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在幕后，<code class="fe oa ob oc nr b">fit</code>方法执行以下操作:</p><ol class=""><li id="7852" class="nb nc iq kx b ky kz lb lc le nd li ne lm nf lq oj nh ni nj bi translated">将所有特征分为三类:数字(数字特征)或分类(文本特征)</li><li id="9333" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated">进一步将分类特征分成两类:“高”分类特征(<em class="np">，其唯一值比特征总数多)</em>和“低”分类特征(<em class="np">，其唯一值比特征总数少)</em></li><li id="f0b8" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated">然后，以这种方式预处理每个特征:</li></ol><ul class=""><li id="092f" class="nb nc iq kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">数字特征:用平均值估算缺失值，然后标准化特征<em class="np">(去除平均值并除以方差)</em></li><li id="374c" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">“高”分类特征:用值“缺失”估算缺失值，然后执行一键编码。</li><li id="80aa" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">“低”分类特性:用值“missing”估算缺失值，然后执行顺序编码(<em class="np">将每个唯一的字符串值转换为整数。</em>在性别列的示例中—“男性”编码为0，“女性”编码为1。)</li><li id="6416" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">使训练数据集适合每个算法。</li><li id="aec4" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">在测试集上测试每个算法。默认情况下，这些指标是经过调整的R平方、R平方、均方根误差和所用时间。</li></ul><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="9c0a" class="nv ls iq nr b be nw nx l ny nz">models, predictions = reg.fit(X_train, X_test, y_train, y_test)<br/>model_dictionary = reg.provide_models(X_train, X_test, y_train, y_test)<br/>models</span></pre><p id="d434" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是结果。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="9542" class="nv ls iq nr b be nw nx l ny nz">| Model                         |   Adjusted R-Squared |   R-Squared |   RMSE |   Time Taken |<br/>|:------------------------------|---------------------:|------------:|-------:|-------------:|<br/>| ExtraTreesRegressor           |                 0.38 |        0.52 |  54.22 |         0.17 |<br/>| OrthogonalMatchingPursuitCV   |                 0.37 |        0.52 |  54.39 |         0.01 |<br/>| Lasso                         |                 0.37 |        0.52 |  54.46 |         0.01 |<br/>| LassoLars                     |                 0.37 |        0.52 |  54.46 |         0.01 |<br/>| LarsCV                        |                 0.37 |        0.51 |  54.54 |         0.02 |<br/>| LassoCV                       |                 0.37 |        0.51 |  54.59 |         0.07 |<br/>| PassiveAggressiveRegressor    |                 0.37 |        0.51 |  54.74 |         0.01 |<br/>| LassoLarsIC                   |                 0.36 |        0.51 |  54.83 |         0.01 |<br/>| SGDRegressor                  |                 0.36 |        0.51 |  54.85 |         0.01 |<br/>| RidgeCV                       |                 0.36 |        0.51 |  54.91 |         0.01 |<br/>| Ridge                         |                 0.36 |        0.51 |  54.91 |         0.01 |<br/>| BayesianRidge                 |                 0.36 |        0.51 |  54.94 |         0.01 |<br/>| LassoLarsCV                   |                 0.36 |        0.51 |  54.96 |         0.02 |<br/>| LinearRegression              |                 0.36 |        0.51 |  54.96 |         0.01 |<br/>| TransformedTargetRegressor    |                 0.36 |        0.51 |  54.96 |         0.01 |<br/>| Lars                          |                 0.36 |        0.50 |  55.09 |         0.01 |<br/>| ElasticNetCV                  |                 0.36 |        0.50 |  55.20 |         0.06 |<br/>| HuberRegressor                |                 0.36 |        0.50 |  55.24 |         0.02 |<br/>| RandomForestRegressor         |                 0.35 |        0.50 |  55.48 |         0.25 |<br/>| AdaBoostRegressor             |                 0.34 |        0.49 |  55.88 |         0.08 |<br/>| LGBMRegressor                 |                 0.34 |        0.49 |  55.93 |         0.05 |<br/>| HistGradientBoostingRegressor |                 0.34 |        0.49 |  56.08 |         0.20 |<br/>| PoissonRegressor              |                 0.32 |        0.48 |  56.61 |         0.01 |<br/>| ElasticNet                    |                 0.30 |        0.46 |  57.49 |         0.01 |<br/>| KNeighborsRegressor           |                 0.30 |        0.46 |  57.57 |         0.01 |<br/>| OrthogonalMatchingPursuit     |                 0.29 |        0.45 |  57.87 |         0.01 |<br/>| BaggingRegressor              |                 0.29 |        0.45 |  57.92 |         0.04 |<br/>| XGBRegressor                  |                 0.28 |        0.45 |  58.18 |         0.11 |<br/>| GradientBoostingRegressor     |                 0.25 |        0.42 |  59.70 |         0.12 |<br/>| TweedieRegressor              |                 0.24 |        0.42 |  59.81 |         0.01 |<br/>| GammaRegressor                |                 0.22 |        0.40 |  60.61 |         0.01 |<br/>| RANSACRegressor               |                 0.20 |        0.38 |  61.40 |         0.12 |<br/>| LinearSVR                     |                 0.12 |        0.32 |  64.66 |         0.01 |<br/>| ExtraTreeRegressor            |                 0.00 |        0.23 |  68.73 |         0.01 |<br/>| NuSVR                         |                -0.07 |        0.18 |  71.06 |         0.01 |<br/>| SVR                           |                -0.10 |        0.15 |  72.04 |         0.02 |<br/>| DummyRegressor                |                -0.30 |       -0.00 |  78.37 |         0.01 |<br/>| QuantileRegressor             |                -0.35 |       -0.04 |  79.84 |         1.42 |<br/>| DecisionTreeRegressor         |                -0.47 |       -0.14 |  83.42 |         0.01 |<br/>| GaussianProcessRegressor      |                -0.77 |       -0.37 |  91.51 |         0.02 |<br/>| MLPRegressor                  |                -1.87 |       -1.22 | 116.51 |         0.21 |<br/>| KernelRidge                   |                -5.04 |       -3.67 | 169.06 |         0.01 |</span></pre><p id="f3b8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是糖尿病数据集回归的完整代码。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="51a8" class="nv ls iq nr b be nw nx l ny nz"> <br/>from lazypredict.Supervised import LazyRegressor<br/>from sklearn import datasets<br/>from sklearn.utils import shuffle<br/>import numpy as np<br/><br/># Import the Diabetes Dataset<br/>diabetes = datasets.load_diabetes()<br/><br/># Shuffle the dataset <br/>X, y = shuffle(diabetes.data, diabetes.target, random_state=13)<br/><br/># Cast the numerical values<br/>X = X.astype(np.float32)<br/>offset = int(X.shape[0] * 0.9)<br/><br/># Split into train and test<br/>X_train, y_train = X[:offset], y[:offset]<br/>X_test, y_test = X[offset:], y[offset:]<br/><br/># Running the Lazypredict library and fit multiple regression libraries<br/># for the same dataset<br/>reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)<br/>models, predictions = reg.fit(X_train, X_test, y_train, y_test)<br/>model_dictionary = reg.provide_models(X_train, X_test, y_train, y_test)<br/>models</span></pre><h1 id="643c" class="mq ls iq bd lt mr ms mt lw mu mv mw lz jw mx jx mc jz my ka mf kc mz kd mi na bi translated">使用Lazypredict进行分类</h1><p id="4f14" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">让我们用Lazypredict进行分类。(如果你只是想要完整的代码，在本文中搜索“完整代码”。</p><p id="f7d9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，导入必要的库。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="fc55" class="nv ls iq nr b be nw nx l ny nz"> from lazypredict.Supervised import LazyClassifier<br/> from sklearn.datasets import load_iris<br/> from sklearn.model_selection import train_test_split</span></pre><p id="f643" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们加载数据，Iris数据集，并将其分成训练集和测试集。这是它包含的内容。</p><blockquote class="od oe of"><p id="7395" class="kv kw np kx b ky kz jr la lb lc ju ld og lf lg lh oh lj lk ll oi ln lo lp lq ij bi translated">数据集由三种鸢尾<em class="iq"/>(<a class="ae ok" href="https://en.wikipedia.org/wiki/Iris_setosa" rel="noopener ugc nofollow" target="_blank"><em class="iq">鸢尾</em> </a>、<a class="ae ok" href="https://en.wikipedia.org/wiki/Iris_virginica" rel="noopener ugc nofollow" target="_blank"> <em class="iq">北美鸢尾</em> </a>和<a class="ae ok" href="https://en.wikipedia.org/wiki/Iris_versicolor" rel="noopener ugc nofollow" target="_blank"> <em class="iq">杂色鸢尾</em> </a>)各50个样本组成。从每个样品中测量四个<a class="ae ok" href="https://en.wikipedia.org/wiki/Features_(pattern_recognition)" rel="noopener ugc nofollow" target="_blank">特征</a>:萼片<a class="ae ok" href="https://en.wikipedia.org/wiki/Sepal" rel="noopener ugc nofollow" target="_blank">和花瓣</a>的长度和宽度，单位为厘米。</p></blockquote><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="8f99" class="nv ls iq nr b be nw nx l ny nz"> data = load_iris()<br/> X = data.data<br/> y= data.target<br/> X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)</span></pre><p id="51e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们初始化LazyClassifier对象。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="0e2b" class="nv ls iq nr b be nw nx l ny nz"># Running the Lazypredict library and fit multiple regression libraries<br/># for the same dataset<br/>clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)<br/><br/>"""<br/>    Parameters<br/>    ----------<br/>    verbose : int, optional (default=0)<br/>        For the liblinear and lbfgs solvers set verbose to any positive<br/>        number for verbosity.<br/>    ignore_warnings : bool, optional (default=True)<br/>        When set to True, the warning related to algorigms that are not able to run are ignored.<br/>    custom_metric : function, optional (default=None)<br/>        When function is provided, models are evaluated based on the custom evaluation metric provided.<br/>    prediction : bool, optional (default=False)<br/>        When set to True, the predictions of all the models models are returned as dataframe.<br/>    classifiers : list, optional (default="all")<br/>        When function is provided, trains the chosen classifier(s).<br/>"""<br/></span></pre><p id="e6d0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，我们调用lazy regressor的<code class="fe oa ob oc nr b">fit</code>方法，它将多重分类算法与lazypredict库相匹配。对于这个小数据集，这一步总共花了1秒钟。</p><p id="0f19" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">(在引擎盖下搜索关键字“<em class="np">，fit方法</em>，跳转到我解释<code class="fe oa ob oc nr b">fit</code>是做什么的部分。)</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="253b" class="nv ls iq nr b be nw nx l ny nz"> models,predictions = clf.fit(X_train, X_test, y_train, y_test)<br/></span></pre><p id="e3b4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们可以使用<code class="fe oa ob oc nr b">provide_models</code>来看看每个模型的表现。这报告了测试集的准确性、平衡准确性、ROC AUC和F1得分。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="108a" class="nv ls iq nr b be nw nx l ny nz"># Calculate performance of all models on test dataset<br/> model_dictionary = clf.provide_models(X_train,X_test,y_train,y_test)<br/> models</span></pre><p id="de31" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是完整的结果。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="ded6" class="nv ls iq nr b be nw nx l ol nz">| Model                         |   Accuracy |   Balanced Accuracy | ROC AUC   |   F1 Score |   Time Taken |<br/>|:------------------------------|-----------:|--------------------:|:----------|-----------:|-------------:|<br/>| LinearDiscriminantAnalysis    |       0.99 |                0.99 |           |       0.99 |         0.01 |<br/>| AdaBoostClassifier            |       0.97 |                0.98 |           |       0.97 |         0.13 |<br/>| PassiveAggressiveClassifier   |       0.97 |                0.98 |           |       0.97 |         0.01 |<br/>| LogisticRegression            |       0.97 |                0.98 |           |       0.97 |         0.01 |<br/>| GaussianNB                    |       0.97 |                0.98 |           |       0.97 |         0.01 |<br/>| SGDClassifier                 |       0.96 |                0.96 |           |       0.96 |         0.01 |<br/>| RandomForestClassifier        |       0.96 |                0.96 |           |       0.96 |         0.19 |<br/>| QuadraticDiscriminantAnalysis |       0.96 |                0.96 |           |       0.96 |         0.01 |<br/>| Perceptron                    |       0.96 |                0.96 |           |       0.96 |         0.01 |<br/>| LGBMClassifier                |       0.96 |                0.96 |           |       0.96 |         0.30 |<br/>| ExtraTreeClassifier           |       0.96 |                0.96 |           |       0.96 |         0.01 |<br/>| BaggingClassifier             |       0.95 |                0.95 |           |       0.95 |         0.03 |<br/>| ExtraTreesClassifier          |       0.95 |                0.95 |           |       0.95 |         0.13 |<br/>| XGBClassifier                 |       0.95 |                0.95 |           |       0.95 |         0.19 |<br/>| DecisionTreeClassifier        |       0.95 |                0.95 |           |       0.95 |         0.01 |<br/>| LinearSVC                     |       0.95 |                0.95 |           |       0.95 |         0.01 |<br/>| CalibratedClassifierCV        |       0.95 |                0.95 |           |       0.95 |         0.04 |<br/>| KNeighborsClassifier          |       0.93 |                0.94 |           |       0.93 |         0.01 |<br/>| NuSVC                         |       0.93 |                0.94 |           |       0.93 |         0.01 |<br/>| SVC                           |       0.93 |                0.94 |           |       0.93 |         0.01 |<br/>| RidgeClassifierCV             |       0.91 |                0.91 |           |       0.91 |         0.01 |<br/>| NearestCentroid               |       0.89 |                0.90 |           |       0.89 |         0.01 |<br/>| LabelPropagation              |       0.89 |                0.90 |           |       0.90 |         0.01 |<br/>| LabelSpreading                |       0.89 |                0.90 |           |       0.90 |         0.01 |<br/>| RidgeClassifier               |       0.88 |                0.89 |           |       0.88 |         0.01 |<br/>| BernoulliNB                   |       0.79 |                0.75 |           |       0.77 |         0.01 |<br/>| DummyClassifier               |       0.27 |                0.33 |           |       0.11 |         0.01 |</span></pre><p id="2598" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里是分类的完整代码。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="c4f4" class="nv ls iq nr b be nw nx l ny nz"><br/>from lazypredict.Supervised import LazyClassifier<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_split<br/><br/># Load dataset<br/>data = load_breast_cancer()<br/>X = data.data<br/>y= data.target<br/><br/># Split data into train and test with a 90:10 ratio<br/>X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.1,random_state =123)<br/><br/># Initialize the Lazypredict library<br/>clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)<br/><br/># Fit all classification algorithms on training dataset<br/>models,predictions = clf.fit(X_train, X_test, y_train, y_test)<br/><br/># Calculate performance of all models on test dataset<br/>model_dictionary = clf.provide_models(X_train,X_test,y_train,y_test)<br/>models</span></pre><h1 id="1c6a" class="mq ls iq bd lt mr ms mt lw mu mv mw lz jw mx jx mc jz my ka mf kc mz kd mi na bi translated">我推荐lazypredict吗？</h1><p id="2eda" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">如果你要安装它，lazypredict使用起来非常简单。它的语法非常接近scikit-learn，使得学习曲线非常平缓。</p><p id="c700" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是它有一些致命的弱点。</p><ol class=""><li id="5643" class="nb nc iq kx b ky kz lb lc le nd li ne lm nf lq oj nh ni nj bi translated">安装困难:许多人报告了安装库的困难，因为开发人员没有添加记录他们所需依赖关系的<code class="fe oa ob oc nr b">requirements.txt</code>。</li><li id="c8b4" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated">有限的文档:我不得不梳理<a class="ae ok" href="https://github.com/shankarpandala/lazypredict/blob/dev/lazypredict/Supervised.py" rel="noopener ugc nofollow" target="_blank">源代码</a>来找出预处理是如何运行的。这并不理想。我也不知道用于执行每个分类和回归任务的超参数。</li><li id="47bd" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated">有限的可定制性:我仍然没有找到定制预处理步骤的方法。</li><li id="9ebd" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated">不清楚如何在lazypredict之后使用模型:一旦你完成了lazypredict库，你会理想地想要选择最好的算法。Lazypredict没有让这变得容易，因为您没有导出最佳算法的简单方法。</li></ol><h1 id="8d4b" class="mq ls iq bd lt mr ms mt lw mu mv mw lz jw mx jx mc jz my ka mf kc mz kd mi na bi translated">主要外卖</h1><p id="f802" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">Lazypredict的致命弱点限制了它的实用性。很好，但是还不发达。</p><p id="a1ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我强烈建议您查看一下在文档和可定制性方面更优秀的<em class="np">其他</em> <em class="np"> AutoML库</em>。</p><p id="a0ef" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是一些替代方案。</p><ol class=""><li id="44c4" class="nb nc iq kx b ky kz lb lc le nd li ne lm nf lq oj nh ni nj bi translated"><strong class="kx ir"> TPOT(此处查看如何使用</strong><a class="ae ok" href="https://medium.com/analytics-vidhya/automate-your-machine-learning-training-process-97e63c584716" rel="noopener"><strong class="kx ir">)TPOT</strong></a><strong class="kx ir">)</strong></li><li id="b52c" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated"><strong class="kx ir">自动Sklearn </strong></li><li id="02b8" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated"><strong class="kx ir">自动ViML </strong></li><li id="d433" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated">H2O汽车公司</li><li id="d29d" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated"><strong class="kx ir">自动角膜刀</strong></li><li id="2c7a" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated"><strong class="kx ir"> MLBox </strong></li><li id="6061" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated"><strong class="kx ir">超视Sklearn </strong></li><li id="29a9" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq oj nh ni nj bi translated"><strong class="kx ir">自动增长</strong></li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/73b9bf9e4fd31d91c153a5b3541f48c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TtK9L8AbmQ8m99TK"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">数据科学家+机器人=魔法。安迪·凯利在<a class="ae ok" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div><div class="ab cl on oo hu op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="ij ik il im in"><p id="24e4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我是Travis Tang，技术部的数据科学家。我分享你如何在媒体上使用开源库。我还每天在LinkedIn上分享数据分析和科学技巧。喜欢这个内容就关注我吧。</p></div></div>    
</body>
</html>