<html>
<head>
<title>A Neural Network that Can Tell the Genres of a Movie</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个可以辨别电影类型的神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-neural-network-that-can-tell-the-genres-of-a-movie-6c218061e422?source=collection_archive---------3-----------------------#2021-01-20">https://pub.towardsai.net/a-neural-network-that-can-tell-the-genres-of-a-movie-6c218061e422?source=collection_archive---------3-----------------------#2021-01-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="375e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>、<a class="ae ep" href="https://towardsai.net/p/category/web-scraping" rel="noopener ugc nofollow" target="_blank">网页抓取</a></h2><div class=""/><div class=""><h2 id="7bf5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">向任何推荐系统介绍新电影的方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/7a29edf611c9b10670d1a8b25fd6533d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wiZOWY23tYXGt2qy"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@charlesdeluvio?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Charles Deluvio </a>在<a class="ae lh" href="https://unsplash.com/s/photos/movie?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="bbd0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me">免责声明:</em> </strong> <em class="me">本文仅出于教育目的。我们不鼓励任何人抓取网站，尤其是那些可能有条款和条件反对此类行为的网站。</em></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="eb62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://towardsdatascience.com/movie-recommendation-system-based-on-movielens-ef0df580cd0e" rel="noopener" target="_blank">之前</a>，我用自然语言处理开发了一个电影推荐系统。它可以向这个平台的任何用户提供建议，而一个已建立的系统预计会不断地包括新电影。在本文中，我将介绍一种方法，当电影制作公司不提供电影类型信息时，该方法可以帮助推荐系统将新电影包含在数据集中。这种方法将利用计算机视觉(CV)来识别电影类型的任何信息。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="3031" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">数据收集</h1><p id="5563" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">首先，让我们来看看<a class="ae lh" href="https://www.kaggle.com/grouplens/movielens-20m-dataset" rel="noopener ugc nofollow" target="_blank"> MovieLens </a>数据集。我假设其列“<em class="me">流派</em>为空的条目代表还没有被记录在推荐系统中的新出现的条目。相比之下，那些“<em class="me">流派</em>”栏不为空的代表记录的流派。在这种情况下，我将尝试使用录制的电影作为训练集，其中将有一个模型被训练，而新来的电影作为测试集。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/ff3e5c8752104570c2e20c22b1ffe638.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/0*Vy8E2Ewi4O_5VAvR.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">成千上万的电影还没有类型标签，我把它们看作是新上映的还没有标签的电影。(图片由作者提供)</figcaption></figure><p id="2064" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下一步是相应地收集海报。在下图中，我们可以看到<a class="ae lh" href="https://www.themoviedb.org/" rel="noopener ugc nofollow" target="_blank">themoviedb.org</a>上的每一个电影ID都在<a class="ae lh" href="https://www.kaggle.com/grouplens/movielens-20m-dataset?select=link.csv" rel="noopener ugc nofollow" target="_blank">这个表格</a>中提供，因此可以根据它找到海报。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/60d36569b843de958ea72ea8f85120ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*PgTVg2hODLLh4jgnr9MR4A.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">MovieLens上的链接表截图(图片由作者提供)</figcaption></figure><p id="04b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，当你打开任何一页关于themoviedb.org的电影，我肯定你会首先注意到海报，这也是我下一步要做的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nl"><img src="../Images/97cdf0df603170713fffafb2c21b53bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qxT4GZDTuYpVsG7Z1QqwNQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">这就是themoviedb.org上一部电影的主页(图片由作者提供)</figcaption></figure><p id="1319" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里是我们需要收集海报的东西:<strong class="lk jd"> scrapy </strong>。因此，请确保将其安装在您的虚拟环境中(如果有):</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="f8a9" class="nr mn it nn b gy ns nt l nu nv">pip install scrapy</span></pre><p id="66d5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后你需要的是创建一个新的scrapy项目和一个网络蜘蛛</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="7c60" class="nr mn it nn b gy ns nt l nu nv">scrapy startproject first_project<br/>cd first_project<br/>scrapy genspider movielens themoviedb.org</span></pre><p id="ce16" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">给你！现在，您应该会在工作目录中看到一个具有以下结构的文件夹:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="65b4" class="nr mn it nn b gy ns nt l nu nv">first_project/<br/>├── scrapy.cfg <br/>├── first_project<br/>    ├── spiders   <br/>        ├── __init__.py<br/>        └── movielens.py  <br/>    ├── __init__.py <br/>    ├── items.py<br/>    ├── middlewares.py<br/>    ├── pipelines.py<br/>    └── settings.py</span></pre><p id="71c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们接下来要做的是让你的蜘蛛在网上抓取猎物。我把这个过程分成两步:</p><ol class=""><li id="3204" class="nw nx it lk b ll lm lo lp lr ny lv nz lz oa md ob oc od oe bi translated">获取海报图像的源URLs</li><li id="032a" class="nw nx it lk b ll of lo og lr oh lv oi lz oj md ob oc od oe bi translated">从URL下载图像到本地机器。</li></ol><p id="6d5b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这种情况下，需要两个蜘蛛，因此首先，我们应该生成另一个蜘蛛应用程序:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="ad16" class="nr mn it nn b gy ns nt l nu nv">scrapy genspider downloader themoviedb.org</span></pre><p id="8146" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后您应该会在“spiders”目录下看到一个名为downloader.py的新文件。此外，两者对应的项目字段应明确定义:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="31bf" class="nr mn it nn b gy ns nt l nu nv">import scrapy</span><span id="3dd7" class="nr mn it nn b gy ok nt l nu nv">class MovielensItem(scrapy.Item):<br/>    # define the fields for your item here like:<br/>    # name = scrapy.Field()<br/>    id = scrapy.Field()<br/>    poster = scrapy.Field()</span><span id="a6cc" class="nr mn it nn b gy ok nt l nu nv">class DownloaderItem(scrapy.Item):<br/>    image_urls = scrapy.Field()<br/>    images = scrapy.Field()</span></pre><p id="05c8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们只需要解析网站的DOM树并下载它们:</p><p id="5702" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> movielens.py </em>:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="0afc" class="nr mn it nn b gy ns nt l nu nv">import scrapy<br/>import pandas as pd<br/>from first_project.items import First_projectItem</span><span id="7325" class="nr mn it nn b gy ok nt l nu nv">class MovielensSpider(scrapy.Spider):<br/>    name = 'movielens'<br/>    allowed_domains = ['themoviedb.org']<br/>    start_urls = ['<a class="ae lh" href="https://www.themoviedb.org/movie/'" rel="noopener ugc nofollow" target="_blank">https://www.themoviedb.org/movie/'</a>]<br/>    movie_codes = pd.read_csv('data/movie_code.csv')['id']</span><span id="b7eb" class="nr mn it nn b gy ok nt l nu nv">def start_requests(self):<br/>        for code in self.movie_codes:<br/>            yield scrapy.Request(url=self.start_urls[0] + str(int(code)), callback=self.parse, dont_filter=True, meta={'id': code})</span><span id="2e21" class="nr mn it nn b gy ok nt l nu nv">def parse(self, response):<br/>        item = MovielensItem()<br/>        item['id'] = response.meta['id']<br/>        link = response.xpath('//div[<a class="ae lh" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>="image_content backdrop"]/img/@data-src').extract_first(default='')<br/>        item['poster'] = link<br/>        yield item</span></pre><p id="d86f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在你的终端中运行<strong class="lk jd"><em class="me">scrapy crawl movie lens-o movie-link . CSV</em></strong>。这个命令将从网站获取海报图像的所有源URL到指定的CSV文件:movie-link.csv。</p><p id="928a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> downloader.py </em>:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="99f3" class="nr mn it nn b gy ns nt l nu nv">import scrapy<br/>import pandas as pd<br/>from movieLens.items import DownloaderItem</span><span id="e48c" class="nr mn it nn b gy ok nt l nu nv">class DownloaderSpider(scrapy.Spider):<br/>    name = 'downloader'<br/>    allowed_domains = ['themoviedb.org']<br/>    start_urls = ['<a class="ae lh" href="http://themoviedb.org/'" rel="noopener ugc nofollow" target="_blank">http://themoviedb.org/'</a>]<br/>    posters = pd.read_csv('data/posters.csv')<br/>    posters = posters[posters['poster'].notna()]<br/>    movie_code = pd.read_csv('data/movie_code.csv')</span><span id="9515" class="nr mn it nn b gy ok nt l nu nv">    train_set = movie_code[movie_code['genres'].notna()]<br/>    train_set = pd.merge(posters, train_set, left_on='id', right_on='id')<br/>    train_set = train_set.drop(columns=['title', 'genres'])</span><span id="1dc3" class="nr mn it nn b gy ok nt l nu nv">    test_set = movie_code[movie_code['genres'].isna()]<br/>    test_set = pd.merge(posters, test_set, left_on='id', right_on='id')<br/>    test_set = test_set.drop(columns=['title', 'genres'])</span><span id="eabb" class="nr mn it nn b gy ok nt l nu nv">def parse(self, response):<br/>        item = DownloaderItem()<br/>        images = []</span><span id="299a" class="nr mn it nn b gy ok nt l nu nv">        # test<br/>        for i in range(self.test_set.shape[0]):<br/>            img_url = self.test_set['poster'].iloc[i]<br/>            image_name = str(self.test_set['id'].iloc[i])<br/>            images.append({'url': img_url, 'name': image_name, 'type': 'test'})</span><span id="7291" class="nr mn it nn b gy ok nt l nu nv">        # train<br/>        for i in range(self.train_set.shape[0]):<br/>            img_url = self.train_set['poster'].iloc[i]<br/>            image_name = str(self.train_set['id'].iloc[i])<br/>            images.append({'url': img_url, 'name': image_name, 'type': 'train'})</span><span id="a503" class="nr mn it nn b gy ok nt l nu nv">        item['image_urls'] = images<br/>        yield item</span></pre><p id="7836" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了确保所有图像都可以下载，需要一个定义图像存储位置和方式的定制管道:</p><p id="72cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> items.py </em>:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="07aa" class="nr mn it nn b gy ns nt l nu nv">from scrapy.pipelines.images import ImagesPipeline<br/>import scrapy<br/>from scrapy.exceptions import DropItem</span><span id="2532" class="nr mn it nn b gy ok nt l nu nv">class CustomedImagesPipeline(ImagesPipeline):<br/>    def get_media_requests(self, item, info):<br/>        for image in item['image_urls']:<br/>            yield scrapy.Request(image['url'], meta={'image_name': image['name'], 'type': image['type']})</span><span id="9a92" class="nr mn it nn b gy ok nt l nu nv">    def file_path(self, request, response=None, info=None):<br/>        if request.meta['type'] == 'test':<br/>            return "/test/%s.jpg" % request.meta['image_name']<br/>        else:<br/>            return "/train/%s.jpg" % request.meta['image_name']</span></pre><p id="ed47" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> settings.py </em>:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="d566" class="nr mn it nn b gy ns nt l nu nv">ITEM_PIPELINES = {'movieLens.pipelines.CustomedImagesPipeline': 1}<br/># storage repository<br/>IMAGES_STORE = 'data'</span></pre><p id="73e3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">运行<strong class="lk jd"> <em class="me"> scrapy爬虫下载器</em> </strong>，查看你的数据文件夹！</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="fe60" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">模特培训</h1><p id="b52d" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">数据清理包括两个方面:重新配置输出列和归一化图像。这个项目的输出是流派的集群，它们最初被收集在一个列表中。我将对它们进行一次性编码，从而将这个问题转化为一个多分类问题。</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="5213" class="nr mn it nn b gy ns nt l nu nv"><strong class="nn jd">from</strong> <strong class="nn jd">sklearn.preprocessing</strong> <strong class="nn jd">import</strong> MultiLabelBinarizer<br/><strong class="nn jd">import</strong> <strong class="nn jd">pandas</strong> <strong class="nn jd">as</strong> <strong class="nn jd">pd</strong></span><span id="c43e" class="nr mn it nn b gy ok nt l nu nv">movies = pd.read_csv('movies.csv', engine='python')<br/>movies = movies[['id', 'title', 'genres']]<br/><br/><em class="me"># genre is empty, for prediction</em><br/>predict_set = movies[movies['genres'].isna()]<br/>movies = movies[movies['genres'].notna()]</span><span id="f9c8" class="nr mn it nn b gy ok nt l nu nv">movies['genres'] = movies['genres'].apply(<strong class="nn jd">lambda</strong> s: [l <strong class="nn jd">for</strong> l <strong class="nn jd">in</strong> str(s).split(', ')])<br/>X_train, X_val, y_train, y_val = train_test_split(movies['id'], movies['genres'], test_size=0.2, random_state=42)</span><span id="3a5e" class="nr mn it nn b gy ok nt l nu nv">y_train = list(y_train) <br/>y_val = list(y_val)</span><span id="c112" class="nr mn it nn b gy ok nt l nu nv">mlb = MultiLabelBinarizer()<br/>mlb.fit(y_train)</span><span id="db82" class="nr mn it nn b gy ok nt l nu nv">y_train_bin = mlb.transform(y_train) y_val_bin = mlb.transform(y_val)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/73625445cb345c9bba8a8c7deff287ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*L-mjW_NhgM7Zj-fYQC60eQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">编码后的结果(图片由作者提供)</figcaption></figure><p id="e039" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">至于图像标准化，重点是我们必须从图像中提取特征，这可以通过解码和调整大小来实现:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="569a" class="nr mn it nn b gy ns nt l nu nv"><strong class="nn jd">import</strong> <strong class="nn jd">tensorflow</strong> <strong class="nn jd">as</strong> <strong class="nn jd">tf</strong></span><span id="47e0" class="nr mn it nn b gy ok nt l nu nv">IMG_SIZE = 224 <em class="me"># Specify height and width of image to match the input format of the model</em><br/>CHANNELS = 3 <em class="me"># Keep RGB color channels to match the input format of the model</em></span><span id="42cc" class="nr mn it nn b gy ok nt l nu nv"><strong class="nn jd">def</strong> parse_function(filename, label):<br/>    <em class="me">"""Function that returns a tuple of normalized image array and labels array.</em><br/><em class="me">    Args:</em><br/><em class="me">        filename: string representing path to image</em><br/><em class="me">        label: 0/1 one-dimensional array of size N_LABELS</em><br/><em class="me">    """</em><br/>    <em class="me"># Read an image from a file</em><br/>    image_string = tf.io.read_file(filename)<br/>    <em class="me"># Decode it into a dense vector</em><br/>    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)<br/>    <em class="me"># Resize it to fixed shape</em><br/>    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])<br/>    <em class="me"># Normalize it from [0, 255] to [0.0, 1.0]</em><br/>    image_normalized = image_resized / 255.0<br/>    <strong class="nn jd">return</strong> image_normalized, label</span></pre><p id="5bf7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">到目前为止，我们已经准备好了最重要的阶段:模型配置。基本上我的策略就是把一个已经建立的特征提取器(<em class="me">移动网</em>)转移到这个问题上。</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="7e03" class="nr mn it nn b gy ns nt l nu nv"><strong class="nn jd">import</strong> <strong class="nn jd">tensorflow_hub</strong> <strong class="nn jd">as</strong> <strong class="nn jd">hub</strong></span><span id="8041" class="nr mn it nn b gy ok nt l nu nv">feature_extractor_url = "https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4"<br/>feature_extractor_layer = hub.KerasLayer(feature_extractor_url,<br/>                                         input_shape=(IMG_SIZE,IMG_SIZE,CHANNELS))<br/>model = tf.keras.Sequential([<br/>    feature_extractor_layer,<br/>    layers.Dense(1024, activation='relu', name='hidden_layer'),<br/>    layers.Dense(N_LABELS, activation='sigmoid', name='output')<br/>])</span></pre><p id="8e18" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据<a class="ae lh" href="https://github.com/ashrefm" rel="noopener ugc nofollow" target="_blank"> Ashref Maiza </a>的<a class="ae lh" href="https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>，他自己开发的<em class="me">宏soft-F1 </em>是比内置的<em class="me">二元交叉熵</em>更好的损失函数选择，因为它学会了更少的“犹豫”，因此当在中间范围之间改变阈值时，系统的性能不会改变太多，这最终能够导致准确的预测。</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="7e4e" class="nr mn it nn b gy ns nt l nu nv"><strong class="nn jd">def</strong> macro_f1(y, y_hat, thresh=0.5):<br/>    <em class="me">"""Compute the macro F1-score on a batch of observations (average F1 across labels)</em><br/><em class="me">    </em><br/><em class="me">    Args:</em><br/><em class="me">        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)</em><br/><em class="me">        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)</em><br/><em class="me">        thresh: probability value above which we predict positive</em><br/><em class="me">        </em><br/><em class="me">    Returns:</em><br/><em class="me">        macro_f1 (scalar Tensor): value of macro F1 for the batch</em><br/><em class="me">    """</em><br/>    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)<br/>    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)<br/>    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)<br/>    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)<br/>    f1 = 2*tp / (2*tp + fn + fp + 1e-16)<br/>    macro_f1 = tf.reduce_mean(f1)<br/>    <strong class="nn jd">return</strong> macro_f1</span><span id="a835" class="nr mn it nn b gy ok nt l nu nv"><strong class="nn jd">def</strong> macro_soft_f1(y, y_hat):<br/>    <em class="me">"""Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).</em><br/><em class="me">    Use probability values instead of binary predictions.</em><br/><em class="me">    </em><br/><em class="me">    Args:</em><br/><em class="me">        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)</em><br/><em class="me">        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)</em><br/><em class="me">        </em><br/><em class="me">    Returns:</em><br/><em class="me">        cost (scalar Tensor): value of the cost function for the batch</em><br/><em class="me">    """</em><br/>    y = tf.cast(y, tf.float32)<br/>    y_hat = tf.cast(y_hat, tf.float32)<br/>    tp = tf.reduce_sum(y_hat * y, axis=0)<br/>    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)<br/>    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)<br/>    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)<br/>    cost = 1 - soft_f1 <em class="me"># reduce 1 - soft-f1 in order to increase soft-f1</em><br/>    macro_cost = tf.reduce_mean(cost) <em class="me"># average on all labels</em><br/>    <strong class="nn jd">return</strong> macro_cost</span></pre><p id="9521" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，模型拟合过程将类似于:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="8882" class="nr mn it nn b gy ns nt l nu nv">model.compile(   optimizer=tf.keras.optimizers.Adam(learning_rate=LR),   loss=macro_soft_f1, metrics=[macro_f1])</span><span id="6221" class="nr mn it nn b gy ok nt l nu nv"><strong class="nn jd">with</strong> tf.device('/device:GPU:0'):<br/>    history = model.fit(train_ds, epochs=EPOCHS, steps_per_epoch=70,<br/>              validation_data=create_dataset(X_val, y_val_bin),<br/>              validation_steps=70)</span></pre><p id="7fa9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后但同样重要的是，我们只需要多一个预测功能来“猜测”一部电影应该包括哪些类型:</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="a9ac" class="nr mn it nn b gy ns nt l nu nv"><strong class="nn jd">def</strong> show_prediction(index, movies_df, model):<br/>    <br/>    <em class="me"># Get movie info</em><br/>    imdbId = movies_df['id'].iloc[index]<br/>    img_path = os.path.join('posters/test/', str(imdbId)+'.jpg')<br/><br/>    <em class="me"># Read and prepare image</em><br/>    img = image.load_img(img_path, target_size=(IMG_SIZE,IMG_SIZE,CHANNELS))<br/>    img = image.img_to_array(img)<br/>    img = img/255<br/>    img = np.expand_dims(img, axis=0)<br/><br/>    <em class="me"># Generate prediction</em><br/>    prediction = (model.predict(img) &gt; 0.6).astype('int')<br/>    prediction = pd.Series(prediction[0])<br/>    prediction.index = mlb.classes_<br/>    prediction = prediction[prediction==1].index.values<br/><br/>    <em class="me"># Dispaly image with prediction</em><br/>    style.use('default')<br/>    plt.figure(figsize=(8,4))<br/>    plt.imshow(Image.open(img_path))<br/>    plt.title('<strong class="nn jd">\n{}\n</strong>Genre Prediction<strong class="nn jd">\n{}</strong>'.format(movies_df['title'].iloc[index], list(prediction)), fontsize=9)<br/>    plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/9b90f0be5815488483e7bd88b1ea4716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*0bdkrSvfIKwr_MfU3J3nVw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">类型预测示例(作者图片)</figcaption></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="1c69" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">万岁！到目前为止，你已经能够训练一个神经网络来帮助你“猜测”一部电影应该有什么类型。这个模型是可扩展的，你可以开发一个更复杂的模型来提高精确度。有了这种预测，您还可以扩展您的电影推荐集，并让它们以流媒体方式播放！</p><p id="137b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你对我的项目感兴趣，请查看我的回购:</p><div class="on oo gp gr op oq"><a href="https://github.com/MemphisMeng/Brilliant-Recommendation-System/blob/master/notebooks/poster_detection.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd jd gy z fp ov fr fs ow fu fw jc bi translated">MemphisMeng/精彩推荐系统</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">用ML Summer 2020保温箱做的。通过创建……为MemphisMeng/Brilliant-推荐信系统的开发做出贡献</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">github.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe lb oq"/></div></div></a></div></div></div>    
</body>
</html>