<html>
<head>
<title>HAC: Learning Multi-Level Hierarchies with Hindsight</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">HAC:事后学习多层次结构</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/learning-multi-level-hierarchies-with-hindsight-1f50d28cb81c?source=collection_archive---------0-----------------------#2019-07-02">https://pub.towardsai.net/learning-multi-level-hierarchies-with-hindsight-1f50d28cb81c?source=collection_archive---------0-----------------------#2019-07-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4a4f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">一种新颖的面向人工智能的分层强化学习方法</h2><div class=""/><div class=""><h2 id="c050" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用于多级层级的后见之明体验重放</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1ba0b272e202bf30f1eec5de9a078179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*NNaMK_Owf0TJGT2ppux_Hg.gif"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">倒立摆上的四级HAC智能体</figcaption></figure><h2 id="0244" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">介绍</h2><p id="bbee" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">我们讨论了一个新颖的分层强化学习(HRL)框架，它可以有效地并行学习多个层次的策略。实验表明，由Andrew Levy等人在2019年ICLR会议上提出的这一框架可以显著加速稀疏回报问题的学习，特别是那些目标是达到某种目标状态的问题。值得注意的是，这是第一个成功地在具有连续状态和动作空间的任务中并行学习3级层次的框架。作者所做的一些实验甚至展示了它驾驭4层层次结构的能力。这个<a class="ae mv" href="https://www.youtube.com/watch?v=DYcVTveeNK0" rel="noopener ugc nofollow" target="_blank">视频</a>展示了它在2级和3级层级中的能力，<a class="ae mv" href="https://www.youtube.com/watch?v=Q_NGMkQ29oU" rel="noopener ugc nofollow" target="_blank">这个</a>用4级代理做了一个简单的演示。</p><h2 id="ce83" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">概观</h2><p id="5740" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">我们首先描述这个框架的整体架构和目的——从这里开始，我们关注它在连续域中的衍生算法，即分层行动者-批评家(HAC)；离散域中的算法，分层Q-学习(HierQ)，应该是几乎相同的，除了它在每一层使用<em class="mw">Q</em>-学习。</p><ul class=""><li id="8635" class="mx my it me b mf mz mi na lq nb lu nc ly nd mu ne nf ng nh bi translated">HAC是为目标导向的任务而设计的，在这种任务中，代理人试图达到某种目标状态——达到目标，我们的意思是代理人在一定范围内接近目标。正如我们将很快看到的，这为HAC提供了在不同情况下设计自己的奖励的灵活性。</li><li id="d38c" class="mx my it me b mf ni mi nj lq nk lu nl ly nm mu ne nf ng nh bi translated">HAC在每一层使用一种非策略RL方法:它可以是<em class="mw">Q</em>-离散动作空间的学习变量或连续空间的DDPG变量。转换被定义为<em class="mw">【初始状态、动作、奖励、下一状态、目标、折扣率】</em>形式的元组。对于低级策略，HAC对策略为达到子目标而采取的最大动作数量<em class="mw"> H </em>进行了明确的约束。</li></ul><p id="196c" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">为了更好地理解，我们也给出了HAC和HIRO之间的简单比较，后者是我们在<a class="ae mv" href="https://towardsdatascience.com/data-efficient-hierarchical-reinforcement-learning-hiro-5d8538c27a80?source=friends_link&amp;sk=9a800a0ccd8f074bbfec7dd09e9835a2" rel="noopener" target="_blank">上一篇文章</a>中讨论过的一种HRL算法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/fb2322b1fe7723942d6624dd344ed181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGeQTRTbcLg5dre3m2Z_ag.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">后知后觉地学习多层次结构。HAC (2级)和HIRO的性能比较。图表显示了平均成功率和1个标准差。</figcaption></figure><p id="cc70" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">HIRO和HAC的目标都是高效地并行学习多个层次的基于价值的政策，但它们在一些基本方面有所不同:1)在高层，HAC以目标状态为子目标，而HIRO以当前状态和目标状态之间的差异为子目标2) HAC不依赖于任何外部奖励；它给没有达到目标的过渡以负奖励，否则为0。另一方面，HIRO采用欧几里德距离作为低级奖励，外部奖励作为高级奖励。3)与HIRO的转变不同，HAC的转变有三个来源:后见之明行动转变、后见之明目标转变和子目标测试转变。</p><p id="6231" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">在本文的其余部分，我们将集中讨论HAC中的转换，看看它们如何帮助共同学习多级策略和处理稀疏奖励问题。</p><h2 id="0238" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">事后行动转变(HAT)</h2><p id="833b" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">大多数基于值(非策略)的强化学习算法都是建立在贝尔曼方程的基础上的，它依靠一个稳定的转移函数来保证理论上的收敛性。在分层强化学习中，高级策略通常面临非平稳的转移函数，因为高级转移函数通常依赖于低级策略，而低级策略随时间不断演化。由于探测通常会引入随机性，使得高级转换函数更加不确定，因此低级探测可能会进一步加剧这种情况。HIRO和诺尔-HRL在以前的文章中讨论过，他们求助于目标重新标记来恢复转换元组。但这更像是一种特别的技术，可能很难推广到两级以上的层次结构。HAC通过训练关于<em class="mw">后见之明行动转移</em>的高层策略来克服这些不稳定问题，这使用<strong class="me jd">最优低层策略层级</strong>来模拟转移函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/64778ffdaeea46e4282fa61debb848cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VB2PikfrCyCOtdNMzQ6uyw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">事后学习多层次结构的例子。轨迹上的tic标记显示了机器人在执行每个原始动作后的下一个状态。粉色圆圈显示了最初的子目标操作。灰色圆圈显示了低级策略最多执行H个操作后所达到的子目标状态。</figcaption></figure><p id="9ae9" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">以报纸上的例子为例。考虑一个两级代理，假设该代理从<em class="mw"> s₀ </em>出发，试图到达黄旗。高层策略计划底层策略要实现的子目标<em class="mw"> g₀ </em>。底层策略使用某种行为策略执行<em class="mw"> H </em>原始动作，但无法实现<em class="mw"> g₀ </em>，而不是在<em class="mw"> s₁ </em>落地。现在我们可以谱写第一个后见之明的动作过渡，<em class="mw">【s₀，s₁，-1，s₁，黄旗，γ】</em>。请注意，如果目标(黄色标志)未实现，奖励始终为<em class="mw"> -1 </em>，否则为<em class="mw"> 0 </em>。此外，我们不再使用子目标<em class="mw"> g₀ </em>作为动作，而是使用<em class="mw"> s₁ </em>，这正是元组中的下一个状态。为了更好地理解它，我们可以这样理解:g₀表示由高层政策选择的行动，而s₁是一些考虑到探索的高层行为政策的结果。以这种方式，转换总是采取好像低级策略是最优的，因此成功地实现了子目标。</p><p id="d72f" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">这些转换还有一个潜在的好处:因为由高层产生的子目标总是远离初始状态的<em class="mw"> H </em>低层动作，通过这些转换，高层策略学习符合每个高层动作的<em class="mw"> H </em>低层动作的时间尺度的子目标。</p><h2 id="f4bc" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">后见之明目标转换(HGT)</h2><p id="3a8a" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">后见之明的行动转换遭受稀疏回报问题，这使得它很难学习。为了让每一个层次都能够更有效地学习，作者提出了<em class="mw">后见之明目标转换</em>，将后见之明体验回放(HER)的想法扩展到了层级设置。在执行每个<em class="mw"> H </em>动作之后，对于每个转换，我们选择一个未来状态作为它的新目标。如果新目标没有实现，奖励为<em class="mw"> -1 </em>，否则奖励为<em class="mw"> 0 </em>。我们对所有级别都这样做。</p><p id="c3fc" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">我们再次以上一节中的例子来说明。假设我们想把她申请到<em class="mw">【s₀，s₁，-1，s₁，黄旗，γ】</em>。如果我们恰好把<em class="mw"> s₁ </em>作为新目标，那么后见之明的目标跃迁就是<em class="mw">【s₀，s₁，0，s₁，s₁，γ】</em>；如果我们把<em class="mw"> s₄ </em>作为新的目标，那么我们就会有<em class="mw">【s₀，s₁，-1，s₁，s₄，γ】</em>。</p><h2 id="4ef8" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated">子目标测试转换(STT)</h2><p id="2c7b" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">后见之明行动和后见之明目标转换使多级策略能够并行有效地学习，但一些关键问题仍然存在。最严重的缺陷是，该策略只允许一个级别了解其较低级别的策略层次能够实现的一组有限的子目标状态。这可能会有问题，特别是对于我们上面使用的奖励函数。回想一下，如果目标没有实现，我们将奖励定义为<em class="mw"> -1 </em>，否则为<em class="mw"> 0 </em>。<em class="mw">Q</em>-一般来说，较低级别的策略层级可实现的那些状态-动作对的值是负值，并且可能小于较低级别的策略层级无法实现的值。因此，高级策略将更喜欢子目标的路径，而它的低级策略层次结构还不能实现这些子目标。</p><p id="6528" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">为了解决这些问题，我们需要惩罚那些较低层次的政策层级无法实现的子目标。在表格RL中，人们可以通过用大的负值初始化<em class="mw"> Q </em>值来容易地实现这一点，但是这不是函数近似的情况，在函数近似中，所有的<em class="mw"> Q </em>值共享相同的参数集。那么，我们该怎么办呢？Levy等人引入了<em class="mw">子目标测试转换</em>来惩罚那些当前策略无法实现的子目标。下面是它的工作原理(请注意<em class="mw">子目标测试</em>和<em class="mw">更低级子目标测试</em>的区别:前者指的是我们所说的层的子目标测试)。</p><ol class=""><li id="dfe8" class="mx my it me b mf mz mi na lq nb lu nc ly nd mu ns nf ng nh bi translated">对于每个级别，我们定义一个布尔变量，<em class="mw">子目标测试</em>。因为每个层的子目标由其较高级别指定，所以该变量指示其较高级别是否将子目标测试转换添加到重放缓冲区。这些变量被初始化为假，只被更高层修改。</li><li id="ed90" class="mx my it me b mf ni mi nj lq nk lu nl ly nm mu ns nf ng nh bi translated">在某一级的动作选择阶段，如果其子目标测试为真，则将其下级子目标测试设置为真；如果没有，我们给它一些机会(<a class="ae mv" href="https://github.com/andrew-j-levy/Hierarchical-Actor-Critc-HAC-/blob/504288596f79ebd5494cfd0978d66e1f7cc7c7df/design_agent_and_env.py#L125" rel="noopener ugc nofollow" target="_blank"> <em class="mw"> 30% </em> </a> by <a class="ae mv" href="https://github.com/andrew-j-levy/Hierarchical-Actor-Critc-HAC-/blob/504288596f79ebd5494cfd0978d66e1f7cc7c7df/layer.py#L109" rel="noopener ugc nofollow" target="_blank">他们的实现</a>)来设置它的较低级的子目标测试为真。</li><li id="02fd" class="mx my it me b mf ni mi nj lq nk lu nl ly nm mu ns nf ng nh bi translated">在每一层，如果它的子目标测试被它的更高层设置为真，我们采取由策略产生的确定性动作，因为我们想知道当前策略是否能够实现子目标；如果没有，我们就像平常一样，在探索的过程中加入一些噪音。</li><li id="46b1" class="mx my it me b mf ni mi nj lq nk lu nl ly nm mu ns nf ng nh bi translated">在每一层，如果较低级别的子目标测试为真，而较低级别的策略层次结构未能实现子目标，则我们将子目标测试转换添加到重放缓冲区。这种转变将子目标作为行动，并使用低奖励，即<em class="mw">惩罚</em>。在他们的实验中，他们设置了<em class="mw">惩罚=-H </em>，子目标最大视界的负值。</li></ol><p id="024b" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">在第2部分中，我们只给了<em class="mw"> 30% </em>个机会来将较低级别的子目标测试设置为真。对于那些无法通过当前政策实现，但仍可能通过<em class="mw"> H </em>步骤中的一些优化政策实现的子目标，这有助于他们的<em class="mw">Q</em>-值向目标<em class="mw"> Q </em>倾斜-值由事后诸葛亮的行动/目标转变规定。</p><h1 id="a0c0" class="nt li it bd lj nu nv nw lm nx ny nz lp ki oa kj lt kl ob km lx ko oc kp mb od bi translated">过渡摘要</h1><p id="e31a" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">事后看来，行动转变时，子目标层可以专注于学习能够达到目标状态的子目标状态序列，而较低级别的策略则专注于学习实现这些子目标状态的行动序列。</p><p id="0b2c" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">事后诸葛亮的目标转变帮助每一级在稀疏的奖励任务中学习目标制约的策略。</p><p id="23f5" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">子目标测试转换会惩罚较低级别的策略层次结构还无法实现的转换，以减少寻求风险的行为。</p><h1 id="22f0" class="nt li it bd lj nu nv nw lm nx ny nz lp ki oa kj lt kl ob km lx ko oc kp mb od bi translated">算法</h1><p id="96ab" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">为了完整起见，我们现在给出伪代码</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/e42b6b512c58c5e0b552ec03a7012303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V9fUrnHNOs6omIl2OIbOdg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">事后诸葛亮学习多层次结构的伪代码</figcaption></figure><p id="406b" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">请注意，在底层<em class="mw"> i=0 </em>时，既不会产生事后分析动作转换，也不会产生子目标测试转换。</p><h1 id="8933" class="nt li it bd lj nu nv nw lm nx ny nz lp ki oa kj lt kl ob km lx ko oc kp mb od bi translated">参考</h1><p id="1cfb" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">Ofir Nachum等.数据-有效的分层强化学习</p><p id="7556" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">Marcin Andrychowicz等.事后诸葛亮经验重演</p><p id="28d5" class="pw-post-body-paragraph mc md it me b mf mz kd mh mi na kg mk lq nn mm mn lu no mp mq ly np ms mt mu im bi translated">后见之明学习多层次层次</p></div></div>    
</body>
</html>