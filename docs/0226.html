<html>
<head>
<title>Keras for Multi-label Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多标签文本分类的Keras</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/keras-for-multi-label-text-classification-86d194311d0e?source=collection_archive---------0-----------------------#2019-12-07">https://pub.towardsai.net/keras-for-multi-label-text-classification-86d194311d0e?source=collection_archive---------0-----------------------#2019-12-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c3807689996243f12d0ccd04d62682f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xNVwgqwOLkCmmOPIPh94Mw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://pixabay.com/photos/wintry-dawn-secret-light-trees-4532412/" rel="noopener ugc nofollow" target="_blank">https://pix abay . com/photos/wintry-dawn-secret-light-trees-4532412/</a></figcaption></figure><h2 id="fb0f" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="7159" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">使用Keras进行多标签文本分类的CNN和LSTMs体系结构</h2></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="c5ac" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">多标签分类可能会变得很棘手，而使用Keras中的预建库使其工作变得更加棘手。这个博客有助于使用<a class="ae jd" href="https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2" rel="noopener" target="_blank">CNN</a>和<a class="ae jd" href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noopener" target="_blank">lstm</a>的多标签分类的工作架构。</p><p id="91fb" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">传统上，多标签分类被用于预测电影概要中的标签、预测YouTube视频上的标签等。</p><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/9731527f4e8363b62a34396e1ec0cec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LKnmBSisRcruSicqqvUeYA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://www.imdb.com/title/tt7286456/?pf_rd_m=A2FGELUUNOQJNL&amp;pf_rd_p=2aa3b559-b3ee-42e7-a17a-c09997710b8c&amp;pf_rd_r=NBBHYD1GYWMXHMYMBSK4&amp;pf_rd_s=center-3&amp;pf_rd_t=15061&amp;pf_rd_i=homepage&amp;ref_=hm_pks_bfc_lk1" rel="noopener ugc nofollow" target="_blank">IMDB网站上的电影流派标签。</a></figcaption></figure><h1 id="33ab" class="mm mn jg bd mo mp mq mr ms mt mu mv mw kv mx kw my ky mz kz na lb nb lc nc nd bi translated"><strong class="ak">让我们定义一下什么是多标签分类？</strong></h1><p id="aa69" class="pw-post-body-paragraph ll lm jg ln b lo ne kq lq lr nf kt lt lu ng lw lx ly nh ma mb mc ni me mf mg ij bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Multi-label_classification" rel="noopener ugc nofollow" target="_blank">多标签</a>分类是多类分类的推广，多类分类是将实例准确分类到两个以上类别中的一个类别的单标签问题，在多标签问题中，对实例可以被分配到多少个类别没有限制，即在用于训练的输出数据中可以有一个、两个或多个标签。</p><p id="d570" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">使用的度量:</strong></p><p id="94e6" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank"> F1得分</a> : F1得分是使用精确度和召回率的调和平均值计算的。</p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="309a" class="no mn jg nk b gy np nq l nr ns">F1 Score = 2 * (precision * recall) / (precision + recall)</span></pre><p id="ee24" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">该F1分数被微平均以将其用作多类分类的度量。它通过计算真阳性、假阳性、真阴性和假阴性的值来计算。在这种情况下，所有预测输出都是列索引，默认情况下按排序顺序使用。</p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="183c" class="no mn jg nk b gy np nq l nr ns"><strong class="nk jq">def</strong> f1micro(y_true, y_pred):<br/>    <strong class="nk jq">return</strong> tf.py_func(f1_score(y_true, y_pred,average='micro'),tf.double)</span></pre><h1 id="a087" class="mm mn jg bd mo mp mq mr ms mt mu mv mw kv mx kw my ky mz kz na lb nb lc nc nd bi translated"><strong class="ak">数据及其理解:</strong></h1><p id="909e" class="pw-post-body-paragraph ll lm jg ln b lo ne kq lq lr nf kt lt lu ng lw lx ly nh ma mb mc ni me mf mg ij bi translated"><a class="ae jd" href="https://www.kaggle.com/cryptexcode/mpst-movie-plot-synopses-with-tags" rel="noopener ugc nofollow" target="_blank">本图使用的</a>数据取自<a class="ae jd" href="https://www.kaggle.com/cryptexcode/mpst-movie-plot-synopses-with-tags" rel="noopener ugc nofollow" target="_blank">卡格尔</a>MPST——电影剧情简介数据。</p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="50fe" class="no mn jg nk b gy np nq l nr ns">df = pd.read_csv(r'F:\mpst_full_data.csv', delimiter=',')<br/>nRow, nCol = df.shape<br/>df.head(5)</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/506a666e6233d8976f3c320e72ed2fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4cc3zsNBati2WcsQbYLUiA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">数据集中的前5行</figcaption></figure><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/c4bdaa09fe4e128c1e870e0f0636eed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6vhife7s2QMr2vo0UcGcg.png"/></div></div></figure><h1 id="d2ac" class="mm mn jg bd mo mp mq mr ms mt mu mv mw kv mx kw my ky mz kz na lb nb lc nc nd bi translated">数据清理</h1><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="9ecf" class="no mn jg nk b gy np nq l nr ns"><strong class="nk jq">import</strong> <strong class="nk jq">re</strong><br/><br/><strong class="nk jq">def</strong> decontracted(phrase):<br/>    <em class="nv"># specific</em><br/>    phrase = re.sub(r"won't", "will not", phrase)<br/>    phrase = re.sub(r"can\'t", "can not", phrase)<br/><br/>    <em class="nv"># general</em><br/>    phrase = re.sub(r"n\'t", " not", phrase)<br/>    phrase = re.sub(r"\'re", " are", phrase)<br/>    phrase = re.sub(r"\'s", " is", phrase)<br/>    phrase = re.sub(r"\'d", " would", phrase)<br/>    phrase = re.sub(r"\'ll", " will", phrase)<br/>    phrase = re.sub(r"\'t", " not", phrase)<br/>    phrase = re.sub(r"\'ve", " have", phrase)<br/>    phrase = re.sub(r"\'m", " am", phrase)<br/>    <strong class="nk jq">return</strong> phrase</span><span id="0e45" class="no mn jg nk b gy nw nq l nr ns">stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've",\<br/>            "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \<br/>            'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their',\<br/>            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', \<br/>            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \<br/>            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \<br/>            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\<br/>            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\<br/>            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\<br/>            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \<br/>            's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', \<br/>            've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn',\<br/>            "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn',\<br/>            "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", \<br/>            'won', "won't", 'wouldn', "wouldn't"])</span></pre><p id="cb70" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">上面定义的函数“decontracted”从数据框中提取一个文本列，并删除所有HTML标签和特殊字符。</p><p id="8ca2" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在下面给出的代码片段中，数据集中提供的情节提要已经被清除。</p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="d4f3" class="no mn jg nk b gy np nq l nr ns"><strong class="nk jq">from</strong> <strong class="nk jq">tqdm</strong> <strong class="nk jq">import</strong> tqdm<br/>preprocessed_synopsis = []<br/><em class="nv"># tqdm is for printing the status bar</em><br/><strong class="nk jq">for</strong> sentance <strong class="nk jq">in</strong> df['plot_synopsis'].values:<br/>    sentance = re.sub(r"http\S+", "", sentance)<br/>    sentance = BeautifulSoup(sentance, 'lxml').get_text()<br/>    sentance = decontracted(sentance)<br/>    sentance = re.sub("\S*\d\S*", "", sentance).strip()<br/>    sentance = re.sub('[^A-Za-z]+', ' ', sentance)<br/>    <em class="nv"># https://gist.github.com/sebleier/554280</em><br/>    sentance = ' '.join(e.lower() <strong class="nk jq">for</strong> e <strong class="nk jq">in</strong> sentance.split() <strong class="nk jq">if</strong> e.lower() <strong class="nk jq">not</strong> <strong class="nk jq">in</strong> stopwords)<br/>    preprocessed_synopsis.append(sentance.strip())</span><span id="45ea" class="no mn jg nk b gy nw nq l nr ns">df['preprocessed_plots']=preprocessed_synopsis</span></pre><h1 id="44c9" class="mm mn jg bd mo mp mq mr ms mt mu mv mw kv mx kw my ky mz kz na lb nb lc nc nd bi translated"><strong class="ak">培训和测试拆分</strong></h1><p id="44d7" class="pw-post-body-paragraph ll lm jg ln b lo ne kq lq lr nf kt lt lu ng lw lx ly nh ma mb mc ni me mf mg ij bi translated">在数据集的输出标签中，电影类型使用“，”分隔，在<a class="ae jd" href="https://towardsdatascience.com/tagged/one-hot-encoder" rel="noopener" target="_blank">一次性编码</a>之前已经清除。因此，在从输出标签中删除空格后，数据被分成训练和测试数据集。</p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="da25" class="no mn jg nk b gy np nq l nr ns"><strong class="nk jq">def</strong> remove_spaces(x):<br/>    x=x.split(",")<br/>    nospace=[]<br/>    <strong class="nk jq">for</strong> item <strong class="nk jq">in</strong> x:<br/>        item=item.lstrip()<br/>        nospace.append(item)<br/>    <strong class="nk jq">return</strong> (",").join(nospace)</span><span id="7bd5" class="no mn jg nk b gy nw nq l nr ns">df['tags']=df['tags'].apply(remove_space</span><span id="f95f" class="no mn jg nk b gy nw nq l nr ns">train=df.loc[df.split=='train']<br/># cv=df.loc[df.split=="val"]<br/># cv=cv.reset_index()<br/>train=train.reset_index()<br/>test=df.loc[df.split=='test']<br/>test=test.reset_index()</span></pre><h1 id="a530" class="mm mn jg bd mo mp mq mr ms mt mu mv mw kv mx kw my ky mz kz na lb nb lc nc nd bi translated">为培训和测试准备标签</h1><p id="70df" class="pw-post-body-paragraph ll lm jg ln b lo ne kq lq lr nf kt lt lu ng lw lx ly nh ma mb mc ni me mf mg ij bi translated">由于这是一个多标签分类，所以输出标签需要一个热编码。为此，我们使用了使用sci-kit learn方法的<a class="ae jd" href="https://scikit-learn.org/stable/modules/feature_extraction.html" rel="noopener ugc nofollow" target="_blank">单词袋</a>技术。</p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="bdf3" class="no mn jg nk b gy np nq l nr ns">vectorizer = CountVectorizer(tokenizer = <strong class="nk jq">lambda</strong> x: x.split(","), binary='true')<br/>y_train = vectorizer.fit_transform(train['tags']).toarray()<br/>y_test=vectorizer.transform(test['tags']).toarray()</span></pre><p id="ffb7" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">输入序列的最大长度</strong></p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="33a8" class="no mn jg nk b gy np nq l nr ns"><strong class="nk jq">def</strong> max_len(x):<br/>    a=x.split()<br/>    <strong class="nk jq">return</strong> len(a)<br/>In [23]:</span><span id="d682" class="no mn jg nk b gy nw nq l nr ns">max(df['plot_synopsis'].apply(max_len))</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/56ef32cee77416d5042987de8ee1e641.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/1*XRWkrKg6Wmuzs7pex_dRUQ.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">绘图的最大长度</figcaption></figure><p id="5259" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">词汇量</strong></p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="1707" class="no mn jg nk b gy np nq l nr ns">vect=Tokenizer()<br/>vect.fit_on_texts(train['plot_synopsis'])<br/>vocab_size = len(vect.word_index) + 1<br/>print(vocab_size)</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/eacf0a6ac2a8ebf06f522080193bbb22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P5A4-fDFeLQICP49_S6kJQ.jpeg"/></div></div></figure><h1 id="c620" class="mm mn jg bd mo mp mq mr ms mt mu mv mw kv mx kw my ky mz kz na lb nb lc nc nd bi translated"><strong class="ak">使用LSTMs建模</strong></h1><ol class=""><li id="6b1b" class="nz oa jg ln b lo ne lr nf lu ob ly oc mc od mg oe of og oh bi translated"><strong class="ln jq">填充并使所有输入序列长度相同，准备输入序列</strong></li></ol><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="5c3f" class="no mn jg nk b gy np nq l nr ns">encoded_docs_train = vect.texts_to_sequences(train['preprocessed_plots'])<br/>max_length = vocab_size<br/>padded_docs_train = pad_sequences(encoded_docs_train, maxlen=1200, padding='post')<br/>print(padded_docs_train)</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/8e57ec9e42bf6b6434ad0643d50a5f3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kw0QILcv27tEV_ClfJPzCA.jpeg"/></div></div></figure><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="330b" class="no mn jg nk b gy np nq l nr ns">encoded_docs_test =  vect.texts_to_sequences(test['preprocessed_plots'])<br/>padded_docs_test = pad_sequences(encoded_docs_test, maxlen=1200, padding='post')<br/>encoded_docs_cv = vect.texts_to_sequences(cv['preprocessed_plots'])<br/>padded_docs_cv = pad_sequences(encoded_docs_cv, maxlen=1200, padding='post')</span></pre><p id="d356" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq"> 2。定义模型:</strong>对于这个问题，我们使用<a class="ae jd" href="https://keras.io/layers/embeddings/" rel="noopener ugc nofollow" target="_blank">嵌入层</a>作为第一层，使用71(唯一标签总数)维密集层作为输出层。</p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="132b" class="no mn jg nk b gy np nq l nr ns">model = Sequential()<br/><em class="nv"># Configuring the parameters</em><br/>model.add(Embedding(vocab_size, output_dim=50, input_length=1200))<br/>model.add(LSTM(128, return_sequences=<strong class="nk jq">True</strong>))  <br/><em class="nv"># Adding a dropout layer</em><br/>model.add(Dropout(0.5))<br/>model.add(LSTM(64))<br/>model.add(Dropout(0.5))<br/><em class="nv"># Adding a dense output layer with sigmoid activation</em><br/>model.add(Dense(n_classes, activation='sigmoid'))<br/>model.summary()</span><span id="b71c" class="no mn jg nk b gy nw nq l nr ns">out[]:</span><span id="9e01" class="no mn jg nk b gy nw nq l nr ns">_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_6 (Embedding)      (None, 1200, 50)          4939100   <br/>_________________________________________________________________<br/>lstm_4 (LSTM)                (None, 1200, 128)         91648     <br/>_________________________________________________________________<br/>dropout_6 (Dropout)          (None, 1200, 128)         0         <br/>_________________________________________________________________<br/>lstm_5 (LSTM)                (None, 64)                49408     <br/>_________________________________________________________________<br/>dropout_7 (Dropout)          (None, 64)                0         <br/>_________________________________________________________________<br/>dense_5 (Dense)              (None, 71)                4615      <br/>=================================================================<br/>Total params: 5,084,771<br/>Trainable params: 5,084,771<br/>Non-trainable params: 0</span></pre><p id="b7b7" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">为什么最后致密层是Sigmoid而不是Softmax？</p><p id="7146" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在上述架构的最后一层，使用sigmoid函数代替softmax。与Softmax相比，使用sigmoid的优势在于一个大纲可能有许多可能的类型。使用Softmax函数意味着一种风格出现的概率取决于其他风格的出现。但是对于这个应用程序来说，我们需要一个函数来给出流派出现的分数，这将独立于任何其他电影流派的出现。</p><div class="ip iq gp gr ir oj"><a href="https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jq gy z fp oo fr fs op fu fw jp bi translated">python中神经网络的多类多标签分类指南</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">通常在机器学习任务中，一个样本有多个可能的标签，但这些标签并不相互排斥。这个…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">www.depends-on-the-definition.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ix oj"/></div></div></a></div><p id="876a" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq"> 3 </strong>。<strong class="ln jq">使用‘Adam’作为优化器和二进制交叉熵作为损失函数的训练。</strong></p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="c83a" class="no mn jg nk b gy np nq l nr ns">model.compile(optimizer='adam', loss='binary_crossentropy')<br/>history = model.fit(padded_docs_train, y_train,<br/>                    class_weight='balanced',<br/>                    epochs=5,<br/>                    batch_size=32,<br/>                    validation_split=0.1,<br/>                    callbacks=[])</span></pre><p id="fce6" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">4.模型的分析和f1微分数的计算:模型中的最终密集层具有71(独特电影类型的总数)个维度。输出中的每个维度都有一个介于0和1之间的分数，0是任何流派最不可能的分数，1是最好的分数。</p><p id="78eb" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">已经定义了阈值矩阵，其值在0.1到0.9的范围内。然后，我们对预测的输出运行一个循环，并将其与阈值进行比较，只有当标签的相应值大于阈值时才选择标签。</p><p id="a972" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">这在两个方面有所帮助:</p><ol class=""><li id="4540" class="nz oa jg ln b lo lp lr ls lu oy ly oz mc pa mg oe of og oh bi translated">选择最佳阈值，并使用它来预测标签。</li><li id="59e5" class="nz oa jg ln b lo pb lr pc lu pd ly pe mc pf mg oe of og oh bi translated">通过比较每次迭代中预测的标签和测试数据集中的原始标签，计算微观平均F1分数。</li></ol><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="8847" class="no mn jg nk b gy np nq l nr ns">predictions=model.predict([padded_docs_test])<br/>thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]</span><span id="4739" class="no mn jg nk b gy nw nq l nr ns"><strong class="nk jq">for</strong> val <strong class="nk jq">in</strong> thresholds:<br/>    pred=predictions.copy()<br/>  <br/>    pred[pred&gt;=val]=1<br/>    pred[pred&lt;val]=0<br/>  <br/>    precision = precision_score(y_test, pred, average='micro')<br/>    recall = recall_score(y_test, pred, average='micro')<br/>    f1 = f1_score(y_test, pred, average='micro')<br/>   <br/>    print("Micro-average quality numbers")<br/>    print("Precision: <strong class="nk jq">{:.4f}</strong>, Recall: <strong class="nk jq">{:.4f}</strong>, F1-measure: <strong class="nk jq">{:.4f}</strong>".format(precision, recall, f1))<br/></span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/ff48d88dc4fceaa8e6e43497844e036d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*Ndv11qw3-qjMMKJqb0yTUg.jpeg"/></div></figure><p id="0d2e" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">不同阈值的F1分数表示F1度量分数如何随不同阈值而变化。它按照预期的那样进行-非常大或非常小的阈值给出了较低的F1度量得分值，因为当基于较低的阈值选择标签时，选择了太多的标签，这降低了F1度量得分，而当阈值变得非常大时，几乎没有标签被选择，从而降低了性能度量。</p><h1 id="00a5" class="mm mn jg bd mo mp mq mr ms mt mu mv mw kv mx kw my ky mz kz na lb nb lc nc nd bi translated"><strong class="ak">使用CNN建模:</strong></h1><ol class=""><li id="b0c7" class="nz oa jg ln b lo ne lr nf lu ob ly oc mc od mg oe of og oh bi translated">第一步与我们在上面的LSTMs模型中所做的一样。这里的第一层也是一样的，我们使用了一个嵌入层，后面是完全连接的层。人们可以使用其他变化和层的深度，也可以尝试不同的辍学值。</li></ol><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="0e5b" class="no mn jg nk b gy np nq l nr ns">model = Sequential()<br/>model.add(Embedding(vocab_size, 71, input_length=1200))<br/>model.add(Conv1D(64, 3, activation='sigmoid'))<br/>model.add(Conv1D(100, 3, activation='sigmoid'))<br/>model.add(Conv1D(100, 3, activation='sigmoid'))<br/><em class="nv"># model.add(Dropout(0.70))<br/></em>model.add(Conv1D(48, 3, activation='sigmoid'))<br/>model.add(Flatten())<br/>model.add(Dense(71))<br/><br/>model.summary()</span><span id="bd93" class="no mn jg nk b gy nw nq l nr ns">out[]:</span><span id="9064" class="no mn jg nk b gy nw nq l nr ns">_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_5 (Embedding)      (None, 1200, 71)          8675845   <br/>_________________________________________________________________<br/>conv1d_5 (Conv1D)            (None, 1198, 64)          13696     <br/>_________________________________________________________________<br/>conv1d_6 (Conv1D)            (None, 1196, 100)         19300     <br/>_________________________________________________________________<br/>conv1d_7 (Conv1D)            (None, 1194, 100)         30100     <br/>_________________________________________________________________<br/>conv1d_8 (Conv1D)            (None, 1192, 48)          14448     <br/>_________________________________________________________________<br/>flatten_2 (Flatten)          (None, 57216)             0         <br/>_________________________________________________________________<br/>dense_9 (Dense)              (None, 71)                4062407   <br/>=================================================================<br/>Total params: 12,815,796<br/>Trainable params: 12,815,796<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="6157" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">使用adam优化器和二进制交叉熵进行训练。</p><pre class="mi mj mk ml gt nj nk nl nm aw nn bi"><span id="3b26" class="no mn jg nk b gy np nq l nr ns">model.compile(optimizer='adam', loss='binary_crossentropy')<br/>model.fit(padded_docs_train, y_train,<br/>                        epochs=10,<br/>                        verbose=<strong class="nk jq">False</strong>,<br/>                        validation_data=(padded_docs_test, y_test),<br/>                        batch_size=16)</span><span id="8b13" class="no mn jg nk b gy nw nq l nr ns">predictions=model.predict([padded_docs_test])<br/><strong class="nk jq">for</strong> val <strong class="nk jq">in</strong> thresholds:<br/>    print("For threshold: ", val)<br/>    pred=predictions.copy()<br/>  <br/>    pred[pred&gt;=val]=1<br/>    pred[pred&lt;val]=0<br/>  <br/>    precision = precision_score(y_test, pred, average='micro')<br/>    recall = recall_score(y_test, pred, average='micro')<br/>    f1 = f1_score(y_test, pred, average='micro')<br/>   <br/>    print("Micro-average quality numbers")<br/>    print("Precision: <strong class="nk jq">{:.4f}</strong>, Recall: <strong class="nk jq">{:.4f}</strong>, F1-measure: <strong class="nk jq">{:.4f}</strong>".format(precision, recall, f1))</span></pre><figure class="mi mj mk ml gt is gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/a129a7da708d6bc50ef9b73327326230.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*eE5pngjWg3hH_IS3zYrITg.jpeg"/></div></figure><p id="fd53" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">对于非常高或非常低的阈值值，度量分数也呈现较低的趋势。</p><h1 id="8b02" class="mm mn jg bd mo mp mq mr ms mt mu mv mw kv mx kw my ky mz kz na lb nb lc nc nd bi translated">结论:</h1><p id="1d33" class="pw-post-body-paragraph ll lm jg ln b lo ne kq lq lr nf kt lt lu ng lw lx ly nh ma mb mc ni me mf mg ij bi translated">在这篇博客中，我们尝试了两种架构，即LSTMs和CNN，然后使其适用于多标签分类问题。我们从数据探索开始，然后使用词汇表的大小定义模型。一旦模型被训练，我们使用不同的阈值，然后基于在测试数据集上给出最佳F1微分数的阈值分数来选择标签。</p><p id="0f45" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><em class="nv">连接阿曼:https://www.linkedin.com/in/aman-s-32494b80</em></p><p id="5152" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">参考文献:</strong></p><p id="c49e" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">完整笔记本参考此处:<a class="ae jd" href="https://github.com/sawarn69/MPST-Movie-Plot-Synopsis/blob/master/LSTMs%20Tag%20from%20Synopsis.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/sawarn 69/MPST-电影-剧情-提要/blob/master/LSTMs % 20 tag % 20 from % 20 Synopsis . ipynb</a></p><p id="20c3" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">www.appliedaicourse.com</p><p id="4034" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><a class="ae jd" href="https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17" rel="noopener" target="_blank">https://towards data science . com/multi-class-text-classification-with-lstm-1590 bee 1 BD 17</a></p><div class="ip iq gp gr ir oj"><a href="https://blog.mimacom.com/text-classification/" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jq gy z fp oo fr fs op fu fw jp bi translated">使用Keras执行多标签文本分类</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">文本分类是应用机器学习的常见任务。无论是问答平台上的问题，还是支持…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">blog.mimacom.com</p></div></div><div class="os l"><div class="pi l ou ov ow os ox ix oj"/></div></div></a></div><p id="7cf9" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><a class="ae jd" href="https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/" rel="noopener ugc nofollow" target="_blank">https://stack abuse . com/python-for-NLP-multi-label-text-classification-with-keras</a></p></div></div>    
</body>
</html>