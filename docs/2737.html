<html>
<head>
<title>These Are the Two Main Types of Adversarial Attacks in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这是神经网络中两种主要的对抗性攻击</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/these-are-the-two-main-types-of-adversarial-attacks-in-neural-networks-91c0954ee747?source=collection_archive---------3-----------------------#2022-05-05">https://pub.towardsai.net/these-are-the-two-main-types-of-adversarial-attacks-in-neural-networks-91c0954ee747?source=collection_archive---------3-----------------------#2022-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f47c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">黑盒和白盒攻击是ML工程师应该了解的两种对抗性攻击。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/12d49ebe14feee6c03ae63d2d9283e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lVfhmbYcEWX28oHb"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://bdtechtalks.com/2020/12/07/vonenet-neurscience-inspired-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://bdtechtalks . com/2020/12/07/von enet-neur science-inspired-deep-learning/</a></figcaption></figure><blockquote class="kz la lb"><p id="fd18" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过125，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="lz ma gp gr mb mc"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">序列</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">与机器学习、人工智能和数据发展保持同步的最佳资源…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">thesequence.substack.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq ks mc"/></div></div></a></div><p id="5181" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">随着人工智能(AI)和深度学习在软件解决方案中变得更加主流，它们将会带动技术领域的其他学科。安全性是需要快速发展以跟上深度学习技术进步的领域之一。虽然我们通常在积极的背景下考虑深度学习，算法试图提高解决方案的智能，但深度学习模型也可以用于策划复杂的安全攻击。更有趣的是，深度学习模型可以用来危及其他智能模型的安全。</p><p id="3b16" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">深度神经网络攻击其他神经网络的想法似乎是空间进化中不可避免的事实。随着软件变得越来越智能，用于攻击和防御该软件的安全技术可能会自然地利用类似的智能水平。深度学习对安全领域提出了我们以前从未见过的挑战，因为我们可以拥有能够快速适应并产生新形式攻击的软件。深度学习领域包括一个被称为敌对网络的分支学科，该分支学科专注于创建可以破坏其他模型功能的神经网络。虽然敌对网络通常被视为提高深度学习模型鲁棒性的博弈论人工制品，但它们也可以被用来制造安全攻击。</p><p id="8d04" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">使用对立的例子来扰乱深度学习分类器的最常见的场景之一。对立的例子是另一个网络设计来诱导错误的深度学习模型的输入。在分类模型的上下文中，您可以将对抗性攻击视为深度学习代理的视错觉😊下图显示了输入数据集中的微小变化如何导致模型将洗衣机错误分类为扬声器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/d548ab823235a4e58491baa7af40878e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/0*3RqMswkyiN8694ZG.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:https://openai.com/blog/adversarial-example-research/</figcaption></figure><p id="5dae" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">如果所有的敌对攻击都像上面的例子一样，它们不会有什么大不了的，但是，想象一下通过使用贴纸或油漆投影停止标志的图像来扰乱自动驾驶汽车的相同技术。深度学习大师Ina Goodfellow在几年前发表的一篇题为<a class="ae ky" href="https://arxiv.org/abs/1602.02697" rel="noopener ugc nofollow" target="_blank">针对机器学习的实用黑盒攻击</a>的研究论文中描述了这种方法。</p><p id="205c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">对抗性攻击在无监督的架构中更有效，如强化学习。与监督学习应用不同，在监督学习应用中，在学习期间处理固定的训练样本数据集，在强化学习(RL)中，这些样本是在整个训练过程中收集的。更简单地说，RL模型训练一个策略，尽管模型目标是相同的，但是训练策略可以显著不同。从对抗性示例的角度来看，我们可以想象，无论它是否能够访问策略网络，攻击技术都会有很大的不同。使用该标准，深度学习研究人员通常将对抗性攻击分为两大类:黑盒与白盒。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/4f4542203cfd325af1e986d392d714ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NQOKgUSVyflYfwOl.png"/></div></div></figure><p id="c41c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated"><a class="ae ky" href="http://rll.berkeley.edu/adversarial/arXiv2017_AdversarialAttacks.pdf" rel="noopener ugc nofollow" target="_blank">在另一篇最近的研究论文</a>中，Ian Goodfellow和他的同事强调了一系列针对RL模型的白盒和黑盒攻击。研究人员对一组知名的RL模型进行了对抗性攻击，如A3C、TRPO和DQN，这些模型学习如何玩不同的游戏，如Atari 2600、Chopper Command、Pong、Seaquest或Space Invaders。</p><h1 id="29f3" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">白盒对抗性攻击</h1><p id="73aa" class="pw-post-body-paragraph lc ld it lf b lg no ju li lj np jx ll mr nq lo lp ms nr ls lt mt ns lw lx ly im bi translated">白盒对抗性攻击描述了攻击者能够访问目标模型的底层培训策略网络的场景。研究发现，即使在训练策略中引入小的扰动，也能极大地影响模型的性能。下面的视频展示了这些结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://www.youtube.com/watch?v=OvQzg4zI2Fs&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=OvQzg4zI2Fs&amp;feature = youtu . be</a></figcaption></figure><h1 id="d23d" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">黑盒对抗性攻击</h1><p id="1f14" class="pw-post-body-paragraph lc ld it lf b lg no ju li lj np jx ll mr nq lo lp ms nr ls lt mt ns lw lx ly im bi translated">黑盒对抗性攻击描述了攻击者无法完全访问策略网络的场景。上面提到的研究将黑盒攻击分为两大类:</p><p id="cd16" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">1)对手可以访问训练环境，并且知道训练算法和超参数。它知道目标策略网络的神经网络架构，但不知道其随机初始化。他们将这种模式称为跨政策的可转移性。</p><p id="35da" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated"><em class="le"> 2)对手还不知道训练算法或超参数。他们把这个模型称为跨算法的可移植性。</em></p><p id="6368" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">不足为奇的是，实验表明，我们发现对手对目标策略了解得越少，对抗性例子的效果就越差。跨算法的可转移性在降低代理性能方面不如跨策略的可转移性有效，后者不如白盒攻击有效。</p></div></div>    
</body>
</html>