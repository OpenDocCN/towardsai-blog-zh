<html>
<head>
<title>Essential Linear Algebra for Data Science and Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学和机器学习的基本线性代数</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/essential-linear-algebra-for-data-science-and-machine-learning-10d47d61000b?source=collection_archive---------0-----------------------#2021-05-25">https://pub.towardsai.net/essential-linear-algebra-for-data-science-and-machine-learning-10d47d61000b?source=collection_archive---------0-----------------------#2021-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8a64" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="5e0d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated"><em class="kr">初学者或成熟的数据科学从业者必须非常熟悉线性代数中的基本概念</em></h2></div><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ks"><img src="../Images/159cbbe2840824ea04bc4fe28bee1305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xBDAb3-3-y8mWeniKeVM3A.png"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated">Benjamin O. Tayo的图片。</figcaption></figure><p id="e5bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">线性代数是数学的一个分支，在数据科学和机器学习中非常有用。线性代数是机器学习中最重要的数学技能。大多数机器学习模型都可以用矩阵形式表示。数据集本身通常表示为矩阵。线性代数用于数据预处理、数据转换和模型评估。以下是您需要熟悉的主题:</p><ul class=""><li id="757a" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> <em class="mn">矢量</em> </strong></li><li id="c1ed" class="me mf it lk b ll mo lo mp lr mq lv mr lz ms md mj mk ml mm bi translated"><strong class="lk jd"> <em class="mn">矩阵</em> </strong></li><li id="ed6e" class="me mf it lk b ll mo lo mp lr mq lv mr lz ms md mj mk ml mm bi translated"><strong class="lk jd"> <em class="mn">矩阵的转置</em> </strong></li><li id="7653" class="me mf it lk b ll mo lo mp lr mq lv mr lz ms md mj mk ml mm bi translated"><strong class="lk jd"> <em class="mn">矩阵的逆</em> </strong></li><li id="0094" class="me mf it lk b ll mo lo mp lr mq lv mr lz ms md mj mk ml mm bi translated"><strong class="lk jd"> <em class="mn">矩阵的行列式</em> </strong></li><li id="f427" class="me mf it lk b ll mo lo mp lr mq lv mr lz ms md mj mk ml mm bi translated"><strong class="lk jd"> <em class="mn">一丝矩阵</em> </strong></li><li id="ebe5" class="me mf it lk b ll mo lo mp lr mq lv mr lz ms md mj mk ml mm bi translated"><strong class="lk jd"> <em class="mn">点积</em> </strong></li><li id="1bd4" class="me mf it lk b ll mo lo mp lr mq lv mr lz ms md mj mk ml mm bi translated"><strong class="lk jd"> <em class="mn">特征值</em> </strong></li><li id="288c" class="me mf it lk b ll mo lo mp lr mq lv mr lz ms md mj mk ml mm bi translated"><strong class="lk jd"> <em class="mn">特征向量</em> </strong></li></ul><p id="f713" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，我们使用科技股数据集来说明线性代数在数据科学和机器学习中的应用，该数据集可以在<a class="ae mt" href="https://github.com/bot13956/datasets/blob/master/tech-stocks-04-2021.csv" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="62cb" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">1.用于数据预处理的线性代数</h1><p id="9928" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">我们从说明线性代数如何用于数据预处理开始。</p><p id="9443" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 1.1导入必要的线性代数库</strong></p><pre class="kt ku kv kw gt nr ns nt nu aw nv bi"><span id="0842" class="nw mv it ns b gy nx ny l nz oa">import numpy as np </span><span id="2506" class="nw mv it ns b gy ob ny l nz oa">import pandas as pd </span><span id="3614" class="nw mv it ns b gy ob ny l nz oa">import pylab </span><span id="0834" class="nw mv it ns b gy ob ny l nz oa">import matplotlib.pyplot as plt </span><span id="6c93" class="nw mv it ns b gy ob ny l nz oa">import seaborn as sns</span></pre><p id="940c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 1.2读取数据集并显示特征</strong></p><pre class="kt ku kv kw gt nr ns nt nu aw nv bi"><span id="3b95" class="nw mv it ns b gy nx ny l nz oa">data = pd.read_csv("tech-stocks-04-2021.csv") </span><span id="5795" class="nw mv it ns b gy ob ny l nz oa">data.head()</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/89520b8fa8ebe886fb3d4571418f1f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w4tiKxGPsNiSxjF3.jpg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated"><strong class="bd mw"> <em class="kr">表1 </em> </strong> <em class="kr">。2021年4月前16天所选股票价格</em></figcaption></figure><pre class="kt ku kv kw gt nr ns nt nu aw nv bi"><span id="b49a" class="nw mv it ns b gy nx ny l nz oa">print(data.shape) </span><span id="9c48" class="nw mv it ns b gy ob ny l nz oa">output = (11,5)</span></pre><p id="5b21" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="mn"> data.shape </em> </strong>函数使我们能够知道数据集的大小。在这种情况下，数据集有5个要素(日期、AAPL、TSLA、GOOGL和AMZN)，每个要素有11个观测值。<em class="mn">日期</em>是指2021年4月的交易日(截止到4月16日)。AAPL、TSLA、谷歌和AMZN分别是苹果、特斯拉、谷歌和亚马逊的收盘价。</p><p id="1c0e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 1.3数据可视化</strong></p><p id="207e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了执行数据可视化，我们需要为要可视化的特征定义<strong class="lk jd"> <em class="mn">列矩阵</em> </strong>:</p><pre class="kt ku kv kw gt nr ns nt nu aw nv bi"><span id="f80b" class="nw mv it ns b gy nx ny l nz oa">x = data['date'] </span><span id="f1b8" class="nw mv it ns b gy ob ny l nz oa">y = data['TSLA'] </span><span id="968e" class="nw mv it ns b gy ob ny l nz oa">plt.plot(x,y) </span><span id="d90e" class="nw mv it ns b gy ob ny l nz oa">plt.xticks(np.array([0,4,9]), ['Apr 1','Apr 8','Apr 15']) </span><span id="0edf" class="nw mv it ns b gy ob ny l nz oa">plt.title('Tesla stock price (in dollars) for April 2021',size=14) </span><span id="cf13" class="nw mv it ns b gy ob ny l nz oa">plt.show()</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9476b70a7d5ff25b11e8021290323360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/0*WGGJS9EdK0NoUCko.jpg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated"><strong class="bd mw"> <em class="kr">图一</em> </strong> <em class="kr">。2021年4月前16天特斯拉股价。</em></figcaption></figure><h1 id="69f6" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">2.协方差矩阵</h1><p id="afc2" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated"><strong class="lk jd"> <em class="mn">协方差矩阵</em> </strong>是数据科学和机器学习中最重要的矩阵之一。它提供了关于特征之间的共同运动(相关性)的信息。假设我们有一个具有<em class="mn"> 4个</em>特征和<em class="mn"> n个</em>观察值的特征矩阵，如<strong class="lk jd">表2 </strong>所示:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a8e0ed36119047647a2b2255ec2c10ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/0*r7Jc-P4X_YxA5dM-.jpg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated"><strong class="bd mw"> <em class="kr">表二</em> </strong> <em class="kr">。具有4个变量和n个观察值的特征矩阵。</em></figcaption></figure><p id="9faf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了可视化特征之间的相关性，我们可以生成散点图:</p><pre class="kt ku kv kw gt nr ns nt nu aw nv bi"><span id="edc5" class="nw mv it ns b gy nx ny l nz oa">cols=data.columns[1:5] </span><span id="f379" class="nw mv it ns b gy ob ny l nz oa">print(cols) </span><span id="af9e" class="nw mv it ns b gy ob ny l nz oa">output = Index(['AAPL', 'TSLA', 'GOOGL', 'AMZN'], dtype='object') </span><span id="65dc" class="nw mv it ns b gy ob ny l nz oa">sns.pairplot(data[cols], height=3.0)</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/0c0ee68266093949949cd56a1418dbd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H4y_XyWyNDJ8fsZd.jpg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated"><strong class="bd mw"> <em class="kr">图2 </em> </strong> <em class="kr">。选定科技股的散点图。</em></figcaption></figure><p id="ceda" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要量化要素之间的相关程度(多重共线性)，我们可以使用以下公式计算协方差矩阵:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi of"><img src="../Images/716c04cb4cfeb3362666bcb28dea9ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*99JMVWh4ZQan-Rfe.jpg"/></div></figure><p id="2a85" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中和分别是特征的均值和标准差。该等式表明，当特征被标准化时，协方差矩阵简单地是特征之间的<strong class="lk jd"> <em class="mn">点积</em> </strong>。</p><p id="d23a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在矩阵形式中，协方差矩阵可以表示为4 x 4实对称矩阵:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8812bccaceccde821135cf27d4217c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*st7nIkWcgHMWpUXC.jpg"/></div></figure><p id="8ea6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该矩阵可以通过执行<strong class="lk jd"> <em class="mn">酉变换</em> </strong>来对角化，也称为主成分分析(PCA)变换，以获得以下内容:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi of"><img src="../Images/369b57a66a89ca43f2a55eb7879e4263.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*NB1PfuaAurKIY_-N.jpg"/></div></figure><p id="d4db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于矩阵 的<strong class="lk jd"> <em class="mn">迹在酉变换下保持不变，我们观察到对角矩阵的特征值之和等于包含在特征X 1、X 2、X 3和X 4中的总方差。</em></strong></p><p id="4506" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2.1计算科技股的协方差矩阵</strong></p><pre class="kt ku kv kw gt nr ns nt nu aw nv bi"><span id="042d" class="nw mv it ns b gy nx ny l nz oa">from sklearn.preprocessing import StandardScaler </span><span id="7aed" class="nw mv it ns b gy ob ny l nz oa">stdsc = StandardScaler() </span><span id="9ff0" class="nw mv it ns b gy ob ny l nz oa">X_std = stdsc.fit_transform(data[cols].iloc[:,range(0,4)].values) </span><span id="3e5a" class="nw mv it ns b gy ob ny l nz oa">cov_mat = np.cov(X_std.T, bias= True)</span></pre><p id="1b2b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，这里使用了标准化矩阵的<strong class="lk jd"> <em class="mn">转置</em> </strong>。</p><p id="f5c8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2.2协方差矩阵的可视化</strong></p><pre class="kt ku kv kw gt nr ns nt nu aw nv bi"><span id="367e" class="nw mv it ns b gy nx ny l nz oa">plt.figure(figsize=(8,8)) </span><span id="e099" class="nw mv it ns b gy ob ny l nz oa">sns.set(font_scale=1.2) </span><span id="55bc" class="nw mv it ns b gy ob ny l nz oa">hm = sns.heatmap(cov_mat, <br/>                 cbar=True, <br/>                 annot=True, <br/>                 square=True, <br/>                 fmt='.2f', <br/>                 annot_kws={'size': 12}, <br/>                 yticklabels=cols, <br/>                 xticklabels=cols) </span><span id="8019" class="nw mv it ns b gy ob ny l nz oa">plt.title('Covariance matrix showing correlation coefficients') </span><span id="16ec" class="nw mv it ns b gy ob ny l nz oa">plt.tight_layout() </span><span id="87dc" class="nw mv it ns b gy ob ny l nz oa">plt.show()</span></pre><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ab4800d483612e1aa315c21cb246a780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UTLZSudB770aov1v.jpg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated"><strong class="bd mw"> <em class="kr">图3 </em> </strong> <em class="kr">。选定科技股的协方差矩阵图。</em></figcaption></figure><p id="a021" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们从图3中观察到，AAPL与GOOGL和AMZN的相关性很强，而与TSLA的相关性很弱。TSLA与AAPL、古格尔和阿姆津的相关性一般较弱，而AAPL、古格尔和阿姆津之间的相关性较强。</p><p id="0126" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2.3计算协方差矩阵的特征值</strong></p><pre class="kt ku kv kw gt nr ns nt nu aw nv bi"><span id="c0b9" class="nw mv it ns b gy nx ny l nz oa">np.linalg.eigvals(cov_mat) </span><span id="0002" class="nw mv it ns b gy ob ny l nz oa">output = array([3.41582227, 0.4527295 , 0.02045092, 0.11099732]) </span><span id="6837" class="nw mv it ns b gy ob ny l nz oa">np.sum(np.linalg.eigvals(cov_mat)) </span><span id="5c1c" class="nw mv it ns b gy ob ny l nz oa">output = 4.000000000000006 </span><span id="bee5" class="nw mv it ns b gy ob ny l nz oa">np.trace(cov_mat) output = 4.000000000000001</span></pre><p id="c15a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们观察到协方差矩阵的迹等于特征值之和，正如所预期的那样。</p><p id="fc2a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2.4计算累积方差</strong></p><p id="8810" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于矩阵的迹在酉变换下保持不变，我们观察到对角矩阵的特征值之和等于包含在特征X 1、X 2、X 3和X 4中的总方差。因此，我们可以定义以下数量:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi of"><img src="../Images/3c983ccccc49cb02df887518dbcd5cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*hIQC9RhldrawWYZv.jpg"/></div></figure><p id="ec44" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，当<em class="mn"> p </em> = 4时，累积方差如预期的那样等于1。</p><pre class="kt ku kv kw gt nr ns nt nu aw nv bi"><span id="ecfc" class="nw mv it ns b gy nx ny l nz oa">eigen = np.linalg.eigvals(cov_mat) </span><span id="1892" class="nw mv it ns b gy ob ny l nz oa">cum_var = eigen/np.sum(eigen) </span><span id="f831" class="nw mv it ns b gy ob ny l nz oa">print(cum_var) </span><span id="850e" class="nw mv it ns b gy ob ny l nz oa">output = [0.85395557 0.11318237 0.00511273 0.02774933] </span><span id="d806" class="nw mv it ns b gy ob ny l nz oa">print(np.sum(cum_var)) output = 1.0</span></pre><p id="8728" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们从累积方差(<strong class="lk jd"> <em class="mn"> cum_var </em> </strong>)中观察到，方差的85%包含在第一特征值中，11%包含在第二特征值中。这意味着当实施PCA时，只有前两个主成分可以被使用，因为总方差的97%由这两个成分贡献。当实现PCA时，这可以基本上将特征空间的维数从4减少到2。</p><h1 id="077a" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">3.线性回归矩阵</h1><p id="e8c8" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">假设我们有一个包含4个预测要素和<em class="mn"> n </em>个观测值的数据集，如下所示。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/2986eb3742d960fdedb5aa72e4fcfce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jICIR4SyAL9XvK0T.jpg"/></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated"><strong class="bd mw"> <em class="kr">表3 </em> </strong> <em class="kr">。具有4个变量和n个观察值的特征矩阵。第5列是目标变量(y)。</em></figcaption></figure><p id="3e0b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们希望构建一个多元回归模型来预测<em class="mn"> y </em>值(第5列)。因此，我们的模型可以表示为</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi og"><img src="../Images/0646618958b6aa336136240a6c50a6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/0*DEqCGmWUyebCsGHG.jpg"/></div></figure><p id="8bd1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在矩阵形式中，该方程可以写成</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/062bce3da6ac2b399e1fe9512b8acc56.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/0*ySRZLdhufSIyvX5c.jpg"/></div></figure><p id="571d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<strong class="lk jd"> X </strong>是(n×4)特征矩阵，<strong class="lk jd"> w </strong>是表示待确定回归系数的(4×1)矩阵，<strong class="lk jd"> y </strong>是包含目标变量y的n个观测值的(n×1)矩阵</p><p id="8118" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意<strong class="lk jd"> X </strong>是一个矩形矩阵，所以我们不能通过取<strong class="lk jd"> X </strong>的逆来解上面的方程。</p><p id="4dbd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了将<strong class="lk jd"> X </strong>转换成方阵，我们将等式的左侧和右侧乘以<strong class="lk jd"> X </strong>，即</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d912390f6f2194f1721b17685f7ae96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/0*zA-tIFMgyZ3Fd-rh.jpg"/></div></figure><p id="21b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个等式也可以表示为</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/6afa4994067153abbd286e44b2df4d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/0*yl8NDTlPonCFhMJE.jpg"/></div></figure><p id="5b18" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在哪里</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/96e9918cbccd9707534b38054a3b0152.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/0*jHPxQNMKPLiq-vDP.jpg"/></div></figure><p id="c0b2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">是(4×4)回归矩阵。显然，我们观察到<strong class="lk jd"> R </strong>是实对称矩阵。请注意，在线性代数中，两个矩阵乘积的转置遵循以下关系</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/583cf8aa335d76d70bb3ada0d0eb5fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/0*iGG4MJ9IewyURBpO.jpg"/></div></figure><p id="eb5a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">既然我们已经简化了我们的回归问题，并用(4×4)实对称可逆回归矩阵<strong class="lk jd"> R </strong>来表示它，那么就可以直接表明回归方程的精确解是</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi om"><img src="../Images/27b305afb74715654f571842bc302765.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/0*PQuBbPl-C1qJsMPT.jpg"/></div></figure><p id="b854" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">预测连续和离散变量的回归分析示例如下:</p><p id="c13a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae mt" rel="noopener ugc nofollow" target="_blank" href="/linear-regression-basics-for-absolute-beginners-68ed9ff980ae">绝对初学者的线性回归基础</a></p><p id="7a2c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae mt" href="https://github.com/bot13956/perceptron_classifier" rel="noopener ugc nofollow" target="_blank">使用最小二乘法构建感知机分类器</a></p><h1 id="b7c2" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">4.线性判别分析矩阵</h1><p id="c752" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">数据科学中实对称矩阵的另一个例子是线性判别分析(LDA)矩阵。该矩阵可以用以下形式表示:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi on"><img src="../Images/cb36776ba268a83e988aa6e798ef3da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/0*itsxe86TwcPQeyml.jpg"/></div></figure><p id="69bd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中是特征内散布矩阵，是特征间散布矩阵。由于两个矩阵都是实对称的，因此<strong class="lk jd"> L </strong>也是实对称的。<strong class="lk jd"> L </strong>的对角化产生了一个特征子空间，优化了类别可分性，降低了维数。因此，LDA是一种监督算法，而PCA不是。</p><p id="9798" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有关LDA实施的更多详细信息，请参见以下参考资料:</p><p id="b5b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae mt" href="https://medium.com/towards-artificial-intelligence/machine-learning-dimensionality-reduction-via-linear-discriminant-analysis-cc96b49d2757" rel="noopener">机器学习:通过线性判别分析降维</a></p><p id="8520" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae mt" href="https://github.com/bot13956/linear-discriminant-analysis-iris-dataset" rel="noopener ugc nofollow" target="_blank">使用Iris数据集实现LDA的GitHub知识库</a><a class="ae mt" href="https://github.com/rasbt/python-machine-learning-book-3rd-edition" rel="noopener ugc nofollow" target="_blank">Sebastian rasch ka的Python机器学习，第3版(第5章)</a></p><h1 id="75c6" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">摘要</h1><p id="b72c" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">总之，我们已经讨论了线性代数在数据科学和机器学习中的几个应用。使用科技股数据集，我们举例说明了矩阵的大小、列矩阵、方阵、协方差矩阵、矩阵的转置、特征值、点积等重要概念。线性代数是数据科学和机器学习的基本工具。因此，对数据科学感兴趣的初学者必须熟悉线性代数中的基本概念。</p><p id="70db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn">原载于</em><a class="ae mt" href="https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html" rel="noopener ugc nofollow" target="_blank"><em class="mn">https://www.kdnuggets.com</em></a><em class="mn">。</em></p></div></div>    
</body>
</html>