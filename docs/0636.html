<html>
<head>
<title>Do You Understand Gradient Descent and Backpropagation? Most Don’t.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你理解梯度下降和反向传播吗？大多数人不知道。</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/do-you-understand-gradient-descent-and-backpropagation-most-dont-929d65f57a6c?source=collection_archive---------2-----------------------#2020-06-29">https://pub.towardsai.net/do-you-understand-gradient-descent-and-backpropagation-most-dont-929d65f57a6c?source=collection_archive---------2-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8eab" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="6f31" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">机器学习中一种常用优化算法背后的简单数学直觉。</h2></div><p id="3c6e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">二元分类在机器学习中非常常见。这是进入梯度下降和反向传播的黑暗世界的一个很好的例子。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/5242ee6fdcec62387294075462f7c775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4nJNY5ngpBGUnpd07L51sg.jpeg"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">米哈伊尔·瓦西里耶夫在<a class="ae md" href="https://unsplash.com/photos/UqFyM-LOgQc" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="5a1a" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">逻辑回归中的梯度下降</h1><p id="64e3" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">我们记得，在用于二进制分类的神经网络中，输入经过仿射变换，并且结果被馈送到sigmoid激活中。因此，输出是0或1之间的值，即预测正类或负类的可能性。</p><p id="0066" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面我们看到这个分类输出的表达式<em class="nb"> P(Y=1) </em>。注意，预测器<em class="nb"> X </em>上的仿射变换取决于两个参数<em class="nb"> β0 </em>和<em class="nb"> β1 </em>。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/6ffe70694c4df251c230b5fd2888e674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/0*S5C-XgnzLJL0gOp_.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">s形逻辑函数</figcaption></figure><p id="c571" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在训练期间，对于任何输入<em class="nb"> Xi </em>，神经网络可以计算似然<em class="nb"> Pi </em>并将其与真实值<em class="nb"> Yi </em>进行比较。通常使用<strong class="kt jd">二进制交叉熵</strong>计算误差。它的作用与用于回归的均方误差相同。</p><p id="0eb1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">二元交叉熵测量预测值<em class="nb"> Pi </em>与真实值<em class="nb"> Yi </em>(为0或1)的距离。下面给出用于计算二元交叉熵<em class="nb"> Li </em>的公式。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nd"><img src="../Images/72fac67d8ba50f04e321fe1e7125486e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fE-pv_Gt3FRavsTR.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">二元交叉熵损失函数。</figcaption></figure><p id="8fb8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如你所见，损失取决于权重<em class="nb"> β0 </em>和<em class="nb"> β1。</em>在梯度下降优化器选择了随机权重后，它挑选一个随机的<em class="nb"> Xi </em>，然后向前传递以计算损失。</p><p id="7c87" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们使用的是基本梯度下降，优化器会对所有输入数据点重复这种计算，如果我们使用的是随机版本，优化器只会对少量数据重复这种计算。总损失是将所有单项损失相加得出的。</p><p id="e65d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在优化器计算出总损失后，它将计算权重的导数。基于导数的符号，它将正向或负向更新权重。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/a6c2d74db7a3debc72cec5a21136741c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/0*yNC4JL4Fe_PGW-Ge.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">二元交叉熵函数的偏导数。</figcaption></figure><p id="1c6a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们知道，我们想要向导数的相反方向前进，我们知道，我们想要向导数成比例地前进一步。<strong class="kt jd">学习率</strong> λ是控制每个权重<em class="nb"> W </em> ( <em class="nb"> β0 </em>或β1 )的比例。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nf"><img src="../Images/56acd898b29d5f299087df96ca3fbacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/0*36wpM3nmMboeCn-A.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">权重在梯度下降期间更新。</figcaption></figure><h1 id="7a2f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">逻辑回归中的反向传播</h1><p id="feea" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">如上图所示，更新权重需要计算每个权重损失的偏导数。</p><p id="654d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们如何计算导数？嗯，我们可以用高中数学来做这个。</p><p id="7837" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在下图中，我们尝试计算<em class="nb"> β0 </em>和<em class="nb"> β1 </em>的交叉熵损失函数的导数。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ng"><img src="../Images/25ce801f798d00470b8cba1d36d9f563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GLUR69GvIsX41Baw.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">如何手工计算偏导数？</figcaption></figure><p id="b224" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于只有一个神经元的网络来说，这是一个很好的解决方案。但是想象一下我们在深度学习中通常会遇到的有数百个神经元的网络。我打赌你不想计算结果导数。</p><p id="e6d7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">即使你成功地做到了这一点，你也必须在每次网络结构改变时更新你的公式，哪怕只是一点点。这就是<strong class="kt jd">反向传播</strong>发挥作用的地方。</p><p id="05f8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">反向传播算法最初是在20世纪70年代提出的，但直到1986年Geoffrey Hinton的一篇论文才充分认识到它的重要性。</p><p id="f519" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">反向传播使用<strong class="kt jd">链规则</strong>，这是编写嵌套函数导数的方便的助记符。</p><p id="a14b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">例如，如果我们有一个网络，其中一个神经元馈入第二个神经元，最后馈入第三个神经元以获得输出。总损失函数<em class="nb"> f </em>是前两个神经元的损失函数<em class="nb"> g </em>的函数，类似地，<em class="nb"> g </em>是第一个神经元的损失函数<em class="nb"> h </em>的函数。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nh"><img src="../Images/b694d4870592aaa0af5e08a9a95fc8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/0*U9O_iwcGSagNJhh1.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">连锁法则。</figcaption></figure><p id="fd36" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当我们从输入开始计算每个神经元的输出直到最后一个神经元时，我们也已经计算了导数的微小分量。</p><p id="91c4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在上面的例子中，我们可以在向前通过第一个神经元时计算出<em class="nb"> dh/dx </em>。</p><p id="b0cb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，我们可以通过第二个神经元向前计算<em class="nb"> dg/dh </em>。</p><p id="1478" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后，我们开始计算<em class="nb"> df/dg </em>向后通过神经元，并通过重用所有已经计算的元素。</p><p id="665f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这就是反向传播这个名字的由来。这种技术有几种实现和风格。为了简单起见，我们在这里保持简单。</p><p id="be4f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了说明链式法则和反向传播是如何工作的，让我们回到具有sigmoid激活的1-神经元网络的损失函数。</p><p id="cf62" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">损失函数被定义为二进制交叉熵，它可以被分成两部分A和B，如下所示。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d94d340b511ba5135840385b17783f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*sHnVx1s1-9BOKIGr.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">二元交叉熵损失函数。</figcaption></figure><p id="bbe6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们仔细看看损失函数的A部分。它可以分成几个块，在下图中用红框突出显示。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/96e4610e47eb4510a8e0289c832fa0d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/0*qT8JJclLIc95vdgH.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">二元交叉熵损失函数的第一部分。</figcaption></figure><p id="dad8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">反向传播要求计算该函数在任何给定数据点<em class="nb"> X </em>处对任何给定权重<em class="nb"> W </em>的导数。</p><p id="67dd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是通过计算每个块的导数并用链式法则把它们放在一起完成的。</p><p id="988e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面我们来看看这对于<em class="nb"> X=3 </em>和<em class="nb"> W=3 </em>是如何工作的。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nk"><img src="../Images/0a586158b9d816da3bb9d980fe1b266c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dj_aEZVa7z6MjPF6.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">反向传播期间进行的计算。</figcaption></figure><p id="d0a4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们所需要的是能够计算小块(上面称为变量)的导数。</p><p id="7f2e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种块是已知的，因为激活函数通常是已知的。它们可以是s形的、线形的、ReLu形的等等。</p><p id="c204" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些是导数已知的可微函数。你可以在这里找到最新激活功能<a class="ae md" href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener" target="_blank">的完整列表</a>。</p><p id="fbfa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，上述计算可以在运行时使用计算图来构建。</p><p id="5789" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Keras等高级API可以查看您的网络架构和每个神经元使用的激活函数，以在模型编译期间构建计算图。</p><p id="d18d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该图在训练期间用于执行正向传递和反向传播。</p><p id="f4ac" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">交叉熵损失函数的计算图的示例如下所示。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nl"><img src="../Images/2b4738f5ed963b48966bf7ba5ac89f3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-4AEdDlLu1ZD58wm.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">Keras生成的计算图示例。</figcaption></figure><h1 id="513f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结论</h1><p id="aee9" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">理解梯度下降和反向传播是如何工作的，是理解深度学习为什么完美工作的重要一步。在本文中，我展示了二元分类环境中的学习过程。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nm"><img src="../Images/204acb503b7122ed9b969eed827c0ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fW6mbhIWCVLvkIBfihn-1w.jpeg"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">米哈伊尔·瓦西里耶夫在<a class="ae md" href="https://unsplash.com/photos/AOCrhDG6O08" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>的照片</figcaption></figure><p id="7cdf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">感谢阅读。</p></div></div>    
</body>
</html>