<html>
<head>
<title>How To Classify Handwritten Digits Using A Multilayer Perceptron Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用多层感知器分类器对手写数字进行分类</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-classify-handwritten-digits-using-a-multilayer-perceptron-classifier-928986f1f4f0?source=collection_archive---------3-----------------------#2021-12-07">https://pub.towardsai.net/how-to-classify-handwritten-digits-using-a-multilayer-perceptron-classifier-928986f1f4f0?source=collection_archive---------3-----------------------#2021-12-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b234" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="7d06" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">什么是多层感知器？MLP的利与弊是什么？我们能用MLP分类器准确地分类手写数字吗？学过的重量是什么样子的？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/434e91efe9ee2b42c92a8fa90c887835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Xtu3NtR5dCfYQU3-4qTxA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图1:多层感知器网络(<a class="ae lh" href="https://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">来源</a>)。</figcaption></figure><h1 id="9511" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">1.简短介绍</h1><h2 id="7aac" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">1.1什么是多层感知器(MLP)？</h2><p id="6b61" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">一个<strong class="mq jd"> MLP </strong>是一个<strong class="mq jd">监督</strong>机器学习(ML)算法，属于前馈人工神经网络类[1]。该算法本质上是对数据进行训练，以便学习一个函数。给定一组特征和目标变量(例如标签)，它学习用于分类或回归的非线性函数。在本文中，我们将只关注分类的情况。</p><h2 id="aa25" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">1.2 MLP和逻辑回归之间有什么相似之处吗？</h2><p id="fe7d" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">有！逻辑回归只有两层，即<strong class="mq jd">输入</strong>和<strong class="mq jd">输出</strong>，然而，在MLP模型的情况下，唯一的区别是我们可以有额外的中间<strong class="mq jd">非线性</strong>层。这些被称为<strong class="mq jd">隐藏层</strong>。除了输入节点(属于输入层的节点)，每个节点都是一个使用<strong class="mq jd">非线性</strong> <strong class="mq jd">激活</strong> <strong class="mq jd">函数</strong>的神经元【1】。由于这种非线性性质，MLP可以学习复杂的非线性函数，从而区分不可线性分离的数据！请参见下面的图2 了解带有一个隐藏层的MLP分类器的可视化表示。</p><h2 id="126d" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">1.3如何训练MLP？</h2><p id="c7ea" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">MLP使用<strong class="mq jd">反向传播</strong>进行训练【1】。你可以看看这个网站<a class="ae lh" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">这里</a>的正式数学公式。</p><h2 id="e1b2" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">1.4 Main的主要优势和劣势</h2><p id="ae86" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated"><strong class="mq jd"> <em class="nh">优点</em> </strong></p><ul class=""><li id="9c2a" class="ni nj it mq b mr nk mu nl mf nm mi nn ml no ng np nq nr ns bi translated">可以学习<strong class="mq jd">非线性</strong>函数<strong class="mq jd">从而</strong>分离不可线性分离的数据【2】。</li></ul><p id="3484" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated"><strong class="mq jd"> <em class="nh">缺点</em> </strong></p><ul class=""><li id="4da4" class="ni nj it mq b mr nk mu nl mf nm mi nn ml no ng np nq nr ns bi translated">隐藏层的损失函数导致一个<strong class="mq jd">非</strong> - <strong class="mq jd">凸</strong>优化问题，因此存在局部最小值。</li><li id="8fbb" class="ni nj it mq b mr nw mu nx mf ny mi nz ml oa ng np nq nr ns bi translated">由于上述问题，不同的权重<strong class="mq jd">初始化</strong>可能会导致不同的输出/权重/结果。</li><li id="8459" class="ni nj it mq b mr nw mu nx mf ny mi nz ml oa ng np nq nr ns bi translated">MLP有一些<strong class="mq jd">超参数</strong>，例如隐藏神经元的数量，需要调整的层数(时间&amp;功耗)[2]。</li><li id="1993" class="ni nj it mq b mr nw mu nx mf ny mi nz ml oa ng np nq nr ns bi translated">MLP可以是<strong class="mq jd">敏感</strong>的特征<strong class="mq jd">缩放</strong>【2】。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/f9baf82c62c6c62544494f443a616593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTEA9vG3S6mOBs5EDk0btg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图2:带有一个隐藏层和一个标量输出的MLP。图片改编自scikit-learn python <a class="ae lh" href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html#neural-networks-supervised" rel="noopener ugc nofollow" target="_blank">文档</a>。</figcaption></figure></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="3e2c" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:<a class="ae lh" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="f1d9" class="li lj it bd lk ll oj ln lo lp ok lr ls ki ol kj lu kl om km lw ko on kp ly lz bi translated">2.使用scikit-learn的Python实践示例</h1><h2 id="9e0c" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">2.1数据集</h2><p id="83f1" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">对于这个动手操作的例子，我们将使用MNIST数据集。MNIST数据库是一个著名的手写数字数据库，用于训练几个ML模型[5]。有10个不同数字的手写图像，因此<strong class="mq jd">类别数</strong>为<strong class="mq jd"> </strong> <code class="fe oo op oq or b">10</code>(参见<strong class="mq jd">图3 </strong>)。</p><p id="d5fd" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated"><strong class="mq jd">注</strong>:由于我们处理的是<strong class="mq jd">图像</strong>，这些用<strong class="mq jd">2D</strong>T27】数组表示，数据的初始维数是每幅图像的<code class="fe oo op oq or b"><strong class="mq jd">28</strong> by <strong class="mq jd">28</strong></code>(<code class="fe oo op oq or b">28x28 pixels</code>)。然后，2D图像被<a class="ae lh" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.flatten.html" rel="noopener ugc nofollow" target="_blank"> <strong class="mq jd">展平</strong> </a>，并因此在最后由矢量表示。<strong class="mq jd">每个</strong> <strong class="mq jd"> 2D </strong> <strong class="mq jd">图像</strong>被转换成一个尺寸为<code class="fe oo op oq or b">[1, 28x28] = <strong class="mq jd">[1, 784]</strong></code>的1D <strong class="mq jd">向量</strong>。最后，我们的数据集有<em class="nh"> </em> <code class="fe oo op oq or b"><strong class="mq jd"><em class="nh">784</em></strong></code> <strong class="mq jd"> <em class="nh">特征/变量/列。</em>T46】</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/e7620493a3de60425cfd89c7d99be1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*Ft2rLuO82eItlvJn5HOi9A.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图3:来自数据集的一些样本(<a class="ae lh" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">来源</a>)。</figcaption></figure><h2 id="b488" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">2.2数据导入和准备</h2><pre class="ks kt ku kv gt ot or ou ov aw ow bi"><span id="5f7f" class="ma lj it or b gy ox oy l oz pa">import matplotlib.pyplot as plt<br/>from sklearn.datasets import fetch_openml<br/>from sklearn.neural_network import MLPClassifier</span><span id="80ce" class="ma lj it or b gy pb oy l oz pa"># Load data<br/>X, y = fetch_openml("mnist_784", version=1, return_X_y=True)</span><span id="0aa4" class="ma lj it or b gy pb oy l oz pa"># Normalize intensity of images to make it in the range [0,1] since 255 is the max (white).<br/>X = X / 255.0</span></pre><p id="630e" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">记住<strong class="mq jd">每个</strong> <strong class="mq jd"> 2D </strong> <strong class="mq jd">图像</strong>现在被转换成一个<strong class="mq jd"> 1D </strong> <strong class="mq jd">向量</strong>和维度<code class="fe oo op oq or b">[1, 28x28] = <strong class="mq jd">[1, 784]</strong></code> <strong class="mq jd">。现在我们来验证一下。</strong></p><pre class="ks kt ku kv gt ot or ou ov aw ow bi"><span id="47c4" class="ma lj it or b gy ox oy l oz pa">print(X.shape)</span></pre><p id="f444" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">这返回:<code class="fe oo op oq or b"><strong class="mq jd">(70000, 784)</strong></code> <strong class="mq jd">。</strong>我们有<strong class="mq jd"> 70k的展平图像</strong>(样本)，每个包含784个像素(28*28=784)(变量/特征)。因此，<strong class="mq jd">输入</strong> <strong class="mq jd">层</strong> <strong class="mq jd">权重</strong> <strong class="mq jd">矩阵</strong>将具有形状<code class="fe oo op oq or b">784 x #neurons_in_1st_hidden_layer.</code><strong class="mq jd">输出层权重矩阵将具有形状</strong> <code class="fe oo op oq or b">#neurons_in_3rd_hidden_layer x #number_of_classes.</code></p><h2 id="5602" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">2.3模型培训</h2><p id="4149" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">现在让我们建立模型，训练它并执行分类。我们将使用<code class="fe oo op oq or b">3</code>隐藏层，每个层分别有<code class="fe oo op oq or b">50,20 and 10</code>个神经元。此外，我们将设置最大迭代次数<code class="fe oo op oq or b">100</code>和学习速率<code class="fe oo op oq or b">0.1</code>。这些就是我在介绍中提到的<strong class="mq jd">超参数</strong>。我们不会在这里对它们进行微调。</p><pre class="ks kt ku kv gt ot or ou ov aw ow bi"><span id="7449" class="ma lj it or b gy ox oy l oz pa"># Split the data into train/test sets<br/>X_train, X_test = X[:60000], X[60000:]<br/>y_train, y_test = y[:60000], y[60000:]</span><span id="d189" class="ma lj it or b gy pb oy l oz pa">classifier = <a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" rel="noopener ugc nofollow" target="_blank">MLPClassifier</a>(<br/>    hidden_layer_sizes=(50,20,10),<br/>    max_iter=100,<br/>    alpha=1e-4,<br/>    solver="sgd",<br/>    verbose=10,<br/>    random_state=1,<br/>    learning_rate_init=0.1,<br/>)</span><span id="d603" class="ma lj it or b gy pb oy l oz pa"># fit the model on the training data<br/>classifier.fit(X_train, y_train)</span></pre><h2 id="3a0b" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">2.4模型评估</h2><p id="bfc8" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">现在，让我们来评估这个模型。我们将估计训练和测试数据和标签的平均准确性。</p><pre class="ks kt ku kv gt ot or ou ov aw ow bi"><span id="e9ff" class="ma lj it or b gy ox oy l oz pa">print("Training set score: %f" % classifier.score(X_train, y_train))<br/>print("Test set score: %f" % classifier.score(X_test, y_test))</span></pre><blockquote class="pc pd pe"><p id="a75f" class="mo mp nh mq b mr nk kd mt mu nl kg mw pf nt my mz pg nu nb nc ph nv ne nf ng im bi translated">训练集得分:<code class="fe oo op oq or b"><em class="it">0.998633</em></code> <br/>测试集得分:<code class="fe oo op oq or b"><em class="it">0.970300</em></code></p></blockquote><p id="b7fe" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">伟大的成果！</p><h2 id="229f" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">2.5可视化成本函数演变</h2><p id="d025" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">训练中损失减少的速度有多快？我们来编一个好看的剧情吧！</p><pre class="ks kt ku kv gt ot or ou ov aw ow bi"><span id="c113" class="ma lj it or b gy ox oy l oz pa">fig, axes = plt.subplots(1, 1)</span><span id="1ef2" class="ma lj it or b gy pb oy l oz pa">axes.plot(classifier.loss_curve_, 'o-')<br/>axes.set_xlabel("number of iteration")<br/>axes.set_ylabel("loss")<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/819fb45dbd18d154687ef409a8fff794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CIXxI3p3NWX_IzbiBfBdoQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图4:训练迭代中损失的演变。图由作者制作。</figcaption></figure><p id="4640" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">在这里，我们看到在训练期间损耗下降得非常快，并且在<code class="fe oo op oq or b">40th</code>迭代之后饱和(记得我们将最大100次迭代定义为<strong class="mq jd">超参数</strong>)。</p><h2 id="eb42" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">2.6可视化学习到的重量</h2><p id="92bf" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">这里，我们首先需要了解权重(每层的学习模型参数)是如何存储的。</p><p id="95c7" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">根据<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html" rel="noopener ugc nofollow" target="_blank">文档</a>，属性<code class="fe oo op oq or b">classifier.coefs_</code>是权重数组的形状<code class="fe oo op oq or b">(n_layers-1, )</code>的列表，其中索引I处的权重矩阵表示层<code class="fe oo op oq or b">i</code>和层<code class="fe oo op oq or b">i+1</code>之间的权重。在这个例子中，我们定义了<strong class="mq jd"> 3个隐藏层</strong>，我们还有<strong class="mq jd">输入层</strong>和<strong class="mq jd">输出层。</strong>因此，<strong class="mq jd"> </strong>我们期望层间权重有4个权重数组(<strong class="mq jd">图5 </strong>中的<code class="fe oo op oq or b">in-L1, L1-L2, L2-L3 </code>和<code class="fe oo op oq or b"> L2-out</code>)。</p><p id="4269" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">类似地，<code class="fe oo op oq or b">classifier.intercepts_</code>是<strong class="mq jd">偏置</strong>向量的列表，其中索引<code class="fe oo op oq or b">i</code>处的向量表示添加到层<code class="fe oo op oq or b">i+1</code>的偏置值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/92a43f14cd96f851f426bd38fc93ee8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ueDnk29cALtbNaah8D_axw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图5:作者在Notes (iOS)上的手工图。</figcaption></figure><p id="4b74" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">让我们验证一下:</p><pre class="ks kt ku kv gt ot or ou ov aw ow bi"><span id="d956" class="ma lj it or b gy ox oy l oz pa">len(classifier.intercepts_) == len(classifier.coefs_) == 4</span></pre><p id="de70" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">正确返回<code class="fe oo op oq or b">True</code>。</p><p id="5315" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">提醒:<strong class="mq jd">输入</strong> <strong class="mq jd">层</strong> <strong class="mq jd">权重</strong> <strong class="mq jd">矩阵</strong>将具有形状<code class="fe oo op oq or b">784 x #neurons_in_1st_hidden_layer.</code><strong class="mq jd">输出层权重矩阵将具有形状</strong> <code class="fe oo op oq or b">#neurons_in_3rd_hidden_layer x #number_of_classes.</code></p><h2 id="fe7f" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">可视化输入图层的学习权重</h2><pre class="ks kt ku kv gt ot or ou ov aw ow bi"><span id="916c" class="ma lj it or b gy ox oy l oz pa">target_layer = 0 #0 is input, 1 is 1st hidden etc</span><span id="3a3c" class="ma lj it or b gy pb oy l oz pa">fig, axes = plt.subplots(1, 1, figsize=(15,6))<br/>axes.imshow(np.transpose(classifier.coefs_[target_layer]), cmap=plt.get_cmap("gray"), aspect="auto")</span><span id="195a" class="ma lj it or b gy pb oy l oz pa">axes.set_xlabel(f"number of neurons in {target_layer}")<br/>axes.set_ylabel("neurons in output layer")<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/7420cb9f9955928a90b1871ab03229d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xc7SuVJTO6xzebMaiVrcBg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图6:输入和第一个隐藏层之间的神经元的学习权重的可视化。图由作者制作。</figcaption></figure><p id="097b" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">或者把它们重新塑造成2D的形象。</p><pre class="ks kt ku kv gt ot or ou ov aw ow bi"><span id="c236" class="ma lj it or b gy ox oy l oz pa"># choose layer to plot<br/>target_layer = 0 #0 is input, 1 is 1st hidden etc</span><span id="f27e" class="ma lj it or b gy pb oy l oz pa">fig, axes = <a class="ae lh" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" rel="noopener ugc nofollow" target="_blank">plt.subplots</a>(4, 4)<br/>vmin, vmax = classifier.coefs_[0].min(), classifier.coefs_[target_layer].max()</span><span id="8808" class="ma lj it or b gy pb oy l oz pa">for coef, ax in zip(classifier.coefs_[0].T, axes.ravel()):<br/>    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)<br/>    ax.set_xticks(())<br/>    ax.set_yticks(())<br/><a class="ae lh" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" rel="noopener ugc nofollow" target="_blank">plt.show</a>()</span></pre><h1 id="17d7" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">3.摘要</h1><p id="d70a" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">MLP分类器是一个非常强大的神经网络模型，能够学习复杂数据的非线性函数。该方法使用前向传播来构建权重，然后计算损失。接下来，使用反向传播来更新权重，从而减少损失。这是以迭代的方式完成的，并且 <strong class="mq jd">迭代</strong>的<strong class="mq jd">号</strong> <strong class="mq jd">是一个输入<strong class="mq jd">超参数</strong>，如我在介绍中所解释的。其他重要的<strong class="mq jd">超参数</strong>是每个隐层的</strong> <strong class="mq jd">神经元</strong>的<strong class="mq jd">数量</strong> <strong class="mq jd">和总共的<strong class="mq jd">隐层数量</strong>。这些都需要微调。</strong></p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="7fb0" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">那都是乡亲们！希望你喜欢这篇文章！</p><h2 id="0aa5" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">只需5秒钟就能订阅我的邮件列表:<a class="ae lh" href="https://seralouk.medium.com/subscribe" rel="noopener">https://seralouk.medium.com/subscribe</a></h2><h1 id="65ca" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">敬请关注并支持这一努力</h1><p id="14ef" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">如果你喜欢并发现这篇文章有用，<strong class="mq jd">关注</strong>我！</p><p id="2da2" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">有问题吗？把它们作为评论贴出来，我会尽快回复。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="9d89" class="li lj it bd lk ll oj ln lo lp ok lr ls ki ol kj lu kl om km lw ko on kp ly lz bi translated">最新帖子</h1><div class="pl pm gp gr pn po"><a href="https://medium.com/mlearning-ai/how-to-use-python-sql-to-manipulate-data-in-1-min-bbf9ec17dc5d" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jd gy z fp pt fr fs pu fu fw jc bi translated">如何使用Python &amp; SQL在1分钟内操作数据</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">请继续阅读！</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">medium.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc lb po"/></div></div></a></div><div class="pl pm gp gr pn po"><a href="https://towardsdatascience.com/time-series-forecasting-predicting-stock-prices-using-facebooks-prophet-model-9ee1657132b5" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jd gy z fp pt fr fs pu fu fw jc bi translated">时间序列预测:用脸书的先知模型预测股票价格</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">使用可从《先知脸书》公开获得的预测模型预测股票价格</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qd l pz qa qb px qc lb po"/></div></div></a></div><div class="pl pm gp gr pn po"><a href="https://towardsdatascience.com/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jd gy z fp pt fr fs pu fu fw jc bi translated">用新冠肺炎假设的例子解释ROC曲线:二分类和多分类…</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我清楚地解释了什么是ROC曲线以及如何阅读它。我用一个新冠肺炎的例子来说明我的观点，我…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qe l pz qa qb px qc lb po"/></div></div></a></div><div class="pl pm gp gr pn po"><a href="https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jd gy z fp pt fr fs pu fu fw jc bi translated">支持向量机(SVM)解释清楚:分类问题的python教程…</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何绘制支持…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qf l pz qa qb px qc lb po"/></div></div></a></div><div class="pl pm gp gr pn po"><a href="https://towardsdatascience.com/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jd gy z fp pt fr fs pu fu fw jc bi translated">关于Python中的最小-最大规范化，您需要知道的一切</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我将解释什么是最小-最大缩放，什么时候使用它，以及如何使用scikit在Python中实现它</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qg l pz qa qb px qc lb po"/></div></div></a></div><div class="pl pm gp gr pn po"><a href="https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jd gy z fp pt fr fs pu fu fw jc bi translated">Scikit-Learn的标准定标器如何工作</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">在这篇文章中，我将解释为什么以及如何使用scikit-learn应用标准化</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qh l pz qa qb px qc lb po"/></div></div></a></div><h1 id="2722" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">参考</h1><p id="8b87" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">[1]https://en.wikipedia.org/wiki/Multilayer_perceptron<a class="ae lh" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank"/></p><p id="eb67" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">[2]<a class="ae lh" href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mlp-tips" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/neural _ networks _ supervised . html # MLP-tips</a></p><p id="dd3a" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated">[3]<a class="ae lh" href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mathematical-formulation" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/neural _ networks _ supervised . html # mathematical-formulation</a></p><p id="9c94" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Backpropagation</a></p><p id="6ea4" class="pw-post-body-paragraph mo mp it mq b mr nk kd mt mu nl kg mw mf nt my mz mi nu nb nc ml nv ne nf ng im bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/MNIST_database</a></p><h1 id="8397" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">和我联系</h1><ul class=""><li id="8f00" class="ni nj it mq b mr ms mu mv mf qi mi qj ml qk ng np nq nr ns bi translated"><strong class="mq jd">邮件列表:</strong><a class="ae lh" href="https://seralouk.medium.com/subscribe" rel="noopener">https://seralouk.medium.com/subscribe</a></li><li id="54cc" class="ni nj it mq b mr nw mu nx mf ny mi nz ml oa ng np nq nr ns bi translated"><strong class="mq jd">领英</strong>:<a class="ae lh" href="https://www.linkedin.com/in/serafeim-loukas/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/serafeim-loukas/</a></li><li id="4c1e" class="ni nj it mq b mr nw mu nx mf ny mi nz ml oa ng np nq nr ns bi translated"><strong class="mq jd">研究门</strong>:【https://www.researchgate.net/profile/Serafeim_Loukas T2】</li></ul></div></div>    
</body>
</html>