<html>
<head>
<title>Google Trains Reinforcement Learning Agents to Ask the Right Questions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌训练强化学习代理提出正确的问题</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/google-trains-reinforcement-learning-agents-to-ask-the-right-questions-3f7eafabb3fe?source=collection_archive---------2-----------------------#2021-05-24">https://pub.towardsai.net/google-trains-reinforcement-learning-agents-to-ask-the-right-questions-3f7eafabb3fe?source=collection_archive---------2-----------------------#2021-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="fe03" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="5ead" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">AQA训练代理主动回答问题。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f8eaacbb5b76e458421298a88222e96e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9UO_u2yr6FNs5jL_"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://www.cigen.com.au/cigenblog/top-5-rpa-questions-customers-ask-we-answer-them" rel="noopener ugc nofollow" target="_blank">https://www . cigen . com . au/cigen blog/top-5-RPA-questions-customers-ask-we-answer-them</a></figcaption></figure><blockquote class="li lj lk"><p id="2bb7" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过80，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到85，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="5093" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">语言智能的大部分努力集中在从文本数据集中提取知识的训练模型上。这种范式假设目标知识已经嵌入数据集中，不需要任何进一步的澄清，但这很少类似于人类的学习方式。当面对一个新的主题时，我们总是被迫提出问题并进行澄清。如果我们可以将同样的技能构建到人工智能(AI)模型中，会怎么样？</p><p id="706f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">提出问题的能力是人类认知过程的基本要素。人类对话的基石依赖于我们用无数种方式表达问题以获得特定答案的能力。问题重构通过澄清一个特定的点来帮助人类克服不确定性。近年来，人工智能(AI)领域在专注于问答(QA)的自然语言处理(NLP)系统方面取得了令人难以置信的进展。尽管取得了进步，但大多数NLP问答系统缺乏像人类一样处理不确定性的能力，通过重新制定问题，发布多次搜索，评估和汇总响应。2018年，来自谷歌的人工智能研究人员发表了一篇<a class="ae lh" href="https://ai.google/research/pubs/pub46733" rel="noopener ugc nofollow" target="_blank">研究论文</a>和一个<a class="ae lh" href="https://github.com/google/active-qa" rel="noopener ugc nofollow" target="_blank">开源TensorFlow包</a>，提出了一种强化学习技术，以训练智能体主动回答问题。</p><p id="6d23" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">谷歌的主动问答代理(AQA)背后的想法相对简单。给定一个特定的问题，AQA代理将多次重新制定问题，以确认正确的答案。例如，考虑问题“<em class="ln">特斯拉是什么时候出生的？</em>”。代理以两种不同的方式重新表述问题:“<em class="ln">特斯拉的生日是什么时候</em>”和“<em class="ln">特斯拉是哪一年出生的</em>”，从QA系统中检索这两个问题的答案。使用所有这些信息，它决定返回“<em class="ln">1856年7月10日</em>”。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/bd1d06311259bb43e58c5cfeedd3fff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*b5ESANvFNeRjElLX.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:谷歌</figcaption></figure><h1 id="d61c" class="ne nf it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">内部AQA</h1><p id="5739" class="pw-post-body-paragraph ll lm it lo b lp nw kd lr ls nx kg lu na ny lx ly nb nz mb mc nc oa mf mg mh im bi translated">Google的主动问答(AQA)代理基于三个基本组件:环境、重构和答案选择模型。AQA模型与黑盒环境交互。AQA用一个问题的多个版本查询它，最后返回找到的最佳答案。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/64dd7f6501ba661b522b501a1f3d0e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/0*nA1SUGcArh-Z0ZRt.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:谷歌</figcaption></figure><h1 id="7519" class="ne nf it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">问答环境</h1><p id="13ac" class="pw-post-body-paragraph ll lm it lo b lp nw kd lr ls nx kg lu na ny lx ly nb nz mb mc nc oa mf mg mh im bi translated">AQA环境基于<a class="ae lh" href="https://allenai.github.io/bi-att-flow/" rel="noopener ugc nofollow" target="_blank">双向注意力流(BiDAF) </a>模型。BiDAF是竞争性的神经问答模型，它能够产生查询感知的上下文表示，而不需要早期的摘要。在AQA的情况下，BiDAF能够从给定文档的连续范围中选择答案。给定一个问题，环境会返回一个答案，并在训练期间返回一个奖励。</p><h1 id="f396" class="ne nf it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">重构模型</h1><p id="8ab5" class="pw-post-body-paragraph ll lm it lo b lp nw kd lr ls nx kg lu na ny lx ly nb nz mb mc nc oa mf mg mh im bi translated">Google的AQA使用一个预训练的<a class="ae lh" href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">序列到序列</a>模型作为它的重构机制。序列对序列技术在包括机器翻译在内的几个NLP领域已经变得流行。在某种程度上，翻译是对不同语言的重新表述😉。在AQA的情况下，重构系统接收问题并以相同的原始语言返回其重构。</p><p id="9ce9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">与传统的序列对序列方法的一个主要偏差是，Google的AQA使用了强化学习和策略梯度方法。对于给定的问题q0，我们希望返回最佳可能答案a*，最大化奖励a* = argmaxa R(ajq0)。相对于原始问题q0计算奖励，同时为q提供答案</p><h1 id="7c4d" class="ne nf it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">答案选择模型</h1><p id="32cc" class="pw-post-body-paragraph ll lm it lo b lp nw kd lr ls nx kg lu na ny lx ly nb nz mb mc nc oa mf mg mh im bi translated">答案选择模型的作用是从一组生成的答案{a1，a2…中确定最佳答案。安}。在培训期间，AQA可以获得针对每个重新制定的qi返回的答案的奖励。然而，在考试时，我们必须预测最佳答案a*。问题选择问题被设计成一个二元分类任务，区分高于和低于平均水平的表现。在训练中，AQA为每个实例计算答案的F1分数。如果重写产生的答案的F1分数大于其他重写的平均分数，则该实例被分配一个正标签。</p><p id="6fb8" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">谷歌团队评估了不同的选项，如FFNNs、LSTM或CNN，以实现答案选择模型。虽然所有选项产生了可比较的结果，但CNN在计算效率方面提供了一些优势。最终，AQA的答案选择模型是使用预先训练好的查询、重写和答案的嵌入实现的。对于每个嵌入，AQA添加一个一维CNN，然后是最大池。然后，三个结果向量被连接起来，并通过产生输出的前馈网络。</p><h1 id="ee79" class="ne nf it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">AQA在行动</h1><p id="754d" class="pw-post-body-paragraph ll lm it lo b lp nw kd lr ls nx kg lu na ny lx ly nb nz mb mc nc oa mf mg mh im bi translated">谷歌团队使用不同的实验评估了其主动问答(AQA)模型。值得注意的是，他们使用了基于一组危险线索的SearchQA数据集。线索是令人困惑的查询，比如这个<em class="ln">“我们的国父‘并没有真的砍倒一棵樱桃树’”。</em>每条线索都与正确答案相关联，例如乔治·华盛顿，以及来自谷歌热门搜索结果的片段列表。SearchQA包含超过14万个问题/答案对和690万个片段。AQA在SearchQA数据集上进行训练，并且能够优于其他问答方法，如基本NMT或MI子查询，如下节所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/3af559cd33296d914c938a3b7553ed72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_PVQTvK40Lvkb9NV.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:谷歌</figcaption></figure><p id="3cd0" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">问题重构是人类对话本质的一部分。虽然人工智能代理在类似人类的语言环境中还有很长的路要走，但谷歌的AQA等技术通过提出正确的问题，提供了一种更有效的方式来减少不确定性。</p></div></div>    
</body>
</html>