<html>
<head>
<title>Fully Explained Ensemble Random Forest Example with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python完整解释了集合随机森林示例</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fully-explained-ensemble-techniques-example-with-python-b83e50310841?source=collection_archive---------0-----------------------#2021-02-11">https://pub.towardsai.net/fully-explained-ensemble-techniques-example-with-python-b83e50310841?source=collection_archive---------0-----------------------#2021-02-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1a55" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="f597" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">基于几种决策树的机器学习方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8352090d228be148ab33da47d926a91b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7q1ZeosgSPjgjoOIVrBjg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">集合模型。作者的照片</figcaption></figure><p id="dc54" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本文中，集成技术是基于多重决策树的。如果我们只讨论在建模后给出高方差的决策树，这会导致过度拟合。使用集合方法的好处是通过平均(bagging)或增强技术给出方差减少的良好预测。</p><p id="2293" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有各种类型的集成技术，如分类和回归，如下所示:</p><ul class=""><li id="720c" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">平均法中有套袋、随机林和额外树。</li><li id="ea04" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">Adaboost、梯度增强、CatBoost等都属于增强方法。</li></ul><p id="bced" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<strong class="lj jd">平均方法</strong>中，通常用于减少方差，而最终预测是基于所有其他弱学习者或基础学习者的平均值。这里的基础学习者是用来做预测的不同算法。</p><p id="f484" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们使用与原始形状相同的自举采样数据集来替换所有其他数据集，以便所有模型的预测不会相同。</p><p id="35b6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<strong class="lj jd"> boosting方法</strong>中，通常用于在预测基于序列方式时减少偏差，指的是从第一个模型到下一个具有改进预测的模型的误差，以此类推，直到最后一个基础模型。</p><div class="mr ms gp gr mt mu"><a href="https://medium.com/towards-artificial-intelligence/become-a-data-scientist-in-2021-with-these-following-steps-5bf70a0fe0a1" rel="noopener follow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jd gy z fp mz fr fs na fu fw jc bi translated">按照以下步骤，在2021年成为一名数据科学家</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">走上数据科学家之路需要具备的基本点</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">medium.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni lb mu"/></div></div></a></div><h2 id="8499" class="nj nk it bd nl nm nn dn no np nq dp nr lq ns nt nu lu nv nw nx ly ny nz oa iz bi translated"><strong class="ak"> <em class="ob">在本文中，我们将讨论用python实现的随机森林算法，其他的将在以后的文章中讨论。</em>T15】</strong></h2><p id="6195" class="pw-post-body-paragraph lh li it lj b lk oc kd lm ln od kg lp lq oe ls lt lu of lw lx ly og ma mb mc im bi translated">sklearn有两种风格的随机森林，即<strong class="lj jd"><em class="oh">RandomForestClassifier</em></strong>和<strong class="lj jd"><em class="oh">RandomForestRegression</em></strong></p><p id="c51a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这两种口味具有不同的标准参数，如下所示:</p><ul class=""><li id="b3bf" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated"><strong class="lj jd">随机分类器</strong>具有<code class="fe oi oj ok ol b">gini</code>和<code class="fe oi oj ok ol b">entropy</code>指标，默认为基尼。</li><li id="c329" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><strong class="lj jd">随机回归</strong>具有<code class="fe oi oj ok ol b">MSE</code>和<code class="fe oi oj ok ol b">MAE</code>指标，即均方误差和平均绝对误差，而这里的默认值是MSE。</li></ul><p id="76a3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其他参数也很有用，如<code class="fe oi oj ok ol b">n_estimator</code>和<code class="fe oi oj ok ol b">max_features</code>。前者的缺省值是100(即100棵决策树),但是越多的估计数越好，直到显著。而后者是最佳分割的随机子集的大小。</p><p id="432c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果<code class="fe oi oj ok ol b">max_features</code>的数量增加，可能会导致更大的偏差，如果数量减少，方差会大大减少。</p><p id="cc1b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另一个值得注意的参数是<code class="fe oi oj ok ol b">n_jobs</code>，这意味着如果我们想要快速处理，那么我们必须将值设为“-1”，而不是默认的<code class="fe oi oj ok ol b">none</code>。这一过程被称为并行化，在这一过程中，机器将使用所有内核进行多处理。</p><p id="d2b2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oi oj ok ol b">bootstrap</code>的默认值为真。</p><p id="b41e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">带回归和分类的Python实现如下所示:</p><div class="mr ms gp gr mt mu"><a href="https://medium.com/towards-artificial-intelligence/concepts-of-bias-and-variance-in-dataset-aa119ccd2118" rel="noopener follow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jd gy z fp mz fr fs na fu fw jc bi translated">数据集中偏差和方差的概念</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">对数据科学中使用的定义有基本的理解</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">medium.com</p></div></div><div class="nd l"><div class="om l nf ng nh nd ni lb mu"/></div></div></a></div><blockquote class="on oo op"><p id="0e0e" class="lh li oh lj b lk ll kd lm ln lo kg lp oq lr ls lt or lv lw lx os lz ma mb mc im bi translated"><strong class="lj jd"> <em class="it">现在我们来做一个用随机森林解决分类问题的练习。</em> </strong></p></blockquote><h2 id="c8fd" class="nj nk it bd nl nm nn dn no np nq dp nr lq ns nt nu lu nv nw nx ly ny nz oa iz bi translated">带分类</h2><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="e8bb" class="nj nk it ol b gy ox oy l oz pa"># Importing the libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span></pre><p id="f022" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">导入库后，现在我们将读取CSV文件，并将特征分成独立变量和因变量。</p><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="207a" class="nj nk it ol b gy ox oy l oz pa"># Importing the dataset<br/>dataset = pd.read_csv('Social_Network_Ads.csv')<br/>x_set_values = dataset.iloc[:, [2, 3]].values<br/>y_set_values = dataset.iloc[:, 4].values</span></pre><p id="4761" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在将数据集分为训练集和测试集。</p><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="e795" class="nj nk it ol b gy ox oy l oz pa">from sklearn.model_selection import train_test_split</span><span id="2152" class="nj nk it ol b gy pb oy l oz pa">X_train, X_test, y_train, y_test = train_test_split(x_set_values, <br/>                y_set_values, test_size = 0.25, random_state = 0)</span></pre><p id="cf0a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在对机器学习算法建模之前，我们应该总是做标准的缩放。</p><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="df91" class="nj nk it ol b gy ox oy l oz pa"># Feature Scaling<br/>from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><p id="f797" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这个算法中，我们将使用熵标准。</p><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="56f6" class="nj nk it ol b gy ox oy l oz pa"># Fitting the classifier to the Training set<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="4f41" class="nj nk it ol b gy pb oy l oz pa">classifier = RandomForestClassifier(n_estimators = 10, criterion = <br/>                                     'entropy', random_state= 0)</span><span id="a788" class="nj nk it ol b gy pb oy l oz pa">classifier.fit(X_train, y_train)</span><span id="ed41" class="nj nk it ol b gy pb oy l oz pa">#output:</span><span id="26cb" class="nj nk it ol b gy pb oy l oz pa">RandomForestClassifier(bootstrap=True, class_weight=None,<br/>    criterion='entropy', max_depth=None, max_features='auto',<br/>    max_leaf_nodes=None, min_impurity_decrease=0.0,  <br/>    min_impurity_split=None, min_samples_leaf=1,<br/>    min_samples_split=2, min_weight_fraction_leaf=0.0,<br/>    n_estimators=10, n_jobs=None, oob_score=False, random_state=0,<br/>    verbose=0, warm_start=False)</span></pre><p id="8523" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，我们将预测数据并制作我们的模型。</p><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="af36" class="nj nk it ol b gy ox oy l oz pa"># Predicting the Test set results<br/>y_pred = classifier.predict(X_test)</span></pre><p id="98b5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，我们计算混淆矩阵。</p><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="d315" class="nj nk it ol b gy ox oy l oz pa"># Making the Confusion Matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="9d08" class="nj nk it ol b gy pb oy l oz pa">#output:<br/>array([[63,  5],<br/>       [ 4, 28]], dtype=int64)</span></pre><p id="2e73" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用熵准则可视化训练和测试结果。</p><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="071f" class="nj nk it ol b gy ox oy l oz pa"># Visualising the Training set results<br/>from matplotlib.colors import ListedColormap</span><span id="727d" class="nj nk it ol b gy pb oy l oz pa">X_set, y_set = X_train, y_train</span><span id="9847" class="nj nk it ol b gy pb oy l oz pa">X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop =<br/>                     X_set[:, 0].max() + 1, step = 0.01),<br/>                     np.arange(start = X_set[:, 1].min() - 1, stop =<br/>                     X_set[:, 1].max() + 1, step = 0.01))</span><span id="34de" class="nj nk it ol b gy pb oy l oz pa">plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), <br/>            X2.ravel()]).T).reshape(X1.shape),<br/>             alpha = 0.5, cmap = ListedColormap(('red', 'green')))</span><span id="9b69" class="nj nk it ol b gy pb oy l oz pa">plt.xlim(X1.min(), X1.max())<br/>plt.ylim(X2.min(), X2.max())</span><span id="3bae" class="nj nk it ol b gy pb oy l oz pa">for i, j in enumerate(np.unique(y_set)):<br/>    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],<br/>                alpha=0.5, c = ListedColormap(('red', 'green'))(i),<br/>                label = j)</span><span id="fc6f" class="nj nk it ol b gy pb oy l oz pa">plt.title('Random Forest Classifier (Training set) with Entropy')<br/>plt.xlabel('Age')<br/>plt.ylabel('Estimated Salary')<br/>plt.legend()<br/>plt.show()</span></pre><p id="6bd9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">测试集的更改</p><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="9491" class="nj nk it ol b gy ox oy l oz pa">X_set, y_set = X_test, y_test</span><span id="043a" class="nj nk it ol b gy pb oy l oz pa">and </span><span id="4b49" class="nj nk it ol b gy pb oy l oz pa">plt.title('Random Forest Classifier (Testing set) with Entropy')</span></pre><div class="ks kt ku kv gt ab cb"><figure class="pc kw pd pe pf pg ph paragraph-image"><img src="../Images/cd9ae3069e7969afcf97af2855d37ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*IkDi5mJpIVxTRV_cKsJjXA.png"/></figure><figure class="pc kw pi pe pf pg ph paragraph-image"><img src="../Images/6583501cd2c3520f54c7463e37f8c29a.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*RaLswD5XE2o8WmEvgMYpJw.png"/><figcaption class="ld le gj gh gi lf lg bd b be z dk pj di pk pl translated">用于训练和测试集的随机森林。作者的照片</figcaption></figure></div><div class="mr ms gp gr mt mu"><a href="https://medium.com/towards-artificial-intelligence/z-statistics-t-statistics-p-statistics-are-still-confusing-you-87557047e20a" rel="noopener follow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jd gy z fp mz fr fs na fu fw jc bi translated">Z-统计量，T-统计量，P-统计量还在迷惑你？</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">机器学习统计学中的定义和概念</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">medium.com</p></div></div><div class="nd l"><div class="pm l nf ng nh nd ni lb mu"/></div></div></a></div><div class="mr ms gp gr mt mu"><a href="https://medium.com/towards-artificial-intelligence/fully-explained-decision-tree-classification-with-python-d90d3bd16836" rel="noopener follow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jd gy z fp mz fr fs na fu fw jc bi translated">用Python全面解释决策树分类</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">分类问题决策树的深入研究</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">medium.com</p></div></div><div class="nd l"><div class="pn l nf ng nh nd ni lb mu"/></div></div></a></div><h2 id="a89c" class="nj nk it bd nl nm nn dn no np nq dp nr lq ns nt nu lu nv nw nx ly ny nz oa iz bi translated">随机森林回归的变化</h2><ul class=""><li id="001b" class="md me it lj b lk oc ln od lq po lu pp ly pq mc mi mj mk ml bi translated">没有分成训练集和测试集。</li><li id="e5d1" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">不需要标准缩放。</li><li id="834c" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">选择回归</li></ul><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="fd42" class="nj nk it ol b gy ox oy l oz pa">#Now applying regression in random forest<br/>from sklearn.ensemble import RandomForestRegressor</span><span id="bfc0" class="nj nk it ol b gy pb oy l oz pa">regressor = RandomForestRegressor(n_estimators=10, random_state = 0)</span><span id="7eae" class="nj nk it ol b gy pb oy l oz pa">regressor.fit(X, y)</span><span id="bace" class="nj nk it ol b gy pb oy l oz pa">#output:</span><span id="1ed3" class="nj nk it ol b gy pb oy l oz pa">RandomForestRegressor(bootstrap=True, criterion='mse',<br/>         max_depth=None, max_features='auto', max_leaf_nodes=None,<br/>         min_impurity_decrease=0.0, min_impurity_split=None,<br/>         min_samples_leaf=1, min_samples_split=2,<br/>         min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,<br/>         oob_score=False, random_state=0, verbose=0, <br/>         warm_start=False)</span></pre><p id="080f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">随机回归的可视化。</p><pre class="ks kt ku kv gt ot ol ou ov aw ow bi"><span id="0ea7" class="nj nk it ol b gy ox oy l oz pa">#Random Forest Regression scatter plot for n_estimator<br/>X_grid = np.arange(min(X), max(X), 0.01)<br/>X_grid = X_grid.reshape((len(X_grid), 1))<br/>plt.scatter(X, y, color = ‘red’)<br/>plt.plot(X_grid, regressor.predict(X_grid), color = ‘blue’)<br/>plt.title(‘(Random Forest Regression)’)<br/>plt.xlabel(‘Position level’)<br/>plt.ylabel(‘Salary’)<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/1d4c510ffd83faef03ff865875cfe690.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*FJhrHWxSDzGQX6yu56Sl2w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">回归可视化。作者的照片</figcaption></figure><p id="31a2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从该图中可以看出，我们在n_estimator中选择的决策树数量是“100 ”,它会计算这些点100次，然后计算这些点的平均值，从而得出预测值。</p><blockquote class="on oo op"><p id="4064" class="lh li oh lj b lk ll kd lm ln lo kg lp oq lr ls lt or lv lw lx os lz ma mb mc im bi translated"><strong class="lj jd">结论:</strong></p></blockquote><p id="07f5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">随机森林算法在分类和回归中非常有用。标准参数给出了分类中数据集的不同树分裂，只有<code class="fe oi oj ok ol b">n_estimator</code>需要回归。</p><p id="d4c0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae ps" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae ps" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="f3cf" class="pt nk it bd nl pu pv pw no px py pz nr ki qa kj nu kl qb km nx ko qc kp oa qd bi translated">推荐文章</h1><ol class=""><li id="60fb" class="md me it lj b lk oc ln od lq po lu pp ly pq mc qe mj mk ml bi translated"><a class="ae ps" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> NLP —用Python从零到英雄</a></li></ol><p id="e2e6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">2.<a class="ae ps" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a></p><p id="932e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">3.<a class="ae ps" href="https://medium.com/towards-artificial-intelligence/python-zero-to-hero-with-examples-c7a5dedb968b?source=friends_link&amp;sk=186aff630c2241aca16522241333e3e0" rel="noopener"> Python:零到英雄附实例</a></p><p id="86cc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">4.<a class="ae ps" href="https://medium.com/towards-artificial-intelligence/fully-explained-svm-classification-with-python-eda124997bcd?source=friends_link&amp;sk=da300d557992d67808746ee706269b2f" rel="noopener">用Python全面讲解SVM分类</a></p><p id="bf47" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">5.<a class="ae ps" href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a?source=friends_link&amp;sk=9c5c613ceb10f2d203712634f3b6fb28" rel="noopener">用Python全面解释K-means聚类</a></p><p id="fdc6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">6.<a class="ae ps" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python全面解释线性回归</a></p><p id="a220" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">7.<a class="ae ps" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python全面解释逻辑回归</a></p><p id="1143" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">8.<a class="ae ps" href="https://medium.com/towards-artificial-intelligence/basic-of-time-series-with-python-a2f7cb451a76?source=friends_link&amp;sk=09d77be2d6b8779973e41ab54ebcf6c5" rel="noopener">Python时间序列基础</a></p><p id="40ca" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">9.<a class="ae ps" href="https://medium.com/towards-artificial-intelligence/numpy-zero-to-hero-with-python-d135f57d6082?source=friends_link&amp;sk=45c0921423cdcca2f5772f5a5c1568f1" rel="noopener"> NumPy:用Python零到英雄</a></p><p id="01cb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">10.<a class="ae ps" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>