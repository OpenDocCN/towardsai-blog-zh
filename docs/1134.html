<html>
<head>
<title>Want to Optimize your Model? Use Learning Rate Decay!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">想优化你的模型？使用学习率衰减！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/want-to-optimize-your-model-use-learning-rate-decay-8ab3ce68addc?source=collection_archive---------2-----------------------#2020-11-12">https://pub.towardsai.net/want-to-optimize-your-model-use-learning-rate-decay-8ab3ce68addc?source=collection_archive---------2-----------------------#2020-11-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3cdf" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="1f30" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">随着时间的推移调整你的学习速度参数会有很大的不同！让我们看看怎么做。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/6079b1238b8115cbe12c402143768033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eKtBDzPp7ggFgjM8"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@simonmigaj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">西蒙·米加杰</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="1ccf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">被困在付费墙后面？点击<a class="ae le" href="https://d3nyal.medium.com/want-to-optimize-your-model-use-learning-rate-decay-8ab3ce68addc?sk=e4f5153cd10dc390dfb3ec32a1445a0c" rel="noopener">这里</a>阅读完整故事与我的朋友链接！</p><p id="5dc9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这篇文章实际上是一个系列的连续体，专注于对深度学习的构建块的基本理解。以前的一些文章是，如果你需要赶上:</p><div class="mb mc gp gr md me"><a href="https://medium.com/analytics-vidhya/want-your-model-to-converge-faster-use-rmsprop-a28afc2b112b" rel="noopener follow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">想让你的模型收敛更快？使用RMSProp！</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">这是另一种用来加速训练的技术。</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">medium.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ky me"/></div></div></a></div><div class="mb mc gp gr md me"><a href="https://medium.com/towards-artificial-intelligence/model-overfitting-use-l2-regularization-9f7ca4aadb19" rel="noopener follow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">模型过度拟合？使用L2正规化！</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">用这个来增强你的深度学习模型！</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">medium.com</p></div></div><div class="mn l"><div class="mt l mp mq mr mn ms ky me"/></div></div></a></div><div class="mb mc gp gr md me"><a href="https://medium.com/towards-artificial-intelligence/training-taking-too-long-use-exponentially-weighted-averages-c15279f3df55" rel="noopener follow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">训练时间太长？使用指数加权平均值！</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">使用这种优化来加快你的训练！</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">medium.com</p></div></div><div class="mn l"><div class="mu l mp mq mr mn ms ky me"/></div></div></a></div></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><p id="343b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">深度学习在一个领域几乎统治了<em class="nc">最受关注行业的名单，</em>我认为原因是它在不久的将来有潜力。我已经多次提到过，10年后，我们将不再有人驾驶出租车，也不再有人是收银员，所有这些工作都将被机器取代。但是我们可以在以后的某一天讨论这个宽泛的方法。</p><p id="2b43" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里，这是一篇技术文章，适合那些对深度学习感兴趣并且知道事物如何工作和做事情的一些基础知识的人。进入这一领域的从业者必须了解基本术语，如<em class="nc">前馈、反向传播、学习速率</em>等。</p><p id="646e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们在这里回顾一下<strong class="lh ja"> <em class="nc">学习率</em> </strong>，因为这是本文的首要重点！</p><h1 id="5d43" class="nd ne iq bd nf ng nh ni nj nk nl nm nn kf no kg np ki nq kj nr kl ns km nt nu bi translated">学习率</h1><p id="5af7" class="pw-post-body-paragraph lf lg iq lh b li nv ka lk ll nw kd ln lo nx lq lr ls ny lu lv lw nz ly lz ma ij bi translated"><em class="nc">学习率</em>是一个<em class="nc">超参数</em>,它控制每次更新模型权重时，根据估计误差改变模型的程度。选择学习率具有挑战性，因为值<em class="nc">太小</em>可能导致<em class="nc">长训练</em>过程停滞，而值<em class="nc">太大</em>可能导致学习一组次优权重<em class="nc">太快</em>或<em class="nc">不稳定训练过程</em>。</p><blockquote class="oa ob oc"><p id="70be" class="lf lg nc lh b li lj ka lk ll lm kd ln od lp lq lr oe lt lu lv of lx ly lz ma ij bi translated">在<a class="ae le" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>和<a class="ae le" href="https://en.wikipedia.org/wiki/Statistics" rel="noopener ugc nofollow" target="_blank">统计</a>中，<strong class="lh ja">学习速率</strong>是<a class="ae le" href="https://en.wikipedia.org/wiki/Mathematical_optimization" rel="noopener ugc nofollow" target="_blank">优化算法</a>中的<a class="ae le" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">调整参数</a>，该算法确定每次迭代的步长，同时向<a class="ae le" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">损失函数</a>的最小值移动。由于它会影响新获得的信息在多大程度上覆盖旧信息，因此它隐喻性地代表了机器学习模型“学习”的速度。在<a class="ae le" href="https://en.wikipedia.org/wiki/Adaptive_control" rel="noopener ugc nofollow" target="_blank">自适应控制</a>文献中，学习率通常被称为<strong class="lh ja">增益</strong>。[1]</p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="0f31" class="nd ne iq bd nf ng og ni nj nk oh nm nn kf oi kg np ki oj kj nr kl ok km nt nu bi translated">学习率衰减</h1><blockquote class="oa ob oc"><p id="a6cb" class="lf lg nc lh b li lj ka lk ll lm kd ln od lp lq lr oe lt lu lv of lx ly lz ma ij bi translated">当使用随机或小批量梯度下降和恒定学习速率训练神经网络时，我们的算法通常以有噪声的方式(MBGD中噪声较小)收敛于最小值，并最终远离实际最小值振荡，为了克服这种情况，研究人员提出了一种建议，即随着时间的推移衰减学习速率，这有助于网络收敛于局部最小值并避免振荡。[2]</p></blockquote><p id="8cd9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这基本上意味着我们会随着时间的推移慢慢降低学习速度。这在使用小批量梯度下降时会有所帮助。</p><p id="9782" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">假设数据有点嘈杂，梯度下降遵循蓝色路径。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/05e99c2f8e12acfa51d2b719c0fdf11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lBJ7Dqj5hllsGkoRc6qzSw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图片来自作者。</figcaption></figure><h1 id="a101" class="nd ne iq bd nf ng nh ni nj nk nl nm nn kf no kg np ki nq kj nr kl ns km nt nu bi translated">有什么效果？</h1><p id="cf78" class="pw-post-body-paragraph lf lg iq lh b li nv ka lk ll nw kd ln lo nx lq lr ls ny lu lv lw nz ly lz ma ij bi translated">最初，你会有一个很大的学习率，梯度下降算法将采取很大的步骤，但随着算法的进展，它将开始采取越来越小的步骤，因此，在最小值附近收敛得更好一点！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/f0359aa11ecc2c577498c2efac185aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NbI1u6rNzJQEmpA7oWOeMw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图片来自作者。</figcaption></figure><p id="9259" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可以在这里看到绿线是腐朽的！</p><h1 id="0303" class="nd ne iq bd nf ng nh ni nj nk nl nm nn kf no kg np ki nq kj nr kl ns km nt nu bi translated">履行</h1><p id="d2b8" class="pw-post-body-paragraph lf lg iq lh b li nv ka lk ll nw kd ln lo nx lq lr ls ny lu lv lw nz ly lz ma ij bi translated">现在，学习率衰减有多种实现方式。</p><pre class="kp kq kr ks gt on oo op oq aw or bi"><span id="fbaf" class="os ne iq oo b gy ot ou l ov ow">å = (1 / (1 + decay_rate * epoch)) * å0 # 1st Implementation</span></pre><p id="7270" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一种是<strong class="lh ja"> <em class="nc">离散楼梯。</em> </strong>在这个实现中，我们在每个特定数量的时期之后将学习速率减半。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/1d0564be7ba1f3c0ec61ea38b63d97bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*UzwOhtYN16qFfsA77m7O7g.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">离散楼梯可视化取自<a class="ae le" href="https://mc.ai/learning-rate-decay-and-methods-in-deep-learning-3/" rel="noopener ugc nofollow" target="_blank">这里的</a>。</figcaption></figure><p id="caa6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一个是<strong class="lh ja"> <em class="nc">指数衰减</em> </strong>:</p><pre class="kp kq kr ks gt on oo op oq aw or bi"><span id="2c4a" class="os ne iq oo b gy ot ou l ov ow">å = (YOUR_CHOICE_NUM ^ epoch) * å0<br/># Where ‘YOUR_CHOICE_NUM’ is any number lesser than 1.</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/9f5622f3a28e6be615bef7f040a2d0bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*CLfgR9jnD5ubiU3fNk90ag.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图片取自<a class="ae le" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fmathcracker.com%2Fexponential-function-calculator&amp;psig=AOvVaw0dzlSG4AXKD9ZV-kQhfrLv&amp;ust=1605005536907000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCPCawOSl9ewCFQAAAAAdAAAAABAJ" rel="noopener ugc nofollow" target="_blank">这里</a>。</figcaption></figure><p id="9e8d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一个实现:</p><pre class="kp kq kr ks gt on oo op oq aw or bi"><span id="3442" class="os ne iq oo b gy ot ou l ov ow">å = (k / sqrt(epoch)) * å0</span></pre><p id="636c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">运筹学</p><pre class="kp kq kr ks gt on oo op oq aw or bi"><span id="80c0" class="os ne iq oo b gy ot ou l ov ow">å = (k / sqrt(mini_batch_size)) * å0</span></pre><p id="e2f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中“k”是另一个变量/超参数，我们可以根据需要进行调整。</p><h1 id="e577" class="nd ne iq bd nf ng nh ni nj nk nl nm nn kf no kg np ki nq kj nr kl ns km nt nu bi translated">人工腐烂</h1><p id="aaae" class="pw-post-body-paragraph lf lg iq lh b li nv ka lk ll nw kd ln lo nx lq lr ls ny lu lv lw nz ly lz ma ij bi translated">有时人们自己带着它，观看模型火车，如果需要的话，改变<em class="nc">学习速率</em>。</p><p id="5c40" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是这种方法是相当<strong class="lh ja">多余的</strong>并且需要很多努力。</p><p id="d687" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">相反，我们可以使用一些专门为此创建的函数。</p><h1 id="5b20" class="nd ne iq bd nf ng nh ni nj nk nl nm nn kf no kg np ki nq kj nr kl ns km nt nu bi translated">自动衰变</h1><p id="221e" class="pw-post-body-paragraph lf lg iq lh b li nv ka lk ll nw kd ln lo nx lq lr ls ny lu lv lw nz ly lz ma ij bi translated">我们可以使用多种算法，以便模型能够自行处理学习速率。其中一些是:</p><ul class=""><li id="349c" class="oz pa iq lh b li lj ll lm lo pb ls pc lw pd ma pe pf pg ph bi translated">基于时间的衰减→在迭代的基础上改变学习率。</li><li id="bfd3" class="oz pa iq lh b li pi ll pj lo pk ls pl lw pm ma pe pf pg ph bi translated">逐级衰减→逐级衰减计划每隔几个历元降低一个因子的学习速率。</li><li id="0253" class="oz pa iq lh b li pi ll pj lo pk ls pl lw pm ma pe pf pg ph bi translated">指数衰减→在迭代的基础上指数衰减。</li></ul><p id="3fdd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所有这些都由函数<a class="ae le" href="https://keras.io/api/callbacks/learning_rate_scheduler/" rel="noopener ugc nofollow" target="_blank"> LearningRateScheduler </a>处理。我们主要传递一个自定义函数，上面提到的函数之一。</p><h1 id="ee68" class="nd ne iq bd nf ng nh ni nj nk nl nm nn kf no kg np ki nq kj nr kl ns km nt nu bi translated">Keras实施</h1><pre class="kp kq kr ks gt on oo op oq aw or bi"><span id="080c" class="os ne iq oo b gy ot ou l ov ow">def DecayFunction(parameter):<br/>   # desired Decay function<br/>   return newLearningRate</span><span id="0138" class="os ne iq oo b gy pn ou l ov ow">model.fit((Train_Data), <br/>   validation_data=(Validation_Data), <br/>   epochs=YOUR_CHOICE, <br/>   batch_size=YOUR_CHOICE, <br/>   callbacks=[LearningRateScheduler], <br/>   verbose=YOUR_CHOICE<br/>)</span><span id="0858" class="os ne iq oo b gy pn ou l ov ow">LearningRateScheduler(DecayFunction)</span></pre><h1 id="8e6b" class="nd ne iq bd nf ng nh ni nj nk nl nm nn kf no kg np ki nq kj nr kl ns km nt nu bi translated">结论</h1><p id="a318" class="pw-post-body-paragraph lf lg iq lh b li nv ka lk ll nw kd ln lo nx lq lr ls ny lu lv lw nz ly lz ma ij bi translated">在本文中，我们讨论了使用恒定的学习速率如何会使我们的模型性能更差，因此，我们讨论了机器学习实践者广泛使用的一种技术来解决这个问题，并根据模型的需要来改变学习速率。</p><p id="6f33" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这样可以让你的模型更好的收敛！</p><h1 id="d5e7" class="nd ne iq bd nf ng nh ni nj nk nl nm nn kf no kg np ki nq kj nr kl ns km nt nu bi translated">参考</h1><p id="0378" class="pw-post-body-paragraph lf lg iq lh b li nv ka lk ll nw kd ln lo nx lq lr ls ny lu lv lw nz ly lz ma ij bi translated">[1] → <a class="ae le" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjx6ryr9fjsAhUMxBQKHZnhBJcQFjAAegQIBBAC&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLearning_rate&amp;usg=AOvVaw1T25UEU3n4avFPLjpGP8c1" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="05e4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2] → <a class="ae le" href="https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b" rel="noopener">链接</a>。</p><h1 id="d1b7" class="nd ne iq bd nf ng nh ni nj nk nl nm nn kf no kg np ki nq kj nr kl ns km nt nu bi translated">进一步阅读</h1><div class="mb mc gp gr md me"><a href="https://medium.com/dev-genius/deep-learning-for-weather-classification-fe877cdc721c" rel="noopener follow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">用于天气分类的深度学习</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">在不同的天气状态之间分类！</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">medium.com</p></div></div><div class="mn l"><div class="po l mp mq mr mn ms ky me"/></div></div></a></div><div class="mb mc gp gr md me"><a href="https://medium.com/analytics-vidhya/lets-discuss-encoders-and-style-transfer-c0494aca6090" rel="noopener follow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">让我们讨论编码器和风格转换</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">自动编码器和风格转移小指南。</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">medium.com</p></div></div><div class="mn l"><div class="pp l mp mq mr mn ms ky me"/></div></div></a></div><div class="mb mc gp gr md me"><a href="https://medium.com/datadriveninvestor/model-overfitting-use-dropout-a32010f0afd0" rel="noopener follow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">模型过度拟合？使用辍学！</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">最好的正规化技术。</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">medium.com</p></div></div><div class="mn l"><div class="pq l mp mq mr mn ms ky me"/></div></div></a></div></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="7d5f" class="nd ne iq bd nf ng og ni nj nk oh nm nn kf oi kg np ki oj kj nr kl ok km nt nu bi translated">联系人</h1><p id="29f2" class="pw-post-body-paragraph lf lg iq lh b li nv ka lk ll nw kd ln lo nx lq lr ls ny lu lv lw nz ly lz ma ij bi translated">如果你想了解我最新的文章和项目，<a class="ae le" href="/@D3nii" rel="noopener ugc nofollow" target="_blank">在Medium </a>上关注我。以下是我的一些联系人详细信息:</p><ul class=""><li id="7862" class="oz pa iq lh b li lj ll lm lo pb ls pc lw pd ma pe pf pg ph bi translated"><a class="ae le" href="https://www.linkedin.com/in/d3ni/" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="d586" class="oz pa iq lh b li pi ll pj lo pk ls pl lw pm ma pe pf pg ph bi translated"><a class="ae le" href="https://github.com/D3nii?tab=repositories" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="75d9" class="oz pa iq lh b li pi ll pj lo pk ls pl lw pm ma pe pf pg ph bi translated"><a class="ae le" href="https://twitter.com/danyal0_o" rel="noopener ugc nofollow" target="_blank">推特</a></li></ul><blockquote class="pr"><p id="996c" class="ps pt iq bd pu pv pw px py pz qa ma dk translated"><em class="qb">快乐学习。:)</em></p></blockquote></div></div>    
</body>
</html>