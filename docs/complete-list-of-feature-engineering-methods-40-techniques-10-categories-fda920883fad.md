# 特征工程方法的完整列表:40 种技术，10 个类别

> 原文：<https://pub.towardsai.net/complete-list-of-feature-engineering-methods-40-techniques-10-categories-fda920883fad?source=collection_archive---------0----------------------->

## [数据科学](https://towardsai.net/p/category/data-science)

## 一个完整的列表(截至目前)展示了用于数据探索的工程技术

[**点击这里了解我，我的项目，我的最新文章。**](http://www.michelangiolo.best/)

在你从事数据科学的第一年，你不太可能理解特征工程的重要性。这是因为当你没有经验或经验很少时，尤其是你独自学习时，你无法获得大数据。事实上，您将只能处理非常小的数据样本，优先使用 pandas 和 plotly 来编辑和获取有关数据的信息。

![](img/454a285f68d0db272caf2e8acd3caf04.png)

检索自:[https://towards data science . com/feature-engineering-what-powers-machine-learning-93ab 191 bcc2d](https://towardsdatascience.com/feature-engineering-what-powers-machine-learning-93ab191bcc2d)

## 为什么选择特征工程？

事实上，当您开始过渡到大数据时，功能工程变得至关重要。大数据分析现在和将来都将保持高需求，因为到目前为止，你几乎没有办法学会如何独自处理庞大的数据集。着手处理大数据的唯一方法是在工作场所与一个专家团队合作。这是真正的数据科学的全方位体验。

处理小数据集的计算量不大。因为你基本上可以使用自己的 GPU 或谷歌或其他云服务提供商免费提供的 GPU 来训练你的模型，没有任何麻烦(例如，在谷歌 Colab 中，你可以通过点击一个按钮来访问它)，你可以训练模型的次数几乎没有限制。即使对于需要大量计算能力的 CNN，在数据集很小的情况下，你也可以在几个小时内完成训练。

## 为什么大数据不同于小样本

大数据则不同:每一个选择都必须非常谨慎，因为你没有多少机会尝试这些模型。每一次尝试都要花费大量的金钱和时间，简而言之:没有出错的余地。使用特征工程，您可以在尝试之前了解如何调整模型或编辑数据。

例如，如果您的数据有太多的异常值，您需要删除它们以获得一个能够很好地概括的模型。另一个例子是，如果有非常相似的特征(它们的相关性接近 1)，您可以删除其中的一个，因为它只需要更多的 GPU 来训练，并且还可能使模型表现最差。

比如 GPT-3 花了 400 万去训练(这当然是夸张了，但是你明白我的意思)。想象一下它背后的团队在一次尝试中花费这么多之前所做的工作。总之，在开始训练您的模型之前，您必须确定使用哪种调优和模型。

这是我到目前为止能够收集到的最新特性列表。我不会解释单个的特性，因为列表是巨大的，我的目标是在一个功能层次中适当地组织每个特性:

# **特征提取**

请注意，我使用术语特征工程来包含特征提取和特征选择。特征提取意味着增加(或从无到有)特征的数量，而特征选择意味着减少其数量。

## **1。编码**

通过编码，我将数据从分类转换成数字。当我们有文本而不是数字时，这变得非常重要，基本上是非结构化数据的混乱。像嵌入神经网络这样的技术变得非常有用。这些是最常见的技术，其中连续矢量是最先进的:

*   One_Hot(已订购)
*   One_Hot(无序)
*   基于索引的编码
*   连续向量

## **2。特征分割**

通过特征分割，我将一个特征转换成两个或多个特征，增加了模型中特征的数量。执行拆分没有规则，它取决于数据。

## 3.插补方法

插补是一种用于填补数据中缺失值的统计方法。因为在处理数据时这是很常见的，有几种方法可以填充缺失的数据，这取决于您正在处理的数据。时间序列可能需要插值，而截面数据意味着插补。

*   数值插补
*   分类插补
*   随机样本插补
*   分布终点插补

# 特征选择

## 4.过滤方法

过滤方法允许根据一些指标的及格分数来选择或取消选择整个特性(本质上是列)。例如，如果一列的方差(意味着值过于分散)太高，我们有充分的理由避免将该列放入模型中，因为这可能会降低其准确性。这同样适用于不同的指标。

*   方差阈值
*   平均绝对差值
*   费希尔评分
*   相关系数
*   AM-GM 不等式
*   基于标准差的异常值检测
*   百分位异常值检测
*   卡方检验
*   信息增益

## 5.包装方法

包装方法将几种使用 R 平方值来衡量某个特征是否应该保留的技术组合在一起。这些技术通过在监视分数变化的同时迭代地使用特征来工作，因此它们递归地工作:

*   详尽的特征选择
*   向前回归
*   向后回归/RFE _ 递归特征消除
*   逐步回归
*   双向消除

## :数值插补

数字插补技术列表。

*   插入文字
*   推断
*   平均插补
*   代替
*   热卡插补
*   冷甲板归集
*   回归插补
*   随机回归插补

## 6.宁滨方法

宁滨方法是降低数据复杂性的一种方式。我们可以将数据分组，并通过对其应用函数(例如平均值)来估算一组数据的值:

*   数值宁滨
*   绝对宁滨

## 7.分组操作方法

分组运算是一种将函数应用于特定值组的方法，使用的算法称为 group by:

*   范畴分组
*   数字分组

## 8.嵌入式方法

嵌入方法是在训练模型时执行特征选择的方法。这在神经网络中很常见，在神经网络中，使用标准化技术自动选择或取消选择特征:

*   L1 归一化
*   L2 归一化
*   随机森林重要性

## 9.混合方法

混合方法是不同技术的组合(你可以从上面的列表中选择)来执行特征工程。

# 特征编辑

虽然这个名称并不存在，但我将所有允许编辑数据的方法，而不是增加或减少特征的数量，归入这一部分。

## 10.特征缩放

构建回归模型时，要素缩放是一种常见的要素工程技术:数据需要放在同一个仪表中才能正常工作。

*   标准化
*   正常化
*   对数变换