<html>
<head>
<title>This Is How You Can Explain SVD to a 7-Year-Old</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这就是你如何向一个7岁的孩子解释奇异值分解</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/this-is-how-you-can-explain-svd-to-a-7-year-old-2a4cb10632f2?source=collection_archive---------0-----------------------#2022-09-15">https://pub.towardsai.net/this-is-how-you-can-explain-svd-to-a-7-year-old-2a4cb10632f2?source=collection_archive---------0-----------------------#2022-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7a42" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解奇异值分解(SVD)算法背后的直觉。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/35688ab5b45c2953acc7b1cbe89ca5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*58vUgUZEksIixjxl"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">本·怀特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="4391" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我写这篇文章是因为，起初，我努力理解SVD算法背后的数学直觉。但如果抛开所有花哨的数学术语，算法背后的思想就很容易理解了。</p><p id="af68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">看完这篇文章，你会对</em> <strong class="lb iu"> <em class="lv">为什么以及如何在各种应用中使用</em></strong><em class="lv"/><strong class="lb iu"><em class="lv">SVD</em></strong><em class="lv">有一个直观的认识。</em></p><p id="70a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我可以抛出奇异值分解的定义，比如:</p><blockquote class="lw lx ly"><p id="850a" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">在线性代数中，奇异值分解(SVD) <strong class="lb iu"> </strong>是一个实矩阵或复矩阵的因式分解。它将具有正交本征基的方阵的本征分解推广到任何<em class="it"> m x n </em>矩阵。[1]</p></blockquote><p id="5426" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯。那是什么？这个定义对于一个初学者或者不太懂数学的人来说没有任何意义。许多人正从软件进入机器学习工程，并努力理解ML中使用的一些高级数学概念。至少我做到了。</p><p id="03f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不要误解我的意思，要完全理解SVD是什么，您需要理解它背后的数学，但是我们在这篇文章中关注的是它是如何工作的，以及我们为什么关心SVD。我们不会关注算法的实际计算。</p><p id="d008" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVD的著名用例有:</p><ul class=""><li id="3f9a" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">伪逆—任何大小矩阵的逆(Moore-Penrose逆)</li><li id="53fb" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">降维(图像、表格数据等。)</li><li id="eefb" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">推荐引擎(FunkSVD算法及其衍生物)</li></ul></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="7f7c" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">奇异值分解背后的直觉</h1><p id="b809" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">让我们把问题简化到人和他们喜欢的食物。我们假设有一个矩阵A，其中m代表人数，n代表可能的食物种类。矩阵值代表一个人对一种食物的喜爱程度，编码为从1到10的等级。</p><p id="4238" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个3x4矩阵的例子，有三个人和四种食物:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/09a1edbacaae1c5ba4b401a63af599ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tmEOm6udBagg3a9z-JBEGA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">矩阵A的例子【图片作者提供】。</figcaption></figure><p id="cf1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVD将任意<strong class="lb iu">矩阵m x n A </strong>分解成三个矩阵，<strong class="lb iu"> U </strong>、<strong class="lb iu"> S、</strong>和<strong class="lb iu"> V </strong>，其中<strong class="lb iu"> A = USVᵀ </strong>。</p><p id="8258" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们如何用关于人和他们喜欢的食物类型的例子来解释这个公式呢？</p><p id="4c77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们先从另一个问题开始回答上面的问题，了解一下背后的思维过程。在我们的示例矩阵A中，我们有三个人，他们对四种食物进行评级。但是，如果我们想了解为什么有些人对一种食物的评价高于另一种食物呢？为了深入研究这一点，我们需要找到一组描述人们和他们喜欢的食物类型之间相互作用的特征。</p><p id="a219" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们想找出为什么“人1”更喜欢吃苹果而不是香肠。</p><p id="9bb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用奇异值分解，我们可以快速得到有助于我们理解上面问题的特征。</p><p id="c1b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是分解后的矩阵<strong class="lb iu"> A = USVᵀ </strong>的样子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/88c8636892d265d58a3f2e535bce0c6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWlFkzJI7vKCCtCoYmkzPA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">美国、Vᵀ矩阵示例[图片由作者提供]。</figcaption></figure><p id="a18a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVD算法创建了一组描述人们和他们喜爱的食物类型之间的相互作用的特征。</p><p id="8a72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，算法学习的特征是健康、肉和熟的。那些被称为<strong class="lb iu">的潜在特征</strong>。</p><p id="ccbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>我们随机标注了潜在特征健康、肉和熟。这些是抽象的表示，我们给它们命名是为了快速理解这个概念，但是它们可以有任何其他的标签。</p><p id="d872" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">矩阵U </strong>是一个<strong class="lb iu"> m x k </strong>矩阵，其中<em class="lv"> k </em>是潜在特征的<em class="lv">数量。它代表了一个<em class="lv">人如何与</em>互动或“感受到】潜在特征</em>，并以数学方式描述了他们是否喜欢健康的、以肉为基础的或煮熟的食物。一般来说，它表示矩阵A的行与潜在因子之间的关系。</p><p id="ef75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">矩阵S是一个对角矩阵，告诉我们每个特征的重要性。特别是，它编码了哪个健康、肉或烹饪特征更重要。更可观的值代表特定的潜在特征携带更多的信息，可以更好地预测这个人喜欢什么类型的食物。S的对角线值按降序排列，这意味着第一个潜在特征总是比其他特征携带更多的信息。</p><p id="d700" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">矩阵Vᵀ </strong>是一个<strong class="lb iu"> k x n </strong>矩阵，其中<strong class="lb iu"> </strong>描述了<em class="lv">潜在特征</em>如何与<em class="lv">类食物</em>相互作用。一般来说，它表示潜在因素与矩阵a的列之间的关系。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="807b" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">让我们更加数学化</h1><p id="9c4d" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">但是我们仍然会保持概念的简单性，并与我们的例子联系起来。</p><h2 id="a2ba" class="nw my it bd mz nx ny dn nd nz oa dp nh li ob oc nj lm od oe nl lq of og nn oh bi translated">压缩</h2><p id="b23f" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">通常SVD用于压缩。假设我们有10亿x 100万个矩阵，这在某些情况下太大而无法处理。因此，使用SVD，我们可以压缩这个矩阵，并且仍然可以计算它的近似值。</p><p id="65f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，我们可以通过只保留最基本的潜在特征“健康食品”来压缩矩阵A现在，U、S和V矩阵看起来像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/6c7a7be9a2bb3fa7e972221322326777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SQvo7UUJy6-aA_MUm4pWyw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">简化的美国、Vᵀ矩阵的例子[图片由作者提供]。</figcaption></figure><p id="f433" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过乘以<strong class="lb iu"> USVᵀ，</strong>，我们仍将得到一个近似初始矩阵a的3×4矩阵，但仅使用我们的初始数据的一部分。</p><p id="a688" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着我们不断添加潜在特征，近似将会更精确，但是压缩的效果将会更小。</p><h2 id="784e" class="nw my it bd mz nx ny dn nd nz oa dp nh li ob oc nj lm od oe nl lq of og nn oh bi translated">推断</h2><p id="c465" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated"><strong class="lb iu">矩阵U </strong>和<strong class="lb iu"> Vᵀ </strong>是正交的，所以我们可以在我们新生成的潜在空间中把它们作为新的参考系。因此，利用这两个矩阵中的任何一个，我们可以将矩阵A投影到潜在特征的空间中。通过将<strong class="lb iu"> A </strong>乘以<strong class="lb iu"> V </strong>，我们最终得到<strong class="lb iu"> A_projected = AV </strong>。</p><p id="3f34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的具体例子中，<strong class="lb iu"> A_projected </strong>是<strong class="lb iu"> </strong>一个人与人之间互动的<strong class="lb iu"> </strong>矩阵，以及他们喜欢健康的、以肉为基础的或熟食的可能性。因此，我们将食物类型的矩阵A投射到潜在空间。现在每个人都被潜在的特征所描述。</p><h2 id="eca5" class="nw my it bd mz nx ny dn nd nz oa dp nh li ob oc nj lm od oe nl lq of og nn oh bi translated">压缩+投影</h2><p id="16d5" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">通过结合上述方法，我们可以快速实现一个<strong class="lb iu">降维</strong>算法。</p><p id="0162" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用只包含“健康”潜在特征的矩阵V，并将其乘以a。</p><p id="1420" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">A_projected将是包含大部分初始信息的3×1矩阵，因为“健康”潜在特征描述了大部分方差。因此，我们将矩阵从4维减少到只有1维。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="8e02" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">如何计算奇异值分解</h1><p id="05fa" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">我们可以用Python和<a class="ae ky" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html" rel="noopener ugc nofollow" target="_blank"> Scipy </a>高效地计算SVD矩阵:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="f128" class="nw my it ok b gy oo op l oq or">import numpy as np<br/>import scipy</span><span id="c62f" class="nw my it ok b gy os op l oq or">A = np.array(<br/>    [[1, 5, 10, 9], [8, 9, 3, 4], [10, 8, 9, 19]], <br/>    dtype=int<br/>)<br/>U, s, V_T = scipy.linalg.svd(A)</span></pre><p id="7f4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在[2]和[3]阅读关于如何计算SVD的更多信息。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="3845" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">结论</h1><p id="1444" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">在这篇文章中，我们通过人们和他们喜欢的食物类型之间的具体例子回顾了U、S和V矩阵的含义。此外，我们还通过下面的同一个例子展示了算法的一些<strong class="lb iu">用例</strong>，比如<strong class="lb iu">压缩</strong>和<strong class="lb iu">降维</strong>。最后，我们展示了用<strong class="lb iu"> Python </strong>计算SVD 是多么容易，以及为什么理解算法背后的直觉是至关重要的，而不是计算本身。</p><p id="0ab0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，您很快理解了为什么SVD如此强大，并深入研究了它的数学。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="3adf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">🎉谢谢你看我的文章！</p><p id="3c10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">📢如果你喜欢这篇文章，并想分享我在AI、ML和MLOps方面的学习历程，你也可以通过<a class="ae ky" href="https://www.linkedin.com/in/pauliusztin/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> LinkedIn </strong> </a>与我联系:</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="700a" class="nw my it bd mz nx ny dn nd nz oa dp nh li ob oc nj lm od oe nl lq of og nn oh bi translated">参考资料:</h2><p id="31ba" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">[1] <a class="ae ky" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">奇异值分解</a>，维基百科。</p><p id="7f5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] Jason Brownlee博士，<a class="ae ky" href="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">如何用Python从零开始计算SVD</a>，机器学习掌握，2018。</p><p id="44e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] <a class="ae ky" href="https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm" rel="noopener ugc nofollow" target="_blank">奇异值分解(SVD)教程</a>，MIT。</p></div></div>    
</body>
</html>