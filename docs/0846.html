<html>
<head>
<title>Easy to use Correlation Feature Selection with Kydavra</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kydavra易于使用的相关性特征选择</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/easy-to-use-correlation-feature-selection-with-kydavra-56a463b97b79?source=collection_archive---------6-----------------------#2020-08-24">https://pub.towardsai.net/easy-to-use-correlation-feature-selection-with-kydavra-56a463b97b79?source=collection_archive---------6-----------------------#2020-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="711a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/ea11c49370aa1152a982abe3712b6291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K3zLjigB8SR_mOxczrvHUQ.png"/></div></div></figure><p id="bcaf" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">几乎每个从事数据科学或机器学习的人都知道，找到预测值y的相关特征的最简单方法之一是找到与y最相关的特征。然而，很少有人(如果不是数学家)知道有许多类型的相关性。在本文中，我将简要介绍3种最流行的相关类型，以及如何将它们与Kydavra一起轻松应用于特性选择。</p><p id="b686" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ja">皮尔逊相关。</strong></p><blockquote class="lf lg lh"><p id="9246" class="kh ki li kj b kk kl km kn ko kp kq kr lj kt ku kv lk kx ky kz ll lb lc ld le ij bi translated">两个变量的协方差中的皮尔逊相关系数除以它们的标准偏差的乘积。</p></blockquote><figure class="ln lo lp lq gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi lm"><img src="../Images/72acb3a53b2e2b183a573a9d0695a1e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8jARnA7fUeh_DkvW"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk translated">图一。计算两个特征之间的皮尔逊相关性的公式。</figcaption></figure><p id="3316" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">它的值在-1和1之间，负值表示相反的关系，正值表示相反的情况。通常我们只取绝对值。因此，如果绝对值大于0.5，则该系列可以具有(是的，可以具有)关系。但是，我们还设置了一个垂直限制，0.7或0.8，因为如果值过于相关，那么可能一个序列是从另一个序列派生出来的(例如月龄和年龄)，或者可能会使我们的模型过度拟合。</p><p id="1ce0" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ja">使用Kydavra PearsonCorrelationSelector。</strong></p><p id="a410" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">首先你应该安装kydavra，如果你还没有安装的话。</p><pre class="ln lo lp lq gt lv lw lx ly aw lz bi"><span id="86de" class="ma mb iq lw b gy mc md l me mf">pip install kydavra</span></pre><p id="48b4" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">接下来，我们应该创建一个对象，并将其应用于炉边疾病UCI数据集。</p><pre class="ln lo lp lq gt lv lw lx ly aw lz bi"><span id="5ebb" class="ma mb iq lw b gy mc md l me mf">from kydavra import PearsonCorrelationSelector</span><span id="c60c" class="ma mb iq lw b gy mg md l me mf">selector = PearsonCorrelationSelector()</span><span id="5b25" class="ma mb iq lw b gy mg md l me mf">selected_cols = selector.select(df, ‘target’)</span></pre><p id="e6c7" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在炉边疾病UCI数据集上应用选择器的默认设置将给出一个空列表。这是因为没有特征与目标特征的相关性高于0.5。这就是为什么我们强烈建议您使用选择器的参数:</p><ul class=""><li id="7165" class="mh mi iq kj b kk kl ko kp ks mj kw mk la ml le mm mn mo mp bi translated"><strong class="kj ja"> min_corr </strong> ( <em class="li"> float，在0和1之间，默认=0.5 </em>)被选为重要特征的相关系数的最小值。</li><li id="58ef" class="mh mi iq kj b kk mq ko mr ks ms kw mt la mu le mm mn mo mp bi translated"><strong class="kj ja"> max_corr </strong> ( <em class="li"> float，在0和1之间，默认=0.5 </em>)被选为重要特征的相关系数的最小值。</li><li id="fea7" class="mh mi iq kj b kk mq ko mr ks ms kw mt la mu le mm mn mo mp bi translated"><strong class="kj ja">erase _ corr</strong>(<em class="li">boolean，default=False </em>)如果设置为True，则该算法将擦除仅保持on之间相关的列，如果为False，则它将保持所有列。</li></ul><p id="9d07" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">实施最后一个要素是因为如果您正在构建一个具有两个高度相关的要素的模型，那么您实际上是在提供相同的信息，从而产生了多重线性问题。因此，将min_corr更改为0.3会得到以下各列:</p><pre class="ln lo lp lq gt lv lw lx ly aw lz bi"><span id="0a29" class="ma mb iq lw b gy mc md l me mf">['sex', 'cp', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']</span></pre><p id="fb31" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">交叉验证分数保持不变——0.81。很好的结果。</p><p id="d826" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ja">斯皮尔曼相关。</strong></p><p id="901f" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">当皮尔逊相关是基于数据正态分布的假设时，斯皮尔曼秩系数不做这种假设。所以价值观不一样。然而，斯皮尔曼等级系数的范围也是-1和1。如何计算它的数学细节超出了本文的范围，所以下面是一些分析它的文章(以及更详细的下一种类型的相关性)。</p><p id="2048" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">所以现在让我们将SpermanCorrelationSelector应用于我们的数据集。</p><pre class="ln lo lp lq gt lv lw lx ly aw lz bi"><span id="e503" class="ma mb iq lw b gy mc md l me mf">from kydavra import SpermanCorrelationSelector</span><span id="e5e2" class="ma mb iq lw b gy mg md l me mf">selector = SpermanCorrelationSelector()</span><span id="6dda" class="ma mb iq lw b gy mg md l me mf">selcted_cols = selector.select(df, ‘target’)</span></pre><p id="17ea" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">使用默认设置，选择器也返回一个空列表。但是将min_corr设置为0.3会给出与PearsonCorrelation相同的列。所有相关选择器的参数都是相同的。</p><p id="1e2e" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ja">肯德尔等级相关。</strong></p><p id="eca4" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">Kydavra库中也实现了Kendall秩相关。我们让理论深入到文章中。因此，要使用Kendall等级相关性，请使用以下模板。</p><pre class="ln lo lp lq gt lv lw lx ly aw lz bi"><span id="06a8" class="ma mb iq lw b gy mc md l me mf">from kydavra import KendallCorrelationSelector</span><span id="0d21" class="ma mb iq lw b gy mg md l me mf">selector = KendallCorrelationSelector()</span><span id="a992" class="ma mb iq lw b gy mg md l me mf">selected_cols = selector.select(df, ‘target’)</span></pre><p id="ab9a" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">测试其性能，我们也让你。下面是一些深入研究相关性指标的文章。</p><figure class="ln lo lp lq gt ka gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/20f73cb96e2fa41d8d3ff82eaee5bd83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/1*UEuA3iIXXDXC-QuXwD_MhA.gif"/></div></figure><p id="ee10" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">如果您使用或尝试过Kyadavra，我们强烈邀请您填写这张<a class="ae mw" href="https://vpapaluta.typeform.com/to/g1EXxlSf" rel="noopener ugc nofollow" target="_blank">表格</a>并分享您的体验。</p><p id="c10e" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">西格蒙德用❤做的。</p><h2 id="6bcf" class="ma mb iq bd mx my mz dn na nb nc dp nd ks ne nf ng kw nh ni nj la nk nl nm iw bi translated"><strong class="ak">资源</strong></h2><ul class=""><li id="02f2" class="mh mi iq kj b kk nn ko no ks np kw nq la nr le mm mn mo mp bi translated"><a class="ae mw" href="https://towardsdatascience.com/kendall-rank-correlation-explained-dee01d99c535" rel="noopener" target="_blank">https://towards data science . com/Kendall-rank-correlation-explained-dee 01d 99 c 535</a></li><li id="94ac" class="mh mi iq kj b kk mq ko mr ks ms kw mt la mu le mm mn mo mp bi translated"><a class="ae mw" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Spearman % 27s _ rank _ correlation _ coefficient</a></li><li id="b340" class="mh mi iq kj b kk mq ko mr ks ms kw mt la mu le mm mn mo mp bi translated"><a class="ae mw" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> https://en .维基百科. org/wiki/Pearson _ correlation _ coefficient</a></li></ul></div></div>    
</body>
</html>