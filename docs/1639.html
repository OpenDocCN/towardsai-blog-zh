<html>
<head>
<title>Tweets Analysis using Bert</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Bert进行推文分析</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/tweets-analysis-using-bert-af211bc28f2a?source=collection_archive---------7-----------------------#2021-03-08">https://pub.towardsai.net/tweets-analysis-using-bert-af211bc28f2a?source=collection_archive---------7-----------------------#2021-03-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0527" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="809f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">Bert的一个简化实现，使用ktrain来预测哪些Tweets是关于真实灾难的。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a1398c10dde6145a5489adff8e399afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FW2t6mMW-Cz4pivb4iocQ.jpeg"/></div></div></figure><p id="d694" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">本教程包括一个简化的、干净的Bert实现，使用ktrain对tweets进行分类，这使我在kaggle的排行榜上名列前15%。</p><h1 id="fc7a" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">问题陈述</h1><blockquote class="mo"><p id="23e3" class="mp mq iq bd mr ms mt mu mv mw mx lv dk translated">预测哪些推文是关于真实的灾难，哪些不是用Bert模型。</p></blockquote><h1 id="2380" class="lw lx iq bd ly lz ma mb mc md me mf mg kf my kg mi ki mz kj mk kl na km mm mn bi translated">涉及的步骤</h1><ol class=""><li id="fd08" class="nb nc iq lc b ld nd lg ne lj nf ln ng lr nh lv ni nj nk nl bi translated">数据可视化</li><li id="ff36" class="nb nc iq lc b ld nm lg nn lj no ln np lr nq lv ni nj nk nl bi translated">缺失值分析</li><li id="5649" class="nb nc iq lc b ld nm lg nn lj no ln np lr nq lv ni nj nk nl bi translated">探索性数据分析(EDA)和异常值检测。</li><li id="37d1" class="nb nc iq lc b ld nm lg nn lj no ln np lr nq lv ni nj nk nl bi translated">使用Ktrain进行文本预处理。</li><li id="763a" class="nb nc iq lc b ld nm lg nn lj no ln np lr nq lv ni nj nk nl bi translated">Ktrain模型和培训</li><li id="05ad" class="nb nc iq lc b ld nm lg nn lj no ln np lr nq lv ni nj nk nl bi translated">预测和评估</li></ol><p id="607f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae nr" href="https://www.kaggle.com/pratiyushmishra/nlp-eda-cleaning-lstm-and-simplified-bert/" rel="noopener ugc nofollow" target="_blank">链接原kaggle笔记本</a></p><h1 id="3816" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">导入库</h1><p id="640d" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">我们将使用numpy和pandas处理我们的数据集，使用matplotlib和seaborn进行数据可视化，使用ktrain实现我们的bert模型。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="f7cc" class="oa lx iq nw b gy ob oc l od oe">import numpy as np<br/>import pandas as pd<br/>import re<br/>from matplotlib import pyplot as plt<br/>import seaborn as sns<br/>import tensorflow as tf<br/>import ktrain <em class="of"># For Bert Model Implementation.</em><br/>from ktrain import text <em class="of"># Preprocessing text for the Bert Model.</em><br/>sns.set()</span></pre><h1 id="765b" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">加载数据集</h1><p id="7b13" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">这里我们使用了来自<a class="ae nr" href="https://www.kaggle.com/c/nlp-getting-started/data" rel="noopener ugc nofollow" target="_blank">的自然语言处理数据集和灾难微博</a></p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="1f25" class="oa lx iq nw b gy ob oc l od oe">train = pd.read_csv('../input/nlp-getting-started/train.csv') <em class="of"># Training Data</em><br/>test = pd.read_csv('../input/nlp-getting-started/test.csv') <em class="of"># Testing Data</em><br/>train_len = len(train) # Size of training data<br/>test_len = len(test) # Size of testing data<br/>print('Training Dataset:',train_len)<br/>print('Testing Dataset:',test_len)<br/>train.head()</span></pre><h1 id="3e97" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">数据可视化</h1><ul class=""><li id="513a" class="nb nc iq lc b ld nd lg ne lj nf ln ng lr nh lv og nj nk nl bi translated"><strong class="lc ja">目标值的分配</strong></li></ul><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="444c" class="oa lx iq nw b gy ob oc l od oe">plt.figure(figsize=(15,5))<br/>plt.subplot(1, 2, 1)<br/>ax = sns.countplot(x='target',data=train,label=['Not a Disaster','Disaster'])<br/>ax.set_xticklabels(['Disaster','Not a Disaster'])<br/>plt.suptitle("Distribution of Target Values")<br/>terms = np.array(['Disaster', 'Not a Disaster'])<br/>weigtage = np.array([len(train[train['target'] == 1]),len(train[train['target'] == 0])])<br/>plt.subplot(1, 2, 2)<br/>plt.pie(weigtage,labels=terms, autopct="<strong class="nw ja">%1.1f%%</strong>")<br/>plt.title('target')<br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2fe22760ea34a0cce9d101912192e935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*caVOYwtIlfR4YuymW8PjPw.png"/></div></figure><ul class=""><li id="bb08" class="nb nc iq lc b ld le lg lh lj oi ln oj lr ok lv og nj nk nl bi translated"><strong class="lc ja">推文长度v/s推文数量</strong></li></ul><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="e7ae" class="oa lx iq nw b gy ob oc l od oe">plt.figure(figsize=(10,5))<br/>word_len = train['text'].map(lambda x: len(x))<br/>plt.hist(word_len)<br/>plt.xlabel('Length of tweets')<br/>plt.ylabel('Number of tweets')<br/>plt.title('Length of tweets v/s Number of tweets')<br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/9f677fe2c93aab84c4696afefbf52861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*_raBlh-xhbyyLEoFtOAwTg.png"/></div></figure><ul class=""><li id="11fa" class="nb nc iq lc b ld le lg lh lj oi ln oj lr ok lv og nj nk nl bi translated"><strong class="lc ja">字数v/s推文数</strong></li></ul><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="6a60" class="oa lx iq nw b gy ob oc l od oe">plt.figure(figsize=(10,5))<br/>word_len = train['text'].str.split().map(lambda x: len(x))<br/>plt.hist(word_len)<br/>plt.xlabel('Number of Words')<br/>plt.ylabel('Number of tweets')<br/>plt.title('Number of Words v/s Number of Tweets')<br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3e4522dfb26a71e668de5c087d6b8464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*J5xX-A8wVSal9wDBJKNXIw.png"/></div></figure><h1 id="74b7" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">缺失值分析</h1><p id="92e3" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">我们首先会看到所有缺少值的特征。这将包括来自训练和测试数据集的数据。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="7319" class="oa lx iq nw b gy ob oc l od oe">def missing_val_analysis(data):<br/>    missing_values = data.isnull().sum()<br/>    missing_values = missing_values[missing_values &gt; 0].sort_values(ascending = False)<br/>    missing_values_data = pd.DataFrame(missing_values)<br/>    missing_values_data.reset_index(level=0, inplace=True)<br/>    missing_values_data.columns = ['Feature','Number of Missing Values']<br/>    missing_values_data['Percentage of Missing Values'] = (100.0*missing_values_data['Number of Missing Values'])/len(data)<br/>    return missing_values_data</span></pre><p id="9382" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们在上面定义了一个函数，它给出了一个包含缺失值的要素的良好数据框。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="06f9" class="oa lx iq nw b gy ob oc l od oe">missing_val_analysis(train) <em class="of"># Missing value analysis in the training data.</em></span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi on"><img src="../Images/1f57f88dd0b714fc0a434de1fcb8a3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*hxpNJyP4RPcWoL_ULXk5Ng.png"/></div><figcaption class="oo op gj gh gi oq or bd b be z dk translated">训练数据中缺少值</figcaption></figure><p id="8a74" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因为缺少关键字的行数非常少，所以我们只需用一个空字符串填充所有缺少的值，将它添加到文本的末尾并删除该列。至于位置，它不太可能有助于模型的预测，因此我们将完全放弃这个特性。我们也会对测试数据采取同样的行动。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="b6a4" class="oa lx iq nw b gy ob oc l od oe"># Training Data<br/>train['keyword'].fillna('',inplace=True)<br/>train['text'] = train['text'] + ' ' + train['keyword']<br/>train['text'] = train['text'].apply(lambda x: x.strip())<br/>train.drop(['keyword'],axis=1,inplace=True)<br/>train.drop(['location'],axis=1,inplace=True)</span><span id="c5ab" class="oa lx iq nw b gy os oc l od oe">## Testing Data<br/>test['keyword'].fillna('',inplace=True)<br/>test['text'] = test['text'] + ' ' + test['keyword']<br/>test['text'] = test['text'].apply(lambda x: x.strip())<br/>test.drop(['keyword'],axis=1,inplace=True)<br/>test.drop(['location'],axis=1,inplace=True)</span></pre><h1 id="f171" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">探索性数据分析(EDA)和异常值检测。</h1><p id="770f" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">仔细检查训练数据集后，可以看到多条记录都有相同的推文，其中一些推文的预测相互矛盾。因此，我们将对此进行分析，并删除重复记录和异常值。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="b09b" class="oa lx iq nw b gy ob oc l od oe">duplicate_records = train[train.duplicated(['text','target'],keep=False)] <em class="of"># Duplicate records with same targets.</em><br/>print('Records having same text and targets:',len(duplicate_records))<br/>duplicate_records.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7847039ed143c004ec5fc74a6b0991bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*4a47KVYL36Wjn1EA0ZLsXw.png"/></div></figure><p id="76a3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们会为每条这样的推文保留一份副本，删除重复的。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="ff81" class="oa lx iq nw b gy ob oc l od oe">train.drop_duplicates(['text','target'],inplace=True) <em class="of"># Dropping the duplicate records having same targets.</em></span></pre><p id="26cd" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在剩下的可能是异常值，也就是具有矛盾预测的相同记录。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="ba5a" class="oa lx iq nw b gy ob oc l od oe">contradicting_records = train[train.duplicated(['text'],keep=False)] <em class="of"># Duplicate records with outliers.</em><br/>print('Records having same text but different targets:',len(contradicting_records))<br/>contradicting_records.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/63d9cf22e706bc847dca39fdf783ae4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*oElhOh-GRU-3cz_I1lq2wQ.png"/></div></figure><p id="19a5" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">由于记录的数量非常少，因此通过人工检查，创建一个包含所有需要移除的异常值的索引的列表。</strong></p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="e6dc" class="oa lx iq nw b gy ob oc l od oe">records_to_drop = [610,2832,3243,3985,4244,4232,4292,4305,4306,4312,4320,4381,4618,5620,6091,6616] <em class="of"># Outliers.</em><br/><br/>train.drop(records_to_drop,inplace=True) <em class="of"># Dropping the outliers.</em><br/>train = train.reset_index(drop=True) <em class="of"># Resetting the indexes.</em></span></pre><h1 id="9835" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">使用Ktrain进行文本预处理</h1><p id="2c86" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">我们将使用ktrain处理推文，将它们转换成可以提供给bert模型的形式，但在此之前，我们将删除推文中的URL和特殊字符。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="9ca0" class="oa lx iq nw b gy ob oc l od oe">def remove_url(text):<br/>    return re.sub(r'https?://\S+|www\.\S+','',text)<br/>def remove_char(text):<br/>    return re.sub(r'[^A-Za-z0-9 ]+', '', text)<br/><br/>def remove_preprocess(text):<br/>    return remove_char(remove_url(text))</span><span id="7d28" class="oa lx iq nw b gy os oc l od oe">train['text'] = train['text'].apply(lambda x: remove_preprocess(x))<br/>test['text'] = test['text'].apply(lambda x: remove_preprocess(x))</span></pre><p id="3103" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在，我们将训练数据分成训练和验证数据集，并使用ktrain对推文进行预处理。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="db88" class="oa lx iq nw b gy ob oc l od oe">train_data = train.head(7000).copy()<br/>val_data = train.tail(525).copy()</span><span id="7cb6" class="oa lx iq nw b gy os oc l od oe">(X_train, y_train), (X_val, y_val), preproc = text.texts_from_df(train_df=train_data,<br/>text_column = 'text',label_columns = 'target',val_df = val_data,maxlen = 256,preprocess_mode = 'bert')</span></pre><p id="014e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，ktrain对数据进行预处理，并为我们提供训练和验证数据集，这些数据集现在可以输入bert模型。</p><h1 id="be1a" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">Ktrain模型和培训</h1><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="a6ea" class="oa lx iq nw b gy ob oc l od oe">model = text.text_classifier(name = 'bert',<br/>                             train_data = (X_train, y_train),<br/>                             preproc = preproc)</span><span id="f808" class="oa lx iq nw b gy os oc l od oe">learner = ktrain.get_learner(model=model, train_data=(X_train,       y_train), val_data = (X_val, y_val), batch_size = 16)</span><span id="51ec" class="oa lx iq nw b gy os oc l od oe">learner.fit_onecycle(lr = 2e-5, epochs = 2)<br/><br/>predictor = ktrain.get_predictor(learner.model, preproc)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/d2215f9a91feaaa2f786e1c829a2db8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7gIPiauLrYgLu1PyzYs_VQ.png"/></div></div><figcaption class="oo op gj gh gi oq or bd b be z dk translated">伯特模型训练</figcaption></figure><p id="cded" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">使用GPU加速器在kaggle中训练模型大约需要15分钟。</p><h1 id="af7d" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">预测和评估</h1><p id="ce82" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj ns ll lm ln nt lp lq lr nu lt lu lv ij bi translated">我们现在将根据测试数据集运行我们的模型。该模型可以直接获取原始数据集，并对其进行处理和预测值。</p><pre class="kp kq kr ks gt nv nw nx ny aw nz bi"><span id="6f7e" class="oa lx iq nw b gy ob oc l od oe">result = pd.DataFrame()<br/>result['id'] = test['id']<br/>result['target'] = predictor.predict(test['text'].values)<br/>result['target'] = result['target'].map(lambda x:1 if x=='target' else 0)<br/>result.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/61ccdd1601029e35945da389255651f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*Qe61CipYzgUx0F26xlHIWQ.png"/></div><figcaption class="oo op gj gh gi oq or bd b be z dk translated">我们模型的预测</figcaption></figure><p id="3f49" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，我们得到了我们的结果，我们可以提交给kaggle，看看我们的表现如何。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/8e096baea0dd2fe52601226234eb35c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M4NL6KCMxddkh_N_BEv35A.png"/></div></div><figcaption class="oo op gj gh gi oq or bd b be z dk translated">我在Kaggle上的提交</figcaption></figure><p id="6444" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，我们得到了0.82991的分数，使我在kaggle排行榜上名列前15%。这个分数可以通过实验模型的超参数的不同值来进一步提高。</p><p id="7b22" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">非常感谢你阅读这篇文章。反馈和掌声是最受欢迎的！！</p></div></div>    
</body>
</html>