<html>
<head>
<title>The Basics of Optimization Algorithms explained in simple words</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用简单的语言解释优化算法的基础</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understanding-optimization-algorithms-309d8065599d?source=collection_archive---------1-----------------------#2021-03-14">https://pub.towardsai.net/understanding-optimization-algorithms-309d8065599d?source=collection_archive---------1-----------------------#2021-03-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0363" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/optimization" rel="noopener ugc nofollow" target="_blank">优化</a></h2><div class=""/><div class=""><h2 id="94f6" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">理解如何在机器学习算法中最小化成本函数的指南</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f8b87355fd1fbfc11f419aa615eea392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vLdFun7AwBfZw9Ph"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://www.pexels.com/@shottrotter" rel="noopener ugc nofollow" target="_blank">汤姆·斯温南</a>在<a class="ae lh" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank">像素</a>上拍摄的照片</figcaption></figure><p id="df67" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">在我攻读数据科学硕士学位期间，我在大多数课程中都遇到过优化人员。起初，我不太理解算法的概念，因为它们用许多数学公式处理，这使我感到更加困惑。然后，我在网上看了一些教程，终于，我能够理解这些优化器背后的含义了。</p><p id="85a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">优化算法在机器学习和深度学习中起着关键作用。没有它，我们无法建立任何模型来进行预测。此外，根据所选择的优化算法，模型将以不同的方式运行。因此，您不仅需要选择能够很好地处理数据的最佳方法，还需要选择算法和该算法的相应超参数。</p><p id="13c6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我想用简单的方式解释基本算法。对于每种算法，我将展示相关的主要方程，我将重点介绍最常用的方法，你们可以在目录中看到。</p><p id="7f36" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">目录:</strong></p><ol class=""><li id="8ec7" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md ms mt mu mv bi translated"><a class="ae lh" href="#67d4" rel="noopener ugc nofollow"> <strong class="lk jd">批量渐变下降</strong> </a></li><li id="5aec" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><a class="ae lh" href="#abeb" rel="noopener ugc nofollow"> <strong class="lk jd">小批量梯度下降</strong> </a></li><li id="c448" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><a class="ae lh" href="#3f94" rel="noopener ugc nofollow"> <strong class="lk jd">随机梯度下降</strong> </a></li><li id="c239" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><a class="ae lh" href="#88ab" rel="noopener ugc nofollow"> <strong class="lk jd"> SGD带动量</strong> </a></li><li id="5bd8" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><a class="ae lh" href="#d16d" rel="noopener ugc nofollow"> <strong class="lk jd">新加坡元带着内斯特罗夫的气势</strong> </a></li><li id="ecb3" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md ms mt mu mv bi translated"><a class="ae lh" href="#58d3" rel="noopener ugc nofollow"> <strong class="lk jd"> RMSprop和Adam </strong> </a></li></ol></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="67d4" class="ni nj it bd nk nl nm nn no np nq nr ns ki nt kj nu kl nv km nw ko nx kp ny nz bi translated">1.批量梯度下降</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/73e473e2e92b3194c3de4c5f9c195bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*77eHJEBWYKrrbBh3bCfYeQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">不同学习率的梯度下降。作者插图</figcaption></figure><p id="6402" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">梯度下降是解决最小化问题的最基本的算法，其对应于成本函数J [1]的最小化。一旦我们定义了成本函数，我们希望通过参数、权重和偏差来最小化它。</p><p id="a30c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如你在下面看到的，你可以推断出这也是一个迭代过程，从一个随机的位置开始，从这里我们想要达到最小值。每次迭代时，参数都会改变。在这个符号中，我使用w来表示参数的值，但是您也可以找到其他符号。</p><p id="7d35" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> w </strong>是指<strong class="lk jd">集合的砝码</strong>。但是还有一些符号也考虑了偏差参数，出于简化的目的，本文没有考虑这些参数。更新规则取决于负梯度的方向</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/a98ac8f3c3fc5db08d1885634ec4938f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phpxB1rnltetywavOXBe0w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">GD更新规则。作者插图</figcaption></figure><p id="ba19" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从步骤k的参数随机配置开始，更新规则由<strong class="lk jd">先前分配</strong>给出，从中减去<strong class="lk jd">学习率</strong>，也称为步长，乘以步骤k的成本函数 J的<strong class="lk jd">梯度。每次更新后，梯度被重新评估为新的权重向量，并且该过程被重复几次。</strong></p><p id="6905" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">梯度下降中的一个重要参数是学习率，它决定了每一步的大小。当学习率过大时，梯度下降可能会跳过山谷，最终到达另一边。所以，太大的步长会导致代价函数的发散。另一方面，如果太小，需要很长时间才能收敛到最小值。因此，我们需要这个参数的平衡，不能太小也不能太大。</p><p id="44ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">注:</strong>这种方法的特点是我们<strong class="lk jd">处理整个训练集</strong>。当训练集中有数百万个示例时，计算开销可能会很大。在这种情况下，梯度下降不再是最小化成本函数的好解决方案。我们需要其他优化者。</p><h1 id="abeb" class="ni nj it bd nk nl oc nn no np od nr ns ki oe kj nu kl of km nw ko og kp ny nz bi translated">2.小批量梯度下降</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/5e019647fdb9b6831a692d4b99933c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dEkAWPYF1mpkZfWOgMOIxA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">批量GD与小批量GD。作者插图</figcaption></figure><p id="f37a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在之前的算法中，我们使用了整个训练集，但在一些有无数例子的情况下，它并不工作得很好。小批量梯度下降的思想是将训练集分成“小批量”，即更小的部分。</p><p id="01bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，假设我们有<strong class="lk jd">6000万个样本</strong>，我们决定每个小批量有<strong class="lk jd"> 100个样本</strong>。然后，训练集的前100个示例将形成第一个小批量，接下来的100个示例将构成第二个小批量，依此类推。我们总共会有<strong class="lk jd"> 600，000个小批量</strong>。</p><p id="cfef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在批量梯度下降中，一次评估N个点。使用小批量梯度下降，单次通过训练集需要<strong class="lk jd">N/小批量</strong> <strong class="lk jd">梯度下降步骤</strong>。在本例中，一次通过将有600，000个GD步骤。当训练集很大时，小批量方法比梯度下降法更快。在上图中，您可以观察到提到的两种方法之间的差异。</p><p id="b41a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在批量梯度下降中，成本函数在每次迭代中降低。而在小批量梯度下降中，成本函数可能不会在每次迭代中降低，并且具有之字形行为。这种之字形移动是由于每个小批量包含不同的示例，其中训练集的一些较小部分可能比其他部分更嘈杂。</p><p id="1f31" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于小批量的大小，需要考虑不同的情况:</p><ul class=""><li id="e6a5" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md oi mt mu mv bi translated">如果<strong class="lk jd"> mini-batch size = N </strong>，我们将使用所有训练集的例子，因此我们使用批量梯度下降。</li><li id="b8be" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md oi mt mu mv bi translated">如果<strong class="lk jd">小批量=1 </strong>，我们以随机梯度下降结束。然后，训练集的每个例子构成一个小批量，导致有N个小批量。</li></ul><p id="3659" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一般来说，最好是小批量，不要太小也不要太大。如果我们有一个巨大的训练集，使用梯度下降更新参数的过程将花费太多时间。另一方面，当有噪声观察时，采用太小的小批量不能很好地工作，并且有时算法不会收敛。典型的小批量是64、128、256和512。您可以观察到，最小批量的大小通常是2的幂。</p><h1 id="3f94" class="ni nj it bd nk nl oc nn no np od nr ns ki oe kj nu kl of km nw ko og kp ny nz bi translated">3.随机梯度下降</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/03ad6ef3023a11561a58de26c392ad66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1GDN-VD4BC2EBW1jHbTdCg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">批量GD与小批量GD与SGD。作者插图</figcaption></figure><p id="3b3d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随机梯度下降的思想是用梯度的随机近似代替梯度下降步骤的梯度[2]。这种随机近似由训练集的单个例子的成本函数的梯度构成。如前所述，如果小批量=1，我们最终得到随机梯度下降。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/35c05be384523a1d03550375d7d7635b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E4eAamK1zYA1K5uG66CrrA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者插图</figcaption></figure><p id="448b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在随机梯度下降的符号中，Ji指的是训练集的单个例子的成本函数。我们希望最小化成本函数J，它是所有实例的总成本。</p><p id="2c79" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用这种方法有一些优点:</p><ul class=""><li id="54f3" class="mn mo it lk b ll lm lo lp lr mp lv mq lz mr md oi mt mu mv bi translated">当训练集中存在冗余信息时，SGD有助于避免进一步的冗余计算。</li><li id="d1b5" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md oi mt mu mv bi translated">随机梯度下降在计算方面是便宜的。它不需要在内存中存储很多值，因为它会立即计算部分梯度，并将其应用到更新规则中，然后在我们删除它之后。</li><li id="944f" class="mn mo it lk b ll mw lo mx lr my lv mz lz na md oi mt mu mv bi translated">SGD行为中的随机噪声有利于摆脱局部极小值并收敛到全局极小值。</li></ul><p id="20c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是也有一些缺点。在随机梯度下降中，我们一般会朝着正确的方向移动，但偶尔甚至会增加误差。正如你从上面的插图中看到的，在路径周围有曲折的行为，这是更少的方向性和更少的规则性。一般来说，这种行为也取决于小批量的大小和学习速度。</p><h1 id="88ab" class="ni nj it bd nk nl oc nn no np od nr ns ki oe kj nu kl of km nw ko og kp ny nz bi translated">4.带动量的SGD</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/acfecdcbfcc7b6a6fea4662c8257ce14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSjVc-c-dlNkC0FohhbSEA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">新币vs有动量的新币。作者插图。</figcaption></figure><p id="5a69" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">动量是优化算法的一个有用技巧。它通常适用于随机梯度下降，但也非常适合梯度下降。在这种情况下，我将重点讨论带动量的随机梯度下降。</p><p id="97f6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如你所知，之前我们只有一个迭代w。现在我们有<strong class="lk jd">两个迭代</strong>、v和w，它们在每一步k都被更新。v更新是通过将<strong class="lk jd">旧v </strong>乘以<strong class="lk jd">常数β</strong>与成本函数的<strong class="lk jd">梯度</strong>相加而获得的。参数β的值在0和1之间，构成少量阻尼。在实践中，beta的典型值是0.9或0.99，这很有效。</p><p id="1c06" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，v就像一个累积的梯度，其中过去的梯度在应用常数β的每一步都减少。如果你看看这张比较新币和有动量的新币的图片，就清楚多了。然后，在这个算法中，我们用v，而不是只用梯度。想法是当动量参数增加时减小步长参数以保持收敛。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/aa124540330d85b0bc5cd27dd518a536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UcTrMDDfsaY-N0BuvVt0JQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者插图</figcaption></figure><p id="fe7b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第二种形式有助于将两步过程合并成一个方程。它被称为<strong class="lk jd">随机重球法</strong>。这个名字的原因是它像一个从山上滚下来的大球。球有动量，所以当它遇到地形变化时不会立即改变方向。表达式的第一部分与SGD相同。在我们加上常数β乘以过去的迭代w和步骤k-1的迭代w之间的差之后。</p><h1 id="d16d" class="ni nj it bd nk nl oc nn no np od nr ns ki oe kj nu kl of km nw ko og kp ny nz bi translated">5.新加坡元与内斯特罗夫的势头</h1><p id="d2de" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">人们很容易将这种势头与内斯特罗夫的势头混淆。它们不是一回事。与常规动量相比，第二个方程有一个小的修正。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/628fd68769701eda9b04d38b372b1589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bEC5zNw5VQYKvWadzujZXQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者插图</figcaption></figure><p id="fe91" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">代替将学习率乘以迭代v，我们将它乘以成本函数的<strong class="lk jd">梯度和参数β的<strong class="lk jd">估计的和</strong>乘以在步骤k+1 </strong>的<strong class="lk jd">迭代。</strong></p><p id="3192" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为常数选择正确的值，内斯特罗夫动量能够在凸问题的情况下加速收敛。但没有理论表明这种加速发生在训练神经网络时，神经网络的特征是没有凸性。</p><h1 id="58d3" class="ni nj it bd nk nl oc nn no np od nr ns ki oe kj nu kl of km nw ko og kp ny nz bi translated">6.RMSprop和Adam</h1><p id="d906" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">RMSprop和Adam被称为<strong class="lk jd">自适应方法</strong>，它们与神经网络配合得非常好。有一些原因导致了更好的表现。两种算法都可以<strong class="lk jd">单独调整每个权重的学习率</strong>，而不是像之前的算法那样使用全局学习率。</p><p id="adb5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">学习率应取决于从每个重量的<strong class="lk jd">梯度获得的信息。为什么全局学习率不能很好地用于神经网络？在卷积神经网络的情况下，有非常不同的操作，例如卷积和最大池。正因为如此，学习率可能在一个层上工作得很好，但在其他层上就不好了。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/9cb427afbe3b4a628fd230e6c951000c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bzjKwLseiVrtH6KbIK0-0A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">RMSprop。作者插图。</figcaption></figure><p id="cc72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> RMSprop </strong>代表均方根。以特定的方式加速梯度下降是有用的。它减缓在<strong class="lk jd">垂直方向</strong>的学习，并加速在<strong class="lk jd">水平方向</strong>的学习。它应用衍生产品平方的指数移动平均值。事实上，在第一个等式中有平方运算和0到1之间的非负参数α。在算法更新参数之后。与前面的方法一样，更新规则由前面的赋值给出，从中我们减去成本函数的梯度除以二阶矩估计值的平方根<strong class="lk jd">加上常数<strong class="lk jd">ε</strong>，这是避免计算中出现问题所需要的(0除以0)。实际上，二阶矩估计值将非常小，很容易发现它非常接近于0。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/6d2ff78a5cb5882682b7c760ee43165a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1EqjmrSP74q_1aSep3KA5g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者插图</figcaption></figure><p id="9951" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Adam是一种算法，它将RMSprop和momentum连接在一起。事实上，它代表自适应矩估计。我们使用指数移动平均线更新<strong class="lk jd">动量</strong>。β作为超参数引入，其值介于0和1之间，作为超参数α。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/1c50d0321cb862bcd3546ee34c38ae37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CNT7AqIWAStRrS-kB2b8fw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者插图</figcaption></figure><p id="b1c0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">beta计算导数的平均值(一阶矩)，alpha计算导数平方的指数加权平均值(二阶矩)。β和α的典型值分别是0.9和0.999，而ε具有非常接近于零的值，例如10^(-8。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="9a29" class="ni nj it bd nk nl nm nn no np nq nr ns ki nt kj nu kl nv km nw ko nx kp ny nz bi translated">最终想法:</h1><p id="aeeb" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">这些是在机器学习和神经模型中最常用的优化算法。还有许多其他的优化器，但是我说的这些是应用最多的主要优化器。</p><p id="b993" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">谢谢你阅读它。大部分解释的东西都是受了吴恩达的《改善深度神经网络》课程和Yann LeCun和Alfredo Canziani的《深度学习》课程的启发。</p><p id="f240" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想深入研究这个话题，请查阅文章底部的参考资料！</p><p id="2fba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">参考文献:</strong></p><p id="0867" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[1] <a class="ae lh" href="https://www.coursera.org/learn/deep-neural-network" rel="noopener ugc nofollow" target="_blank">吴恩达，改进深度神经网络:超参数调整、正则化和优化</a></p><p id="4826" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2]<a class="ae lh" href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-1/" rel="noopener ugc nofollow" target="_blank">https://at cold . github . io/py torch-Deep-Learning/en/week 05/05-1/</a></p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><p id="b649" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">你喜欢我的文章吗？</strong> <a class="ae lh" href="https://eugenia-anello.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="ov">成为会员</em> </strong> </a> <strong class="lk jd"> <em class="ov">每天无限获取数据科学新帖！这是一种间接的支持我的方式，不会给你带来任何额外的费用。如果您已经是会员，</em> </strong> <a class="ae lh" href="https://eugenia-anello.medium.com/subscribe" rel="noopener"> <strong class="lk jd"> <em class="ov">订阅</em> </strong> </a> <strong class="lk jd"> <em class="ov">每当我发布新的数据科学和python指南时，您都可以收到电子邮件！</em> </strong></p></div></div>    
</body>
</html>