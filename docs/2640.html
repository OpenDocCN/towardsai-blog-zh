<html>
<head>
<title>From Lidar to 3D Renders — Waymo and Google Research</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从激光雷达到3D渲染——Waymo和谷歌研究</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/from-lidar-to-3d-renders-waymo-and-google-research-cc72a7a2383d?source=collection_archive---------2-----------------------#2022-03-25">https://pub.towardsai.net/from-lidar-to-3d-renders-waymo-and-google-research-cc72a7a2383d?source=collection_archive---------2-----------------------#2022-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9be2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">结合激光雷达和常规相机进行3D物体探测！</h2></div><blockquote class="ki kj kk"><p id="89c0" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">最初发表于<a class="ae li" href="https://www.louisbouchard.ai/waymo-lidar/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae li" href="https://www.louisbouchard.ai/waymo-lidar/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上读到的！</p></blockquote><h2 id="4cf7" class="lj lk it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">观看视频</h2><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="9084" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">自动驾驶汽车怎么看？</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/28e7d359fc332d2009f631f26d20a19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*SCXMTbzNcGGTE9lB3wbd1A.gif"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">自动驾驶汽车。图片由作者提供。</figcaption></figure><p id="44cd" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">你可能听说过他们使用的<a class="ae li" href="https://en.wikipedia.org/wiki/Lidar" rel="noopener ugc nofollow" target="_blank">激光雷达传感器</a>或其他奇怪的相机。但是他们是如何工作的，他们是如何看待这个世界的，与我们相比，他们到底看到了什么？如果我们想让它们上路，理解它们是如何工作的是至关重要的，主要是如果你在政府部门工作或建立下一个法规。也是这些服务的客户。</p><p id="7030" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">我们之前报道过<a class="ae li" rel="noopener ugc nofollow" target="_blank" href="/tesla-ai-day-in-10-minute-show-does-teslas-autopilot-work-3990252082dc">特斯拉autopilot如何看待和工作</a>，但它们与传统的自动驾驶汽车不同。特斯拉只使用摄像头来了解世界，而大多数像<a class="ae li" href="https://blog.waymo.com/2020/03/introducing-5th-generation-waymo-driver.html" rel="noopener ugc nofollow" target="_blank"> Waymo </a>一样，使用普通摄像头和3D激光雷达传感器。这些激光雷达传感器非常容易理解:它们不会像普通相机那样产生图像，而是3D点云。激光雷达相机测量物体之间的距离，计算脉冲激光投射到物体上的传播时间。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mx"><img src="../Images/e6303ad8487cbc8719f237db4d8e625f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I1glMdaRW8sa_yxtZBHI2g.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">“激光雷达传感器支持距离的3D检测”，<a class="ae li" href="https://www.electronicspecifier.com/products/sensors/lidar-sensor-enables-3d-detection-of-distances" rel="noopener ugc nofollow" target="_blank">电子说明符</a></figcaption></figure><p id="8cc1" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">这样，它们将产生非常少的具有有价值的精确距离信息的数据点，正如您在这里看到的。这些数据点被称为点云，这意味着我们将看到的只是许多位于正确位置的点，创建了某种三维世界模型。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi my"><img src="../Images/645d2dd8d17c8b8eeac87517133a17b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3etsYUbf_vSAqMfMrpm7Ww.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">“激光雷达传感器支持距离的3D检测”，<a class="ae li" href="https://www.electronicspecifier.com/products/sensors/lidar-sensor-enables-3d-detection-of-distances" rel="noopener ugc nofollow" target="_blank">电子说明符</a></figcaption></figure><p id="9eaf" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">在这里，您可以看到右侧的激光雷达并没有精确到可以理解它所看到的东西，但它可以很好地用很少的信息来理解深度，这对于高效地实时计算数据来说是完美的。自动驾驶汽车的基本标准。</p><p id="7b92" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">这种最小量的数据和高空间精度是完美的，因为结合RGB图像，如左图所示，我们既有精确的距离信息，也有单独使用激光雷达数据时缺少的精确的对象信息，特别是来自远处对象或人的信息。这就是为什么Waymo和其他自动驾驶汽车公司使用这两种传感器来了解世界。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi mz"><img src="../Images/d49cee0295616d9264e4ea4b02445529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vAJdt9hwG4b99qcx.png"/></div></a></figure><p id="62b9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">然而，我们如何有效地组合这些信息并让车辆理解它呢？这辆车最后看到了什么？只有到处分？在我们的道路上行驶足够了吗？我们将通过Waymo和Google Research的一篇新研究论文对此进行研究[1]。</p><p id="6024" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">我想我无法用他们在文章中使用的句子来更好地概括这篇论文；</p><blockquote class="ki kj kk"><p id="3fc8" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">“我们提出了4D网络，它学习如何及时结合3D点云和RGB相机图像，以实现3D对象检测在自动驾驶中的广泛应用。”[1]</p></blockquote><p id="632d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">我希望你喜欢这篇文章。请让我知道你是否喜欢这本书……我只是在开玩笑！让我们更深入地研究一下这个句子。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi na"><img src="../Images/ddc9ba23a7664e0d0eb554920568de1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7od2cl2EfQs3LU7-VcV_2A.gif"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">我们合并激光雷达和RGB帧的4D网络架构。图片来自论文[1]。</figcaption></figure><p id="e4c9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">这就是我们说的3D物体检测的样子。这也是这辆车最终会看到的。它非常精确地再现了车辆周围的世界，所有物体都清晰可见。</p><p id="a055" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">那看起来有多酷？而且更有意思的是，他们是怎么得出这个结果的？</p><p id="7abf" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">他们使用激光雷达数据(称为时间点云(PCiT ))和常规相机(这里称为RGB视频)制作了这一视图。这两者都是4维输入，就像我们人类看待和理解世界一样。四维来自及时拍摄的视频，因此车辆可以访问过去的帧，以帮助理解上下文和对象，像我们一样猜测未来的行为，从而创建第四维。另外三个是我们熟悉的3D空间。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/3a19da52cf82b7f4387e4b73ab95dea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*S8XmU5By7M30NlBbHzHQHg.gif"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">场景理解。图片由作者提供。</figcaption></figure><p id="cb70" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">我们称之为任务场景理解，它已经在计算机视觉中得到广泛研究，并随着该领域和机器学习算法的最新进展而取得了许多进展。这在自动驾驶汽车中也至关重要，因为我们希望对场景有近乎完美的理解。</p><p id="31f7" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">如果我们回到上面看到的网络，你可以看到两个网络总是用连接互相“对话”。这主要是因为当我们拍摄图像时，我们在镜头中有不同范围和不同比例的对象。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/e07dbc1f760dba47ada61be55da4a564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YC2YlCLzi4OIwA1gzL01jA.gif"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">不同比例的汽车。图片由作者提供。</figcaption></figure><p id="8fc7" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">前面的车看起来会比远处的车大很多，但还是要两者兼顾。就像我们一样，当我们看到远处的人，觉得他是我们的朋友，但等待靠近以确定后再喊他的名字，汽车会缺乏如此遥远物体的细节。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/af6f8980cf7057c752e5e320b48a7151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*EqKaVJvNi9nUhKz9JD5_nQ.gif"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">图片由作者提供。</figcaption></figure><p id="1873" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">为了弥补这一点，我们将从网络的不同层面提取和共享信息。在整个网络中共享信息是一个强大的解决方案，因为神经网络使用固定大小的小型检测器来压缩图像，我们越深入网络。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/8d0ebe30c089a4e06b2f89266bf87414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*yYa7Cyjtbx7L7ASIa6OqEw.gif"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">早期和深层卷积神经网络中的滤波器示例。图片由作者提供。</figcaption></figure><p id="301e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">这意味着早期的层将能够检测小物体，并且只能检测较大物体的边缘或部分。更深的层将丢失小的物体，但是能够非常精确地检测大的物体。</p><p id="f1af" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">这种方法的主要挑战是通过这些连接将这两种截然不同的信息结合起来；激光雷达3D空间数据和更多常规RGB帧。如前所述，在所有网络步骤中使用这两种信息有助于更好地理解整个场景。</p><p id="a664" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">但是，我们如何合并两个不同的信息流，并有效地使用时间维度呢？这两个分支之间的数据转换是网络在训练期间以监督的方式学习的，其过程类似于我在<a class="ae li" rel="noopener ugc nofollow" target="_blank" href="/will-transformers-replace-cnns-in-computer-vision-55657a196833">以前的文章</a>中介绍的自我关注机制，通过尝试重新创建世界的真实模型。但是为了促进这种数据转换，他们使用了一种称为PointPillars的模型，该模型采用点云并给出二维表示。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nc"><img src="../Images/31245971233da4f5ba94ffae21f22536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qSCZM1PMRx8a1vI-YEjN_w.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">点柱。图片来自论文[1]。</figcaption></figure><p id="10f5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">你可以看到这是点云的伪图像，正如他们所说的，创建了一个代表点云的常规图像，具有与其他分支中的RGB图像相同的属性。像素不是红-绿-蓝颜色，而是简单地表示物体的深度和位置(x，y，z)坐标。这个伪图像也非常稀疏，这意味着这个表示上的信息只在重要对象周围密集，并且最有可能对模型有用。关于时间，我们只需要输入图像中的第四维来跟踪帧。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi na"><img src="../Images/ddc9ba23a7664e0d0eb554920568de1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7od2cl2EfQs3LU7-VcV_2A.gif"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">我们合并激光雷达和RGB帧的4D网络架构。图片来自论文[1]。</figcaption></figure><p id="7ef8" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">我们看到的这两个分支是卷积神经网络，它们对图像进行编码，正如我在<a class="ae li" rel="noopener ugc nofollow" target="_blank" href="/how-ai-generates-new-images-gans-put-simply-674e413bc22a">的多篇文章</a>中所描述的那样，然后对这些编码信息进行解码，以重建我们这里的3D表示。因此，它对两个分支使用非常相似的编码器，彼此共享信息，并使用解码器重建世界的3D模型。</p><p id="c4cc" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">瞧！这就是Waymo车辆如何看待我们的世界，通过这些我们在上面图像右侧看到的世界的3D模型。它可以在164毫秒内处理32个时间点云和16个RGB帧，产生比其他方法更好的结果。这可能没有任何意义，所以我们可以将其与下一个最好的方法进行比较，后者不太准确，需要300毫秒，几乎是处理时间的两倍。</p><p id="0501" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">当然，这是谷歌研究和Waymo的这篇新论文的概述。我建议阅读下面链接的论文，以了解更多关于他们模型的架构和其他我没有深入研究的功能，如时间信息的效率问题。</p><p id="c307" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">我希望你喜欢这篇文章，如果你喜欢，请考虑订阅<a class="ae li" href="https://youtu.be/0nJMnw1Ldks" rel="noopener ugc nofollow" target="_blank">频道</a>来支持我在YouTube上的工作，并评论你对这篇摘要的看法。我很想看看你的想法！</p><p id="8bca" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">感谢您的阅读，下周我将带着另一篇精彩的论文与您见面！</p><h2 id="5e5e" class="lj lk it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">参考</h2><ul class=""><li id="f03b" class="nd ne it ko b kp nf ks ng ls nh lw ni ma nj lh nk nl nm nn bi translated">视频:</li><li id="007b" class="nd ne it ko b kp no ks np ls nq lw nr ma ns lh nk nl nm nn bi translated">Piergiovanni，A.J .，Casser，v .，Ryoo，M.S .和Angelova，a .，2021。<a class="ae li" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Piergiovanni_4D-Net_for_Learned_Multi-Modal_Alignment_ICCV_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank"> 4d-net用于学习多模态比对</a>。IEEE/CVF国际计算机视觉会议论文集(第15435-15445页)。</li><li id="5ae3" class="nd ne it ko b kp no ks np ls nq lw nr ma ns lh nk nl nm nn bi translated">谷歌研究的博文:<a class="ae li" href="https://ai.googleblog.com/2022/02/4d-net-learning-multi-modal-alignment.html?m=1" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2022/02/4d-net-learning-multi-modal-alignment . html？m=1 </a></li></ul></div></div>    
</body>
</html>