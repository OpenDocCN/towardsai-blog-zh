<html>
<head>
<title>Random Walk in Node Embeddings (DeepWalk, node2vec, LINE, and GraphSAGE)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">节点嵌入中的随机行走(DeepWalk、node2vec、LINE和GraphSAGE)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/random-walk-in-node-embeddings-deepwalk-node2vec-line-and-graphsage-ca23df60e493?source=collection_archive---------0-----------------------#2019-12-24">https://pub.towardsai.net/random-walk-in-node-embeddings-deepwalk-node2vec-line-and-graphsage-ca23df60e493?source=collection_archive---------0-----------------------#2019-12-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e546" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">图形嵌入</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c886f5b852c8b83438d91b2177facd3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JyWevKTOEz5e0Sw9"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">史蒂文·魏在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><blockquote class="kz la lb"><p id="ebe1" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以考虑使用图形神经网络(GNN)来执行节点分类问题，而不是使用传统的机器学习分类任务。通过提供节点的显式链接，该分类问题不再被分类为独立的问题，而是利用了诸如节点度的图结构。图形属性的有用性假设单个节点与其他相似节点相关。</p><p id="9d16" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">典型的例子是社交媒体网络。想象一下，脸书是如何根据你喜欢的帖子、你在哪里签到等将你和其他人联系起来的。一个图表能够代表这种关系，我们可以利用它来训练GNN。GNN的详细用例将在后面的故事中介绍。</p></blockquote><p id="3c3b" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在最后一个<a class="ae ky" href="https://medium.com/towards-artificial-intelligence/a-gentle-introduction-to-graph-embeddings-c7b3d1db0fa8" rel="noopener">故事</a>中，我们经历了知识图嵌入。它涵盖了一个通过TransE训练图嵌入的框架，并且是大规模的复杂。在这个故事中，我们将讨论图结构和基于随机游走的学习图嵌入的模型。以下部分涵盖了<a class="ae ky" href="https://arxiv.org/pdf/1403.6652.pdf" rel="noopener ugc nofollow" target="_blank"> DeepWalk </a> (Perozzi等人，2014)<a class="ae ky" href="https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf" rel="noopener ugc nofollow" target="_blank">node 2 vec</a>(Grover和Leskovec，2016)<a class="ae ky" href="https://arxiv.org/pdf/1503.03578.pdf" rel="noopener ugc nofollow" target="_blank">LINE</a>(Tang等人，2015)<a class="ae ky" href="https://arxiv.org/pdf/1706.02216.pdf" rel="noopener ugc nofollow" target="_blank">graph sage</a>(Hamilton等人，2018)。</p><h1 id="974d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">图形结构</h1><p id="be1f" class="pw-post-body-paragraph lc ld it lf b lg mu ju li lj mv jx ll lz mw lo lp ma mx ls lt mb my lw lx ly im bi translated">在ego通过那些基于随机行走的模型之前，我们需要理解一些基本的图结构。</p><ul class=""><li id="f7e5" class="mz na it lf b lg lh lj lk lz nb ma nc mb nd ly ne nf ng nh bi translated">一阶邻近性:称为同向性。如果两个节点相连，则它们是同向的或具有一阶邻近性。</li><li id="fe72" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">二阶接近度:如果两个节点共享许多连接，则它们具有更高的二阶接近度。即下图1中的节点5和节点6。尽管节点5和节点6没有直接相连，但它们共享完全相同的邻居节点。这表明它们有一定程度的相似性。另一方面，它有助于收敛那些较少的邻居节点。</li><li id="a492" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">结构对等:指相似的结构角色。例如，如果两个节点连接到三个不同的节点，则两个节点是结构等价的。即下图2中的节点u和节点s6。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/1b015a86e7645a85db15bf8b1f104b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*Qhd7AWWWnn-b4fEKHzR_mg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图1(唐等，2015)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/567bfefc1a945340d64b0fa4b1e9a8ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*Z6OrZF-UpmUxgY5oS_roTw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图2(格罗弗和莱斯科维奇，2016年)</figcaption></figure><h1 id="6112" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">DeekWalk</h1><p id="a654" class="pw-post-body-paragraph lc ld it lf b lg mu ju li lj mv jx ll lz mw lo lp ma mx ls lt mb my lw lx ly im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1403.6652.pdf" rel="noopener ugc nofollow" target="_blank">引入DeepWalk </a> (Perozzi等人，2014)通过随机行走和word2vec (Mikolo等人，2013) <a class="ae ky" href="https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a" rel="noopener" target="_blank"> word2vec </a>算法学习节点嵌入。在自然语言处理(NLP)中，我们可以应用跳格模型和输入一个句子(一系列单词)来训练单词嵌入。简而言之，训练目标是使用中心单词来预测周围的单词。你可以查看这个<a class="ae ky" href="https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a" rel="noopener" target="_blank">故事</a>的细节。</p><p id="2058" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">回到DeepWalk，它随机选取一个节点，然后随机“行走”到一个邻居节点，直到它达到最大长度(或某个随机长度)。给定下面的知识图(上半部分)，我们分别选取“A”和“F”作为起点。稍后，我们将节点“A”移动到节点“C ”,依此类推。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/0406de36def21af8e4dd0df307493288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*xXKTpDyha6yaRz9CGNzM6Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">上图:知识图。下图:随机漫步序列</figcaption></figure><p id="5177" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在这种情况下，我们将节点和一系列节点分别视为“单词”和“句子”。稍后，我们可以重用skip-gram算法来收敛节点嵌入。</p><h1 id="db88" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">node2vec</h1><p id="2e04" class="pw-post-body-paragraph lc ld it lf b lg mu ju li lj mv jx ll lz mw lo lp ma mx ls lt mb my lw lx ly im bi translated"><a class="ae ky" href="https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf" rel="noopener ugc nofollow" target="_blank"> node2vec </a> (Grover和Leskovec，2015)是DeepWalk (Perozzi等人，2014)的高级版本。DeepWalk (Perozzi et al .，2014)的一个局限是你无法控制路径。node2vec (Grover和Leskovec，2015)也使用随机行为，但有权重。</p><p id="5377" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">作者引入宽度快速采样(BFS)和深度优先采样(DFS)来控制随机行为，而不是随机“行走”。BFS到达直接邻居，而DFS更喜欢远离源的节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/567bfefc1a945340d64b0fa4b1e9a8ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*Z6OrZF-UpmUxgY5oS_roTw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">BFS:广度优先抽样。DFS:深度优先采样(Grover和Leskovec，2016年)</figcaption></figure><p id="e2d8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">为了提供这种灵活性，随机游走概率不再是不加权的，而它可以通过以下设置来实现。dtx表示到源的距离，而p和q是参数。较大的p确保对被访问节点进行采样的机会较小。q允许我们控制我们想要BFS还是DFS。较大的q倾向于围绕中心随机游走。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/751121bdda2517af2f5ccce9026b7cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*zkbK7YhEVV-GXVODeYCMCQ.png"/></div></figure><h1 id="220a" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">线条</h1><p id="483d" class="pw-post-body-paragraph lc ld it lf b lg mu ju li lj mv jx ll lz mw lo lp ma mx ls lt mb my lw lx ly im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1503.03578.pdf" rel="noopener ugc nofollow" target="_blank"> LINE </a>(唐等，2015)，又名LINE大规模信息网络嵌入，提出利用一阶近似和二阶近似来学习节点嵌入。</p><p id="2704" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">为了最小化一阶邻近目标，连接的节点将在嵌入空间中彼此收敛。注意，它只适用于无向图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/6ecdf69eafa76fc2bc103a11ca1035a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*iEnO0SP2lY9lpNIDTAzFXA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">一阶邻近目标(唐等人，2015年)</figcaption></figure><p id="ee59" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">同上，我们需要最小化共享相似邻居节点的收敛节点的二阶邻近目标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6a5bcba234c08bd48d91bfc09695bd48.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*X0quOlxpblcTj_bLtIP9nQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">二阶邻近目标(唐等，2015)</figcaption></figure><h1 id="3628" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">图表法</h1><p id="74c4" class="pw-post-body-paragraph lc ld it lf b lg mu ju li lj mv jx ll lz mw lo lp ma mx ls lt mb my lw lx ly im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1706.02216.pdf" rel="noopener ugc nofollow" target="_blank"> GraphSAGE </a> (Hamilton et al .，2018)，又名Graph SAmple and aggreGatE，。是一个动态生成节点嵌入的模型。与其他模型不同，它不训练特定的节点嵌入，而是训练一个聚合器。由于这种性质，它可以处理看不见的节点。</p><p id="443c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">该方法包括采样邻居节点和聚集邻居节点两个主要步骤。下图说明了该步骤:</p><ol class=""><li id="b615" class="mz na it lf b lg lh lj lk lz nb ma nc mb nd ly nt nf ng nh bi translated">选择一个节点(即该图的中心)。在每层中挑选一些邻居节点(即k=1，k=2)。</li><li id="230c" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly nt nf ng nh bi translated">聚合每层(蓝色层和绿色层)中的节点</li><li id="7bd6" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly nt nf ng nh bi translated">向神经网络输入嵌入信息并进行预测</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/9e674c232a4131655a8ef5536e7837a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vf-BMl8tQ-je4wv5ED1ROQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">GraphSAGE样本和集合方法(Hamilton等人，2018年)</figcaption></figure><p id="854b" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如前所述，在每个小批量中，它只采样一些邻居节点进行训练。使用随机行走方法，并且它是不加权的(与深度行走相同)。</p><p id="a84c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">但是我们如何聚合它呢？作者提出了均值聚合、LSTM聚合和汇集聚合四种方法。</p><ul class=""><li id="8da6" class="mz na it lf b lg lh lj lk lz nb ma nc mb nd ly ne nf ng nh bi translated">均值聚合器:取邻居的平均值并与目标节点连接。</li><li id="089d" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">GCN聚集器:连接邻居和节点后取平均值。与均值聚合器相比，顺序正好相反。实践证明，该方法取得了较好的效果。</li><li id="65bf" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">LSTM聚合器:随机排列邻居订单并馈入LSTM层。</li><li id="2702" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">汇集聚合器:提供给多层感知器(MLP)并执行最大汇集。</li></ul><h1 id="6c9d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">拿走</h1><ul class=""><li id="c1dc" class="mz na it lf b lg mu lj mv lz nv ma nw mb nx ly ne nf ng nh bi translated">DeepWalk的想法很简单，但有一些限制。由于这是一个纯粹的随机行走(未加权)，频率或重要性不能影响嵌入和说明图的结构。因为它需要“行走”来从邻居那里获得信息，所以很难支持看不见的数据。</li><li id="26fb" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">node2vec解决了DeepWalk的一些限制，例如未加权问题。然而，它可能遭受更高的计算要求。</li><li id="4b4e" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">DeepWalk和LINE分别对所有节点使用深度优先搜索(DFS)策略，对低度节点使用广度优先搜索(BFS)策略。另一方面，node2vec混合了BFS和DFS来学习节点嵌入。</li></ul><h1 id="59ec" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">延伸阅读</h1><ul class=""><li id="d9e9" class="mz na it lf b lg mu lj mv lz nv ma nw mb nx ly ne nf ng nh bi translated"><a class="ae ky" href="https://github.com/phanein/deepwalk" rel="noopener ugc nofollow" target="_blank"> DeepWalk </a>仓库</li><li id="117e" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated"><a class="ae ky" href="https://github.com/aditya-grover/node2vec" rel="noopener ugc nofollow" target="_blank"> node2vec </a>储存库</li><li id="083f" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated"><a class="ae ky" href="https://github.com/williamleif/GraphSAGE" rel="noopener ugc nofollow" target="_blank"> GraphSAGE </a>存储库</li><li id="c196" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">PyTorch周正，<a class="ae ky" href="https://arxiv.org/abs/1903.02428" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1903.02428</a></li></ul><h1 id="43f0" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">关于我</h1><p id="cb33" class="pw-post-body-paragraph lc ld it lf b lg mu ju li lj mv jx ll lz mw lo lp ma mx ls lt mb my lw lx ly im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。欢迎在<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与<a class="ae ky" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>联系，或者在<a class="ae ky" href="https://medium.com/@makcedward/" rel="noopener"> Medium </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="0f6b" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><ul class=""><li id="c68b" class="mz na it lf b lg mu lj mv lz nv ma nw mb nx ly ne nf ng nh bi translated">B.佩罗齐，R. A. Rfou和S. Skiena。<a class="ae ky" href="https://arxiv.org/pdf/1403.6652.pdf" rel="noopener ugc nofollow" target="_blank">深度行走:在线学习社交表征</a>。2014</li><li id="a565" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">J.唐、瞿、王、张、严军、梅青。<a class="ae ky" href="https://arxiv.org/pdf/1503.03578.pdf" rel="noopener ugc nofollow" target="_blank">线:大规模信息网络嵌入</a>。2015</li><li id="3d8a" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">A.格罗弗和j .莱斯科维奇。<a class="ae ky" href="https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf" rel="noopener ugc nofollow" target="_blank"> node2vec:网络的可扩展特征学习</a>。2016</li><li id="63a5" class="mz na it lf b lg ni lj nj lz nk ma nl mb nm ly ne nf ng nh bi translated">W.l .汉密尔顿、r .英和j .莱斯科维奇。<a class="ae ky" href="https://arxiv.org/pdf/1706.02216.pdf" rel="noopener ugc nofollow" target="_blank">大型图上的归纳表示学习</a>。2018</li></ul></div></div>    
</body>
</html>