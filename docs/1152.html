<html>
<head>
<title>Logistic Regression with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PyTorch进行逻辑回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/logistic-regression-with-pytorch-198a4ec80649?source=collection_archive---------0-----------------------#2020-11-17">https://pub.towardsai.net/logistic-regression-with-pytorch-198a4ec80649?source=collection_archive---------0-----------------------#2020-11-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="da5a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/64052be0a9b4536bfa631a6787b83688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*voRVprJVG8I1nOAv_vd0jQ.jpeg"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">来源:<a class="ae kl" href="https://www.pexels.com/photo/photo-of-road-during-dawn-3588998/" rel="noopener ugc nofollow" target="_blank"> Pexels </a></figcaption></figure><p id="1986" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在上一篇文章中，我们学习了线性回归，现在我们来学习逻辑回归。</p><p id="18ef" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因为我们不能对分类任务使用线性回归，所以我们使用逻辑，它是对分类任务的线性回归的扩展。</p><p id="eb43" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于需要概率作为输出的问题，逻辑回归是一种很好的方法。</p><p id="7996" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有两种方法可以使用输出，</p><p id="b8c2" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">1]你可以把它们转换成二进制，2]你可以直接使用概率。</p><p id="a786" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们看看两个例子，<strong class="ko ja"> 1】我们如何使用概率，因为它是，</strong></p><p id="06ac" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设我们创建一个模型来预测一名前锋在客场比赛时的概率。我们称之为概率:</p><p id="c979" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> p(比分|客场_比赛)</strong></p><p id="b75f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果一个模型预测一个<strong class="ko ja"> p(score | Away_Match) </strong>为<em class="lk"> 0.05 </em>，那么总的客场比赛，前锋将会打进大约<em class="lk"> 1 </em>个进球。</p><p id="7484" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">goal =<strong class="ko ja">p(score | Away _ Match)</strong>* Total _ Away _ Match:</p><p id="946b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk"> 0.05 * 19 = 0.95 ~ 1目标。</em></p><p id="cc7d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2]现在让我们看看二进制分类的第二种方式，</p><p id="42a9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">数据集</p><p id="7b63" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这里，我们的任务是根据白细胞(白血球)计数和血压来确定肿瘤是良性(无害)还是恶性(有害)。</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="1baf" class="lu lv iq lq b gy lw lx l ly lz">import pandas as pd<br/>data = pd.read_csv('tumor.csv')<br/>data.head()</span></pre><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/d581a7c47a115d62663d2c77e25ff254.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*ngBjSSkR4Ui9BE8bYtNx-w.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">数据主管</figcaption></figure><p id="4ec3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们定义X和y，</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="258a" class="lu lv iq lq b gy lw lx l ly lz">X = data[['leukocyte_count', 'blood_pressure']].values<br/>y = data['tumor_class'].values</span></pre><p id="6220" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们绘制数据。</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="d13e" class="lu lv iq lq b gy lw lx l ly lz">import matplotlib.pyplot as plt<br/>colors = {'benign' : 'orange', 'malignant' : 'green'}<br/>plt.scatter(data['leukocyte_count'], data['blood_pressure'], c = [colors[item] for item in y], edgecolors = 'k')<br/>plt.xlabel('leukocyte_count')<br/>plt.ylabel('blood_pressure')<br/>plt.legend(['malignant', 'benign'], loc = 'upper right')<br/>plt.show()</span></pre><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/52bd7cf2bf2c760e381f2c714ddf2ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*0FL05MFKftT0E3nshQfNfA.png"/></div></figure><p id="eeba" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">拆分数据</strong></p><p id="960f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们现在将数据随机分成3组:训练、验证和测试。</p><ul class=""><li id="98ef" class="mc md iq ko b kp kq kt ku kx me lb mf lf mg lj mh mi mj mk bi translated">Train:用于训练我们的模型。</li><li id="cfa6" class="mc md iq ko b kp ml kt mm kx mn lb mo lf mp lj mh mi mj mk bi translated">Val:用于在培训期间验证我们模型的性能。</li><li id="3625" class="mc md iq ko b kp ml kt mm kx mn lb mo lf mp lj mh mi mj mk bi translated">测试:用于评估我们完全训练好的模型。</li></ul><p id="b073" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在分割数据时，我们将使用分层参数。当我们有一个分类任务时，那么我们必须确保每个分裂都有相同的类别分布(这有助于我们很好地预测类别)</p><p id="6226" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，我们的变量y是一个二元分类变量，值为“良性”和“恶性”，有38%的“良性”和61%的“恶性”，分层= y将确保您的随机分割有38%的“良性”和61%的“恶性”。</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="b3ed" class="lu lv iq lq b gy lw lx l ly lz"># Importing library<br/>from sklearn.model_selection import train_test_split</span><span id="0ca3" class="lu lv iq lq b gy mq lx l ly lz"># Defining sets<br/>TRAIN_SIZE = 0.7<br/>VAL_SIZE = 0.15<br/>TEST_SIZE = 0.15<br/>SHUFFLE = True</span><span id="149b" class="lu lv iq lq b gy mq lx l ly lz"># Creating a function <br/>def split(X, y, val_size, test_size, shuffle):<br/>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, stratify = y, shuffle = shuffle)<br/>    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = val_size, stratify = y_train, shuffle = shuffle)<br/>    return X_train, X_val, X_test, y_train, y_val, y_test</span><span id="e277" class="lu lv iq lq b gy mq lx l ly lz">X_train, X_val, X_test, y_train, y_val, y_test = split(X, y, VAL_SIZE, TEST_SIZE, SHUFFLE)</span></pre><p id="6791" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以看到，我们的类标签是文本，我们需要做点什么，万岁！sklearn让我们…</p><p id="d288" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在此之前，我们只处理数字数据，但现在我们在文本中有了标签。</p><p id="1245" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，我们需要将它转换成数字形式。为了将标签转换成数字形式，在sklearn中有一个叫做“LabelEncoder”的方法。我们将应用这一点。</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="d76d" class="lu lv iq lq b gy lw lx l ly lz">from sklearn.preprocessing import LabelEncoder<br/>le = LabelEncoder()</span><span id="f376" class="lu lv iq lq b gy mq lx l ly lz"># fitting on train data<br/>y_le = le.fit(y_train)<br/>classes = y_le.classes_</span></pre><p id="6af3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将把标签转换成令牌</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="0e8a" class="lu lv iq lq b gy lw lx l ly lz"># Convert labels to tokens<br/><strong class="lq ja"># Before transforming - y_train[0] - 'malignant'</strong><br/>print(f'y_train[0] : {y_train[0]}')<br/>y_train = y_le.transform(y_train)<br/>y_val = y_le.transform(y_val)<br/>y_test = y_le.transform(y_test)</span><span id="b824" class="lu lv iq lq b gy mq lx l ly lz"><strong class="lq ja"># After transforming - y_train[0] - 1</strong><br/>print(f'y_train[0] : {y_train[0]}')<br/></span></pre><p id="1773" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">标准化数据</strong></p><p id="b4b9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们需要标准化我们的数据(零均值和单位方差)</p><p id="ddef" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> z(标准化值)</strong>=<em class="lk"/>-<em class="lk">μ</em>/<em class="lk">σ</em></p><ul class=""><li id="3b15" class="mc md iq ko b kp kq kt ku kx me lb mf lf mg lj mh mi mj mk bi translated"><em class="lk"> xi </em> =输入</li><li id="a24d" class="mc md iq ko b kp ml kt mm kx mn lb mo lf mp lj mh mi mj mk bi translated"><em class="lk"> μ </em> =平均值</li><li id="b01e" class="mc md iq ko b kp ml kt mm kx mn lb mo lf mp lj mh mi mj mk bi translated"><em class="lk"> σ </em> =标准差</li></ul><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="c90b" class="lu lv iq lq b gy lw lx l ly lz">from sklearn.preprocessing import StandardScaler<br/>ss = StandardScaler().fit(X_train)</span></pre><p id="a804" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将把拟合的数据应用于训练和测试数据</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="dcfa" class="lu lv iq lq b gy lw lx l ly lz">X_train = ss.transform(X_train)<br/>X_val = ss.transform(X_val)<br/>X_test = ss.transform(X_test)</span></pre><p id="918d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将定义输入维度和输出维度</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="17a7" class="lu lv iq lq b gy lw lx l ly lz">INPUT_DIM = X_train.shape[1]<br/>NUM_CLASSES = len(classes)</span></pre><p id="d896" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将使用torch.nn.functional来定义一个自定义层</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="46d1" class="lu lv iq lq b gy lw lx l ly lz">import torch.nn.functional as F<br/>from torch import nn<br/>class LogisticRegression(nn.Module):<br/>    def __init__(self, input_dim, num_classes):<br/>        super(LogisticRegression, self).__init__()<br/>        self.fc1 = nn.Linear(input_dim, num_classes)<br/>        <br/>    def forward(self, x_in, apply_softmax = False):<br/>        y_pred = self.fc1(x_in)<br/>        if apply_softmax:<br/>            y_pred = F.softmax(y_pred, dim = 1)<br/>        return y_pred</span></pre><p id="2ac7" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">就像我们对线性回归所做的那样，我们创建一个线性层，然后在其上应用softmax(这是我们用nn.functional制作的)。</p><p id="0482" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> nn.module </strong> —当我们想要创建一个定制模型时，我们使用它。这意味着我们正在以我们的方式定义我们的模型。它是用输入和输出维度初始化的，这是我们在上面定义的。<br/> <strong class="ko ja"> nn。线性</strong> —对我们的输入和输出样本应用线性变换。<br/>是全连接层，有输入有输出。现在，它的输入会有一个权重。它的输出将是<em class="lk"/>【XW+b】，因为它完全是线性的，没有激活函数。</p><p id="d6ad" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">f . soft max</strong>——<a class="ae kl" href="https://pytorch.org/docs/stable/nn.functional.html?highlight=softm#torch.nn.functional.softmax" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/nn.functional.html?highlight = softm # torch . nn . functional . soft max</a></p><p id="5e5c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将初始化模型。</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="cff4" class="lu lv iq lq b gy lw lx l ly lz">model = LogisticRegression(input_dim = INPUT_DIM, num_classes = NUM_CLASSES)</span></pre><p id="0175" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">损失函数</p><p id="7115" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">LR的损失函数是对数损失:</p><ul class=""><li id="0e44" class="mc md iq ko b kp kq kt ku kx me lb mf lf mg lj mh mi mj mk bi translated"><strong class="ko ja"> -ylog(y') </strong></li><li id="bed5" class="mc md iq ko b kp ml kt mm kx mn lb mo lf mp lj mh mi mj mk bi translated">y '-预测值</li><li id="f083" class="mc md iq ko b kp ml kt mm kx mn lb mo lf mp lj mh mi mj mk bi translated">y-实际值</li></ul><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="deb1" class="lu lv iq lq b gy lw lx l ly lz">loss_fn = nn.CrossEntropyLoss()</span></pre><p id="eef6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">韵律学</p><p id="ecfd" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这里，我使用准确性作为我的衡量标准</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="6810" class="lu lv iq lq b gy lw lx l ly lz">def accuracy_fn(y_pred, y_true):<br/>    n_correct = torch.eq(y_pred, y_true).sum().item()<br/>    accuracy = (n_correct / len(y_pred)) * 100<br/>    return accuracy</span></pre><p id="a0b3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">优化器</strong></p><p id="0a04" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们在上面看到了<strong class="ko ja">损失函数</strong>(这是一种衡量你离目的地有多远的方法)。考虑到这一点，现在让我们了解一下什么是优化器？</p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mr"><img src="../Images/2a03be11fa26ecb2edd1bc8d66ac2ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmIyx0jYMXXGkWHwRjk3AA.jpeg"/></div></div></figure><p id="f69e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们以陶器为例。这是一个用粘土和其他陶瓷材料制成容器的过程，这些材料在高温下烧制，使它们坚硬耐用。</p><p id="126e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">就像这个过程，在训练我们的模型时，这里陶工是我们的优化器，粘土和陶瓷材料是参数(权重)和损失函数，使模型更准确或更无误差。</p><p id="a240" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，罐子有不同的形状，所以陶工会试着把罐子做成想要的形状，就像陶工一样。优化器将尝试获得正确的权重形状。</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="b027" class="lu lv iq lq b gy lw lx l ly lz">learning_rate = 1e-1<br/>optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)</span></pre><p id="956d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里，我们使用ADAM优化器；它使用自适应学习率。我们不必像在学习率调度器中那样手动调整学习率。</p><p id="f994" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有时优化者往往会超越目的地，亚当试图通过在开始时采取较大的步骤，在到达目的地时采取较小的步骤来克服它。</p><p id="6cff" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们需要将数据转换成张量。为此，我们使用<strong class="ko ja">火炬。张量</strong>。</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="6549" class="lu lv iq lq b gy lw lx l ly lz"># Convert data to tensors<br/>X_train = torch.Tensor(X_train)<br/>y_train = torch.LongTensor(y_train)<br/>X_val = torch.Tensor(X_val)<br/>y_val = torch.LongTensor(y_val)<br/>X_test = torch.Tensor(X_test)<br/>y_test = torch.LongTensor(y_test)</span></pre><p id="95e1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们训练我们的模型</p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="33d8" class="lu lv iq lq b gy lw lx l ly lz">EPOCHS = 100<br/>for epoch in range(EPOCHS):<br/>    y_pred = model(X_train)<br/>    loss = loss_fn(y_pred, y_train)<br/>    optimizer.zero_grad()<br/>    loss.backward()<br/>    optimizer.step()<br/>    if epoch%10==0: <br/>        predictions = y_pred.max(dim=1)[1] # class<br/>        accuracy = accuracy_fn(y_pred=predictions, y_true=y_train)<br/>        print (f"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}")</span></pre><p id="3180" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">精度</strong></p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="aaa3" class="lu lv iq lq b gy lw lx l ly lz"># Accuracy<br/>train_acc = accuracy_score(y_train, pred_train)<br/>test_acc = accuracy_score(y_test, pred_test)<br/>print (f"train acc: {train_acc:.2f}, test acc: {test_acc:.2f}")</span></pre><p id="4fb3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">想象边界</p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ms"><img src="../Images/86eaecaf162621f36127144caed55b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Ob3_OSRnx_ET8v-UMazwg.png"/></div></div></figure><p id="960e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">分类报告</p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/d512661d7e552ba86569891fd1e6f3b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*w-k3I3rzKkAZBeentYtdcw.png"/></div></figure><p id="6fe4" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正如我们所看到的，分类报告中有如此多的新术语，让我们一个一个地了解它们。</p><p id="f28a" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">绩效评估</strong></p><p id="23e1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">1]准确性:正确分类的点数/一组点数</p><p id="f8ca" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们举个例子:</p><p id="cfea" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设我们有100分，其中60分是正分，40分是负分。</p><p id="1480" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，在这100个点中，我们的模型正确地分类了88个点(53个来自正面类，35个来自负面类)，并且不正确的分类，即错误，是12个点。</p><p id="1f0b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以我们模型的准确率是88%，误差是12 %。</p><ul class=""><li id="2da6" class="mc md iq ko b kp kq kt ku kx me lb mf lf mg lj mh mi mj mk bi translated">这个指标很容易理解</li><li id="4fd4" class="mc md iq ko b kp ml kt mm kx mn lb mo lf mp lj mh mi mj mk bi translated">它在不平衡数据上表现不佳。</li></ul><p id="9546" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">混乱矩阵</strong></p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mu"><img src="../Images/4179b94ddc11eb3226eb84021001bfd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ILwTTOboBNjPJr4m49TbzA.jpeg"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">混淆矩阵</figcaption></figure><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mu"><img src="../Images/7b0b937cb9d584d26e056358c584a05e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JNhEeBZT1H4M3S3fWkXlAw.jpeg"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">混淆矩阵</figcaption></figure><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mv"><img src="../Images/6b16c4840ddf64f1bfcfd5a7b89aa83d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArDX2-LCgfBWKiOpKcbMWQ.jpeg"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">混淆矩阵</figcaption></figure><p id="b5ac" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">精度和召回</strong></p><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mw"><img src="../Images/24f48c9d8d0da0b3aa86b0ae0ac9955f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmwUw299GxY6fDC3lFZ_Sg.jpeg"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">精确度和召回率</figcaption></figure><p id="6c72" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">推论</strong></p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="c7a7" class="lu lv iq lq b gy lw lx l ly lz">X1 = pd.DataFrame([{'leukocyte_count': 13, 'blood_pressure': 16}])<br/># Standardize<br/>X1 = ss.transform(X1)</span></pre><p id="7114" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">预测</strong></p><pre class="ll lm ln lo gt lp lq lr ls aw lt bi"><span id="3605" class="lu lv iq lq b gy lw lx l ly lz"># Predict<br/>y1 = model(torch.Tensor(X), apply_softmax=True)<br/>prob, _class = y.max(dim=1)<br/>print (f"You have {classes[_class.detach().numpy()[0]]} tumor with {prob.detach().numpy()[0]*100.0:.0f}% of probability")</span></pre><figure class="ll lm ln lo gt ka gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/5bc1c41a728af22a5602636d7f5200ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*ER4rEFt4SAJR2PSrKxWAHg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">推理的结果</figcaption></figure><p id="c704" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">代码</strong>-<a class="ae kl" href="https://github.com/pratikraut1/Logistic-regression-with-pytorch" rel="noopener ugc nofollow" target="_blank">https://github . com/prati kraut 1/Logistic-regression-with-py torch</a></p><p id="298b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">免责声明</strong>-标题中无链接的图片为作者所有；否则，会提供一个链接。</p></div></div>    
</body>
</html>