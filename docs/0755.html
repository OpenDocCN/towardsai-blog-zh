<html>
<head>
<title>Create Your Own Harry Potter Short Story Using RNNs and TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用RNNs和TensorFlow创建您自己的哈利波特短篇故事</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/create-your-own-harry-potter-short-story-using-rnn-and-tensorflow-853b3ed1b8f3?source=collection_archive---------1-----------------------#2020-08-03">https://pub.towardsai.net/create-your-own-harry-potter-short-story-using-rnn-and-tensorflow-853b3ed1b8f3?source=collection_archive---------1-----------------------#2020-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="99eb" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/7ab5308a4fe7bb4d6299a56f26371c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f8maLoW3VWeX0AjC"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@rae_1991?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Rae Tian </a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="4fc8" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">“当然，它是在你脑子里发生的，哈利，但这究竟为什么意味着它不是真实的呢？”</h1></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="d3c1" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">还在等你的霍格沃茨来信吗？想在大会堂享受盛宴吗？<br/>探索霍格沃茨的秘密通道？从奥利凡德商店买你的第一根魔杖？<br/>*叹*你不是一个人。</p><p id="f3b8" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">我已经(<em class="ms">过了这么久？</em> ) <em class="ms">一直</em>痴迷<a class="ae ko" href="https://harrypotter.bloomsbury.com/uk/" rel="noopener ugc nofollow" target="_blank">哈利波特</a>，最近开始学习神经网络。看到深度学习能让你变得多么有创造力是很有趣的，所以我想为什么不<em class="ms">酝酿</em>它们呢？</p><p id="6bd6" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">所以我使用TensorFlow执行了一个简单的文本生成模型来创建我自己版本的哈利波特短篇小说(不能像J.K .罗琳那样好，咄！)</p><p id="af31" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">本文向您展示了我为实现它而编写的全部代码。<br/>但是对于所有的赫敏，你可以直接在这里找到github代码<a class="ae ko" href="https://github.com/amisha-jodhani/text-generator-harry-potter" rel="noopener ugc nofollow" target="_blank">并自己运行它！</a></p><p id="84ee" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">所以这里有一些东西会在你被隔离期间给你的无聊蒙上一层【T21驱逐咒】。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="b306" class="kp kq it bd kr ks mt ku kv kw mu ky kz la mv lc ld le mw lg lh li mx lk ll lm bi translated">背景</h1><h2 id="6241" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">什么是RNN？</h2><p id="2f50" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">递归神经网络不同于其他神经网络，因为它有一个<strong class="lw jd">存储器</strong>，该存储器存储到目前为止它已经处理的所有层的信息，并根据该存储器计算下一层。关于RNNs的简单介绍，可以参考<a class="ae ko" href="https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><h2 id="11cf" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">GRU对LSTM</h2><p id="ec36" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">这两者对文本生成都很好，但是GRUs是一个较新的概念…而且实际上没有办法确定哪一个更好。<strong class="lw jd">调整好你的超参数</strong>比选择一个好的架构更能提高你的模型性能。</p><p id="ad7b" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">但是，如果数据量不成问题，LSTMs的性能会更好。如果你有较少的数据，gru就有较少的参数，所以它们训练得更快，并能很好地概括较少的数据。</p><p id="7b64" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">请随意查看这篇<a class="ae ko" href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" rel="noopener ugc nofollow" target="_blank">文章</a>以获得更详细的解释。</p><h2 id="5f78" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">为什么基于字符？</h2><p id="0ff3" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">当处理像这样的大型数据集时，<strong class="lw jd">语料库中唯一单词的完整数量远远高于唯一字符的数量</strong>。一个大的数据集将会有很多很多独特的单词，当我们给这样大的矩阵分配一次性编码时，我们很可能会遇到内存问题。光是我们的标签就可以占用数兆兆字节的内存。</p><p id="2d99" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">所以，你用来预测单词的相同原则也可以应用到这里，但是现在你将使用更小的词汇量。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="b3f4" class="kp kq it bd kr ks mt ku kv kw mu ky kz la mv lc ld le mw lg lh li mx lk ll lm bi translated">代码</h1><p id="9edc" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">所以让我们开始吧！</p><h2 id="7ab9" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">首先，导入您需要的库</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="5822" class="my kq it nt b gy nx ny l nz oa">import tensorflow as tf<br/>import numpy as np<br/>import os<br/>import time</span></pre><h2 id="8ad7" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">现在，读取数据</h2><p id="d609" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">你可以从这个数据集中找到并下载所有哈利波特书籍的抄本。在这里，我将所有七本书合并成一个名为<em class="ms">‘Harry Potter . txt’的文本文件。如果你愿意，你也可以在任何一本书上训练你的模型。尝试一下吧！</em></p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="c5f3" class="my kq it nt b gy nx ny l nz oa">files= [‘1SorcerersStone.txt’, ‘2ChamberofSecrets.txt’, ‘3ThePrisonerOfAzkaban.txt’, ‘4TheGobletOfFire.txt’, ‘5OrderofthePhoenix.txt’, ‘6TheHalfBloodPrince.txt’, ‘7DeathlyHollows.txt’]<br/></span><span id="4ca9" class="my kq it nt b gy ob ny l nz oa">with open(‘harrypotter.txt’, ‘w’) as outfile:<br/>for file in files:<br/>  with open(file) as infile:<br/>    outfile.write(infile.read())<br/>text = open(‘harrypotter.txt’).read()</span></pre><h2 id="61c3" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">看着这些数据</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="646d" class="my kq it nt b gy nx ny l nz oa">print(text[:300])</span></pre><blockquote class="oc od oe"><p id="c53d" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated">《哈利·波特与魔法石》</p><p id="2a28" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated">第一章</p><p id="0539" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated">男孩世卫组织活着</p><p id="a129" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated">女贞路4号的德思礼夫妇自豪地说，他们完全正常，非常感谢。他们是你最不可能想到会卷入任何奇怪或神秘事件的人，因为他们“</p></blockquote><h2 id="9d14" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">处理数据</h2><p id="c07a" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">我们通过制作两个查找表将<code class="fe oi oj ok nt b">vocab</code>中的所有唯一字符串映射到数字:</p><ol class=""><li id="d50f" class="ol om it lw b lx ly mb mc mf on mj oo mn op mr oq or os ot bi translated">将字符映射到数字(<code class="fe oi oj ok nt b">char2index</code>)</li><li id="b8af" class="ol om it lw b lx ou mb ov mf ow mj ox mn oy mr oq or os ot bi translated">将数字映射回字符(<code class="fe oi oj ok nt b">index2char</code>)</li></ol><p id="bc24" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">然后将我们的文本转换成数字..</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="2297" class="my kq it nt b gy nx ny l nz oa">vocab = sorted(set(text))<br/>char2index = {u:i for i, u in enumerate(vocab)}<br/>index2char = np.array(vocab)<br/>text_as_int = np.array([char2index[c] for c in text])</span><span id="4477" class="my kq it nt b gy ob ny l nz oa">#how it looks:<br/>print ('{} -- characters mapped to int -- &gt; {}'.format(repr(text[:13]), text_as_int[:13]))</span></pre><blockquote class="oc od oe"><p id="a7d2" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated">“哈利·波特”——映射到int的字符→ [39 64 81 81 88 3 47 78 83 83 68 81 3]</p></blockquote><p id="e4c1" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">我们模型的每个输入序列将包含文本中的<code class="fe oi oj ok nt b">seq_length</code>个字符，其对应的目标序列将具有相同的长度，所有字符都向右移动一个位置。所以我们把文本分成几段<code class="fe oi oj ok nt b">seq_length+1</code>。⁴</p><p id="a07b" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated"><code class="fe oi oj ok nt b">tf.data.Dataset.from_tensor_slices</code>将文本向量转换成字符索引流，<code class="fe oi oj ok nt b">batch</code>方法让我们将这些字符分组为所需长度的批次。</p><p id="6612" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">通过使用<code class="fe oi oj ok nt b">map</code>方法将一个简单的函数应用于每一批，我们创建了我们的输入和目标。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="e367" class="my kq it nt b gy nx ny l nz oa">seq_length = 100<br/>examples_per_epoch = len(text)//(seq_length+1)<br/>char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)<br/>sequences = char_dataset.batch(seq_length+1, drop_remainder=True)</span><span id="9143" class="my kq it nt b gy ob ny l nz oa">def split_input_target(data):<br/>  input_text = data[:-1]<br/>  target_text = data[1:]<br/>  return input_text, target_text</span><span id="0132" class="my kq it nt b gy ob ny l nz oa">dataset = sequences.map(split_input_target)</span></pre><p id="d514" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">在将这些数据输入到模型中之前，我们对数据进行洗牌，并将其分成几批。<code class="fe oi oj ok nt b">tf.data</code>维护一个缓冲区，它在其中混洗元素。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="e5e2" class="my kq it nt b gy nx ny l nz oa">BATCH_SIZE = 64<br/>BUFFER_SIZE = 10000</span><span id="167c" class="my kq it nt b gy ob ny l nz oa">dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)</span></pre><h2 id="3088" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">构建模型</h2><p id="7bf8" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">给定到目前为止计算的所有字符，下一个字符会是什么？这是我们将训练我们的RNN模型来预测的。</p><p id="8df5" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">我使用了<code class="fe oi oj ok nt b"><strong class="lw jd">tf.keras.Sequential</strong></code>来定义模型，因为模型中的所有层只有一个输入并产生一个输出。使用的不同层是:</p><ul class=""><li id="4c73" class="ol om it lw b lx ly mb mc mf on mj oo mn op mr oz or os ot bi translated"><code class="fe oi oj ok nt b"><strong class="lw jd">tf.keras.layers.Embedding</strong></code>:这是输入层。嵌入用于将所有独特的字符映射到多维空间中的向量，具有<code class="fe oi oj ok nt b">embedding_dim</code>维度。</li><li id="f8e6" class="ol om it lw b lx ou mb ov mf ow mj ox mn oy mr oz or os ot bi translated"><code class="fe oi oj ok nt b"><strong class="lw jd">tf.keras.layers.GRU</strong></code>:一种带有<code class="fe oi oj ok nt b">rnn_units</code>数量单位的RNN。(您也可以在此处使用LSTM图层来查看最适合您的数据的图层)</li><li id="7a2d" class="ol om it lw b lx ou mb ov mf ow mj ox mn oy mr oz or os ot bi translated"><code class="fe oi oj ok nt b"><strong class="lw jd">tf.keras.layers.Dense</strong></code>:这是输出层，有<code class="fe oi oj ok nt b">vocab_size</code>输出。</li></ul><p id="977e" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">另外，<strong class="lw jd">单独定义所有超参数</strong>也很有用，这样您可以更容易地在以后更改它们，而无需编辑模型定义。</p><figure class="no np nq nr gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi pa"><img src="../Images/a7f8069d7aad2218e55345bb536b9350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xH-zr7BB7NdKlpwg33HbHA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">文本生成训练示例。<a class="ae ko" href="https://www.tensorflow.org/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="fa9a" class="my kq it nt b gy nx ny l nz oa">vocab_size = len(vocab)<br/>embedding_dim = 300<br/># Number of RNN units <br/>rnn_units1 = 512<br/>rnn_units2 = 256<br/>rnn_units= [rnn_units1, rnn_units2]</span><span id="0ec8" class="my kq it nt b gy ob ny l nz oa">def build_model(vocab_size, embedding_dim, rnn_units, batch_size):<br/>  model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(vocab_size, embedding_dim,<br/>       batch_input_shape=[batch_size, None]),</span><span id="6be9" class="my kq it nt b gy ob ny l nz oa">    tf.keras.layers.GRU(rnn_units1, return_sequences=True,<br/>       stateful=True,recurrent_initializer='glorot_uniform'),</span><span id="ca72" class="my kq it nt b gy ob ny l nz oa">    tf.keras.layers.GRU(rnn_units2, return_sequences=True,<br/>       stateful=True,recurrent_initializer='glorot_uniform'),</span><span id="aeea" class="my kq it nt b gy ob ny l nz oa">    tf.keras.layers.Dense(vocab_size)</span><span id="bbe8" class="my kq it nt b gy ob ny l nz oa">  ])<br/>  return model</span><span id="cbee" class="my kq it nt b gy ob ny l nz oa">model = build_model(<br/>vocab_size = vocab_size,<br/>embedding_dim=embedding_dim,<br/>rnn_units=rnn_units,<br/>batch_size=BATCH_SIZE)</span></pre><h2 id="0fd7" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">训练模型</h2><p id="5a6d" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">标准的<code class="fe oi oj ok nt b">tf.keras.losses.sparse_categorical_crossentropy</code>损失函数最适合我们的模型，因为它应用于预测的最后一层。我们将<code class="fe oi oj ok nt b">from_logits</code>设置为True，因为模型返回logits。然后我们选择<code class="fe oi oj ok nt b">adam</code>优化器并编译我们的模型。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="791e" class="my kq it nt b gy nx ny l nz oa">def loss(labels, logits):<br/>  return tf.keras.losses.sparse_categorical_crossentropy(labels,<br/>         logits, from_logits=True)</span><span id="92c6" class="my kq it nt b gy ob ny l nz oa">model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])</span></pre><p id="0160" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">您可以像这样配置检查点，以确保在训练期间保存检查点。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="1cae" class="my kq it nt b gy nx ny l nz oa"># Directory where the checkpoints will be saved<br/>checkpoint_dir = ‘./training_checkpoints’<br/># Name of the checkpoint files<br/>checkpoint_prefix = os.path.join(checkpoint_dir, “ckpt_{epoch}”)<br/>checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(<br/>   filepath=checkpoint_prefix, save_weights_only=True)</span></pre><p id="65b3" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">每个时期的训练时间取决于您使用的模型层和超参数。我已经将纪元设置为50，以观察准确性和损失如何随时间变化，但可能不需要对所有50个纪元进行训练。当你看到你的损失开始增加或在几个时期内保持不变时，一定要停止训练。您训练的最后一个纪元将存储在<code class="fe oi oj ok nt b">latest_check</code>中。如果使用Google Colab，将运行时间设置为<strong class="lw jd"> GPU </strong>以减少训练时间。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="b959" class="my kq it nt b gy nx ny l nz oa">EPOCHS= 50<br/>history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])<br/>latest_check = tf.train.latest_checkpoint(checkpoint_dir)</span></pre><h2 id="3b0f" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated">文本生成</h2><p id="d825" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">如果您希望使用不同的批处理大小，则需要在运行之前重建模型并重新加载检查点。为了简单起见，我使用了1的<code class="fe oi oj ok nt b">batch_size</code>。</p><p id="c700" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">(您可以运行<code class="fe oi oj ok nt b">model.summary()</code>来深入了解您的模型层以及每层之后的输出形状)</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="3af5" class="my kq it nt b gy nx ny l nz oa">model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)<br/>model.load_weights(latest_check)<br/>model.build(tf.TensorShape([1, None]))<br/>model.summary()</span></pre><p id="a673" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">下面的函数现在生成文本:</p><ul class=""><li id="4890" class="ol om it lw b lx ly mb mc mf on mj oo mn op mr oz or os ot bi translated">它接受一个<code class="fe oi oj ok nt b">start_string</code>，初始化RNN状态，并将输出字符数设置为<code class="fe oi oj ok nt b">num_generate</code></li><li id="0402" class="ol om it lw b lx ou mb ov mf ow mj ox mn oy mr oz or os ot bi translated">使用<code class="fe oi oj ok nt b">start_string</code>和RNN状态获得下一个字符的预测分布。然后，它计算预测字符的索引，这是我们对模型的下一个输入。</li><li id="0507" class="ol om it lw b lx ou mb ov mf ow mj ox mn oy mr oz or os ot bi translated">模型返回的输出状态被反馈到模型中，这样它就有了更多的上下文，(如下所示)。预测完下一个字符后，循环继续。通过这种方式，RNN在从之前的输出中建立记忆的同时进行学习。⁴</li></ul><figure class="no np nq nr gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi pb"><img src="../Images/75a5b8f6e0efd3b2640ca7793efeb76c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R-GMu4sbzzWQ_4IxtJeLNg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">文本生成抽样。<a class="ae ko" href="https://www.tensorflow.org/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank">信号源</a></figcaption></figure><ul class=""><li id="5e88" class="ol om it lw b lx ly mb mc mf on mj oo mn op mr oz or os ot bi translated">较低的<code class="fe oi oj ok nt b">scaling</code>产生更可预测的文本，而较高的<code class="fe oi oj ok nt b">scaling</code>给出更令人惊讶的文本。</li></ul><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="7e3f" class="my kq it nt b gy nx ny l nz oa">def generate_text(model, start_string):</span><span id="8853" class="my kq it nt b gy ob ny l nz oa">  num_generate = 1000  #can be anything you like</span><span id="d469" class="my kq it nt b gy ob ny l nz oa">  input_eval = [char2index[s] for s in start_string]<br/>  input_eval = tf.expand_dims(input_eval, 0)</span><span id="0f28" class="my kq it nt b gy ob ny l nz oa">  text_generated = []</span><span id="2372" class="my kq it nt b gy ob ny l nz oa">  scaling = 0.5 #kept at a lower value here</span><span id="2e44" class="my kq it nt b gy ob ny l nz oa">  # Here batch size == 1<br/>  model.reset_states()<br/>  for i in range(num_generate):<br/>    predictions = model(input_eval)<br/>    # remove the batch dimension<br/>    predictions = tf.squeeze(predictions, 0)<br/>    predictions = predictions / scaling<br/>    predicted_id = tf.random.categorical(predictions, <br/>       num_samples=1)[1,0].numpy()<br/>    input_eval = tf.expand_dims([predicted_id], 0)<br/>    text_generated.append(idx2char[predicted_id])</span><span id="4900" class="my kq it nt b gy ob ny l nz oa">return (start_string + ‘’.join(text_generated))</span></pre><p id="504f" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">你完了！</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h1 id="6c43" class="kp kq it bd kr ks mt ku kv kw mu ky kz la mv lc ld le mw lg lh li mx lk ll lm bi translated">输出</h1><p id="5bac" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated">您可以尝试给它不同的开始字符串，以获得不同的输出。</p><p id="6322" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">下面是使用我最喜欢的字符输出的一部分:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="2884" class="my kq it nt b gy nx ny l nz oa">print(generate_text(model, start_string=u”Severus Snape“))</span></pre><blockquote class="oc od oe"><p id="dec1" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated"><strong class="lw jd">西弗勒斯·斯内普</strong>搬到猩红色霍格沃茨的学生。赫敏说，“嗯，我觉得没事了，没事了，以前有点死了。。。."<br/>“我想我得去别的地方，而不是你去帮他问职员的问题，桌子和门开了，他盯着钟看哈利。”我想这是格兰芬多的剑，他也在那里，他在枕头上，他和罗恩盯着他看。“我确信我们可以打扰那个男孩——”<br/>“你应该在那里，”罗恩说，他面带奇怪之色。<br/>“我是说，他真的很棒<strong class="lw jd"> … </strong></p></blockquote><p id="238c" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">你也可以尝试不同的句子:</p><blockquote class="oc od oe"><p id="ccd5" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated"><strong class="lw jd">伏地魔死于冠状病毒。</strong><br/>“你不知道该做什么，”哈利说，“那是一件环绕的斗篷，是他支撑你走到那条路上的。“是啊，好吧，我想你可能已经这么做了！”她说，大步走上台阶，他的力量是如此之大，因为他是一个相当大的帐篷，这是他们可能第一次意识到我看到他被摧毁和尖叫的人群穿过黑暗的时间喊叫声和沉默。<br/>“你看，哈利！“我不知道，看得出你和魔法部的一个混蛋没有任何关系”</p></blockquote><p id="f289" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">这里有一个例子，如果你只使用第一本书<em class="ms">魔法石:</em>来训练模型</p><blockquote class="oc od oe"><p id="e68e" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated"><strong class="lw jd">邓布利多</strong>在破釜酒吧，现在空无一人。哈里以前从未去过伦敦。尽管海格的眼睛看起来很冷很绿。他还在发抖。</p><p id="ed85" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated">哈利在那碗豌豆旁边坐了下来。“你跟邓布利多教授谈了些什么。”</p><p id="b178" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated">她用既震惊又怀疑的眼光看着他。</p><p id="7b9d" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated">“谁在那里？”当他们爬上街道时，他突然说道。他只能看见四号台阶上的毯子。</p><p id="b975" class="lu lv ms lw b lx ly lz ma mb mc md me of mg mh mi og mk ml mm oh mo mp mq mr im bi translated">达力最喜欢的出气筒是哈利，但是他不能经常抓住他。哈利什么也没说<strong class="lw jd">……</strong></p></blockquote><p id="fd79" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">你会看到这个模型知道何时将单词大写，创建一个新段落，并且它模仿了一个<em class="ms">神奇的</em>写作词汇！</p><p id="eb06" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated"><em class="ms">恶作剧成功了。</em></p><p id="acd2" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">为了使句子更加连贯，您可以通过以下方式改进模型</p><ul class=""><li id="2993" class="ol om it lw b lx ly mb mc mf on mj oo mn op mr oz or os ot bi translated">改变不同的参数值，如<code class="fe oi oj ok nt b">seq_length</code>、<code class="fe oi oj ok nt b">rnn_units</code>、<code class="fe oi oj ok nt b">embedding_dims</code>、<code class="fe oi oj ok nt b">scaling</code>，以找到最佳设置</li><li id="2f25" class="ol om it lw b lx ou mb ov mf ow mj ox mn oy mr oz or os ot bi translated">为更多的时代训练它</li><li id="76c5" class="ol om it lw b lx ou mb ov mf ow mj ox mn oy mr oz or os ot bi translated">添加更多的GRU / LSTM层</li></ul><p id="b52c" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">这个模型可以在你喜欢的任何其他系列上训练。一定要在评论里分享你自己的故事，玩得开心！</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="9e1b" class="my kq it bd kr mz na dn kv nb nc dp kz mf nd ne ld mj nf ng lh mn nh ni ll iz bi translated"><strong class="ak">参考文献:</strong></h2><p id="8231" class="pw-post-body-paragraph lu lv it lw b lx nj lz ma mb nk md me mf nl mh mi mj nm ml mm mn nn mp mq mr im bi translated"><strong class="lw jd">【1】</strong>j . k .罗琳，<a class="ae ko" href="https://www.bloomsbury.com/uk/harry-potter-and-the-deathly-hallows-9780747591054/" rel="noopener ugc nofollow" target="_blank"> <em class="ms">哈利波特与死亡圣器</em> </a> <em class="ms">，2007年</em></p><p id="6c9e" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated"><strong class="lw jd">【2】</strong><em class="ms">递归神经网络教程，第四部分——用Python和Theano实现一个GRU/LSTM RNN</em>，<a class="ae ko" href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" rel="noopener ugc nofollow" target="_blank">2015年10月27日</a>作者<a class="ae ko" href="http://www.wildml.com/author/dennybritz/" rel="noopener ugc nofollow" target="_blank">丹尼·布里兹</a></p><p id="171f" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated"><strong class="lw jd">【3】</strong>j . k .罗琳，<a class="ae ko" href="https://shop.scholastic.com/parent-ecommerce/books/harry-potter-and-the-sorcerers-stone-9780545582889.html" rel="noopener ugc nofollow" target="_blank"> <em class="ms">《哈利波特与魔法石》</em> </a>，1998年</p><p id="2e23" class="pw-post-body-paragraph lu lv it lw b lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated"><strong class="lw jd"><strong class="lw jd"/><a class="ae ko" href="https://www.tensorflow.org/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank"><em class="ms">文本生成与RNN </em> </a> <em class="ms">、张量流</em></strong></p></div></div>    
</body>
</html>