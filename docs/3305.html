<html>
<head>
<title>6 Tips Save Me Time &amp; Memory When Training Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练机器学习模型时节省时间和内存的6个技巧</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/6-tips-save-me-time-memory-when-training-machine-learning-models-1294c252baf1?source=collection_archive---------1-----------------------#2022-11-14">https://pub.towardsai.net/6-tips-save-me-time-memory-when-training-machine-learning-models-1294c252baf1?source=collection_archive---------1-----------------------#2022-11-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="2e91" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ko translated">训练机器学习模型可能会耗费时间和内存，尤其是当你的数据非常大的时候。因为通常情况下，您必须训练多个模型或使用不同的超参数多次训练模型，以找到适合您的模型的最佳超参数。因此，能够优化您的工作流以尽可能节省计算时间和内存消耗，从而能够找到最佳模型并有效地训练它们，这一点非常重要。</p><p id="2602" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本文中，我将与您分享六个实用技巧，这些技巧是我个人在从事数据科学项目或训练机器学习模型时用来减少计算时间和内存消耗的。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi kx"><img src="../Images/4e2e2d05f04fdf29863c6fa8f813ee97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8nR-D9GPrF-cr9Sd"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">训练你的模特时节约资源/图片由<a class="ae ln" href="https://unsplash.com/@andretaissin?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Andre Taissin </a>在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="8896" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将在整篇文章中使用<a class="ae ln" href="https://www.tensorflow.org/datasets/catalog/mnist" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> MNSIT数据集</strong> </a> <strong class="js iu"> </strong>，并将在其上应用所有提示，并查看结果以及时间和内存消耗是如何受到影响的。让我们加载数据并对其进行整形，以便能够用于我们的模型:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/6016b16b55586d059233fca91841841d.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*zT6W_oR7wp6AEqI2sc3r3w.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图一。MNSIT数据集的一位数字。</figcaption></figure><p id="cb1c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，让我们打印数据的形状:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/6e079c6bd12e20d14670ca05f1523487.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*6-s9K2lioS9ryNBAluwhuQ.png"/></div></figure><p id="f2d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，让我们重塑数据，以便能够对其应用降维算法:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/a5b6c0d996861bd4a7ddff8b48de673b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*nV1gEHipRQNOL8Ktilh-Ag.png"/></div></figure><h2 id="4227" class="lt lu it bd lv lw lx dn ly lz ma dp mb kb mc md me kf mf mg mh kj mi mj mk ml bi translated"><strong class="ak">目录:</strong></h2><ol class=""><li id="b51e" class="mm mn it js b jt mo jx mp kb mq kf mr kj ms kn mt mu mv mw bi translated"><strong class="js iu">训练所有的CPU内核</strong></li><li id="ce87" class="mm mn it js b jt mx jx my kb mz kf na kj nb kn mt mu mv mw bi translated"><strong class="js iu">使用随机PCA </strong></li><li id="7fd7" class="mm mn it js b jt mx jx my kb mz kf na kj nb kn mt mu mv mw bi translated"><strong class="js iu">使用增量PCA </strong></li><li id="b107" class="mm mn it js b jt mx jx my kb mz kf na kj nb kn mt mu mv mw bi translated"><strong class="js iu">应用链式法则</strong></li><li id="4e9e" class="mm mn it js b jt mx jx my kb mz kf na kj nb kn mt mu mv mw bi translated"><strong class="js iu">使用线性SVM </strong></li><li id="3c60" class="mm mn it js b jt mx jx my kb mz kf na kj nb kn mt mu mv mw bi translated"><strong class="js iu">使用小批量K-Means </strong></li></ol><p id="8e10" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以在GitHub资源库中找到本文中使用的代码:</p><div class="nc nd gp gr ne nf"><a href="https://github.com/youssefHosni/Practical-Machine-Learning-Tips" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">GitHub-youssefHosni/实用机器学习技巧</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">github.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt lh nf"/></div></div></a></div></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="08d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">如果你想免费学习数据科学和机器学习，可以看看这些资源:</strong></p><ul class=""><li id="73ea" class="mm mn it js b jt ju jx jy kb ob kf oc kj od kn oe mu mv mw bi translated">免费互动路线图，自学数据科学和机器学习。从这里开始:<a class="ae ln" href="https://aigents.co/learn/roadmaps/intro" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn/roadmaps/intro</a></li><li id="04fc" class="mm mn it js b jt mx jx my kb mz kf na kj nb kn oe mu mv mw bi translated">数据科学学习资源搜索引擎(免费)。将你最喜欢的资源加入书签，将文章标记为完整，并添加学习笔记。<a class="ae ln" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></li><li id="0fb9" class="mm mn it js b jt mx jx my kb mz kf na kj nb kn oe mu mv mw bi translated">想要在导师和学习社区的支持下从头开始学习数据科学吗？免费加入这个学习圈:<a class="ae ln" href="https://community.aigents.co/spaces/9010170/" rel="noopener ugc nofollow" target="_blank">https://community.aigents.co/spaces/9010170/</a></li></ul><p id="c335" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">如果你想在数据科学&amp; AI领域开始职业生涯，但你不知道如何开始。我提供数据科学指导课程和长期职业指导:</strong></p><ul class=""><li id="e675" class="mm mn it js b jt ju jx jy kb ob kf oc kj od kn oe mu mv mw bi translated">长期师徒:【https://lnkd.in/dtdUYBrM】T4</li><li id="b1f0" class="mm mn it js b jt mx jx my kb mz kf na kj nb kn oe mu mv mw bi translated">辅导课程:<a class="ae ln" href="https://lnkd.in/dXeg3KPW" rel="noopener ugc nofollow" target="_blank">https://lnkd.in/dXeg3KPW</a></li></ul><p id="2fa6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="of">加入</em> </strong> <a class="ae ln" href="https://youssefraafat57.medium.com/membership" rel="noopener"> <strong class="js iu"> <em class="of">中等会员</em> </strong> </a> <strong class="js iu"> <em class="of">计划，只需5美元，继续学习，没有限制。如果你使用下面的链接，我会收到一小部分会员费，不需要你额外付费。</em> </strong></p><div class="nc nd gp gr ne nf"><a href="https://youssefraafat57.medium.com/membership" rel="noopener follow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">加入我的介绍链接媒体-优素福胡斯尼</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">阅读Youssef Hosni(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">youssefraafat57.medium.com</p></div></div><div class="no l"><div class="og l nq nr ns no nt lh nf"/></div></div></a></div></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="25d6" class="oh lu it bd lv oi oj ok ly ol om on mb oo op oq me or os ot mh ou ov ow mk ox bi translated">1.训练所有的CPU核心</h1><p id="44ae" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb oy kd ke kf oz kh ki kj pa kl km kn im bi translated">Bagging和post是学习方法，其中我们对每个预测器使用相同的训练算法，但在训练集的不同随机子集上训练它们。</p><p id="4e9d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当进行替换抽样时，这种方法称为<strong class="js iu">打包</strong>(bootstrap aggregate的简称)。不更换样品时，称为<strong class="js iu">粘贴</strong>。</p><p id="59d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，打包和粘贴都允许跨多个预测器对训练实例进行多次采样，但只有打包允许对同一预测器对训练实例进行多次采样。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi pb"><img src="../Images/718eb6c541dac89d661362b0901b5003.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3tbEG4uMY1y3UC3g"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">劳拉·奥克尔在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="55e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除了它们的良好效果之外，打包和粘贴之所以出名的一个主要原因是它们可以以一种简单的方式很好地扩展。由于预测器是独立的，因此可以并行训练，因此可以在多个CPU核心或多个服务器上训练。预测也可以这样做。</p><p id="2dc9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Scikit learn提供了一个简单的API，用于通过BaggingClassifier类打包和粘贴。参数<strong class="js iu"> n_jobs </strong>定义了可用于训练和预测的内核数量。下面的代码<strong class="js iu"> n_jobs = -1 </strong>，意思是使用所有可用的内核。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/e0c65d76bccdf465819cd65613644e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*VyEuJamrWqcXa2Q_LxHSFQ.png"/></div></figure><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/b768e9eae9188b0ea8f2dbf50451eeca.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*XamyGpSgqDBf7mapA-Hoaw.png"/></div></figure><p id="cd32" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如你所看到的，使用所有的CPU核心减少了至少4倍的计算时间。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="35f6" class="oh lu it bd lv oi oj ok ly ol om on mb oo op oq me or os ot mh ou ov ow mk ox bi translated">2.使用随机PCA</h1><p id="cd51" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb oy kd ke kf oz kh ki kj pa kl km kn im bi translated">如果想提高PCA的速度，可以在Scikit-Learn中将svd_solver超参数设置为“随机化”。这将使它使用一种称为随机化PCA的随机算法，快速找到前d个主成分的近似值。它的计算复杂度为O(m × d2) + O(d3)，而不是完全SVD方法的O(m × n2) + O(n3)，因此当d远小于n时，它比完全SVD快得多:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/8dd131ec3b5ce751644e07af2977e467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*B-hiDHvp2HUYzW96_0TedA.png"/></div></figure><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/c20d8bd7d63219d9f81241d1b06760c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*dw75q0c_v_98OQE_UDl6rw.png"/></div></figure><p id="a71b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">默认情况下，svd_solver实际上设置为“auto”:如果m或n大于500，并且d小于m或n的80%，Scikit-Learn会自动使用随机PCA算法，否则它会使用完整的svd方法。如果想强制Scikit-Learn使用完整的SVD，可以将svd_solver超参数设置为“<strong class="js iu"> full </strong>”。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="855c" class="oh lu it bd lv oi oj ok ly ol om on mb oo op oq me or os ot mh ou ov ow mk ox bi translated">3.使用增量PCA</h1><p id="baba" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb oy kd ke kf oz kh ki kj pa kl km kn im bi translated">如果您希望将PCA应用于大型数据集，您可能会面临内存错误，因为PCA的实现需要将整个训练集放入内存中，以便运行算法。</p><p id="45d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">幸运的是，𝗜𝗻𝗰𝗿𝗲𝗺𝗲𝗻𝘁𝗮𝗹 𝗣𝗖𝗔 (𝗜𝗣𝗖𝗔)算法已经被开发出来解决这个问题:您可以将训练集分成小批，一次一个小批地输入IPCA算法。这对于大型训练集和在线应用PCA非常有用。</p><p id="3766" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下代码将训练数据集拆分为100个小批(使用NumPy的array_split()函数)，并将它们提供给Scikit-Learn的IncrementalPCA类，以将训练数据集的维数降低到100维。请注意，您必须对每个小批量调用partial_fit()方法，而不是对整个定型集调用fit()方法:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/a8ebdde0e36551e9483302fc63682959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*x5XzQwmubKQryzPB7hykUg.png"/></div></figure><p id="0863" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您的数据非常大，无法放入内存，这将非常有用。但是，与传统的PCA相比，它会增加计算时间，因此只有当数据太大而无法存储时，才应该使用它。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="cfe9" class="oh lu it bd lv oi oj ok ly ol om on mb oo op oq me or os ot mh ou ov ow mk ox bi translated">4.应用链规则</h1><p id="a3d9" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb oy kd ke kf oz kh ki kj pa kl km kn im bi translated">如果您想使用一种非常慢的降维方法，尤其是如果您的数据是非线性的，如<strong class="js iu">局部线性嵌入</strong> (LLE)或t-分布式随机邻居嵌入(t-DSNE)，您可以将两种降维算法串联起来，以减少LLE的计算时间，同时仍能获得几乎相同的结果。</p><p id="fa04" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，对整个数据应用更快的降维算法，将维度降低到一个较低的维度，然后应用更慢的算法，以更高的质量降低剩余的维度。这总是有效的，因为两个维度的减少对更高的维度有相似的影响。</p><p id="c204" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以先对你的数据应用主成分分析，快速剔除大量无用维度，然后使用LLE。这将产生与仅使用LLE类似的性能，但只是其时间的一小部分。</p><p id="a4a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们首先将LLE应用于数据集，以缩减MNSIT数据集的子集，并将数据缩减为两个部分:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/a52b0616fa990f5a893d44b7162fa222.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*93g_Og90cWDabWQMRhxB9A.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图二。利用LLE算法简化MNSIT数据集。</figcaption></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/10d6a625791afaa77316c8820ab7c238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*8qfsxEV-BRaKd-CGMydNPg.png"/></div></figure><p id="75a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到，使用LLE将数据降低到二维所需的时间是15秒。现在让我们应用链式法则，首先将PCA应用于MNSIT数据，并将其减少到100个维度，然后将LLE应用于这个减少的数据，以将维度减少到仅2个分量。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/41fcc2b3f5732cdd4356ca82dbf71236.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*J9SJ-mZDJkfSE0FvJb8OTw.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图3。使用主成分分析和LLE的简化MNSIT数据集</figcaption></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/4b463c3fe8bb64ef00f01f4eafd35d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*ZBvYCyPGEpHe0kBJ0_lepQ.png"/></div></figure><p id="599c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如你所看到的，两种方法产生了相似的结果，但是第二种方法肯定比第一种快。此外，值得一提的是，我只对数据的一小部分应用了这两种方法，并且存在4秒的时间差。随着数据大小的增加，时间差将会增加，并将显示应用这种技巧的更多优势。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="4b4b" class="oh lu it bd lv oi oj ok ly ol om on mb oo op oq me or os ot mh ou ov ow mk ox bi translated">5.使用线性SVM</h1><p id="0fdd" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb oy kd ke kf oz kh ki kj pa kl km kn im bi translated">因为𝐋𝐢𝐧𝐞𝐚𝐫𝐒𝐕𝐂类是基于𝐥𝐢𝐛𝐥𝐢𝐧𝐞𝐚𝐫库的，它实现了线性支持向量机的优化算法。它不支持内核技巧，但它与训练实例的数量和特征的数量𝐬𝐜𝐚𝐥𝐞𝐬 𝐚𝐥𝐦𝐨𝐬𝐭 𝐥𝐢𝐧𝐞𝐚𝐫𝐥𝐲:它的训练时间复杂度大致为O(m × n)。</p><p id="b0ff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您要求非常高的精度，该算法需要更长的时间。这是由公差超参数ϵ(在𝐒𝐜𝐢𝐤𝐢𝐭-𝐋𝐞𝐚𝐫𝐧).称为𝐭𝐨𝐥)控制的在大多数分类任务中，默认容差是合适的。如果您有一个大的数据集，但是是线性可分的，这将是一个完美的选择。</p><p id="2a0d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一方面，𝐒𝐕𝐂类是基于𝐥𝐢𝐛𝐬𝐯𝐦库的，它实现了一个支持内核技巧的算法。训练时间复杂度通常在O(m2 × n)到O(m3 × n)之间。不幸的是，这意味着当训练实例的数量变大时(例如，几十万个实例)，它会变得非常慢。</p><p id="ae6e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个算法非常适合𝐜𝐨𝐦𝐩𝐥𝐞𝐱𝐛𝐮𝐭𝐬𝐦𝐚𝐥𝐥𝐨𝐫𝐦𝐞𝐝𝐢𝐮𝐦𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠𝐬𝐞𝐭𝐬.然而，它随着特征的数量而很好地扩展，特别是对于稀疏特征(即，当每个实例几乎没有非零特征时)。在这种情况下，该算法大致与每个实例中非零要素的平均数量成比例。</p><p id="4f7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们在MNSIT数据集上尝试一下，看看计算时间的差异。首先，我们将在MNSIT数据集上训练<strong class="js iu"> linearSVC </strong>模型，并计算训练和推理时间:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/8806e5e595a67ad91ba068568c961f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*ToE4ossl-jhZs_GtEznfFg.png"/></div></figure><p id="fbfa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练精度为<strong class="js iu"> 93%，</strong>，计算训练时间为<strong class="js iu"> 565 </strong>秒。我们来看看测试精度和推断时间:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/f5ebff01fc703ef98d776d8e96f631ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*nZaxXan4loHWFQ_3N0xhyQ.png"/></div></figure><p id="a083" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">LinearSVC分类器的测试精度为<strong class="js iu"> 91%，</strong>，推理计算时间为<strong class="js iu"> 0.07秒</strong>，非常小。</p><p id="4693" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们将其与内核SVC分类器进行比较:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/e28edbf358d5a2b8a87621a35c944b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*XjgUjem5Y6UFx_ZdUaZBtw.png"/></div></figure><p id="56be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练准确率达到99%,远远高于线性支持向量机。然而，计算时间是<strong class="js iu"> 679秒</strong>，这也比LinearSVC分类器的计算时间长。</p><p id="bf54" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，让我们比较一下内核SVC分类器和线性SVC的测试精度和计算推理时间:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/4f93f579c45c0969cea282b666f92b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*GPXaGm4qmHFft9iId2Zn7Q.png"/></div></figure><p id="b5f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">测试准确度为<strong class="js iu"> 97% </strong>，同样远高于LinearSVC分类器的测试准确度。推理过程中的计算时间为78.85秒，与LinearSVC推理计算时间相比，这是前者的100倍。</p><p id="9d04" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总之，核支持向量机分类器的性能优于线性支持向量机，但计算时间，特别是计算推理时间，远高于线性支持向量机。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="98dc" class="oh lu it bd lv oi oj ok ly ol om on mb oo op oq me or os ot mh ou ov ow mk ox bi translated">6.使用小批量K均值</h1><p id="904a" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb oy kd ke kf oz kh ki kj pa kl km kn im bi translated">如果要将<strong class="js iu"> K_Means </strong>算法应用于大型数据集，可以使用MiniBatchKmeans类。该算法不需要在每次迭代中使用完整数据集，而是能够使用小批量，在每次迭代中仅略微移动质心。这通常会将算法的速度提高3或4倍，从而可以对不适合内存的大型数据集进行群集。</p><p id="4cbc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Scikit-Learn在<strong class="js iu">minibatch kmans</strong>类中实现了这个算法。您可以像使用KMeans类一样使用这个类。让我们将此应用于MNSIT数据集，并将其群集为九个群集，计算计算时间，然后将其与<strong class="js iu"> K-Means </strong>算法进行比较:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/e1c421407a2ef2471e4498989226362f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*hWIE5XTnDs43oClqg0ZEtQ.png"/></div></figure><p id="15e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用<strong class="js iu">microbatch kmans</strong>对数据进行聚类的计算时间为<strong class="js iu"> 1.9秒。</strong>我们来对比一下<strong class="js iu"> K-Means </strong>聚类算法:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi po"><img src="../Images/fa46f4ea0c71c99e7b3c78b23ad9d939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*1E1mgooaW90gtO_5SgTT3g.png"/></div></figure><p id="d358" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用K-Means对数据进行聚类的计算时间为<strong class="js iu"> 65.53秒</strong>，比<strong class="js iu">minibatch kmans</strong>聚类算法的计算时间多50倍。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="9fff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="of">喜欢的文章？成为</em> </strong> <a class="ae ln" href="https://youssefraafat57.medium.com/membership" rel="noopener"> <strong class="js iu"> <em class="of">中型成员</em> </strong> </a> <strong class="js iu"> <em class="of">继续无限制学习。如果您使用以下链接，我将收取一小部分会员费，而不收取额外费用。</em> </strong></p><div class="nc nd gp gr ne nf"><a href="https://youssefraafat57.medium.com/membership" rel="noopener follow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">使用我的推荐链接加入media-Youssef Hosni</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">读一读优素福·胡斯尼(以及Medium上成千上万其他作家)的所有故事。您的会员费直接支持…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">youssefraafat57.medium.com</p></div></div><div class="no l"><div class="og l nq nr ns no nt lh nf"/></div></div></a></div><p id="3e2d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="of">感谢阅读！如果喜欢文章，一定要鼓掌(最多50！)并在</em></strong><a class="ae ln" href="https://www.linkedin.com/in/youssef-hosni-b2960b135/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="of">LinkedIn</em></strong></a><strong class="js iu"><em class="of">上与我联系，并在</em> </strong> <a class="ae ln" href="https://youssefraafat57.medium.com/" rel="noopener"> <strong class="js iu"> <em class="of">上关注我的【中型】</em> </strong> </a> <strong class="js iu"> <em class="of">以保持更新我的新文章</em> </strong></p></div></div>    
</body>
</html>