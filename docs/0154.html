<html>
<head>
<title>PEARL: Probabilistic Embeddings for Actor-Critic RL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">珀尔:演员评论家RL的概率嵌入</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pearl-probabilistic-embeddings-for-actor-critic-rl-42a5cbe05bfb?source=collection_archive---------0-----------------------#2019-09-08">https://pub.towardsai.net/pearl-probabilistic-embeddings-for-actor-critic-rl-42a5cbe05bfb?source=collection_archive---------0-----------------------#2019-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9027" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">潜入珍珠| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向艾</a></h2><div class=""/><div class=""><h2 id="a898" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一种样本有效的元强化学习方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/5c7e3a039d9b130c2d821a41b5f358bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-f-BD-lCvV3HT4och0u1w.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:联合国人类住区规划署</figcaption></figure><h1 id="a8a0" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">介绍</h1><p id="7974" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">元强化学习尤其具有挑战性，因为智能体不仅要适应新的输入数据，还要找到探索新环境的有效方法。当前的meta-RL算法严重依赖于基于策略的经验，这限制了它们的采样效率。更糟糕的是，他们中的大多数人在适应新任务时缺乏推理任务不确定性的机制，这限制了他们在稀疏回报问题上的有效性。</p><p id="f206" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们讨论了试图解决这些挑战的meta-RL算法。简而言之，该算法，即Rakelly &amp; Zhou等人在2019年提出的演员-评论家RL(PEARL)的概率嵌入，由两部分组成:它学习充分描述任务的概率潜在上下文；以该潜在上下文为条件，非策略RL算法学习采取行动。在这个框架中，概率潜在上下文充当当前任务的信念状态。通过在潜在上下文上调节RL算法，我们期望RL算法学会区分不同的任务。此外，这将任务推理从行动决策中分离出来，正如我们将在后面看到的，这使得一个非策略算法适用于元学习。</p><p id="8896" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">文章的其余部分分为三个部分。首先，我们介绍推理架构，PEARL的基石。在此基础上，我们论证了PEARL中非策略学习的有效性，并简要讨论了Rakelly &amp; Zhou等人所采用的非策略学习方法。最后，我们将这两个部分结合起来，形成了PEARL算法。</p><h1 id="3f8c" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">推理架构</h1><p id="5b45" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">推理网络获取关于当前任务应该如何在潜在概率上下文变量<em class="na"> Z </em>中执行的知识，在此基础上，我们将策略限定为<em class="na"> 𝜋(a|s，z) </em>，以便使其行为适应任务。在本节中，我们将重点关注推理网络如何利用来自各种训练任务的数据来学习从新任务的最近经历中推断出<em class="na"> Z </em>的值。</p><p id="b69b" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">对于特定的任务，我们对一批最近收集的转换进行采样，并通过网络𝜙对每个转换<em class="na"> cₙ </em>进行编码，以提取概率潜在上下文𝛹 <em class="na"> _𝜙(z|cₙ】，</em>通常是高斯后验概率。然后，我们计算所有这些高斯因子的乘积，以形成潜在上下文变量的后验概率:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/a74e1fd635108db751296c3d1a92b750.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*W-jrd5W7c-jW8-Gx6I6j6Q.png"/></div></figure><p id="3786" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">下图演示了这一过程</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/c83354b5ab7c20a53bc1f485375d99aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zlBPxG-mWY4L4cVamPblhg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">推理架构。来源:通过概率上下文变量的有效非策略元强化学习</figcaption></figure><p id="08ef" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">请注意，这里使用的转换是从一组最近收集的转换中随机采样的，这不同于我们稍后用来训练非策略算法的转换。作者还试验了其他架构和采样策略，如顺序转换的RNN，但没有一个表现出优越的性能。</p><h2 id="4561" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">最佳化</h2><p id="94d6" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">我们通过变分下界来优化推理网络<em class="na"> q_𝜙(z|c) </em>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/4abce2fa73b6f9862d50a4c69e982a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*4Ak1sGX_yWIp-TgbsccuNg.png"/></div></div></figure><p id="3a14" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">其中<em class="na"> R </em>是某个下游任务的目标，<em class="na"> 𝒩(0，I) </em>是单位高斯先验。如果我们取<em class="na"> R </em>作为重建损失，则可以很容易地根据𝛽-variational自动编码器的推导得出这个目标。Rakelly &amp; Zhou等人根据经验发现，训练编码器恢复状态-动作值函数(使用<em class="na">Q</em>-函数)优于优化它以最大化行动者回报(使用政策)，或重建状态和奖励(使用VAE结构)。</p><h2 id="7be8" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">为什么不使用确定性上下文呢？</h2><p id="3286" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">概率上下文的优势在于它可以对任务的信念状态进行建模，这对于下游的偏离策略算法实现深度探索是至关重要的。深度探索在稀疏奖励设置中尤其重要，在稀疏奖励设置中，一致的探索策略比随机探索更有效。我们建议感兴趣的读者参考Osband等人2016年的<a class="ae np" href="https://arxiv.org/abs/1602.04621" rel="noopener ugc nofollow" target="_blank">第5节，以获取说明性示例。下图在一个带有稀疏回报的2D导航问题上比较了这两种情况。</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/24709fad26859973acea37f84f68cbe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uItLpuBsLpBHFygU9ypnzQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">确定性上下文和概率性上下文的比较。来源:通过概率上下文变量的有效非策略元强化学习</figcaption></figure><h1 id="db57" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">将非策略学习与元学习相结合</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/5491b19300f73abf3b190ae3b40aef09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OaOJLTRWJY1b7btWyPaWzQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:通过概率上下文变量的有效非策略元强化学习</figcaption></figure><p id="9e42" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">现代元学习算法主要依赖于这样的假设，即用于适应的数据分布将在元训练和元测试中匹配。在RL中，这意味着在元训练期间应该使用策略上的数据，因为在元测试时，策略上的数据将用于适应。PEARL通过将任务推理的负担从RL方法卸载到推理网络上来释放这种约束。这样，PEARL不再需要在元测试时微调RL方法，并且可以在元训练时应用非策略方法。事实上，这里对非策略RL方法的唯一修改是使每个网络以<em class="na"> z </em>为条件，并保持其他网络不变。</p><p id="fbd3" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">PEARL的官方实现采用软行动者-批评家(<a class="ae np" href="https://arxiv.org/abs/1801.01290" rel="noopener ugc nofollow" target="_blank"> SAC </a>)，因为SAC表现出良好的采样效率和稳定性，并且进一步具有与概率潜在上下文很好地结合的概率解释。长话短说，SAC由五个网络组成:两个状态值函数<em class="na"> V </em>和<em class="na"> \bar V </em> ( <em class="na"> \bar V </em>是<em class="na"> V </em>的目标网络)、两个动作值函数<em class="na"> Q₁ </em>和<em class="na"> Q₂ </em>，以及一个策略函数𝜋；它通过以下损失函数优化这些函数</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/6e4147275358cb85302c3a306a4e638e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yuh7fe2TH3bSW_GfunUXAg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">SAC中的损失函数</figcaption></figure><p id="4c4f" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">其中<em class="na"> Q=min(Q₁，Q₂) </em>和<em class="na"> \bar z </em>表示没有通过它计算梯度。我们让感兴趣的读者去我的个人博客了解更多关于SAC的细节。</p><h1 id="6209" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">算法</h1><p id="8e0b" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">既然我们已经介绍了所有必要的组件，现在是时候将它们放在一起并呈现整个算法了。</p><h2 id="ac77" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">元训练</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/45f80674c6835c1f2324feee39942a65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnentUGNVIpPpUplLl0gfQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">元训练时的PEARL伪代码</figcaption></figure><p id="4a44" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">有几件事值得注意:</p><ol class=""><li id="5a55" class="nu nv it mb b mc mv mf mw mi nw mm nx mq ny mu nz oa ob oc bi translated">上下文<em class="na"> c </em>是一个元组<em class="na"> (s，a，r)</em>；它还可以包括用于任务分布的<em class="na">s’</em>，其中动态在任务之间变化。</li><li id="4bec" class="nu nv it mb b mc od mf oe mi of mm og mq oh mu nz oa ob oc bi translated">有一个隐含的for循环回绕线6和7，使得<em class="na"> z </em>在每个轨迹后被重新采样。第8 &amp; 9行也是如此。另外，请注意，在许多任务中，我们不会将第9行收集的数据添加到上下文缓冲区中(在大多数<a class="ae np" href="https://github.com/katerakelly/oyster/tree/master/configs" rel="noopener ugc nofollow" target="_blank">配置</a>中<code class="fe oi oj ok ol b">num_steps_posterior</code>为零)；这表明第12行中的上下文<em class="na"> c </em>是由来自先前分布的以<em class="na"> z </em>为条件的策略收集的。Rakelly &amp;周等人发现，这种设置在这些有形的奖励环境中效果更好，在这些环境中，探索似乎对识别和解决任务并不至关重要。[5]</li><li id="31b7" class="nu nv it mb b mc od mf oe mi of mm og mq oh mu nz oa ob oc bi translated">推理网络<em class="na"> q_𝜙(z|c) </em>是使用来自我们之前所述的<em class="na"> Q </em>网络的贝尔曼更新的梯度来训练的。</li></ol><h2 id="09bb" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">元测试</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/c978276123e28dc749b25fcc391df9ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ApKEBHPNvRTjhwAs98W9w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">测试时PEARL的伪代码</figcaption></figure><p id="39b6" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">与以前的方法不同，PEARL在元测试时不微调任何网络；它依靠推理网络的泛化能力来适应新的任务。</p><h1 id="6ccd" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">实验结果</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/eb82af77d24173882b0b95cc0b3cae0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFJuQ7xvqLbJsvKnb69GCQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">任务绩效与元训练期间收集的样本。RL通过PPO实施，MAML通过TRPO实施。来源:通过概率上下文变量的有效非策略元强化学习</figcaption></figure><p id="cd74" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">上图展示了六种连续控制环境下不同方法的任务绩效。这些运动任务家族需要适应不同的奖励功能(半猎豹前进-后退、蚂蚁前进-后退、人形-Direc-2D、半猎豹-Vel的目标速度和蚂蚁-Goal2D的目标位置)或不同的动力学(沃克-2D-Params的随机系统参数)。我们可以看到，PEARL在样本效率方面优于现有算法20-100倍，在这些任务中的渐近性能也优于现有算法</p><h1 id="656f" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">参考</h1><ol class=""><li id="c2fb" class="nu nv it mb b mc md mf mg mi oo mm op mq oq mu nz oa ob oc bi translated">凯特·拉凯利、奥瑞克·周、迪尔德丽·奎伦、切尔西·芬恩和谢尔盖·莱文。通过概率上下文变量进行有效的非策略元强化学习</li><li id="9882" class="nu nv it mb b mc od mf oe mi of mm og mq oh mu nz oa ob oc bi translated">伊恩·奥斯本、查尔斯·布伦德尔、亚历山大·普里策尔和本杰明·范·罗伊。通过自举DQN进行深度探索</li><li id="a878" class="nu nv it mb b mc od mf oe mi of mm og mq oh mu nz oa ob oc bi translated">Tuomas Haarnoja、Aurick Zhou、Pieter Abbeel和Sergey Levine。软行动者-批评家:随机行动者下的非策略最大熵深度强化学习</li><li id="a582" class="nu nv it mb b mc od mf oe mi of mm og mq oh mu nz oa ob oc bi translated">代号:<a class="ae np" href="https://github.com/katerakelly/oyster" rel="noopener ugc nofollow" target="_blank">https://github.com/katerakelly/oyster</a></li><li id="c008" class="nu nv it mb b mc od mf oe mi of mm og mq oh mu nz oa ob oc bi translated"><a class="ae np" href="https://github.com/katerakelly/oyster/issues/8#issuecomment-525923243" rel="noopener ugc nofollow" target="_blank">https://github . com/katera Kelly/oyster/issues/8 # issue comment-525923243</a></li></ol></div></div>    
</body>
</html>