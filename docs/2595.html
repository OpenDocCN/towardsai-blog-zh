<html>
<head>
<title>Backpropagation and Vanishing Gradient Problems in Rnn Are Clearly Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">清楚地解释了Rnn中的反向传播和消失梯度问题</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/backpropagation-and-vanishing-gradient-problem-in-rnn-clearly-explained-efce8824971b?source=collection_archive---------0-----------------------#2022-03-06">https://pub.towardsai.net/backpropagation-and-vanishing-gradient-problem-in-rnn-clearly-explained-efce8824971b?source=collection_archive---------0-----------------------#2022-03-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ee90" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理论与代码</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/67807c5742e12fabaf7670279c2a84be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dwms1KxUa_fmMuee.jpeg"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated"><a class="ae kr" href="https://unsplash.com/@emilep" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/@emilep</a></figcaption></figure><h2 id="0a02" class="ks kt iq bd ku kv kw dn kx ky kz dp la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">介绍</h2><p id="dee6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lb lx ly lz lf ma mb mc lj md me mf mg ij bi translated">在这篇文章中，我不打算解释RNN模型的应用或直觉——事实上，我希望读者已经对它有所熟悉，以及消失梯度问题和一般的反向传播算法。我还将展示在反向传播过程中消失梯度问题的来源，强调一些经常被误解的细节。</p><p id="27bb" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">我将介绍递归神经网络的多对多架构，这意味着，例如，我们有一个字符序列作为输入，我们希望模型能够预测接下来T个时间步的接下来的字符。</p><h2 id="0daf" class="ks kt iq bd ku kv kw dn kx ky kz dp la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">正向传播</h2><p id="c01e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lb lx ly lz lf ma mb mc lj md me mf mg ij bi translated">让我们首先看看如何通过时间向前传播。为简单起见，我们假设有3个时间步长。具有3个时间步长的RNN如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/76d6abafd06169597c7159250b323fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*rUWMndBd73HYaU2hAWTIcQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片——展开的RNN(图1)</figcaption></figure><p id="e4a9" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">以及不同成分的配方:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/559369a9521a32fdb26cd51e2682d582.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*OHV0s8HmACfptADLa2HIbA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片— RNN公式(图2)</figcaption></figure><p id="4ab8" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">因此，我们有形状<em class="mo">的输入xₜ(n—例子的数量，t—时间步长，即字符或单词的数量，D —嵌入维数)</em>，存储关于形状<em class="mo">的过去的信息的隐藏状态向量hₜ(n，H —隐藏维数)</em>，形状<em class="mo"> (H，H) </em>的wₕₕ—hₜ的权重矩阵，形状<em class="mo"> (D，H) </em>的wₓₕ—xₜ的权重矩阵，以及预测形状<em class="mo">的输出yₜ的权重矩阵Wᵧₕ从输入和前一个隐藏状态开始，我们得到下一个隐藏状态hₜ和为该状态预测的输出yₜ.请注意，RNN在每个时间步使用<strong class="lq ir">相同的权重矩阵</strong> <strong class="lq ir">，而不是在不同的时间步学习不同的矩阵，这将需要大量的内存，并且效率低下，因为我们经常以非常高的时间步数结束模型。</strong></em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><h2 id="9a11" class="ks kt iq bd ku kv kw dn kx ky kz dp la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">反向传播</h2><p id="3036" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lb lx ly lz lf ma mb mc lj md me mf mg ij bi translated">在前向神经网络中，通过时间的反向传播比正常的反向传播更棘手，因为不同时间步长的参数是共享的，并且下一层中的隐藏状态取决于前一层中的隐藏状态。</p><p id="ad6a" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">查看上面的正向传播公式(<em class="mo">图2 </em>)，让我们计算不同元素的偏导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/e03c232b8b6dfcd94104284b473ca9e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Tbgr8lrlhi3MA8eJOdzIBQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片—偏导数(图3)</figcaption></figure><p id="0c45" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">让我们计算每个时间步长上所有单个损失相对于Wₓₕ的导数(其他参数可以类似地计算)，因为所有损失都被权重参数修改。首先，注意Wₓₕ是如何通过当前的隐藏状态以及之前的隐藏状态影响输出的，这些隐藏状态都依赖于Wₓₕ(红色箭头)。因此，计算L2相对于<em class="mo"> h2 </em>的偏导数变成如下图4所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/6e21368d325fa11cd586688c62e77ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*nOX5NrFAm4TB0bDMYsPgvA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片—反向传播(图4)</figcaption></figure><p id="9224" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">现在让我们看看<em class="mo">总损失，</em>是所有单项损失的总和。我们可以注意到，当计算总的<em class="mo"> L </em>相对于<em class="mo"> h2 w </em> e的导数时，在<em class="mo"> h2 </em>处有2个分量或方向(红色箭头)——第一个来自<em class="mo"> L3 </em>到<em class="mo"> h3 </em>，第二个来自<em class="mo"> L2 </em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/54baf3b5a41fa6db63a53866725efd1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*APzQdA2_w4Z9gI6168bXLw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者提供的图片—一个时间步长的反向传播(图5)</figcaption></figure><p id="86ce" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">现在让我们计算Wₓₕ相对于总损失的导数，总损失是所有单个损失的总和:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi my"><img src="../Images/16866b06ad1ad9ef8ec2c317b618758b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7eUBxrAC7c243lrFK3DavA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者提供的图片—反向传播总损失wrt Wxh(图6)</figcaption></figure><p id="15e2" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">当编码反向传播时，我们在一个循环中计算这些导数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="3e71" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">乍一看，它可能与图6 中的<em class="mo">解决方案不同。因此，为了更好地理解，让我们分析和扩展代码中发生的事情(注意，我们对红色平方的偏导数求和来计算Wₓₕ的总导数) :</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mz"><img src="../Images/160b2bceb736d31632df3f70c921c55b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O-3Dqs0imMwNlusFtly2zw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片—反向传播循环(图7)</figcaption></figure><p id="1f9b" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">现在应该更清楚了，图6 中的代码和结果是完全一样的。</p><h2 id="35c0" class="ks kt iq bd ku kv kw dn kx ky kz dp la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">消失渐变</h2><p id="d373" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lb lx ly lz lf ma mb mc lj md me mf mg ij bi translated">众所周知，rnn遭受消失梯度，这是因为作为<em class="mo"> h </em>相对于<em class="mo"> z </em> ( <em class="mo">图6 </em>)的偏导数的结果的许多Wₕₕ矩阵相乘在一起，如果Wₕₕ的最大奇异值是&lt; 1，则引发梯度消失，或者如果&gt; 1，则引发梯度爆炸(这里我们假设第一种情况)。然而，这个概念经常被误解，认为某些系数的整个梯度确实消失了。实际上，<em class="mo">总损耗</em>相对于<em class="mo"> </em> Wₓₕ的导数不会变为零，但是对于更远离<em class="mo"> Li </em>的输入，特定损耗<em class="mo"> Li </em>的梯度将为零，这在调整权重Wₓₕ时不会被考虑，但是对于正确预测<em class="mo">yi</em><em class="mo">可能比局部上下文更重要。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi na"><img src="../Images/7b6ab0a9aeccb30a7b6dca18fd11c338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8vApEMB2pfl4cqtqlGQ6Ww.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片—消失渐变dL3dWxh(图8)</figcaption></figure><p id="5db5" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">因此，红色方块中的最后一项(包含关于X₁如何影响<em class="mo"> L3 </em>的上下文)将接近于零，而蓝色方块中的前两项(包含关于X₂和X₃如何影响<em class="mo"> L3 </em>的上下文)将不会接近于零，并且该信息将被用于调整Wₓₕ试图减少<em class="mo"> L3 </em>的权重。为了澄清，如果我们假设一个6个单词的句子:“因为市场上没有食物，约翰不得不跳过晚餐。”在这种情况下，为了预测单词“晚餐”,模型需要引用“食物”。但是因为单词“晚餐”在Wₓₕ的损失的梯度对于与单词“食物”相关的总和的部分将接近于零，所以它在Wₓₕ将不被考虑，并且该模型将主要依赖于接近“晚餐”的单词来尝试调整该权重，这几乎不提供预测“晚餐”的相关信息。</p><h2 id="dced" class="ks kt iq bd ku kv kw dn kx ky kz dp la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ak">数值例子</strong></h2><p id="304d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lb lx ly lz lf ma mb mc lj md me mf mg ij bi translated">在下面的代码中，我们计算每个Wₓₕ损耗的所有Xᵢ <em class="mo"> </em>分量的梯度。例如，<em class="mo">【2】</em>包含了X₁、X₂、X₃如何影响<em class="mo"> L3、</em>的梯度，如图<em class="mo">图8 </em>所示。这样，我们就可以从数字上看到渐变消失的效果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="df43" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">让我们看看每个组件的损耗梯度<em class="mo"> L3 </em>:</p><pre class="kg kh ki kj gt nb nc nd bn ne nf bi"><span id="8140" class="ng kt iq nc b be nh ni l nj nk">display(losses[2])<br/><br/>{0: array([[ 0.0132282 ,  0.01965245,  0.00556892, -0.01311703],<br/>        [-0.00498197, -0.00740145, -0.00209735,  0.0049401 ],<br/>        [-0.00430128, -0.00639019, -0.00181079,  0.00426513]]),<br/> 1: array([[ 0.00375982,  0.01506674,  0.01860143,  0.00016598],<br/>        [-0.0030325 , -0.01215215, -0.01500307, -0.00013388],<br/>        [ 0.0080649 ,  0.03231846,  0.03990044,  0.00035604]]),<br/> 2: array([[-0.12964021, -0.36447594,  1.01880983,  0.68256384],<br/>        [ 0.05655798,  0.15900947, -0.44447492, -0.2977813 ],<br/>        [-0.02370473, -0.06664448,  0.18628953,  0.1248069 ]])}<br/><br/>display([f"component {e+1} : {np.linalg.norm(losses[2][e])}" for e in losses[2]])<br/><br/># Let's see the magnitudes of them:<br/><br/>['component 1: 0.03087853793900866',<br/> 'component 2: 0.06058764108583098',<br/> 'component 3: 1.4225029296044476']</span></pre><p id="4823" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">从上面我们可以看到，距离<em class="mo"> L3 </em>最近的X₃更新最大，而X₁和X₂对Wₓₕ更新的贡献要小得多。</p><p id="6809" class="pw-post-body-paragraph lo lp iq lq b lr mh jr lt lu mi ju lw lb mj ly lz lf mk mb mc lj ml me mf mg ij bi translated">我们还可以看到，如图8中的<em class="mo">，</em>所示，所有组件上所有损耗的梯度之和等于总梯度<em class="mo"> dWx。</em></p><pre class="kg kh ki kj gt nb nc nd bn ne nf bi"><span id="abac" class="ng kt iq nc b be nh ni l nj nk"># we can also see that if we sum all the losses across all <br/># components we get total gradient for dWx<br/>np.allclose(dWx, np.sum([losses[l][e] for l in losses for e in losses[l]], 0))<br/># Output : True</span></pre><h2 id="5bdf" class="ks kt iq bd ku kv kw dn kx ky kz dp la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">结论</h2><p id="ca64" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lb lx ly lz lf ma mb mc lj md me mf mg ij bi translated">显然，在现实中你不需要自己做反向传播，因为许多软件抽象出了反向传播的所有数学。不过，为了更好地理解这些模型，手动进行所有推导是一个很好的练习。这就是为什么您现在应该尝试对其他参数——wₕₕ、Wᵧₕ、b和xₜ.——做同样的事情</p><h2 id="768b" class="ks kt iq bd ku kv kw dn kx ky kz dp la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">参考</h2><p id="14af" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lb lx ly lz lf ma mb mc lj md me mf mg ij bi translated"><a class="ae kr" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank">http://cs231n.stanford.edu/</a>T8<a class="ae kr" href="https://jramapuram.github.io/ramblings/rnn-backrpop/" rel="noopener ugc nofollow" target="_blank">https://jramapuram.github.io/ramblings/rnn-backrpop/</a>T11】T12】https://ieeexplore.ieee.org/document/279181/</p></div></div>    
</body>
</html>