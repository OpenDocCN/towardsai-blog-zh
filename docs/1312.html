<html>
<head>
<title>Conditional Story Generation — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">条件故事生成—第1部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/conditional-story-generation-part-1-6a5a16a63744?source=collection_archive---------2-----------------------#2020-12-28">https://pub.towardsai.net/conditional-story-generation-part-1-6a5a16a63744?source=collection_archive---------2-----------------------#2020-12-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="6c5f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="8ac3" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><strong class="ak"> <em class="ko">帮助故事作者用文字生成</em> </strong></h2></div><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/b60a2b12070347d99ad7d80137f5a723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c02I8psbLASgbFcx1Gfv7g.jpeg"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">图片由<a class="ae lf" href="https://www.pexels.com/it-it/@minan1398?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">安民</a>从<a class="ae lf" href="https://www.pexels.com/it-it/foto/arte-attrezzatura-business-classico-1448709/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a></figcaption></figure><p id="bf9b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">自然语言处理(NLP)是一个非常复杂的领域，由两个主要分支组成:自然语言理解(NLU)和自然语言生成(NLG)。如果我们在谈论一个学习英语的孩子，我们会简单地称他们为阅读和写作。在NLP工作是一个令人兴奋的时刻:2017年推出的变形金刚模型极大地提高了性能，今年早些时候发布的看似全能的<a class="ae lf" href="https://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results" rel="noopener ugc nofollow" target="_blank">GPT-3带来了一波</a><a class="ae lf" href="https://www.forbes.com/sites/bernardmarr/2020/10/05/what-is-gpt-3-and-why-is-it-revolutionizing-artificial-intelligence/?sh=8af7b5e481ad" rel="noopener ugc nofollow" target="_blank">兴奋</a>。我们稍后会详细讨论这些。</p><p id="e4fa" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">首先，一个简单但有力的认识:写作并不容易。作者经常陷入寻找单词的完美组合的困境，一遍又一遍地阅读同一个句子，直到单词本身不再有意义；或者只是用尽了想法，像机关枪一样写作，直到有些东西卡住了，留下我们盯着空白的纸，好像我们突然忘记了我们想说的话，想知道一分钟前文字是如何如此毫不费力地流动的。</p><p id="50b9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">像<a class="ae lf" href="https://www.grammarly.com/" rel="noopener ugc nofollow" target="_blank">语法</a>这样的软件负责语法方面；我们希望帮助创作努力，从已经写好的无数故事中提供灵感，给出提示，帮助作者的想法发展成一个美丽的故事。</p><blockquote class="mc"><p id="3514" class="md me iq bd mf mg mh mi mj mk ml mb dk translated">“我们应该互相讲述故事”~尼尔·盖曼</p></blockquote><p id="bdce" class="pw-post-body-paragraph lg lh iq li b lj mm ka ll lm mn kd lo lp mo lr ls lt mp lv lw lx mq lz ma mb ij bi translated">为了开发真正有用的东西，我们首先必须研究、学习和理解写一个故事意味着什么以及它需要什么。我写这篇文章是为了与您分享我们到目前为止学到的东西，帮助其他有兴趣跟随我们脚步的人，并为那些有兴趣在自然语言生成和故事写作的浩瀚海洋中导航而不迷失方向的人提供一个起点。</p><p id="e02d" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">要查看我们在相关主题上所做的工作，通过个性化插图帮助儿童书籍变得更加身临其境，请查看我们之前的文章。</p><h1 id="f781" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak">自然语言生成简史:从n-grams到变形金刚</strong></h1><p id="db0a" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated"><em class="no">(尽管我尽了最大努力让它对读者友好，但这部分有点技术性——如果你不关心它是如何工作的，或者想先看看结果，可以直接跳到最后的“魔法在行动”部分)</em></p><p id="a0f3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">一个模型是如何生成文本的？一个模型必须先经过<em class="no">训练</em>，而要训练，需要大量的例子(语料库)来学习。支持无监督训练的模型常用的语料库有<a class="ae lf" href="https://yknzhu.wixsite.com/mbweb" rel="noopener ugc nofollow" target="_blank"> BookCorpus </a>(由16种不同流派的11038本未出版书籍组成的数据集)、维基百科的文章，以及来自web的web crawls如<a class="ae lf" href="https://commoncrawl.org/" rel="noopener ugc nofollow" target="_blank"> CommonCrawl </a>。即使拥有巨大的计算能力，也需要几天甚至几周的时间来学习单词之间的相关性，即概率的统计分布，这些概率将单词联系在一起并赋予它们整体意义。最重要的是，最终算法的“推理”将基于给定的文本，因此仔细选择它非常重要，因为在两个不同的文本语料库上训练的相同模型最终会给出非常不同的结果。但是，我们不要急于求成，看看我们是如何做到这一点的。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi np"><img src="../Images/94f5bd5956367ff6eb47bb3e78ad2de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*cg6xW7mlh4mtIJwFJVUatg.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">来自<a class="ae lf" href="https://infograph.venngage.com" rel="noopener ugc nofollow" target="_blank">venengage</a>的模板</figcaption></figure><h2 id="e45d" class="nq ms iq bd mt nr ns dn mx nt nu dp nb lp nv nw nd lt nx ny nf lx nz oa nh iw bi translated"><strong class="ak">统计模型</strong></h2><p id="7cb3" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated">NLP的一个最简单的方法，称为词袋，利用每个词的出现次数作为文本分类和其他任务的特征。然而，单词出现的次数并不保留其他相关信息，例如每个单词在句子中的位置及其与其他单词的关系。因此，它不适合文本生成。</p><p id="2d11" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">N元模型解决了这个问题:我们不是计算单词的出现次数，而是计算N个连续单词的出现次数。一旦N-gram被计数，生成文本只是一个简单的统计问题:给定文本中最新的N-1个单词，下一个最有可能是什么？这可以反复应用，直到一个完整的句子、段落或一本书完成。为N选择一个较高的数字会产生一个更精确的模型，但也更有可能只是在语料库的文本中复制别人写的东西。</p><blockquote class="ob oc od"><p id="f8f8" class="lg lh no li b lj lk ka ll lm ln kd lo oe lq lr ls of lu lv lw og ly lz ma mb ij bi translated"><strong class="li ja"> n-grams示例</strong></p><p id="557e" class="lg lh no li b lj lk ka ll lm ln kd lo oe lq lr ls of lu lv lw og ly lz ma mb ij bi translated"><strong class="li ja"> <em class="iq">句子:</em> </strong> <em class="iq">【你好媒体读者】</em></p><p id="d3ea" class="lg lh no li b lj lk ka ll lm ln kd lo oe lq lr ls of lu lv lw og ly lz ma mb ij bi translated"><strong class="li ja"><em class="iq">1-克:</em> </strong> <em class="iq">(你好)，(中等)，(读者)</em></p><p id="8f36" class="lg lh no li b lj lk ka ll lm ln kd lo oe lq lr ls of lu lv lw og ly lz ma mb ij bi translated"><strong class="li ja"><em class="iq">2-克:</em> </strong> <em class="iq">(你好，中等)，(中等，读者)</em></p></blockquote><p id="f1f2" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">选择下面的单词可以通过贪婪的方式(总是选择最有可能的一个)或通过抽样(根据概率分布在最有可能的单词中进行选择)。为了生成一个句子，我们可以迭代地使用这些方法中的一种，但是它经常给出次优的结果，因为最好的句子可能不仅仅是一系列最好的单词。理想情况下，我们会尝试每一种可能的单词组合，并找到最好的一个，但这并不省时。一个很好的中间解决方案是波束搜索:同时探索最有希望的路径中的多条路径(用户可以选择数量)，只有在句子结束时，得分最高的路径才会被选择，从而避免局部最小值。</p><p id="9bd6" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这最后一种方法似乎特别有助于向人类作家呈现不同的可能性:要么提出他以前没有考虑过的新想法，要么建议用其他方式来表达他已经写好的东西。</p><h2 id="3837" class="nq ms iq bd mt nr ns dn mx nt nu dp nb lp nv nw nd lt nx ny nf lx nz oa nh iw bi translated"><strong class="ak">深度学习模型:RNNs </strong></h2><p id="90e8" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated">过去十年的深度学习革命使得使用基于神经网络的模型成为可能，能够学习比n-gram更复杂的单词之间的依赖关系。</p><p id="39c9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">早在20世纪80年代，人们发明了“简单的”<a class="ae lf" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>(程序员可能会认为它们是“for”循环神经网络)来处理文本并“记住”单词之间的依赖关系。然而，还需要30年才能有足够的数据和计算能力来超越统计方法。它们存在几个局限性:梯度(反向传播的基础，深度学习的核心)在长句和文档中消失或爆炸，使训练非常不稳定。</p><p id="1e66" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">为了解决这个问题，1997年引入了<a class="ae lf" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">长短期记忆</a>(以及门控循环单元)，这是一种特殊类型的rnn，由于记忆门，它可以防止梯度消失问题，但仍然不能真正用于迁移学习(复制其他人在另一项任务中训练的模型的一部分，节省了无数的训练时间和资源)，需要为每个任务标记和专门设计的数据集。</p><p id="2ecc" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">LSTMs是<a class="ae lf" href="https://en.wikipedia.org/wiki/Seq2seq" rel="noopener ugc nofollow" target="_blank"> Seq-2-Seq </a>模型使用的编码器-解码器架构的基础，最初是为机器翻译开发的，它接受整个句子，并在“解码”和选择输出之前对其进行“编码”，从而产生更连贯和更明显的结果。</p><p id="fb54" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><a class="ae lf" href="https://medium.com/analytics-vidhya/https-medium-com-understanding-attention-mechanism-natural-language-processing-9744ab6aed6a" rel="noopener">注意力机制</a>也被发明出来，给模型一种方法来识别输入句子的哪些部分是最相关和相互依赖的。</p><h2 id="b3ae" class="nq ms iq bd mt nr ns dn mx nt nu dp nb lp nv nw nd lt nx ny nf lx nz oa nh iw bi translated"><strong class="ak">深度学习模型:变形金刚</strong></h2><p id="83b8" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated">基于编码器-解码器架构和<a class="ae lf" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力</a>(通常是<a class="ae lf" href="https://blogs.oracle.com/datascience/multi-head-self-attention-in-nlp" rel="noopener ugc nofollow" target="_blank">多头注意力</a>，并行运行几次注意力)，变形金刚于2017年问世:一个不使用RNNs的递归顺序处理，而是使用<a class="ae lf" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="noopener ugc nofollow" target="_blank">位置编码</a>的模型，以“推理”每个单词的位置，并计算注意力的相关性分数(单词彼此之间的关联程度)，非常自然地将自己借给<a class="ae lf" href="https://en.wikipedia.org/wiki/Parallel_computing" rel="noopener ugc nofollow" target="_blank">并行化</a>(这意味着可以训练具有数百亿参数的模型，如变形金刚<a class="ae lf" href="https://www.youtube.com/watch?v=S27pHKBEp30" rel="noopener ugc nofollow" target="_blank">更深入的解释可以在这里</a>找到。</p><p id="5e7c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">Transformers的主要优势是更容易训练，并且与迁移学习非常兼容，而LSTMs在处理非常长或无限长的文本序列时，或者出于某种原因，您无法在大型语料库上进行预先训练时，仍然具有优势。</p><h2 id="2fc5" class="nq ms iq bd mt nr ns dn mx nt nu dp nb lp nv nw nd lt nx ny nf lx nz oa nh iw bi translated"><strong class="ak">替代方法:GANs和VAEs </strong></h2><p id="584c" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated">生成对抗网络(GANs)和变分自动编码器(VAEs)在自动生成图像方面取得了巨大的成功，那么为什么它们在自然语言生成中没有得到普遍应用呢？从RNNs创建GANs提出了一个技术问题，该问题与生成的单词的选择的不可微性有关，并且因此不可能通过网络反向传播梯度，这使得很难训练它们。已经找到了这个问题的不同解决方案，例如利用Gumbel-Softmax近似或<a class="ae lf" href="https://arxiv.org/abs/1609.05473" rel="noopener ugc nofollow" target="_blank"> SeqGAN </a>，利用基于随机策略的强化学习模型作为生成器，具有不同程度的成功。</p><p id="07e5" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">虽然值得一提，但gan很少用于文本生成，因为它们给出的结果比变形金刚更差(<a class="ae lf" href="https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/" rel="noopener ugc nofollow" target="_blank">这里有一个更深入的解释</a>为什么由它们的发明者伊恩·古德菲勒写)，所以我们不再赘述它们。</p><h2 id="c108" class="nq ms iq bd mt nr ns dn mx nt nu dp nb lp nv nw nd lt nx ny nf lx nz oa nh iw bi translated"><strong class="ak">写故事的诸多挑战</strong></h2><p id="307c" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated">无数的课程教授有抱负的艺术家如何为一本书、一部电影或一场演出写一个有效的情节:要产生一个像样的结果，需要专业知识和想象力，这是自然语言生成领域的终极挑战。</p><p id="9dae" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">一个故事有一个情节，人物，一个场景，一种基调，一种风格，冲突和解决方案。它应该让读者感到熟悉，同时又是新的和创新的；教动；启发和娱乐，同时也是迷人的。作为一个社会，我们庆祝的大多数人都是伟大的故事讲述者:从JK罗琳到史蒂芬斯皮尔伯格，从莎士比亚到华特·迪士尼，电影院和图书馆当然充满了这样的典范。但在不太明显的背景下也可以找到例子:营销就是讲故事，甚至像史蒂夫·乔布斯和埃隆·马斯克这样的科技企业家也拥有分享他们愿景的罕见能力，通过讲述一个关于科技和我们未来的故事来激励我们。</p><blockquote class="mc"><p id="2372" class="md me iq bd mf mg mh mi mj mk ml mb dk translated"><strong class="ak">“营销不再是你做的东西，而是你讲的故事”</strong><a class="ae lf" href="https://www.sethgodin.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ak">~赛斯·戈丁</strong> </a></p></blockquote><p id="77dc" class="pw-post-body-paragraph lg lh iq li b lj mm ka ll lm mn kd lo lp mo lr ls lt mp lv lw lx mq lz ma mb ij bi translated">到目前为止，NLG算法甚至连自己写短篇小说都很困难:它们难以理解时间和因果关系(并非巧合，在测试新的NLP模型时会评估这些关系)以及长期一致性(尽管《变形金刚》基于注意力的架构有助于这一点)。</p><p id="35bc" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">为了解决这些挑战，已经提出了许多解决方案，并取得了不同程度的成功，其中包括:<a class="ae lf" href="https://arxiv.org/abs/1808.06945" rel="noopener ugc nofollow" target="_blank"> RL生成的骨架</a>来构建故事；<a class="ae lf" href="https://arxiv.org/abs/1805.04833" rel="noopener ugc nofollow" target="_blank">一个分层生成框架</a>，首先规划一个故事情节，然后基于此生成一个故事；<a class="ae lf" href="https://www.cc.gatech.edu/~riedl/pubs/aaai13.pdf" rel="noopener ugc nofollow" target="_blank">众包剧情图</a>和<a class="ae lf" href="https://ojs.aaai.org/index.php/AAAI/article/view/5536/5392" rel="noopener ugc nofollow" target="_blank">以人物为中心的方法</a>。</p><p id="3dc5" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">尽管这些模型目前有许多局限性，但我们相信，如果利用好它们的建议，以及上千本书籍和不同体裁的知识，它们会对人类作家有很大帮助。让我们专注于帮助那些在写作瓶颈中挣扎的人，要么是因为缺乏想法，要么是因为他想不出一种方式来表达他们想要的方式，并深化NLG的这一领域。</p><h2 id="aaad" class="nq ms iq bd mt nr ns dn mx nt nu dp nb lp nv nw nd lt nx ny nf lx nz oa nh iw bi translated"><strong class="ak">条件文本生成</strong></h2><p id="7633" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated"><a class="ae lf" href="https://arxiv.org/pdf/1909.03409" rel="noopener ugc nofollow" target="_blank">有条件的文本生成</a>指的是从一个语境出发并适应它而生成新文本的任务:它的语气、作者、目标、情绪、常识以及其他类似的外部因素。这意味着将大多数更高级、更有创造性的任务留给人类(我们将在下一节中看到一些例子)，但也增加了必须生成更个性化和逻辑关联的内容以达到和谐的人机交互的复杂性。</p><h2 id="4450" class="nq ms iq bd mt nr ns dn mx nt nu dp nb lp nv nw nd lt nx ny nf lx nz oa nh iw bi translated"><strong class="ak">微调的重要性</strong></h2><p id="e97f" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated">我们之前提到过，一个模型需要大量的例子，文本语料库来学习，那么我们如何在没有过多例子的情况下教会它一种特定的风格呢？</p><p id="718c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这就是微调发挥作用的地方:在一个新的、精选的语料库上快速重新训练一个已经预先训练好的模型，以专注于一项特定的任务。该模型已经学会了如何从更大的语料库中写作，现在调整自己的统计表示，足以产生更符合预期输出的结果。</p><p id="33f4" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">文本的选择是至关重要的，因为不同的材料会导致非常不同的风格，我们不希望一个现代时代的故事听起来像是莎士比亚写的(见下面的例子，基于<a class="ae lf" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">这篇Huggingface文章</a>和<a class="ae lf" href="https://minimaxir.com/2019/09/howto-gpt2/" rel="noopener ugc nofollow" target="_blank">这篇Max Woolf的博客文章</a>)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oh"><img src="../Images/53a51222374d802d1dcfb0ccfcc84005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0HW19Gzun2vGOLwO6OaLsQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">从正常的GPT-2模型生成的文本，带有incipit“你好，中等读者，这是我的智慧:“(它有点小，但它是这样写的:“如果你不知道你在做什么，我不会告诉你怎么做，但我会给你一些提示来帮助你开始”)</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oi"><img src="../Images/5ce955c2b0867dde3a26bac461f10102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wmrlxpfxsbwmArBAzBpk3w.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">由GPT-2生成的文本与之前一样根据莎士比亚的作品进行了微调</figcaption></figure><p id="a3e3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">如果作者有足够的材料用于自己的创作，也许可以根据他自己的风格对模型进行微调，使建议与所写的其他内容更加合拍(个性化文本生成)。替代方法可以是:根据故事的背景(例如，故事发生的时代)、主题(主题感知文本生成)或作者想要传达的情绪(情感故事生成)来微调模型。</p><h1 id="73a7" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">魔法在行动</h1><p id="0880" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated">传说斯蒂芬·金写下一句简短却有力的句子时只有22岁:</p><blockquote class="mc"><p id="7576" class="md me iq bd mf mg mh mi mj mk ml mb dk translated">"黑衣男子穿过沙漠逃跑，持枪歹徒紧随其后."</p></blockquote><p id="2fe0" class="pw-post-body-paragraph lg lh iq li b lj mm ka ll lm mn kd lo lp mo lr ls lt mp lv lw lx mq lz ma mb ij bi translated">这是一个史诗冒险的开始，一个跨越3000页，八卷，作者30多年努力的故事，被称为<a class="ae lf" href="https://en.wikipedia.org/wiki/The_Dark_Tower_(series)" rel="noopener ugc nofollow" target="_blank">黑暗之塔</a>。</p><p id="c7d9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">但是，如果金有一段时间没能写下第一句话之后的任何东西，因此决定继续前进，写点别的东西，那该怎么办呢？有多少伟大的故事想法被草草写在一张纸上，却没有机会发展成更多的东西，因为作者不知道如何继续，如何将它们培育成一个完整的故事？</p><p id="51d2" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这就是我们的项目出现的原因:闲聊够了，让我们看看我们已经用一些最近的算法实现了什么，以及有什么比著名的黑暗塔incipit更好的句子来开始？</p><p id="37c1" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="no">(完全披露:所有的例子都是经过精心挑选的，但我向你保证，生成的例子中有一半以上是有效的)</em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oj"><img src="../Images/146e0ebcf32038a9782163d98482fe49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWCtUJCJIuEa2T2nvRCI9w.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">剧情转折！谁开枪打了他们？他们死了吗？新人是实际主角吗？</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ok"><img src="../Images/559632c86014c5c41ea05a446472c873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kSHx5hIhnVxA_dSNFNu-qg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">也许这本书不是以幻想世界为背景，而追逐是由汽车来完成的？为什么不呢？</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ol"><img src="../Images/3c4f6e92f94db316c3335bd99ff291b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gefuaCz4nuT7raRnEo6FhA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">第二个人是谁？尽管中枪了，他还活着吗？</figcaption></figure><p id="7d07" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">正如我们所看到的，这个模型利用了现代世界的环境，因为大多数书都是在那里发生的。让我们根据莎士比亚的作品进行微调，看看莎士比亚的风格是否更适合我们心目中的故事类型:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi om"><img src="../Images/d834ba1a733238788c0daa8c4e24ad35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*XxdJtiqiFhuNDdNair6FDg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">哇，太好了！色系家伙的想法让我想起了<a class="ae lf" href="https://en.wikipedia.org/wiki/Reservoir_Dogs" rel="noopener ugc nofollow" target="_blank">落水狗</a>。</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi on"><img src="../Images/3e1556bfe526d451fc7c6aaca36425f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bw_UvNoukTgG9lfnfV5MkA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">第二句和第三句是真正的黑暗塔风格。后面是倒叙，有何不可？</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi on"><img src="../Images/0e2ff1cf3c22248985f9e3af32112213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5PchOY7ASnw_t9F45xSFw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">这个有点神秘，但它有正确的精神。</figcaption></figure><p id="1226" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">当然，你可以选择哪些想法你想保留，哪些不符合你的想法。让我们看另一个例子，即使是一个相当简单的开端，也可以有许多不同的方向:我决定写一个名叫亨利的国王的垮台(“亨利曾经是一个伟大的国王，直到……”)并寻找一些灵感:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/8315be230c3543cc1e726c581bf765fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*3GZdwxCUOVmJaWGo_ctOBQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">一个政治阴谋！是的:一个国王，一个王子，婚姻和战争，它可以工作。</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi op"><img src="../Images/eac55bb23443a1190275c1f2bfbe6892.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*Pu4dYW7HQaKeJmWC5ERQGQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">“他死了”可能是一个有点乏味的解释，但关于国王死亡和国家回避的情节有潜力。</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/1553d6ba01d687692c41900c8c516045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*QDQkGDTit93X5g7meqGAEQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">这里隐藏了一些东西，国王对奴隶的情节，虽然有点乱。</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi or"><img src="../Images/29c963b5341fb570d51709744cd04893.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*Ul_RZzBU0HIMVcflLMZjUw.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">一个女巫！或者是女巫！更像幻想，与之前的方向不同</figcaption></figure><p id="ec53" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">只需要一分钟就可以产生无数的可能性来汲取灵感，我们希望我们的工作将有助于提供重新点燃你的创造力之火的火花！</p><h1 id="c2d9" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak">第一部分结束</strong></h1><p id="df30" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated">变形金刚和新模型的引入比以往任何时候都更强大，这为帮助作家的新工具的开发提供了完美的温床。在本文中，我们简要回顾了NLP模型的历史、我们的目标、条件文本生成领域与实现它的关系，以及一些例子。我们希望到目前为止你喜欢这次旅行。更多即将推出！</p><p id="bfee" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="no">感谢您的阅读！</em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi os"><img src="../Images/4a1d45a6a47a6907fd4e369994a2e17b.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*iBazx7g8ceBKLeJ8F7iOqg.png"/></div></figure><h2 id="26dc" class="nq ms iq bd mt nr ns dn mx nt nu dp nb lp nv nw nd lt nx ny nf lx nz oa nh iw bi translated">关于Digitiamo</h2><p id="ab21" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated">Digitiamo是一家来自意大利的初创公司，专注于使用人工智能来帮助公司管理和利用他们的知识。要了解更多信息，<a class="ae lf" href="https://www.digitiamo.com/" rel="noopener ugc nofollow" target="_blank">请访问我们的</a>。</p><h2 id="1f85" class="nq ms iq bd mt nr ns dn mx nt nu dp nb lp nv nw nd lt nx ny nf lx nz oa nh iw bi translated">关于作者</h2><p id="483f" class="pw-post-body-paragraph lg lh iq li b lj nj ka ll lm nk kd lo lp nl lr ls lt nm lv lw lx nn lz ma mb ij bi translated"><a class="ae lf" href="https://medium.com/u/56f43ec01c1e?source=post_page-----576ed5f7988b--------------------------------" rel="noopener"> <em class="no">法比奥·丘萨诺</em> </a> <em class="no">是</em><a class="ae lf" href="https://www.digitiamo.com/" rel="noopener ugc nofollow" target="_blank"><em class="no">Digitiamo</em></a><em class="no">的数据科学负责人；</em> <a class="ae lf" href="https://medium.com/u/ec9f76d504e0?source=post_page-----576ed5f7988b--------------------------------" rel="noopener"> <em class="no">弗朗西斯科·福马加利</em> </a> <em class="no">是一名正在实习的有抱负的数据科学家。</em></p></div></div>    
</body>
</html>