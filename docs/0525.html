<html>
<head>
<title>Reading: MegDet — A Large Mini-Batch Object Detector, 1st Place of COCO 2017 Detection Challenge (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阅读:MegDet —大型微型批次物体探测器，COCO 2017探测挑战赛(物体探测)第一名</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f?source=collection_archive---------3-----------------------#2020-05-25">https://pub.towardsai.net/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f?source=collection_archive---------3-----------------------#2020-05-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="6d83" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">计算机视觉</h2><div class=""/><div class=""><h2 id="1f3a" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用预热学习率策略和跨GPU批处理标准化，训练时间从33小时减少到4小时</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/95eaaa7fc6b220377224c90f2b0faf58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*oBJmm6IsslOLooZIXQXg1A.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd la"> mmAP在256批次时增长更快</strong></figcaption></figure><p id="9462" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi lx translated"><span class="l ly lz ma bm mb mc md me mf di">在</span>这篇文章中，简单介绍了由北京大学和清华大学合作的<strong class="ld ja"> MegDet:一个大型微型批量天体探测器</strong>。在本文中:</p><ul class=""><li id="33c7" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated">提出了一个大型小批量对象检测器(MegDet ),以便能够使用高达256个的<strong class="ld ja">大型小批量进行训练，这样我们可以有效地利用最多<strong class="ld ja"> 128个GPU</strong>来显著缩短训练时间。</strong></li><li id="15c6" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><strong class="ld ja">提出了预热学习率策略</strong>和<strong class="ld ja">跨GPU批处理规范化</strong>，它们一起允许我们在更短的时间内成功训练大型小型批处理检测器(例如<strong class="ld ja">从33小时到4小时</strong>)，并实现更好的准确性。</li><li id="2dfd" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">MegDet是我们向<strong class="ld ja"> COCO 2017挑战赛</strong>提交(mmAP 52.5%)的支柱，在那里我们<strong class="ld ja">获得了探测任务</strong>的第一名。</li></ul><p id="a010" class="pw-post-body-paragraph lb lc iq ld b le lf ka lg lh li kd lj lk ll lm ln lo lp lq lr ls lt lu lv lw ij bi translated">这是一篇<a class="ae mu" href="https://arxiv.org/abs/1711.07240" rel="noopener ugc nofollow" target="_blank">论文</a>发表在<strong class="ld ja"> 2018 CVPR </strong>上，引用超过<strong class="ld ja"> 100次</strong>。</p><h1 id="d355" class="mv mw iq bd la mx my mz na nb nc nd ne kf nf kg ng ki nh kj ni kl nj km nk nl bi translated">概述</h1><ol class=""><li id="9f2a" class="mg mh iq ld b le nm lh nn lk no lo np ls nq lw nr mm mn mo bi translated"><strong class="ld ja">预热学习率政策</strong></li><li id="6e21" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw nr mm mn mo bi translated"><strong class="ld ja">跨GPU批量规格化(CGBN) </strong></li><li id="74b7" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw nr mm mn mo bi translated"><strong class="ld ja">实验结果</strong></li></ol><h1 id="6cfe" class="mv mw iq bd la mx my mz na nb nc nd ne kf nf kg ng ki nh kj ni kl nj km nk nl bi translated"><strong class="ak"> 1。热身学习率政策</strong></h1><h2 id="c142" class="ns mw iq bd la nt nu dn na nv nw dp ne lk nx ny ng lo nz oa ni ls ob oc nk iw bi translated">1.1.一般物体探测损失</h2><ul class=""><li id="862c" class="mg mh iq ld b le nm lh nn lk no lo np ls nq lw ml mm mn mo bi translated">通常，物体探测损失为:</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d74d106153fce03a7b6d8c3729bc660a.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*xRWcOlUy3ummZ3EWGB9_eQ.png"/></div></figure><ul class=""><li id="1da0" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated">其中<em class="oe"> N </em>为小批量，<em class="oe"> l </em> ( <em class="oe"> x </em>，<em class="oe"> w </em>)为特定任务损失，<em class="oe"> l </em> ( <em class="oe"> w </em>)为规则化损失。</li><li id="3ec8" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><em class="oe"> l </em> ( <em class="oe"> xi </em>，<em class="oe"> w </em>)是RPN预测损失，RPN包围盒回归损失，预测损失，包围盒回归损失。</li></ul><h2 id="d576" class="ns mw iq bd la nt nu dn na nv nw dp ne lk nx ny ng lo nz oa ni ls ob oc nk iw bi translated">1.2.方差等价</h2><ul class=""><li id="ab98" class="mg mh iq ld b le nm lh nn lk no lo np ls nq lw ml mm mn mo bi translated">在图像分类中，每个图像只有一个注释，并且<em class="oe"> l </em> ( <em class="oe"> x </em>，<em class="oe"> w </em>)是交叉熵的简单形式。</li><li id="e944" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><strong class="ld ja">关于对象检测，每个图像具有不同数量的框注释，导致图像之间不同的地面实况分布。</strong>在目标检测中，不同小批量之间的梯度等效假设可能不太可能出现。</li><li id="0d7b" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">与梯度等效假设不同，<strong class="ld ja">我们假设梯度的方差在<em class="oe"> k </em>步中保持不变。</strong></li><li id="2ec3" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">对于每个样品的梯度∇ <em class="oe"> l </em> ( <em class="oe"> xi </em>，<em class="oe"> w </em>)，一个<strong class="ld ja">正常小批量</strong> <em class="oe"> l </em> ( <em class="oe"> x </em>，<em class="oe"> w </em>):</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/033dac840fe99d933d47fa4130622185.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*M2kQAQdyLEhsraF2KoHnUQ.png"/></div></figure><ul class=""><li id="ec63" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated">同样，对于<strong class="ld ja">大型小批量^<em class="oe">n</em>=<em class="oe">k</em>×<em class="oe">n</em></strong>:</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/04794866315e2c46aa8cc3330636e144.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*EdCi0-8SLoH1Px3zUDYbfA.png"/></div></figure><blockquote class="oh oi oj"><p id="586e" class="lb lc oe ld b le lf ka lg lh li kd lj ok ll lm ln ol lp lq lr om lt lu lv lw ij bi translated"><em class="iq">这里我们要维护</em> <strong class="ld ja"> <em class="iq">大批量^ </em> N <em class="iq">中一次更新的方差等于</em> k <em class="iq">小批量</em> N <em class="iq">中的累计步数。</em> </strong></p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi on"><img src="../Images/9d5a3509792d211f9bc0a22a88ae405f.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*F68PmlrFx0rzfqVzN3j4Mg.png"/></div></figure><ul class=""><li id="5e95" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated">当且仅当<strong class="ld ja">^<em class="oe">r</em>=<em class="oe">k</em>×<em class="oe">r</em></strong>时，上述等式成立，其中<strong class="ld ja">为^ <em class="oe"> r </em> </strong>给出相同的线性缩放规则。</li><li id="ddad" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">经过上述推导，虽然最终的缩放规则实际上与图像分类规则相同，但给出了新的解释。</li></ul><h2 id="ae4e" class="ns mw iq bd la nt nu dn na nv nw dp ne lk nx ny ng lo nz oa ni ls ob oc nk iw bi translated">1.3.预热策略</h2><ul class=""><li id="8964" class="mg mh iq ld b le nm lh nn lk no lo np ls nq lw ml mm mn mo bi translated"><strong class="ld ja">线性渐进预热</strong>来自arXiv的另一份技术报告:<a class="ae mu" href="https://arxiv.org/abs/1706.02677" rel="noopener ugc nofollow" target="_blank">使用精确的大型迷你批次SGD:1小时训练ImageNet</a>。</li><li id="cc10" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">也就是<strong class="ld ja">学习率一开始就设置得足够小，比如<em class="oe"> r </em>。然后，在每次迭代之后，学习率以恒定的速度增加，直到到达^ <em class="oe"> r </em>。</strong></li></ul></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><h1 id="e34d" class="mv mw iq bd la mx ov mz na nb ow nd ne kf ox kg ng ki oy kj ni kl oz km nk nl bi translated"><strong class="ak"> 2。跨GPU批量归一化</strong></h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/7a4e4762bb427bc2c7211cdf316a0ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*4iNmQc8_chO9MsyYbFPwqg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd la">跨GPU批量归一化</strong></figcaption></figure><ul class=""><li id="99d0" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated">对于目标检测，检测器需要处理各种尺度的目标，因此需要更高分辨率的图像作为其输入。</li><li id="7e6f" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">对于使用800×800大小的输入，<strong class="ld ja">一个设备上可能的样本数量非常有限。</strong></li><li id="0d7f" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">因此，<strong class="ld ja">应在多个GPU</strong>之间执行批量标准化，以从更多样本中收集足够的统计数据。</li><li id="cbf0" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">给定总数为<em class="oe"> n </em>的GPU设备，首先根据分配给设备<em class="oe"> k </em>的训练样本计算和值<em class="oe"> sk </em>。</li><li id="b366" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">通过平均所有装置的总和值，我们获得当前小批量的平均值<em class="oe"> μβ </em>。该步骤需要一个<strong class="ld ja">全缩减操作</strong>。</li><li id="d8ab" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">计算每个器件的方差，得到σ <em class="oe"> β </em>。</li><li id="1dcd" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">将σ <em class="oe"> β </em>广播到各器件后，我们可以通过以下方式执行标准归一化:</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/fc177becb81f46b06f6b1abd359c56a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*gsA-ZW9IIA802ts2OTQCCg.png"/></div></figure><ul class=""><li id="7028" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated">NVIDIA集体通信库(NCCL)用于有效地执行接收和广播的所有减少操作。</li></ul></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><h1 id="8df0" class="mv mw iq bd la mx ov mz na nb ow nd ne kf ox kg ng ki oy kj ni kl oz km nk nl bi translated"><strong class="ak"> 3。实验结果</strong></h1><ul class=""><li id="bc05" class="mg mh iq ld b le nm lh nn lk no lo np ls nq lw ml mm mn mo bi translated"><strong class="ld ja">使用COCO检测数据集</strong>，分为训练、验证、测试，包含80个类别，超过250，000张图片。</li><li id="5502" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><strong class="ld ja">超过118000张训练图像</strong>用于训练，<strong class="ld ja"> 5000张验证图像</strong>用于评估。</li><li id="e892" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">ImageNet预训练<a class="ae mu" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank">T5】ResNetT7<strong class="ld ja">-50</strong>作为<strong class="ld ja">骨干网</strong>。</a></li><li id="81e0" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><a class="ae mu" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="ld ja">特征金字塔网络(FPN) </strong> </a>用作<strong class="ld ja">检测框架</strong>。</li></ul><h2 id="4af4" class="ns mw iq bd la nt nu dn na nv nw dp ne lk nx ny ng lo nz oa ni ls ob oc nk iw bi translated">3.1.大小型批量，无BN</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/aeceb5de803b7db0644ff1a96ddf80e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*vAHptY1MAhuRTjXwyDIv2A.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd la">不同小批量的比较，无BN。</strong></figcaption></figure><ul class=""><li id="d579" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated"><strong class="ld ja">对于小批量32 </strong>，培训已经开始有<strong class="ld ja">一些失败的机会</strong>，即使使用预热策略。</li><li id="0626" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">与使用16的基线相比，没有<strong class="ld ja">精度损失</strong>。</li><li id="dfac" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><strong class="ld ja">对于小批量64 </strong>，即使预热，训练<strong class="ld ja">也无法收敛。</strong> <strong class="ld ja">将学习率降低一半</strong>可以让训练到<strong class="ld ja">收敛</strong>。但是有<strong class="ld ja">明显的精度损失。</strong></li><li id="0c8e" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><strong class="ld ja">对于小批量128，训练失败</strong>，预热和学习率减半。</li><li id="2658" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">当小批量和学习率较大时，即使采用预热策略，训练也会更加困难甚至不可能。</li></ul><h2 id="cba8" class="ns mw iq bd la nt nu dn na nv nw dp ne lk nx ny ng lo nz oa ni ls ob oc nk iw bi translated">3.2.大小型批量，带CGBN</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/08d43e4c884761e249f0a4bda7c30866.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*fDEwsexKhAonnkcOCZYmxg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd la">不同小批量和BN大小的mmAP和训练时间。</strong></figcaption></figure><ul class=""><li id="3e07" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated"><strong class="ld ja">在小批量的增长过程中，准确度几乎保持在同一水平</strong>，始终优于基线(16碱基)。</li><li id="a5c6" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><strong class="ld ja">小批量越大，培训周期越短。</strong>例如，<strong class="ld ja">具有128个GPU的256个小批量实验仅用4.1小时就完成了COCO训练</strong>，这意味着与33.2小时的基线相比，加速了8倍。</li><li id="f891" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><strong class="ld ja">最佳BN大小(用于BN统计的图像数量)是32。图像太少，例如2、4或8，BN统计非常不准确</strong>，从而导致性能下降。</li><li id="e0b5" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">但是，<strong class="ld ja">增加到64，精度下降。</strong>这证明了图像分类和对象检测任务之间的不匹配。</li><li id="1214" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">还尝试了长期训练策略。较长的训练时间会略微提高准确性。</li><li id="3601" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">比如“32(长)”就比对应的(37.8 v.s. 37.3)好。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/03dc3ef7e64d005c9c73cc3b18db75b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*AW6Lzp4GnRCuqa-S1C6TnA.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd la">使用长训练策略</strong>验证16(长)和256(长)探测器的准确性</figcaption></figure><ul class=""><li id="8365" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated">256(长)在早期更差，但在最后阶段赶上了16(长)(在第二次学习速率衰减之后)。</li></ul><h2 id="1750" class="ns mw iq bd la nt nu dn na nv nw dp ne lk nx ny ng lo nz oa ni ls ob oc nk iw bi translated">3.3.可可检测挑战</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/7b5bcd840d97e7bc792279ce2c5c29f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*MEkzWzvLTps0s1QBJkboiw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd la">(增强的)MegDet对COCO数据集测试开发的结果。</strong></figcaption></figure><ul class=""><li id="baab" class="mg mh iq ld b le lf lh li lk mi lo mj ls mk lw ml mm mn mo bi translated"><strong class="ld ja">提出的MegDet集成了多种技术</strong>，包括OHEM【35】，atrous卷积(<a class="ae mu" href="https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------" rel="noopener" target="_blank"> DilatedNet </a>，<a class="ae mu" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank">deeplab v1&amp;deeplab v2</a>)【40，2】，更强的基本模型(<a class="ae mu" href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=post_page---------------------------" rel="noopener" target="_blank"> ResNeXt </a>，<a class="ae mu" href="https://towardsdatascience.com/review-senet-squeeze-and-excitation-network-winner-of-ilsvrc-2017-image-classification-a887b98b2883?source=post_page---------------------------" rel="noopener" target="_blank">SENet</a>)【38，18】，大核(<a class="ae mu" href="https://towardsdatascience.com/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2?source=post_page---------------------------" rel="noopener" target="_blank">GCN</a>)【28】，分段监督【27，34】，多样的网络结构(<a class="ae mu" href="https://towardsdatascience.com/review-maxout-network-image-classification-40ecd77f7ce4?source=post_page---------------------------" rel="noopener" target="_blank">max(如果感兴趣，请阅读那些参考文献的故事或论文。有些我没覆盖，太多了……)</a></li><li id="3a3c" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated"><strong class="ld ja">在验证集</strong>上获得50.5 mmAP，在测试开发上获得<strong class="ld ja"> 50.6 mmAP。</strong></li><li id="8d59" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">四个探测器的集合最终达到52.5 mmAP。上表汇总了COCO 2017挑战赛排行榜的参赛作品。</li><li id="af59" class="mg mh iq ld b le mp lh mq lk mr lo ms ls mt lw ml mm mn mo bi translated">下图给出了一些典型结果。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ph pi di pj bf pk"><div class="gh gi pg"><img src="../Images/d1b6e09985a05a99e05df7d4d1975953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WCbuY1BTVzIYLfLI4txBoA.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated"><strong class="bd la">可可上的MegNet</strong></figcaption></figure></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><blockquote class="oh oi oj"><p id="c61b" class="lb lc oe ld b le lf ka lg lh li kd lj ok ll lm ln ol lp lq lr om lt lu lv lw ij bi translated">在冠状病毒的日子里，这个月再写30和35个故事的挑战已经完成。让我挑战40层楼！！这是本月的第36个故事..感谢访问我的故事..</p></blockquote></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><h2 id="175b" class="ns mw iq bd la nt nu dn na nv nw dp ne lk nx ny ng lo nz oa ni ls ob oc nk iw bi translated">参考</h2><p id="7103" class="pw-post-body-paragraph lb lc iq ld b le nm ka lg lh nn kd lj lk pl lm ln lo pm lq lr ls pn lu lv lw ij bi translated">【2018 CVPR】【MegDet】<br/>论文:<a class="ae mu" href="https://arxiv.org/abs/1711.07240" rel="noopener ugc nofollow" target="_blank"> MegDet:大型迷你批量物体探测器</a></p><h2 id="6bc5" class="ns mw iq bd la nt nu dn na nv nw dp ne lk nx ny ng lo nz oa ni ls ob oc nk iw bi translated">目标检测</h2><p id="f6cc" class="pw-post-body-paragraph lb lc iq ld b le nm ka lg lh nn kd lj lk pl lm ln lo pm lq lr ls pn lu lv lw ij bi translated">[ <a class="ae mu" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------" rel="noopener">过食</a> ] [ <a class="ae mu" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a> ] [ <a class="ae mu" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------" rel="noopener">快R-CNN </a> ] [ <a class="ae mu" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快R-CNN</a>][<a class="ae mu" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae mu" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a>][<a class="ae mu" href="https://towardsdatascience.com/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=post_page---------------------------" rel="noopener" target="_blank">CRAFT</a>][<a class="ae mu" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank">R-FCN</a>][<a class="ae mu" href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------" rel="noopener" target="_blank">ION</a>][<a class="ae mu" href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------" rel="noopener" target="_blank">multipath ne [</a><a class="ae mu" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------" rel="noopener">TDM</a>][<a class="ae mu" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae mu" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>][<a class="ae mu" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolov 1</a>][<a class="ae mu" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2/yolo 9000</a>][<a class="ae mu" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolov 3</a>][<a class="ae mu" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank">FPN</a>][<a class="ae mu" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank">retina net</a>[<a class="ae mu" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank">DCN/DCN v1</a>[<a class="ae mu" href="https://medium.com/@sh.tsang/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864" rel="noopener">级联R-1</a></p><h2 id="ceb4" class="ns mw iq bd la nt nu dn na nv nw dp ne lk nx ny ng lo nz oa ni ls ob oc nk iw bi translated"><a class="ae mu" href="https://medium.com/@sh.tsang/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我之前的其他阅读材料</a></h2></div></div>    
</body>
</html>