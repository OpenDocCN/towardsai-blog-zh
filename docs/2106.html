<html>
<head>
<title>Sentence Correction using Attention-based Sequence-to-Sequence Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于注意的序列对序列模型的句子校正</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/sentence-correction-using-attention-based-sequence-to-sequence-model-95cf21cdacf?source=collection_archive---------1-----------------------#2021-08-19">https://pub.towardsai.net/sentence-correction-using-attention-based-sequence-to-sequence-model-95cf21cdacf?source=collection_archive---------1-----------------------#2021-08-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b05d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/d3617d4acbcf9ba17b52acac85d6ac91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ogI-j2Vcm4w4Avj9M_yuKg.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">照片由<a class="ae ko" href="https://unsplash.com/@dimhou?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Dim侯</a>在<a class="ae ko" href="https://unsplash.com/s/photos/text?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><h1 id="4b6d" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">概观</h1><p id="d380" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">社交媒体总是有自己的规则。例如，首字母缩写词和缩写词已经成为社交平台上独特语言的一部分。如果你一直在观察社交媒体的对话，你可能会遇到大量的行话或首字母缩略词，让你需要一段时间才能弄明白。像TFW、TBH和LMK这样的社交媒体首字母缩略词很随意地出现在评论、标题和人们之间的对话中。语法错误是原始文本中经常发现的另一个大问题。无论语言是口语还是书面语，自然语言处理都使用人工智能来获取现实世界的输入，对其进行处理，并以计算机可以理解的方式理解它。NLP使计算机能够像人类一样理解自然语言。但是在它被任何基于人工智能的系统使用之前，它需要被转换成一种干净的格式。数据预处理包括准备和清理文本数据，以便机器能够对其进行分析。预处理将数据置于可工作的形式，并突出算法可以处理的文本中的特征。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ml"><img src="../Images/ff8adaec2b30cca5e1bdb2963daeecc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_zJ0QPSIECJTW68iwsgQTQ.png"/></div></div></figure><p id="8d4d" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">深度学习特别是自然语言处理的最新发展促使研究人员将句子纠正技术应用于解决机器翻译、自动语法检查等挑战性问题。在这个项目中，我们使用基于编码器-解码器的序列到序列递归神经网络和长短期记忆单元来解决英语句子校正的问题。通过将神经机器翻译模型应用于语法纠正，我们可以纠正更多我们在写作时可能犯的语法错误。</p><h1 id="049f" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">商业问题</h1><p id="623f" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">这个项目是一个大型文本数据库项目的子模块，其目标是创建一个系统，将原始的野生文本转换为标准的英语文本，同时保留文本的语义。</p><p id="9c6a" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">像<a class="ae ko" href="https://www.google.com/docs/about/" rel="noopener ugc nofollow" target="_blank"> Google Docs </a>这样的应用程序使用神经机器翻译来纠正语法错误。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mv"><img src="../Images/b680ee1c93f8966df26441b5e109599c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4N4zbzTM6oWTJGQy8B-HIg.gif"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">图片作者:<a class="ae ko" href="https://cloud.google.com/" rel="noopener ugc nofollow" target="_blank">谷歌云</a></figcaption></figure><h1 id="1cad" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">问题陈述:深度学习视角</h1><p id="3bef" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">该项目旨在创建一种预处理方法，用于将包含可能错误或通配符/互联网首字母缩写词/文本缩写词/消息缩写词的原始文本转换为标准英语格式的干净文本。我们的系统目标是将损坏的输入句子转换成目标句子，输出的目标是保留输入文本的潜在属性(情感、命名实体等)。)但在某种程度上发生了变异，将这些属性嵌入到其他自然语言处理系统所熟悉的表示中。</p><p id="8bde" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">文献调查显示，相关工作已在这一领域使用各种机制，从基于规则的模型到随机模型。隐马尔可夫模型已经显示出捕捉自然英语语言的许多高级动态的强大能力；然而，这种模型在模拟关键的长期依赖关系时会崩溃。另一方面，递归神经网络和LSTM更擅长捕捉这些长期动态。因此，在我们的工作中，我们转向最近神经网络在自然语言处理领域的成功来获得模型灵感。</p><h2 id="5bb5" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated"><strong class="ak">性能指标:</strong></h2><p id="0ce0" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">以交叉熵作为损失函数来衡量性能，由下式给出:</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d4cc5cf9c01192a28c2561626b3fc3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*yWbUtawqYpE7s3euyoRdKg.png"/></div></figure><h1 id="dd2e" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">数据集详细信息</h1><p id="0937" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lp jd">数据集1: </strong> <a class="ae ko" href="https://www.comp.nus.edu.sg/~nlp/corpora.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lp jd">国大社交媒体文本规范化与翻译语料库</strong> </a></p><p id="3c73" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">对相关文献的详细回顾产生了一个由新加坡国立大学的2000篇文章组成的英语语料库。语料库是为社交媒体文本规范化和翻译而创建的。它是通过从新加坡国立大学英语短信语料库中随机选择2000条消息来构建的。这些信息首先被规范化成正式的英语，然后被翻译成正式的汉语。从我们的评论来看，这似乎是唯一公开可用的文本规范化语料库。</p><p id="e5ee" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">该数据集包含社交媒体文本及其规范化文本和规范化文本的中文翻译。对于我们的问题，我们只需要社交媒体文本及其规范化的英语文本。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ni"><img src="../Images/2a3e15f88dbeeeaa68f93d96a86211f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UdQSTWUgIACdLlJOPwx4YQ.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">样本数据集1</figcaption></figure><p id="8da8" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd">数据集2: </strong> <a class="ae ko" href="https://sites.google.com/site/naistlang8corpora/" rel="noopener ugc nofollow" target="_blank"> <strong class="lp jd"> NAIST Lang-8学习者语料库</strong> </a></p><p id="dd37" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">该语料库包含从Lang-8中提取的英语学习者的文本。它有29，012个活跃用户写的100，051个英语条目，并且包括自动时态/体注释。</p><p id="74d5" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">提取数据集1的2000个数据点和数据集2的100000个数据点，合并得到最终数据集。最终数据集包含102000个数据点。数据集包含165个重复行，删除后产生101835个数据点。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nj"><img src="../Images/26a02ea0fc5dee5a692b7d152caafd98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQX9nNaMYQM993VL2Xl6Zg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">样本数据集2</figcaption></figure><h1 id="6675" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">探索性数据分析</h1><p id="4d6a" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">在统计学中，探索性数据分析是一种分析数据集以总结其主要特征的方法，通常使用统计图形和其他数据可视化方法。</p><h2 id="de25" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">1.字符级的输入文本可视化</h2><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nk"><img src="../Images/3782d54df59512b8897e266b5c1dc5ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45Qm9Dcy9iLVpKnVbhPJrA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">每个输入文本中的字符数</figcaption></figure><h2 id="a3c4" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">观察结果:</h2><ul class=""><li id="325d" class="nl nm it lp b lq lr lu lv ly nn mc no mg np mk nq nr ns nt bi translated">输入文本的最大字符长度:550</li><li id="48cf" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">输入文本中字符的平均长度:61</li><li id="14c2" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">输入文本中字符的最小长度:2</li><li id="45b2" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">文本的大部分是25到200个字符(包括空格)。</li><li id="7672" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">一小部分文本的字符长度大于302。</li><li id="715c" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第90百分位是108，而第100百分位是550。差别巨大。</li><li id="e18b" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第99百分位是189，而第100百分位是550。</li><li id="1a49" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第99.8百分位是260，而第99.9百分位是302。差别巨大。因此输入文本的最大长度可以被认为是260。</li></ul><h2 id="a107" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">2.单词级的输入文本可视化</h2><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nk"><img src="../Images/49a239de66e814988c82cd585a210917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKPmGe6G5_iSxhkoVxiauQ.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">每个输入文本中的字数</figcaption></figure><h2 id="1ef4" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated"><strong class="ak">观察值:</strong></h2><ul class=""><li id="0cca" class="nl nm it lp b lq lr lu lv ly nn mc no mg np mk nq nr ns nt bi translated">输入文本中的最大长度单词:125</li><li id="c78a" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">输入文本中单词的平均长度:13</li><li id="d928" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">输入文本中单词的最小长度:1</li><li id="5db3" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">正文的大部分是10到50个单词。</li><li id="2623" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">很少一部分文字的字数超过60个。</li><li id="1671" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第90百分位是23，而第100百分位是125，差别很大。</li><li id="8af7" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第99百分位是39，而第100百分位是125。</li><li id="4363" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第99.9百分位是63，而第100百分位是125。差别巨大。因此，输入文本的最大单词可以被认为是63。</li></ul><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nz"><img src="../Images/8b98974d2612380a0e95a0b40dc31d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBdl1gOiNc41ARBsq--EWg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">输入文本单词云</figcaption></figure><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oa"><img src="../Images/f153f8de0412ffa2739801df3278cefd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xe0lEe3jLgkjkD3VUIO-wg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">输入文本位置标签计数(前20位)</figcaption></figure><h2 id="80d3" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">3.在字符级别输出文本可视化</h2><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nk"><img src="../Images/bfcc6b7246a7453599fe2c9b2ecef5a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FkFaD1wQQ7zNnnHpwowTtA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">每个输出文本中的字符数</figcaption></figure><h2 id="f0e2" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated"><strong class="ak">观察结果:</strong></h2><ul class=""><li id="9c35" class="nl nm it lp b lq lr lu lv ly nn mc no mg np mk nq nr ns nt bi translated">输出文本的最大字符长度:637</li><li id="f5d9" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">输出文本中字符的平均长度:64</li><li id="aad6" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">输出文本中的最小字符长度:1</li><li id="04a6" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">文本的大部分是27到250个字符(包括空格)。</li><li id="912e" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">一小部分文本的字符长度大于262。</li><li id="4371" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第90百分位是111，而第100百分位是637。差别巨大。</li><li id="8743" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第99百分位是193，而第100百分位是637。</li><li id="1154" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第99.8百分位是262，而第99.9百分位是303。很少有句子的长度超过262。因此，输出文本的最大长度可以认为是262。</li></ul><h2 id="20ce" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">4.在单词级别输出文本可视化</h2><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nk"><img src="../Images/05ba2f911f89d0aab9718ac9e57873a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8lI_fvIbNkSbYKYFCjMIJw.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">每个输出文本中的字数</figcaption></figure><h2 id="c1dd" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated"><strong class="ak">观察结果:</strong></h2><ul class=""><li id="133b" class="nl nm it lp b lq lr lu lv ly nn mc no mg np mk nq nr ns nt bi translated">输出文本中最大长度单词:131</li><li id="f977" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">输出文本中单词的平均长度:13</li><li id="789e" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">输出文本中单词的最小长度:1</li><li id="2575" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">正文的大部分是10到50个单词。</li><li id="13e5" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">一小部分文本的字符长度大于50。</li><li id="284a" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第90百分位是23，而第100百分位是131。差别巨大。</li><li id="d390" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第99百分位是40，而第100百分位是131。</li><li id="bb82" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">第99.9百分位是63，而第100百分位是131。因此，输出文本的最大字数可以被认为是63。</li></ul><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nz"><img src="../Images/253d6b4388cfb605a8c710c635a619d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Br8Kcyz-1ReDQviSsno4pw.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">输出文本单词云</figcaption></figure><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oa"><img src="../Images/8da7ad709731359ec213285e8794f50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SV3xayi_XB548N-K0eV4UA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">输出文本位置标签计数(前20名)</figcaption></figure><h2 id="ee6c" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">5.在字符级别识别异常值</h2><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/a777743e1e8b3970b5bb20c87812864e.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*Jicw52yNkRQzOwnBbxB52Q.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">输出文本中字符的频率</figcaption></figure><h2 id="bbd0" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">观察结果:</h2><ul class=""><li id="7169" class="nl nm it lp b lq lr lu lv ly nn mc no mg np mk nq nr ns nt bi translated">很少有句子的最小字符数只有1，并且这种字符数少于3。这样的点可以被认为是异常值，可以被去除。</li></ul><h2 id="4f5d" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">6.在单词级别识别异常值</h2><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oc"><img src="../Images/48f27e7c9c217e4512c52de6c18f83b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GVjx_11Zeb-skfibyfZVxw.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">输入文本中前20个单词的频率</figcaption></figure><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi od"><img src="../Images/bbb43ecb67c8688b1e85068a928fa0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0hl4JB96ksg2qckYUlOz6Q.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">输出文本中前20个单词的频率</figcaption></figure><h2 id="0836" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">观察结果:</h2><ul class=""><li id="b58e" class="nl nm it lp b lq lr lu lv ly nn mc no mg np mk nq nr ns nt bi translated">很少有句子的最小单词数只有1，而许多这样的单词数也只有1。这样的点可以被认为是异常值，可以被去除。</li></ul><h1 id="0730" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">预处理</h1><p id="754c" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">数据预处理是建立任何基于机器学习的模型的基本步骤。它通过消除冗余、不必要的数据，帮助我们将原始数据转换成可读的格式。这样的数据只能应用于建立任何模型。还通过应用某些数据清理步骤对可用数据进行预处理。这些步骤包括删除基本的非字母和数字。文本不会转换为小写，因为某些首字母缩略词或字母可能代表互联网速记文本中流行的表达式或短语。</p><h2 id="436e" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">检查缩写</h2><p id="ab0d" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">原始数据尤其是互联网文本通常包含首字母缩写词和缩略语。这样的词往往代表一些隐藏的意思或短语。我们数据集的输入和输出文本也包含这样的缩写。输入和输出文本中最常见的20个缩写如下所示。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oe"><img src="../Images/e597f6835103c4081d5968255cf262a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CnW4jeMvBE3UpZ2AXuu-yA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">输入文本中最常见的20个缩写</figcaption></figure><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oe"><img src="../Images/acd8c7577d12d9e88a530efa1991a639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9bHBv8G95PlNlQSq1IwaGQ.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">输出文本中最常见的20个缩写</figcaption></figure><h2 id="88ea" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">观察结果:</h2><ul class=""><li id="d1c1" class="nl nm it lp b lq lr lu lv ly nn mc no mg np mk nq nr ns nt bi translated">在输入文本中总共发现75个缩写，在输出文本中总共发现64个缩写。</li></ul><h1 id="f093" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">数据准备</h1><h2 id="b5c5" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">1.在字符级别</h2><p id="1633" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">在建立字符级的基线模型之前，我们已经根据字符长度过滤了句子。根据EDA，输入文本的最大长度可以被认为是260，输出文本的最大长度可以被认为是262。但是由于计算的限制，我们选择输入和输出文本的最大长度为110。</p><p id="6a33" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">下一步，在过滤句子之后，我们将数据按照9:1的比例分成训练和测试。之后，创建一个tokenizer对象，将char_level设置为True，并根据训练数据进行调整。这导致创建大小为75的输入词汇表和大小为68的输出词汇表。</p><p id="6e26" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">还创建了一个训练和测试数据加载器，批量大小为512，最大长度为110。</p><figure class="mm mn mo mp gt kd"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="57ee" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">2.在单词级别</h2><p id="df54" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">在构建单词级别的基线模型之前，我们已经基于单词长度过滤了句子。根据EDA，输入文本的最大长度可以被认为是63，输出文本的最大长度可以被认为是63。但是由于计算的限制，我们选择输入和输出文本的最大长度为25。</p><p id="614c" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">下一步，在过滤句子之后，我们将数据按照9:1的比例分成训练和测试。之后，创建一个tokenizer对象，将char_level设置为False，并根据训练数据进行调整。这导致创建大小为35510的输入词汇表和大小为29350的输出词汇表。</p><p id="1468" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">还创建了一个训练和测试数据加载器，批量大小为512，最大长度为25。</p><figure class="mm mn mo mp gt kd"><div class="bz fp l di"><div class="of og l"/></div></figure><h1 id="c99e" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">基线模型</h1><p id="b967" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">假设我们的原始表示是字符，我们现在可以转向对英语本身建模的问题。我们选择使用简单的编码器-解码器作为我们的基线模型，因为最近的文献已经显示了它在捕捉流利英语的许多动态方面的有效性。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d2a5d96ba2ef961fe09639038ed328b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*m4pqJh3GwXcReULBM5Bdpw.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">简单编码器-解码器模型</figcaption></figure><figure class="mm mn mo mp gt kd"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="1d51" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">在字符级别</h2><p id="45e7" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">我们选择利用长短期记忆(LSTM)单元作为神经网络中的隐藏单元，因为LSTM单元具有特殊的“门控”机制，允许它们在许多时间步长上传播误差信号；我们推断，考虑到我们决定对字符而不是单词建模，这种选择是合乎逻辑的，这导致了作为字符链的非常长的句子分解，以及更细粒度、更难以捕捉的字符级语言动态。</p><ol class=""><li id="07b3" class="nl nm it lp b lq mq lu mr ly oi mc oj mg ok mk ol nr ns nt bi translated"><strong class="lp jd">可训练嵌入的1层LSTM:</strong>该模型通过保持编码器和解码器的嵌入层为可训练的，嵌入维数为300来训练。编码器和解码器单元被选择为100。这导致loss和val_loss都是nan。</li></ol><p id="2890" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">字符级模型在翻译过程中引入了更多的变化，其中字符通常被映射到空格。因为源单词和目标单词经常不对齐，所以字符的映射会导致无意义的结果。</p><h2 id="839a" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">在单词级别</h2><p id="50f7" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">我们选择利用门控循环单元(GRU)、长短期记忆(LSTM)单元和双向LSTM单元作为我们的神经网络中的隐藏单元，在单词级别，每个单元都具有可训练的嵌入层、手套嵌入和快速文本嵌入。手套嵌入加载有长度为400000的单词向量和300的嵌入维度。Fasttext嵌入加载了长度为999995的单词向量和300的嵌入维数。可训练嵌入层维度也被选择为300，并且编码器和解码器单元被选择为100。每个模型用adam优化器的默认值训练20个时期。</p><ol class=""><li id="4dfb" class="nl nm it lp b lq mq lu mr ly oi mc oj mg ok mk ol nr ns nt bi translated"><strong class="lp jd">具有可训练嵌入的1层GRU:</strong>该模型导致train loss为1.38，val_loss为1.69。该模型实现了0.44的平均训练BLEU分数和0.41的平均测试BLEU分数。</li><li id="24d4" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有手套嵌入的1层GRU:</strong>该模型导致1.59的train损失和1.67的val_loss。该模型已经实现了0.45的平均训练BLEU分数和0.43的平均测试BLEU分数。</li><li id="b34e" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有快速文本嵌入的1层GRU:</strong>该模型导致1.64的train损失和1.69的val_loss。该模型实现了0.46的平均训练BLEU分数和0.44的平均测试BLEU分数。</li><li id="76fc" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有可训练嵌入的1层LSTM:</strong>该模型导致train loss为1.38，val_loss为1.67。该模型实现了0.44的平均训练BLEU分数和0.44的平均测试BLEU分数。</li><li id="aecd" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有手套嵌入的1层LSTM:</strong>该模型导致1.63的train损失和1.68的val_loss。该模型实现了0.44的平均训练BLEU分数和0.43的平均测试BLEU分数。</li><li id="d1d9" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有快速文本嵌入的1层LSTM:</strong>该模型导致1.71的train损失和1.72的val_loss。该模型实现了0.43的平均训练BLEU分数和0.46的平均测试BLEU分数。</li><li id="4c5b" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有可训练嵌入的1层双向LSTM:</strong>该模型导致1.18的train损耗和1.54的val_loss。该模型实现了0.43的平均训练BLEU分数和0.42的平均测试BLEU分数。</li><li id="e035" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有手套嵌入的1层双向LSTM:</strong>该模型导致1.26的train损耗和1.44的val_loss。该模型实现了0.42的平均训练BLEU分数和0.42的平均测试BLEU分数。</li><li id="f2d7" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有快速文本嵌入的1层双向LSTM:</strong>该模型导致1.38的列车损失和1.46的val_loss。该模型实现了0.42的平均训练BLEU分数和0.41的平均测试BLEU分数。</li></ol><p id="bd32" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">我们基线的单词级模型在翻译中取得了更好、更有意义的结果。</p><h1 id="a312" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">基于注意力的模型</h1><p id="e605" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">进一步，为了提高模型的性能，我们选择利用基于注意力的编码器-解码器作为我们的神经翻译模型。注意模型或注意机制是神经网络的输入处理技术，允许网络专注于复杂输入的特定方面，一次一个，直到整个数据集被分类。目标是将复杂的任务分解成更小的注意力区域，然后依次处理。这类似于人类的大脑如何通过将一个新问题分成更简单的任务并逐个解决它们来解决它。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div class="gh gi om"><img src="../Images/621ee2cb679b7067173fa49053cdee02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*AACAispNmaLw085tAyj1yQ.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">编码器-解码器递归神经网络模型。<br/>摘自《使用统计机器翻译的RNN编码器-解码器学习短语表示》</figcaption></figure><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div class="gh gi on"><img src="../Images/58526a288bbe414819a9c11b597d7e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*K_Ix1HbNfpcETEN5GRgC1g.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">注意力示例<br/>摘自《联合学习对齐和翻译的神经机器翻译》，2015。</figcaption></figure><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/80893b75d97a7ef0232a5a6e0c10cb78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*ch7CZi3_6G-a5mHWiqO40A.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">将隐藏状态作为输入提供给解码器<br/>摘自《基于注意力的神经机器翻译的有效方法》，2015年。</figcaption></figure><figure class="mm mn mo mp gt kd"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="0093" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">在字符级别</h2><p id="3488" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">我们选择利用长短期记忆(LSTM)单元作为我们基于注意力的神经网络中的隐藏单元。在注意力层，我们使用了“点”评分函数，其他函数如“一般”和“串联”也可以进行实验。该模型使用嵌入维数300和批量大小512进行训练。编码器单元、解码器单元和注意单元被选择为64。使用默认值的adam优化器对该模型进行总共80个时期的训练。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div class="gh gi op"><img src="../Images/d7702ec855f2f233d2d28fbe8074da05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*bW-ZxOxUSTZNepEdYA9kjQ.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">注意力评分功能</figcaption></figure><ol class=""><li id="4774" class="nl nm it lp b lq mq lu mr ly oi mc oj mg ok mk ol nr ns nt bi translated"><strong class="lp jd">具有可训练嵌入的1层LSTM:</strong>通过保持编码器和解码器的嵌入层为可训练的来训练该模型。这导致了0.354的损失和0.359的val_loss。该模型在80个历元时达到了0.01的平均训练BLEU分数和0.03的平均测试BLEU分数。</li></ol><p id="94c1" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">基于注意力的字符级模型在翻译过程中引入了更多的变化，其中字符通常被映射到空格。因为源单词和目标单词经常不对齐，所以字符的映射会导致不准确的结果。</p><h2 id="12e0" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">在单词级别</h2><p id="f87d" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">我们选择利用1层和2层长短期记忆(LSTM)单元和1层双向LSTM单元作为我们的基于注意力的神经网络中的隐藏单元，在单词级别。使用的嵌入是可训练嵌入层和快速文本嵌入。Fasttext嵌入加载了长度为999995的单词向量和300的嵌入维数。可训练嵌入层维度也被选择为300。在关注层，我们使用了“点”评分功能。使用adam优化器和512的批量大小为不同的时期训练每个模型。</p><ol class=""><li id="ad42" class="nl nm it lp b lq mq lu mr ly oi mc oj mg ok mk ol nr ns nt bi translated"><strong class="lp jd">具有可训练嵌入的1层LSTM:</strong>编码器单元、解码器单元和注意单元被选择为64。<strong class="lp jd"> </strong>该模型经过100个历元的训练，得到train loss为0.26，val_loss为0.27。该模型实现了0.74的平均训练BLEU分数和0.44的平均测试BLEU分数。</li><li id="1bfb" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有快速文本嵌入的1层LSTM:</strong>编码器单元、解码器单元和注意单元被选择为256。该模型被训练110个时期，并导致0.197的训练损失和0.194的val_loss。该模型在90°历元达到了0.69的平均训练BLEU分数和0.49的平均测试BLEU分数。</li><li id="7436" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有可训练嵌入的2层LSTM:</strong>编码器单元、解码器单元和注意单元被选择为256。该模型被训练70个时期，并导致0.198的训练损失和0.193的val_loss。该模型在70个历元上取得了0.81的平均训练BLEU分数和0.47的平均测试BLEU分数。</li><li id="9e6e" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk ol nr ns nt bi translated"><strong class="lp jd">具有快速文本嵌入的1层双向LSTM:</strong>编码器单元、解码器单元和注意单元被选择为256。该模型被训练80个时期，并导致0.12的训练损失和0.10的val_loss。该模型在80个历元时达到了0.84的平均训练BLEU分数和0.50的平均测试BLEU分数。</li></ol><p id="7075" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">与简单的编码器-解码器模型相比，基于注意力的单词级模型在翻译中取得了更好、更有意义的结果。</p><h1 id="6bd1" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">预测结果</h1><p id="b1ba" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">下面列出了使用注意力模型(1层-双向LSTM和快速文本嵌入)的一些最终预测结果。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oq"><img src="../Images/053f1e04090a88e38cd6ca74c0eb4b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eoy7ScHuLI-1T4Xzgl5Wrg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">使用注意力模型训练集结果(1层-双向LSTM和快速文本嵌入)</figcaption></figure><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi or"><img src="../Images/02f32c369afa0d3ab9e3948015d19c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YG9LmN_66t8KtNL7-aCqJA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">使用注意力模型的测试集结果(1层-双向LSTM和快速文本嵌入)</figcaption></figure><h1 id="89cb" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">比较摘要</h1><h2 id="6119" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">基线模型</h2><p id="72b8" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lp jd">概要:</strong></p><p id="89b3" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">下面列出了具有最佳训练BLEU分数和最佳测试BLEU分数的每个基线模型的比较总结。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi os"><img src="../Images/889e9aa1a7a2833636a08d5cb51fdf36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TXUQZV4GhkALFRDJFAt8Xw.png"/></div></div></figure><p id="46bf" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> BLEU评分:</strong></p><p id="6908" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">具有最佳训练BLEU分数和最佳测试BLEU分数的每个基线模型的比较条形图如下所示。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ot"><img src="../Images/571c36d5443a85823b14c87b304bf5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QewfN7CyOOB_vD-EjT6hwA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">基线模型的BLEU分数</figcaption></figure><p id="a528" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd">损失值:</strong></p><p id="55ef" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">各基线模型与列车损失值和测试损失值的对比柱状图如下所示。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ot"><img src="../Images/261367d6d3abdc97a7916334d8f35ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MPMbkYXAciKi5J_DnMOsJg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">基线模型的损失值</figcaption></figure><h2 id="faba" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">注意力模型</h2><p id="877d" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lp jd">总结:</strong></p><p id="73af" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">下面列出了在不同时期值具有最佳训练BLEU分数和最佳测试BLEU分数的每个基于注意力的模型的比较总结。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ou"><img src="../Images/a580b0c155f4730afdbba0e446a972fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0OMpP9_NFICoJzQ2D8uGjA.png"/></div></div></figure><p id="e6ac" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> BLEU评分:</strong></p><p id="b249" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">具有最佳训练BLEU分数和最佳测试BLEU分数的每个注意力模型的比较条形图如下所示。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ni"><img src="../Images/b82c33da60d7a01df612b288976dfd70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oCrEP6oe8KExY6kdg-2mTQ.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">注意力模型的BLEU评分</figcaption></figure><p id="8f75" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd">损失值:</strong></p><p id="a755" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">每个注意力模型与训练损失和测试损失值的对比条形图如下所示。</p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ni"><img src="../Images/86a752f228b3e09bf04505141955d1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tfz06z60SXU3mYuMlHPfdg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">注意力模型的损失值</figcaption></figure><h1 id="8bbd" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">结论和未来工作</h1><ul class=""><li id="a0b0" class="nl nm it lp b lq lr lu lv ly nn mc no mg np mk nq nr ns nt bi translated">我们发现使用字符级模型在翻译过程中引入了更多的变化，其中字符通常被映射到空格。因为源单词和目标单词经常不对齐，所以字符的映射会导致不准确的结果。因此，我们基线的单词级模型在翻译中取得了更好、更有意义的结果。</li><li id="2618" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">单词级基线模型已经显示了其在捕捉流利英语文本的许多动态方面的有效性。另一方面，基于注意力的模型被证明更善于捕捉这些长期的动态。</li><li id="53c9" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">在未来的工作中，基于注意力的模型的不同变体可以用于改进模型性能。</li><li id="e40f" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">可以采用更健壮的模型，如Transformers，它利用注意力来提高这些模型的训练速度。</li><li id="7b29" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated">最后，我们非常有兴趣探索我们的文本校正模型在其他语言中的应用，并评估其性能；如果文本校正方案可以扩展到其他语言，该模型可以是机器翻译管道中重要的预处理和后处理步骤。</li></ul><h1 id="abae" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">参考</h1><ul class=""><li id="b996" class="nl nm it lp b lq lr lu lv ly nn mc no mg np mk nq nr ns nt bi translated"><a class="ae ko" href="https://cs224d.stanford.edu/reports/Lewis.pdf" rel="noopener ugc nofollow" target="_blank">使用递归神经网络的句子校正</a></li><li id="bacb" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated"><a class="ae ko" href="https://arxiv.org/abs/1406.1078" rel="noopener ugc nofollow" target="_blank">使用用于统计机器翻译的RNN编码器-解码器学习短语表示</a></li><li id="1518" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated"><a class="ae ko" href="https://arxiv.org/abs/1508.04025" rel="noopener ugc nofollow" target="_blank">基于注意力的神经机器翻译的有效方法</a></li><li id="6350" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated"><a class="ae ko" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">联合学习对齐和翻译的神经机器翻译</a></li><li id="6c8f" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated"><a class="ae ko" href="https://sites.google.com/site/naistlang8corpora/" rel="noopener ugc nofollow" target="_blank"> NAIST Lang-8学习者语料库</a></li><li id="998e" class="nl nm it lp b lq nu lu nv ly nw mc nx mg ny mk nq nr ns nt bi translated"><a class="ae ko" href="https://www.comp.nus.edu.sg/~nlp/corpora.html" rel="noopener ugc nofollow" target="_blank">新加坡国立大学自然语言处理组</a></li></ul></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><h2 id="b018" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">Github:</h2><p id="ccd0" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><a class="ae ko" href="https://github.com/shekhartz/sentence-correction" rel="noopener ugc nofollow" target="_blank">https://github.com/shekhartz/sentence-correction</a></p><h2 id="af4f" class="mw kq it bd kr mx my dn kv mz na dp kz ly nb nc ld mc nd ne lh mg nf ng ll iz bi translated">LinkedIn:</h2><p id="16ff" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><a class="ae ko" href="https://www.linkedin.com/in/shekhartz/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/shekhartz/</a></p></div></div>    
</body>
</html>