<html>
<head>
<title>Machine Learning: Python Linear Regression Estimator Using Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:使用梯度下降的Python线性回归估计器</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/machine-leaning-python-linear-regression-estimator-using-gradient-descent-b0b2c496e463?source=collection_archive---------2-----------------------#2018-12-24">https://pub.towardsai.net/machine-leaning-python-linear-regression-estimator-using-gradient-descent-b0b2c496e463?source=collection_archive---------2-----------------------#2018-12-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2024" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">Python中的简单线性回归估计器| <a class="ae ep" href="https://www.towardsai.net" rel="noopener ugc nofollow" target="_blank">走向AI </a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div class="gh gi jz"><img src="../Images/878dbcc82c1ef25c22c12b0947475d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*DXnbLSTpfII2V3g63IzbLg.png"/></div></figure><p id="1701" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将描述如何构建一个简单的python估算器，以使用梯度下降法执行线性回归。假设我们有一个包含单个要素(X)和结果(y)的一维数据集，并假设数据集中有N个观测值:</p><figure class="lf lg lh li gt kd gh gi paragraph-image"><div class="gh gi le"><img src="../Images/419e73eee2aac92658f767bc5c4215d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*aKLpeAh9_ZkzO5ebd6_srQ.png"/></div></figure><p id="f506" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">拟合数据的线性模型如下所示:</p><figure class="lf lg lh li gt kd gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/38e3ff7ea8228c839481371ea0443c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*g1LFYgusZWsOXdTyoXvL9w.png"/></div></figure><p id="d167" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中w0和w1是算法在训练期间学习的权重。</p><h2 id="3e20" class="lk ll it bd lm ln lo dn lp lq lr dp ls kr lt lu lv kv lw lx ly kz lz ma mb iz bi translated"><strong class="ak">梯度下降算法</strong></h2><p id="0e1d" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">如果我们假设模型中的误差是独立的且呈正态分布，则似然函数如下所示:</p><figure class="lf lg lh li gt kd gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/f54fe7266533dd8f722ae2cb3cc7149e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*kQo7e101n7pB13fcjJuQTw.png"/></div></figure><p id="5cb3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了最大化似然函数，我们最小化w0和w1的误差平方和(SSE ):</p><figure class="lf lg lh li gt kd gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/670abab2688cd449a17cdd3672bf35fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*6hvZqXdBKvg_WLik-gROyA.png"/></div></figure><p id="9a37" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目标函数或我们的SSE函数通常使用梯度下降(GD)算法来最小化。在GD方法中，权重根据以下程序更新:</p><figure class="lf lg lh li gt kd gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/7f5e830ba6e9173847d6e255e51dbf10.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*reastR9T7cnUaP2wM9xRFA.png"/></div></figure><p id="8b40" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">即在与梯度相反的方向上。这里，eta是一个小的正常数，称为学习率。该等式可以用分量形式写成:</p><figure class="lf lg lh li gt kd gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/eaa8b27d29b457bad86c41248520f814.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*mDOqDmTdfnIjYJkTJwCQVQ.png"/></div></figure><p id="88eb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想了解更多关于GD算法及其工作原理的信息，请参阅以下文章:<a class="ae ml" href="https://medium.com/@benjaminobi/machine-learning-how-the-gradient-descent-algorithm-works-61682d8570b6" rel="noopener">https://medium . com/@ Benjamin bi/machine-learning-how-the-gradient-descent-algorithm-works-61682d 8570 b 6</a></p><h2 id="f55b" class="lk ll it bd lm ln lo dn lp lq lr dp ls kr lt lu lv kv lw lx ly kz lz ma mb iz bi translated">使用Python Estimator实现</h2><pre class="lf lg lh li gt mm mn mo mp aw mq bi"><span id="30b0" class="lk ll it mn b gy mr ms l mt mu">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>class GradientDescent(object):<br/>    """Gradient descent optimizer.<br/>    Parameters<br/>    ------------<br/>    eta : float<br/>        Learning rate (between 0.0 and 1.0)<br/>    n_iter : int<br/>        Passes over the training dataset.<br/>        <br/>    Attributes<br/>    -----------<br/>    w_ : 1d-array<br/>        Weights after fitting.<br/>    errors_ : list<br/>        Error in every epoch.<br/>    """</span><span id="8bb8" class="lk ll it mn b gy mv ms l mt mu">    def __init__(self, eta=0.01, n_iter=10):<br/>        self.eta = eta<br/>        self.n_iter = n_iter<br/>        <br/>    def fit(self, X, y):<br/>        """Fit the data.<br/>        <br/>        Parameters<br/>        ----------<br/>        X : {array-like}, shape = [n_points]<br/>        Independent variable or predictor.<br/>        y : array-like, shape = [n_points]<br/>        Outcome of prediction.<br/>        Returns<br/>        -------<br/>        self : object<br/>        """<br/>        self.w_ = np.zeros(2)<br/>        self.errors_ = []<br/>        <br/>        for i in range(self.n_iter):<br/>            errors = 0<br/>            for j in range(X.shape[0]):<br/>                self.w_[1:] += self.eta*X[j]*(y[j] - self.w_[0] -                     self.w_[1]*X[j])<br/>                self.w_[0] += self.eta*(y[j] - self.w_[0] - self.w_[1]*X[j])<br/>                errors += 0.5*(y[j] - self.w_[0] - self.w_[1]*X[j])**2<br/>            self.errors_.append(errors)<br/>        return self</span><span id="55b3" class="lk ll it mn b gy mv ms l mt mu">    def predict(self, X):<br/>        """Return predicted y values"""<br/>        return self.w_[0] + self.w_[1]*X</span></pre><h2 id="7ad3" class="lk ll it bd lm ln lo dn lp lq lr dp ls kr lt lu lv kv lw lx ly kz lz ma mb iz bi translated">Python估计器的应用</h2><p id="5a5e" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated"><strong class="ki jd"> a)创建数据集</strong></p><pre class="lf lg lh li gt mm mn mo mp aw mq bi"><span id="6621" class="lk ll it mn b gy mr ms l mt mu">np.random.seed(1)<br/>X=np.linspace(0,1,10)<br/>y = 2*X + 1<br/>y = y + np.random.normal(0,0.05,X.shape[0])</span></pre><p id="252e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jd"> b)拟合和预测</strong></p><pre class="lf lg lh li gt mm mn mo mp aw mq bi"><span id="a92a" class="lk ll it mn b gy mr ms l mt mu">gda = GradientDescent(eta=0.1, n_iter=100)<br/>gda.fit(X,y)<br/>y_hat=gda.predict(X)</span></pre><p id="26cb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jd"> c)绘图输出</strong></p><pre class="lf lg lh li gt mm mn mo mp aw mq bi"><span id="c13c" class="lk ll it mn b gy mr ms l mt mu">plt.figure()<br/>plt.scatter(X,y, marker='x',c='r',alpha=0.5,label='data')<br/>plt.plot(X,y_hat, marker='s',c='b',alpha=0.5,label='fit')<br/>plt.xlabel('x')<br/>plt.ylabel('y')<br/>plt.legend()</span></pre><figure class="lf lg lh li gt kd gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/8d88511ba948afd0c021027fcbb6f62d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*6TmGAGUPKblqbt1mxDdk3g.png"/></div></figure><p id="31cc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jd"> d)计算R平方值</strong></p><pre class="lf lg lh li gt mm mn mo mp aw mq bi"><span id="3357" class="lk ll it mn b gy mr ms l mt mu">R_sq = 1-((y_hat - y)**2).sum()/((y-np.mean(y))**2).sum()<br/>R_sq<br/>0.991281901588877</span></pre><p id="1aaf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总之，我们已经展示了如何使用GD算法在Python中构建和实现一个简单的线性回归估计器。如果你想看看GD算法在一个真实的机器学习分类算法中是如何使用的，请看下面的<a class="ae ml" href="https://github.com/bot13956/LogisticRegression_gradient_descent" rel="noopener ugc nofollow" target="_blank"> Github知识库</a>。</p><p id="dcbb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读。</p></div></div>    
</body>
</html>