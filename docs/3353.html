<html>
<head>
<title>How To Build Your Own K-Means Algorithm Implementation in Python From Scratch With K-Means++ Initialization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用K-Means++初始化从零开始用Python构建自己的K-Means算法实现</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-build-your-own-k-means-algorithm-implementation-in-python-from-scratch-with-k-means-f652ed08ea31?source=collection_archive---------5-----------------------#2022-11-29">https://pub.towardsai.net/how-to-build-your-own-k-means-algorithm-implementation-in-python-from-scratch-with-k-means-f652ed08ea31?source=collection_archive---------5-----------------------#2022-11-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7cbf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">有什么比自己从0开始实现更好的加深对算法原理知识的方法？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2755fb9ccfa0d8a6edf853ebf7a28337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DQ8rwmJy7wtyOZl8m-toSA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">梅尔·普尔在<a class="ae kv" href="https://unsplash.com/s/photos/clusters?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h2 id="3aa0" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">什么是K-Means？</strong></h2><p id="b744" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">K-Means是一种<strong class="lu ir">无监督机器学习技术</strong> <em class="ml"> </em>，用于将多个<em class="ml">【n】个观察值</em>拆分成<em class="ml">【k】个不同的聚类</em>，其中每个观察值属于质心最近的聚类。结果将是数据集被分割成<em class="ml"> Voronoi单元</em>。</p><p id="5a2e" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">假设我们有一个包含两个要素的数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b06254ec4106e0d4d4eacd8fa7a52859.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*L8qEHPIlEOMjvYA1JxI1iw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">图一。随机生成的二维无标签数据集。作者插图。</figcaption></figure><p id="c23f" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">正如您在<em class="ml">图1 </em>中所注意到的，肉眼很容易就可以看出这个数据集可以被划分成六个不同的集群<em class="ml">(或组)</em>。但是算法如何确定哪个观测值属于哪个聚类呢？</p><p id="90bc" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">为了给每个样本分配一个特定的组，K-Means算法遵循以下步骤:</p><ol class=""><li id="1103" class="ms mt iq lu b lv mm ly mn lf mu lj mv ln mw mk mx my mz na bi translated">初始化“k”个质心，每个聚类一个。</li><li id="9436" class="ms mt iq lu b lv nb ly nc lf nd lj ne ln nf mk mx my mz na bi translated">根据最近的质心为每个样本分配一个聚类。</li><li id="2da0" class="ms mt iq lu b lv nb ly nc lf nd lj ne ln nf mk mx my mz na bi translated">基于指定的点重新计算聚类中心。</li><li id="3ea9" class="ms mt iq lu b lv nb ly nc lf nd lj ne ln nf mk mx my mz na bi translated">重复步骤2和3，直到质心不再改变。</li></ol><p id="37c5" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">瞧，K-Means就是这样得出最终结果的。</p><h2 id="001d" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">k-意味着用例</h2><p id="5888" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">这种聚类算法的一些最常见的用例包括诸如<strong class="lu ir"> <em class="ml">搜索引擎、异常检测</em> </strong>和<strong class="lu ir"> <em class="ml">基于先前行为(兴趣、购买等)的客户分割</em> </strong>等主题。)</p><h2 id="c3a0" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">质心的初始化方法</strong></h2><p id="078f" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们应该考虑的一件重要的事情是<strong class="lu ir">最终的结果将取决于</strong>对<strong class="lu ir">聚类中心的初始化</strong>方法<strong class="lu ir">。最常用的两种初始化方法是<em class="ml">‘random’</em>和<em class="ml">‘k-means++’</em>。这两者之间的主要区别是“k-means++”试图推动质心尽可能远离彼此，这意味着<strong class="lu ir">它会更快地收敛到最终的解决方案</strong>。</strong></p><p id="5744" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">对于这个实现，我们将使用<em class="ml">‘k-means++’</em>作为初始化方法。</p><h2 id="d061" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">实现</strong></h2><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="e823" class="nl kx iq nh b be nm nn l no np">class myKMeans:<br/>    def __init__(self, n_clusters, iters):<br/>        """<br/>        KMeans Class constructor.<br/><br/>        Args:<br/>          n_clusters (int) : Number of clusters used for partitioning.<br/>          iters (int) : Number of iterations until the algorithm stops.<br/><br/>        """<br/>        self.n_clusters = n_clusters<br/>        self.iters = iters<br/>        <br/>    def kmeans_plus_plus(self, X, n_clusters):<br/>        pass<br/><br/>    def find_closest_centroids(self, X, centroids):<br/>        pass<br/><br/>    def compute_centroids(self, X, idx, K):<br/>        pass<br/><br/>    def fit_predict(self, X):<br/>        pass</span></pre><p id="ea46" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">这是我们将要构建的类的结构。<em class="ml"> 'kmeans_plus_plus()'、' find _ closed _ Centros()'</em>和<em class="ml">' compute _ Centros()'</em>方法将分别执行算法的第一、第二和第三步。<em class="ml">‘拟合_预测()’</em>方法将获取数据集并在标签上进行预测。</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="fc02" class="nl kx iq nh b be nm nn l no np">def kmeans_plus_plus(self, X, n_clusters):<br/>    """<br/>    My implementation of the KMeans++ initialization method for computing the centroids.<br/><br/>    Args:<br/>        X (ndarray): Dataset samples<br/>        n_clusters (int): Number of clusters<br/><br/>    Returns:<br/>        centroids (ndarray): Initial position of centroids<br/>    """<br/>    # Assign the first centroid to a random sample from the dataset.<br/>    idx = random.randrange(len(X))<br/>    centroids = [X[idx]]<br/><br/>    # For each cluster<br/>    for _ in range(1, n_clusters):<br/><br/>        # Get the squared distance between that centroid and each sample in the dataset<br/>        squared_distances = np.array([min([np.inner(centroid - sample,centroid - sample) for centroid in centroids]) for sample in X])<br/><br/>        # Convert the distances into probabilities that a specific sample could be the center of a new centroid<br/>        proba = squared_distances / squared_distances.sum()<br/><br/>        for point, probability in enumerate(proba):<br/>            # The farthest point from the previous computed centroids will be assigned as the new centroid as it has the highest probability.<br/>            if probability == proba.max():<br/>                centroid = point<br/>                break<br/><br/>        centroids.append(X[centroid])<br/><br/>    return np.array(centroids)</span></pre><p id="b2d5" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><em class="ml">以上功能实现了K-Means算法第一步<strong class="lu ir">(初始化方法)</strong>。</em></p><p id="b547" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">我们从数据集中随机抽取一个样本，并将其指定为<strong class="lu ir"> <em class="ml">第一个质心</em> </strong>。然后<em class="ml">重复计算</em>每个样本<em class="ml">与所有质心</em>之间的距离，并将剩余的所有质心分配到距离先前计算的中心最远的<em class="ml">点</em>。</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="eb33" class="nl kx iq nh b be nm nn l no np">def find_closest_centroids(self, X, centroids):<br/>    """<br/>    Computes the distance to the centroids and assigns the new label to each sample in the dataset.<br/><br/>    Args:<br/>        X (ndarray): Dataset samples  <br/>        centroids (ndarray): Number of clusters<br/><br/>    Returns:<br/>        idx (ndarray): Closest centroids for each observation<br/><br/>    """<br/><br/>    # Set K as number of centroids<br/>    K = centroids.shape[0]<br/><br/>    # Initialize the labels array to 0<br/>    label = np.zeros(X.shape[0], dtype=int)<br/><br/>    # For each sample in the dataset<br/>    for sample in range(len(X)):<br/>        distance = []<br/>        # Take every centroid<br/>        for centroid in range(len(centroids)):<br/>            # Compute Euclidean norm between a specific sample and a centroid<br/>            norm = np.linalg.norm(X[sample] - centroids[centroid])<br/>            distance.append(norm)<br/><br/>        # Assign the closest centroid as it's label<br/>        label[sample] = distance.index(min(distance))<br/><br/>    return label</span></pre><p id="3187" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated"><em class="ml">上的函数实现了K-Means算法的第二步</em>(求最近质心)。</p><p id="03d9" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">对于数据集中的<em class="ml">每个样本</em>，我们取<em class="ml">每个质心</em>和<strong class="lu ir">计算它们之间的欧几里德范数</strong>。我们将其存储在一个列表中，最后，我们将观察结果分配给<em class="ml">最接近的质心</em>。</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="f7e2" class="nl kx iq nh b be nm nn l no np">def compute_centroids(self, X, idx, K):<br/>    """<br/>    Returns the new centroids by computing the mean of the data points assigned to each centroid.<br/><br/>    Args:<br/>        X (ndarray): Dataset samples <br/>        idx (ndarray): Closest centroids for each observation <br/>        K (int): Number of clusters<br/><br/>    Returns:<br/>        centroids (ndarray): New centroids computed<br/>    """<br/><br/>    # Number of samples and features<br/>    m, n = X.shape<br/><br/>    # Initialize centroids to 0<br/>    centroids = np.zeros((K, n))<br/><br/>    # For each centroid<br/>    for k in range(K):   <br/>        # Take all samples assigned to that specific centroid<br/>        points = X[idx == k]<br/>        # Compute their mean<br/>        centroids[k] = np.mean(points, axis=0)<br/><br/>    return centroids</span></pre><p id="82fe" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">上面的<em class="ml">功能实现了K-Means算法的</em>第三步<strong class="lu ir">(重新计算新的聚类中心)</strong>。</p><p id="352c" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">对于<em class="ml">的每个质心</em>，我们取<em class="ml">所有分配给该<em class="ml">特定组的点</em>和<strong class="lu ir">计算其平均值</strong>。结果会给我们<em class="ml">新的聚类中心</em>。</em></p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="9303" class="nl kx iq nh b be nm nn l no np">def fit_predict(self, X):<br/>    """<br/>    My implementation of the KMeans algorithm.<br/><br/>    Args:<br/>        X (ndarray): Dataset samples<br/><br/>    Returns:<br/>        centroids (ndarray):  Computed centroids<br/>        labels (ndarray):     Predicts for each sample in the dataset.<br/>    """<br/>    # Number of samples and features<br/>    m, n = X.shape<br/><br/>    # Compute initial position of the centroids<br/>    initial_centroids = self.kmeans_plus_plus(X, self.n_clusters)<br/><br/>    centroids = initial_centroids   <br/>    labels = np.zeros(m)<br/><br/>    prev_centroids = centroids<br/><br/>    # Run K-Means<br/>    for i in range(self.iters):<br/>        # For each example in X, assign it to the closest centroid<br/>        labels = self.find_closest_centroids(X, centroids)<br/><br/>        # Given the memberships, compute new centroids<br/>        centroids = self.compute_centroids(X, labels, self.n_clusters)<br/>        <br/>        # Check if centroids stopped changing positions<br/>        if centroids.tolist() == prev_centroids.tolist():<br/>            print(f'K-Means converged at {i+1} iterations')<br/>            break<br/>        else:<br/>            prev_centroids = centroids<br/><br/>    return centroids, labels</span></pre><p id="1629" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">最后但并非最不重要的是，将调用<em class="ml">' fit _ project()'</em>函数对数据集中的样本进行预测。</p><p id="ebb0" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">最后<em class="ml">，</em>您的<em class="ml"> K-Means类</em>应该是这样的<strong class="lu ir"/>:</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="3fdd" class="nl kx iq nh b be nm nn l no np">class myKMeans:<br/>    def __init__(self, n_clusters, iters):<br/>        """<br/>        KMeans Class constructor.<br/>  <br/>        Args:<br/>          n_clusters (int) : Number of clusters used for partitioning.<br/>          iters (int) : Number of iterations until the algorithm stops.<br/>  <br/>        """<br/>        self.n_clusters = n_clusters<br/>        self.iters = iters<br/>        <br/>    def kmeans_plus_plus(self, X, n_clusters):<br/>        """<br/>        My implementation of the KMeans++ initialization method for computing the centroids.<br/>  <br/>        Args:<br/>            X (ndarray): Dataset samples<br/>            n_clusters (int): Number of clusters<br/>  <br/>        Returns:<br/>            centroids (ndarray): Initial position of centroids<br/>        """<br/>        # Assign the first centroid to a random sample from the dataset.<br/>        idx = random.randrange(len(X))<br/>        centroids = [X[idx]]<br/>  <br/>        # For each cluster<br/>        for _ in range(1, n_clusters):<br/>  <br/>            # Get the squared distance between that centroid and each sample in the dataset<br/>            squared_distances = np.array([min([np.inner(centroid - sample,centroid - sample) for centroid in centroids]) for sample in X])<br/>  <br/>            # Convert the distances into probabilities that a specific sample could be the center of a new centroid<br/>            proba = squared_distances / squared_distances.sum()<br/>  <br/>            for point, probability in enumerate(proba):<br/>                # The farthest point from the previous computed centroids will be assigned as the new centroid as it has the highest probability.<br/>                if probability == proba.max():<br/>                    centroid = point<br/>                    break<br/>  <br/>            centroids.append(X[centroid])<br/>  <br/>        return np.array(centroids)<br/>  <br/>    def find_closest_centroids(self, X, centroids):<br/>        """<br/>        Computes the distance to the centroids and assigns the new label to each sample in the dataset.<br/>  <br/>        Args:<br/>            X (ndarray): Dataset samples  <br/>            centroids (ndarray): Number of clusters<br/>  <br/>        Returns:<br/>            idx (ndarray): Closest centroids for each observation<br/>  <br/>        """<br/>  <br/>        # Set K as number of centroids<br/>        K = centroids.shape[0]<br/>  <br/>        # Initialize the labels array to 0<br/>        label = np.zeros(X.shape[0], dtype=int)<br/>  <br/>        # For each sample in the dataset<br/>        for sample in range(len(X)):<br/>            distance = []<br/>            # Take every centroid<br/>            for centroid in range(len(centroids)):<br/>                # Compute Euclidean norm between a specific sample and a centroid<br/>                norm = np.linalg.norm(X[sample] - centroids[centroid])<br/>                distance.append(norm)<br/>  <br/>            # Assign the closest centroid as it's label<br/>            label[sample] = distance.index(min(distance))<br/>  <br/>        return label<br/>  <br/>    def compute_centroids(self, X, idx, K):<br/>        """<br/>        Returns the new centroids by computing the mean of the data points assigned to each centroid.<br/>  <br/>        Args:<br/>            X (ndarray): Dataset samples <br/>            idx (ndarray): Closest centroids for each observation <br/>            K (int): Number of clusters<br/>  <br/>        Returns:<br/>            centroids (ndarray): New centroids computed<br/>        """<br/>  <br/>        # Number of samples and features<br/>        m, n = X.shape<br/>  <br/>        # Initialize centroids to 0<br/>        centroids = np.zeros((K, n))<br/>  <br/>        # For each centroid<br/>        for k in range(K):   <br/>            # Take all samples assigned to that specific centroid<br/>            points = X[idx == k]<br/>            # Compute their mean<br/>            centroids[k] = np.mean(points, axis=0)<br/>  <br/>        return centroids<br/>  <br/>    def fit_predict(self, X):<br/>        """<br/>        My implementation of the KMeans algorithm.<br/>  <br/>        Args:<br/>            X (ndarray): Dataset samples<br/>  <br/>        Returns:<br/>            centroids (ndarray):  Computed centroids<br/>            labels (ndarray):     Predicts for each sample in the dataset.<br/>        """<br/>        # Number of samples and features<br/>        m, n = X.shape<br/>  <br/>        # Compute initial position of the centroids<br/>        initial_centroids = self.kmeans_plus_plus(X, self.n_clusters)<br/>  <br/>        centroids = initial_centroids   <br/>        labels = np.zeros(m)<br/>        <br/>        prev_centroids = centroids<br/>  <br/>        # Run K-Means<br/>        for i in range(self.iters):<br/>            # For each example in X, assign it to the closest centroid<br/>            labels = self.find_closest_centroids(X, centroids)<br/>  <br/>            # Given the memberships, compute new centroids<br/>            centroids = self.compute_centroids(X, labels, self.n_clusters)<br/>            <br/>            # Check if centroids stopped changing positions<br/>            if centroids.tolist() == prev_centroids.tolist():<br/>                print(f'K-Means converged at {i+1} iterations')<br/>                break<br/>            else:<br/>                prev_centroids = centroids<br/>  <br/>        return labels, centroids</span></pre><p id="2cee" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">酷。现在让我们看看<strong class="lu ir">我们的实现的结果</strong>与相比看起来是怎样的(相比于K-Means的<em class="ml"> sklearn </em>版本):</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="a7d4" class="nl kx iq nh b be nm nn l no np">import random<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.cluster import KMeans</span></pre><p id="2f58" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">接下来，我们需要一个数据集，我们将对其执行聚类。为了简单起见，我将使用<em class="ml"> 'make_blobs()' </em>函数从<em class="ml">sklearn . dataset .</em>生成一个虚拟数据集</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="a587" class="nl kx iq nh b be nm nn l no np">from sklearn.datasets import make_blobs<br/><br/># Generate 2D classification dataset<br/>X, y = make_blobs(n_samples=1500, centers=6, n_features=2, random_state=67)</span></pre><p id="a2f9" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">上面的代码片段将为我们生成以下数据集:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b64b594c23ac5f797ad6497d027f0799.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*fyLT1fCoA_pqZWsa0VLjuQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">图2。随机生成的二维标记数据集。作者的插图。</figcaption></figure><p id="7fd0" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">现在，让我们运行K-Means的两个版本(<em class="ml">拥有</em>和<em class="ml"> sklearn </em>实现)并看看它们是如何运行的。</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="14de" class="nl kx iq nh b be nm nn l no np"># sklearn version of KMeans<br/>kmeans = KMeans(n_clusters=5)<br/>sklearn_labels = kmeans.fit_predict(X)<br/>sklearn_centers = kmeans.cluster_centers_<br/><br/># own implementation of KMeans<br/>my_kmeans = myKMeans(5, 50)<br/>mykmeans_labels, mykmeans_centers = my_kmeans.fit_predict(X)</span></pre><p id="3eb1" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">很好。现在我们有了推论，让我们把它们和Voronoi细胞一起可视化。😁</p><pre class="kg kh ki kj gt ng nh ni bn nj nk bi"><span id="b95d" class="nl kx iq nh b be nm nn l no np">plt.figure(figsize=(12,4)) <br/>vor = Voronoi(sklearn_centers)<br/>fig = voronoi_plot_2d(vor, plt.subplot(1, 2, 1))<br/>plt.subplot(1, 2, 1)<br/>plt.title("sklearn KMeans Predicts")<br/>plt.xlabel("Feature 1")<br/>plt.ylabel("Feature 2")<br/>plt.xlim([-13, 11])<br/>plt.ylim([-14, 13])<br/>plt.scatter(X[:, 0], X[:, 1], 4, c=sklearn_labels) <br/>plt.scatter(sklearn_centers[:, 0], sklearn_centers[:, 1], marker='x', c='red', s=50)<br/>vor = Voronoi(mykmeans_centers)<br/>fig = voronoi_plot_2d(vor, plt.subplot(1, 2, 2))<br/>plt.subplot(1, 2, 2)<br/>plt.title("My KMeans Predicts")<br/>plt.xlabel("Feature 1")<br/>plt.ylabel("Feature 2")<br/>plt.xlim([-13, 11])<br/>plt.ylim([-14, 13])<br/>plt.scatter(X[:, 0], X[:, 1], 4, c=mykmeans_labels) <br/>plt.scatter(mykmeans_centers[:, 0], mykmeans_centers[:, 1], marker='x', c='red', s=50)<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/ee45e902cb2278b1d52220aaa346c282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CT9zwS0m5I67RO0v2blRFg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">图3。我们从头开始实现K-Means和sklearn版本的比较。作者插图。</figcaption></figure><p id="3b5e" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">哇哦。如果你问我，那看起来真的令人印象深刻 。结果基本相同。</p><h2 id="db78" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">结论</h2><p id="dd09" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">总之，这或多或少是您需要了解的关于这个强大的聚类算法的一切。我希望这篇文章能帮助你了解K-Means原则。感谢阅读！</p><p id="0c23" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">如果你对这篇文章有任何意见，请写在评论里！我很想读读它们😋</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h2 id="f1ab" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">关于我</h2><p id="08f5" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">大家好，我叫Alex，是一名年轻热情的机器学习和数据科学学生。</p><p id="4a51" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">如果你喜欢的内容，请考虑下降关注和鼓掌，因为他们真的很感激。此外，请随时在<a class="ae kv" href="https://www.linkedin.com/in/alexandru-florin-belengeanu-74b1a3128/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系，以便获得一些关于机器学习相关主题的每周见解。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h2 id="8801" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">参考</h2><p id="30be" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">[1]大卫·亚瑟和谢尔盖·瓦西里维茨基，<a class="ae kv" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf" rel="noopener ugc nofollow" target="_blank"> k-means++精心播种的优势</a> (2007)，<a class="ae kv" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf" rel="noopener ugc nofollow" target="_blank">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a></p><p id="5558" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">[2]<a class="ae kv" href="https://en.wikipedia.org/wiki/K-means_clustering#Applications" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/K-means _ clustering #应用</a></p><p id="7d03" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">[3]<a class="ae kv" href="https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnuggets . com/2020/06/centroid-initial ization-k-means-clustering . html</a></p></div></div>    
</body>
</html>