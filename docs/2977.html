<html>
<head>
<title>UNet++ Clearly Explained — A Better Image Segmentation Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">UNet++清楚地解释了——更好的图像分割架构</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/unet-clearly-explained-a-better-image-segmentation-architecture-f48661c92df9?source=collection_archive---------2-----------------------#2022-07-22">https://pub.towardsai.net/unet-clearly-explained-a-better-image-segmentation-architecture-f48661c92df9?source=collection_archive---------2-----------------------#2022-07-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/cbada927e89eea797e1d61ad4dbd9a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Yu8WJ82wyTISGSko"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">由<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kc" href="https://unsplash.com/@pietrozj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Pietro Jeng </a>拍摄</figcaption></figure><h1 id="d37c" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">目录</h1><p id="c167" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">∘ <a class="ae kc" href="#3e17" rel="noopener ugc nofollow"> ⭐️ U-Net重述</a> <br/> ∘ <a class="ae kc" href="#35d6" rel="noopener ugc nofollow"> ⭐️ UNet++创新</a> <br/> ∘ <a class="ae kc" href="#b4e5" rel="noopener ugc nofollow"> ⭐️损失函数</a> <br/> ∘ <a class="ae kc" href="#a4a6" rel="noopener ugc nofollow"> ⭐️表现</a> <br/> <a class="ae kc" href="#7f9a" rel="noopener ugc nofollow">引文</a></p><p id="9aee" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在本文中，我们将向您介绍UNet++，本质上是U-Net的升级版本。这篇文章旨在帮助你用尽可能少的时间直观而彻底地理解它。建议您至少对U-Net有一个非常粗略的概念，但是我们还是要做一个回顾！</p><h2 id="3e17" class="me ke iq bd kf mf mg dn kj mh mi dp kn lm mj mk kr lq ml mm kv lu mn mo kz mp bi translated">⭐️ U-Net概要</h2><p id="623d" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">U-Net于2015年推出，旨在专门在医学成像领域执行图像分割任务。它的名字来源于它的“U形”建筑。</p><p id="cb6e" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">该架构由一个<strong class="ld ir">契约路径</strong>(又名。下采样路径，编码器)，其中特征图的宽度和高度被收缩，同时通道以因子2扩展，直到它达到1024 ( <em class="mq">通常是CNN</em>的 <em class="mq">最大推荐级别)，作为“转折点”的<strong class="ld ir">瓶颈</strong>，以及<strong class="ld ir">扩展路径</strong>(又名。上采样路径、解码器)，其中特征图的宽度和高度被扩展到掩模的维度。</em></p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/d1cac05d63b014d08226cb6291cfd6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uIfOc2XFQztDgcJT.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图1: U-Net架构</figcaption></figure></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h2 id="35d6" class="me ke iq bd kf mf mg dn kj mh mi dp kn lm mj mk kr lq ml mm kv lu mn mo kz mp bi translated">⭐️ UNet++创新</h2><p id="08ca" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">UNet++从U-Net“升级”而来，本质上增加了密集的卷积块(图1中的蓝色和图3)和嵌套在网络顶层的深度监管设计(图2中的红色)。新提出的模型看起来像这样:</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/71d65bb6aefee216ef10fdb095ffdcde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jOZxlxGRWhvQzDCtxfwmfQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图2</figcaption></figure><p id="2314" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">第一个设计变化是增加了<strong class="ld ir">密集卷积模块</strong>，图2直观而简洁地显示了其工作原理。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/66598acc05c48d022598266aebc15575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QUn0GJAtwoQWAfLgJkkbMg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图3</figcaption></figure><p id="a30d" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在U-Net中，从编码器生成的特征映射被自动传递到相同级别的解码器(在图3中以黑色示出)。然而，在UNet++中，情况发生了变化(如图3中蓝色和绿色所示)。要理解它，下面是解释:</p><ul class=""><li id="86ac" class="nf ng iq ld b le lz li ma lm nh lq ni lu nj ly nk nl nm nn bi translated">在图3顶部所示的公式中，<strong class="ld ir"> H </strong>是DenseNet的复合函数，它结合了<strong class="ld ir">批量归一化、</strong>ReLU激活和一个<strong class="ld ir"> 3x3卷积</strong>。</li><li id="713c" class="nf ng iq ld b le no li np lm nq lq nr lu ns ly nk nl nm nn bi translated"><strong class="ld ir"> [] </strong>中的元素被连接在一起作为<strong class="ld ir"> H </strong>复合函数的输入。</li><li id="6fc2" class="nf ng iq ld b le no li np lm nq lq nr lu ns ly nk nl nm nn bi translated"><strong class="ld ir"> U </strong>是U-Net的复合函数；默认情况下，当你使用U-Net自己的主干网时，你应该期待<strong class="ld ir">两个</strong><strong class="ld ir">3×3卷积</strong>的ReLU激活(如图1所示，每一层是如何构成的)。</li><li id="d1c5" class="nf ng iq ld b le no li np lm nq lq nr lu ns ly nk nl nm nn bi translated">应该注意的是，在中间引入的密集块不仅考虑了来自同一级别中的先前“节点”的信息，还考虑了其下一级别中的“节点”的信息(如图2所示)。这是一个真正紧密相连的网络！</li></ul><p id="8776" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">因此，新引入的密集连接将有助于减少“编码器和解码器的特征图之间的语义差距”(图1)，因此该模型可以具有更容易的学习任务，因为那些特征图将更加“语义相似”</p><p id="6eed" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">UNet++的第二个变化是增加了<strong class="ld ir">深度监督</strong>设计(图4红色部分)。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d37bc5cdeebe4281a015afb0900be491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*5xDpgP4YoZkGS75SKnpXsQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图4</figcaption></figure><p id="536b" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">深度监管并没有看起来那么难。本质上，它有助于模型在两种模式下运行:<br/> 1)精确模式(来自级别0中所有分支的输出被平均以产生最终结果)<br/> 2)快速模式(并非所有分支都被选择用于输出)</p><p id="9067" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">图5显示了快速模式中的不同选择如何产生不同的模型</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/eda604e02d879606d72570bf0ba7acd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*_S_1I8LyZyvo2wvlV294Og.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图5</figcaption></figure><h2 id="b4e5" class="me ke iq bd kf mf mg dn kj mh mi dp kn lm mj mk kr lq ml mm kv lu mn mo kz mp bi translated">⭐️损失函数</h2><p id="6857" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在该论文中，作者提出了二元交叉熵和骰子损失的组合损失函数，如公式1所示。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/e7ece479a36f19c548bb9c4e8ff8d326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h8beHnrGx1RUUpQmD58Fmw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">一级方程式</figcaption></figure><p id="0f4f" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">作者对BCE损失使用0.5权重，对骰子损失使用1.0权重。注:骰子系数相当于F1分。在实施过程中，当使用骰子系数作为损失时，建议使用<strong class="ld ir"> 1减去骰子系数</strong>。因此，本文中显示的这种做法可能有待改进。</p><p id="aa8b" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">此外，由于骰子损失的非凸性质，骰子损失通常难以收敛。因此，一个最近的解决方案是通过将它包装在一个log和cosh函数中来“平滑曲线”(<a class="ae kc" href="https://arxiv.org/pdf/2006.14822.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2006.14822.pdf</a>)</p><p id="acc6" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">此外，将BCE损失与骰子损失相结合通常会产生更好的结果。</p><h2 id="a4a6" class="me ke iq bd kf mf mg dn kj mh mi dp kn lm mj mk kr lq ml mm kv lu mn mo kz mp bi translated">⭐️表演</h2><p id="7f95" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">作者在四个不同的数据集上训练了该模型，并且都产生了比U-Net和Wide U-Net模型更好的性能。表1显示了结果。DS的意思是深度监管。结果显示在IoU得分(重叠面积/并集面积)中，它说明了模型的精确度。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/64d8b8dcb6ef2740829d24eaa3cceedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sN_edhoRQge01tBsPtvG6g.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">表1</figcaption></figure><p id="2bb6" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">结果表明，与它的前身U-Net相比，UNet++确实有所改进。</p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><p id="cee5" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">接下来，我们将向您展示UNet 3+是如何工作的。是UNet++的升级版！</p><p id="f04b" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><a class="ae kc" href="https://medium.com/@mlquest0/unet-3-fully-explained-next-generation-unet-2a8e204e4cf9" rel="noopener"> <em class="mq"> UNet 3+详细说明—下一代UNet</em></a></p><p id="ad9d" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">谢谢大家！❤️</p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="7f9a" class="kd ke iq bd kf kg nx ki kj kk ny km kn ko nz kq kr ks oa ku kv kw ob ky kz la bi translated">引文</h1><p id="3449" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">[1] Z. Zhou，M. Siddiquee，N. Tajbakhsh，J. Liang，UNet++: <a class="ae kc" href="https://arxiv.org/abs/1807.10165" rel="noopener ugc nofollow" target="_blank">一种用于医学图像分割的嵌套U-Net架构</a> (2015)，2015计算机视觉与模式识别<br/> [2]: O. Ronneberger，P. Fischer，T. Brox，<a class="ae kc" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-Net:卷积网络用于生物医学图像分割(2015) </a>，2015计算机视觉与模式识别</p></div></div>    
</body>
</html>