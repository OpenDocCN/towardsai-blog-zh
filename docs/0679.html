<html>
<head>
<title>Main Types of Neural Networks and Their Applications — Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的主要类型及其应用——教程</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e?source=collection_archive---------0-----------------------#2020-07-14">https://pub.towardsai.net/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e?source=collection_archive---------0-----------------------#2020-07-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/baa24ba5ad9159b2f768cb57bbf5265b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8QQDK0U1DCBJ7uFXCO36Mw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:神经网络的主要类型，由app.diagrams.net设计，该图是由<a class="ae jg" href="https://www.researchgate.net/profile/Stefan_Leijnen" rel="noopener ugc nofollow" target="_blank">斯蒂芬·莱伊宁</a>和<a class="ae jg" href="https://www.researchgate.net/profile/Fjodor_Van_Veen" rel="noopener ugc nofollow" target="_blank">弗约德尔·范·维恩</a>在<a class="ae jg" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下通过4.0</a><a class="ae jg" href="https://www.asimovinstitute.org/neural-network-zoo/" rel="noopener ugc nofollow" target="_blank">5</a><a class="ae jg" href="https://creativecommons.org/licenses/" rel="noopener ugc nofollow" target="_blank">6</a>授权的<a class="ae jg" href="https://www.researchgate.net/publication/341373030_The_Neural_Network_Zoo" rel="noopener ugc nofollow" target="_blank">知识共享神经网络动物园</a>的衍生物。</figcaption></figure><h2 id="6c45" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>、<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>、<a class="ae ep" href="https://towardsai.net/p/category/tutorial" rel="noopener ugc nofollow" target="_blank">教程</a></h2><div class=""/><div class=""><h2 id="72d5" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">关于神经网络的主要类型及其在现实世界挑战中的应用的教程。</h2></div><p id="ece1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">作者:</strong>普拉蒂克·舒克拉，<a class="ae jg" href="https://mktg.best/vguzs" rel="noopener ugc nofollow" target="_blank">罗伯特·伊里翁多</a></p><p id="eaae" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后更新于2022年3月17日</p><div class="is it gp gr iu md"><a href="https://members.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">加入人工智能，成为会员，你将不仅支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><p id="d4c3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi ms translated"><span class="l mt mu mv bm mw mx my mz na di">N</span>OWA days<a class="ae jg" href="https://mld.ai/mldcmu" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">深度学习</strong> </a>中有多种类型的神经网络，用于不同的目的。在本文中，我们将浏览神经网络中最常用的拓扑，简要介绍它们的工作原理，以及它们在现实世界挑战中的一些应用。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/f8e0ec9cdff386f02803ae43762230e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6FIMoDkN-BDjqy-w4evkg.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:感知机:大脑中信息存储和组织的概率模型[3] |来源:康奈尔航空实验室Frank Rosenblat的Mark I感知机。纽约布法罗，1960年[4]</figcaption></figure><blockquote class="ng"><p id="c661" class="nh ni jj bd nj nk nl nm nn no np mc dk translated">📚这篇文章是我们关于神经网络的第三篇教程，从第一篇开始，用Python代码和数学详细检查神经网络。📚</p></blockquote><h1 id="2092" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky ob kz oc lb od lc oe le of lf og oh bi translated">神经网络拓扑</h1><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/b6fe378f1ef2666d25f2a7c084810b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/0*TgjOfgXvpp1QZIq9.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3:感知器(p)的表示。</figcaption></figure><h1 id="e06a" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">1.感知机(P):</h1><p id="e711" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">感知器模型也称为单层神经网络。这个神经网络只包含两层:</p><ul class=""><li id="4374" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated"><strong class="lj jt">输入层</strong></li><li id="12be" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated"><strong class="lj jt">输出层</strong></li></ul><p id="0dd0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这种类型的神经网络中，没有隐藏层。它接受一个输入，并计算每个节点的加权输入。然后，它使用一个激活函数(通常是一个sigmoid函数)进行分类。</p><p id="9c5a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="fb0f" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">分类。</li><li id="9bbe" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">编码数据库(多层感知器)。</li><li id="f693" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">监控访问数据(多层感知器)。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/0d8612e693685b054ebef6b52311d3d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/0*fbbKIJ-fxDyCfnd2.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:前馈神经网络的表示。</figcaption></figure><h1 id="4792" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">2.前馈(FF):</h1><p id="103b" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">前馈神经网络是一种人工神经网络，其中的节点从不形成循环。在这种神经网络中，所有的感知器都按层排列，输入层接受输入，输出层产生输出。隐藏的层与外部世界没有联系；这就是为什么它们被称为隐藏层。在前向神经网络中，一层中的每个感知器都与下一层中的每个节点相连。因此，所有节点都是完全连接的。另外需要注意的是，同一层中的节点之间没有可见或不可见的连接。前馈网络中没有反馈回路。因此，为了最小化预测中的误差，我们通常使用反向传播算法来更新权重值。</p><p id="e19f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="00ca" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">数据压缩。</li><li id="7f3c" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">模式识别。</li><li id="c7e2" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">计算机视觉。</li><li id="0a9c" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">声纳目标识别。</li><li id="160f" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">语音识别。</li><li id="45b5" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">手写字符识别。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/c0453e1d3c413878c5dd97369dbe5b45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/0*Dezf_up8pNrdsl34.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图5:径向基网络(RBN)的表示。</figcaption></figure><h1 id="7ec2" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">3.径向基网络(RBN):</h1><p id="a4a3" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">径向基函数网络一般用于函数逼近问题。它们可以与其他神经网络相区别，因为它们具有更快的学习速度和通用逼近能力。径向基网络和前馈网络的主要区别在于，RBNs使用<strong class="lj jt">径向基函数</strong>作为<strong class="lj jt">激活函数</strong>。逻辑函数(sigmoid函数)给出一个介于0和1之间的输出，以确定答案是“是”还是“否”。这样做的问题是，如果我们有连续的值，那么就不能使用RBN。RBIs决定了我们的产出与目标产出的差距。这些在连续值的情况下非常有用。总之，RBI表现为使用不同激活函数的FF网络。</p><p id="7cbd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="856a" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">函数逼近。</li><li id="8faa" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">时间序列预测。</li><li id="c85a" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">分类。</li><li id="5d69" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">系统控制。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/e7b66f0f46cbf9497a6658c9942806a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*06pOOKAQd5KURDZi.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图6:深度前馈神经网络的表示。</figcaption></figure><h1 id="3f56" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">4.深度前馈(DFF):</h1><p id="732c" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">深度前馈网络是使用多于一个隐藏层的前馈网络。仅使用一个隐藏层的主要问题是过度拟合，因此通过添加更多的隐藏层，我们可以(并非在所有情况下)减少过度拟合并提高泛化能力。</p><p id="009d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="5d55" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">数据压缩。</li><li id="a9f7" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">模式识别。</li><li id="8db6" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">计算机视觉。</li><li id="92f8" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">ECG噪声滤波。</li><li id="6bd5" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">财务预测。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/57feee0e0f264768fefae21af199a612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8AfmjQTGGEmEFxIa.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7:递归神经网络(RNN)的表示</figcaption></figure><h1 id="d840" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">5.递归神经网络(RNN):</h1><p id="a2e8" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">递归神经网络(RNNs)是前馈(FF)网络的变体。在这种类型中，隐藏层中的每个神经元以特定的时间延迟接收输入。当我们需要在当前迭代中访问以前的信息时，我们使用这种类型的神经网络。例如，当我们试图预测句子中的下一个单词时，我们需要首先知道以前使用的单词。rnn可以处理输入，并随时共享任何长度和重量。模型的大小不会随着输入的大小而增加，此模型中的计算会考虑历史信息。然而，这种神经网络的问题是计算速度慢。此外，它不能考虑当前状态的任何未来输入。它不记得很久以前的信息。</p><p id="0396" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="a3cc" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">机器翻译。</li><li id="0005" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">机器人控制。</li><li id="cfc2" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">时间序列预测。</li><li id="707c" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">语音识别。</li><li id="637e" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">语音合成。</li><li id="57c6" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">时间序列异常检测。</li><li id="b7b1" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">节奏学习。</li><li id="1586" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">音乐创作。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/cf665494d626263acde594060a3191f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DMCqyw3xRMmbdvsG.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:一个长短期记忆(LSTM)网络的图示。</figcaption></figure><h1 id="31d2" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">6.长/短时记忆(LSTM):</h1><p id="6bab" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">LSTM网络引入了存储单元。它们可以处理有内存间隙的数据。上面，我们可以注意到，我们可以在RNNs中考虑时间延迟，但如果我们的RNN在我们有大量相关数据的时候失败了，我们想从中找出相关数据，那么LSTMs就是要走的路。此外，与LSTMs相比，rnn不能记住很久以前的数据。</p><p id="f7d4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="b8e2" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">语音识别。</li><li id="f2e3" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">书写识别。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pr"><img src="../Images/9f8891fddcfc4b482e95a3dfb6a2a624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Hl1WiO_5i6rQXqR-.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:门控循环单元(GRU)网络的示意图。</figcaption></figure><h1 id="2bf0" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">7.门控循环单元(GRU):</h1><p id="7a14" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">门控循环单位是LSTMs的一种变体，因为它们都具有相似的设计，并且大多产生同样好的结果。GRUs只有三个门，并且它们不维持内部细胞状态。</p><p id="7fed" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> a. </strong>更新门:决定将多少过去的知识传递给未来。<br/> <strong class="lj jt"> b. </strong>重置门:决定了要忘记多少过去的知识。<br/> <strong class="lj jt"> c. </strong>当前记忆门:复位命运的子部分。</p><p id="8c66" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="4065" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">复调音乐造型。</li><li id="6678" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">语音信号建模。</li><li id="1c5f" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">自然语言处理。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/73787fefccfc72202f405a78d9b86902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/0*30h4uFH8GxOT23fk.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:自动编码器(AE)网络示意图。</figcaption></figure><h1 id="5753" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">8.自动编码器(AE):</h1><p id="adf4" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">自动编码器<strong class="lj jt"> </strong>神经网络是一种无监督的机器学习算法。在自动编码器中，隐藏单元格的数量小于输入单元格的数量。自动编码器中输入单元的数量等于输出单元的数量。在AE网络上，我们训练它显示输出，输出与fed输入一样接近，这迫使AEs找到共同的模式并概括数据。我们对输入的较小表示使用自动编码器。我们可以从压缩数据中重建原始数据。该算法相对简单，因为AE要求输出与输入相同。</p><ul class=""><li id="81e0" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">编码器:转换低维输入数据。</li><li id="b242" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">解码器<strong class="lj jt"> : </strong>重建压缩数据。</li></ul><p id="6299" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="8a8c" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">分类。</li><li id="93d7" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">聚类。</li><li id="a920" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">特征压缩。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/9a10a3cc1b4916e3f4d2da70e21333d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/0*XjcTzoqpLbssMPXW.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:变化的自动编码器网络(VAE)的表示。</figcaption></figure><h1 id="a39b" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">9.可变自动编码器(VAE):</h1><p id="ee67" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">变分自动编码器(VAE)使用概率方法来描述观测值。它显示了特征集中每个属性的概率分布。</p><p id="c32f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="911c" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">在句子之间插入。</li><li id="eec7" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">自动图像生成。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pu"><img src="../Images/c6e3802303b8fba81d3ce33b3a3e407c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/0*-zaRSp4a5zIPBg6w.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12:去噪自动编码器网络(DAE)示意图。</figcaption></figure><h1 id="a108" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">10.去噪自动编码器(DAE):</h1><p id="03d8" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">在这种自动编码器中，网络不能简单地将输入复制到其输出，因为输入也包含随机噪声。在DAE上，我们生产它是为了减少噪音并在其中产生有意义的数据。在这种情况下，该算法迫使隐藏层学习更鲁棒的特征，使得输出是噪声输入的更精确版本。</p><p id="adb2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="0e1c" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">特征提取。</li><li id="4798" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">降维。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/3e938c9f238eb570e030b30ff97d2890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/0*GpOGXkYHgPLEOYU6.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图13:稀疏自动编码器网络(SAE)的表示。</figcaption></figure><h1 id="b576" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">11.稀疏自动编码器(SAE):</h1><p id="c93b" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">在稀疏自动编码器网络上，我们将通过惩罚隐藏层的<strong class="lj jt"> </strong>激活来构造我们的损失函数，使得当我们将单个样本馈送到网络中时，只有少数节点被激活。这种方法背后的直觉是，例如，如果一个人声称自己是科目A、B、C和D的专家，那么这个人可能是这些科目的多面手。然而，如果此人仅声称致力于主题D，则很可能会从其对主题D的了解中获得洞察力。</p><p id="28c9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="51ef" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">特征提取。</li><li id="8217" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">手写数字识别。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/f0fd16dc295ea37e321c147684eee229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/0*dChgf9FjD3CSjOwK.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图14:马尔可夫链网络(MC)表示。</figcaption></figure><h1 id="097c" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">12.马尔可夫链(MC):</h1><p id="b489" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">马尔可夫链是一种数学系统，它根据一些概率规则经历从一种状态到另一种状态的转换。转换到任何特定状态的概率仅取决于当前状态和经过的时间。</p><p id="acff" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如，一些可能的状态集合可以是:</p><ul class=""><li id="37f9" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">信件。</li><li id="32fc" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">数字。</li><li id="3f74" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">天气状况。</li><li id="88f1" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">棒球比分。</li><li id="fdb6" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">股票表现。</li></ul><p id="5083" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="a036" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">语音识别。</li><li id="1f82" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">信息和通信系统。</li><li id="3c3d" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">排队理论。</li><li id="6a55" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">统计学。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi px"><img src="../Images/b2547d8b0e9e1652048d8a4baec03824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/0*_BBqdkQsG4SVgNQL.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图15:霍普菲尔德网络(HN)表示。</figcaption></figure><h1 id="f063" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">13.霍普菲尔德网络(HN):</h1><p id="c5eb" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">在Hopfield神经网络中，每个神经元都与其他神经元直接相连。在这个网络中，神经元不是开就是关。神经元的状态可以通过接收来自其他神经元的输入而改变。我们通常使用Hopfield网络(HNs)来存储模式和记忆。当我们在一组模式上训练神经网络时，它可以识别模式，即使它有些扭曲或不完整。当我们给它输入不完整的信息时，它可以识别完整的模式，从而返回最佳猜测。</p><p id="a3b8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="38c2" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">优化问题。</li><li id="a4b6" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">图像检测和识别。</li><li id="7c0e" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">医学图像识别。</li><li id="6e83" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">增强x光图像。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi py"><img src="../Images/247d2b94de1065c0333685705189d059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/0*gfn3sfTKKjwpoF5d.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图16:波尔兹曼机器网络(BM)的表示</figcaption></figure><h1 id="c675" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">14.玻尔兹曼机器(BM):</h1><p id="7b97" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">玻尔兹曼机器网络涉及从原始数据集中学习概率分布，并使用它来推断看不见的数据。在BMs中，有输入节点和隐藏节点，一旦我们所有的隐藏节点改变了它的状态，我们的输入节点就转换成输出节点。例如:假设我们在一家核电厂工作，在那里安全必须是第一位的。我们的工作是确保动力装置中的所有组件都可以安全使用，每个组件都有相关的状态，为简单起见，使用布尔1表示可用，0表示不可用。但是，也将有一些组件，我们将无法定期测量其状态。</p><p id="2962" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，我们没有数据告诉我们，如果隐藏的组件停止工作，发电厂将在何时爆炸。因此，在这种情况下，我们构建一个模型，当组件改变它的状态时，它会注意到。因此，当它发生时，我们将被通知检查那个部件，并确保电厂的安全。</p><p id="693c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="e2d9" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">降维。</li><li id="db3b" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">分类。</li><li id="f0f9" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">回归。</li><li id="0fa9" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">协同过滤。</li><li id="35d4" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">特征学习。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/234fcc98bdcf46612e2a7a9b918a6013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/0*WCxR5yWK_cmEfo7l.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图17:受限玻尔兹曼机器(RBM)网络的示意图。</figcaption></figure><h1 id="bee4" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">15.受限玻尔兹曼机(RBM):</h1><p id="9e18" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">RBM是BMs的变体。在该模型中，输入层和隐藏层中的神经元之间可能具有对称连接。需要注意的一点是，每一层内部都没有内部连接。相比之下，玻尔兹曼机器在隐藏层可能有内部联系。BMs中的这些限制允许模型的有效训练。</p><p id="8b20" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="2567" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">过滤。</li><li id="4fb2" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">特征学习。</li><li id="cbe6" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">分类。</li><li id="49a7" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">风险检测。</li><li id="d9a3" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">商业和经济分析。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qa"><img src="../Images/a4b899993957456ea515751d6761bfc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pjOA7yh-wQ2LhaFl.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图18:深层信念网络的代表(DBN)。</figcaption></figure><h1 id="c8d4" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">16.深度信仰网络(DBN):</h1><p id="6e11" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">深度信念网络包含许多隐藏层。我们可以用无监督算法调用DBNs，因为它首先在没有任何监督的情况下学习。DBN中的图层充当要素检测器。在非监督训练之后，我们可以用监督方法训练我们的模型来执行分类。我们可以将DBNs表示为受限玻尔兹曼机器(RBM)和自动编码器(AE)的组合，最后DBNs使用概率方法来处理其结果。</p><p id="3cca" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="8610" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">文档/图像的检索。</li><li id="7858" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">非线性降维。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qb"><img src="../Images/30704e75bb611afccba46f1881df7168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2EK9jpWmpogvIGet.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图19:深度卷积神经网络(DCN)的表示。</figcaption></figure><h1 id="dc5b" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">17.深度卷积网络(DCN):</h1><p id="0cbe" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">卷积神经网络是主要用于图像分类、图像聚类和对象识别的神经网络。DNNs能够无监督地构建分层图像表示。DNNs用于向它添加更复杂的功能，以便它可以更准确地执行任务。</p><p id="db85" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="bba1" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">辨认面孔，街道标志，肿瘤。</li><li id="85f6" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">图像识别。</li><li id="31c5" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">视频分析。</li><li id="5ba3" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">NLP。</li><li id="5b80" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">异常检测。</li><li id="0935" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">药物发现。</li><li id="1ef1" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">跳棋游戏。</li><li id="c49e" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">时间序列预测。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/a096a9458491c3381dfefc96bb81dead.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/0*uIpL1S11wF7dykB8.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图20:去卷积神经网络(DN)的表示。</figcaption></figure><h1 id="db75" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">18.去进化神经网络(DN):</h1><p id="c360" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">去卷积网络是以相反过程工作的卷积神经网络(CNN)。尽管DN在工作性质上类似于CNN，但它在AI中的应用是非常不同的。反进化网络有助于在网络中找到以前认为有用的丢失特征或信号。DN可能由于与其他信号进行了卷积而丢失信号。反进化网络可以提取一个矢量，并把它做成一幅图像。</p><p id="b825" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="0778" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">图像超分辨率。</li><li id="bd35" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">从图像估计表面深度。</li><li id="69a2" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">光流估计。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qd"><img src="../Images/414b5d47101930ba2628e0cb083d8ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RRs7tcZsnmuh8Fy0.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图21:深度卷积反图形网络(DC-IGN)的表示</figcaption></figure><h1 id="af61" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">19.深度卷积逆图形网络(DC-IGN)；</h1><p id="61f6" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">深度卷积反图形网络(DC-IGN)旨在将图形表示与图像相关联。它使用照明、物体位置、纹理和图像设计的其他方面的元素来进行非常复杂的图像处理。它使用不同的层来处理输入和输出。深度卷积反图形网络使用初始层通过各种卷积进行编码，利用最大池，然后使用后续层以反卷积进行解码。</p><p id="5085" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="aacb" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">对人脸的处理。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qe"><img src="../Images/341950c56cca18e7353efbe261a5ff80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wDTai055oQ7F0AoW.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图22:生成对抗网络(GAN)的表示</figcaption></figure><h1 id="0ac9" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">20.生成对抗网络(GAN):</h1><p id="cf2e" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">给定训练数据，gan学习生成具有与训练数据相同的统计数据的新数据。例如，如果我们在照片上训练我们的GAN模型，那么经过训练的模型将能够生成人眼看起来真实的新照片。GANs的目标是区分真实结果和合成结果，以便生成更真实的结果。</p><p id="56f1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="86a9" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">生成新的人体姿态。</li><li id="37b4" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">照片到表情符号。</li><li id="5483" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">面部衰老。</li><li id="8a22" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">超分辨率。</li><li id="69bf" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">服装翻译。</li><li id="9686" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">视频预测。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qf"><img src="../Images/24cf0bed880cfc2f49d7d45faff006de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jRgtvZn2wQ7KrFC4.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图23:表示液态状态机(LSM)。</figcaption></figure><h1 id="1c05" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">21.液态机器(LSM):</h1><p id="62d5" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">液态状态机(LSM)是一种特殊的脉冲神经网络。LSM由大量的神经元组成。这里，每个节点从外部源和其他节点接收输入，这些输入会随时间而变化。请注意，LSM上的节点随机地相互连接。在LSM中，激活函数由阈值水平代替。只有当LSM达到阈值水平时，特定的神经元才会发出其输出。</p><p id="3a98" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="7e2b" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">语音识别。</li><li id="4e48" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">计算机视觉。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qg"><img src="../Images/2394392f956a6fc54c0606ee1aef7f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ym-7t4_l6WUecovN.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图24:极限学习机(ELM)网络的表示。</figcaption></figure><h1 id="7614" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">22.极限学习机(ELM):</h1><p id="b4e8" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">对于更大规模的数据集，传统系统的主要缺点是:</p><ul class=""><li id="c517" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">基于梯度算法的学习速度慢。</li><li id="752b" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">反复调整所有参数。</li></ul><p id="396f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">ELMs随机选择隐藏节点，然后解析地确定输出权重。因此，这些算法比一般的神经网络算法要快得多。还有，在极限学习机网络上，随机分配的权重一般不会更新。ELMs只需一步就能学习输出权重。</p><p id="c45f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="c78b" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">分类。</li><li id="6302" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">回归。</li><li id="f259" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">聚类。</li><li id="f404" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">稀疏近似。</li><li id="6d85" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">特征学习。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qh"><img src="../Images/504d7fd031ad56a6fd059c288563213a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KYJWUP33EUKgcOqG.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图25:回声状态网络(ESN)的表示。</figcaption></figure><h1 id="957a" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">23.回声状态网络(ESN):</h1><p id="2168" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">回声状态网络(ESN)是递归神经网络的一个子类型。这里，每个输入节点接收非线性信号。在ESN中，隐藏节点是稀疏连接的。隐藏节点的连接性和权重是随机分配的。在ESN上，最终输出权重是可训练的，并且可以更新。</p><p id="002b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="866f" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">时间序列预测。</li><li id="f4ac" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">数据挖掘。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qi"><img src="../Images/41ebd0d0cf08e32c57b919e2c794fa7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*15K3kNduy3linSzx.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图26:深层剩余网络的表示(DRN)。</figcaption></figure><h1 id="d2f6" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">24.深度剩余网络(DRN):</h1><p id="912a" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">具有许多层的深度神经网络可能很难训练，并且在训练阶段花费很多时间。也可能导致成绩下降。深度残差网络(drn)防止结果降级，即使它们有许多层。对于DRNs，其输入的一些部分传递到下一层。因此，这些网络可能相当深(可能包含大约300层)。</p><p id="6719" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="4f01" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">图像分类。</li><li id="54e4" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">物体检测。</li><li id="5a30" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">语义分割。</li><li id="6b05" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">语音识别。</li><li id="af97" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">语言识别。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/2e0a5d4ce587b18d8be8c27fa5751248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*vC3UxyqOEW40BsKc.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图27:koho nen网络(KN)的表示。</figcaption></figure><h1 id="3734" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">25.科霍宁网络公司(KN):</h1><p id="baf4" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">Kohonen网络是一种无监督算法。Kohonen网络也被称为自组织地图，当我们的数据分散在多个维度中，而我们只希望它在一个或两个维度中时，它非常有用。可以认为是一种降维的方法。我们使用Kohonen网络来可视化高维数据。他们使用竞争学习而不是纠错学习。</p><p id="29af" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">各种拓扑:</strong></p><ul class=""><li id="edd8" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">矩形网格拓扑。</li><li id="3def" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">六边形网格拓扑。</li></ul><p id="878d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="e30c" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">降维。</li><li id="cc85" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">水质评价和预测。</li><li id="272e" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">沿海水域管理。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qk"><img src="../Images/f431625cf5a961d0d69d27ba916bff35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w2fuNgfTLExx1jlL.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图28:支持风险投资机器(SVM)的表示。</figcaption></figure><h1 id="ca0b" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">26.支持向量机(SVM):</h1><p id="1bb4" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">支持向量机神经网络是支持向量机和神经网络的混合算法。对于一组新的例子，它总是试图将它们分为两类是或否(1或0)。支持向量机通常用于二元分类。这些通常不被认为是神经网络。</p><p id="8a71" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="602a" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">人脸检测。</li><li id="9cd5" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">文本分类。</li><li id="352f" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">分类。</li><li id="e956" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">生物信息学。</li><li id="40f0" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">手写识别。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ql"><img src="../Images/92d2e58deadddbef2b216aeab71edc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bmQgPDdNJal3Hbug.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图29:神经转向机器(NTM)的代表。</figcaption></figure><h1 id="cfc0" class="nq nr jj bd ns nt nu nv nw nx ny nz oa ky oj kz oc lb ok lc oe le ol lf og oh bi translated">27.神经图灵机(NTM):</h1><p id="1764" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">神经图灵机(NTM)架构包含两个主要组件:</p><ul class=""><li id="1508" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">神经网络控制器。</li><li id="92e4" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">记忆银行。</li></ul><p id="d8d4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这个神经网络中，控制器通过输入和输出向量与外部世界进行交互。它还通过与存储器矩阵交互来执行选择性读和写R/W操作。图灵机<strong class="lj jt"> </strong>据说在计算上等同于现代计算机。因此，ntm通过与外部存储器交互来扩展标准神经网络的能力。</p><p id="6113" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">应用:</strong></p><ul class=""><li id="d6eb" class="or os jj lj b lk ll ln lo lq ot lu ou ly ov mc ow ox oy oz bi translated">机器人技术。</li><li id="793b" class="or os jj lj b lk pa ln pb lq pc lu pd ly pe mc ow ox oy oz bi translated">建造一个人造人脑。</li></ul></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><p id="9778" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们希望你喜欢这个主要类型的神经网络概述。如果您有任何反馈，或者有什么需要修改或重新审视的地方，请在评论中告诉我们，或者发送电子邮件至<strong class="lj jt">pub@towardsai.net</strong>。</p><blockquote class="ng"><p id="5048" class="nh ni jj bd nj nk qm qn qo qp qq mc dk translated">📚查看<a class="ae jg" href="https://towardsai.net/machine-learning-algorithms" rel="noopener ugc nofollow" target="_blank">机器学习算法的概述</a>，为初学者提供Python代码示例📚</p></blockquote></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><p id="4c03" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">使用条款:</strong>本作品是在<a class="ae jg" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">知识共享署名4.0国际许可</strong> </a>下许可的衍生作品。原参考图归于<a class="ae jg" href="https://www.researchgate.net/profile/Stefan_Leijnen" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> Stefan Leijnen </strong> </a>和<a class="ae jg" href="https://www.researchgate.net/profile/Fjodor_Van_Veen" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Fjodor van Veen</strong></a>，可在<a class="ae jg" href="https://www.researchgate.net/publication/341373030_The_Neural_Network_Zoo" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">研究门</strong> </a>找到。</p><p id="ef99" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">免责声明:</strong>本文所表达的观点为作者个人观点，不代表卡内基梅隆大学的观点。这些文章并不打算成为最终产品，而是当前思想的反映，同时也是讨论和改进的催化剂。</p><p id="7c87" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过<a class="ae jg" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向艾</a>发布</p></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><h2 id="0db3" class="qr nr jj bd ns qs qt dn nw qu qv dp oa lq qw qx oc lu qy qz oe ly ra rb og jp bi translated">推荐文章</h2><p id="27e7" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">一、<a class="ae jg" href="https://towardsai.net/p/machine-learning/best-datasets-for-machine-learning-and-data-science-d80e9f030279" rel="noopener ugc nofollow" target="_blank">机器学习和数据科学最佳数据集</a> <br/>二。<a class="ae jg" href="http://towardsai.net/ai-salaries" rel="noopener ugc nofollow" target="_blank">艾薪资冲天</a>三世<br/>。<a class="ae jg" href="https://towardsai.net/p/machine-learning/what-is-machine-learning-ml-b58162f97ec7" rel="noopener ugc nofollow" target="_blank">什么是机器学习？</a> <br/>四世。<a class="ae jg" href="https://towardsai.net/ml-masters" rel="noopener ugc nofollow" target="_blank">2020年最佳机器学习硕士项目</a> <br/>五、<a class="ae jg" href="https://towardsai.net/ml-phd" rel="noopener ugc nofollow" target="_blank">2020年最佳机器学习博士项目</a> <br/>六、<a class="ae jg" href="https://towardsai.net/p/machine-learning/best-machine-learning-blogs-6730ea2df3bd" rel="noopener ugc nofollow" target="_blank">最佳机器学习博客</a> <br/>七。<a class="ae jg" href="https://towardsai.net/p/machine-learning/key-machine-learning-ml-definitions-43e837ec6add" rel="noopener ugc nofollow" target="_blank">关键机器学习定义</a> <br/>八。<a class="ae jg" href="https://towardsai.net/ml-captcha" rel="noopener ugc nofollow" target="_blank">用机器学习在0.05秒内破解验证码</a> <br/>九。<a class="ae jg" href="https://towardsai.net/p/machine-learning/machine-learning-vs-ai-important-differences-between-them/robiriondo/3432/" rel="noopener ugc nofollow" target="_blank">机器学习vs. AI及其重要区别</a> <br/>十.<a class="ae jg" href="https://towardsai.net/p/machine-learning/moocs-vs-academia-ensuring-success-starting-in-a-machine-learning-ml-career-304b2e42315e" rel="noopener ugc nofollow" target="_blank">确保成功开创机器学习事业(ML) </a> <br/> XI。<a class="ae jg" href="https://towardsai.net/p/machine-learning/machine-learning-algorithms-for-beginners-with-python-code-examples-ml-19c6afd60daa" rel="noopener ugc nofollow" target="_blank">机器学习算法初学者</a> <br/>十二。<a class="ae jg" href="https://towardsai.net/neural-networks-with-python" rel="noopener ugc nofollow" target="_blank">神经网络从零开始详细用Python代码和数学</a> <br/> XIII。<a class="ae jg" href="https://towardsai.net/p/machine-learning/building-neural-networks-with-python-code-and-math-in-detail-ii-bbe8accbf3d1" rel="noopener ugc nofollow" target="_blank">用Python构建神经网络</a> <br/> XIV。<a class="ae jg" href="https://towardsai.net/p/machine-learning/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e" rel="noopener ugc nofollow" target="_blank">神经网络的主要类型</a> <br/>十五。<a class="ae jg" href="https://towardsai.net/p/machine-learning/monte-carlo-simulation-an-in-depth-tutorial-with-python-bcf6eb7856c8" rel="noopener ugc nofollow" target="_blank">用Python编写的蒙特卡洛模拟教程</a> <br/> XVI。<a class="ae jg" href="https://towardsai.net/p/nlp/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0" rel="noopener ugc nofollow" target="_blank">Python自然语言处理教程</a></p></div><div class="ab cl pf pg hx ph" role="separator"><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk pl"/><span class="pi bw bk pj pk"/></div><div class="im in io ip iq"><h2 id="bfcf" class="qr nr jj bd ns qs qt dn nw qu qv dp oa lq qw qx oc lu qy qz oe ly ra rb og jp bi translated">参考资料:</h2><p id="ae91" class="pw-post-body-paragraph lh li jj lj b lk om kt lm ln on kw lp lq oo ls lt lu op lw lx ly oq ma mb mc im bi translated">[1]激活功能|维基百科|<a class="ae jg" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Activation_function</a></p><p id="c90c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]感知器:大脑中信息存储和组织的概率模型| Frank Rosenblatt |宾夕法尼亚大学|<a class="ae jg" href="https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf" rel="noopener ugc nofollow" target="_blank">https://www . ling . upenn . edu/courses/cogs 501/Rosenblatt 1958 . pdf</a></p><p id="0b91" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3]康奈尔航空实验室的弗兰克·罗森布拉特的Mark I感知机。纽约州布法罗，1960 | Instagram，卡耐基梅隆大学机器学习系|【https://www.instagram.com/p/Bn_s3bjBA7n/ T4】</p><p id="fb81" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4]反向传播|维基百科|<a class="ae jg" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Backpropagation</a></p><p id="9194" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]神经网络动物园| Stefan Leijnen和Fjodor van Veen | Research Gate |<a class="ae jg" href="https://www.researchgate.net/publication/341373030_The_Neural_Network_Zoo" rel="noopener ugc nofollow" target="_blank">https://www . Research Gate . net/publication/341373030 _ The _ Neural _ Network _ Zoo</a></p><p id="c71d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6]https://creativecommons.org/licenses/by/4.0/知识共享许可CCBY |<a class="ae jg" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>