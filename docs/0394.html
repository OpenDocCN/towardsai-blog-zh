<html>
<head>
<title>Review: DCNv2 — Deformable ConvNets v2 (Object Detection &amp; Instance Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:DCNv2 —可变形ConvNets v2(对象检测和实例分割)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/review-dcnv2-deformable-convnets-v2-object-detection-instance-segmentation-3d8a18bee2f5?source=collection_archive---------0-----------------------#2020-04-11">https://pub.towardsai.net/review-dcnv2-deformable-convnets-v2-object-detection-instance-segmentation-3d8a18bee2f5?source=collection_archive---------0-----------------------#2020-04-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e147" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">增强的<a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN / DCNv1 </a>，更易变形，效果更好</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/476f5f40db3ea87a012fe2988c6a09ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8IOU89YUu7yPSG5f6dXnIA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">可变形RoI合并</strong></figcaption></figure><p id="108c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di">在</span>这篇文章中，评述了由中国科学技术大学和微软亚洲研究院(MSRA)合作的<strong class="kz ir">可变形ConvNets v2 (DCNv2) </strong>。在本文中，<strong class="kz ir"> DCNv2增强了2017年ICCV发表的</strong> <a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="kz ir"> DCNv1 </strong> </a>，通过<strong class="kz ir">引入多一个调制模块来调制来自不同空间位置/面元</strong>的输入特征幅度。并发表在<strong class="kz ir"> 2019 CVPR </strong>上，引用次数超过100次。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----3d8a18bee2f5--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><h1 id="0683" class="me mf iq bd kw mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">概述</h1><ol class=""><li id="e6e1" class="mv mw iq kz b la mx ld my lg mz lk na lo nb ls nc nd ne nf bi translated"><strong class="kz ir">简评</strong> <a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="kz ir"> DCNv1 </strong> </a></li><li id="80d8" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls nc nd ne nf bi translated"><strong class="kz ir">DCN v2中的调制可变形模块</strong></li><li id="134c" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls nc nd ne nf bi translated"><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> <strong class="kz ir"> R-CNN </strong> </a> <strong class="kz ir">特征模仿</strong></li><li id="c174" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls nc nd ne nf bi translated"><strong class="kz ir">一些分析</strong></li><li id="78f8" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls nc nd ne nf bi translated"><strong class="kz ir">实验结果</strong></li></ol><h1 id="c1a5" class="me mf iq bd kw mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated"><strong class="ak"> 1。简要回顾</strong> <a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCNv1 </a></h1><h2 id="504d" class="nl mf iq bd kw nm nn dn mj no np dp mn lg nq nr mp lk ns nt mr lo nu nv mt nw bi translated">1.1.可变形卷积</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nx"><img src="../Images/b1d989a882a0f7909d8ece9a720230dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*75fkXORpx0Xv9TNw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw"/><a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="bd kw">DCN v1</strong></a>中的可变形卷积</figcaption></figure><ul class=""><li id="ed83" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated">规则卷积是在规则网格<em class="oc"> R </em>上操作的。</li><li id="1efd" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">可变形卷积运算在<em class="oc"> R </em>上进行，但是每个点都增加了一个可学习的偏移量∈<em class="oc">pn</em>。</li><li id="e29b" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">卷积用于生成2个<em class="oc"> N个</em>对应于<em class="oc"> N个</em> 2D偏移<strong class="kz ir">∈<em class="oc">pn</em>(<em class="oc">x</em>-方向和<em class="oc">y</em>-每个偏移的方向)</strong>的特征图。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi od"><img src="../Images/78028185edcf358046d023048e83fd2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/0*QBIsJwttGikdAY5j.png"/></div></figure><ul class=""><li id="4062" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated">如上所示，可变形卷积将根据输入图像或特征图为卷积选取不同位置的值。</li></ul><h2 id="3eb3" class="nl mf iq bd kw nm nn dn mj no np dp mn lg nq nr mp lk ns nt mr lo nu nv mt nw bi translated">1.2.可变形RoI池</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oe"><img src="../Images/dd9fbae66fe039180e3bb9bd0a9fed57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mM3KfvI7BX72i4Vm.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">可变形RoI汇集在</strong> <a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="bd kw"> DCNv1 </strong> </a></figcaption></figure><ul class=""><li id="98e2" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated">常规RoI池将任意大小的输入矩形区域转换为固定大小的特征。</li><li id="c938" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">在可变形RoI合并中，<strong class="kz ir">首先，在顶部路径</strong>，我们仍然需要<strong class="kz ir">常规RoI合并</strong>来生成合并后的特征图。</li><li id="8d62" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">然后，<strong class="kz ir">全连接(fc)层生成归一化偏移</strong><strong class="kz ir">∈<em class="oc">p</em>̂<em class="oc">ij</em></strong>和<strong class="kz ir">，然后转换为偏移∈<em class="oc">pij</em></strong>(右下方的等式)，其中γ=0.1。</li><li id="fd54" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">偏移归一化对于使偏移学习不随RoI尺寸变化是必要的。</li><li id="3641" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">最后，<strong class="kz ir">在底部路径，</strong>我们执行<strong class="kz ir">可变形RoI合并。输出要素地图基于具有增大偏移的区域进行合并。</strong></li></ul></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><h1 id="fad0" class="me mf iq bd kw mg om mi mj mk on mm mn jw oo jx mp jz op ka mr kc oq kd mt mu bi translated"><strong class="ak"> 2。</strong>在<strong class="ak"> DCNv2 </strong>中调制的可变形模块</h1><h2 id="3bd8" class="nl mf iq bd kw nm nn dn mj no np dp mn lg nq nr mp lk ns nt mr lo nu nv mt nw bi translated">2.1.DCNv2中的调制可变形卷积</h2><ul class=""><li id="1363" class="mv mw iq kz b la mx ld my lg mz lk na lo nb ls ob nd ne nf bi translated">在DCNv2中，每个样本不仅经历一个学习偏移(<a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCNv1 </a>)，还被<strong class="kz ir">一个学习特征幅度</strong>调制。网络模块因此被给予改变空间分布和<strong class="kz ir">其样本的相对影响</strong>的能力。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9bb141b3c99ebd62d8d130c90d5b4fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*BmXfYTRyXQL60zU2EA8U7Q.png"/></div></figure><ul class=""><li id="e346" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated"><strong class="kz ir">δMK是第k个位置</strong>的调制标量。(<em class="oc"> K </em>是卷积网格内的位置数。)</li><li id="8389" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">调制标量δMK位于[0，1] 范围内<strong class="kz ir">。</strong></li><li id="8875" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">δPK和δMK都是通过应用于相同输入特征映射x的单独卷积层获得的。</li><li id="04f1" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">该卷积层具有与当前卷积层相同的空间分辨率和膨胀。</li><li id="9dc0" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">输出为3K通道，其中<strong class="kz ir">前2K个通道对应于学习到的偏移δPK</strong>，而<strong class="kz ir">剩余的K个通道被进一步馈送到sigmoid层以获得调制标量δMK</strong>。</li></ul><h2 id="6986" class="nl mf iq bd kw nm nn dn mj no np dp mn lg nq nr mp lk ns nt mr lo nu nv mt nw bi translated">2.2.DCNv2中的调制可变形RoI池</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi os"><img src="../Images/91f23eda5ccc534a88724e41f204f861.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*8XPTaEJeHE663xC5irviiA.png"/></div></figure><ul class=""><li id="c29e" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated">类似地，在RoI合并中，增加δMK以在学习的偏移位置调制输入特征值的幅度。</li></ul></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><h1 id="9b96" class="me mf iq bd kw mg om mi mj mk on mm mn jw oo jx mp jz op ka mr kc oq kd mt mu bi translated">3.<a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a> <strong class="ak">功能模仿</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/420eea1fd3b003ac847821665ecf6f6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*ni6nymXj8a0aPE8XE7oMwA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">网络训练用</strong><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"><strong class="bd kw">R-CNN</strong></a><strong class="bd kw">功能模仿</strong></figcaption></figure><ul class=""><li id="7c61" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated">在可变形的<a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快的R-CNN </a>的每RoI特征上结合了特征模拟损失，以迫使它们类似于从裁剪图像中提取的<a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a>特征。</li><li id="e61f" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">这个辅助训练的目的是为了让<strong class="kz ir">驱动可变形的</strong> <a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="kz ir">更快的R-CNN </strong> </a> <strong class="kz ir">去学习更“专注”的特征表现像</strong> <a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> <strong class="kz ir"> R-CNN </strong> </a>。</li><li id="6b41" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated"><strong class="kz ir">特征模仿损失定义在</strong><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"><strong class="kz ir">R-CNN</strong></a><strong class="kz ir">的特征和</strong> <a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="kz ir">更快R-CNN </strong> </a>的特征之间的余弦相似度上:</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/57e5270fd4b63f2943bbbd2542d96cd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*-I4CkKgEtkLd_ch2nRCZjQ.png"/></div></figure><ul class=""><li id="d145" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated"><strong class="kz ir">网络训练是由特征模仿损失和</strong> <a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> <strong class="kz ir"> R-CNN </strong> </a> <strong class="kz ir">分类损失，连同原损失项在</strong> <a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="kz ir">中更快的R-CNN </strong> </a> <strong class="kz ir">。</strong>新引入的两个损失项的损失权重是原<a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">fast R-CNN</a>损失项的0.1倍。</li><li id="bdc9" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated"><strong class="kz ir">在推论中，只有更快的</strong><a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="kz ir">R-CNN</strong></a><strong class="kz ir">网络被应用在测试图像上，而没有辅助的</strong><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"><strong class="kz ir">R-CNN</strong></a><strong class="kz ir">分支。因此，<a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a>特征模仿在推理中没有引入额外的计算。</strong></li></ul></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><h1 id="eee9" class="me mf iq bd kw mg om mi mj mk on mm mn jw oo jx mp jz op ka mr kc oq kd mt mu bi translated">4.<strong class="ak">一些分析</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ov"><img src="../Images/fad74ac13c2e6d65faf4bac860e5d2e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JmOdYV8CI4vAEc_WMsTsfA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">第一行:有效采样位置，第二行:有效感受野，第三行:误差限制显著区域</strong>(( c)中省略了有效采样位置，因为它们与(b)中的相似)</figcaption></figure><ul class=""><li id="95ef" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated"><strong class="kz ir">有效采样位置</strong>:卷积后的采样位置。</li><li id="2e8a" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated"><strong class="kz ir">有效感受野:</strong>并非网络节点感受野内的所有像素都对其响应做出同等贡献。这些贡献的差异由有效感受野表示，其值被计算为节点响应相对于每个图像像素的强度扰动的梯度。</li><li id="fc76" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated"><strong class="kz ir">误差限制的显著区域</strong>:如果我们移除不影响它的图像区域，网络节点的响应将不会改变，正如最近对图像显著性的研究所示。基于这一特性，我们可以将节点的支持区域确定为在小误差范围内给出与完整图像相同响应的最小图像区域。</li><li id="6963" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated"><strong class="kz ir">与</strong> <a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="kz ir"> DCNv1 </strong> </a> <strong class="kz ir">相比，DCNv2中丰富的可变形建模的空间支持展现出对图像内容更好的适应性。</strong></li></ul></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><h1 id="06a7" class="me mf iq bd kw mg om mi mj mk on mm mn jw oo jx mp jz op ka mr kc oq kd mt mu bi translated">5.<strong class="ak">实验结果</strong></h1><h2 id="7058" class="nl mf iq bd kw nm nn dn mj no np dp mn lg nq nr mp lk ns nt mr lo nu nv mt nw bi translated">5.1.调制可变形Conv和调制可变形感兴趣区域池</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ow"><img src="../Images/75705202ed5e8c85aae273ec4d6bf03c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*erBHtB7Jb3M9hi9lS2gsAw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">COCO 2017验证集上的对象检测和实例分割(1000像素的短边作为输入图像)</strong></figcaption></figure><ul class=""><li id="53b0" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated"><a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快的R-CNN </a>和<a class="ae kf" href="https://medium.com/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4" rel="noopener">屏蔽R-CNN </a>和<a class="ae kf" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet-50 </a>用作基线，如上图所示。</li><li id="7979" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">仅用<strong class="kz ir">普通的conv </strong>、<a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">、<strong class="kz ir">更快的R-CNN </strong>、</a>、<strong class="kz ir"> : 32.1% AP </strong>进行物体探测任务。以及<a class="ae kf" href="https://medium.com/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4" rel="noopener"> <strong class="kz ir">屏蔽R-CNN</strong></a><strong class="kz ir">:32.2% AP</strong>用于实例分割任务。</li><li id="a9cb" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">在conv5中应用了<a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCNv1 </a>中的<strong class="kz ir">可变形conv </strong>，在<a class="ae kf" href="https://medium.com/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4" rel="noopener">中使用了<strong class="kz ir">对齐RoI池</strong>掩膜R-CNN </a>、<a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="kz ir">更快R-CNN</strong></a><strong class="kz ir">:38.0% AP</strong>、<strong class="kz ir"> </strong> <a class="ae kf" href="https://medium.com/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4" rel="noopener"> <strong class="kz ir">掩膜R-CNN</strong></a><strong class="kz ir">:35.3% AP</strong>。</li><li id="6417" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">将DCNv2中的<strong class="kz ir">调制可变形conv </strong>应用于conv3至conv5，将<strong class="kz ir">调制可变形RoI合并</strong>、<a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="kz ir">更快R-CNN</strong></a><strong class="kz ir">:41.7% AP</strong>、<strong class="kz ir"> </strong> <a class="ae kf" href="https://medium.com/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4" rel="noopener"> <strong class="kz ir">屏蔽R-CNN</strong></a><strong class="kz ir">:37.3% AP</strong>。</li></ul><h2 id="5098" class="nl mf iq bd kw nm nn dn mj no np dp mn lg nq nr mp lk ns nt mr lo nu nv mt nw bi translated">5.2.<a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> <strong class="ak"> R-CNN </strong> </a>特征模仿</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/a2fc95812734bfc0e3911b84c8516f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*Iv-tcwb1P8XEpnqxZduOwA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">特征模仿变体</strong></figcaption></figure><ul class=""><li id="b6e2" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated">仅考虑前景物体的特征进行特征模仿损失，获得最高的AP。</li></ul><h2 id="6a52" class="nl mf iq bd kw nm nn dn mj no np dp mn lg nq nr mp lk ns nt mr lo nu nv mt nw bi translated">5.3.主干变体</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/fe1c6a82f7c4117e8e37c6da861c0643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*DWTrNqDqqwSCmZaAKA06oA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">主干变异</strong></figcaption></figure><ul class=""><li id="e75e" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated">有了<a class="ae kf" href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=post_page---------------------------" rel="noopener" target="_blank"> ResNeXt </a>这样更深更好的主干，就获得了更高的AP。</li></ul><h2 id="453a" class="nl mf iq bd kw nm nn dn mj no np dp mn lg nq nr mp lk ns nt mr lo nu nv mt nw bi translated">5.4.ImageNet图像分类任务</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/f03a26138177612c1db3bc667860af75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*XIMjepw_PYuJjKMIYRdW9Q.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><strong class="bd kw">验证集的前1名和前5名分类准确率</strong></figcaption></figure><ul class=""><li id="7284" class="mv mw iq kz b la lb ld le lg ny lk nz lo oa ls ob nd ne nf bi translated">作者还尝试了ImageNet图像分类任务。</li><li id="04c0" class="mv mw iq kz b la ng ld nh lg ni lk nj lo nk ls ob nd ne nf bi translated">DCNv2在常规和<a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCNv1 </a>基线的基础上实现了显著的改进，而额外的计算开销很小。</li></ul></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><h1 id="9ad3" class="me mf iq bd kw mg om mi mj mk on mm mn jw oo jx mp jz op ka mr kc oq kd mt mu bi translated">参考</h1><p id="a6b9" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg pa li lj lk pb lm ln lo pc lq lr ls ij bi translated">【2019 CVPR】【dcnv 2】<br/><a class="ae kf" href="https://arxiv.org/abs/1811.11168" rel="noopener ugc nofollow" target="_blank">可变形ConvNets v2:可变形性更强，效果更好</a></p><h1 id="5423" class="me mf iq bd kw mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">目标检测</h1><p id="d16c" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg pa li lj lk pb lm ln lo pc lq lr ls ij bi translated">[ <a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------" rel="noopener">过食</a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------" rel="noopener">快R-CNN </a> ] [ <a class="ae kf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快R-CNN</a>][<a class="ae kf" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae kf" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a>][<a class="ae kf" href="https://towardsdatascience.com/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=post_page---------------------------" rel="noopener" target="_blank">CRAFT</a>][<a class="ae kf" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank">R-FCN</a>][<a class="ae kf" href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------" rel="noopener" target="_blank">ION</a>][<a class="ae kf" href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------" rel="noopener" target="_blank">multipath ne [</a><a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------" rel="noopener">TDM</a>][<a class="ae kf" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae kf" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>][<a class="ae kf" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolo v1</a>][<a class="ae kf" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolo v2/yolo 9000</a>][<a class="ae kf" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolo v3</a>][<a class="ae kf" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank">FPN</a>][<a class="ae kf" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank">retina net</a>][<a class="ae kf" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank">DCN/DCN v1</a>[<a class="ae kf" href="https://medium.com/towards-artificial-intelligence/review-dcnv2-deformable-convnets-v2-object-detection-instance-segmentation-3d8a18bee2f5" rel="noopener">DCN v2</a></p><h1 id="557e" class="me mf iq bd kw mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">实例分割</h1><p id="799d" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg pa li lj lk pb lm ln lo pc lq lr ls ij bi translated">[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b?source=post_page---------------------------" rel="noopener">SDS</a>][<a class="ae kf" href="https://towardsdatascience.com/review-hypercolumn-instance-segmentation-367180495979?source=post_page---------------------------" rel="noopener" target="_blank">Hypercolumn</a>][<a class="ae kf" href="https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339?source=post_page---------------------------" rel="noopener" target="_blank">deep Mask</a>][<a class="ae kf" href="https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61?source=post_page---------------------------" rel="noopener" target="_blank">sharp Mask</a>][<a class="ae kf" href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------" rel="noopener" target="_blank">multipath net</a>][<a class="ae kf" href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34?source=post_page---------------------------" rel="noopener" target="_blank">MNC</a>][<a class="ae kf" href="https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92?source=post_page---------------------------" rel="noopener" target="_blank">instance fcn</a>][<a class="ae kf" href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=post_page---------------------------" rel="noopener" target="_blank">FCIS</a>][<a class="ae kf" href="https://medium.com/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4" rel="noopener">Mask R-CNN</a>][<a class="ae kf" href="https://medium.com/towards-artificial-intelligence/review-dcnv2-deformable-convnets-v2-object-detection-instance-segmentation-3d8a18bee2f5" rel="noopener">DCN v2</a>]</p><h1 id="5013" class="me mf iq bd kw mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated"><a class="ae kf" href="https://medium.com/@sh.tsang/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我之前的其他评论</a></h1></div></div>    
</body>
</html>