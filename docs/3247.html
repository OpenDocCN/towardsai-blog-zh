<html>
<head>
<title>Google UniTune: Text-driven Image Editing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Google UniTune:文本驱动的图像编辑</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/google-unitune-text-driven-image-editing-4b176b1b16a1?source=collection_archive---------1-----------------------#2022-10-24">https://pub.towardsai.net/google-unitune-text-driven-image-editing-4b176b1b16a1?source=collection_archive---------1-----------------------#2022-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="37c3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何用文字修改你的图像</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1483edf1a657b4c3966f3dd1753bd753.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uz22iMSioPrtJKlbXUrWiw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">作者使用<a class="ae kv" href="https://arxiv.org/pdf/2204.06125.pdf" rel="noopener ugc nofollow" target="_blank"> DALL-E 2 </a>生成的图像</figcaption></figure><p id="956b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谷歌最近在arXiv上发布了一款新机型:<a class="ae kv" href="https://arxiv.org/pdf/2210.09477.pdf" rel="noopener ugc nofollow" target="_blank"> UniTune </a>。该模型能够进行一般的文本驱动的图像编辑。让我们一起来发现它在做什么，为什么这种模式是一种进步。</p><p id="0a0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">为什么是这种模式？为什么是现在？</strong></p><p id="7176" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">任何试图从事人工智能艺术的人都注意到了一些奇怪的事情。当然，DALL-E、稳定扩散和中途提出了非凡的结果，但在最初的热情之后，确切地得到你想要的是相当复杂的。你必须对参数做一些改动，尝试改变用词，尝试添加更多的形容词。</p><p id="74d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法被称为<strong class="ky ir">即时工程</strong>，在这种方法中，你试图将所有的元素组装起来，以达到预期的效果。还有一种叫做<strong class="ky ir">逆向工程</strong>的方法，从一幅图像和它的文本提示开始，你试图要么重新创建它，要么识别允许它生成的元素。</p><p id="32ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">顺便说一下，也有一些资源允许你尝试直接从图像开始，尝试重建文本提示的元素(你可以试试这个<a class="ae kv" href="https://colab.research.google.com/github/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator.ipynb#scrollTo=ytxkysgmrJEi" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>)。然而，结果远不如人们所希望的那么令人兴奋，并且经常需要多次尝试。</p><p id="953d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有深度学习经验的人都知道<strong class="ky ir">微调</strong>是什么。微调模型可以使模型适合您的特定情况，而不必再次从头开始训练它。该模型考虑了一般能力，并适应于特定任务(例如，对数百万张图像进行训练的卷积网络进行微调，以识别花卉品种)。这种方法也有可能稳定扩散。这就引出了一个问题，<strong class="ky ir">微调一个从文本提示生成图像的模型意味着什么？</strong></p><p id="d76e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在稳定扩散的情况下，<a class="ae kv" href="https://towardsdatascience.com/how-to-fine-tune-stable-diffusion-using-textual-inversion-b995d7ecc095" rel="noopener" target="_blank">微调是指对嵌入</a>进行微调，根据自定义样式或对象创建个性化图像。我们不从头开始微调模型，而是用某些类型的图像的新示例来呈现它，以允许它专门化。</p><p id="8490" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">问题是我们经常不知道我们需要微调多少个和哪些例子。Google Uni-tune是如何解决这个问题的？</p><p id="5b7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Google UniTune </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/cb86af307055d5faaacd73e782b9bdb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yealr5QCvlcZLspH0FuEow.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来自原文章:<a class="ae kv" href="https://arxiv.org/pdf/2210.09477.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="c4b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">先来个大概描述。Google Unitune获取图像和文本描述并编辑图像，同时保持对原始图像的高度语义和视觉保真度。此外，不像其他模型，它不需要另一个输入或面具或草图。</p><blockquote class="lt lu lv"><p id="4bb3" class="kw kx lw ky b kz la jr lb lc ld ju le lx lg lh li ly lk ll lm lz lo lp lq lr ij bi translated">我们方法的核心是观察到，通过正确选择参数，我们可以在单个图像上微调大型文本到图像扩散模型，鼓励模型保持对输入图像的保真度，同时仍然允许表达操作。我们使用Imagen作为我们的文本到图像模型，但我们希望UniTune也能与其他大规模模型一起工作。我们在一系列不同的用例中测试了我们的方法，并展示了它的广泛适用性——<a class="ae kv" href="https://arxiv.org/pdf/2210.09477.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ma"><img src="../Images/cc9f974375a3b8a21ea0c1dc4dd786f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DEJuKiBQqircRjzHEg64lg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来自原文章:<a class="ae kv" href="https://arxiv.org/pdf/2210.09477.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="3eb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同时，从描述中我们可以看到，UniTune背后的模型是Imagen(一个稳定的类扩散模型，以文本提示作为输入，返回一个图像)。第二，一般来说，我们尽量避免过拟合，微调有助于防止过拟合。然而，在这种情况下，由于我们希望保持图像的高保真度，一些过度拟合是有益的。</p><p id="c905" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型采用左边的图像和提示(在本例中为“minion”)，使用不同数量的分类器自由引导权重，结果演变并微调图像</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mb"><img src="../Images/ab14a73d3214359ca8310e91b47f98b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v7zz2SqlhRXAVm9oQKb_Rg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来自原文章:<a class="ae kv" href="https://arxiv.org/pdf/2210.09477.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="b8c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文简要解释了微调过程:</p><blockquote class="lt lu lv"><p id="0f6a" class="kw kx lw ky b kz la jr lb lc ld ju le lx lg lh li ly lk ll lm lz lo lp lq lr ij bi translated">我们的目标是将(base_image，edit_prompt)的输入转换为edited_image。简而言之，我们的系统在(base_image，rare_tokens)对上微调文本到图像和超分辨率图像模型，用于非常少的迭代次数，然后在以“[rare_tokens] edit_prompt”的形式调节文本的同时从模型中采样。</p></blockquote><p id="5c8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们还使用无分类器指导，这是Imagen等文本到图像模型使用的一种技术，用于指导模型与文本提示保持一致。<br/>此外，作者还使用插值进行了插值实验。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mc"><img src="../Images/9b9dffa95e63d51671b657bf70d9dc26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uFPsrPY1SOsgUZ8veIllMA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来自原文章:<a class="ae kv" href="https://arxiv.org/pdf/2210.09477.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="5556" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，in-panting(由DALL-E 2使用)需要一个显式遮罩来执行编辑。另一方面，UniTune不需要掩码，只需要文本提示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mb"><img src="../Images/60e93293195d3cf45c5d99fa2c858b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IzWqW4hCghGZv8GCo4Z5Rw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来自原文章:<a class="ae kv" href="https://arxiv.org/pdf/2210.09477.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="ce34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">UniTune并不缺乏限制，通常，当Imagen遇到困难时，这些限制就会出现(这意味着该技术的能力受到底层文本到图像模型的限制)。在某些情况下，主体面部可能会被交换或克隆(重复不止一次)。另一个突出的问题是，在某些情况下，我们很难在保真度和表现力之间找到一个好的平衡(尤其是在处理小的编辑时)。</p><p id="78cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，延迟也是一个问题；第一步(微调基础模型)使用TPUv4需要3分钟，并且需要对每个输入图像运行一次。另一方面，后续步骤需要大约30秒。此外，UniTune使用许多参数来调整最终输出。</p><p id="d592" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">离别的思念</strong></p><p id="404b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，Google展示了UniTune，这是一种简单的文本驱动图像编辑方法。UniTune在场景中放置对象或进行全局编辑，仅保留文本描述中的语义细节。</p><p id="aaf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在未来，这种方法可以用来编辑照片或做其他图像编辑，通过文本提示向程序解释你想做什么改变。此外，手机用户也可以使用这种方法(过滤，在上传照片前快速编辑照片)。无论如何，作者指出，它可以在未来得到改进(自动调整保真度-表现力度盘，提高成功率，加快生成过程)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi md"><img src="../Images/6927f43c64339ef088d3d08e49a36e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vlj_x52kpK-nGdKWw8UlKA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来自原文章:<a class="ae kv" href="https://arxiv.org/pdf/2210.09477.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="c7a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，这种想法开辟了更多的全球视角，例如更好地理解扩散模型嵌入，减少权重的数量，以及通过单个示例测试对其他模型(如GPT)使用微调的假设。</p><p id="ea64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，在文章中，他们也讨论了他们工作中潜在的问题:</p><blockquote class="lt lu lv"><p id="8a38" class="kw kx lw ky b kz la jr lb lc ld ju le lx lg lh li ly lk ll lm lz lo lp lq lr ij bi translated">然而，我们认识到这项研究的应用可能会以复杂的方式影响个人和社会(见[2]的概述)。特别是，这种方法说明了这种模型可以很容易地用于改变敏感特征，如肤色、年龄和性别。尽管通过图像编辑软件这早已成为可能，但文本到图像的模式可以使它变得更容易</p></blockquote><h1 id="d6ad" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">如果你觉得有趣:</h1><p id="6c16" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">你可以寻找我的其他文章，你也可以<a class="ae kv" href="https://salvatore-raieli.medium.com/subscribe" rel="noopener"> <strong class="ky ir">订阅</strong> </a>在我发表文章时获得通知，你也可以在<strong class="ky ir"/><a class="ae kv" href="https://www.linkedin.com/in/salvatore-raieli/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">LinkedIn</strong></a><strong class="ky ir">上连接或联系我。</strong>感谢您的支持！</p><p id="64c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我的GitHub知识库的链接，我计划在这里收集代码和许多与机器学习、人工智能等相关的资源。</p><div class="nb nc gp gr nd ne"><a href="https://github.com/SalvatoreRa/tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ir gy z fp nj fr fs nk fu fw ip bi translated">GitHub - SalvatoreRa/tutorial:关于机器学习、人工智能、数据科学的教程…</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">关于机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码(python…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">github.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns kp ne"/></div></div></a></div><p id="dc0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者随意查看我在Medium上的其他文章:</p><div class="nb nc gp gr nd ne"><a href="https://medium.com/mlearning-ai/reimagining-the-little-prince-with-ai-7e9f68ed8b3c" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ir gy z fp nj fr fs nk fu fw ip bi translated">用艾重新想象小王子</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">人工智能如何从《小王子》中的人物描述中重构他们的形象</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">medium.com</p></div></div><div class="nn l"><div class="nt l np nq nr nn ns kp ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="https://towardsdatascience.com/how-artificial-intelligence-could-save-the-amazon-rainforest-688fa505c455" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ir gy z fp nj fr fs nk fu fw ip bi translated">人工智能如何拯救亚马逊雨林</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">亚马逊正处于危险之中，人工智能可以帮助保护它</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="nu l np nq nr nn ns kp ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="https://towardsdatascience.com/blending-the-power-of-ai-with-the-delicacy-of-poetry-3671f82d2e1" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ir gy z fp nj fr fs nk fu fw ip bi translated">融合人工智能的力量和诗歌的细腻</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">人工智能现在能够从文本中生成图像，如果我们给它们提供伟大诗人的话语会怎么样？梦幻之旅…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="nv l np nq nr nn ns kp ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="https://towardsdatascience.com/speaking-the-language-of-life-how-alphafold2-and-co-are-changing-biology-97cff7496221" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ir gy z fp nj fr fs nk fu fw ip bi translated">说生命的语言:AlphaFold2和公司如何改变生物学</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">人工智能正在重塑生物学研究，并开辟治疗的新领域</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="nw l np nq nr nn ns kp ne"/></div></div></a></div></div></div>    
</body>
</html>