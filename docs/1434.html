<html>
<head>
<title>Deep Hashing for Similarity Search</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于相似性搜索的深度散列</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/deep-hashing-for-similarity-search-9273aac054db?source=collection_archive---------0-----------------------#2021-01-26">https://pub.towardsai.net/deep-hashing-for-similarity-search-9273aac054db?source=collection_archive---------0-----------------------#2021-01-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2be8" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="d834" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">文献中的新型深度哈希神经网络综述</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/dcee2dfa96e8ef35cc77e2a191902755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3xFytoyicZOLYE1dPlANTQ.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@tolga__?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">托尔加·乌尔坎</a>在<a class="ae le" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="5a9e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di">近年来</span><strong class="lh ja">近似最近邻(ANN)</strong>【1】搜索已经成为一个突出的研究课题，以有效地处理现实世界应用中不断增长的数据量。人工神经网络有多种应用<a class="ae le" href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Applications" rel="noopener ugc nofollow" target="_blank">包括模式识别、推荐系统、相似性搜索、聚类分析等。然而，在本文中，我们将主要关注<strong class="lh ja">相似性搜索</strong>的应用。此外，在现有的人工神经网络技术中，<strong class="lh ja">哈希</strong>因其快速的查询速度和低内存成本而在高维数据的管理、存储和处理中变得非常流行[2–10]。</a></p><p id="7222" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">哈希模型采用输入数据点(图像、文档等)。)并输出表示该数据点的比特序列或哈希码。此外，基于基本属性的不同散列模型的<a class="ae le" href="https://learning2hash.github.io/base-taxonomy/" rel="noopener ugc nofollow" target="_blank">分类法</a>如下所示，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mk"><img src="../Images/d8df5a97123eed14f438cc80489007e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*alo0ARiFidehbKNL9XOjTA.png"/></div></div></figure><p id="cec0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文中，我们将主要关注<strong class="lh ja">数据相关散列法、</strong>，其中散列函数的学习考虑了<strong class="lh ja"> </strong>输入数据分布。这一类有代表性的方法有<strong class="lh ja">学习哈希</strong><strong class="lh ja">(L2H)</strong>【11–13】方法。</p><p id="83e6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://cs.nju.edu.cn/lwj/slides/L2H.pdf" rel="noopener ugc nofollow" target="_blank">L2H</a>【12，13】是一组数据相关的哈希方法，旨在学习具有较短哈希代码的紧凑且相似性保持的按位表示，以这种方式将相似的输入映射到附近的二进制哈希代码。具体从不同的L2H方法，我们将深入到<strong class="lh ja">深度哈希</strong>。深度散列构成了利用深度学习的监督L2H，并且包括不同的散列方法，其中一些关于图像数据的相似性搜索的新颖方法以及它们不同的功能设计方面被进一步阐述，</p><p id="9805" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">深度成对监督哈希(DPSH) </strong></p><p id="21ec" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">李等人[14]提出了一种新的深度哈希方法，称为<strong class="lh ja">深度成对监督哈希(DPSH) </strong>，它在端到端架构中使用监督成对标签同时进行特征学习和哈希码学习，利用反馈机制来学习更好的哈希码。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ml"><img src="../Images/e2ddd13beb5985b1a08ffb53679150fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enn9GL49hON7Yq4vgoBAJg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">深度成对监督哈希(DPSH)的端到端架构[14]</figcaption></figure><p id="67de" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如上图所示，用于图像检索的端到端学习框架具有三个关键组件，第一个组件是从像素学习图像表示的<strong class="lh ja"><em class="mm">【CNN】</em></strong>，第二个组件是将学习到的图像表示映射到散列码的<strong class="lh ja"> <em class="mm">散列函数</em> </strong>，第三个组件是与成对标签相比较来测量生成的散列码的<strong class="lh ja"> <em class="mm">损失函数</em> </strong>。</p><p id="53ad" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">基于三元组的深度哈希</strong></p><p id="b8df" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">受李等人关于的工作的启发，王等人[15]提出了一种新的<strong class="lh ja">基于三元组的深度哈希</strong>方法(对标签进行排序的一种特殊情况)，该方法以端到端的方式同时执行图像特征和哈希码学习，旨在最大化输入三元组标签的可能性。该方法通过实验评估优于基于<br/>成对标签的深度散列和基于预先存在的三元标签的深度散列方法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mn"><img src="../Images/99f471675c95fb7688b28c7c3fbd6f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W7PRqTrWw3uJ_Ox31UG0UA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">基于三元组的深度哈希的端到端架构[15]</figcaption></figure><p id="d009" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如上图所示，这个端到端学习框架有三个组件，一个<strong class="lh ja"> <em class="mm">深度神经网络</em> </strong> <em class="mm"> </em>从图像中学习特征，一个<strong class="lh ja"> <em class="mm">全连接层</em> </strong>从这些特征中学习哈希码，以及<strong class="lh ja"> <em class="mm">损失函数</em> </strong>。</p><blockquote class="mo mp mq"><p id="b3de" class="lf lg mm lh b li lj ka lk ll lm kd ln mr lp lq lr ms lt lu lv mt lx ly lz ma ij bi translated"><strong class="lh ja">目标函数</strong></p></blockquote><p id="f99c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">潜在因素散列法(LFH) </strong></p><p id="c238" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于学习相似性保持哈希码的目标函数，Zhang等人[16]首先提出了一种监督哈希方法，<strong class="lh ja">【潜在因子哈希(LFH) </strong>基于潜在因子模型学习相似性保持二进制码，可以有效地用于训练大规模监督哈希问题。LFH将成对的<br/>相似性的可能性建模为对应点之间的<em class="mm">海明距离</em>的函数。然而，该目标函数的优化是一个不容易解决的离散优化问题，因此通过将离散码转换为连续码来放松该约束，这可能无法实现令人满意的性能[10]。</p><p id="8057" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，利用LFH的深度哈希方法提出了新的策略，以离散的方式解决这一离散优化问题，从而提高精度，如DPSH [14]。Wang等人[15]在基于三元组的深度哈希<strong class="lh ja"> </strong>中更倾向于以张等人[16]提出的连续方式解决这种离散优化问题，但是考虑了损失函数中这种松弛所引起的量化误差。</p><blockquote class="mo mp mq"><p id="4a37" class="lf lg mm lh b li lj ka lk ll lm kd ln mr lp lq lr ms lt lu lv mt lx ly lz ma ij bi translated"><strong class="lh ja">损失函数优化</strong></p></blockquote><p id="81c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">深度哈希网络(DHN) </strong></p><p id="9c09" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于成对监督哈希的优化损失函数，Zhu等人[17]提出了一种新的<strong class="lh ja">深度哈希网络(DHN) </strong>架构，可以同时优化学习到的语义相似对上的成对交叉熵损失和学习到的哈希码上的成对量化损失。这有利于更好地保持相似性的学习和控制所学习的散列码的质量。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mu"><img src="../Images/17f044de6060310030be71fdd8a6bad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z9SbsCbq_VwuMtFYOqjBhQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">深度哈希网络的架构(DHN) [17]</figcaption></figure><p id="1096" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如上图所示，DHN的流水线架构由四个关键组件组成，第一个组件是由多个卷积池层组成的<strong class="lh ja"><em class="mm"/></strong>以学习图像表示，第二个组件是用于生成紧凑哈希码的<strong class="lh ja"> <em class="mm">全连接哈希层</em> </strong>，第三个组件是<strong class="lh ja"> <em class="mm">成对交叉熵损失函数</em> </strong>，第四个组件是<strong class="lh ja"> <em class="mm">成对量化损失函数<em class="mm"/></em></strong></p><p id="b9de" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">深度监督离散哈希(DSDN) </strong></p><p id="417b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">李等人[18]提出了一种新的深度哈希方法<strong class="lh ja">深度监督离散哈希(DSDN) </strong>，其使用成对标签和分类信息在一个流框架中生成离散哈希码。优化过程保持了散列码的这种离散特性，以减少量化误差，因此导出了交替最小化方法来优化损失函数。</p><blockquote class="mo mp mq"><p id="66b9" class="lf lg mm lh b li lj ka lk ll lm kd ln mr lp lq lr ms lt lu lv mt lx ly lz ma ij bi translated">本文分享的见解是为科研工作<a class="ae le" href="http://dx.doi.org/10.25673/31719" rel="noopener ugc nofollow" target="_blank"> <em class="iq">嵌入式数据高维相似性搜索深度哈希评估</em> </a> <em class="iq">进行的系统性文献综述的一部分。</em></p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><blockquote class="mo mp mq"><p id="8dba" class="lf lg mm lh b li lj ka lk ll lm kd ln mr lp lq lr ms lt lu lv mt lx ly lz ma ij bi translated">这篇文章被《走向AI》团队收录在<a class="ae le" href="https://medium.com/towards-artificial-intelligence/optical-character-recognition-ocr-for-text-localization-detection-and-more-9018fa171324" rel="noopener"> <em class="iq">编辑选择的每月精选文章</em> </a>中。</p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="f263" class="nc nd iq bd ne nf ng dn nh ni nj dp nk lo nl nm nn ls no np nq lw nr ns nt iw bi translated">参考</h2><p id="0db0" class="pw-post-body-paragraph lf lg iq lh b li nu ka lk ll nv kd ln lo nw lq lr ls nx lu lv lw ny ly lz ma ij bi translated">[1] A. Andoni和P. Indyk，“高维近似最近邻的近似最优哈希算法”，《美国计算机学会通讯》，第51卷，第1期，第117页，2008年。</p><p id="4984" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2] B. Kulis和K. Grauman，“用于可伸缩图像搜索的内核化的位置敏感散列”。，“在ICCV，第9卷，第2130-2137页，2009年。</p><p id="ca8f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3] Y. Gong，S. Lazebnik，A. Gordo和F. Perronnin，“迭代量化:学习二进制码用于大规模图像检索的procrustean方法”，IEEE模式分析和机器智能汇刊，第35卷，第12期，第2916-2929页，2012年。</p><p id="29c2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4] W .孔和w .-J. Li，“各向同性散列”，载于《神经信息处理系统进展》，第1646-1654页，2012年。</p><p id="a2b3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[5]刘伟、王军、纪瑞根、江玉刚和张世锋，“带核的监督哈希算法”，2012年IEEE计算机视觉和模式识别会议，第2074–2081页，IEEE，2012年。</p><p id="018a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[6] M. Rastegari、J. Choi、S. Fakhraei、D. Hal和L. Davis，“可预测的双视图哈希”，载于机器学习国际会议，第1328-1336页，2013年。</p><p id="1cc4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[7] K. He、F. Wen和J. Sun，“K-means散列法:一种学习二进制紧凑码的相似性保持量化方法”，载于IEEE计算机视觉和模式识别会议论文集，第2938-2945页，2013年。</p><p id="af92" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[8] G. Lin、C. Shen、Q. Shi、A. Van den Hengel和D. Suter，“使用决策树对高维数据进行快速监督哈希运算”，载于IEEE计算机视觉和模式识别会议论文集，第1963–1970页，2014年。</p><p id="d039" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[9] F. Shen，C. Shen，W. Liu和h .，“有监督的离散哈希算法”，载于IEEE计算机视觉和模式识别会议论文集，第37–45页，2015年。</p><p id="a537" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[10]康伟忠、李伟杰、周志宏，“基于列采样的离散监督哈希算法”，第三十届人工智能会议，2016。</p><p id="07c1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[11] Y. Gong，S. Lazebnik，A. Gordo和F. Perronnin，“迭代量化:学习二进制码用于大规模图像检索的procrustean方法”，IEEE模式分析和机器智能汇刊，第35卷，第12期，第2916-2929页，2012年。</p><p id="e70f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[12] W. Kong和W.-J. Li，“各向同性散列”，载于《神经信息处理系统进展》，第1646-1654页，2012年。</p><p id="a851" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[13] J. Wang、W. Liu、S. Kumar和S.-F. Chang，“学习散列法用于索引大数据—一项调查”，《IEEE学报》，第104卷，第1期，第34–57页，2015年。</p><p id="7b6a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[14] W.-J. Li、S. Wang和W.-C. Kang，“基于特征学习的具有成对标签的深度监督散列”，arXiv预印本arXiv:1511.03855，2015年。</p><p id="8ac4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[15] X. Wang，Y. Shi，K. M. Kitani，“具有三元标签的深度监督哈希算法”，亚洲计算机视觉会议，第70–84页，Springer，2016年。</p><p id="08e2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[16] P. Zhang、W. Zhang、W.-J. Li和M. Guo，“潜在因素模型的监督散列”，载于第37届国际ACM信息检索研究与发展会议论文集，第173-182页，ACM，2014年。</p><p id="489b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[17] H. Zhu，M. Long，J. Wang，和Y. Cao，“面向高效相似性检索的深度哈希网络”，第三十届人工智能会议，2016</p><p id="20e9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[18] Q. Li、Z. Sun、R. He和T. Tan，“深度监督离散散列”，载于《神经信息处理系统进展》，第2482-2491页，2017年。</p></div></div>    
</body>
</html>