<html>
<head>
<title>Impact of Optimizers in Image Classifiers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化器对图像分类器的影响</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/impact-of-optimizers-in-image-classifiers-3b04ed20823a?source=collection_archive---------1-----------------------#2022-08-28">https://pub.towardsai.net/impact-of-optimizers-in-image-classifiers-3b04ed20823a?source=collection_archive---------1-----------------------#2022-08-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/bec236659991fe8fc65da8c6d924e9b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4yFUSNfuVNDN9Gnk"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">由<a class="ae kc" href="https://unsplash.com/@aleskrivec?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Ales Krivec </a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="4c71" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">介绍</h1><p id="1f8d" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">有没有想过为什么DNN在准确性方面没有达到预期的高性能，特别是当有官方或非官方的专家和爱好者报告在您使用的相同网络和相同数据集上获得了最高性能时？我记得有段时间我很难接受这样的想法:我的模型在预期表现良好的时候却失败了。这是什么原因造成的？事实上，有许多因素都有不同程度的潜在可能影响您的体系结构的性能。然而，在本文中我将只讨论其中的一个。这个因素就是“要使用的优化算法的选择”。</p><p id="9a25" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">什么是优化器？优化器是被创建并用于神经网络属性修改(即，权重、学习速率)的函数或算法，目的是加速收敛，同时最小化损失并最大化精度。DNN使用数十亿个参数，您需要正确的权重来确保您的DNN能够从给定数据中很好地学习，同时能够很好地归纳和适应未知相关数据的良好性能。</p><p id="d53a" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">多年来，已经建立了不同的优化算法，其中一些算法相对于其他算法有其优点，也有其缺点。因此，有必要了解这些算法的基础，以及理解正在处理的问题，以便我们能够选择最佳的优化器来工作。</p><p id="8a7f" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">此外，我注意到许多研究人员使用SGD-M(带动量的随机梯度下降)优化器，但在行业中，Adam更受青睐。在这篇文章中，我将对人工智能世界中最流行的优化器进行简要的描述。实际上，我必须做大量的实验来了解这些优化器之间的区别，回答一些关于使用这些优化器的问题，并根据我的观察给出关于哪个优化器是最好的以及何时/如何使用它们的线索。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="620d" class="kd ke iq bd kf kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la bi translated">不同优化器的基本描述</h1><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/bae54833d34f725548923c3c8bb1fb8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0EG3kQDZ56ofY5Mraw3kJg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated"><a class="ae kc" href="https://ruthwik.github.io/machinelearning/2018-01-15-gradient-descent/" rel="noopener ugc nofollow" target="_blank">https://Ruth wik . github . io/machine learning/2018-01-15-gradient-descent/</a></figcaption></figure><p id="9f11" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在本节中，我将简要讨论带动量的随机梯度下降(SGDM)、自适应梯度算法(ADAGRAD)、均方根传播(RMSProp)和Adam优化器。</p><p id="e33c" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir"> SGDM: </strong>由于梯度下降(GD)优化器使用整个训练数据来更新模型的权重，当我们有数百万个数据点时，它的计算变得非常昂贵。因此，随机梯度下降法(SGD)通过使用每个数据点来更新权重，从而解决了这个问题。然而，这对于神经网络(NN)来说在计算上是昂贵的，NN中使用的每个数据点都需要前向和后向传播。此外，对于SGD，当它试图达到全局最小值时，我们无法提高学习速率。这使得在使用SGD时收敛速度非常慢。SGDM解决了这个问题，因为它在普通SGD中增加了一个动量项，从而提高了收敛速度。更深入的解释，点击<a class="ae kc" href="https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f" rel="noopener" target="_blank">这里</a>。</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/fabdbd4454ff8850c8b448d349c3ab34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L5lNKxAHLPYNc6-Zs4Vscw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图片来自<a class="ae kc" href="https://ruder.io/optimizing-gradient-descent/index.html#momentum" rel="noopener ugc nofollow" target="_blank">塞巴斯蒂安·鲁德</a></figcaption></figure><p id="e438" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir"> ADAGRAD: </strong>自适应梯度算法(ADAGRAD)是一种基于梯度的优化算法，它试图使学习速率适应参数。学习率通过结合来自过去观察的见解来逐个组件地拟合参数。它对与经常出现的特性相关的参数进行小的更新，对那些不经常出现的特性进行大的更新。Adagrad还消除了手动调整学习率的需要，因为它会根据参数自动更新学习率。然而，学习速率收缩得很快，使得模型认为它接近实现收敛，并在预期性能之下停止。要了解更多信息，请点击<a class="ae kc" href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="6417" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir"> RMSProp: </strong>由Geoffrey Hinton提出(尽管尚未发表)，RMSProp是梯度下降的GD和AdaGrad版本的扩展，它在每个参数的步长调整中使用部分梯度的衰减平均值。发现梯度的大小对于不同的参数可以是不同的，并且可以在训练期间改变。因此，Adagrad对学习速率的自动选择可能是非优化的选择。Hinton通过使用平方梯度的移动平均值更新学习的权重来解决这个问题。要了解更多信息，请点击<a class="ae kc" href="https://machinelearningmastery.com/gradient-descent-with-rmsprop-from-scratch/#:~:text=Root%20Mean%20Squared%20Propagation%2C%20or,step%20size%20for%20each%20parameter." rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="df87" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir"> Adam: </strong>这个优化器是由Diederik Kingma和Jimmy Ba在2015年提出的，可以说是有史以来最受欢迎的优化器。它结合了SGDM和RMSProp的优点和好处，既利用了SGDM的动力，又利用了RMSProp的伸缩性。与GD和SGD不同，它的计算效率很高，并且只需要很少的内存。它被设计用于处理非常嘈杂/稀疏的梯度问题。要了解更多信息，请点击<a class="ae kc" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">此处</a>或<a class="ae kc" href="https://arxiv.org/pdf/1412.6980.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="3d6c" class="kd ke iq bd kf kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la bi translated">实验</h1><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/4c2d9cd3c09d2947475d9df23ffa7d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hNAHDLJO7qNRnVP1"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">凯文·Ku在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="1676" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">由于我的计算资源的规模，我决定在CIFAR-10数据集上重点使用LeNet和AlexNet。CIFAR-10数据集由50000幅训练图像和10000幅测试图像组成。我使用SGD、SGDM、Adagrad、RMSProp和Adam优化器对这些模型进行了50个时期的训练。对于SGDM，我使用了0.9的动量。我的第一组实验的全局学习率是0.001 (1e-3)。</p><p id="6b76" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">注意:我不寻求非常好的结果。相反，我试图了解每个优化器对模型性能的影响。</p><p id="3e70" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">我首先调用重要的库:</p><figure class="mr ms mt mu gt jr"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="27f3" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">然后，我加载并转换了CIFAR-10数据集:</p><figure class="mr ms mt mu gt jr"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="aaf7" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">LeNet和AlexNet模型:</p><figure class="mr ms mt mu gt jr"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="241d" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">要获得完整的代码，请查看这个</strong> <a class="ae kc" href="https://github.com/Ti-Oluwanimi/Impact-of-Optimizers" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">资源库</strong> </a> <strong class="ld ir">(如果你不介意，给它一颗星)。</strong></p><p id="9159" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">结果如下。</strong></p><div class="mr ms mt mu gt ab cb"><figure class="mz jr na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/df032b5b678fa6377cf6918a548bbc2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*hCMmSYx-HIBCQ9GiKizMlw.png"/></div></figure><figure class="mz jr na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/caee5dfeda5e06d2a0b6233ba6cd9bf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*oy0RUzV0D6E5fXtFeTDZ5g.png"/></div></figure></div><div class="ab cb"><figure class="mz jr na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/47308555f4ec1bff0940293494d4de54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*ARHKu0jMKdyCXJSYSe52vA.png"/></div></figure><figure class="mz jr na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/7d8dc4d6a2b560a93340e95796c37105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*fk-97bxKnzQqx_sHMYpUrQ.png"/></div></figure></div><p id="4b3c" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在LeNet模型上，SGDM的测试准确率最高，接近70%，而其训练损失为0.635。亚当的训练损失最小，但是他们的测试准确率只有67%。与阿达格拉德的LeNet是可悲的，有48%的测试准确率远低于54.03%的SGD。RMSProp给出了65%的测试准确度和0.630的训练损失。</p><p id="4dd8" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">至于AlexNet模型，SGDM的测试准确率仍然最高，为83.75%，其次是Adagrad，为82.79%。然而，SGD的训练损失是0.016，而Adagrad是0.005，这是如此之小，使得模型几乎没有改进的空间。鉴于它在人工智能领域的高度评价，Adam的结果出人意料地低。RMSProp似乎没有收敛信心，但与Adam具有相似的测试准确性。</p><p id="708b" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">从LeNet结果中，人们可以很容易地得出结论，Adagrad是一个糟糕的优化器，而从AlexNet结果中，RMSProp看起来像一个能够帮助模型适应训练数据的优化器，但这不仅仅是做出这个早期结论。必须进行更多的实验来研究这个问题。</p><h1 id="d943" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">进一步的实验</h1><p id="7fb1" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">由于RMSProp和Adam的结果，在使用AlexNet模型的同时，进行了另一个实验，这次使用1e-5的学习率。</p><div class="mr ms mt mu gt ab cb"><figure class="mz jr na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/7ffcf601adf3eea6dd49647e41498d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*zcyROchoWjn6G1mYMX7wsQ.png"/></div></figure><figure class="mz jr na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/a92cfb9cedfba74492b28eaee42416a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*avcS-PgRp8I38lGutUbzPw.png"/></div></figure></div><div class="ab cb"><figure class="mz jr na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/54975ce3c1eee49278fdea85e33ff2cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*8oTh7ryP5GO_U1eYkM0RtQ.png"/></div></figure><figure class="mz jr na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/9910d9709d2beb38c172707568a96c16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*5BTZsnrgdAd6NAiOHqCyFw.png"/></div></figure></div><p id="fbce" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">现在，这才像话。较低的学习率稳定了RMSProp优化器，提高了Adam的性能。我们可以很容易地得出结论，对于采用缩放的优化器，最好使用较低的学习速率。然而，我们需要确定这不是一般的，所以我尝试用较低的SGDM学习率，结果很差。因此，较低的学习速率更适合缩放优化器。</p><p id="2cea" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">尽管如此，我们还没有足够的实验来进行其他观察，所以在下一节中，我将讨论每个优化器上的当前短期实验的当前观察结果。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="f1cb" class="kd ke iq bd kf kg ml ki kj kk mm km kn ko mn kq kr ks mo ku kv kw mp ky kz la bi translated">讨论和结论</h1><p id="7e98" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir">新币:</strong> <strong class="ld ir">不推荐！</strong>虽然它肯定会收敛，但通常需要时间来学习。SGDM或Adam能在50个时代学到的东西，SGD将在大约500个时代学到。然而，当你以较大的学习率(即1e-1)开始时，你很有可能获得一些不错的结果。如果有足够的时间等待收敛，也可以使用；否则，远离。</p><p id="9078" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir"> SGDM:推荐！这个优化器在实验中给出了最好的结果。然而，如果开始的学习率很低，它可能不会很好地工作。否则，它收敛快，也有助于模型的推广。完全推荐！</strong></p><p id="ccb0" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">阿达格拉德:推荐！</strong>从实验来看，可以说这个优化器是最不适合使用的，尤其是在复杂数据集上使用LeNet这样的小模型时。然而，在更深的网络中，它可以提供良好的结果，但不能保证最佳性能。</p><p id="8615" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir"> RMSProp:推荐！</strong>这个优化器也有非常好的性能。当以较低的学习速率使用时，它可以提供更好的性能。除了性能之外，它的收敛速度很快，我们可以看到它有时被用于生产部门(工业)的原因。</p><p id="5de7" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">亚当:推荐！</strong>根据一些专家的说法，亚当学习所有模式，包括训练集中的噪声，因此收敛很快。然而，在上面的实验中，我们可以看到它没有SGDM收敛得好，但它收敛和学习速度很快。此外，我敢打赌，它在更大的数据集上的性能(当然，这将包含更多的噪声)将优于上面讨论的其他优化器。</p><p id="2459" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">通过对目前使用的流行优化器的实际观察，我希望您对为什么需要优化器以及这些优化器如何影响模型性能有了一些了解和直觉。如果您有建议和反馈，请在<a class="ae kc" href="https://www.linkedin.com/in/toluwaniaremu/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上留言或联系我。谢谢你。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="3178" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">要了解这些优化器，以及本文没有提到的其他优化器，请使用这个<a class="ae kc" href="https://d2l.ai/chapter_optimization/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="ebbd" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">访问这里使用的代码，<a class="ae kc" href="https://github.com/Ti-Oluwanimi/Impact-of-Optimizers" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">储存库</strong> </a>。</p></div></div>    
</body>
</html>