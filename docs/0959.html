<html>
<head>
<title>The Disadvantage of MSE Loss and How to Remove Them</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MSE损耗的危害及如何消除</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-disadvantage-of-mse-loss-and-how-to-remove-them-a18c5fa224ee?source=collection_archive---------1-----------------------#2020-09-22">https://pub.towardsai.net/the-disadvantage-of-mse-loss-and-how-to-remove-them-a18c5fa224ee?source=collection_archive---------1-----------------------#2020-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ac73" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="f2f2" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何改进使用均方误差损失获得的结果</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/23a2f7a00645ef3b49f15c1d679d016a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TeZ8vqnPJSjjt6w3"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@anniespratt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安妮·斯普拉特</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="4bee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">均方差是机器学习和数据科学中最常用和最直接的基于回归的损失函数之一。它被用在一系列任务中，例如从表格数据到计算机视觉、NLP、强化学习等特定用例的线性回归。除了MSE之外，MAE的应用也很广泛，与MSE损耗高度相似。</p><p id="ce52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管在机器学习中被高度使用，但它也有缺陷，这是我想在本文中强调的。有具体的方法可以最小化它的缺点以获得更好的结果，这将在最后讨论。为了简单和更好的理解，讨论和用例保持与计算机视觉相关。</p><h1 id="1107" class="mn mo it bd mp mq mr ms mt mu mv mw mx ki my kj mz kl na km nb ko nc kp nd ne bi translated">缺点:</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nf"><img src="../Images/cd16bf09d6dae9c35aee95315e5e807f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zdvw0wN5sXOz7HXD"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@antoine1003?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Antoine Dautry </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="a73d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">MSE、MAE等损失函数通常受到数据集中存在的不确定性的影响。在训练模型时，可能会出现这样的情况:对于相同的输入，可能会有各种同样可能的结果。在所有这些情况下，MSE损失函数旨在通过模糊预测或简单地说，取可能输出的平均值来适应预测中的不确定性。这是因为所有可能结果的平均值将导致训练期间参数空间的全局最小值。类似地，MAE loss预测了所有这些结果的中位数。基于L-范数的损失函数的这种结果模糊导致诸如低质量结果、模糊等伪像。</p><blockquote class="ng"><p id="2791" class="nh ni it bd nj nk nl nm nn no np md dk translated">这些伪像在图像到图像和视频到视频的任务中很常见，例如超分辨率、视频预测、结构化预测、相机姿态回归等。</p></blockquote><h1 id="b71a" class="mn mo it bd mp mq mr ms mt mu mv mw mx ki nq kj mz kl nr km nb ko ns kp nd ne bi translated">统计解释:</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/4f163ae8b25ddfcf8d16eb1b2df5939f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ksu4Toe6qdx7GIPOofkOCw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">(来源:作者)</figcaption></figure><p id="a206" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">损失函数假设数据集中的数据样本遵循高斯分布，这对于任何真实的数据集来说几乎都不是这样。这种假设也导致了数据集分布本质上应该是单峰的限制，这意味着应该只有最可能的结果。这种假设在大多数情况下严重失败，因为现实生活中的数据集。</p><p id="c4d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果数据集分布不是单峰的，则MSE损失用于训练网络。然后，它将尝试将多峰分布拟合为高斯分布，这通常意味着在MSE损失的情况下对峰值进行平均，或者在MAE损失的情况下对中值进行合并。</p><p id="35b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用MSE损失，作为来自网络的所有可能结果的平均值的预测将试图最小化误差，因为获得的预测将是全局最优的，因此避免了更精细的细节，例如面部特征和细微的帧间运动，因为它们被网络认为是噪声。</p><h1 id="9754" class="mn mo it bd mp mq mr ms mt mu mv mw mx ki my kj mz kl na km nb ko nc kp nd ne bi translated">如何提高成绩？</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/264f990a4af2bc30bbb461177d1a09eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eS8y_2OudqmSO7di"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@hjwinunsplsh?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jungwoo Hong </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="38d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了改善结果，机器学习研究人员尝试了许多不同的方法来解决平均可能预测的问题。还应注意，网络的大部分是确定性的，即，因为它们对相同的输入返回相同的结果。大多数神经网络被设计成产生最可能的结果，而不是返回许多可能的结果。</p><p id="5e1f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">给出了许多不同的方法来解决MSE损失函数中的上述问题，两种最常用的方法如下</p><ul class=""><li id="7233" class="nv nw it lk b ll lm lo lp lr nx lv ny lz nz md oa ob oc od bi translated">对抗训练</li><li id="c172" class="nv nw it lk b ll oe lo of lr og lv oh lz oi md oa ob oc od bi translated">知觉丧失</li></ul><h2 id="80c0" class="oj mo it bd mp ok ol dn mt om on dp mx lr oo op mz lv oq or nb lz os ot nd iz bi translated">对抗训练:</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/78fda0a2f83595ff2de17abd468bb22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J_Ht72Um3Q6zkS5aGsDKlQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">(来源:【https://pixabay.com T4】)</figcaption></figure><p id="a26c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自2014年GANs问世以来，通过使用极大极小游戏对数据分布进行建模，研究取得了突破性进展，并改进了结果以生成输出。在对抗性训练中，使用鉴别器通过与来自数据集的样本竞争网络预测来训练模型。使用鉴别器帮助，网络产生微妙的纹理并消除模糊效果。</p><p id="6cc7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，应该注意的是，通过使用对抗性训练引入的纹理是肤浅的，并且不符合地面真相。</p><h2 id="5e83" class="oj mo it bd mp ok ol dn mt om on dp mx lr oo op mz lv oq or nb lz os ot nd iz bi translated">感知损失:</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/b4895b069a639939f030fc2e57971a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7nD1HomD1VLyi-KXmc3cA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://www.researchgate.net/publication/334279687_Automatic_Extraction_and_Synthesis_of_Regular_Repeatable_Patterns" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/publication/334279687 _ Automatic _ Extraction _ and _ Synthesis _ of _ Regular _ Repeatable _ Patterns</a></figcaption></figure><p id="e75b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在感知损失中，地面真实和预测通过预先训练的神经网络，并且中间层的MSE被计算为损失。此过程中最常用的网络是在ImageNet数据集上训练的VGG19。VGG19的前五层用于此过程。</p><p id="175d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用感知损失的理由是，它提取输出图像/视频的特征和表示，并试图最小化它们之间的差异。因此，有人认为，通过使用感知损失函数，网络正试图了解数据集分布的微妙特征。</p><p id="e2a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该损失与MSE损失一起使用，并且在训练期间通常具有&lt; 1%的系数，因为许多图像可能具有相似的纹理。我们试图学习的是图像之间的相似性，而不是纹理。所以，大多数时候，它通常被用作一个正则化。</p><h1 id="fd62" class="mn mo it bd mp mq mr ms mt mu mv mw mx ki my kj mz kl na km nb ko nc kp nd ne bi translated">结论:</h1><p id="aeb0" class="pw-post-body-paragraph li lj it lk b ll ow kd ln lo ox kg lq lr oy lt lu lv oz lx ly lz pa mb mc md im bi translated">使用所有这些方法来减少平均的影响已经被证明是有帮助的，并且给出一些更好的结果。但是，大多数方法仍然依赖于基于像素距离的损失函数。因此，回归均值问题仍然是一个活跃的研究领域。</p></div></div>    
</body>
</html>