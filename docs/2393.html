<html>
<head>
<title>An In-depth Guide to Local Outlier Factor (LOF) for Outlier Detection in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中离群点检测的局部离群点因子(LOF)深度指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/an-in-depth-guide-to-local-outlier-factor-lof-for-outlier-detection-in-python-5a6f128e5871?source=collection_archive---------0-----------------------#2021-12-04">https://pub.towardsai.net/an-in-depth-guide-to-local-outlier-factor-lof-for-outlier-detection-in-python-5a6f128e5871?source=collection_archive---------0-----------------------#2021-12-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4c4b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="16bb" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">理论直觉、数学定义和实际代码示例</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/97b47f875eba607ba47bdae9c0c05319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sJ_mwIMK9NaPBCDH.jpg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来自托马斯·卡斯特拉佐的<a class="ae lh" href="https://commons.wikimedia.org/wiki/File:Hot_air_balloons_in_leon.jpg" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></figcaption></figure><p id="722c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> O </span> utlier detection是一项机器学习任务，旨在识别偏离给定数据的“常态”或一般分布的罕见项目、事件或观察值。</p><p id="6486" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章是对异常值检测的局部异常值因子(LOF)算法的理论和应用的深入指导。我们将介绍LOF分数，考虑直觉，深入研究数学定义，并学习如何在python中使用LOF。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="06b1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">关于离群点检测的介绍、常见挑战和用于离群点检测的<code class="fe mu mv mw mx b">pyod</code> python库，请查看最后的<strong class="lk jd">延伸阅读</strong>部分。</p><h1 id="7581" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">引入局部异常因子</h1><p id="39d7" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated"><strong class="lk jd">局部异常值因子(LOF)测量每个观测值相对于其邻居的“局部密度偏差”。</strong>LOF使用k个最近邻点的平均局部密度与数据点的局部密度之比来测量数据点的密度与其k个最近邻点的密度有何不同。一个数据点的<em class="nv">局部</em> <em class="nv">密度</em>简单来说就是它的k个最近邻居平均有多近(或多远)。<em class="nv">假设非均匀分布，每个数据点的局部密度对于每个点是不同的。</em></p><p id="ec9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">相对于它们最近的邻居，更“孤立”的观察值得分更高。相反，位于相对密集的空间区域的观测值，例如在一个星团的中间，会得到较低的LOF分数。</p><p id="2f79" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">LOF的一个优点是它同时考虑了数据集的局部和全局属性。离群值不是以绝对的术语来识别的，而是相对于它们的局部邻域来识别的。当数据集中存在不同密度的不同聚类时，该算法表现良好。</p><p id="e668" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">LOF适用于适度高维的数据集。</p><h2 id="0781" class="nw mz it bd na nx ny dn ne nz oa dp ni lr ob oc nk lv od oe nm lz of og no iz bi translated">图形示例</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/90075c40423b042c9a57422c98968e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*0tQWBNnz7DyPs1ELsKH_1Q.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">相对于两个不同密度聚类的局部异常值的图示。来源:原始论文[1]</figcaption></figure><p id="f82c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上图中，C1聚类包含400个点，C2聚类包含100个点。C2星团的密度比C1大得多。点o1和o2是“局部”异常值。点o1与所有其他点完全隔离，是一个“明显的”异常值。点o2也是一个<em class="nv">局部</em>异常值，因为尽管o2和它最近的邻居之间的距离类似于C1集群中的点之间的距离，但是这个距离比附近的C2集群中的点之间的距离大得多。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/78b45d57353b9ffdab2d692e6e2ddb23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*Nj3Z7TSyJLDUFTxOeS3J_g.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/neighbors/plot_lof_outlier_detection.html" rel="noopener ugc nofollow" target="_blank">使用本地异常值因子的异常值检测(LOF) — scikit-learn 1.0.1文档</a></figcaption></figure><p id="4c56" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的例子中，数据点被标为黑点。每个点周围红色圆圈的大小表示异常值，圆圈越大表示异常值越大。该图表明，距离较近的数据点具有较小的异常值。距离其邻居较远的点具有较大的异常值分数。</p><h1 id="0d48" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">工作原理:数学</h1><p id="6037" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">为了从数学上定义LOF，我将首先提供几个定义。为了使方程更直观，数学符号是非正式的。</p><p id="d4f6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数据点<em class="nv"> p </em>和<em class="nv"> o </em>之间的<strong class="lk jd">可达性距离</strong>要么是<em class="nv"> p </em>和<em class="nv"> o </em>之间的距离，要么是<em class="nv"> o </em>的k距离，以较大者为准<em class="nv">。</em><strong class="lk jd"><em class="nv">k</em>-距离(o) </strong>是距离<em class="nv"> o </em>的距离，使得<em class="nv"> k </em>邻居正好是距离<em class="nv"> o </em>的这个距离或者更近。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/47b3d44e201d158d82f41eed8afb8803.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*WEIff1wXv-wIZ3qRzeV4Zg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">数据点<em class="ok"> p </em>和o的可达距离</figcaption></figure><p id="0360" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一个观察值<em class="nv"> p </em>的<strong class="lk jd">局部可达性<em class="nv">密度</em>密度</strong>是<em class="nv">k</em>-p的最近邻居的平均可达性距离的倒数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/23a167d772f549883f1383ae0a972898.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*p9_DlHNDfjuXhlk47cpECw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">观察p的局部可达性密度。</figcaption></figure><p id="fd14" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">局部离群因子</strong>是观测值<em class="nv"> p </em>的局部可达性密度与<em class="nv"> p </em>的<em class="nv"> k- </em>最近邻的局部可达性密度之比的平均值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/4c634a3f9abf01641db632b5062e5c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*8KQK0k62lG7nNj9qnOWnPg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">计算观测值p的局部异常因子的方程。</figcaption></figure><p id="31ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从LOF方程中，我们可以推出一些东西。</p><ol class=""><li id="86a9" class="on oo it lk b ll lm lo lp lr op lv oq lz or md os ot ou ov bi translated"><strong class="lk jd">当<em class="nv"> p </em>相对于其邻居更加孤立时，</strong>即局部可达性密度远低于<em class="nv"> p </em>的<em class="nv">k</em>-最近邻居的局部可达性密度，<strong class="lk jd">LOF值更高</strong>。</li><li id="3db1" class="on oo it lk b ll ow lo ox lr oy lv oz lz pa md os ot ou ov bi translated"><strong class="lk jd">当<em class="nv"> p </em>在一个簇</strong>内，即<em class="nv"> p </em>与其邻居<strong class="lk jd">的局部可达性密度相似时，则LOF值接近1 </strong>。</li></ol><h1 id="822b" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">蟒蛇皮LOF</h1><p id="e267" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">LOF在<code class="fe mu mv mw mx b">scikit-learn</code>和<code class="fe mu mv mw mx b">pyod</code> python包中都有实现；不过，<strong class="lk jd">我推荐用</strong> <code class="fe mu mv mw mx b"><strong class="lk jd">pyod</strong></code>。</p><p id="c254" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe mu mv mw mx b">scikit-learn</code>的实现侧重于新颖性检测，对于异常值检测来说，这种用法是次优的。</p><p id="f302" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe mu mv mw mx b">pyod</code>实现包装了<code class="fe mu mv mw mx b">scikit-learn</code>模型并做了改进。PyOD修正了异常分数，使得离群值被赋予较大的异常分数，而内联值具有较小的异常分数。它还将新奇和异常检测选项集成到一个更易于使用的统一界面中。</p><h2 id="1ac2" class="nw mz it bd na nx ny dn ne nz oa dp ni lr ob oc nk lv od oe nm lz of og no iz bi translated">超参数</h2><p id="eae5" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">这两个包都实现了用于调整LOF中最近邻计算的参数-使用多少个近邻、使用哪种近邻算法以及如何计算数据点之间的距离。此外，还有一个<code class="fe mu mv mw mx b">contamination</code>参数允许模型选择阈值，将离群值分数与内联值分数分开。</p><p id="2c4d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最重要的参数是邻居的数量(<code class="fe mu mv mw mx b">n_neighbors</code>)。<strong class="lk jd">在实践中，默认的n_neighbors=20往往效果很好。</strong>当异常值的比例预计较高时(例如大于10%)，从20增加n_neighbors。</p><p id="9a8f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有关更多详细信息，请参考文档(<a class="ae lh" href="https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof" rel="noopener ugc nofollow" target="_blank"> PyOD </a>、<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>)。</p><h2 id="ea44" class="nw mz it bd na nx ny dn ne nz oa dp ni lr ob oc nk lv od oe nm lz of og no iz bi translated">示例数据</h2><p id="00d7" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">对于<code class="fe mu mv mw mx b">pyod</code>和<code class="fe mu mv mw mx b">scikit-learn</code>示例，我将使用以下合成数据集。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="b8e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面我使用PyOD的<code class="fe mu mv mw mx b">generate_data</code>函数来生成一个包含200个训练样本和100个测试样本的合成数据集。正态样本由多元高斯分布生成；异常样本是使用均匀分布生成的。</p><p id="2cc1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">训练和测试数据集都有5个特征，10%的行被标记为异常。我在数据中加入了一点随机噪声，使得完美区分正常点和异常点变得稍微困难一些。</p><p id="fd91" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面，我绘制了训练和测试数据集，使用<a class="ae lh" href="https://scikit-learn.org/stable/modules/decomposition.html#pca" rel="noopener ugc nofollow" target="_blank"> PCA </a>投影到二维空间。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/390063840f4eb166a4f9aae728e3cea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kexDN2WQa5UFK-r-ty4gw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">合成的训练和测试数据集，通过PCA降低到2维。</figcaption></figure><h2 id="6f0a" class="nw mz it bd na nx ny dn ne nz oa dp ni lr ob oc nk lv od oe nm lz of og no iz bi translated">PyOD示例</h2><p id="fa13" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">在下面的代码示例中，我在综合训练数据上拟合了LOF模型，并将其应用于综合测试数据。<em class="nv">因为LOF是一个无监督的模型，所以它不需要标签来拟合。</em></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="70ef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">运行<code class="fe mu mv mw mx b">fit</code>和<code class="fe mu mv mw mx b">decision_function</code>方法的时间不到1秒。</p><p id="cd53" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还可以看看LOF模型中决策得分的分布。在下图中，我们看到正常数据(蓝色)的分数集中在1.0左右。这是意料之中的，因为LOF是数据点的局部密度与其相邻点的平均局部密度之比。异常数据点(橙色)的分数都大于1.0，并且通常高于正常数据。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/e340b5e9513a165cfb30d4fb33a2d61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*lX6vVh36Q3uz1rlwMud1_Q.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">使用PyOD库生成的内层和外层数据的LOF外层分数分布。</figcaption></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pb pc l"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">生成异常值分数分布图的代码</figcaption></figure><p id="a0af" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在PyOD中，拟合的异常值检测器有两个关键功能:<code class="fe mu mv mw mx b">decision_function</code>和<code class="fe mu mv mw mx b">predict</code>。</p><ul class=""><li id="56e4" class="on oo it lk b ll lm lo lp lr op lv oq lz or md pf ot ou ov bi translated"><code class="fe mu mv mw mx b">decision_function</code>返回每行的异常值。</li><li id="bbb0" class="on oo it lk b ll ow lo ox lr oy lv oz lz pa md pf ot ou ov bi translated"><code class="fe mu mv mw mx b">predict</code>返回一个由0和1组成的数组，指示每一行被预测为正常(0)还是异常值(1)。<code class="fe mu mv mw mx b">predict</code>函数只是对<code class="fe mu mv mw mx b">decision_function</code>返回的异常分数应用一个阈值。初始化探测器时，阈值会根据指定的<code class="fe mu mv mw mx b">contamination</code>速率参数设置自动校准。</li></ul><p id="4ab4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">LOF模型<em class="nv">必须</em>适合某些训练数据，以便用<code class="fe mu mv mw mx b">decision_function</code>生成分数。</p><h2 id="279d" class="nw mz it bd na nx ny dn ne nz oa dp ni lr ob oc nk lv od oe nm lz of og no iz bi translated">sci kit-学习示例</h2><p id="9705" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">在<code class="fe mu mv mw mx b">scikit-learn</code>中应用LOF进行异常检测时，有两种选择:异常检测模式(<code class="fe mu mv mw mx b">novelty=False</code>或新奇检测模式(<code class="fe mu mv mw mx b">novelty=True</code>)。</p><p id="4e94" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在异常值检测模式下，只有用于生成异常值预测的<code class="fe mu mv mw mx b">fit_predict</code>方法可用。可以使用<code class="fe mu mv mw mx b">negative_outlier_factor_</code>属性检索训练数据的异常值分数，但是不能为看不见的数据生成分数。在引擎盖下，模型根据<code class="fe mu mv mw mx b">contamination</code>参数(默认值为0.1)自动选择异常值分数的阈值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pb pc l"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">使用sklearn的LOF</figcaption></figure><p id="c925" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在新颖性检测模式下，只有用于生成异常值分数的<code class="fe mu mv mw mx b">decision_function</code>可用。<code class="fe mu mv mw mx b">fit_predict</code>方法不可用，但<code class="fe mu mv mw mx b">predict</code>方法可用于生成异常值预测。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="c195" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如下图所示，sklearn LOF模型中的异常值得分是反向的-异常值得分低于内部值。这是违反直觉的，因为异常值检测器通常应该给更不寻常的数据点更高的异常值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/4fa57a0d8ee411ab4b49f3004c95b6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*Y9L75WyYqH7Eg5L_J6j0Fw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">使用sklearn库生成的内层和外层数据的LOF外层分数分布。</figcaption></figure><h1 id="4055" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">进一步阅读</h1><p id="f6d5" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">如果您有兴趣了解更多关于离群点检测的知识，如果您喜欢这篇文章，您可能也会喜欢这些相关的帖子。</p><div class="ph pi gp gr pj pk"><a rel="noopener  ugc nofollow" target="_blank" href="/why-outlier-detection-is-hard-94386578be6c"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd jd gy z fp pp fr fs pq fu fw jc bi translated">为什么异常值检测很难</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">异常检测机器学习任务的考虑因素</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">pub.towardsai.net</p></div></div><div class="pt l"><div class="pu l pv pw px pt py lb pk"/></div></div></a></div><div class="ph pi gp gr pj pk"><a href="https://towardsdatascience.com/pyod-a-unified-python-library-for-anomaly-detection-3608ec1fe321" rel="noopener follow" target="_blank"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd jd gy z fp pp fr fs pq fu fw jc bi translated">PyOD:用于异常检测的统一Python库</h2><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="pz l pv pw px pt py lb pk"/></div></div></a></div><div class="ph pi gp gr pj pk"><a href="https://towardsdatascience.com/fast-accurate-anomaly-detection-based-on-copulas-copod-3133ce9041fa" rel="noopener follow" target="_blank"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd jd gy z fp pp fr fs pq fu fw jc bi translated">基于COPOD的快速准确异常检测</h2><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="qa l pv pw px pt py lb pk"/></div></div></a></div><div class="ph pi gp gr pj pk"><a href="https://medium.com/geekculture/replace-outlier-detection-by-simple-statistics-with-ecod-f95a7d982f79" rel="noopener follow" target="_blank"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd jd gy z fp pp fr fs pq fu fw jc bi translated">用ECOD简单统计取代异常值检测</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">一种新的基于python的、简单的、无参数的、可解释的无监督异常检测方法</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">medium.com</p></div></div><div class="pt l"><div class="qb l pv pw px pt py lb pk"/></div></div></a></div><h1 id="906c" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">不是中等会员？今天就加入！</h1><div class="ph pi gp gr pj pk"><a href="http://alexandra-amidon.medium.com/membership" rel="noopener follow" target="_blank"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd jd gy z fp pp fr fs pq fu fw jc bi translated">通过我的推荐链接加入媒体-亚历山德拉·阿米登</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">alexandra-amidon.medium.com</p></div></div><div class="pt l"><div class="qc l pv pw px pt py lb pk"/></div></div></a></div><h1 id="2b18" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">参考</h1><p id="8660" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">[1] Breunig、Kriegel、Ng和Sander (2000) <a class="ae lh" href="http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf" rel="noopener ugc nofollow" target="_blank"> LOF:识别基于密度的局部异常值。</a> Proc。ACM SIGMOD</p><p id="ee94" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2] <a class="ae lh" href="https://scikit-learn.org/stable/modules/outlier_detection.html" rel="noopener ugc nofollow" target="_blank"> 2.7。新奇和异常检测— scikit-learn 1.0.1文档</a></p><p id="8543" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3] <a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/neighbors/plot_lof_outlier_detection.html" rel="noopener ugc nofollow" target="_blank">使用局部异常值因子的异常值检测(LOF) — scikit-learn 1.0.1文档</a></p><p id="9a57" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4]<a class="ae lh" href="https://pyod.readthedocs.io/en/latest/_modules/pyod/models/lof.html" rel="noopener ugc nofollow" target="_blank">pyod . models . lof—pyod 0 . 9 . 5文档</a></p></div></div>    
</body>
</html>