<html>
<head>
<title>Nothing but NumPy: Understanding &amp; Creating Neural Networks with Computational Graphs from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">只有数字:从零开始理解和创建带有计算图的神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0?source=collection_archive---------0-----------------------#2019-06-22">https://pub.towardsai.net/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0?source=collection_archive---------0-----------------------#2019-06-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8a8a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming/python" rel="noopener ugc nofollow" target="_blank"> Python </a></h2><div class=""/><p id="4b93" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">更新:这篇文章得到了积极的反馈，尤其是我所敬仰的人工智能社区的人们，这让我受宠若惊。我也感谢所有分享这篇文章并花时间阅读它的人，因为你这篇文章被授予了KDnugget的金质徽章。谢谢大家！</p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="la lb l"/></div></figure><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="la lb l"/></div></figure><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="la lb l"/></div></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="3ba0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">理解新概念可能会很难，尤其是现在有大量的资源，而对复杂的概念只有粗略的解释。这篇博客是缺乏如何以计算图的形式创建神经网络的详细演练的结果。</p><p id="24cd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这篇以及后面的一些博文中，我将把我所学到的东西整合起来，作为一种回馈社区和帮助新加入者的方式。我将在NumPy的帮助下创建普通形式的神经网络。</p><p id="b33e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">这篇博文分为两部分，第一部分将理解神经网络的基础知识，第二部分将包含实现第一部分所学内容的代码。</em></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="a71e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">第一部分:了解神经网络</h1><h1 id="995e" class="lk ll iq bd lm ln mi lp lq lr mj lt lu lv mk lx ly lz ml mb mc md mm mf mg mh bi translated">让我们开始吃吧🍽️</h1><p id="5cfa" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">神经网络是一种受大脑工作方式启发的模型。类似于大脑中的神经元，我们的“数学神经元”也是直观地相互连接的；它们接受输入(树突)，对其进行一些简单的计算，并产生输出(轴突)。</p><p id="ba26" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">学习某样东西的最好方法是去构建它。先从一个简单的神经网络入手，手动求解。这将让我们了解计算是如何通过神经网络的。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/da5bee41646dea2f414752902cecdb65.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*2WiemDQ8NE8Nyjxol84vmw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图一。简单仅输入输出神经网络</figcaption></figure><p id="6e2c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如上图所示，大多数时候你会看到一个神经网络以类似的方式描绘。但是这张简洁简单的图片隐藏了一点复杂性。让我们把它展开。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/78201059a512d3247ac2439cdd1868c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*Fy7uD4pEvdKt5bAnJXEQYg.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图二。扩展神经网络</figcaption></figure><p id="4c20" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在，让我们仔细检查图表中的每个节点，看看它代表什么。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi na"><img src="../Images/53659781f6e31fbfe7013c212ec12c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*5WDvzA6x2Daz4E3BAYDmNQ.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图三。输入节点<strong class="bd lm"> <em class="nb"> x₁ </em> </strong>和<strong class="bd lm"> <em class="nb"> x₂ </em> </strong></figcaption></figure><p id="46a0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这些节点代表我们对第一个和第二个特征的输入，<strong class="jy ja"><em class="ku"/></strong><strong class="jy ja"><em class="ku">【x₂】</em></strong>定义了我们馈送给神经网络<em class="ku"/>的单个示例，因此称为<strong class="jy ja"> <em class="ku">【输入层】</em> </strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi na"><img src="../Images/52b87cb9bf5afafa8d3859a4b4a23072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*t6yQBDdIdkCzERnwHTMQMQ.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图4。砝码</figcaption></figure><p id="06a4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"><em class="ku"/></strong><strong class="jy ja"><em class="ku">w₂</em></strong>代表我们的权重向量(在一些神经网络文献中权重用<em class="ku">θ</em>符号、<strong class="jy ja"> <em class="ku"> θ </em> </strong>表示)。直观地说，这些决定了每个输入要素在计算下一个结点时的影响程度。如果你是新手，可以把它们想象成线性方程中的“斜率”或“梯度”常数。</p><p id="58c0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">权重是我们的神经网络必须“学习”的主要值</em>。因此，最初，我们将把它们设置为随机值<strong class="jy ja"><em class="ku"/></strong>，并让我们的神经网络的“<em class="ku">学习算法”</em>决定产生正确输出的最佳权重。</p><p id="52ae" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为什么随机初始化？稍后将详细介绍。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1d87ba22b59e730a3c4737b2a6cfdfaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*uRdQCmJdDO5rWmlBjxL9lw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图五。线性操作</figcaption></figure><p id="55a4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个节点代表一个线性函数。简单地说，<em class="ku">它接受所有输入，并从中创建一个线性方程/组合</em>。(<em class="ku">按照惯例，应当理解，除了输入层中的输入节点之外，权重和输入的线性组合是每个节点的一部分，因此该节点在图中经常被省略，如图1所示。</em>在这个示例中，我将把它留在这里)</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi na"><img src="../Images/9f2e2b353cec6621a0f621e07f7784e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*zxZv-MUiRR-8grEGYyuSeg.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图六。输出节点</figcaption></figure><p id="2e72" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个<strong class="jy ja"> <em class="ku"> σ </em> </strong>节点(sigmoid node)接受输入，并将其传递给以下函数，称为<strong class="jy ja"> sigmoid函数</strong>(因为它是S形曲线)，也称为<strong class="jy ja">逻辑函数</strong>:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/70d612a3e805bc5c3446226a071277a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*9Opw1gREJLI9a0-VrJa2Yw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图7。Sigmoid(逻辑)函数</figcaption></figure><p id="3cbd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Sigmoid是神经网络中使用的许多“激活函数”之一。激活功能的作用是将输入改变到不同的范围。例如，如果<em class="ku"> z &gt; 2 </em>那么，<em class="ku"> σ(z) ≈ 1 </em>同样，如果<em class="ku"> z &lt; -2那么，σ(z) ≈ 0。S </em> o，sigmoid函数将输出范围挤压到(0，1) <em class="ku">(此'()'符号暗示排他边界；从不完全输出0或1作为函数渐近线，而是达到非常接近边界值)</em></p><p id="1ce1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">在我们上面的神经网络中由于它是最后一个节点，它执行输出</strong>的功能。预测输出用<strong class="jy ja"> <em class="ku"> ŷ.表示</em> </strong> ( <em class="ku">注:在一些神经网络文献中，这被表示为'</em><strong class="jy ja"><em class="ku">【h(θ)'</em></strong><em class="ku">)，其中“h”被称为假设，即这是神经网络的假设，也称为输出预测，给定参数θ；其中θ是神经网络的权重)</em></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="74ae" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们知道了每个事物代表什么，让我们用一些玩具数据来计算每个节点。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/cea578a74b5af417b57204c50b83634d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*Fkhcwm_SIIK7kdiHh7I50A.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图8。或门</figcaption></figure><p id="bb35" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">以上数据代表一个<strong class="jy ja">或</strong>门(如果任何输入为1，则输出为1)。表格中的每一行都代表了我们希望我们的神经网络学习的一个<em class="ku">‘例子’</em>。在从给定的例子中学习之后，我们希望我们的神经网络执行或门的功能；给定输入特征，<strong class="jy ja"><em class="ku"/></strong><strong class="jy ja"><em class="ku"/></strong><em class="ku">，</em>试着输出相应的<strong class="jy ja"><em class="ku">【y】(也叫‘标签’)</em></strong><em class="ku">。</em>我还在二维平面上绘制了这些点，以便于可视化(绿色十字表示输出(<strong class="jy ja"><em class="ku">【y】</em></strong>为<strong class="jy ja"> <em class="ku"> 1 </em> </strong>的点，红色圆点表示输出为<strong class="jy ja"> <em class="ku"> 0 </em> </strong>的点)。</p><p id="bdfd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个或门数据特别有趣，因为它是可线性分离的<strong class="jy ja"><em class="ku"/></strong>，也就是说，我们可以画一条直线来分离绿色十字和红色圆点。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/2706fad275bafb76b5ea0cebc51bead9.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*VCLUzFzyX6T5kujMe75jfA.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图九。表明或门数据是线性可分的</figcaption></figure><p id="9d19" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们将很快看到我们简单的神经网络如何执行这项任务。</p><p id="378b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在我们的神经网络中，数据从左向右流动。用专业术语来说，这个过程叫做<strong class="jy ja">‘正向传播’</strong>；来自每个节点的计算被转发到它所连接的下一个节点。</p><p id="23b2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们检查一下给定第一个例子时我们的神经网络将执行的所有计算，<strong class="jy ja"><em class="ku"/></strong><strong class="jy ja"><em class="ku">x₂=0</em></strong>。同样，我们将初始化权重<strong class="jy ja"> </strong>到<strong class="jy ja"> <em class="ku"> w₁=0.1 </em> </strong>和<strong class="jy ja"> <em class="ku"> w₂=0.6 </em> </strong> <em class="ku">(回想一下，这些权重是随机选择的)</em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nf"><img src="../Images/6a32b1954e82c248132bbd49a77523a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fW1kWWa1HGosROKkpjzFQw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图10。从或表数据向前传播第一个示例</figcaption></figure><p id="8417" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">以我们目前的权数，<strong class="jy ja"><em class="ku">【w₁=】0.1，</em></strong><strong class="jy ja"><em class="ku">w₂= 0.6</em></strong>，<strong class="jy ja"> <em class="ku"> </em> </strong>我们的<strong class="jy ja"> <em class="ku"> </em> </strong>网络的输出离我们想要的地方有点远。预测产量，<strong class="jy ja"> <em class="ku"> ŷ，</em> </strong>应该是<strong class="jy ja"> <em class="ku"> ŷ≈0 </em> </strong>为<strong class="jy ja"> <em class="ku"> x₁=0 </em> </strong>和<strong class="jy ja"> <em class="ku"> x₂=0 </em> </strong>，现在是<strong class="jy ja"> <em class="ku"> ŷ=0.5 </em> </strong>。</p><p id="ff3c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">那么，如何告诉神经网络它离我们想要的输出有多远呢？进来的是<strong class="jy ja"> <em class="ku">失去功能</em> </strong>来救援。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h2 id="5d95" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">损失函数</h2><p id="b84b" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated"><em class="ku"/><strong class="jy ja"><em class="ku">损失函数</em> </strong> <em class="ku">是一个简单的等式，它告诉我们我们的神经网络的预测输出(</em><strong class="jy ja"><em class="ku">【ŷ】</em></strong><em class="ku">)离我们期望的输出(</em><strong class="jy ja"><em class="ku">【y】</em></strong><em class="ku">)，</em> <strong class="jy ja"> <em class="ku">有多远，仅作为一个例子。</em> </strong></p><p id="d772" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">损失函数的</em> <strong class="jy ja"> <em class="ku">导数</em> </strong> <em class="ku">指示是否增加或减少权重。正导数意味着减少权重，负导数意味着增加权重。</em> <strong class="jy ja"> <em class="ku">斜率越陡，预测越不正确。</em> </strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a5d516f62991238f9f371c0723de9bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*3SJ0g7H9ietw8jfdYdYiew.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图11。损失函数可视化</figcaption></figure><p id="134a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">上图11所示的损失函数曲线是一个理想的版本。在现实世界的情况下，损失函数可能不会如此平滑，沿途会有一些颠簸和鞍点达到最小值。</em></p><p id="dec7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">有许多不同种类的损失函数<em class="ku"> </em> <strong class="jy ja"> <em class="ku">，每一个本质上都是计算预测输出和期望输出</em> </strong>之间的误差。这里我们将使用一个最简单的损失函数，<strong class="jy ja"> <em class="ku">平方误差损失函数。</em> </strong>定义如下:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/998fd9c174eadbe9d81aa85d13200692.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*k0sb1WH5CZNKO4-kNaAU3A.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图12。损失函数。计算单个示例的误差</figcaption></figure><p id="1093" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">取<strong class="jy ja">的平方使一切都变得美好而正</strong>并且<strong class="jy ja">的分数(1/2)是存在的，以便在取平方的</strong> <strong class="jy ja">项</strong> <em class="ku">的导数时抵消掉(在一些机器学习实践者中省略分数</em>是很常见的)。</p><p id="2ce7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">直观地说，平方误差损失函数帮助我们最小化预测线(蓝线)和实际数据(绿点)之间的垂直距离。在幕后，这条预测线就是我们的<strong class="jy ja"><em class="ku"/></strong>(线性函数)<strong class="jy ja"> <em class="ku"> </em> </strong>节点。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/283f66c518375dc0bb09d064a401ad65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*0g4I1QuoRvTIfaggl5uxjQ.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图13。损失函数效果的可视化</figcaption></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="052e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们知道了损失函数的目的，让我们计算我们当前预测的误差<strong class="jy ja"><em class="ku"/></strong><strong class="jy ja"><em class="ku">y = 0</em></strong>对于第一个例子<strong class="jy ja"> <em class="ku">。</em>T75】</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ny"><img src="../Images/fff25d0e16608846b7b99e6956ea42a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JM5Tpb-NuW0fvEoCo9knzw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图14。为1ˢᵗ示例计算的损失</figcaption></figure><p id="8c92" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们可以看到<em class="ku">损失</em>是<strong class="jy ja"> <em class="ku"> 0.125 </em> </strong>。鉴于此，<em class="ku">我们现在可以使用损失函数的导数来检查我们是否需要增加或减少我们的权重。</em></p><p id="1afb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个过程被称为<strong class="jy ja"><em class="ku"/></strong>反向传播，因为我们将做与<em class="ku">正向</em>阶段相反的事情。<em class="ku">我们不是从输入到输出，而是从输出到输入进行回溯</em>。<em class="ku">简单地说，反向传播允许我们计算出神经网络的每个部分造成了多少损失</em>。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="b789" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为了执行反向传播，我们将采用以下技术:<em class="ku">在每个节点，我们仅计算我们的局部梯度(该节点的偏导数)，然后在反向传播期间，当我们从上游接收梯度的数值时，我们将这些数值与局部梯度相乘，以将它们传递到它们各自连接的节点。</em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/9ff759382894655e2438b62645887f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*jmfVit0CiT7GdiJo1eeW6w.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图15。梯度流动</figcaption></figure><p id="cdc0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">这是从微积分中概括出的</em> <strong class="jy ja"> <em class="ku">链式法则</em> </strong> <em class="ku">。</em></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="3230" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">由于<strong class="jy ja"><em class="ku">【ŷ】</em></strong>(预测标号)决定了我们的<strong class="jy ja"><em class="ku"/></strong><strong class="jy ja"><em class="ku"/></strong>【实际标号】<strong class="jy ja"> <em class="ku"> </em> </strong>是常数，举个简单的例子，<em class="ku">我们将对</em><strong class="jy ja"><em class="ku"/></strong>的损失取偏导数</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b9ea37b6fa15dd534c05d27a5df4b845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*CCwwTIo0guZ7ikIwxfUi5w.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图16。损失的偏导数w.r.t <em class="nb"> ŷ </em></figcaption></figure><p id="bffe" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">由于反向传播步骤看起来有点复杂，我将一步一步地介绍它们:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ob"><img src="../Images/5d8a7a7bf6646ad0b24d38c198cfa7ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t814Dc4-En5111CQUpHaqg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图17a .反向传播</figcaption></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="fd9b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">对于下一个计算，我们需要sigmoid函数的导数，因为它形成了红色节点的局部梯度。让我们推导一下。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/aa5ae1050196646bc331bc1a24799c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*VgsCSBlfrXUrwSlrQzL7IQ.png"/></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f43e0f2d2d739d112dfdbaf2e3a98796.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*EcAuO3MLJTqsZXF2dm9djA.png"/></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi oe"><img src="../Images/c078af293d2cdf105fa537a5986b4b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gg4teWjLl5u2lC7ye3ds0w.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图18。Sigmoid函数的导数。</figcaption></figure><p id="60ba" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们在下一次逆向计算中使用它</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi of"><img src="../Images/d5b8f2249dbb8cf5fe7415173ed61464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*grL5g_LhxK14WDeInWUsiw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图17b .反向传播</figcaption></figure><p id="6099" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">反向计算不应一直传播到输入，因为我们不想更改输入数据(即红色箭头不应指向绿色节点)。我们只想改变与输入相关的权重。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi og"><img src="../Images/792050a576ea7e48402d6293b1872523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I4CJOzmktb33W-Ew3Cj1_g.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图17.c .反向传播</figcaption></figure><p id="0d72" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">注意到奇怪的事情了吗？<em class="ku">损失相对于weights,w₁ &amp; w₂的导数，为零</em>！如果它们的导数为零，我们就不能增加或减少权重。那么，在这种情况下，如果我们不知道如何调整权重，我们如何获得我们想要的输出呢？<em class="ku">这里需要注意的关键点是，局部渐变(</em><strong class="jy ja">【∂z/∂w₁】</strong><em class="ku">和</em><strong class="jy ja">【∂z/∂w₂】</strong><em class="ku">)是</em><strong class="jy ja"><em class="ku">【x₁】</em></strong><em class="ku">和</em><strong class="jy ja"><em class="ku">【x₂】，</em> </strong>在本例中，这两者恰好为零(即不提供任何信息)</p><p id="140f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这就给我们带来了<strong class="jy ja">偏差<em class="ku">的概念。</em> </strong></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h2 id="a7ea" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">偏见</h2><p id="66db" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">回想一下你高中时代的直线方程。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/07e2ded011ee7017893cf59212effd80.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*zqb9VvAzbqh_vEhgmfrK3Q.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图19。直线方程</figcaption></figure><p id="8458" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这里<strong class="jy ja"> <em class="ku"> b </em> </strong>是偏置项。直观地说，偏差告诉我们，用<strong class="jy ja"><em class="ku">x</em></strong><em class="ku"/>计算的所有输出都应该有一个加性偏差<strong class="jy ja"><em class="ku"/></strong>b .因此，当<strong class="jy ja"> <em class="ku"> x=0 </em> </strong>(没有来自<em class="ku">自变量的信息)时，输出应该被偏置到正好是</em> <strong class="jy ja"> <em class="ku"> b. </em> </strong></p><p id="24cc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">注意，如果没有偏置项，一条直线只能通过原点(0，0)，那么直线之间的唯一区分因素将是梯度</em> <strong class="jy ja"> <em class="ku"> m. </em> </strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/3e8b398d5e629b665a3b39ce443cd023.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*oW9ll3xUg5HUjof2SgPNRQ.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图20。从原点开始的线</figcaption></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="8613" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，利用这些新信息，让我们给神经网络增加另一个节点；偏置节点。(<em class="ku">在神经网络文献中，除了输入层，每一层都假设有一个偏置节点，就像线性节点一样，所以这个节点在图中也经常被省略。</em>)</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi oj"><img src="../Images/f4978e7dbfa7df50feb4b93dd846ef62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n2J_qM3iCOL4h0tPBt6Mhw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图21。具有偏置节点的扩展神经网络</figcaption></figure><p id="7833" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在让我们用同样的例子做一个正向传播，<strong class="jy ja"><em class="ku">【x₁=0】</em></strong>y = 0，让我们设置bias，<strong class="jy ja"> <em class="ku"> b=0 </em> </strong> <em class="ku">(初始bias总是设置为零，而不是一个随机数)</em>，我们让Loss的反向传播算出如何调整bias。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ok"><img src="../Images/1058450500f328a2d4f65acd641d79e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eP9h5e8Q8NWTw0bXjr7lnA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图22。带有偏差单位的OR表数据的第一个示例的前向传播</figcaption></figure><p id="1bbb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">嗯，偏差为“<strong class="jy ja"> <em class="ku"> b=0 </em> </strong>”的前向传播根本没有改变我们的输出，但在我们做出最终判断之前，让我们先做一下后向传播。</p><p id="d818" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">像以前一样，让我们一步一步地进行反向传播。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ol"><img src="../Images/b96fa6f36bb3699c4eab4fae733ce9a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jnGWjzItjUaCuQsjhXrwFA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图23.a .有偏置的反向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi om"><img src="../Images/f05a4ac24ec3ebd180561fa1662d9df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDsA0MnzBGb2Be738eZmYg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图23.b .有偏置的反向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi on"><img src="../Images/15ce77f049259c2a8a6dc4b1773f8ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UM3uyn4hy37TrV9cGv9z3g.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图23.c .有偏置的反向传播</figcaption></figure><p id="d7d6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">万岁！我们刚刚算出了调整偏差的幅度。你注意到最酷的事情了吗？回忆反向传播允许我们计算出神经网络的每个部分造成了多少“损失”。当输入没有提供信息时，反向传播算法将损失函数中的所有误差分配给偏置项。</p><p id="0e96" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">由于偏差的导数(<strong class="jy ja"> ∂L/∂b </strong>)为正0.125，我们将需要通过向梯度的负方向移动来调整偏差(回想之前的损失函数曲线)。这在技术上称为<strong class="jy ja"> <em class="ku">渐变下降</em> </strong>，因为我们使用渐变的方向从倾斜区域“下降”到平坦区域。就这么办吧。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/b36b05979bccb3a386080f7f908a2ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*6VlThy-5n3x6SdQH6n8mGw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图24。使用梯度下降计算新偏差</figcaption></figure><p id="335e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在，我们已经将偏差稍微调整为<strong class="jy ja"> <em class="ku"> b=-0.125，</em> </strong>，让我们通过进行<strong class="jy ja">正向传播</strong>和<strong class="jy ja">检查新的损失<em class="ku">来测试我们是否做了正确的事情。</em>T9】</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ok"><img src="../Images/4b1f505b44d5b1af2e22f34de5cf8f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*diwyd3eqLg3617-DEPbyew.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图25。具有新计算的偏差的正向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi oo"><img src="../Images/3b0aa32df55b9dff1690f0730fb4f458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0h3ouVIMgPaxGfjyqTYh4A.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图26。新计算偏差后的损失</figcaption></figure><p id="65ea" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们预测的产量<strong class="jy ja"> <em class="ku"> </em> </strong>是<strong class="jy ja"><em class="ku">【ŷ≈0.469】</em></strong><em class="ku">(四舍五入到小数点后3位)</em><strong class="jy ja"><em class="ku"/></strong>比之前的0.5略有提高，损耗从0.125下降到0.109左右<strong class="jy ja"><em class="ku"/></strong>。这种轻微的校正是神经网络仅仅通过将其预测输出与期望输出<strong class="jy ja"> <em class="ku"> y </em> </strong>进行比较，然后向与梯度<strong class="jy ja"> <em class="ku">相反的方向移动而‘学习’的东西。</em> </strong>挺酷的吧？</p><p id="6528" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在你可能想知道，这仅仅是对之前结果的一个小的改进，我们如何达到最小的损失。两件事开始起作用:<strong class="jy ja"> <em class="ku"> a) </em>我们执行多少次迭代的‘训练’</strong>(每个训练周期都是前向传播，计算损失，然后是反向传播，通过梯度下降更新权重)。<strong class="jy ja"> <em class="ku"> b) </em>学习率。</strong></p><p id="4785" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">学习率？？？那是什么？我们来谈谈吧</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h2 id="951a" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">学习率</h2><p id="a44e" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">回想一下，上面我们是如何通过向梯度的相反方向移动来计算新的偏差的(即<strong class="jy ja"> <em class="ku">梯度下降</em> </strong>)。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi op"><img src="../Images/47c92e2e13379ed311f40fb640aa6738.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*uVv6QFIJcqGDEiErz5Hujg.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图27。更新偏差的方程式</figcaption></figure><p id="03bc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">请注意，当我们更新偏差时，我们将<strong class="jy ja"> <em class="ku">向渐变的相反方向移动了1步。</em>T51】</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi oq"><img src="../Images/a29b17955595f1dffb4f17e94536142a.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*xX63wGFR0ghaH2EPrw-5gA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图28。显示“步长”的更新偏差方程</figcaption></figure><p id="8ecb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们可以在梯度的相反方向上移动0.5、0.9、2、3或我们想要的任何一部分步长。这个'<em class="ku">步数'就是我们定义的</em> <strong class="jy ja"> <em class="ku">学习率</em> </strong>，通常用<strong class="jy ja"><em class="ku">α</em></strong>【alpha】<strong class="jy ja"><em class="ku">来表示。</em> </strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi or"><img src="../Images/734a0ade087475b81d15fb77519a0c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*XeOLDBlfQoQqN3seB8AVXA.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图29。梯度下降的一般方程</figcaption></figure><p id="601c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">学习率</em>定义了我们达到最小损失的速度。让我们在下面想象一下学习率的情况:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi os"><img src="../Images/c405b555a0f9545c8d6be60e99522a31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BkzqzoHeDgcK_00jBLzjUg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图30。可视化学习速度的影响。</figcaption></figure><p id="0942" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">正如你所看到的，学习率越低(α=0.5)，我们沿着曲线下降的速度越慢，我们要走很多步才能到达最低点。另一方面，学习率越高(α=5 ),我们迈出的步伐越大，到达最小值的速度也越快。</p><p id="e799" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">眼尖的人可能已经注意到，当我们越来越接近最小值时，梯度下降步骤(绿色箭头)变得越来越小，这是为什么呢？回想一下，学习率正乘以曲线上该点的梯度；当我们从倾斜区域下降到u形曲线的平坦区域时，在最小点附近，梯度变得越来越小，因此阶梯也成比例地变小。因此，在训练期间改变学习率是不必要的(梯度下降的一些变化从高学习率开始快速下降，然后逐渐降低，这被称为“学习率退火”或“学习率衰减”)</p><p id="5a44" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">那么外卖是什么？只要把学习率设置的尽可能的高，快速达到最优损耗就可以了。不会。学习率可能是一把双刃剑。<em class="ku">学习率太高</em>并且参数(权重和偏差)没有达到最优，而是开始偏离最优。<em class="ku">学习率</em>太小，参数收敛到最优需要太长时间。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ot"><img src="../Images/f1ec1b18624c52f8067b6b7bb8735cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYFrrV1r4KnAGRMeLA4UNQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图31。将非常低的学习率和非常高的学习率的效果可视化。</figcaption></figure><p id="9a5e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">小学习rate(α=5*10⁻ ⁰)导致无数的步骤达到最小点是不言自明的；将梯度乘以一个小数值(α)会产生一个成比例的小步长。</p><p id="33c1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">导致梯度下降发散的大学习率(α=50)可能是混杂的，但答案是相当简单的；请注意，在每一步，梯度下降通过直线移动(图中的绿色箭头)来近似其向下的路径，简而言之，它估计其向下的路径。当学习速率太高时，我们强制梯度下降以采取更大的步长。较大的步长倾向于过高估计向下的路径，并越过最小值点，然后为了校正不好的估计，梯度下降试图向最小值点移动，但是由于较大的学习速率，再次越过最小值。这种持续高估的循环最终导致结果出现偏差(即每次训练循环后的损失增加，而不是减少)。</p><p id="fd17" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">学习率就是所谓的<strong class="jy ja"> <em class="ku">超参数</em> </strong>。超参数是神经网络基本上不能通过梯度的反向传播来学习的参数，它们必须由神经网络模型的创建者根据问题及其数据集来手动调整。<em class="ku">(上述损失函数的选择也是超参数)</em></p><p id="46b9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">简而言之，目标不是找到“完美的学习率”,而是一个足够大的学习率，以便神经网络成功有效地训练而不发散。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="f324" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">到目前为止，我们只使用了一个例子(<strong class="jy ja"> <em class="ku"> x₁=0 </em> </strong>和<strong class="jy ja"> <em class="ku"> x₂=0 </em> </strong>)来调整我们的权重和偏差(<em class="ku">实际上，到目前为止只有我们的偏差</em>🙃)这减少了我们整个数据集(或选通表)中一个例子的损失。但我们有不止一个例子可以借鉴，我们希望减少所有例子中的损失。<strong class="jy ja">理想情况下，在一次训练迭代中，我们希望减少所有训练示例的损失</strong>。这被称为<strong class="jy ja">批量梯度下降</strong>(或全批量梯度下降)，因为我们在每次训练迭代中使用整批训练样本来提高我们的权重和偏差。<em class="ku">(其他形式有</em> <strong class="jy ja"> <em class="ku">小批量梯度下降</em> </strong> <em class="ku">，其中我们在每次迭代中使用数据集的子集，以及</em> <strong class="jy ja"> <em class="ku">随机梯度下降</em> </strong> <em class="ku">，其中我们到目前为止在每次训练迭代中仅使用一个示例)。</em></p><p id="2611" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">神经网络遍历所有训练样本的训练迭代称为</em> <strong class="jy ja"> <em class="ku">时期</em> </strong>。<em class="ku">如果使用小批量，则在神经网络遍历所有小批量之后，一个时期将是完整的，类似于随机梯度下降，其中批量只是一个例子。</em></p><p id="937e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在我们继续之前，我们需要定义一个叫做<strong class="jy ja"> <em class="ku">的成本函数</em> </strong>。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h2 id="7754" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">价值函数</h2><p id="3234" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">当我们执行"<em class="ku">批次梯度下降"</em>时，我们需要稍微改变损失函数，以适应批次中的所有示例，而不仅仅是一个示例。这个调整后的损失函数被称为<strong class="jy ja">成本函数</strong>。</p><p id="04d3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">另外，注意成本函数的曲线与损失函数的曲线相似(同样的U型)。</em></p><p id="6b72" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> <em class="ku">成本函数不是计算一个示例的损失，而是计算所有示例的平均损失。</em>T49】</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/852878818c2e4a07c77da07fd4169368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*qwugnYb7_amC4eizcHqtIw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图32。价值函数</figcaption></figure><p id="2f62" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">直观上，成本函数扩展了损失函数的能力。回想一下，损失函数是如何帮助最小化单个<em class="ku">数据点和预测线(<strong class="jy ja"> <em class="ku"> z </em> </strong>)之间的垂直距离的。<strong class="jy ja">成本函数有助于同时最小化多个数据点之间的垂直距离(平方误差损失)。</strong></em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/1b732b68921e70d9447d69dd6dad6966.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*PR4OoB0H5HsNWZzOo63kag.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图33。成本函数效果的可视化</figcaption></figure><p id="0df0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">在批量梯度下降过程中，我们将使用成本函数</strong>的导数，而不是损失函数，来引导我们在所有示例中达到最小成本的路径。<em class="ku">(在一些神经网络文献中，成本函数有时也用字母</em><strong class="jy ja"><em class="ku">【J】</em></strong><em class="ku">来表示)。)</em></p><p id="e9a1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们看看成本函数的导数方程与损失函数的普通导数有什么不同。</p><h2 id="a7e2" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">成本函数的导数</h2><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/27802732f8a117a9d2bd655f0528f32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*6B9g8pubPikbp5k_53DRCQ.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图34。成本函数显示它采用输入向量</figcaption></figure><p id="918f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">对这个成本函数求导可能有点冒险，成本函数将向量作为输入并求和。所以，在推广导数之前，让我们从一个简单的例子开始。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ox"><img src="../Images/c966fc78c85c0bc47d1c8c2d78aebccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LzdMVOr0hN_PlzcrkGRouA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图35。简单矢量化示例的成本计算</figcaption></figure><p id="2c7f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在成本计算方面没有什么新的东西。正如预期的那样，最终的成本是损失的平均值，但现在实现是矢量化的<em class="ku">(我们执行矢量化减法，然后执行元素幂运算，称为Hadamard幂运算)</em>。让我们推导偏导数。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi oy"><img src="../Images/636370cc4697249bf3e9d146954f3371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gfeLddxHuKBReuSK-qcDBw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图36。简单例子上雅可比矩阵的计算</figcaption></figure><p id="0f89" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">由此，我们可以推广偏导数方程。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi oz"><img src="../Images/7142c5293df9c7c600b96435a6ddff84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mdb9v1HLN46RWbw5YPWhew.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图37。广义偏导数方程</figcaption></figure><p id="6450" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们应该花点时间来注意损失的导数和成本的导数有什么不同。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pa"><img src="../Images/e1b3e6314b219a726f5314f47c5af2e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oklVtPMbTDpX5Nkcyxg7Qg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图38。损失和成本相对于(w.r.t) <strong class="bd lm"> ŷ⁽ⁱ⁾ </strong>的偏导数比较</figcaption></figure><p id="4f0e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们以后会看到这个小变化是如何在梯度的计算中表现出来的。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="d359" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">回到批量梯度下降。</p><p id="6115" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">有两种方法执行批量梯度下降:</p><p id="ba17" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 1。</strong>对于每个训练迭代，创建单独的临时变量(大写deltas，δ),这些变量将累积来自我们训练集中的每个<strong class="jy ja"> " <em class="ku"> m" </em> </strong>示例的权重和偏差的梯度(小写deltas，δ),然后在时期结束时使用累积梯度的平均值更新权重。这是一个缓慢的方法。<em class="ku">(对于那些熟悉时间复杂度分析的人，你可能会注意到，随着训练数据集的增长，这变成了一个多项式时间算法，O(n )) </em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/6f0863691bb4521af0d195e12a7f6858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*7bL3Aqxa2zGTZDhnkpZkfw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图39。批量梯度下降慢法</figcaption></figure><p id="c7a5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 2。</strong>更快的方法类似于上述方法，但是使用矢量化计算来一次性计算所有训练示例的所有梯度，因此去除了内部循环。矢量化计算在计算机上运行得更快。这是所有流行的神经网络框架所采用的方法，也是我们将在本博客的其余部分遵循的方法。</p><p id="86b3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">对于矢量化计算，我们将对神经网络计算图的“Z”节点进行调整，并使用成本函数代替损失函数。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pc"><img src="../Images/55f83929b92b3f307c7dca1abe89ab19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0SXKGjj2HtyNHJUbQyWzg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图40。Z节点的矢量化实现</figcaption></figure><p id="58df" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">注意，在上图中，我们在<strong class="jy ja"> <em class="ku"> W </em> </strong>和<strong class="jy ja"> <em class="ku"> X </em> </strong>之间取<strong class="jy ja">点积</strong>，它可以是适当大小的矩阵或向量。偏差<strong class="jy ja"> <em class="ku"> b </em> </strong>在这里仍然是一个单一的数字<em class="ku">(一个标量)</em>，并且将以元素方式添加到点积的输出中。预测的输出将不仅仅是一个数字，而是一个向量，<strong class="jy ja"><em class="ku"/></strong>，其中每个元素都是它们各自示例的预测输出。</p><p id="c0ef" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在进行前向和后向传播之前，让我们设置输出数据(<strong class="jy ja"> <em class="ku"> X，W，b &amp; Y </em> </strong>)。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pd"><img src="../Images/ce1a8817ba6717244d62bba9fcb84f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Df6eOrvIU6eJbKuNYnRgQQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图41。为矢量化计算设置数据。</figcaption></figure><p id="b1a6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们现在终于准备好使用<strong class="jy ja"> Xₜᵣₐᵢₙ </strong>、<strong class="jy ja"> Yₜᵣₐᵢₙ </strong>、<strong class="jy ja"> W、</strong>和<strong class="jy ja"> b </strong>进行正向和反向传播。</p><p id="df46" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">(注:以下所有结果均四舍五入至小数点后3位，仅为简洁起见)</em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pe"><img src="../Images/5c5d4e66609fd312666ac8c7fafa7c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJwiwK6IqErfG9ruo0SddQ.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pf"><img src="../Images/0b3b7c827d3a7d81286edf39d3109e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YHQnlbnxMnlV8ctzXM1luA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图42。或门数据集上的矢量化前向传播</figcaption></figure><p id="d5c2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">多酷啊，我们一次就计算出了数据集中所有例子的所有正向传播步骤，仅仅是通过对我们的计算进行矢量化。</p><p id="65de" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们现在可以根据这些输出预测来计算<strong class="jy ja">成本</strong>。<em class="ku">(我们将详细检查计算，以确保没有混淆)</em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pg"><img src="../Images/dfafa3dfd052189ec405688b01b53979.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eLly2bVO_4YjhSSXpZuFqQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图43。“或”门数据的成本计算</figcaption></figure><p id="3a67" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们的<strong class="jy ja">成本</strong>与我们当前的重量<strong class="jy ja"> W </strong>，结果是<strong class="jy ja"> 0.089 </strong>。我们现在的目标是使用反向传播和梯度下降来降低这个成本。和以前一样，我们将一步一步地进行反向传播</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ph"><img src="../Images/dabce6e8bd7e88e75428e0908b6ae43e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vAKWvK7Q6QGvaslHqeXmqg.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pi"><img src="../Images/47b8e3b69e08f70fb7e7af871a67e7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JhuHyrPl3p6nPPphLou0UA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图44.a .或门数据的反向矢量化</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pj"><img src="../Images/3a766461e5a31179e5aad18c1ace6844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dqj1FEprxRU_VMF0LDVhCQ.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pi"><img src="../Images/ea0dc2f30f535b2a358aea9cf83a0ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RY5hRYtRuKrEYXztDqdJhA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图44.b .或门数据的反向矢量化</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pj"><img src="../Images/647d82670cd34bc54787c42c7bd9aad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oFV-bXznrdj_WTqwQMNRtg.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pk"><img src="../Images/17cf7af6d3c937a5508aae05f60c0bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U3hJ9965Xn9oo0gI5KT6pw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图44.c .或门数据的反向矢量化</figcaption></figure><p id="4c49" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">瞧，我们使用<em class="ku">批量梯度下降</em>的矢量化实现来一次性计算所有梯度。</p><p id="3669" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">(眼尖的人可能想知道在这最后一步中，局部梯度和最终梯度是如何计算的。别担心，我会在最后一步解释梯度的推导，很快。现在，可以说最后一步定义的梯度是对计算∂Cost/∂W和∂Cost/∂b的简单方法的优化</em></p><p id="f694" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们更新权重和偏差，保持学习速率与之前的非矢量化实现相同，即<strong class="jy ja"> <em class="ku"> α=1。</em>T51】</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/001ce72b66d6cfd17a11d32f5d8c64ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*TcgjZyEXpM30os-VVOnJrw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图45。计算出新的权重和偏差</figcaption></figure><p id="73d5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们已经更新了权重和偏差，让我们做一个<strong class="jy ja">正向传播</strong>和<strong class="jy ja">计算新的成本</strong>来检查我们是否做了正确的事情。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pm"><img src="../Images/b3526ece1423098150595f20d7fae437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vTdAvUG7jNX8a3t-PuVnfw.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi pn"><img src="../Images/1b46e86781e85b1c4d6f69856cacbdd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T4jO523IKcndwPxvH1drEg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图46。具有更新的权重和偏差的矢量化前向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi po"><img src="../Images/2e58f0f38aabadd0b39a1452f025c160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hop21GNfSaJB2AauSydk8g.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图47。更新参数后的新成本</figcaption></figure><p id="a89a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，我们<em class="ku">将我们的成本(所有示例的平均损失)</em>从最初的大约<strong class="jy ja"> <em class="ku"> 0.089 </em> </strong>降低到<strong class="jy ja"> 0.084 </strong>。在我们能够收敛到一个低的<strong class="jy ja"> <em class="ku"> </em> </strong>成本之前，我们将需要进行多次训练迭代。</p><p id="62e9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这一点上，我建议您自己执行反向传播步骤。那的结果应该是大概的(四舍五入到小数点后3位):<strong class="jy ja"> <em class="ku"> ∂Cost/∂W = [-0.044，-0.035】</em></strong><strong class="jy ja"><em class="ku">∂cost/∂b =[-0.031]。</em>T19】</strong></p><p id="3d52" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">回想一下，在我们训练神经网络之前，我们如何预测神经网络可以在图9中分离两个类，在大约5000个历元(完整的批量训练迭代)之后，成本稳定地降低到大约<strong class="jy ja"> <em class="ku"> 0.0005 </em> </strong>并且我们得到以下判定边界<strong class="jy ja"> <em class="ku"> : </em> </strong></p><div class="kv kw kx ky gt ab cb"><figure class="pp kz pq pr ps pt pu paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/2cfc6e7f97eff66cc8958236e3b64448.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*bzw1lyWE4D7dAbFGa14ipw.png"/></div></figure><figure class="pp kz pv pr ps pt pu paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/948132cc962e33b4cc70c2c4ca256119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*-Qu6JQo59hlXxNa4JDJW0w.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk pw di px py translated">图48。5000个时代后的成本曲线和决策边界</figcaption></figure></div><p id="7d39" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">成本曲线</strong>基本上是在一定数量的迭代(在这种情况下是时期)后绘制的成本值。请注意，成本曲线在大约3000个时期后变平，这意味着神经网络的权重和偏差已经收敛，因此进一步的训练只会稍微改善我们的权重和偏差。为什么？回想一下u形损耗曲线，随着我们越来越接近最低点(平坦区域),梯度变得越来越小，因此梯度下降的步长非常小。</p><p id="aea7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">决策边界</strong>显示了神经网络的决策从一个输出变为另一个输出的路线。我们可以通过给决策边界上下的区域着色来更好地形象化这一点。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/5a6322135b51992a5396bc3f9a0a6250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*LY1sgsZJnHnZzsq5UPcx8A.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图49。5000个时代后的决策边界可视化</figcaption></figure><p id="fa4b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这就清楚多了。红色阴影区域是决策边界以下的区域，决策边界以下的所有内容都有一个输出(<strong class="jy ja"> ŷ </strong>)为<strong class="jy ja"> 0 </strong>。类似地，决策边界以上的所有内容(绿色阴影)的输出为<strong class="jy ja"> 1 </strong>。总之，我们的简单神经网络通过查看训练数据并找出如何分离其两个输出类(<strong class="jy ja"> y=1 </strong>和<strong class="jy ja"> y=0 </strong>)来学习决策边界🙌。现在输出神经元活跃起来🔥(产生1)每当<strong class="jy ja"><em class="ku"/></strong>或<em class="ku"> </em> <strong class="jy ja"> <em class="ku"> x₂ </em> </strong>或两者都为1。</p><p id="df4d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在是查看成本函数中的“<strong class="jy ja">1/m</strong>”(“<strong class="jy ja">m</strong>”是训练数据集中的样本总数)如何在梯度的最终计算中表现出来的好时机。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qa"><img src="../Images/b3cba710b035da93e4e06af10a5faf03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e85QKH2CL1cMapA8ld_Dmg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图50。比较衍生小波变换成本和损失对神经网络参数的影响</figcaption></figure><p id="4c53" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">由此可知，要知道的最重要的一点是，使用成本函数来更新我们的权重的梯度是在训练迭代期间计算的所有梯度的平均值；</strong><strong class="jy ja"/><strong class="jy ja">同样适用于偏置</strong>。您可能希望通过亲自检查矢量化计算来确认这一点。</p><p id="48f3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">取所有梯度的平均值有一些好处。首先，它给我们一个较少噪声的梯度估计。第二，得到的学习曲线是平滑的，帮助我们容易地确定神经网络是否在学习。当在更复杂的数据集上训练神经网络时，例如那些可能错误标记了样本的数据集，这两个特征都非常方便。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="5410" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">这很好，但是你怎么计算∂Cost/∂W和∂Cost/∂b的梯度呢？🤔。</h1><p id="a04c" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">我学习的神经网络指南和博客帖子经常会省略复杂的细节，或者给出非常模糊的解释。不是在这个博客里，我们会不遗余力地检查每一件事。</p><h2 id="b5cc" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">首先，我们要处理∂Cost/∂b.，为什么我们要对梯度求和？</h2><p id="6b28" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">为了解释这一点，我在三个非常简单的方程上使用了我们的计算图形技术。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/51c56afb3675c1891d41ec750993a75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*M7Ce9TrPab5neHHYY4hbSw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图51。简单方程的计算图</figcaption></figure><p id="7913" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我对<strong class="jy ja"> <em class="ku"> b </em> </strong>节点特别感兴趣，我们就这个做反向传播吧。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qc"><img src="../Images/8ffbe9c29d890782ab30d7c87e3fb9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ahiviCqq6B0R_XWBmgvHkA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图52。简单方程计算图上的反向传播</figcaption></figure><p id="e985" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">注意，<strong class="jy ja"> <em class="ku"> b </em> </strong>节点正在从<strong class="jy ja">两个</strong>其他节点接收渐变。所以流入节点<strong class="jy ja"> <em class="ku"> b </em> </strong>的渐变的总和就是流入的两个渐变的总和<em class="ku"/>。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qd"><img src="../Images/bc95a28bc04c6911e00a108290d2570f.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*A58FkCRT-v8tSx-wDBPEDA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图53。流入节点<strong class="bd lm"> b </strong>的梯度之和</figcaption></figure><p id="0677" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">从这个例子中，我们可以归纳出以下规则:<strong class="jy ja"> <em class="ku">从所有可能的路径中，将所有进入的渐变求和到一个节点。</em> </strong></p><p id="01a3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们想象一下如何在<strong class="jy ja">偏差</strong>的计算中使用该规则。<em class="ku">我们的神经网络可以被视为对我们的每个示例</em>进行 <strong class="jy ja"> <em class="ku">独立的</em> </strong> <em class="ku">计算，但是在批量训练迭代期间使用权重和偏差的共享参数。Below bias ( <strong class="jy ja"> <em class="ku"> b </em> </strong>)被可视化为我们的神经网络执行的所有单独计算的共享参数。</em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/2e0c6c06aadb6a9fe96bd408e7a737a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*0k67vmx2icCbHihGvXmEag.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图54。可视化跨训练时期共享的偏差参数。</figcaption></figure><p id="eb0f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">遵循上面定义的一般规则，我们将对从所有可能路径到偏置节点的所有输入梯度求和，<strong class="jy ja"> b </strong>。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/64022327e1e33a5255cce1fa70d5d5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*xyK8Gj7BMO1B3qPan2hMiw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图55。可视化到共享偏置参数的所有可能的反向传播路径</figcaption></figure><p id="e7c9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因为∂Z/∂b(在z节点的局部梯度)等于<strong class="jy ja"> 1 </strong>，所以在<strong class="jy ja">t5】bT7】的总梯度是来自每个例子的关于成本的梯度的总和。</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qf"><img src="../Images/cdeff07c11063b1697ce0c2c5a1712c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WO1fGdWYj8bVLdo0IBUn3w.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图56。∂Cost/∂b是上游梯度之和的证明</figcaption></figure><p id="c879" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">既然我们已经得到了偏差的导数，让我们继续讨论权重的导数，更重要的是权重的局部梯度。</p><h2 id="4719" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">局部gradient(∂Z/∂W)如何等于输入训练数据的转置(X_train)？</h2><p id="9e20" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">这可以用与上述偏差计算类似的方式来回答，但是这里的主要复杂之处是计算权重矩阵(<strong class="jy ja"><em class="ku">【w】</em></strong>)和数据矩阵(<strong class="jy ja"><em class="ku">【xₜᵣₐᵢₙ】</em></strong>)之间的点积的导数，这形成了我们的局部梯度。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qg"><img src="../Images/8e2eaca2376227e05f5b1a2e9df71915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-H_pSmae0x-RGIeWxih2A.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图57a .计算点积的导数。</figcaption></figure><p id="196c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">点积的导数有点复杂，因为我们不再处理标量；取而代之的是，无论是<strong class="jy ja"> <em class="ku"> W </em> </strong>和<strong class="jy ja"> <em class="ku"> X </em> </strong>都是矩阵，<strong class="jy ja"> <em class="ku"> W⋅ X </em> </strong>的结果也是矩阵。让我们先用一个简单的例子来更深入一点，然后再从中归纳。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qh"><img src="../Images/bead25b4e18843cf98e2d97302cd3344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uk--KqsUGKQ5ohnMWpec3g.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图57b .计算点积的导数。</figcaption></figure><p id="b857" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们计算一下<strong class="jy ja"> <em class="ku"> A </em> </strong>相对于<strong class="jy ja"> <em class="ku"> W </em> </strong>的导数。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qi"><img src="../Images/3bbdb6d048a9dce89735f219994a8046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50eUQstsqSQiE9F6CJFDDw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图57c .计算点积的导数。</figcaption></figure><p id="ccbc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们在批量训练迭代的情况下将这一点形象化，其中多个例子同时被处理。<em class="ku">(注意输入的例子是列向量。)</em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/bd9ebe364014d9cd6b043e8956d537b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*VPZ9IA3J__b7QCxiWLvyfQ.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图58。可视化在整个训练时期共享的权重</figcaption></figure><p id="61a5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">正如偏差(<strong class="jy ja"> b) </strong>在批量训练迭代中的每个计算中被共享一样，权重(<strong class="jy ja"> <em class="ku"> W </em> </strong>)也被共享。我们还可以将梯度流回权重可视化，如下所示<em class="ku">(注意，每个示例w.r.t对</em> <strong class="jy ja"> <em class="ku"> W </em> </strong> <em class="ku">的局部导数导致输入示例的行向量，即输入的转置)</em>:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/ab1d1e37858a97844f189747ef5b78fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*ePHF4T7pqdGGjIv4-NY0Sw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图59。将所有可能的反向传播路径可视化到共享权重参数</figcaption></figure><p id="ab19" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">同样，遵循上面定义的一般规则，我们将对从所有可能路径到权重节点的所有传入梯度求和，<strong class="jy ja"> W </strong>。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qf"><img src="../Images/f364b9c5fe64023e4e5386b3d51d7ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQT8UkOhGaT38ew9BFb_lA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图60。可视化后∂Cost/∂W的推导。</figcaption></figure><p id="ef36" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">到目前为止，我们所做的计算，<strong class="jy ja"><em class="ku"/></strong>，虽然是正确的，并且是一个很好的解释，但是，这不是一个优化的计算。我们也可以将这种计算矢量化。让我们接下来做那个</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qk"><img src="../Images/aa44e279c5d9d76159c9a7cd16ca1598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4f7Rrj6-0mRFLtQV9Kxtqw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图61。证明∂Cost/∂W是上游梯度和<strong class="bd lm"><em class="nb"/></strong>转置的点积</figcaption></figure><h2 id="786b" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">有没有更简单的方法来解决这个问题，不需要数学？</h2><p id="f2e9" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">是啊！使用<strong class="jy ja"> <em class="ku">量纲分析</em> </strong>。</p><p id="fbe1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在我们的OR门示例中，我们知道流入节点<strong class="jy ja"><em class="ku"/></strong>的梯度是<strong class="jy ja"> <em class="ku"> </em> </strong> a (1 × 4)矩阵，Xₜᵣₐᵢₙ是(2 × 4)矩阵，并且成本相对于<strong class="jy ja"> <em class="ku"> W </em> </strong>的导数需要与<strong class="jy ja"> <em class="ku"> W </em> </strong>大小相同，即(1 × 2)。因此，生成(1 × 2)矩阵的唯一方法是取<strong class="jy ja"><em class="ku"/></strong>和<strong class="jy ja"><em class="ku">【xₜᵣₐᵢₙ】</em></strong>之间的点积。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/db4108182f29732586dea5c18bf65dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*qUs8FLC3S9SwRxsGH_hXBw.png"/></div></figure><p id="d14c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">同样，已知bias，<strong class="jy ja"> <em class="ku"> b </em> </strong>，是一个简单的(1 × 1)矩阵，流入节点Z的梯度是(1 × 4)，使用量纲分析我们可以确定成本w.r.t <strong class="jy ja"> <em class="ku"> b </em> </strong>的梯度也需要是一个(1 × 1)矩阵。我们能做到这一点的唯一方法，给定局部梯度(<strong class="jy ja"><em class="ku">【∂z/∂b】</em></strong>)正好等于<strong class="jy ja"><em class="ku">【1】</em></strong>)，是通过对上游梯度求和。</p><p id="5040" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">最后一点要注意的是，推导导数表达式时，先做一些小例子，然后从这些例子中进行归纳。例如，在这里，当计算点积w.r.t对<strong class="jy ja"><em class="ku">【W，</em> </strong> <em class="ku">的导数时，我们使用单个列向量作为测试用例，并从那里进行推广，如果我们使用整个数据矩阵，那么导数将产生一个(4 × 1 × 2)张量(多维矩阵)，在此基础上的计算可能会有点麻烦。</em></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="a883" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在结束本节之前，让我们看一个稍微复杂一点的例子。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/2c25eeaf44e08c2a983621faca9caad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*F1lPFm31-h6NJO25SoqOmA.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图62。异或门数据</figcaption></figure><p id="852e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">上面的图62显示了一个异或门数据。看着它注意标签，<strong class="jy ja"><em class="ku">【y】</em></strong>，等于<strong class="jy ja"><em class="ku"/></strong>只有当其中一个值<strong class="jy ja"><em class="ku"/></strong>或<strong class="jy ja"><em class="ku"/></strong>等于<strong class="jy ja"><em class="ku">【1】</em></strong>，<em class="ku">不同时等于</em>。这使得它成为一个特别具有挑战性的数据集，因为数据不是线性可分的，即没有单一的直线判定边界可以成功地将数据中的两个类(<strong class="jy ja"> <em class="ku"> y=1 </em> </strong>和<strong class="jy ja"> <em class="ku"> y=0 </em> </strong>)分开。XOR曾经是早期人工神经网络的祸根。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi qn"><img src="../Images/b5184a79b7432faa61ea38a5eecd4a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ludDf_YrCWdKiAXq8FzVrg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图63。一些错误的线性决策界限</figcaption></figure><p id="1072" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">回想一下，我们当前的神经网络之所以成功，只是因为它能够计算出能够成功分离两类或门数据集的直线决策边界。直线不会在这里切断它。那么，我们如何让神经网络来解决这个问题呢？</p><p id="9e10" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">嗯，我们可以做两件事:</p><ol class=""><li id="f89a" class="qo qp iq jy b jz ka kd ke kh qq kl qr kp qs kt qt qu qv qw bi translated">修改数据本身，以便除了特征<strong class="jy ja"><em class="ku"/></strong>和<strong class="jy ja"><em class="ku">【x₂】</em></strong>之外，第三个特征提供一些附加信息来帮助神经网络决定好的决策边界。这个过程称为<strong class="jy ja"> <em class="ku">特征工程</em> </strong>。</li><li id="d37d" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt qt qu qv qw bi translated">改变神经网络的架构，使其更深入。</li></ol><p id="c436" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们看一看哪一个更好。</p><h2 id="c6c6" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">特征工程</h2><p id="3c7e" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">让我们看一个类似于XOR数据的数据集，它将帮助我们实现一个重要的认识。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi rc"><img src="../Images/2bde7e1815b1ca5bef748c154e9dd6df.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*HsKAsQnBI8AM_Q-kTig_Dw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图64。不同象限中的类XOR数据</figcaption></figure><p id="9f1a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">图64中的数据与XOR数据完全一样，只是每个数据点分布在不同的象限。请注意，在<strong class="jy ja"> 1ˢᵗ和3ʳᵈ象限中，x₁和x₂的乘积为正</strong>，而在<strong class="jy ja"> 2ⁿᵈ和4ᵗʰ象限中，x₁和x₂的乘积为负。</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi rc"><img src="../Images/fbff7cb007a04fe8932b0a4568bcac78.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*s2ILqJW26WJ48XGWkmPU0A.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图65。正负象限</figcaption></figure><p id="ca81" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这是为什么呢？在<strong class="jy ja"> 1ˢᵗ </strong>和<strong class="jy ja"> 3ʳᵈ </strong>象限<strong class="jy ja">中，数值的符号被平方</strong>，而在<strong class="jy ja"> 2ⁿᵈ </strong>和<strong class="jy ja"> 4ᵗʰ </strong>象限<strong class="jy ja">中，数值是一个负数和一个正数之间的简单乘积，从而产生一个负数。</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rd"><img src="../Images/53fa299de1d75433104ea7141ff7b019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8_3aQXFZamWVwVUuIXeQQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图66。特性乘积的结果</figcaption></figure><p id="dcc6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">所以，这给了我们一个使用两个特性的<em class="ku">产品的模式。我们甚至可以在XOR数据中看到类似的模式，其中每个象限都可以以类似的方式识别。</em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi re"><img src="../Images/57e9e3f0623ed2627f4cc1435d267c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*EAiSwouM-VCINp2wsDXrZg.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图67。XOR数据图中的象限模式</figcaption></figure><p id="4407" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> <em class="ku">因此，一个好的第三个特征，x₃，将是特征x₁和x₂(i.e. x₁*x₂).的乘积</em>T3】</strong></p><p id="418f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">特征的乘积称为一个<strong class="jy ja"> <em class="ku">特征交叉</em> </strong>并产生一个新的<strong class="jy ja"> <em class="ku">合成特征</em> </strong>。<em class="ku">特征交叉可以是特征本身(如</em><strong class="jy ja"><em class="ku">【x₁】【x₁】…</em></strong><em class="ku">)，也可以是两个或两个以上特征的乘积(如</em><strong class="jy ja"><em class="ku">【x₁*x₂】【x₁*x₂*x₃】…</em></strong><em class="ku">)，甚至是两者的组合(如</em><strong class="jy ja"><em class="ku">【x₁*x₂</em></strong><em class="ku">)。例如，在房屋数据集中，输入要素是房屋的宽度和长度(以码为单位),标注是房屋在地图上的位置，此位置的更好预测因子可能是房屋宽度和长度之间的要素交叉，这为我们提供了一个新要素“房屋的平方码大小”。</em></p><p id="df08" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们将新的合成特征添加到我们的训练数据中，Xₜᵣₐᵢₙ.</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rf"><img src="../Images/86e3686c2390357cb113093852d733ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJAe4mSh_8CpQKTzZF5N6g.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图68。新培训数据</figcaption></figure><p id="2613" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用这种特征交叉，我们现在可以成功地学习决策边界，而无需显著改变神经网络的架构。我们只需要为<strong class="jy ja"> <em class="ku"> x₃ </em> </strong>添加一个输入节点，并给输入层添加一个对应的权重(<em class="ku">随机设置为0.2 </em>)。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/2a2bd6afc9f1ba3767379d728393b720.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*Y4z_qkBxBlN0v22jz5e6YQ.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图69。具有特征cross(x₃的神经网络)作为输入</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi rg"><img src="../Images/3658325cb15f74be3bcfb94aa339b18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*g193OCCGszv74VQooTAlCg.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图70。具有特征cross(x₃的扩展神经网络)作为输入</figcaption></figure><p id="7433" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">下面给出的是神经网络的第一次训练迭代，你可以自己进行计算并确认它们，因为它们是一个很好的练习。由于我们已经熟悉了这种神经网络架构，我不会像以前一样一步一步地完成所有的计算，尽管这将是你展示肌肉并与下面的答案相符的好时机。</p><p id="59c0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">(以下所有计算均四舍五入至小数点后3位)</em></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rh"><img src="../Images/7ce596e331001b5c0d89f2a351a3abac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tbpG1k-bmFQK_Sliehu5JA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图71。第一次训练迭代中的正向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ri"><img src="../Images/d769c60d5144731c67a9f2af416faf74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bCCQ6EIJ12auTswUfubcQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图72。第一次训练迭代中的反向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi rj"><img src="../Images/b33193060ddff11e1bbba265489d61da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*8tfY7s66xUCkFcyQL27f8Q.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图73。在第一次训练迭代中，为新的权重和偏差进行梯度下降更新</figcaption></figure><p id="36c6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">经过5000个时期后，学习曲线和决策边界看起来如下:</p><div class="kv kw kx ky gt ab cb"><figure class="pp kz rk pr ps pt pu paragraph-image"><img src="../Images/373df12a026c8717acc07b3cacd3f13b.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*A0ARH20B7rkZ2OU4D18MMw.png"/></figure><figure class="pp kz rl pr ps pt pu paragraph-image"><img src="../Images/2de0a56750d6299c9fb2c957492300b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*PWCZXTt-x7NXh5roWl4PCg.png"/><figcaption class="mv mw gj gh gi mx my bd b be z dk rm di rn py translated">图74。具有特征交叉的神经网络的学习曲线和决策边界</figcaption></figure></div><p id="843e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">和以前一样，为了更好地可视化，我们可以对神经网络的决策从一个变化到另一个的区域进行着色。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ro"><img src="../Images/74d0990a3321d1c11ae3c5facb4b5681.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*X3wf-ctTi-mpiIQTNQplCg.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图75。阴影决策边界更好的可视化</figcaption></figure><p id="5f4a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> <em class="ku">注意，特征工程允许我们创建一个非线性的决策边界。</em> </strong>它是怎么做到的？我们只需要看看<strong class="jy ja"> <em class="ku"> Z </em> </strong>节点在计算什么函数。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rp"><img src="../Images/b45a98f515b42448f2c331507e994db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkyqZ564EBQca39wLZS9gQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图76。添加特征交叉后，节点<strong class="bd lm"> Z </strong>正在计算多项式</figcaption></figure><p id="5ad2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，特征交叉帮助我们创建复杂的非线性决策边界。</p><p id="a2d4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> <em class="ku">这是一个非常强大的想法！</em> </strong></p><h2 id="f0bb" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">改变神经网络结构</h2><p id="518d" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">这是更有趣的方法，因为它允许我们自己绕过特征工程，而<strong class="jy ja"> <em class="ku">让神经网络计算出特征本身！</em>T9】</strong></p><p id="1ed6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们来看看下面这个神经网络:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rq"><img src="../Images/f99179e4d036686d7d68211594a89dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Eay70SnBJQOw1qWt-yvpg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图77。单隐层神经网络。</figcaption></figure><p id="8eff" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，我们在“或”门示例的神经网络架构中间添加了一些新节点，保持输入层和输出层相同。中间这列新节点叫做<strong class="jy ja"> <em class="ku">隐藏层。</em> </strong> <em class="ku">为什么要隐藏图层？因为在定义它之后，我们不能直接控制隐藏层中的神经元如何学习，不像输入和输出层，我们可以通过改变数据来改变它；此外，由于隐藏层既不构成神经网络的输出也不构成神经网络的输入，所以它们本质上对用户是隐藏的。</em></p><p id="b850" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> <em class="ku">我们可以有任意数量的隐藏层，每层中有任意数量的神经元</em> </strong>。这种结构需要由神经网络的创建者来定义。<em class="ku">因此，</em> <strong class="jy ja"> <em class="ku">隐藏层的数量和每层中神经元的数量也是超参数。我们添加的隐藏层越多，我们的神经网络架构就变得越深，我们在隐藏层中添加的神经元越多，网络架构就变得越宽。神经网络模型的深度就是“深度学习”这个术语的来源。</em>T25】</strong></p><p id="5793" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">经过一些实验，选择了图77中具有三个乙状结肠神经元的一个隐藏层的架构。</em></p><p id="9503" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">由于这是一个新的架构，我将一步一步地进行计算。</p><p id="154f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">首先，让我们扩展神经网络。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rr"><img src="../Images/8b43b1ffb0ca90cb533a96061aee2bd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ngu9oEHNj9VoS97sy5hlAQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图78。单隐层扩展神经网络</figcaption></figure><p id="5192" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在让我们执行一个<strong class="jy ja"> <em class="ku">正向传播:</em> </strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rs"><img src="../Images/d671d0d6a5d45784f8ab9fb7f881818c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Wunm1SOH39jS-MGgtMXtg.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rt"><img src="../Images/8016a1b80220253904acefe52974fb34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rcRQarQvU5Hytxkr6AbaMA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图79.a .带有隐藏层的神经网络上的前向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ru"><img src="../Images/309c646f84efdf47b799250a958e1bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qfYr3SYiWN-XkHRXbWzbJg.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rt"><img src="../Images/ef1da386ba532e1f86d19e0610724fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iRzlVUmevF78S6IPpdwKSw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图79b .带有隐藏层的神经网络的前向传播</figcaption></figure><p id="f957" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们现在可以计算成本:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rv"><img src="../Images/162f8bcbf1b73fcdc00228d00c1d1337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWnLYXjPrygC0rY_FXAwfQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图80。在<strong class="bd lm">第一个</strong>前向传播之后，具有一个隐藏层的神经网络的成本</figcaption></figure><p id="6bda" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">计算完成本后，我们现在可以进行反向传播，并改进权重和偏差。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rw"><img src="../Images/037fd339215b693da96f2c659d2c51d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zeN9nRee79xNFFR-EnFDvQ.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rt"><img src="../Images/ab3feb63dab6126825908e211da98967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2gcVj1EMgF_Iau7HfyORvA.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图81.a .带有隐藏层的神经网络的反向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rw"><img src="../Images/db0e5ddc28f2c14991a2084d31d029a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_htRBjokxnqFN2fWpyuB5Q.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rt"><img src="../Images/23d1314fb0228f2b3995db8a59bb9ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RwMkeV-9TDn3QsZu7Ma2uQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图81b .带有隐藏层的神经网络的反向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rw"><img src="../Images/6e2bc8ccb432234e72c10814ffd4121e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AQaN1E5FND1lhPlh_VUd2w.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rt"><img src="../Images/15981133ec934c01acd09ceaae35f353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sfZh_O1JvsDSu8qPB4ZxZg.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图81c .带有隐藏层的神经网络的反向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rw"><img src="../Images/021a156f1bc22c38e6c26ae8f51ba64b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z2RbG7oLKs6lDsvt59lKqw.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rx"><img src="../Images/95931b3e011ac28c092895f27989161b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*58nP0KSufQ4_dkwTIrOZkw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图81d .具有隐藏层的神经网络上的反向传播</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rw"><img src="../Images/f3aab2331f1b5860b44c2d5b161b4cab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0O6SFzx1hdXl_qP-JhOMFg.png"/></div></div></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi rt"><img src="../Images/ab30ce88a41d5f593260122b01b889a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QGtqM_65E07QpuEXnlQvpw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图81e .带有隐藏层的神经网络的反向传播</figcaption></figure><p id="9b49" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">嚄😅！这是很多，但它大大提高了我们的理解。让我们执行梯度下降更新:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi na"><img src="../Images/56327f1f6c6108c105d6fe60d9fb5428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*ec6D8nyIroDBB5z1j1bGHA.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图82。隐层神经网络的梯度下降更新</figcaption></figure><p id="5d88" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这一点上，我鼓励所有读者自己执行一次训练迭代。产生的梯度应该约为(四舍五入到小数点后3位):</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ry"><img src="../Images/59890893771dc9fcb6881243a9b27f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*qqAO8ChVxGLDFCEZnjxGFw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图83。在2ⁿᵈ训练迭代期间计算的导数</figcaption></figure><p id="01cb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在5000个历元之后，成本稳步下降到大约<strong class="jy ja"> <em class="ku"> 0.0009 </em> </strong>，并且我们得到以下学习曲线和决策边界:</p><div class="kv kw kx ky gt ab cb"><figure class="pp kz rz pr ps pt pu paragraph-image"><img src="../Images/9f98e30c2e1ceb78bfe8a4cb3ec900ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*tYlofvncEmawwsYWvJ8IoQ.png"/></figure><figure class="pp kz sa pr ps pt pu paragraph-image"><img src="../Images/dceb9402d629175e0c8644f7a0b18999.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*AUCnhUT119zaVfwIxKLCUw.png"/><figcaption class="mv mw gj gh gi mx my bd b be z dk rm di rn py translated">图84。单隐层神经网络的学习曲线和决策边界</figcaption></figure></div><p id="8f47" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们也想象一下神经网络的决策从0(红色)变为1(绿色)的位置:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ro"><img src="../Images/7b039cb2f45916fbf0ef4adae023b4a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*J9Ik-s-GdtYCt6zxnAfOzw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图85。单隐层神经网络的阴影决策边界</figcaption></figure><p id="959f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这表明神经网络实际上已经学会了在哪里启动(输出1)和在哪里休眠(输出0)。</p><p id="a40a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果我们添加另一个可能有2或3个sigmoid神经元的隐藏层，我们可以得到一个更复杂的决策边界，可能会更紧密地适合我们的数据，但让我们把它留给编码部分。</p><p id="e77a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在结束本节之前，我想回答一些剩余的问题:</p><h2 id="9255" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">1-那么，特征工程和深度神经网络哪个更好呢？</h2><p id="18e9" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">嗯，答案取决于很多因素。一般来说，如果我们有大量的训练数据，我们可以使用深度神经网络来达到可接受的精度，但如果数据有限，我们可能需要执行一些特征工程，以从我们的神经网络中提取更多的性能。正如您在上面的特征工程示例中所看到的，要进行良好的特征交叉，您需要对他们正在处理的数据集有深入的了解。</p><p id="0085" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">特征工程与深度神经网络是一个强大的组合。</em></p><h2 id="a551" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">2-如何计算神经网络的层数？</h2><p id="a66e" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">按照惯例，如果没有可调权重和偏差，我们不会计算层数。因此，虽然输入层是一个单独的“层”,但在指定神经网络的深度时，我们不将其计算在内。</p><p id="be15" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">所以，我们的最后一个例子是一个“<em class="ku"> 2层神经网络</em>”(一个隐藏层+输出层)，而它之前的所有例子只是一个“<em class="ku"> 1层神经网络</em>”(仅输出层)。</p><h2 id="e128" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">3-为什么使用激活功能？</h2><p id="30e2" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated"><strong class="jy ja"> <em class="ku">激活函数是非线性函数，给神经元增加非线性。特征十字是隐藏层</em> </strong>中激活功能叠加的结果。因此，一组激活函数的组合导致复杂的非线性决策边界。在这篇博客中，我们使用了sigmoid/logistic激活函数，但是还有许多其他类型的激活函数(<em class="ku"> ReLU是隐藏层</em>的流行选择)，每一种都提供了一定的好处。<strong class="jy ja"> <em class="ku">在创建神经网络时，激活函数的选择也是一个超参数。</em>T9】</strong></p><p id="78f2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> <em class="ku">没有激活函数来增加非线性，无论我们叠加多少线性函数，它们的结果仍然是线性的。</em> </strong>考虑以下情况:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi sb"><img src="../Images/ba2fc0d7a9bd971ea59334e37cd3740f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*W8uSJ4wwZWc4c4CI8pSHRA.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图86。显示堆叠线性层/函数导致线性层/函数</figcaption></figure><p id="26ec" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你可以使用任何非线性函数作为激活函数。有些研究者甚至使用了<strong class="jy ja"> <em class="ku"> cos </em> </strong>和<strong class="jy ja"> <em class="ku"> </em> sin </strong>函数。优选地，激活函数应该是连续的，即在函数的域中没有间断。</p><h2 id="e157" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">4-为什么随机初始化权重？</h2><p id="efcd" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">这个问题现在回答起来容易多了。请注意，如果我们将一个层中的所有权重设置为相同的值，那么通过每个节点的梯度将是相同的。简而言之，该层中的所有节点将学习关于数据的相同特征。将权重设置为<strong class="jy ja"> <em class="ku">随机值有助于打破权重</em> </strong>的对称性，使得层中的每个节点都有机会学习训练数据的独特方面</p><p id="efcb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在神经网络中有许多随机设置权重的方法。对于小型神经网络，将权重设置为较小的随机值是可以的。对于较大的网络，我们倾向于使用“Xavier”或“He”初始化方法(<em class="ku">将在编码部分</em>)。这两种方法仍然将权重设置为随机值，但控制它们的方差。现在，可以说，当网络似乎没有收敛，并且当使用将权重设置为小的随机值的“简单”方法时，成本变得静态或降低非常缓慢时，使用这些方法就足够了。权重初始化是一个活跃的研究领域，将成为未来“除了数字什么都没有”博客的主题。</p><p id="5d4f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">偏差也可以随机初始化。但在实践中，它似乎对神经网络的性能没有太大的影响。也许这是因为神经网络中偏向项的数量比权重少得多。</p><p id="ee53" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们在这里创建的神经网络类型被称为“<strong class="jy ja"> <em class="ku">全连接前馈网络</em> </strong>”或简称为“<strong class="jy ja"> <em class="ku">前馈网络</em> </strong>”。</p><p id="000a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">第一部分到此结束。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><a href="https://www.buymeacoffee.com/rafaykhan"><div class="gh gi sc"><img src="../Images/bdb1aed53d63b0bff2c1599be3aa9dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*q8O4kAyuVD-9slT_aAKc1Q.png"/></div></a><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">如果你喜欢的话！</figcaption></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="7ee4" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">第二部分:模块化神经网络的编码</h1><p id="5122" class="pw-post-body-paragraph jw jx iq jy b jz mn kb kc kd mo kf kg kh mp kj kk kl mq kn ko kp mr kr ks kt ij bi translated">这一部分的实现遵循OOP原则。</p><p id="c2da" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们先来看一下<strong class="jy ja">线性层</strong>类。构造函数将以下参数作为参数:传入数据的形状(<code class="fe sd se sf sg b">input_shape</code>)、层输出的神经元数量(<code class="fe sd se sf sg b">n_out</code>)以及需要执行哪种类型的随机权重初始化(<code class="fe sd se sf sg b">ini_type=”plain”</code>，默认为“普通”，这只是小的随机高斯数)。</p><p id="34b7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><code class="fe sd se sf sg b">initialize_parameters</code>是用于定义权重和偏差的辅助函数。稍后，我们将分别讨论它。</p><p id="415d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">线性层实现以下功能:</p><ul class=""><li id="f402" class="qo qp iq jy b jz ka kd ke kh qq kl qr kp qs kt sh qu qv qw bi translated"><code class="fe sd se sf sg b">forward(A_prev)</code>:该功能允许线性层接收来自前一层的激活(输入数据可视为来自输入层的激活)，并对其执行线性操作。</li><li id="9bd4" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt sh qu qv qw bi translated"><code class="fe sd se sf sg b">backward(upstream_grad)</code>:该函数计算前一层的成本w.r.t权重、偏差和激活的导数(分别为<code class="fe sd se sf sg b">dW</code>、<code class="fe sd se sf sg b">db </code>、&amp;、<code class="fe sd se sf sg b">dA_prev</code>)</li><li id="04b6" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt sh qu qv qw bi translated"><code class="fe sd se sf sg b">update_params(learning_rate=0.1)</code>:该功能使用<code class="fe sd se sf sg b">backward</code>功能中计算的导数对权重和偏差执行梯度下降更新。默认学习率(α)是0.1</li></ul><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="si lb l"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图87。线性图层类</figcaption></figure><p id="28f4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在让我们来看一下<strong class="jy ja"> Sigmoid Layer </strong>类，它的构造函数将来自它前面的线性层的数据的形状作为参数。</p><p id="a087" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Sigmoid层实现以下功能:</p><ul class=""><li id="9605" class="qo qp iq jy b jz ka kd ke kh qq kl qr kp qs kt sh qu qv qw bi translated"><code class="fe sd se sf sg b">forward(Z)</code>:该功能允许sigmoid层接收来自前一层的线性计算(<code class="fe sd se sf sg b">Z</code>)并对其执行sigmoid激活。</li><li id="c135" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt sh qu qv qw bi translated"><code class="fe sd se sf sg b">backward(upstream_grad)</code>:此函数计算成本w . r . t<strong class="jy ja"><em class="ku">Z</em></strong>(<code class="fe sd se sf sg b">dZ</code>)的导数。</li></ul><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="si lb l"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图88。Sigmoid激活层类</figcaption></figure><p id="c2b5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><code class="fe sd se sf sg b">initialize_parameters</code>函数仅在线性层中用于设置权重和偏差。使用输入(<code class="fe sd se sf sg b">n_in</code>)和输出(<code class="fe sd se sf sg b">n_out</code>)的大小，定义权重矩阵和偏差向量需要的形状。然后，该辅助函数将python字典中的权重(W)和偏差(b)返回给相应的线性图层。</p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="si lb l"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图89。设置权重和偏差的辅助函数</figcaption></figure><p id="d062" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">最后，成本函数<code class="fe sd se sf sg b"> compute_cost(Y, Y_hat)</code>将来自最后一层的激活(<code class="fe sd se sf sg b">Y_hat</code>)和真实标签(<code class="fe sd se sf sg b">Y</code>)作为自变量，并计算和返回平方误差成本(<code class="fe sd se sf sg b">cost</code>)及其导数(<code class="fe sd se sf sg b">dY_hat</code>)。</p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="si lb l"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图90。计算平方误差成本和导数的函数</figcaption></figure><p id="6055" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">此时，您应该打开</em><a class="ae lj" href="https://github.com/RafayAK/NothingButNumPy/blob/master/Understanding_and_Creating_NNs/2_layer_toy_network_XOR.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="jy ja"><em class="ku">2 _ layer _ toy _ network _ XOR</em></strong></a><em class="ku">Jupyter笔记本，从这个</em> <a class="ae lj" href="https://github.com/RafayAK/NothingButNumPy/tree/master/Understanding_and_Creating_Binary_Classification_NNs" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja"> <em class="ku">资源库</em> </strong> </a> <em class="ku">中打开一个单独的窗口，并排浏览这个博客和笔记本。</em></p><p id="b8c0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们准备创建我们的神经网络。让我们使用<em class="ku">图77 </em>中为XOR数据定义的架构。</p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="si lb l"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图91。定义层和训练参数</figcaption></figure><p id="7b89" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在我们可以开始主要的训练循环了:</p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="si lb l"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">图92。训练循环</figcaption></figure><p id="f0bf" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在笔记本中运行循环，我们看到在4900个历元之后，成本降低到大约0.0009</p><pre class="kv kw kx ky gt sj sg sk sl aw sm bi"><span id="cd06" class="nk ll iq sg b gy sn so l sp sq">...<br/>Cost at epoch#4600: 0.001018305488651183<br/>Cost at epoch#4700: 0.000983783942124411<br/>Cost at epoch#4800: 0.0009514180100050973<br/>Cost at epoch#4900: 0.0009210166430616655</span></pre><p id="afff" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">学习曲线和决策界限如下所示:</p><div class="kv kw kx ky gt ab cb"><figure class="pp kz sr pr ps pt pu paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/0f636bc94f55c9612da04f608a396047.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*9rnHMOBVxRUKK7bEq5jC8g.png"/></div></figure><figure class="pp kz ss pr ps pt pu paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/71a825a6462c4a45dd31e5ff8ad4f66c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*-3Bnjcm4KQLQ9DG-llqQmw.png"/></div></figure><figure class="pp kz ss pr ps pt pu paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/f1b6f65929ae922801c746cf9cebac91.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*eFyWO1PU7miowGHJdtyhGQ.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk st di su py translated">图93。学习曲线、决策边界和阴影决策边界。</figcaption></figure></div><p id="b745" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们训练过的神经网络产生的预测是准确的。</p><pre class="kv kw kx ky gt sj sg sk sl aw sm bi"><span id="bd24" class="nk ll iq sg b gy sn so l sp sq">The predicted outputs:<br/> [[ 0.  1.  1.  0.]]<br/>The accuracy of the model is: 100.0%</span></pre><p id="1a7c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">请务必查看<a class="ae lj" href="https://github.com/RafayAK/NothingButNumPy" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja"> <em class="ku">资源库</em> </strong> </a>中的其他笔记本。我们将在未来的“除了NumPy之外什么都不是”博客中建立我们在这个博客中学到的东西，因此，作为一个练习，你应该从记忆中创建层类，并尝试重现<strong class="jy ja"> <em class="ku">部分</em></strong><em class="ku"/><strong class="jy ja"><em class="ku">ⅰ</em></strong><em class="ku">中的OR门示例。</em></p><p id="040b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">博客到此结束🙌🎉。我希望你喜欢。</p><p id="4251" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如有任何问题，请随时通过<a class="ae lj" href="https://twitter.com/RafayAK" rel="noopener ugc nofollow" target="_blank"> twitter </a> <a class="ae lj" href="https://twitter.com/RafayAK" rel="noopener ugc nofollow" target="_blank"> @RafayAK </a>联系我</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><a href="https://www.buymeacoffee.com/rafaykhan"><div class="gh gi sc"><img src="../Images/bdb1aed53d63b0bff2c1599be3aa9dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*q8O4kAyuVD-9slT_aAKc1Q.png"/></div></a><figcaption class="mv mw gj gh gi mx my bd b be z dk translated">如果你喜欢的话！</figcaption></figure></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h2 id="9a05" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">准备好了吗？查看本系列的下一篇博客:</h2><ol class=""><li id="f01e" class="qo qp iq jy b jz mn kd mo kh sv kl sw kp sx kt qt qu qv qw bi translated">从零开始理解和创建具有计算图形的神经网络(<em class="ku">当前</em>)</li><li id="8c5b" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt qt qu qv qw bi translated"><a class="ae lj" href="https://towardsdatascience.com/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c" rel="noopener" target="_blank">理解&amp;创建<em class="ku">二元分类</em>从零开始使用计算图的神经网络</a></li></ol></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h2 id="fc15" class="nk ll iq bd lm nl nm dn lq nn no dp lu kh np nq ly kl nr ns mc kp nt nu mg iw bi translated">如果没有以下资源和人员，这个博客是不可能的:</h2><ul class=""><li id="0db5" class="qo qp iq jy b jz mn kd mo kh sv kl sw kp sx kt sh qu qv qw bi translated">安德烈·卡帕西(<a class="ae lj" href="https://twitter.com/karpathy" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja"> @ </strong>卡帕西</a>)斯坦福<a class="ae lj" href="http://cs231n.stanford.edu" rel="noopener ugc nofollow" target="_blank">课程</a></li><li id="1e59" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt sh qu qv qw bi translated">克里斯托弗·奥拉赫(<a class="ae lj" href="https://twitter.com/ch402" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja"> @ </strong> ch402 </a> ) <a class="ae lj" href="https://colah.github.io/" rel="noopener ugc nofollow" target="_blank">博客</a>年代</li><li id="0d4f" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt sh qu qv qw bi translated">安德鲁·特拉斯克(<a class="ae lj" href="https://twitter.com/iamtrask" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja"> @ </strong>艾姆特拉斯克</a> ) <a class="ae lj" href="https://iamtrask.github.io/" rel="noopener ugc nofollow" target="_blank">博客</a></li><li id="9bdf" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt sh qu qv qw bi translated">吴恩达(<a class="ae lj" href="http://twitter.com/AndrewYNg" rel="noopener ugc nofollow" target="_blank"> @AndrewYNg </a>)和他的Coursera关于<a class="ae lj" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>和<a class="ae lj" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>的课程</li><li id="9aec" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt sh qu qv qw bi translated"><a class="ae lj" href="http://parrt.cs.usfca.edu/" rel="noopener ugc nofollow" target="_blank">特伦斯·帕尔</a>(<a class="ae lj" href="https://twitter.com/the_antlr_guy" rel="noopener ugc nofollow" target="_blank"><strong class="jy ja">@</strong>the _ antlr _ guy</a>)和<a class="ae lj" href="http://www.fast.ai/about/#jeremy" rel="noopener ugc nofollow" target="_blank">杰瑞米·霍华德</a> ( <a class="ae lj" href="https://twitter.com/jeremyphoward" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja"> @ </strong>杰里米·沃德</a>)(<a class="ae lj" href="https://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">https://explained.ai/matrix-calculus/index.html</a>)</li><li id="62c0" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt sh qu qv qw bi translated">伊恩·古德费勒(<a class="ae lj" href="https://twitter.com/goodfellow_ian" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja"> @ </strong>古德费勒_伊恩</a>)和他那本令人惊叹的<a class="ae lj" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">书</a></li><li id="ca6f" class="qo qp iq jy b jz qx kd qy kh qz kl ra kp rb kt sh qu qv qw bi translated">最后，感谢Hassan-uz-Zaman(<a class="ae lj" href="https://twitter.com/OKidAmnesiac" rel="noopener ugc nofollow" target="_blank"><strong class="jy ja">@</strong>OKidAmnesiac</a>)和Hassan Tauqeer的宝贵反馈。</li></ul></div></div>    
</body>
</html>