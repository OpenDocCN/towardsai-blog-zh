<html>
<head>
<title>Predicting Name Gender: From Notebook to Production</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测姓名性别:从笔记本到生产</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/predicting-name-gender-from-notebook-to-production-99e51d2aabd7?source=collection_archive---------2-----------------------#2021-10-18">https://pub.towardsai.net/predicting-name-gender-from-notebook-to-production-99e51d2aabd7?source=collection_archive---------2-----------------------#2021-10-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="78c4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="3797" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">基于计数向量机和逻辑回归的文本分类探索</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f4de3e33e975bb471ae9417c0503c466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*emalaybb8XnhLL_O"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@dainisgraveris?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Dainis Graveris </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><blockquote class="li lj lk"><p id="c474" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">免责声明——我确实意识到性别是可变的，名字是一个微弱的性别预测。Quis中的功能是出于科学好奇心而创建的，而本文是出于教育目的。欢迎所有反馈:)</p></blockquote></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="405d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">我开发了<a class="ae lh" href="https://quis.co" rel="noopener ugc nofollow" target="_blank"> Quis </a>，这是一个联系人应用程序，可以按时间顺序对联系人进行排序，并在地图上看到他们，还有其他实验功能。其中一个功能是<strong class="lo jd">按性别过滤联系人的能力</strong>，即使联系人数据库中没有该信息。</p><p id="9a74" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">这个过滤器是由一个<strong class="lo jd">轻量级监督机器学习模型实现的，更准确地说，是一个结合了逻辑回归的计数矢量器，它对来自世界各地的数十万个名字进行了训练，嵌入在应用</strong>中。</p><p id="d44c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">话真多！本文将分解整个过程，从在Jupyter笔记本中训练模型到将其部署到生产中。整个代码很短，在这里可以找到<a class="ae lh" href="https://gist.github.com/joachimvalente/b0b98375e53c0237a11edfa040fa358b" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/93671e404edd6e19ac2e5d4e6675d71c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*ttIvADx3EwRjlQSveNvSkQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">本文中描述的带有性别过滤器的联系人应用Quis的屏幕截图。你自己试试吧！在<a class="ae lh" href="https://play.google.com/store/apps/details?id=co.quis.quis" rel="noopener ugc nofollow" target="_blank">安卓</a>或<a class="ae lh" href="https://apps.apple.com/us/app/quis-contacts/id1558252000" rel="noopener ugc nofollow" target="_blank"> iOS </a>上免费下载。</figcaption></figure><h1 id="f61c" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">先决条件</h1><p id="a932" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">如果你不熟悉<a class="ae lh" href="https://realpython.com/jupyter-notebook-introduction/" rel="noopener ugc nofollow" target="_blank"> Python/Jupyter笔记本</a>、<a class="ae lh" href="https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/" rel="noopener ugc nofollow" target="_blank">熊猫</a>和<a class="ae lh" href="https://scikit-learn.org/stable/tutorial/basic/tutorial.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>，先看看那些介绍吧。</p><h1 id="b522" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">问题是</h1><p id="9b8d" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">世界上有数百万个名字，你无法像智能手机那样将它们全部存储在资源有限的环境中——假设100万个名字平均包含10个字符，则需要1，000，000 x (10 + 1)字节≈ 10 MB。此外，你可能会偶然发现从未见过的名字，比如说<a class="ae lh" href="https://www.google.com/search?q=khaleesi" rel="noopener ugc nofollow" target="_blank">卡丽熙</a>或<a class="ae lh" href="https://www.google.com/search?q=anakin" rel="noopener ugc nofollow" target="_blank">阿纳金</a>，并且仍然想预测他们的性别。</p><p id="0e60" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">解决方案是训练一个模型，它学习<em class="ln">长什么样</em>女性和男性的名字。</p><h1 id="cc87" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">机器学习过程</h1><p id="63f0" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">一个有监督的机器学习项目通常由5个步骤组成。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/1d2847a56815699645d182ff2c51012c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cGnWtNRII0p4P-Q5gCXBMw.png"/></div></div></figure><h1 id="58ad" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">步骤1 —收集培训数据</h1><p id="7030" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">第一步也是最关键的一步是收集训练数据。</p><p id="bf66" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">在姓名性别的情况下，网络上有多个免费的数据集，例如从1930年到2015年美国的95，000个名字的<a class="ae lh" href="https://data.world/howarder/gender-by-name" rel="noopener ugc nofollow" target="_blank">这个数据集</a>，或者从182个国家的177，000个名字的<a class="ae lh" href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YPRQH8" rel="noopener ugc nofollow" target="_blank">这个</a> [0】，我们将在这里使用。所以继续下载名为<code class="fe nr ns nt nu b">wgnd_noctry.tab</code>的CSV文件。</p><h2 id="222b" class="nv mu it bd mv nw nx dn mz ny nz dp nd mp oa ob nf mq oc od nh mr oe of nj iz bi translated">探索数据</h2><p id="0f98" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">创建新的Jupyter笔记本。让我们加载数据并预览其内容:</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="128d" class="nv mu it nu b gy ok ol l om on"># import pandas<br/><strong class="nu jd">import pandas as pd</strong></span><span id="9bb1" class="nv mu it nu b gy oo ol l om on"># read CSV into a pandas dataframe<strong class="nu jd"><br/>df = pd.read_csv('wgnd_noctry.csv')</strong></span><span id="62e6" class="nv mu it nu b gy oo ol l om on"># print shape (number of rows and columns)<strong class="nu jd"><br/>df.shape</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/2c6d5d6b7680214fd711dd57b7973ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vq9ck8Jig8rHsDZzK2fG0w.png"/></div></div></figure><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="bb0f" class="nv mu it nu b gy ok ol l om on"># display 5 random rows<br/><strong class="nu jd">df.sample(5)</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/fc06d09896869f46e97e48c4b1df12f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XI1M7FtWjEBXD7QIiMAuDg.png"/></div></div></figure><h2 id="bcb0" class="nv mu it bd mv nw nx dn mz ny nz dp nd mp oa ob nf mq oc od nh mr oe of nj iz bi translated">预处理</h2><p id="2a40" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">我们的目标是预测给定<code class="fe nr ns nt nu b">name</code>(输入<code class="fe nr ns nt nu b">X</code>)的<code class="fe nr ns nt nu b">gender</code>(输出或<strong class="lo jd">标签</strong> <code class="fe nr ns nt nu b">y</code>)。此外，我们需要一些预处理来清理原始数据。</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="e7d0" class="nv mu it nu b gy ok ol l om on"># exclude rows where gender is undefined<br/><strong class="nu jd">df = df[df.gender.isin(['M', 'F'])]</strong></span><span id="14ef" class="nv mu it nu b gy oo ol l om on"># define input<br/><strong class="nu jd">X = df.name</strong></span><span id="b176" class="nv mu it nu b gy oo ol l om on"># define labels (feminine = 0, masculine = 1)<br/><strong class="nu jd">y = df.gender == 'M'</strong></span></pre><h2 id="201f" class="nv mu it bd mv nw nx dn mz ny nz dp nd mp oa ob nf mq oc od nh mr oe of nj iz bi translated">分割训练和测试数据</h2><p id="d837" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">在有监督的机器学习问题中，我们总是留出一部分(通常在10%到20%之间)数据集进行测试——这允许我们计算模型的准确性，并验证我们的模型是否如预期的那样工作。Scikit-Learn为此提供了便利的功能。</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="ba80" class="nv mu it nu b gy ok ol l om on"># relevant sklearn import<br/><strong class="nu jd">from sklearn.model_selection import train_test_split</strong></span><span id="d920" class="nv mu it nu b gy oo ol l om on"># set aside 10% of the dataset for test<strong class="nu jd"><br/>X_train, X_test, y_train, y_test = \<br/>    train_test_split(X, y, test_size=0.1)</strong></span><span id="813b" class="nv mu it nu b gy oo ol l om on"># inspect resulting shapes<br/><strong class="nu jd">X_train.shape, y_train.shape, X_test.shape, y_test.shape</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/aee6eb8224ee607bb77eb96c856bb458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFjsroI_cAtLbUfx4trW4Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">(n，)表示大小为n的向量</figcaption></figure><h1 id="c300" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">步骤2-生成要素</h1><p id="4dbd" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">收集数据后，下一步是构建用作模型输入的要素。在这种情况下，许多特征可能会浮现在脑海中:字母的数量、元音的数量、姓名是以<code class="fe nr ns nt nu b">a</code>还是<code class="fe nr ns nt nu b">o</code>结尾(对西班牙语或意大利语姓名有用)、是否以辅音结尾，等等。</p><p id="dc60" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">但我们将自动构建数以千计的功能，而不是手工设计。怎么会？使用一种叫做<strong class="lo jd">计数矢量器</strong>的东西。计数矢量器是一种从语料库中自动构建词汇和特征的方法。更准确地说，一个(<em class="ln">p</em>–<em class="ln">q</em>)-char count矢量器通过收集出现在语料库中的<strong class="lo jd"> n-grams </strong>生成特征，其中<em class="ln"> p ≤ n ≤ q </em>，换句话说就是大小在<em class="ln"> p </em>和<em class="ln">q</em>T29】之间的<strong class="lo jd">子串。</strong></p><p id="2e60" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">我们来举一个具体的例子。假设我们有五个名字:克里斯、克里斯蒂娜、马里乌斯、玛尔塔和马提亚斯。然后,( 2–4)字符计数矢量器将学习以下词汇。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/9516bd3e08d3f9859e0c8cc6dd97296a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WWiox2YT5TFmdzNhxpi8mQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">词汇表是在训练示例中找到的二元模型、三元模型和四元模型的集合。</figcaption></figure><p id="6898" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">有了这个词汇表，计数矢量器可以将任何名字(无论是否在语料库中)转换成一个大型稀疏矢量<code class="fe nr ns nt nu b">v</code>，其中<code class="fe nr ns nt nu b">v(i)</code>是单词<code class="fe nr ns nt nu b">i</code>在名字中出现的次数。例如，姓名Rita和Laetitia将被转换成大小为55的向量，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/0ff0cfe017461794e436aed21c7d59a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XyFuom2J7fV6xp7x-YI6Zg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">“Tina”这个名字包含三个二元模型、两个三元模型和一个四元模型。</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/b57c28768ddc81e6351646a4f77f39b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NT_9dUcdVkN17EcRY1PaAg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">“Laetitia”这个名字包含了两次双字母“ti”。它包括我们的词汇中没有的其他n-grams，如“lae”。</figcaption></figure><p id="33c9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">这个<em class="ln">整个</em>过程可以使用Scikit-Learn在3行中完成。</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="d485" class="nv mu it nu b gy ok ol l om on"># relevant sklearn import<br/><strong class="nu jd">from sklearn.feature_extraction.text import CountVectorizer</strong></span><span id="320b" class="nv mu it nu b gy oo ol l om on"># configure our count vectorizer<strong class="nu jd"><br/>vec = CountVectorizer(<br/>    min_df=50,<br/>    ngram_range=(2, 5),<br/>    analyzer='char',<br/>    preprocessor=lambda x: f'^{x.lower().replace(" ", "")}$',<br/>)</strong></span><span id="d8c1" class="nv mu it nu b gy oo ol l om on"># train it<strong class="nu jd"><br/>vec.fit(X_train)</strong></span></pre><p id="68cd" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">您会注意到几个参数:</p><ul class=""><li id="cc42" class="ou ov it lo b lp lq ls lt mp ow mq ox mr oy mh oz pa pb pc bi translated"><code class="fe nr ns nt nu b">min_df</code>是语料库中作为词汇表一部分的单词的最小出现次数。(例如，如果<code class="fe nr ns nt nu b">khal</code>在卡丽熙这个名字中只出现过一次，我们就将它丢弃)。<code class="fe nr ns nt nu b">min_df</code>有助于<a class="ae lh" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过度拟合</a>，并减少我们的词汇和模型的大小。</li><li id="0457" class="ou ov it lo b lp pd ls pe mp pf mq pg mr ph mh oz pa pb pc bi translated"><code class="fe nr ns nt nu b">ngram_range</code>是令牌中的最小和最大字符数。</li><li id="530c" class="ou ov it lo b lp pd ls pe mp pf mq pg mr ph mh oz pa pb pc bi translated"><code class="fe nr ns nt nu b">analyzer='char'</code> <strong class="lo jd"> </strong>表示我们使用字符级标记化，而不是单词级标记化。</li><li id="b4b0" class="ou ov it lo b lp pd ls pe mp pf mq pg mr ph mh oz pa pb pc bi translated"><code class="fe nr ns nt nu b">preprocessor</code>是在构建词汇表之前应用于每个名称的函数。这里我们添加了一个前导<code class="fe nr ns nt nu b">^</code>和一个尾随<code class="fe nr ns nt nu b">$</code>，这是表示开始和结束的常见做法。这很重要，因为例如，名字末尾的字母<code class="fe nr ns nt nu b">a</code>与中间的字母<code class="fe nr ns nt nu b">a</code>非常不同。</li></ul><p id="634e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">让我们来探讨一下所学的词汇。提示:调用<code class="fe nr ns nt nu b">.fit()</code>后由Sckit-Learn估计器学习的参数总是以下划线为后缀。</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="b31a" class="nv mu it nu b gy ok ol l om on">len(vec.vocabulary_)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/3ccb5007f161e63fefb297805a54b74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_eMRakNy9j9LrYpHFa1-Q.png"/></div></div></figure><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="3a95" class="nv mu it nu b gy ok ol l om on">sorted(list(vec.vocabulary_))[:20]</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/b0399c5ba4cfb8eb1690f00a89836de6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p9Mlfpq87W1Q9E5K8zpnig.png"/></div></div></figure><h1 id="5dc2" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">步骤3 —模型架构</h1><p id="e746" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">由于我们的计数矢量器，我们现在可以将训练集中的150，000个姓名转换为8414个数字特征。</p><p id="5afa" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">我们需要训练一个模型，将这8414个数字转换成一个预测，该预测表示一个名字是阴性(<em class="ln"> p </em> = 0)还是阳性(<em class="ln"> p </em> = 1)。有许多架构可供选择:逻辑回归、<a class="ae lh" href="https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775" rel="noopener" target="_blank">决策树</a>、神经网络、支持向量机等等。然而，因为我们将在步骤4中序列化模型并将其嵌入到应用程序中，所以我们希望它尽可能地轻量级，并将使用<strong class="lo jd">逻辑回归</strong>。</p><p id="08a4" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">逻辑回归模型的形式是<code class="fe nr ns nt nu b">f(x1, ... xn) = sigmoid(k1*x1 + k2*x2 + ... + kn*xn + b)</code>，其中<code class="fe nr ns nt nu b"><a class="ae lh" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">sigmoid</a>(x) = 1 / (1 + exp(-x))</code>总是在0和1之间，可以解释为概率。该模型通过一个称为<a class="ae lh" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降(或SGD) </a>的迭代过程，通过逐渐减小其对输入给它的训练样本的预测误差，来学习其<strong class="lo jd">参数</strong> <code class="fe nr ns nt nu b">k1</code>、<code class="fe nr ns nt nu b">k2</code>、…、<code class="fe nr ns nt nu b">kn</code>(称为<strong class="lo jd">权重</strong>)和<code class="fe nr ns nt nu b">b</code>(称为<strong class="lo jd">偏差</strong>)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/996fac92847c1770fad3b4aca70ea870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*UYuzDaWwcHsqvipc2eEnqQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">逻辑回归模型将向量(例如，计数矢量器的输出)映射到概率。</figcaption></figure><p id="5ddf" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">为了避免两步过程——首先训练计数矢量器，然后是逻辑回归——我们可以使用Scikit-Learn <strong class="lo jd">管道</strong>。管道将连续的步骤包装到一个模型中，这样您只需调用<code class="fe nr ns nt nu b">.fit()</code>一次。</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="d2d4" class="nv mu it nu b gy ok ol l om on"># relevant sklearn imports<br/><strong class="nu jd">from sklearn.linear_model import LogisticRegression<br/>from sklearn.pipeline import Pipeline</strong></span><span id="a9e6" class="nv mu it nu b gy oo ol l om on"># same as before<strong class="nu jd"><br/>vec = CountVectorizer(...)</strong></span><span id="c573" class="nv mu it nu b gy oo ol l om on"># logistic regression with default parameters but more iterations<br/># for the SGD to converge<strong class="nu jd"><br/>clf = LogisticRegression(max_iter=500)</strong></span><span id="2565" class="nv mu it nu b gy oo ol l om on"># a pipeline to wrap sequential named steps<strong class="nu jd"><br/>pipeline = Pipeline([('vectorizer', vec), ('logreg', clf)])</strong></span><span id="2d29" class="nv mu it nu b gy oo ol l om on"># build the vocabulary and train the model in a single command<strong class="nu jd"><br/>pipeline.fit(X_train, y_train)</strong></span></pre><p id="41ac" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">我们现在可以预测性别了！</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="2f82" class="nv mu it nu b gy ok ol l om on"># some masculine names<strong class="nu jd"><br/>pipeline.predict([<br/>    'jeff', 'elon', 'bernard', 'bill', 'mark', 'warren', 'larry',<br/>    'sergey', 'mukesh', 'amancio', 'zhong', 'steve', 'ma', 'carlos',<br/>])</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/b39378c00cfd62c73ca6b1fc974ff5f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MIrD5PfaJcGHrCa_G-BArA.png"/></div></div></figure><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="870a" class="nv mu it nu b gy ok ol l om on"># some feminine names<strong class="nu jd"><br/>pipeline.predict([<br/>    'francoise', 'alice', 'mackenzie', 'julia', 'miriam',<br/>    'jacqueline', 'yang', 'susanne', 'gina', 'iris',<br/>])</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/175a1c1625a2c3dbe81e5015f11c1254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wnMO5YnrVIVI4Ej4T-5GOg.png"/></div></div></figure><p id="3399" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">它似乎干得不错。你可以注意到，对于用拉丁字符(<code class="fe nr ns nt nu b">Ma</code>和<code class="fe nr ns nt nu b">Yang</code>)转录的中文名字来说，这并不是很好，但<a class="ae lh" href="https://qr.ae/pGS44W" rel="noopener ugc nofollow" target="_blank">是预期的</a>。</p><p id="816c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">我们也可以计算原始概率来获得一种信心。下面的例子表明，它勉强把<code class="fe nr ns nt nu b">Françoise</code>做对了，但是对<code class="fe nr ns nt nu b">Gina</code>作为一个女性化的名字很有信心。</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="27cc" class="nv mu it nu b gy ok ol l om on"># get raw probabilities instead<strong class="nu jd"><br/>pipeline.predict_proba([<br/>    'francoise', 'alice', 'mackenzie', 'julia', 'miriam',<br/>    'jacquline', 'yang', 'susanne', 'gina', 'iris',<br/>])</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/0c0fa1d83622740ae1898540dc2243a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GkjJvOD4YdRpdhw2yl9qxg.png"/></div></div></figure><p id="7026" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated"><em class="ln">可选。</em>我们可以进一步探索，看看模型认为哪些n-grams是典型的阳性和阴性。</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="eac7" class="nv mu it nu b gy ok ol l om on"># <em class="ln">pipeline.named_steps.vectorizer.vocabulary_</em> is the learned<br/># vocabulary, mapping n-gram to its index in the vocabulary, but we<br/># need the inverse<br/><strong class="nu jd">inverted_index = {<br/>    v: k<br/>    for k, v in pipeline.named_steps.vectorizer.vocabulary_.items()<br/>}</strong></span><span id="632f" class="nv mu it nu b gy oo ol l om on"># <em class="ln">pipeline.named_steps.logreg.coef_</em> are the weights (called<br/># k1 ... k8414 previously) learned by the logistic regression<br/><strong class="nu jd">sorted_ngrams = sorted(<br/>    enumerate(pipeline.named_steps.logreg.coef_[0]),<br/>    key=lambda x: x[1]<br/>)</strong></span><span id="2a5b" class="nv mu it nu b gy oo ol l om on"># large negative parameters indicate typically feminine n-grams<br/><strong class="nu jd">[inverted_index[ngram[0]] for ngram in sorted_ngrams[:10]]</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/82e7df7c0b946ee2c890062a27718e57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtZKHZgTppxEkjVYZpDLjw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">我们可以认出<em class="pm"> -nice </em>，一个表示胜利的女性后缀，后缀-a可能是世界上最常见的女性后缀，前缀Nya-常见于斯瓦希里语名字中。</figcaption></figure><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="1dcc" class="nv mu it nu b gy ok ol l om on"># large positive parameters indicate typically masculine n-grams<br/><strong class="nu jd">[inverted_index[ngram[0]] for ngram in sorted_ngrams[-10:]]</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/9e55e1b5494968ee81536c6cd8ab7a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUrTSdji5CasO67dW3p8Fw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">我们可以认出<em class="pm"> Abd- </em>，它是许多男性阿拉伯名字的前缀，日语后缀-ichi，以及-io和-lino，它们是意大利语和西班牙语中常见的男性后缀。</figcaption></figure><p id="05da" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">最后，我们可以评估模型在测试集上<strong class="lo jd">有多准确</strong>；换句话说，它预测正确性别的频率。</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="d2b2" class="nv mu it nu b gy ok ol l om on"># relevant sklearn import<br/><strong class="nu jd">from sklearn.metrics import accuracy_score</strong></span><span id="211d" class="nv mu it nu b gy oo ol l om on"># predict gender of test names<br/><strong class="nu jd">y_pred = pipeline.predict(X_test)</strong></span><span id="669a" class="nv mu it nu b gy oo ol l om on"># compute accuracy<br/><strong class="nu jd">accuracy_score(y_test, y_pred)</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/78ea07aa26386c5e2d8075b21c40731a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*svna-A0kdQv0mp-KMQu-zg.png"/></div></div></figure><p id="c3e2" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">87%的准确率——对于我们的第一次尝试来说还不错，但肯定还可以提高。</p><h1 id="dd49" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">步骤4 —部署</h1><p id="06c7" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">许多书籍和在线教程都没有涉及到这一步，然而这却是至关重要的一步。一旦你在笔记本上训练了一个模型，你如何在生产中使用它？</p><p id="f417" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">最简单的方法是使用<code class="fe nr ns nt nu b">joblib</code>(对Python的<code class="fe nr ns nt nu b">pickle</code>的改进)序列化您的模型:</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="7522" class="nv mu it nu b gy ok ol l om on"># in your notebook<br/><strong class="nu jd">from joblib import dump<br/>dump(pipeline, 'model.joblib')</strong></span><span id="e6fe" class="nv mu it nu b gy oo ol l om on"># in prod<br/><strong class="nu jd">from joblib import load<br/>pipeline = load('model.joblib')</strong></span></pre><p id="e2e5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">然而，它有两个明显的缺点:如果您的笔记本和prod中的Scikit-Learn版本不同，它可能会中断，并且它假设您的生产环境使用Python。</p><p id="49aa" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">像<a class="ae lh" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>这样更精细的解决方案已经被开发出来，但是因为我们有意保持我们的模型简单，我们将做一些不同的事情:<strong class="lo jd">手动序列化它</strong>。我们需要保存参数——模型学习到的东西，如词汇和逻辑回归权重和偏差——并编写我们自己版本的计数矢量器和逻辑回归。现在，您应该能够做到所有这些了！</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="2ce1" class="nv mu it nu b gy ok ol l om on"># vocabulary, found in <em class="ln">pipeline.named_steps.vectorizer.vocabulary_<br/></em><strong class="nu jd">_VOCABULARY = {<br/>    ^j': 514,<br/>    'ji': 4676,<br/>    'il': 4255,<br/>    'll': 5228,<br/>    'li': 5126,<br/>    'is': 4450,<br/>    's$': 7010,<br/>    '^ji': 576,<br/>    ...<br/>}</strong></span><span id="7761" class="nv mu it nu b gy oo ol l om on"># logistic regression weights (k1 ... k8414) and bias, found in<br/># <em class="ln">pipeline.named_steps.logreg.coef_</em> and<em class="ln"><br/></em># <em class="ln">pipeline.named_steps.logreg.intercept_<br/></em><strong class="nu jd">_WEIGHTS = [<br/>    0.16934584,<br/>    -0.04714591,<br/>    -0.48276778,<br/>    ...,<br/>    0.17204447,<br/>    -0.00234139,<br/>    -0.35968848,<br/>]</strong><br/><strong class="nu jd">_BIAS = 0.19512928</strong></span><span id="010f" class="nv mu it nu b gy oo ol l om on"># a count vectorizer is nothing more than an n-gram counter<br/><strong class="nu jd">def count_ngrams(name):<br/>    num_ngrams = len(_VOCABULARY)<br/>    vector = [0] * num_ngrams<br/>    normalized_name = f'^{name.lower().replace(" ", "")}$'<br/>    name_length = len(normalized_name)<br/>    for n in range(2, 6):<br/>        for i in range(name_length - n + 1):<br/>            ngram = normalized_name[i : i + n]<br/>            if ngram in _VOCABULARY:<br/>                vector[_VOCABULARY[ngram]] += 1<br/>    return vector</strong></span><span id="10d0" class="nv mu it nu b gy oo ol l om on"># a logistic regression is nothing more than a few additions and<br/># multiplications feeding into a sigmoid<br/><strong class="nu jd">from math import exp</strong></span><span id="fbec" class="nv mu it nu b gy oo ol l om on"><strong class="nu jd">def logreg(vector):<br/>    num_ngrams = len(_VOCABULARY)<br/>    sigmoid_input = _BIAS + sum(<br/>        _WEIGHTS[i] * vector[i] for i in range(num_ngrams)<br/>    )<br/>    return 1.0 / (1.0 + exp(-sigmoid_input))</strong></span><span id="3a43" class="nv mu it nu b gy oo ol l om on"># our model!<br/><strong class="nu jd">def model_predict(name):<br/>    return logreg(count_ngrams(name))</strong></span><span id="ff94" class="nv mu it nu b gy oo ol l om on"># sanity check<br/><strong class="nu jd">expected = pipeline.predict_proba(['elizabeth'])[0][1]<br/>actual = model_predict('elizabeth')<br/>print(actual == expected)</strong></span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ca"><img src="../Images/b9b3a887f8fe522c427e870e6a00f00c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p6jUXTB4YSb4R4P95ZuxsQ.png"/></div></div></figure><p id="7f43" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">当然，在实践中，您可以将这些代码翻译成编写服务器的语言。</p><p id="7051" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">这样的模型需要多大的空间？</p><pre class="ks kt ku kv gt og nu oh oi aw oj bi"><span id="e15c" class="nv mu it nu b gy ok ol l om on">space ≈ sizeof(_VOCABULARY) + sizeof(_WEIGHTS)<br/>      ≈ 8414 * (<br/>           average_length(ngram) * sizeof(char) + sizeof(double)<br/>           + sizeof(double)<br/>        )<br/>      ≈ 8414 * (3.5 * 1 + 4 + 4)<br/>      ≈ 100 kB</span></pre><p id="2d3c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mp lw lx ly mq ma mb mc mr me mf mg mh im bi translated">这比存储一个详尽的列表节省了<strong class="lo jd">100倍的空间</strong>,非常适合嵌入到智能手机应用程序中。</p><h1 id="a9bf" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">步骤5 —监控和迭代</h1><p id="12c3" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">在将模型部署到生产环境中之后，应该对其进行监控，以衡量其实际性能。不幸的是，我们的模型并不完美，需要迭代才能变得更好。这里有几个问题和改进的思路。</p><ul class=""><li id="a5eb" class="ou ov it lo b lp lq ls lt mp ow mq ox mr oy mh oz pa pb pc bi translated">87%的准确率无疑可以通过玩模型架构来提高。例如，我们可以尝试用另一种分类器替代逻辑回归，或者使用计数矢量器<a class="ae lh" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">超参数</a>。(如果这样做，记得使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Validation_data_set" rel="noopener ugc nofollow" target="_blank">验证集</a>。)</li><li id="266e" class="ou ov it lo b lp pd ls pe mp pf mq pg mr ph mh oz pa pb pc bi translated">有些名字在一些国家是阳性的，而在另一些国家是阴性的(比如<code class="fe nr ns nt nu b">Alexis</code>或<code class="fe nr ns nt nu b">Andrea</code>，在美国通常是阴性的，而在法国和意大利分别是阳性的)。您可以将国家作为一个特征添加到模型中。或者，你可以在每个国家培训<strong class="lo jd">一个模特。</strong></li><li id="16b0" class="ou ov it lo b lp pd ls pe mp pf mq pg mr ph mh oz pa pb pc bi translated">词汇表的大小可能仍然太大，这是模型大小的一个问题，可能会引入<a class="ae lh" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过度拟合</a>。一种解决方案是增加<strong class="lo jd">正则化</strong>，例如，通过增加<code class="fe nr ns nt nu b">min_df</code>或减少<code class="fe nr ns nt nu b">ngram_range</code>。</li><li id="3a0e" class="ou ov it lo b lp pd ls pe mp pf mq pg mr ph mh oz pa pb pc bi translated">当前的数据集包含数十万个名称，无论其受欢迎程度如何，这些名称都被同等对待。然而，事实上，通用名的错误比不常用名的错误更糟糕，因为它们会影响更多的用户。可能的解决方法包括从训练集中删除最不常用的名字，并使用名字流行度作为<strong class="lo jd">样本权重</strong>。</li><li id="ae83" class="ou ov it lo b lp pd ls pe mp pf mq pg mr ph mh oz pa pb pc bi translated">像<code class="fe nr ns nt nu b">Alex</code>和<code class="fe nr ns nt nu b">Leslie</code>这样的名字男女通用。我们可以为中性或无性别的名字引入第三个标签类别<strong class="lo jd">。</strong></li></ul></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="d946" class="mt mu it bd mv mw pp my mz na pq nc nd ki pr kj nf kl ps km nh ko pt kp nj nk bi translated">整个代码</h1><p id="4e1a" class="pw-post-body-paragraph ll lm it lo b lp nl kd lr ls nm kg lu mp nn lx ly mq no mb mc mr np mf mg mh im bi translated">在<a class="ae lh" href="https://gist.github.com/joachimvalente/b0b98375e53c0237a11edfa040fa358b" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到笔记本。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pu pv l"/></div></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="95f9" class="mt mu it bd mv mw pp my mz na pq nc nd ki pr kj nf kl ps km nh ko pt kp nj nk bi translated">参考</h1><blockquote class="li lj lk"><p id="3019" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">[0]拉福，胡里奥；Lax-Martinez，Gema，2018，《WGND 1.0》，<a class="ae lh" href="https://doi.org/10.7910/DVN/YPRQH8" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.7910/DVN/YPRQH8</a>，哈佛数据世界，V1，UNF:6:bu iqbd/6 H9 bup 1 zi 2 pnwa = =[file UNF]</p></blockquote></div></div>    
</body>
</html>