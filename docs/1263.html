<html>
<head>
<title>Let’s Compress the CNN Training Like a JPEG Compression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们像JPEG压缩一样压缩CNN训练</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/lets-compress-the-cnn-training-like-a-jpeg-compression-ca8237c56f3c?source=collection_archive---------3-----------------------#2020-12-15">https://pub.towardsai.net/lets-compress-the-cnn-training-like-a-jpeg-compression-ca8237c56f3c?source=collection_archive---------3-----------------------#2020-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c5ae" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div class="gh gi jw"><img src="../Images/2f7fef11c54fb5006897663751c1a4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2SJ2d9N0qtANbkqJiLPfAw.jpeg"/></div><figcaption class="kd ke gj gh gi kf kg bd b be z dk translated">IEEE CVPR 2020</figcaption></figure><figure class="ki kj kk kl gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kh"><img src="../Images/0d29be92148f48b29e03e2f6262670ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aG55xYfv1laghNqm"/></div></div><figcaption class="kd ke gj gh gi kf kg bd b be z dk translated">由<a class="ae kq" href="https://unsplash.com/@franki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗兰基·查马基</a>在<a class="ae kq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="4b94" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi lp translated"><span class="l lq lr ls bm lt lu lv lw lx di">在</span>这个故事中，介绍了阿里巴巴集团和亚利桑那州立大学在频域中的学习。这是作为IEEE CVPR 2020的技术论文发表的。本文提出了一种解决由于输入图像文件较大而导致图像大小转换为224x224x3时空间分辨率降低(信息量减少)问题的方法。使用离散余弦变换(DCT)系数作为输入方法比传统的空间下采样方法在预处理阶段更好地保留了图像信息，传统的空间下采样方法将图像大小调整为224×224，这是大多数CNN模型的默认输入大小，并且在图像分类、对象检测和实例分割任务中实现了更好的准确性。结果表明，该方法提高了图像分类、目标检测和实例分割任务的准确性。它也是程序员友好的，因为它很容易实现，只需要对现有的CNN模型做很少的改变。</p><p id="679c" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">让我们看看他们是如何做到的。我只讲解DCTNet的精髓，所以如果你有兴趣阅读我的博客，请点击<a class="ae kq" href="https://arxiv.org/abs/2002.12416" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ja"> DCTNet </strong> </a> <strong class="kt ja">和</strong><a class="ae kq" href="https://github.com/calmevtime/DCTNet" rel="noopener ugc nofollow" target="_blank"><strong class="kt ja">Github</strong></a><a class="ae kq" href="https://arxiv.org/pdf/1903.06391.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kt ja">。</strong> </a></p><figure class="ki kj kk kl gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ly"><img src="../Images/06ef6f1cc228497431482aec5b322ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NAy3FH1tLcNCXkoD3HPlQ.png"/></div></div></figure><h1 id="2e32" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">这篇论文说了什么？</h1><p id="f78d" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated">通过将在JPEG压缩中使用的离散余弦变换(DCT)通道输入到DNN输入数据，图像大小可以通过将其变换到具有大分辨率的频域而直接输入到DNN。与传统的图像输入方法相比，它还提高了DNN的准确性，同时保持输入数据量不变。</p><p id="70bc" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">图4示出了DCT通道选择，其静态地选择如JPEG压缩中的低频通道，并且关注亮度分量而不是色度分量。与JPEG压缩一样，低频通道是静态选择的，重点放在亮度分量而不是色度分量上。这确保了具有最高激活概率的频道被馈送到CNN模型。</p><figure class="ki kj kk kl gt ka gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/cef509d48e156064e772424bfdfebfad.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*_VqEg67BVDG1bIn48cAC7g.png"/></div></figure><p id="9289" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">对于分类、检测和分割等各种任务，在预处理阶段，DCTNet比下采样方法(调整图像大小)更好地保留了图像信息。结果，我们表明，它实现了准确性的提高。对于ImageNet分类任务，对于ResNet-50为+ 1.41%，对于MobileNetV2为+ 0.66%，对于对象检测和实例分割任务的Mask R-CNN为+ 0.8%。</p><h1 id="ff8d" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">方法学</h1><p id="2261" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated">基于频域模型，提出了一种基于学习的通道选择方法，以探索给定CNN模型的频谱偏差，即哪些频率分量更有利于后续的推理任务。为什么作者决定使用类似JPEG压缩的方法进行DNN预处理？作者通过实验得出这样的事实:人类视觉系统(HVS)对不同频率成分的敏感度是不相等的[Kim et al .，2017]，以及CNN模型对低频通道比对高频通道更敏感，这与HSV模型一致。</p><h1 id="445a" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">DCTNet</h1><p id="d07d" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated">高分辨率RGB图像首先转换到YCbCr色彩空间，然后转换到频域。相同频率的所有2D DCT系数分量被分组到一个通道中，以成为3D DCT立方体。这样就产生了多个频道。由于某些频率通道(如低频通道和亮度分量)对推断精度的影响比其他通道更大，因此只保留最重要的频率通道；在YCbCr颜色空间中选择的通道被组合以形成单个张量。最后，每个频率通道通过从训练数据集计算的均值和方差进行归一化，并将数据发送到GPU进行推理。</p><figure class="ki kj kk kl gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nd"><img src="../Images/53649ef30696d8def8a8928906571da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*buz7RTjx16qLaa1O-VBznw.png"/></div></div><figcaption class="kd ke gj gh gi kf kg bd b be z dk translated">在图2中，调整了图像的大小，但是在DCTNet中，不需要调整图像的大小。</figcaption></figure><p id="4334" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">对于频域中的输入特征，在空间域中开发的所有现有CNN模型都可以以最小的修改来应用。具体地，如图3所示，只需要去除输入的CNN层，其余的残留块需要保留。第一残差层被用作输入层，并且输入通道的数量被修改以匹配DCT系数输入的维度。由于频域中的输入特征图在<em class="ne"> H </em>和<em class="ne"> W </em>维度上小于空间域中的输入特征图，而在<em class="ne"> C </em>维度上大于空间域中的输入特征图，因此我们跳过传统CNN模型的输入层(步长-2卷积)。如果在输入卷积之后有一个最大池操作符(例如ResNet-50)，我们也跳过最大池操作符。然后，调整下一层的通道大小，以匹配频域中的通道数量。</p><figure class="ki kj kk kl gt ka gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4789a9cb80290f0852b597de2b224499.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*E_F4163leRrrt4Eij845XA.png"/></div></figure><h1 id="1bbb" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">结果</h1><p id="7063" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated">使用Imagenet，表明与传统的图像输入方法相比，仅使用有益于DNN性能的DCT通道，同时保持输入数据量恒定，可以提高DNN精度。</p><figure class="ki kj kk kl gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ng"><img src="../Images/814eae2a4086c43cf26d647941f9a388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JrWxJjHZW-OHGE9a4l71gw.png"/></div></div></figure><p id="752d" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">类似地，通过仅使用适合DNN性能的DCT通道，可以在实例分割任务中实现高性能分割。</p><figure class="ki kj kk kl gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ca"><img src="../Images/2de23def9a2e037eb92dd9d11ef2383b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FrjgyAvCYjeSSuNKmjZhA.png"/></div></div></figure><h2 id="4290" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lc nm nn mn lg no np mr lk nq nr mv iw bi translated">参考</h2><p id="fd08" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated">[Kim et al .，2017] J. Kim和S. Lee，“图像质量评估框架中人类视觉敏感度的深度学习”，IEEE CVPR，2017。</p><p id="0e5f" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">[<a class="ae kq" href="https://arxiv.org/abs/2002.12416" rel="noopener ugc nofollow" target="_blank">DCTNet</a>][<a class="ae kq" href="https://github.com/calmevtime/DCTNet" rel="noopener ugc nofollow" target="_blank">Github</a>]徐国良、秦明敏、孙福峰、王永源、陈永源、任福峰，“频域中的学习”，IEEE 2020。</p><h1 id="b50d" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">过去论文摘要列表</h1><h2 id="1d68" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lc nm nn mn lg no np mr lk nq nr mv iw bi translated">深度学习方法</h2><p id="2ee6" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated"><strong class="kt ja"> 2020年:【</strong><a class="ae kq" href="https://medium.com/towards-artificial-intelligence/lets-compress-the-cnn-training-like-a-jpeg-compression-ca8237c56f3c" rel="noopener"><strong class="kt ja">DCTNet</strong></a><strong class="kt ja">】</strong></p><h2 id="0939" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lc nm nn mn lg no np mr lk nq nr mv iw bi translated">不确定性学习</h2><p id="5033" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated"><strong class="kt ja">2020:</strong><strong class="kt ja"/><a class="ae kq" href="https://mako95.medium.com/cvpr2020-paper-summary-data-uncertainty-in-face-recognition-1f17547473a2" rel="noopener"><strong class="kt ja">DUL</strong></a><strong class="kt ja"/></p><h2 id="9a73" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lc nm nn mn lg no np mr lk nq nr mv iw bi translated">异常检测</h2><p id="975d" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated"><strong class="kt ja">2020:【</strong><a class="ae kq" href="https://medium.com/towards-artificial-intelligence/for-safety-reasons-self-driving-cars-must-not-miss-detecting-the-signs-bb26e65e721" rel="noopener"><strong class="kt ja">FND</strong></a><strong class="kt ja"/></p><h2 id="ff06" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lc nm nn mn lg no np mr lk nq nr mv iw bi translated">一级分类</h2><p id="b343" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated"><strong class="kt ja">2019:【</strong><a class="ae kq" href="https://medium.com/swlh/paper-summary-deep-one-class-classification-doc-adc4368af75c" rel="noopener"><strong class="kt ja">DOC</strong></a><strong class="kt ja"/></p><p id="c4c4" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kt ja"> 2020年:【</strong><a class="ae kq" href="https://medium.com/the-shadow/exploring-important-feature-repressions-in-deep-one-class-classification-droc-d04a59558f9e" rel="noopener"><strong class="kt ja">DROC</strong></a><strong class="kt ja"/></p><h2 id="61dd" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lc nm nn mn lg no np mr lk nq nr mv iw bi translated">图象分割法</h2><p id="c731" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated"><strong class="kt ja">2018:</strong><a class="ae kq" href="https://medium.com/swlh/paper-summary-biomedical-image-segmentation-and-object-detection-uolo-c1175ba5c8c4" rel="noopener"><strong class="kt ja">【UOLO】</strong></a></p><p id="7424" class="pw-post-body-paragraph kr ks iq kt b ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kt ja"> 2020年:【</strong><a class="ae kq" href="https://medium.com/towards-artificial-intelligence/efficient-biomedical-segmentation-when-only-a-few-label-images-are-available-2e0b2513703d" rel="noopener"><strong class="kt ja">ssCPCseg</strong></a><strong class="kt ja">】</strong></p><h2 id="3929" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lc nm nn mn lg no np mr lk nq nr mv iw bi translated">图像聚类</h2><p id="3c6b" class="pw-post-body-paragraph kr ks iq kt b ku mx kw kx ky my la lb lc mz le lf lg na li lj lk nb lm ln lo ij bi translated"><strong class="kt ja">2020:</strong><a class="ae kq" href="https://medium.com/swlh/paper-deep-transfer-clustering-dtc-learning-to-discover-novel-visual-categories-ec5a26aea075" rel="noopener"><strong class="kt ja">【DTC】</strong></a></p></div></div>    
</body>
</html>