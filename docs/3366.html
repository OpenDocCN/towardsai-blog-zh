<html>
<head>
<title>Window Functions in SQL and PySpark ( Notebook)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SQL和PySpark中的窗口函数(笔记本)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/window-functions-in-sql-and-pyspark-notebook-9145ae96beec?source=collection_archive---------2-----------------------#2022-12-03">https://pub.towardsai.net/window-functions-in-sql-and-pyspark-notebook-9145ae96beec?source=collection_archive---------2-----------------------#2022-12-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8e27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你是一名数据工程师，窗口函数是你几乎每天在工作中都会用到的东西。窗口功能使工作变得非常简单。它们有助于解决一些复杂的问题，并有助于轻松执行复杂的操作。</p><p id="2030" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们深入了解窗口函数的用法以及使用它们可以执行的操作。</p><p id="7c14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注</strong>:下面的一切，我都在Databricks社区版中实现了。这不是一篇书面文章；把笔记本贴在这里。以此作为我前进的参考。</p><p id="c0fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">数据砖块笔记本— </strong></p><p id="39f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本笔记本将向您展示如何创建和查询您上传到DBFS的表格或数据框架。DBFS是一个数据块文件系统，允许你在数据块内部存储查询数据。这个笔记本假设你有一个文件已经在DBFS，你想阅读。</p><p id="86bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个笔记本是用**Python**编写的，所以默认的单元类型是Python。但是，您可以通过使用`%LANGUAGE '语法来使用不同的语言。Python，Scala，SQL，R都支持。</p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="b34d" class="ku kv iq kq b be kw kx l ky kz"># File location and type<br/>file_location = "/FileStore/tables/wage_data.csv"<br/>file_type = "csv"<br/># CSV options<br/>infer_schema = "true"<br/>first_row_is_header = "true"<br/>delimiter = ","<br/># The applied options are for CSV files. For other file types, these will be ignored.<br/>df = spark.read.format(file_type) \<br/>.option("inferSchema", infer_schema) \<br/>.option("header", first_row_is_header) \<br/>.option("sep", delimiter) \<br/>.load(file_location)<br/>display(df)</span></pre><p id="54d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从Pyspark数据框架创建视图或表格</p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="8664" class="ku kv iq kq b be kw kx l ky kz">temp_table_name = "wage_data_csv"<br/>df.createOrReplaceTempView(temp_table_name)</span></pre><p id="3c9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从表中检索数据</p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="0fbc" class="ku kv iq kq b be kw kx l ky kz">select * from `wage_data_csv`</span></pre><p id="70f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将它注册为临时视图后，它将只对该特定笔记本可用。如果您希望其他用户能够查询该表，您也可以从DataFrame创建一个表。</p><p id="f385" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦保存，该表将在集群重启后保持不变，并允许不同笔记本电脑上的不同用户查询该数据。</p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="4b85" class="ku kv iq kq b be kw kx l ky kz">permanent_table_name = "wage_data_csv"<br/>df.write.format("parquet").saveAsTable(permanent_table_name)</span></pre><p id="74ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有三种类型的窗口功能:</p><ol class=""><li id="88a8" class="la lb iq jp b jq jr ju jv jy lc kc ld kg le kk lf lg lh li bi translated">聚合— (AVG、最大值、最小值、总和、计数)</li></ol><p id="0991" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在SQL中— </strong></p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="7263" class="ku kv iq kq b be kw kx l ky kz">%sql<br/><br/> -- 1. Aggregate<br/><br/> -- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -<br/> -- Limitations of GroupBy and normal aggregate functions<br/><br/> select distinct(region) from wage_data_csv<br/> select distinct(education) from wage_data_csv<br/> select distinct(jobclass) from wage_data_csv<br/> select region, year, education, jobclass, avg(wage) from wage_data_csv group by region, education, jobclass order by avg(wage) desc<br/> -- Here if we want to select year then we have to use nested queries and then select as above sql statement will throw an error.<br/> <br/> -- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -<br/> -- Aggregate AVG function<br/><br/> select *, avg(wage) over() as average_salary from wage_data_csv<br/> select region, education, jobclass, avg(wage) over( partition by region, education, jobclass) as avg_wage from wage_data_csv<br/> select *, avg(wage) over( partition by region, education, jobclass) as avg_wage from wage_data_csv<br/> select region, education, jobclass, age, avg(wage) over( partition by region, education, jobclass order by age) as avg_wage from wage_data_csv<br/> <br/>-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - <br/> --Functionaltites:<br/><br/> select distinct region, education, jobclass, avg(wage) over( partition by region, education, jobclass) as avg_wage from wage_data_csv<br/> select distinct region, education, jobclass, max(wage) over( partition by region, education, jobclass) as max_wage from wage_data_csv<br/> select distinct region, education, jobclass, count(wage) over( partition by region, education, jobclass) as count_wage from wage_data_csv<br/> select distinct region, education, jobclass, min(wage) over( partition by region, education, jobclass) as min_wage from wage_data_csv<br/> select distinct region, education, jobclass, sum(wage) over( partition by region, education, jobclass) as sum_wage from wage_data_csv<br/> <br/>-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span></pre><p id="5f2e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Pyspark —</p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="0a3f" class="ku kv iq kq b be kw kx l ky kz">#Pyspark<br/>#reading data<br/>df = spark.sql("select * from wage_data_csv")<br/>df.display()<br/><br/># Aggregate Window Functions<br/>from pyspark.sql.functions import col,avg,sum,min,max,count<br/>from pyspark.sql import Window<br/><br/>df = df.withColumn("avg_salary", avg(col("wage")).over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/>df = df.withColumn("max_salary", max(col("wage")).over(Window.partitionBy("region", "education", "jobclass")))<br/>df = df.withColumn("min_salary", min(col("wage")).over(Window.partitionBy("region", "education", "jobclass")))<br/>df = df.withColumn("sum_salary", sum(col("wage")).over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/>df = df.withColumn("count_salary_units", count(col("wage")).over(Window.partitionBy("region", "education", "jobclass")))<br/><br/>#df.display()<br/>#df.distinct().display()<br/>df.select("region", "education", "jobclass","wage","avg_salary","max_salary","min_salary","sum_salary","count_salary_units").distinct().filter(df.education=="1. &lt; HS Grad").display()<br/>#df.select("region", "education", "jobclass", "avg_salary").display()</span></pre><p id="8070" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.排名—(行编号，排名，密集排名，百分比排名，整体)</p><p id="be13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在SQL中—</p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="c340" class="ku kv iq kq b be kw kx l ky kz"><br/> -- 2. Ranking<br/><br/> -- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - <br/> -- Functionalities:<br/><br/>select region, education, jobclass, wage, row_number() over(partition by region, education, jobclass order by wage) from wage_data_csv<br/> -- row_number() adds an integer to the records w.r.t order by column based on partitions.<br/> <br/>select region, education, jobclass, wage, rank() over(partition by region, education, jobclass order by wage) from wage_data_csv<br/> -- rank() adds an integers w.r.t order by column based on partitions, but if there is tie same number is assigned and for subsequent records, numbers are skipped.<br/> <br/>select region, education, jobclass, wage, dense_rank() over(partition by region, education, jobclass order by wage) from wage_data_csv<br/> -- dense_rank() adds an integers w.r.t order by column based on partitions,if there is tie same number is assigned but for subsequent records, numbers are not skipped.<br/> <br/>select region, education, jobclass, wage, percent_rank() over(partition by region, education, jobclass order by wage) from wage_data_csv<br/> -- percent_rank() adds an integers w.r.t order by column based on partitions, Here rank is scaled between 0 and 1 as percentages<br/> <br/>select region, education, jobclass, wage, ntile(10) over(partition by region, education, jobclass order by wage) from wage_data_csv<br/> --ntile(10) adds an integers w.r.t order by column based on partitions, Here data is divided into chunks.<br/><br/> -- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - <br/> <br/>-- usage<br/><br/> -- to select records with wages more than 50%<br/> select * from (select region, education, jobclass, wage, percent_rank() over(partition by region, education, jobclass order by wage) as pr from wage_data_csv) where pr&gt;0.5<br/> <br/>-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span></pre><p id="64f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Pyspark —</p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="e7a1" class="ku kv iq kq b be kw kx l ky kz"># Ranking Functions<br/><br/>from pyspark.sql.functions import row_number, rank, dense_rank, ntile, percent_rank<br/>from pyspark.sql import Window<br/><br/>df = df.withColumn("row_number", row_number().over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/>df = df.withColumn("rank", rank().over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/>df = df.withColumn("dense_rank", dense_rank().over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/>df = df.withColumn("percent_rank", percent_rank().over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/>df = df.withColumn("ntile", ntile(10).over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/><br/>#df.display()<br/>#df.distinct().display()<br/>df.select("region", "education", "jobclass","wage","row_number","rank","dense_rank","percent_rank","ntile").distinct().filter(df.education=="1. &lt; HS Grad").display()<br/>#df.select("region", "education", "jobclass", "avg_salary").display()</span></pre><p id="8d81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.值—(提前期、滞后期、第一个值、最后一个值、第n个值)</p><p id="d47f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在SQL中—</p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="3bc4" class="ku kv iq kq b be kw kx l ky kz">-- 3. Value Functions<br/><br/> - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - <br/> -- Functionalities<br/><br/>select region, education, jobclass, wage, lag(wage) over(partition by region, education, jobclass order by wage) from wage_data_csv where education = "1. &lt; HS Grad"<br/> -- Shifts entire wage column down by a value based on partition, first value is none<br/> <br/>select region, education, jobclass, wage, lead(wage) over(partition by region, education, jobclass order by wage) from wage_data_csv where education = "1. &lt; HS Grad"<br/> -- Shifts entire wage column up by a value based on partition, last value is none based on partition<br/> <br/>select region, education, jobclass, wage, FIRST_VALUE(wage) over(partition by region, education, jobclass order by wage) from wage_data_csv where education = "1. &lt; HS Grad"<br/> -- is used to retrieve first value from the column passed based on partition<br/> <br/>select region, education, jobclass, wage, LAST_VALUE(wage) over(partition by region, education, jobclass) from wage_data_csv where education = "1. &lt; HS Grad"<br/> -- is used to retrieve last value from the column passed based on partition<br/> <br/>select region, education, jobclass, wage, nth_value(wage,2) over(partition by region, education, jobclass order by wage) from wage_data_csv where education = "1. &lt; HS Grad"<br/> -- is used to retrieve nth value from the column passed based on partition, above nth value will be none<br/><br/>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span></pre><p id="6dce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Pyspark —</p><pre class="kl km kn ko gt kp kq kr bn ks kt bi"><span id="c940" class="ku kv iq kq b be kw kx l ky kz"># Analytical Window Functions<br/>from pyspark.sql.functions import cume_dist, lag, lead<br/>from pyspark.sql import Window<br/><br/>df = df.withColumn("cume_dist", cume_dist().over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/>df = df.withColumn("lag", lag("wage").over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/>df = df.withColumn("lead", lead("wage").over(Window.partitionBy("region", "education", "jobclass").orderBy("wage")))<br/><br/>#df.display()<br/>#df.distinct().display()<br/>df.select("region", "education", "jobclass","wage","cume_dist","lag","lead").distinct().filter(df.education=="1. &lt; HS Grad").display()</span></pre><p id="2f63" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我写这只是作为一个参考给我…..</p><p id="325a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我觉得我的大脑是一本图书馆手册，里面保存了所有概念的参考资料，在某一天，如果它想检索某个概念的更多细节，它可以从手册参考资料中选择这本书，并通过查看它来检索数据。</p><p id="0a1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以这就是为什么我写…</p><p id="6525" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">快乐学习..</p></div></div>    
</body>
</html>