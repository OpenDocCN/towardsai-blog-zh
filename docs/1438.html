<html>
<head>
<title>Supervised Contrastive Learning for Cassava Leaf Disease Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">木薯叶部病害分类的监督对比学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/supervised-contrastive-learning-for-cassava-leaf-disease-classification-9dd47779a966?source=collection_archive---------1-----------------------#2021-01-27">https://pub.towardsai.net/supervised-contrastive-learning-for-cassava-leaf-disease-classification-9dd47779a966?source=collection_archive---------1-----------------------#2021-01-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="7d8a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="875a" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">应用深度学习和监督对比学习检测木薯叶片病害。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/4766b2a3717f0a88dfeb8672c2e0a90b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DiEuvGRKwbOnG6Gv"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@malmanxx?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马尔曼xx </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="7274" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">有监督的对比学习(Prannay Khosla等人)是一种在分类任务上优于交叉熵监督训练的训练方法。<br/>想法是使用<strong class="lh ja">监督对比学习(SCL) </strong>训练模型可以使模型编码器从样本中学习更好的类表示，这应该导致对图像和标签损坏的更好的泛化和鲁棒性。</p><blockquote class="mk"><p id="03d7" class="ml mm iq bd mn mo mp mq mr ms mt ma dk translated">在本文中，您将了解监督对比学习是什么，以及如何工作，您将看到代码实现，一个用例应用程序，最后是SCL和常规交叉熵之间的比较。</p></blockquote><h2 id="ded2" class="mu mv iq bd mw mx my dn mz na nb dp nc lo nd ne nf ls ng nh ni lw nj nk nl iw bi translated">简而言之，这就是SCL的工作方式:</h2><blockquote class="nm nn no"><p id="9067" class="lf lg np lh b li lj ka lk ll lm kd ln nq lp lq lr nr lt lu lv ns lx ly lz ma ij bi translated">属于同一类的点的聚类在嵌入空间中被拉在一起，同时推开来自不同类的样本的聚类。</p></blockquote><p id="19fb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有许多对比学习方法，如“<strong class="lh ja">监督对比学习</strong>”、“<strong class="lh ja">自我监督对比学习</strong>”、“<strong class="lh ja"> SimCLR </strong>”等，它们共有的对比部分是它们学习将一个领域的样本与其他领域的样本进行对比(推开)，但是<strong class="lh ja"> SCL以监督的方式利用标签信息</strong>来完成这项任务。欲了解更多详细信息，请查阅论文。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/237815512cb74875a95bdf5f8a56a3e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-t-40VshTIHBSlRQxy20jA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">不同的训练方法和架构。</figcaption></figure><p id="6f6e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本质上，使用监督对比学习训练分类模型分两个阶段执行:</p><ol class=""><li id="c6a1" class="nu nv iq lh b li lj ll lm lo nw ls nx lw ny ma nz oa ob oc bi translated">训练编码器以学习产生输入图像的矢量表示，使得同一类中的图像的表示与不同类中的图像的表示相比更相似。</li><li id="ce79" class="nu nv iq lh b li od ll oe lo of ls og lw oh ma nz oa ob oc bi translated">在冻结的编码器上训练分类器。</li></ol><h1 id="4d11" class="oi mv iq bd mw oj ok ol mz om on oo nc kf op kg nf ki oq kj ni kl or km nl os bi translated">使用案例</h1><p id="e353" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">我们将对来自Kaggle竞赛的数据集应用监督对比学习(<a class="ae le" href="https://www.kaggle.com/c/cassava-leaf-disease-classification" rel="noopener ugc nofollow" target="_blank">木薯叶疾病分类</a>)目标是将木薯植物的叶片图像分为5类:</p><pre class="kp kq kr ks gt oy oz pa pb aw pc bi"><span id="b297" class="mu mv iq oz b gy pd pe l pf pg">0: Cassava Bacterial Blight (CBB)<br/>1: Cassava Brown Streak Disease (CBSD)<br/>2: Cassava Green Mottle (CGM)<br/>3: Cassava Mosaic Disease (CMD)<br/>4: Healthy</span></pre><p id="986d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们有四种疾病和一类健康叶子，以下是一些图像样本:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ph"><img src="../Images/40874cea67be5095c240cb83c5d498b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zO_wriwCdsD6sQbLmY41Aw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">木薯叶图像样本来自竞赛。</figcaption></figure><blockquote class="nm nn no"><p id="6243" class="lf lg np lh b li lj ka lk ll lm kd ln nq lp lq lr nr lt lu lv ns lx ly lz ma ij bi translated">更多关于木薯叶疾病的信息，请点击PlantVillage 的链接。</p></blockquote><p id="6903" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">该数据有用于训练的<strong class="lh ja"> 21397 </strong>幅图像和用于测试集的<strong class="lh ja"> 15000 </strong>幅左右的图像。</p><h1 id="9487" class="oi mv iq bd mw oj ok ol mz om on oo nc kf op kg nf ki oq kj ni kl or km nl os bi translated">实验装置</h1><p id="3d57" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">— <strong class="lh ja">数据:</strong>分辨率为512 x 512像素的图像。<br/> — <strong class="lh ja">型号(编码器):</strong>B3高效网。<br/> Obs:你可以在这里查看<a class="ae le" href="https://www.kaggle.com/dimitreoliveira/cassava-leaf-supervised-contrastive-learning" rel="noopener ugc nofollow" target="_blank">完整代码。</a></p><p id="4d9e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通常，如果每个训练批次都有每个类的样本，对比学习方法会工作得更好，这将有助于编码器学习分批对比一个域与其他域的样本，这意味着使用较大的批次大小，在这种情况下，我已经对少数类进行了<strong class="lh ja">过采样</strong>，因此每个批次从每个类中获得样本的概率大致相同。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pi"><img src="../Images/d6bb24dd6fdb84e0274fd6c5c32c00c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oQ073RGIAD9BLjtgISK2Fg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">过采样后数据集的类分布。</figcaption></figure><p id="cd0b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">数据增强</strong>通常有助于计算机视觉任务，在我的实验中，我也看到了数据增强的改进，这里我使用了剪切、旋转、翻转、裁剪、剪切以及饱和度、对比度和亮度的变化，这看起来可能很多，但图像与原始图像没有太大的不同。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ph"><img src="../Images/72f22944b959286ee7a487a5c18f8562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXONtvTGHKfzhK-WccUQOQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">扩充数据样本。</figcaption></figure><h2 id="968a" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">现在我们可以看代码了</h2><h2 id="91b4" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated"><strong class="ak">编码器</strong></h2><p id="0aee" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">我们的编码器将是一个“<strong class="lh ja"> EfficientNet B3 </strong>”，但在编码器的顶部有一个平均池层，这个池层将输出一个大小为2048的向量，稍后它将用于检查编码器学习的表示。</p><pre class="kp kq kr ks gt oy oz pa pb aw pc bi"><span id="a36b" class="mu mv iq oz b gy pd pe l pf pg">def encoder_fn(input_shape):<br/>    inputs = L.Input(shape=input_shape, name=’inputs’)<br/>    base_model = efn.EfficientNetB3(input_tensor=inputs, <br/>                                    include_top=False,<br/>                                    weights=’noisy-student’, <br/>                                    pooling=’avg’)<br/> <br/>    model = Model(inputs=inputs, outputs=base_model.outputs)<br/>    return model</span></pre><h2 id="0016" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">投影头</h2><p id="f06e" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">投影头将放置在编码器的顶部，它将负责将编码器嵌入层的输出投影到一个<strong class="lh ja">更小的维度</strong>，在我们的例子中，它将把2048维编码器投影到一个128维向量。</p><pre class="kp kq kr ks gt oy oz pa pb aw pc bi"><span id="7916" class="mu mv iq oz b gy pd pe l pf pg">def add_projection_head(input_shape, encoder):<br/>    inputs = L.Input(shape=input_shape, name='inputs')<br/>    features = encoder(inputs)<br/>    outputs = L.Dense(128, activation='relu', <br/>                      name='projection_head', <br/>                      dtype='float32')(features)<br/>    <br/>    model = Model(inputs=inputs, outputs=outputs)<br/>    return model</span></pre><h2 id="9825" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">分类头</h2><p id="995e" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">分类器头用于训练的可选第二阶段，在SCL训练阶段之后，我们可以移除投影头并将该分类器头添加到编码器，并使用常规的<strong class="lh ja">交叉熵</strong>损失来微调模型，这应该在编码器的层冻结的情况下完成。</p><pre class="kp kq kr ks gt oy oz pa pb aw pc bi"><span id="e2f2" class="mu mv iq oz b gy pd pe l pf pg">def classifier_fn(input_shape, N_CLASSES, encoder, trainable=False):<br/>    for layer <strong class="oz ja">in</strong> encoder.layers:<br/>        layer.trainable = trainable<br/>        <br/>    inputs = L.Input(shape=input_shape, name='inputs')<br/>    <br/>    features = encoder(inputs)<br/>    features = L.Dropout(.5)(features)<br/>    features = L.Dense(1000, activation='relu')(features)<br/>    features = L.Dropout(.5)(features)<br/>    outputs = L.Dense(N_CLASSES, activation='softmax', <br/>                      name='outputs', dtype='float32')(features)<br/><br/>    model = Model(inputs=inputs, outputs=outputs)<br/>    return model</span></pre><h2 id="dd13" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">监督对比学习损失</h2><p id="4f6c" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">这是SCL损失的代码实现，这里唯一的参数是<strong class="lh ja">温度</strong>,“0.1”是默认值，但它可以调整，较大的温度可以导致班级更加分离，但较小的温度可以受益于更长的训练。</p><pre class="kp kq kr ks gt oy oz pa pb aw pc bi"><span id="8866" class="mu mv iq oz b gy pd pe l pf pg">class <strong class="oz ja">SupervisedContrastiveLoss</strong>(losses.Loss):<br/>    def __init__(self, temperature=0.1, name=None):<br/>        super(SupervisedContrastiveLoss, self).__init__(name=name)<br/>        self.temperature = temperature<br/><br/>    def __call__(self, labels, ft_vectors, sample_weight=None):<br/>        <em class="np"># Normalize feature vectors</em><br/>        ft_vec_normalized = tf.math.l2_normalize(ft_vectors, axis=1)<br/>        <em class="np"># Compute logits</em><br/>        logits = tf.divide(<br/>            tf.matmul(ft_vec_normalized, <br/>                      tf.transpose(ft_vec_normalized)<br/>            ), temperature<br/>        )<br/>        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)</span></pre><blockquote class="nm nn no"><p id="b00d" class="lf lg np lh b li lj ka lk ll lm kd ln nq lp lq lr nr lt lu lv ns lx ly lz ma ij bi translated">“<strong class="lh ja"> tfa </strong>是<a class="ae le" href="https://www.tensorflow.org/addons" rel="noopener ugc nofollow" target="_blank"> Tensorflow插件包</a>的别名。</p></blockquote><h1 id="7059" class="oi mv iq bd mw oj ok ol mz om on oo nc kf op kg nf ki oq kj ni kl or km nl os bi translated">培训</h1><p id="ae5b" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">我将跳过Tensorflow样板培训代码，因为它非常标准，但是您可以在这个<a class="ae le" href="https://www.kaggle.com/dimitreoliveira/cassava-leaf-supervised-contrastive-learning/notebook#Training-(supervised-contrastive-learning)" rel="noopener ugc nofollow" target="_blank">笔记本</a>中查看完整的代码。</p><h2 id="b7cc" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">第一阶段培训(编码器+投影头)</h2><p id="6576" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">第一阶段训练使用编码器+投影头，使用监督对比学习损失。</p><p id="56f6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">建立模型</strong></p><pre class="kp kq kr ks gt oy oz pa pb aw pc bi"><span id="ca0d" class="mu mv iq oz b gy pd pe l pf pg">with strategy.scope(): # Inside a strategy because I am using a TPU<br/>  encoder = encoder_fn((None, None, CHANNELS)) # Get the encoder<br/>  encoder_proj = add_projection_head((None, None, CHANNELS),encoder)<br/>  # Add the projection head to the encoder</span><span id="f792" class="mu mv iq oz b gy po pe l pf pg">encoder_proj.compile(optimizer=optimizers.Adam(lr=3e-4), <br/>                    loss=SupervisedContrastiveLoss(temperature=0.1))</span></pre><p id="2e18" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">培训</strong></p><pre class="kp kq kr ks gt oy oz pa pb aw pc bi"><span id="e582" class="mu mv iq oz b gy pd pe l pf pg">model.fit(x=get_dataset(TRAIN_FILENAMES, <br/>                        repeated=True, <br/>                        augment=True), <br/>          validation_data=get_dataset(VALID_FILENAMES, <br/>                                      ordered=True), <br/>          steps_per_epoch=100, <br/>          epochs=10)</span></pre><h2 id="4127" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">第二阶段培训(编码器+分类器头)</h2><p id="5241" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">对于训练的第二阶段，我们移除投影头，并在编码器的顶部添加分类器头，它现在已经训练了权重。对于这一步，我们可以使用常规的交叉熵损失，照常训练模型。</p><p id="81ea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">建立模型</strong></p><pre class="kp kq kr ks gt oy oz pa pb aw pc bi"><span id="b504" class="mu mv iq oz b gy pd pe l pf pg">with strategy.scope():<br/>    model = classifier_fn((None, None, CHANNELS), N_CLASSES, <br/>                          encoder, # trained encoder<br/>                          trainable=False) # with frozen weights</span><span id="90d8" class="mu mv iq oz b gy po pe l pf pg">    model.compile(optimizer=optimizers.Adam(lr=3e-4),<br/>                  loss=losses.SparseCategoricalCrossentropy(), <br/>                  metrics=[metrics.SparseCategoricalAccuracy()])</span></pre><p id="d02f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">训练<br/> </strong>跟以前差不多</p><pre class="kp kq kr ks gt oy oz pa pb aw pc bi"><span id="16db" class="mu mv iq oz b gy pd pe l pf pg">model.fit(x=get_dataset(TRAIN_FILENAMES, <br/>                        repeated=True, <br/>                        augment=True), <br/>          validation_data=get_dataset(VALID_FILENAMES, <br/>                                      ordered=True), <br/>          steps_per_epoch=100, <br/>          epochs=10)</span></pre><h1 id="2526" class="oi mv iq bd mw oj ok ol mz om on oo nc kf op kg nf ki oq kj ni kl or km nl os bi translated">可视化嵌入输出</h1><p id="e4b6" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">评估编码器的学习表示的一种有趣方式是可视化特征嵌入的输出，在我们的情况下，它是编码器的最后一层，即平均池层。<br/>这里我们将比较用SCL训练的模型和另一个用常规交叉熵训练的模型，你可以在<a class="ae le" href="https://www.kaggle.com/dimitreoliveira/cassava-leaf-supervised-contrastive-learning" rel="noopener ugc nofollow" target="_blank">参考笔记本</a>中看到完整的训练。<br/>通过在验证数据集的嵌入输出处应用t-SNE来生成可视化。</p><h2 id="1b38" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">交叉熵嵌入</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pp"><img src="../Images/8f75ac59457610ceae5f735861125ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YbHAf1w-Jj6bTgsbfV1CvQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">交叉熵训练模型的嵌入可视化。</figcaption></figure><h2 id="4dd0" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">监督对比学习嵌入</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pp"><img src="../Images/2f9767873fe378147ca12bb5e8bb5b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTQqaQvQ-bNm5VP2dYtYGw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">用SCL训练的模型的嵌入可视化。</figcaption></figure><p id="a481" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以看到，这两个模型似乎在将每个类别的样本聚类在一起方面做得很好，但是查看用SCL训练的模型的嵌入，每个类别的样本比其他类别的样本聚类得更远，这是对比学习的结果，我们还可以预期这种行为将导致更好的泛化，因为类别决策边界将更加清晰， 理解这一优势的一个直观练习是，尝试在每次嵌入时绘制决策边界线来分隔类，SCL嵌入会让您轻松得多。</p><h1 id="bce1" class="oi mv iq bd mw oj ok ol mz om on oo nc kf op kg nf ki oq kj ni kl or km nl os bi translated">结论</h1><p id="2b3a" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">我们看到，使用监督对比学习方法的训练既容易实现又高效，它可以导致更好的准确性和更好的类表示，这反过来也可以导致能够更好地概括的更健壮的模型。如果你愿意尝试一下SCL，一定要看看下面的链接。</p><h2 id="77c5" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">参考</h2><p id="fe70" class="pw-post-body-paragraph lf lg iq lh b li ot ka lk ll ou kd ln lo ov lq lr ls ow lu lv lw ox ly lz ma ij bi translated">——<a class="ae le" href="https://arxiv.org/pdf/2004.11362.pdf" rel="noopener ugc nofollow" target="_blank">监督对比学习论文</a>。<br/>-<a class="ae le" href="https://www.youtube.com/watch?v=MpdbFLXOOIw" rel="noopener ugc nofollow" target="_blank">《SCL论文评论》(Yannic Kilcher拍摄的视频)</a>。<br/> - <a class="ae le" href="https://keras.io/examples/vision/supervised-contrastive-learning/" rel="noopener ugc nofollow" target="_blank">官方Keras资源库的SCL教程</a>。<br/> - <a class="ae le" href="https://www.kaggle.com/dimitreoliveira/cassava-leaf-supervised-contrastive-learning" rel="noopener ugc nofollow" target="_blank"> SCL用于木薯叶部病害分类(Kaggle竞争)</a>。<br/> - <a class="ae le" href="https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/206454" rel="noopener ugc nofollow" target="_blank"> SCL讨论线程(Kaggle竞赛)</a>。</p><h2 id="f97f" class="mu mv iq bd mw mx pj dn mz na pk dp nc lo pl ne nf ls pm nh ni lw pn nk nl iw bi translated">致谢:</h2><ul class=""><li id="c23c" class="nu nv iq lh b li ot ll ou lo pq ls pr lw ps ma pt oa ob oc bi translated">论文作者:作者:Prannay Khosla、Piotr Teterwak、、Aaron Sarna、永龙田、Phillip Isola、Aaron Maschinot、、陈爱龙克里希南。</li><li id="33fc" class="nu nv iq lh b li od ll oe lo of ls og lw oh ma pt oa ob oc bi translated">Keras教程:Khalid Salama。</li></ul><p id="f6e2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你想了解如何使用Tensorflow建立计算机视觉的训练管道，请查看这篇文章:“<a class="ae le" href="https://medium.com/swlh/efficiently-using-tpu-for-image-classification-ed20d2970893" rel="noopener">有效地使用TPU进行图像分类</a>”。</p></div></div>    
</body>
</html>