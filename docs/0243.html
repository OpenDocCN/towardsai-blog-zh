<html>
<head>
<title>A Gentle Introduction to Graph Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形嵌入的简明介绍</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-gentle-introduction-to-graph-embeddings-c7b3d1db0fa8?source=collection_archive---------0-----------------------#2019-12-17">https://pub.towardsai.net/a-gentle-introduction-to-graph-embeddings-c7b3d1db0fa8?source=collection_archive---------0-----------------------#2019-12-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/cbd8df29fbdfb258771f2b3475d5e805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_KFZI8moFtwJ99YN"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><div class=""/><p id="b38d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以考虑使用图形神经网络(GNN)来执行节点分类问题，而不是使用传统的机器学习分类任务。通过提供节点的显式链接，该分类问题不再被分类为独立的问题，而是利用了诸如节点度的图结构。图形属性的有用性假设单个节点与其他相似节点相关。</p><p id="007e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">典型的例子是社交媒体网络。想象一下，脸书是如何根据你喜欢的帖子、你在哪里签到等将你和其他人联系起来的。一个图表能够代表这种关系，我们可以利用它来训练GNN。GNN的详细用例将在后面的故事中介绍。</p><p id="f9f6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这次我们将探索图形嵌入。图形嵌入与自然语言处理中的<a class="ae jg" href="https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a" rel="noopener" target="_blank">单词嵌入</a>一样，使用低维表示来表示具有语义相似性的实体。换句话说，相似的实体(例如苹果和橘子都是水果)具有相似的矢量表示。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="28e1" class="ll lm jj bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">模型</h1><p id="2a52" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">许多研究者研究了GNN是如何工作的。本节将介绍<a class="ae jg" href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" rel="noopener ugc nofollow" target="_blank"> TransE </a> (Border等人，2013)<a class="ae jg" href="https://pdfs.semanticscholar.org/68a3/3a3afac65eb6e0fb3726c1f9c8b727f32a42.pdf?_ga=2.21151099.1397092755.1575835510-317581445.1533093975" rel="noopener ugc nofollow" target="_blank">RESCAL</a>(Nickle等人，2011)<a class="ae jg" href="https://arxiv.org/pdf/1412.6575v4.pdf" rel="noopener ugc nofollow" target="_blank">dist mult</a>(Yang等人，2015)<a class="ae jg" href="https://arxiv.org/pdf/1606.06357.pdf" rel="noopener ugc nofollow" target="_blank">ComplEx</a>(Trouillon等人，2016)。</p><h2 id="2582" class="mo lm jj bd ln mp mq dn lr mr ms dp lv kr mt mu lz kv mv mw md kz mx my mh mz bi translated">迷睡</h2><p id="b87e" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">如果你熟悉<a class="ae jg" href="https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a" rel="noopener" target="_blank"> word2vec </a> (Mikolov等人，2013)，你可以假设<a class="ae jg" href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" rel="noopener ugc nofollow" target="_blank"> TransE </a> (Border等人，2013)与word2vec相似。给定主体实体(又名头)、关系和对象实体(又名尾)，如果主体实体与对象实体相似，则对象实体嵌入应该接近主体实体嵌入加关系嵌入。否则，主体实体应该远离客体实体。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c0736e986b99e6b7a31360e42709f63f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*LW02QZoKBA_gWjg-.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">Word2vec示例:国王+女人~=女王(<a class="ae jg" href="https://cfss.uchicago.edu/slides/text-analysis-fundamentals-and-sentiment-analysis/#1" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><h2 id="320d" class="mo lm jj bd ln mp mq dn lr mr ms dp lv kr mt mu lz kv mv mw md kz mx my mh mz bi translated">重新校准</h2><p id="36d0" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated"><a class="ae jg" href="https://pdfs.semanticscholar.org/68a3/3a3afac65eb6e0fb3726c1f9c8b727f32a42.pdf?_ga=2.21151099.1397092755.1575835510-317581445.1533093975" rel="noopener ugc nofollow" target="_blank"> RESCAL </a> (Nickle et al .，2011)使用多个矩阵来表示实体之间的关系。假设实体总数为<code class="fe nf ng nh ni b">n</code>，关系总数为<code class="fe nf ng nh ni b">m</code>，参数总数为<code class="fe nf ng nh ni b">n</code> x <code class="fe nf ng nh ni b">n</code> x <code class="fe nf ng nh ni b">m</code>。如果实体<code class="fe nf ng nh ni b">i</code>和实体<code class="fe nf ng nh ni b">j</code>之间没有关系，则该值被设置为零。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/466ca8921f29bac00e521a7a8565c4c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*xXDqgwCAsykucFn5lRtKwg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">实体(E)和关系(R)的矩阵(Nickle等人，2011年)</figcaption></figure><p id="ad4e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RESCAL的挑战之一(Nickle等人，2011年)是可扩展性。由于矩阵存储了每个主体实体和客体实体之间的关系，因此引入了大量的参数。</p><h2 id="7652" class="mo lm jj bd ln mp mq dn lr mr ms dp lv kr mt mu lz kv mv mw md kz mx my mh mz bi translated">DistMult</h2><p id="65f0" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1412.6575v4.pdf" rel="noopener ugc nofollow" target="_blank"> DistMult </a>(杨等，2015)与RESCAL (Nickle等，2011)类似，只是参数个数不同。代替使用复数矩阵，Yang等人通过仅使用对角矩阵(即，受限矩阵)来减少关系参数的数量。它需要较少的训练参数。RESCAL的许多参数可能是DistMult的十到一百倍以上。</p><p id="1766" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DistMult拥有少量参数(与TransE相同)来实现卓越的性能。在计算中，DistMult类似于TransE，DistMult使用乘法相互作用，而TransE使用加法相互作用。</p><p id="291f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DistMult的一个问题是它只能模拟对称关系，而不适用于一般知识图，因为它使用对角矩阵来简化关系。</p><h2 id="5782" class="mo lm jj bd ln mp mq dn lr mr ms dp lv kr mt mu lz kv mv mw md kz mx my mh mz bi translated">复杂的</h2><p id="99fb" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">为了处理对称和反对称关系，<a class="ae jg" href="https://arxiv.org/pdf/1606.06357.pdf" rel="noopener ugc nofollow" target="_blank"> Trouillon等人</a>，(2016)提出使用复嵌入(实部和虚部)。如果a不等于b，对称关系意味着sRo = oRs，而s是主体实体，R是关系，o是客体实体。如果它只适用于a等于b，那么它就是反对称关系。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/bdbf2bd53d80e10339c2f6458b892107.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/0*lp3nyNKJBP9tNm37.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">复数的例子(<a class="ae jg" href="https://en.wikipedia.org/wiki/Complex_number" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="029b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">评分函数类似于DistMult，因为引入了对角矩阵来对向量进行评分。DistMult评分函数帮助计算对称部分，而反对称部分由虚嵌入处理。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/32e0efaab101782b75e03dcfedd4315f.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*kyDDP9GcVgVl_O6d8eW3FA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">复杂的评分函数。w:对角矩阵，e_s:主语实体向量，e_o:宾语实体向量，Re:实向量，Im:虚向量。(Trouillon等人，2016年)</figcaption></figure></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="a009" class="ll lm jj bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">培训目标</h1><p id="130d" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated"><code class="fe nf ng nh ni b">Link prediction</code> ( <a class="ae jg" href="https://www.cs.cornell.edu/home/kleinber/link-pred.pdf" rel="noopener ugc nofollow" target="_blank"> Nowell和Kleinberg，2004 </a>)是训练一个实体嵌入的方法之一。给定定义的节点关系(即图形)，我们可以生成负样本(即损坏的关系，我们将在后面的部分中讨论)来扰乱模型，并允许模型学习实体之间的关系。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/693ac84ffededb0c31296a487a3ab7a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/0*9VV-NMXF83ZstvHR.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">顾客和食物之间的关系(<a class="ae jg" href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="a667" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们有了正样本和负样本，我们可以使用排序损失、逻辑损失或softmax损失作为损失函数来对这些样本进行评分。</p><ul class=""><li id="4440" class="nn no jj ki b kj kk kn ko kr np kv nq kz nr ld ns nt nu nv bi translated">排名损失:如果正样本分数大于负样本分数且误差较小，则会引入损失。</li><li id="7b35" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">逻辑损失:它可以预测边缘存在的概率，而不是对样本进行排序。</li><li id="d9a2" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">Softmax损失:了解实体连接的概率分布。</li></ul></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="e7b8" class="ll lm jj bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">大规模训练</h1><p id="87ca" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我们通过几种无监督的学习方法来训练实体嵌入。假设你在脸书或优步工作，有大量的节点和边。我们怎样才能把这些数据放进内存呢？Lerer等人在2019年发布了一个<a class="ae jg" href="https://arxiv.org/pdf/1903.12287.pdf" rel="noopener ugc nofollow" target="_blank"> PyTorch-BigGraph (PBG) </a>支持百万个节点和万亿条边。PBG提供了一种在多台机器上执行分布式执行的方法。</p><h2 id="0a4f" class="mo lm jj bd ln mp mq dn lr mr ms dp lv kr mt mu lz kv mv mw md kz mx my mh mz bi translated">划分</h2><p id="ee9b" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">执行分布式执行的第一步是对数据进行分区。步骤是:</p><ol class=""><li id="2f49" class="nn no jj ki b kj kk kn ko kr np kv nq kz nr ld ob nt nu nv bi translated">对每个实体进行分区(如有必要)。</li><li id="f862" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ob nt nu nv bi translated">桶的边缘分开。如果边与源分区p1和目的分区p2连接，则将边放入桶(p1，p2)中。</li><li id="8782" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ob nt nu nv bi translated">打乱分区内的存储桶顺序。重要的是，至少一个分区(即桶(p1，p2)中的p1或p2)被训练为预期第一个分区。从经验上来说，它比随机订单要好。</li></ol><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/4b5212163b0399891e04dd5408df7982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EGh3LXr1adOtO_k7tPR-Qw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">左图:节点被分割成多个分区。不重叠的分区可以并行执行。中心:基数小的实体没有分区。正确:订单很重要，而铲斗订单保证至少有一个铲斗之前经过培训。(勒勒等人，2019年)</figcaption></figure><p id="5cc4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">执行分布式执行时有两个基本变化。负样本从相同的分区中抽取，并且不再对边缘进行独立和相同分布的采样(i.i.d .)。其中一个影响是收敛速度变慢。我们将在后面的章节中再来讨论这些问题。</p><h2 id="105c" class="mo lm jj bd ln mp mq dn lr mr ms dp lv kr mt mu lz kv mv mw md kz mx my mh mz bi translated">分配</h2><p id="6d0a" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">数据经过分区后，可以发送到不同的机器上进行并行训练。传统机制使用参数服务器存储嵌入信息，并在工人发送梯度后异步更新参数。然而，缺点之一是巨大的网络带宽开销。因此，Lerer等人提出了一种解决方案。</p><p id="8477" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">建议在单个机器中锁定分区嵌入。如果分区是不相交的，它可以被并行训练，否则，嵌入将被锁定在<code class="fe nf ng nh ni b">Lock Server</code>中。只有共享参数才会被同步。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/79b9162466835e29159f55395cc3b862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRR89L17yn2tqlfVIJidgQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">1.2级培训师向锁服务器请求铲斗。2.从共享分区交换分区。3.从共享文件系统加载边并执行训练。(勒勒等人，2019年)</figcaption></figure><h2 id="893e" class="mo lm jj bd ln mp mq dn lr mr ms dp lv kr mt mu lz kv mv mw md kz mx my mh mz bi translated">负采样</h2><p id="f044" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">PBG有3种阴性取样方法。<code class="fe nf ng nh ni b">All negatives</code>方法是为所有数据生成所有可能边的最简单的方法。<code class="fe nf ng nh ni b">Same-batch negatives</code>该方法仅生成一批中所有可能的边。它减少了否定的总数和网络带宽开销。最后一种方法是<code class="fe nf ng nh ni b">Uniformly-sampled negatives</code>，它在一个批次中生成固定数量的底片样本。您可以访问<a class="ae jg" href="https://torchbiggraph.readthedocs.io/en/latest/loss_optimization.html#negative-sampling" rel="noopener ugc nofollow" target="_blank">文档</a>以更深入地了解负采样方法。</p><h1 id="0682" class="ll lm jj bd ln lo oe lq lr ls of lu lv lw og ly lz ma oh mc md me oi mg mh mi bi translated">拿走</h1><ul class=""><li id="c9cb" class="nn no jj ki b kj mj kn mk kr oj kv ok kz ol ld ns nt nu nv bi translated">PyTorch BigGraph (PBG)到目前为止(2019年11月)只支持CPU，而作者正在努力支持GPU。敬请关注。</li><li id="9c2f" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">PyTorch BigGraph (PBG) <a class="ae jg" href="https://torchbiggraph.readthedocs.io/en/latest/scoring.html#operators" rel="noopener ugc nofollow" target="_blank">实现了</a>trans，RESCAL，DistMult和ComplEx模型(有小的修改)。换句话说，你只需要提供正确的格式数据，PBG会为你完成剩下的工作。</li><li id="b594" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1606.06357.pdf" rel="noopener ugc nofollow" target="_blank"> Trouillon等人</a>，(2016)评估了每个阳性训练样本产生的阴性数量的影响。他们发现，每个正面训练样本有50个负面例子，这是准确性和训练时间之间的一个很好的权衡。</li><li id="bcec" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">前面提到的模型和损失函数在<a class="ae jg" href="https://github.com/facebookresearch/PyTorch-BigGraph" rel="noopener ugc nofollow" target="_blank"> PBG </a>中被重新实现，你可以简单地调用这个库来训练图形嵌入。</li></ul><h1 id="1c18" class="ll lm jj bd ln lo oe lq lr ls of lu lv lw og ly lz ma oh mc md me oi mg mh mi bi translated">额外阅读</h1><ul class=""><li id="9b3e" class="nn no jj ki b kj mj kn mk kr oj kv ok kz ol ld ns nt nu nv bi translated"><a class="ae jg" href="https://github.com/facebookresearch/PyTorch-BigGraph" rel="noopener ugc nofollow" target="_blank"> PyTorch BigGraph </a>源库</li><li id="fbd0" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">图学在<a class="ae jg" href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener ugc nofollow" target="_blank">妖孽吃</a></li></ul><h1 id="e9db" class="ll lm jj bd ln lo oe lq lr ls of lu lv lw og ly lz ma oh mc md me oi mg mh mi bi translated">关于我</h1><p id="0b6c" class="pw-post-body-paragraph kg kh jj ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。欢迎在<a class="ae jg" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与<a class="ae jg" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>联系，或者在<a class="ae jg" href="http://medium.com/@makcedward/" rel="noopener"> Medium </a>或<a class="ae jg" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="831b" class="ll lm jj bd ln lo oe lq lr ls of lu lv lw og ly lz ma oh mc md me oi mg mh mi bi translated">参考</h1><ul class=""><li id="38d7" class="nn no jj ki b kj mj kn mk kr oj kv ok kz ol ld ns nt nu nv bi translated">D.诺埃尔和克莱恩伯格。<a class="ae jg" href="https://www.cs.cornell.edu/home/kleinber/link-pred.pdf" rel="noopener ugc nofollow" target="_blank">社交网络的链接预测问题</a>。2004</li><li id="0b75" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">米（meter的缩写））nicke v . Tresp和H. Kriegel。<a class="ae jg" href="https://pdfs.semanticscholar.org/68a3/3a3afac65eb6e0fb3726c1f9c8b727f32a42.pdf?_ga=2.21151099.1397092755.1575835510-317581445.1533093975" rel="noopener ugc nofollow" target="_blank">多关系数据集体学习的三方模型</a>。2011</li><li id="85d9" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">南巴加、g .科尔莫德和s .穆图克里希南。<a class="ae jg" href="https://arxiv.org/pdf/1101.3291.pdf" rel="noopener ugc nofollow" target="_blank">社交网络中的节点分类</a>。2011</li><li id="5586" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">A.博德斯，n .乌苏尼尔和A .G .杜兰。<a class="ae jg" href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" rel="noopener ugc nofollow" target="_blank">翻译用于多关系数据建模的嵌入</a>。2013.</li><li id="8400" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">B.杨、易文涛、何晓霞、高军、邓力平。<a class="ae jg" href="https://arxiv.org/pdf/1412.6575v4.pdf" rel="noopener ugc nofollow" target="_blank">在知识库中嵌入用于学习和推理的实体和关系</a>。2015</li><li id="43a3" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">T.Trouillon、J. Welbl、S. Riedel、E. Gaussier和G. Bouchard。<a class="ae jg" href="https://arxiv.org/pdf/1606.06357.pdf" rel="noopener ugc nofollow" target="_blank">用于简单链路预测的复杂嵌入</a>。2016</li><li id="bda9" class="nn no jj ki b kj nw kn nx kr ny kv nz kz oa ld ns nt nu nv bi translated">A.莱雷尔、吴、沈、拉克鲁瓦、魏尔斯泰特、博斯和佩萨霍维奇。<a class="ae jg" href="https://arxiv.org/pdf/1903.12287.pdf" rel="noopener ugc nofollow" target="_blank"> PyTorch-BigGraph:大规模图嵌入框架</a>。2019</li></ul></div></div>    
</body>
</html>