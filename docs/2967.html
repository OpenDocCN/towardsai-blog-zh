<html>
<head>
<title>10 of the Most Important Recurrent Neural Networks For AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能最重要的10个递归神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/10-of-the-most-important-recurrent-neural-networks-for-ai-8de9989db315?source=collection_archive---------3-----------------------#2022-07-19">https://pub.towardsai.net/10-of-the-most-important-recurrent-neural-networks-for-ai-8de9989db315?source=collection_archive---------3-----------------------#2022-07-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6e2b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">递归神经网络的优势以及十大网络、顶级用例、挑战和最佳实践</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d7229a665d676dce0349e454e93722f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXc4etkKhDazvjpT1yzbDQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由来自Pexels的<a class="ae ky" href="https://www.pexels.com/@pixabay/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><h1 id="bb80" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">递归神经网络是可以根据随时间变化的数据进行预测的神经网络。</h1><h2 id="c429" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">更多技术信息:</h2><p id="5fb0" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">递归神经网络(RNN)是一种人工智能，用于对显示时间或顺序行为的数据进行建模[1]。这种类型的神经网络非常适合手写识别和机器翻译任务。rnn可以学习数据中的复杂模式，并长期存储信息。</p><p id="d206" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">传统的神经网络不太适合[25]对表现出时间或顺序行为的数据进行建模，因为传统的神经网络将每个输入向量视为独立的观察值[2]。rnn旨在通过整合反馈回路来模拟数据序列，从而允许网络保持“记忆”[3]，有效地允许rnn长时间学习模式。</p><p id="6f2d" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">有两种公认的递归神经网络:Elman网络和Jordan网络[4]。Elman networks是RNN开发的早期实施方案，也是跨用例采用和部署的简单终端。另一方面，与Elman网络相比，Jordan网络是最近才发展起来的。</p><p id="d806" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">Elman网络包含一个隐藏层和一个输出层。隐藏层包含一组完全连接到输入和输出层的神经元。隐藏层还包含一组与其自身相连的神经元(自循环)。这种自我连接形成了一个反馈回路，使网络能够保持“记忆”</p><p id="8b52" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">Jordan网络类似于Elman网络，但它们包含两个隐藏层，而不是一个。第一个隐藏层完全连接到输入层，但不连接到第二个隐藏层。第二隐藏层连接到第一和输出层。像Elman网络一样，Jordan网络在每个隐藏层中也包含自循环连接。实际上，这些通过连接的反馈回路允许乔丹网络比埃尔曼网络模拟更复杂的模式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/fe13b2c9fe7c03c55434f147e18b874a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gv2Of1rjfXlbpJASehGSQA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象</figcaption></figure><h1 id="7353" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">递归神经网络的优势(持续更新)</strong></h1><p id="3d3c" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">与传统的神经网络相比，RNNs有几个优点。首先，RNNs可以学习传统神经网络无法学习的数据中的复杂模式。其次，RNNs可以长时间存储信息，这对于机器翻译这样的任务来说是必不可少的。最后，与传统的神经网络相比，RNNs不太可能过度拟合训练数据[5]。另外:</p><p id="35ad" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">1.卷积神经网络(CNN)[6]更擅长检测空间数据中的模式，使它们更有效地用于图像识别任务。</p><p id="9c21" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">2.rnn可以学习序列模式，这使它们成为语言建模和机器翻译的理想选择。</p><p id="b4a1" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">3.rnn可以记住长期依赖关系，允许它们有效地模拟复杂的现实世界问题。</p><p id="a25e" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">4.rnn对噪声和异常值更鲁棒，使它们不太可能过度拟合训练数据。</p><p id="4327" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">5.由于参数数量减少，rnn比传统的神经网络更容易训练。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/e4b223a00ef4c69bbc70d8dcb62490a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7rvrvAuCfDbcZcMPxjroHg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的<a class="ae ky" href="https://www.pexels.com/@ds-stories/" rel="noopener ugc nofollow" target="_blank"> DS stories </a></figcaption></figure><h1 id="c826" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">十大递归神经网络</strong></h1><p id="7bcf" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">1.递归神经网络(RNN)是一种人工神经网络，其中节点之间的连接沿着时间序列形成有向图[26]。这允许它展示一个时间序列或文本的动态行为。</p><p id="bf7e" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">2.Elman网络是一种递归神经网络，其中隐含层包含一个抽头延迟线。</p><p id="7aa2" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">3.Jordan网络类似于Elman网络，但是具有从隐藏层到输出层的双向连接。</p><p id="195f" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">4.Hopfield网络[7]是具有联想记忆的递归神经网络。</p><p id="c7f3" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">5.长短期记忆(LSTM)是一种RNN，旨在学习长期依赖性。</p><p id="338c" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">6.门控循环单元(GRU) [8]是另一种类型的RNN，使用门来控制信息流。</p><p id="5230" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">7.回声状态网络(ESN) [9]是一种递归神经网络，具有随机连接的隐藏层。</p><p id="4e40" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">8.神经历史压缩(NHC)是一种递归神经网络，使用Hebbian学习[10]来记忆其输入/输出模式。</p><p id="0e17" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">9.时间相关性发现网络(tcdn)[11]是一种RNN，可以发现数据流中隐藏的相关性。</p><p id="fd08" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">10.脉冲神经网络(SNNs) [12]是一种通过使用脉冲代替传统的人工神经元来模拟神经元活动的神经网络。</p><h1 id="4e16" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">顶级用例(持续更新)</strong></h1><p id="b38d" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">简单列出，没有拆开个性化的细节:</p><p id="08e7" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">1.自然语言处理:情感分析、主题建模、命名实体识别；</p><p id="9ab0" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">2.时间序列分析:预测、异常检测；</p><p id="b5ce" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">3.计算机视觉:图像分类、物体检测、分割；</p><p id="29a0" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">4.音频分析:语音识别、音乐推荐；和</p><p id="091f" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">5.推荐系统:产品推荐，用户分析。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/2416a820b098c2a60838fddc016c92cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hvq3mpQzrJ96bUyKPv58aA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">作者Pixabay</figcaption></figure><h1 id="8595" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">顶级挑战(持续更新)</strong></h1><p id="0b76" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">使用RNNs有一些缺点。首先，由于消失梯度问题，RNNs可能难以训练[13]。其次，训练RNNs在计算上可能很昂贵。第三，rnn往往需要大量的训练数据才能有效学习。</p><p id="184c" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">当网络试图学习长期依赖关系时，误差梯度变得非常小，使得网络难以学习复杂的模式，这时就会出现消失梯度问题。已经提出了几种方法来解决消失梯度问题。一种方法是使用不同的激活函数，如整流线性单元(RELU) [14]。另一种方法是使用不同类型的RNN，如长短期记忆(LSTM) [15]网络(LSTM网络是一种RNN，旨在解决消失梯度问题。)</p><h2 id="902f" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">另外:</h2><p id="89c5" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">1.递归神经网络面临的最大挑战之一是有效地训练它们。这是因为它们对输入的变化非常敏感，这使得训练它们变得困难。</p><p id="0ea4" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">2.另一个挑战是，递归神经网络在时间和计算能力方面可能是非常资源密集型的，这使得它们对于许多应用来说是不切实际的。</p><p id="7078" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">3.递归神经网络易受过度拟合的影响，这意味着它们可能在训练[27]数据上表现良好，但不能很好地推广到新数据。</p><p id="5f1b" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">4.最后，递归神经网络可能不稳定，每次训练或运行时，即使输入相同，它们也可能产生不同的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/c8e2c5ef7746ac77006572ac1ba28501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DvnYmXkOPnWBa1OwcJzwow.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的Markus Spiske</figcaption></figure><h1 id="10eb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">将递归神经网络应用于机器学习问题的最佳实践(持续更新)</strong></h1><p id="3cff" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">1.在训练您的模型时，请始终使用训练/测试/验证分割，以帮助防止过度拟合，并在看不见的数据上为您提供模型性能的更准确估计。</p><p id="aa96" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">2.规范化您的输入数据。这将有助于您的模型更快地收敛并提高其准确性。</p><p id="85ae" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">3.使用2的幂作为批量大小有助于确保模型训练的效率和效果。</p><p id="b2c1" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">4.使用介于0.001和0.0001 [19]之间的学习率可获得最佳结果。过大的学习率会导致模型发散，而过小的学习率会导致模型收敛时间过长。</p><p id="76d8" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">5.使用L1或L2正则化[16]你的重量，以防止过度拟合。</p><p id="6423" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">6.在你的网络中的每个完全连接的层之后添加dropout [16],作为一种正则化形式来进一步防止过度拟合。</p><p id="b24b" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">7.尽可能使用ReLU激活函数[14],因为它们比其他激活函数收敛得更快。</p><p id="283d" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">8.尽可能使用Xavier初始化你的权重来帮助控制渐变的比例，防止渐变爆炸。</p><p id="935b" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">9.如果可能的话，用小批量[17]训练你的模型。这将有助于减少训练模型所需的时间，并提高收敛速度和准确性。</p><p id="217c" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">10.如果您正在训练深度神经网络，请使用适当的优化器，如Adam、RMSprop或SGD with momentum [21][22][23]。</p><p id="4dc9" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">11.使用适当的度量(如损失函数值和精确度)来监控模型的训练进度，以帮助确保模型按预期收敛，并尽早识别潜在问题。</p><p id="cdf0" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">12.使用早期停止来防止验证集过度拟合，以确保您的最终模型是可概括的，并且不会过度拟合验证数据。</p><p id="ad2a" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">13.培训后保存您的模型，以便您可以在以后需要时重用它们，而不必每次都从头开始重新培训。</p><p id="98cd" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">14.在生产中部署您的模型时，监视其性能以尽早捕捉任何潜在的问题(并在必要时相应地更新您的模型)。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="d358" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">如果您有任何编辑/修改建议或关于进一步扩展此主题的建议，请考虑与我分享您的想法。</p><h1 id="4b9f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">另外，请考虑订阅我的每周简讯:</h1><div class="np nq gp gr nr ns"><a href="https://pventures.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">周日报告#1</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">设计思维与AI的共生关系设计思维能向AI揭示什么，AI又能如何拥抱…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">pventures.substack.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og ks ns"/></div></div></a></div><h1 id="c7de" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">我写了以下与这篇文章相关的内容；他们可能与你有相似的兴趣:</strong></h1><h2 id="9b3e" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">1.我最近写了关于机器学习的贝叶斯推理</h2><div class="np nq gp gr nr ns"><a href="https://medium.com/@AnilTilbe/bayesian-inference-the-best-5-models-and-10-best-practices-for-machine-learning-11238a43929e" rel="noopener follow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">贝叶斯推理:机器学习的5个最佳模型和10个最佳实践</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">将贝叶斯推理应用于机器学习问题的优势、5大模型和10大最佳实践</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">medium.com</p></div></div><div class="ob l"><div class="oh l od oe of ob og ks ns"/></div></div></a></div><h2 id="099d" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">2.非结构化数据与结构化数据</h2><div class="np nq gp gr nr ns"><a href="https://medium.com/@AnilTilbe/unstructured-vs-structured-data-the-5-most-important-differences-8d86078a163b" rel="noopener follow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">非结构化数据与结构化数据:5个最重要的区别</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">结构化数据、非结构化数据的分类、各自的优势，以及如何将它们部署在一起…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">medium.com</p></div></div><div class="ob l"><div class="oi l od oe of ob og ks ns"/></div></div></a></div><h2 id="ca07" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">3.回归分析</h2><div class="np nq gp gr nr ns"><a href="https://medium.com/@AnilTilbe/regression-analysis-is-exceedingly-difficult-how-to-master-it-without-coding-542ae1d6edaf" rel="noopener follow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">回归分析极其困难:不编码如何掌握它</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">对关键概念、3个经过实战检验的模型和3个挑战有一个很好的基本理解，包括…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">medium.com</p></div></div><div class="ob l"><div class="oj l od oe of ob og ks ns"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="7300" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok">参考文献。</em></p><p id="4431" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 1。杰尼特，y .，格雷夫，e .，朱林，a .，&amp;# 38；t .米科洛夫(2016年11月18日)。递归神经网络中的变量计算。ArXiv.Org。</em><a class="ae ky" href="https://arxiv.org/abs/1611.06188" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://arxiv.org/abs/1611.06188</em></a></p><p id="ffef" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 2。基于递归神经网络的限价委托单序列分类。(未注明)。计算科学杂志，24，277–286。</em><a class="ae ky" href="https://doi.org/10.1016/j.jocs.2017.08.018" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://doi.org/10.1016/j.jocs.2017.08.018</em></a></p><p id="d168" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 3。PredRNN:用于时空预测学习的递归神经网络。(未注明)。IEEE Xplore。检索到2022年7月19日，来自</em><a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/9749915/" rel="noopener ugc nofollow" target="_blank">【https://ieeexplore.ieee.org/abstract/document/9749915/】T21</a></p><p id="5363" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 4。用遗传算法训练Elman和Jordan网络进行系统识别。(未注明)。工程中的人工智能，13(2)，107–117。</em><a class="ae ky" href="https://doi.org/10.1016/S0954-1810(98)00013-2" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://doi . org/10.1016/s 0954-1810(98)00013-2</em></a></p><p id="84f6" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 5。用于剩余使用寿命估算的通用神经网络。(未注明)。IEEE Xplore。检索2022年7月19日，来自</em><a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/4711422" rel="noopener ugc nofollow" target="_blank"><em class="ok"/></a></p><p id="53b4" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 6。用于放射学文本报告分类的卷积神经网络(CNN)和递归神经网络(RNN)架构的比较有效性。(未注明)。医学中的人工智能，97，79–88。</em><a class="ae ky" href="https://doi.org/10.1016/j.artmed.2018.11.004" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://doi.org/10.1016/j.artmed.2018.11.004</em></a></p><p id="07b0" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 7。拉姆绍尔、h .施福尔、b .莱赫纳、j .塞德尔、p .韦德里奇、m .阿德勒、t .格鲁伯、l .霍尔兹莱特纳、m .帕夫洛维奇、m .桑德维、G. K .格雷夫、v .克雷伊、d .科普、m .克兰鲍尔、g .布兰德施泰特、j .、&amp;# 38；Hochreiter，S. (2020年7月16日)。霍普菲尔德网络是你所需要的。ArXiv.Org。</em><a class="ae ky" href="https://arxiv.org/abs/2008.02217" rel="noopener ugc nofollow" target="_blank">【https://arxiv.org/abs/2008.02217】T21</a></p><p id="11e2" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">8。门控递归单元(GRU)神经网络的门变量。(未注明)。IEEE Xplore。检索到2022年7月19日，来自<a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/8053243" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://ieeexplore.ieee.org/abstract/document/8053243</em></a></p><p id="d759" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 9。用回声状态网络学习语法结构。(未注明)。神经网络，20(3)，424–432。</em><a class="ae ky" href="https://doi.org/10.1016/j.neunet.2007.04.013" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://doi.org/10.1016/j.neunet.2007.04.013</em></a></p><p id="b3ac" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 10。人工神经网络中Hebb学习的生物学背景。(未注明)。神经计算，152，27–35。</em><a class="ae ky" href="https://doi.org/10.1016/j.neucom.2014.11.022" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://doi.org/10.1016/j.neucom.2014.11.022</em></a></p><p id="93a0" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 11。(未注明)。ACM数字图书馆。检索2022年7月19日，转自</em><a class="ae ky" href="https://dl.acm.org/doi/abs/10.1145/3097983.3098145" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://dl.acm.org/doi/abs/10.1145/3097983.3098145</em></a></p><p id="6d7a" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 12。脉冲神经网络中的深度学习。(未注明)。神经网络，111，47–63。</em><a class="ae ky" href="https://doi.org/10.1016/j.neunet.2018.12.002" rel="noopener ugc nofollow" target="_blank"><em class="ok"/></a></p><p id="fd4a" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">13。胡，y，胡贝尔，a，安穆拉，j .，&amp;# 38；刘(2018 . 1 . 18)。克服平面递归网络中的消失梯度问题。ArXiv.Org。<a class="ae ky" href="https://arxiv.org/abs/1801.06105" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://arxiv.org/abs/1801.06105</em></a></p><p id="7afe" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 14。Agarap，A. F. (2018年3月22日)。使用校正线性单元(ReLU)的深度学习。ArXiv.Org。</em><a class="ae ky" href="https://arxiv.org/abs/1803.08375" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://arxiv.org/abs/1803.08375</em></a></p><p id="7006" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 15。舍斯汀斯基(2020)。递归神经网络(RNN)和长短期记忆(LSTM)网络基础。物理D:非线性现象，404，132306。</em><a class="ae ky" href="https://doi.org/10.1016/j.physd.2019.132306" rel="noopener ugc nofollow" target="_blank">【https://doi.org/10.1016/j.physd.2019.132306】T21</a></p><p id="a17a" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">16。Dropout改进了用于手写识别的递归神经网络。(未注明)。IEEE Xplore。检索到2022年7月19日，来自<a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/6981034" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://ieeexplore.ieee.org/abstract/document/6981034</em></a></p><p id="d482" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">17。批量归一化递归神经网络。(未注明)。IEEE Xplore。检索2022年7月19日，转自<a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/7472159" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://ieeexplore.ieee.org/abstract/document/7472159</em></a></p><p id="0a0e" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">18。双重整流线性单元(DReLUs):准递归神经网络中tanh激活函数的替代。(未注明)。模式识别字母，116，8–14。<a class="ae ky" href="https://doi.org/10.1016/j.patrec.2018.09.006" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://doi.org/10.1016/j.patrec.2018.09.006</em></a></p><p id="711f" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 19。Laje，&amp;# 38；早上好。(2013).通过驯服递归神经网络中的混沌实现鲁棒的定时和运动模式。自然神经科学，16(7)，925–933。</em><a class="ae ky" href="https://doi.org/10.1038/nn.3405" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://doi.org/10.1038/nn.3405</em></a></p><p id="033e" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 20。西里亚诺，j .，&amp;# 38；k . spiliopoulos(2019年7月9日)。具有xavier初始化和收敛到全局最小值的神经网络的标度极限。ArXiv.Org。</em><a class="ae ky" href="https://arxiv.org/abs/1907.04108" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://arxiv.org/abs/1907.04108</em></a></p><p id="f5e1" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 21。纽约州艾达，纽约州藤原，&amp;# 38；岩村，S. (2016年5月31日)。基于协方差矩阵预处理的深度神经网络自适应学习速率。ArXiv.Org。</em><a class="ae ky" href="https://arxiv.org/abs/1605.09593" rel="noopener ugc nofollow" target="_blank"><em class="ok"/></a></p><p id="666d" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">22。陈，周，丁，唐，杨，张，曹，&amp;# 38；古，q .(2018 . 6 . 18)。缩小自适应梯度方法在训练深度神经网络中的推广差距。ArXiv.Org。<a class="ae ky" href="https://arxiv.org/abs/1806.06763" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://arxiv.org/abs/1806.06763</em></a></p><p id="016a" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 23。基于双向深度递归神经网络的移动边缘计算在线主动缓存。(未注明)。IEEE Xplore。检索2022年7月19日，来自</em><a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/8660445" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://ieeexplore.ieee.org/abstract/document/8660445</em></a></p><p id="dab1" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok"> 24。帕斯卡努，r，米科洛夫，t .，&amp;# 38；纽约州本吉奥市(2013年5月26日)。递归神经网络的训练难度。PMLR。</em><a class="ae ky" href="https://proceedings.mlr.press/v28/pascanu13.html" rel="noopener ugc nofollow" target="_blank">【https://proceedings.mlr.press/v28/pascanu13.html】T21</a></p><p id="829d" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated">25。用超网反转区别表示。<a class="ae ky" href="https://blog.singularitynet.io/inverting-discriminative-representations-with-hypernets-b259dc64530c" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://blog . singularity net . io/inverting-discriminal-representations-with-hypernets-b 259 DC 64530 c</em></a></p><p id="0732" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok">二十六。用超网反转区别表示。</em><a class="ae ky" href="https://blog.singularitynet.io/inverting-discriminative-representations-with-hypernets-b259dc64530c" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://blog . singularity net . io/inverting-discriminal-representations-with-hypernets-b 259 DC 64530 c</em></a></p><p id="e1d8" class="pw-post-body-paragraph mg mh it mi b mj mz ju ml mm na jx mo lw nb mq mr lz nc mt mu mc nd mw mx my im bi translated"><em class="ok">二十七。机器学习中的模型复杂度&amp;过拟合。</em><a class="ae ky" href="https://vitalflux.com/model-complexity-overfitting-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"><em class="ok">https://vital flux . com/model-complexity-overfitting-in-machine-learning/</em></a></p></div></div>    
</body>
</html>