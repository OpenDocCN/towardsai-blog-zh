<html>
<head>
<title>Why Accuracy Is Not A Good Metric For Imbalanced Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么准确度不是不平衡数据的好指标</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/why-accuracy-is-not-a-good-metric-for-imbalanced-data-bbc9cc3f90a2?source=collection_archive---------3-----------------------#2022-08-01">https://pub.towardsai.net/why-accuracy-is-not-a-good-metric-for-imbalanced-data-bbc9cc3f90a2?source=collection_archive---------3-----------------------#2022-08-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="82d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">机器学习中的分类是一种监督学习概念，其中数据点被分类到不同的类别中。例如，确定电子邮件是“垃圾邮件”还是“非垃圾邮件”以及确定患者的血型。</p><p id="cbf4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">机器学习的分类一般分为三类:</p><ul class=""><li id="ff4c" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">二元分类</li><li id="e40b" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">多类分类</li><li id="58fd" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">多标签分类</li></ul><h2 id="1032" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">什么是不平衡的类或数据？</h2><p id="3c69" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">不平衡数据指的是已知类之间的实例分布有偏差的问题(一个类比另一个类有更多的实例)。例如，一个类可能有10000个实例，而另一个类只有100个实例。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/84c1ff97656ef8a194c2e91c46a45c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KZhapuFAZsJK1E9a.jpeg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">具有多数实例的类比具有少数实例的类权重更大——Google</figcaption></figure><p id="d73d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在类的实例数量上，数据不平衡的范围从很小到很大。小的数据不平衡如4:1，10:1等。，不会对你的模型造成太大伤害，但是随着数据不平衡开始增加到1000:1和5000:它会给你的机器学习模型造成问题。</p><p id="6681" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">具有许多实例的不平衡分类问题中的类被称为<strong class="jp ir">多数类。</strong></p><p id="70f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在不平衡分类问题中很少有实例的类被称为<strong class="jp ir">少数类。</strong></p><h2 id="656c" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">为什么不平衡的班级会产生问题？</h2><p id="877a" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">当处理不平衡的数据时，少数类是我们最感兴趣的。就像检测“垃圾”邮件一样，与“非垃圾”邮件相比，它们的数量要多得多。因此，如果数据高度不平衡，机器学习算法会偏向较大的类，有时甚至会忽略较小的类。</p><p id="3cf7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">机器学习算法旨在从训练数据中学习，以最小化损失并最大化准确性。让我们看看机器学习算法如何处理高度不平衡的数据。</p><h2 id="b568" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">一个例子</h2><p id="4535" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">考虑这个例子，其中有100个类“A”的实例和9900个类“B”的实例。</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="f49d" class="kz la iq mo b gy ms mt l mu mv">x, y = make_classification(n_samples=10000, weights=[0.99], flip_y=0)</span></pre><p id="ddd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据集的计数图可以用<a class="ae mw" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> seaborn库</a>创建</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="ee7a" class="kz la iq mo b gy ms mt l mu mv">np.unique(y,return_counts=True)<br/>y=np.where(y==0,'A','B')<br/>sns.countplot(x=y)</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/6bcbd988b96935507bd5d7c9c7f734f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*9h32l0P8nHjj9ZQsEsVE8g.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">数据集的计数图。</figcaption></figure><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="3b8c" class="kz la iq mo b gy ms mt l mu mv">xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20, random_state=42)<br/>print(np.unique(ytrain,return_counts=True))<br/>print(np.unique(ytest,return_counts=True))</span></pre><p id="676f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在使用测试规模为20%的<a class="ae mw" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"> train_test_split </a>将数据集分割成训练和测试数据之后，我们剩下7919个用于类“A”的训练示例和81个用于类“B”的训练示例。“A”级的测试示例为1981，而“B”级的测试示例为19。</p><p id="fa26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">先用我们的训练数据训练一个<a class="ae mw" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>模型。</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="d956" class="kz la iq mo b gy ms mt l mu mv">lr=LogisticRegression()<br/>lr.fit(xtrain,ytrain)<br/>lr.score(xtest,ytest)</span></pre><p id="1c9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，如果我们使用评分方法检查模型的准确性，它是0.992。99.2%的准确率？它表现很好，对吗？让我们检查一下混淆矩阵。</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="0cf9" class="kz la iq mo b gy ms mt l mu mv">pred_lr=lr.predict(xtest)<br/>print(confusion_matrix(ytest,pred_lr))</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6b7e02924321d69d7e94331eada58e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*910AeyBPG831QNUeE8NurA.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">逻辑回归的混淆矩阵</figcaption></figure><p id="9a0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管类别“A”具有100%的准确性，但是19个测试示例中只有3个被正确分类。这一定是个错误，对吗？</p><p id="1524" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们在同一个数据集上使用<a class="ae mw" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">随机森林分类器</a>并检查发生了什么。</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="fb02" class="kz la iq mo b gy ms mt l mu mv">rfc=RandomForestClassifier()<br/>rfc.fit(xtrain,ytrain)<br/>rfc.score(xtest,ytest)</span></pre><p id="ecf8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这次的准确率分数是0.991，但是上次我们学到了什么？真实的结果隐藏在准确性的背后。让我们检查随机森林分类器预测的混淆矩阵。</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="5dba" class="kz la iq mo b gy ms mt l mu mv">pred_rfc=rfc.predict(xtest)<br/>print(confusion_matrix(ytest,pred_rfc))</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/033714f16fd60e05bee3dd5e9cefbc54.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*TDfGgQQ3IABfvpS921aaGg.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">随机森林分类器的混淆矩阵</figcaption></figure><p id="531a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1981个“A”类测试样本中只有1个分类错误，但19个“B”类测试样本中只有2个分类正确。</p><h2 id="50a7" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">我们的机器学习模型在这里做什么？</h2><p id="60f2" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">正如我们之前讨论过的，机器学习模型试图最大限度地提高准确性，这就是这里正在发生的事情。由于类“A”的实例构成了99%的数据，机器学习模型学习正确地对它们进行分类，并忽略或不学习太多关于类“B”的信息，因为将所有数据分类到类“A”将获得99%的准确性。</p><p id="6c54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您只需用python编写一条语句，就可以匹配这些模型的准确性。震惊？</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="daec" class="kz la iq mo b gy ms mt l mu mv">pred=['A']*len(ytest)</span></pre><p id="05e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该语句创建一个长度为2000的列表(因为总测试数据是2000或10000的20%)，并用“A”填充它。由于99%的样本只是一个类，所以我们使用<a class="ae mw" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html" rel="noopener ugc nofollow" target="_blank">准确度分数</a>得到99%的准确度。</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="fd12" class="kz la iq mo b gy ms mt l mu mv">accuracy_score(ytest,pred)</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c006f1627734cb655c4384571f6517e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*_FXGtQE4xXbwMLmu1LpRrw.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">“pred”列表的混淆矩阵</figcaption></figure><h2 id="3279" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">如何处理不平衡的数据集？</h2><p id="6e71" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">有许多方法可以处理不平衡的数据集。有些要求你有领域知识，有些使用不同的算法来增加少数类的实例(过采样)和减少多数类的实例(欠采样)。</p><ul class=""><li id="c5a0" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">通过领域知识，您可以将多数类拆分成多个类，也可以将不同的少数类合并成一个具有多个实例的类。</li><li id="2c16" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><a class="ae mw" href="https://imbalanced-learn.org/" rel="noopener ugc nofollow" target="_blank">不平衡学习</a>是一个python库，它有不同的欠采样和过采样方法。</li><li id="15b8" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">另一种方法是在创建机器学习模型对象时为不同的类设置权重。检查class_weights如何在<a class="ae mw" href="https://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>上工作。</li><li id="19e3" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><a class="ae mw" href="https://imbalanced-learn.org/" rel="noopener ugc nofollow" target="_blank">不平衡学习</a>中的一些过采样方法有<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html?highlight=smote#imblearn.over_sampling.SMOTE" rel="noopener ugc nofollow" target="_blank"> SMOTE </a>、<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html" rel="noopener ugc nofollow" target="_blank">randomversampler</a>、<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.BorderlineSMOTE.html" rel="noopener ugc nofollow" target="_blank">borderline mote</a>、<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.KMeansSMOTE.html" rel="noopener ugc nofollow" target="_blank">kmeansmote</a>和<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html" rel="noopener ugc nofollow" target="_blank"> ADASYN </a>。</li><li id="bea2" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><a class="ae mw" href="https://imbalanced-learn.org/" rel="noopener ugc nofollow" target="_blank">不平衡学习</a>中的欠采样方法有<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.ClusterCentroids.html" rel="noopener ugc nofollow" target="_blank"> ClusterCentroids </a>、<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.CondensedNearestNeighbour.html" rel="noopener ugc nofollow" target="_blank">condensed nearest neighborhood</a>、<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.EditedNearestNeighbours.html" rel="noopener ugc nofollow" target="_blank">edited nearest neighborhoods</a>、<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.NearMiss.html" rel="noopener ugc nofollow" target="_blank"> NearMiss </a>、<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.OneSidedSelection.html" rel="noopener ugc nofollow" target="_blank">单侧选择</a>、<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html" rel="noopener ugc nofollow" target="_blank"> RandomUnderSampler </a>、<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.TomekLinks.html" rel="noopener ugc nofollow" target="_blank">tomklinks</a>和<a class="ae mw" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.NeighbourhoodCleaningRule.html" rel="noopener ugc nofollow" target="_blank">邻域清洗规则</a></li></ul></div></div>    
</body>
</html>