<html>
<head>
<title>Reinforcing the Science Behind Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化强化学习背后的科学</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/reinforcing-the-science-behind-reinforcement-learning-d2643ca39b51?source=collection_archive---------2-----------------------#2020-08-20">https://pub.towardsai.net/reinforcing-the-science-behind-reinforcement-learning-d2643ca39b51?source=collection_archive---------2-----------------------#2020-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a22a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，强化学习</h2><div class=""/><div class=""><h2 id="0a35" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">强化学习，Q学习，贝尔曼方程。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/20b21c1fa194bc3f5155e1ac1a8c9432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkXetk7jXin9KbDt2zeY_Q.png"/></div></div></figure><p id="8000" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你在禁闭中感到无聊，你决定玩电脑游戏来打发时间。</p><p id="1723" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你推出了象棋，选择和电脑对战，你输了！</p><p id="5d3a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但这是怎么发生的呢？你怎么能输给一台50年前就存在的机器呢？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lz"><img src="../Images/6a1cfa9543cddbdd5986e34455b9763d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lVHoz81NvTXyxMsx"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">由<a class="ae me" href="https://unsplash.com/@maqov?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">皮奥特·马科夫斯基</a>在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="6d20" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这就是<strong class="lf jd">强化学习的神奇之处。</strong></p><p id="aafc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">强化学习</strong>位于<strong class="lf jd">机器学习的保护伞下。他们的目标是在复杂的动态环境中开发智能行为。如今，由于人工智能的范围正在极大地扩展，我们可以很容易地找到它们在我们周围的重要性。从<em class="mf">自动驾驶、推荐搜索引擎、电脑游戏到机器人技能，</em>人工智能正在发挥至关重要的作用。</strong></p><h1 id="2a9b" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">巴甫洛夫条件反射</h1><p id="cfb9" class="pw-post-body-paragraph ld le it lf b lg my kd li lj mz kg ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">当我们想到人工智能时，我们有一种思考未来的感觉，但我们的想法把我们带回了19世纪晚期，<em class="mf">伊凡·巴甫洛夫</em>，一位俄罗斯生理学家正在研究狗的唾液分泌效应。他很想知道当狗看到食物时会分泌多少唾液，但是，在进行实验时，他注意到狗甚至在看到任何食物之前就会分泌唾液。在实验得出结论后，巴甫洛夫会在给它们喂食前摇铃，不出所料，它们又开始流口水了。他们行为背后的原因可能是他们的学习能力，因为他们知道铃响后，他们会被喂食。另一件要思考的事情是，狗分泌唾液不是因为铃声响了，而是因为过去的经验告诉它食物会跟着铃声走。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/624e8d60b3a20b6a4700d1c83e62b22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8nMYL5_2S1RrBEFb"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">照片由<a class="ae me" href="https://unsplash.com/@enginakyurt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> engin akyurt </a>在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><h1 id="79b4" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">什么是强化学习？</h1><p id="96ed" class="pw-post-body-paragraph ld le it lf b lg my kd li lj mz kg ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">强化学习是机器学习技术的一部分，它使人工智能代理能够与环境交互，从而从自己的动作和经验序列中学习。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/91ca1d0071c6c7fe2ad028d6ce82e78b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*oEpHUSwK6gQXZ25m.jpeg"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated"><a class="ae me" href="https://blog.exxactcorp.com/wp-content/uploads/2020/03/1.png" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="8a59" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了便于说明，假设你被困在一个孤岛上。你可以期待自己先崩溃，但是，别无选择，你将开始为生存而战。你会找一个地方打猎，你会找一个地方睡觉，你会检查吃什么和避免什么。如果你呆在一个安全的地方，你会注意到这是你必须采取的正确行动，同时，如果你吃了一些导致腹泻的动物，你会避免在未来吃这些动物。随着时间的推移，你的行为会变得更好，通过学习，你会很容易适应新的环境。强化学习遵循相同的方法，其中我们期望代理人体验新环境，通过发现错误和奖励来跟踪其行为和后果，并学习变得更好或以最大化奖励为目标。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nf"><img src="../Images/723c159b54961ab39ca86e347a5192bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0ZMQsaZ3eJ72BE9Q"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">阿列克斯·达尔伯格在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="f14b" class="ng mh it bd mi nh ni dn mm nj nk dp mq lm nl nm ms lq nn no mu lu np nq mw iz bi translated">但是，它与监督学习相比如何呢？</h2><p id="3c9e" class="pw-post-body-paragraph ld le it lf b lg my kd li lj mz kg ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">可以使用监督学习方法来代替强化学习技术。但是，为此，我们需要一个真正大的数据集来构成每一个行动及其后果。它的下一个不利结果将是有限的学习，假设如果跟踪最佳球员的行动，但他仍然不完美，并跟随他的行动，机器可能会变得像他一样伟大，但不能超过他的分数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/f97f2a3cfdbe2949e764ee11cbd04032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IkEO6K_cbmdyS8kR030m0w.png"/></div></div></figure><h2 id="286a" class="ng mh it bd mi nh ni dn mm nj nk dp mq lm nl nm ms lq nn no mu lu np nq mw iz bi translated">它是如何反对无监督学习的？</h2><p id="01a9" class="pw-post-body-paragraph ld le it lf b lg my kd li lj mz kg ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">在无监督学习中，输入和输出之间没有直接联系，而是旨在识别模式，相反，强化学习是从过去输入提供的输出中学习。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/5a27a381696ef0d7a2955096f170ccb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSRoQNRC2MbHzzVVk1ls1A.png"/></div></div></figure><h2 id="af3f" class="ng mh it bd mi nh ni dn mm nj nk dp mq lm nl nm ms lq nn no mu lu np nq mw iz bi translated">那么，是深度学习吗？</h2><p id="3ca1" class="pw-post-body-paragraph ld le it lf b lg my kd li lj mz kg ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">深度学习无可争议地属于机器学习的范畴，能够计算需要类似人类智能的复杂问题。</p><p id="0241" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">维恩图显示了所有机器学习技术之间的关系，根据<a class="ae me" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem#:~:text=In%20the%20mathematical%20theory%20of,given%20function%20space%20of%20interest.&amp;text=Most%20universal%20approximation%20theorems%20can%20be%20parsed%20into%20two%20classes." rel="noopener ugc nofollow" target="_blank">通用逼近定理</a> (UAT)，我们可以使用神经网络解决任何问题，但这些不一定是每个问题的最佳解决方案，因为它们需要处理大量数据，并且通常难以解释。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/557fb9456d26d7694204551fa035397a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*StEZbf7HtFrGxxIe.png"/></div></div></figure><p id="ade3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">分析该图，它表明我们不需要对每个强化学习问题都使用深度学习，这澄清了一个神话，即<strong class="lf jd">它不仅仅依赖于深度学习</strong>。</p><h1 id="7ac7" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">强化学习是如何工作的？</h1><p id="f309" class="pw-post-body-paragraph ld le it lf b lg my kd li lj mz kg ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">在强化学习中，我们的目标是主体和环境的相互作用。</p><ul class=""><li id="90ae" class="nt nu it lf b lg lh lj lk lm nv lq nw lu nx ly ny nz oa ob bi translated">一个<strong class="lf jd"> Agent </strong>可以看作是“解”，是我们期望做出决策来解决决策问题的计算机程序。</li><li id="2e26" class="nt nu it lf b lg oc lj od lm oe lq of lu og ly ny nz oa ob bi translated">一个<strong class="lf jd">环境</strong>可以被认为是“问题”,这是代理所做的决定被执行的地方。</li></ul><p id="b090" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">例如，在国际象棋比赛的情况下，我们可以认为代理人是一个球员和环境构成董事会和竞争者。</p><p id="19b4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这两个部分在某种程度上是相互依赖的，即主体试图根据环境的影响来调整其行为，而环境对主体的行为作出反应。</p><p id="19d3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">环境受到一组变量的约束，这些变量通常与决策问题有关。所有可能值的集合可以被视为<strong class="lf jd">状态空间</strong>。<strong class="lf jd">状态</strong>是状态空间的一部分，即变量取值。</p><p id="d1f8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在每个状态下，环境有权向代理提供一组动作，它应该从中选择一个。代理试图使用这些动作来影响环境，并且环境可以作为对代理动作的响应而改变状态。<strong class="lf jd">转移函数</strong>就是追踪这些关联的东西。</p><p id="e155" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">环境根据个体的行为奖励或惩罚个体。<strong class="lf jd">奖励</strong>是在代理人的最后一次行动有助于实现有利目标时提供的积极反馈。<strong class="lf jd">惩罚</strong>是如果主体的最后一个动作导致偏离目标，环境提供的负反馈。代理人的目标是最大化整体回报，并不断使其行动更好，以达到预期的最终结果。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4d680c5ba1d04396b7874f78c70c0944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/0*_I47FWkOwADUUWI4.jpeg"/></div></figure><p id="475d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">强化学习需要的另一件事是大量的训练时间，因为直到一集(游戏)结束，奖励才会透露给代理人。例如，如果我们的计算机正在与我们下棋，并且它赢了，那么它将会得到奖励(因为我们期望的结果是<em class="mf">赢</em>),但是它仍然需要弄清楚它是因为哪些动作而得到奖励的，这只有在给它大量训练时间和数据的情况下才能实现。</p><h1 id="f282" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">强化学习是如何学习的？(Q-学习)</h1><blockquote class="oi"><p id="e4d3" class="oj ok it bd ol om on oo op oq or ly dk translated">目标:最大化总报酬</p></blockquote><figure class="ot ou ov ow ox kw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/a27373d5c4a83cf3cd99b4013d978b03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*4al_mLV8Nm92Fb5oCoGWKg.png"/></div></figure><p id="0cbe" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们期望，回报能尽早到来，以使我们的训练更快，从而快速达到预期的结果。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f2a716c39e2a7786c861e6e5ae6c1997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*OcDYigE9fU8F9BaCHFUsRQ.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">理想情况</figcaption></figure><p id="39b2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但是，在实际情况中，我们会遇到延迟奖励，为了惩罚延迟奖励，我们将引入折扣因子()。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/c0eec37c568cacbf7d89f041db11c749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*LKG1kNcfzJeXOlbX-0_KhA.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">真实案例</figcaption></figure><p id="60cc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在真实情况下，当我们向右移动时，不确定性会增加。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/afec34c5d1bd9c1e1d3c620e1d55035f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bkMz0bvjXs55bTbm.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">q学习</figcaption></figure><h2 id="dbe3" class="ng mh it bd mi nh ni dn mm nj nk dp mq lm nl nm ms lq nn no mu lu np nq mw iz bi translated">贝尔曼方程</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/584e93dd0603bade9b382aabf3e7ccbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*rNZL69DxOS2E3HMz6I8ppA.png"/></div></figure><p id="f4c3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们的目标是最大化回报，或者我们可以说最小化错误(损失)。</p><p id="dbb4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了最小化损失，我们可以使用均方误差损失来实现梯度下降。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/39d2e057306358e50d5e7c60d2b93a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cW16PGIxHc84ca9dQ4ICPA.png"/></div></div></figure><h1 id="a0fb" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">勘探v/s开发权衡</h1><p id="5150" class="pw-post-body-paragraph ld le it lf b lg my kd li lj mz kg ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">强化学习的其他有趣组成部分是<strong class="lf jd">探索</strong>和<strong class="lf jd">利用</strong>。为了快速获得回报，代理必须遵循过去的经验。但是为了检测这样的动作，它必须首先尝试动作。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/25ec9c417587d2958b727c7f09f8f18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZrXQZutNFmuW7gbN.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">勘探与开采，<a class="ae me" href="https://prakhartechviz.blogspot.com/2019/02/exploration-exploitation-dilemma-rl.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="a36e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简而言之，为了获得快速的回报，一个代理人必须利用T7，但是它也应该探索使它的行为更好，因此这可能有助于它获得更好的回报。</p><p id="2bb5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们回到岛上，你有三个钓鱼地点，每个地点是三种鱼的家园，地点1是有毒的黑色鱼的栖息地，地点2是美味且营养丰富的橙色鱼的家园，地点3是营养和味道最好的灰色鱼。我们的目标是不吃黑鱼，试着吃灰色的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/195002ea7e37568da6f50de1085c80e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z48419hvCocx62HWYL69CQ.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">Spot1 vs Spot2 vs Spot3</figcaption></figure><p id="d82f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们假设，在第一天，你选择了地点1钓鱼，结果吃了一条黑鱼，拉肚子了。在第二天，你到达了第二点，最后吃了一顿美味的晚餐。现在，你的本能将试图利用你选择的道路，即通往第二点的道路，因为根据你过去的经验，第二点似乎是一个更好的政策。因此，你的思维会停留在一个为了适度奖励而牺牲的政策上。</p><blockquote class="oi"><p id="9381" class="oj ok it bd ol om on oo op oq or ly dk translated"><strong class="ak">探索</strong>:帮助你尝试各种动作；一开始很好。</p><p id="c2cc" class="oj ok it bd ol om on oo op oq or ly dk translated"><strong class="ak">开发</strong>:从过去的好经验中取样；需要内存空间；结尾很好</p></blockquote><h1 id="bd8c" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki pf kj ms kl pg km mu ko ph kp mw mx bi translated">结论</h1><p id="a541" class="pw-post-body-paragraph ld le it lf b lg my kd li lj mz kg ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated">希望这篇文章能帮助你以最好的方式理解强化学习，并帮助你实践它。</p><p id="4b10" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一如既往，非常感谢您的阅读，如果您觉得这篇文章有用，请分享！</p></div><div class="ab cl pi pj hx pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="im in io ip iq"><p id="fcd6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请随意连接:</p><blockquote class="pp pq pr"><p id="004f" class="ld le mf lf b lg lh kd li lj lk kg ll ps ln lo lp pt lr ls lt pu lv lw lx ly im bi translated">领英~<a class="ae me" href="https://www.linkedin.com/in/dakshtrehan/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/dakshtrehan/</a></p><p id="97ee" class="ld le mf lf b lg lh kd li lj lk kg ll ps ln lo lp pt lr ls lt pu lv lw lx ly im bi translated">insta gram ~<a class="ae me" href="https://www.instagram.com/_daksh_trehan_/" rel="noopener ugc nofollow" target="_blank">https://www.instagram.com/_daksh_trehan_/</a></p><p id="53e1" class="ld le mf lf b lg lh kd li lj lk kg ll ps ln lo lp pt lr ls lt pu lv lw lx ly im bi translated">github ~<a class="ae me" href="https://github.com/dakshtrehan" rel="noopener ugc nofollow" target="_blank">https://github.com/dakshtrehan</a></p></blockquote><p id="b44b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">关注更多机器学习/深度学习博客。</p><blockquote class="pp pq pr"><p id="63ef" class="ld le mf lf b lg lh kd li lj lk kg ll ps ln lo lp pt lr ls lt pu lv lw lx ly im bi translated">中等~<a class="ae me" href="https://medium.com/@dakshtrehan" rel="noopener">https://medium.com/@dakshtrehan</a></p></blockquote><h1 id="d8ee" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">想了解更多？</h1><p id="751d" class="pw-post-body-paragraph ld le it lf b lg my kd li lj mz kg ll lm na lo lp lq nb ls lt lu nc lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/detecting-covid-19-using-deep-learning-262956b6f981" rel="noopener" target="_blank">利用深度学习检测新冠肺炎</a></p><p id="f06e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/the-inescapable-ai-algorithm-tiktok-ad4c6fd981b8" rel="noopener" target="_blank">无法逃脱的人工智能算法:抖音</a></p><p id="905a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/an-insiders-guide-to-cartoonization-using-machine-learning-ce3648adfe8" rel="noopener">使用机器学习的卡通化内幕指南</a></p><p id="a526" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/@dakshtrehan/why-are-you-responsible-for-george-floyds-murder-delhi-communal-riots-4c1edb7acbc5" rel="noopener">为什么你要为乔治·弗洛伊德的谋杀和德里的骚乱负责？</a></p><p id="804d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/decoding-science-behind-generative-adversarial-networks-4d188a67d863" rel="noopener">解码生成性对抗网络背后的科学</a></p><p id="202e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/understanding-lstms-and-gru-s-b69749acaa35" rel="noopener">理解LSTM和GRU的</a></p><p id="f643" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/recurrent-neural-networks-for-dummies-8d2c4c725fbe" rel="noopener">虚拟递归神经网络</a></p><p id="2c32" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/convolutional-neural-networks-for-dummies-afd7166cd9e" rel="noopener">虚拟卷积神经网络</a></p><p id="a8b1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/diving-deep-into-deep-learning-f34497c18f11" rel="noopener">深入钻研深度学习</a></p><p id="925c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/why-choose-random-forest-and-not-decision-trees-a28278daa5d" rel="noopener">为什么选择随机森林而不是决策树</a></p><p id="5ccf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/@dakshtrehan/clustering-what-it-is-when-to-use-it-a612bbe95881" rel="noopener">聚类:是什么？什么时候用？</a></p><p id="1ede" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从k个最近邻居开始你的ML之旅</p><p id="880a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/swlh/things-you-never-knew-about-naive-bayes-eb84b6ee039a" rel="noopener">朴素贝叶斯解释了</a></p><p id="9a16" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/analytics-vidhya/activation-functions-explained-8690ea7bdec9" rel="noopener">激活功能说明</a></p><p id="6a1b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/parameters-optimization-explained-876561853de0" rel="noopener" target="_blank">参数优化说明</a></p><p id="1ba6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c" rel="noopener" target="_blank">梯度下降解释</a></p><p id="ecbc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/logistic-regression-explained-ef1d816ea85a" rel="noopener" target="_blank">逻辑回归解释</a></p><p id="01e0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/linear-regression-explained-f5cc85ae2c5c" rel="noopener">线性回归解释</a></p><p id="b780" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/datadriveninvestor/determining-perfect-fit-for-your-ml-model-339459eef670" rel="noopener">确定最适合您的ML模型</a></p><blockquote class="pp pq pr"><p id="27c5" class="ld le mf lf b lg lh kd li lj lk kg ll ps ln lo lp pt lr ls lt pu lv lw lx ly im bi translated">干杯</p></blockquote></div></div>    
</body>
</html>