<html>
<head>
<title>Vendor-agnostic Setup for Running ML &amp; DL Experiments With GPU Support</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在GPU支持下运行ML &amp; DL实验的独立于供应商的设置</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/vendor-agnostic-setup-for-running-ml-dl-experiments-with-gpu-support-1a6ffa6f673d?source=collection_archive---------2-----------------------#2022-06-11">https://pub.towardsai.net/vendor-agnostic-setup-for-running-ml-dl-experiments-with-gpu-support-1a6ffa6f673d?source=collection_archive---------2-----------------------#2022-06-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1416b37dc58609c706c7788d55012db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uZmMWkUyx7kRj89s"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">汤姆·温克尔斯在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="342a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着许多新兴解决方案的出现，如AWS Sagemaker、Microsoft Azure Machine Learning Studio、Google Cloud AI Platform等，考虑到成本限制和用例，选择一个解决方案可能会非常困难。</p><p id="19a5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用云供应商解决方案有一些优点和缺点:<br/>优点:<br/> -一个紧密集成的平台，用于数据访问、IAM、培训、测试和部署ML &amp; DL工作负载<br/> -像AWS Sagemaker这样的解决方案支持开箱即用的多个内核配置，可用于在不同的环境配置上运行同一台笔记本电脑<br/> -云支持可以方便地解决平台问题<br/> -一键式环境创建&amp;作为托管服务，这些平台通常很少出现基础设施管理问题</p><p id="6072" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">缺点:<br/> -此类托管解决方案的定价可能高于加速硬件(GPU)虚拟机。<br/> -所创建的解决方案和管道将主要依赖于供应商，基于所使用的托管服务或工具。这引入了供应商锁定，使得评估或转向不同供应商的云解决方案变得困难。</p><h1 id="bba5" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">独立于供应商的设置</strong></h1><p id="26cd" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们可以在同一台装有卷的机器上使用Docker容器运行多个工作负载，这有助于快速原型开发和实验。这种设置可以通过系统化和集成各种开源解决方案来发展，如Kubeflow、用于编排的Flytelab、用于模型库和版本控制的MLFlow、用于分布式培训的Horovod或Ray等。</p><p id="b05e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">优点:<br/> -较小的工作负载可以共享相同的虚拟机及其资源(包括GPU)。<br/> -使用挂载的卷，相同的文件可以在相同虚拟机上托管的所有容器中访问。对于ML &amp; DL库，我们也可以使用像venv或conda这样的环境管理器，但是通常在这些环境中管理包会很麻烦。我们还可以定制每个容器的python版本，而不会对基本VM或其他容器产生任何影响。<br/> -将基础映像拉至虚拟机的一次性等待时间:下载完所有依赖项后，生成另一个容器只需不到几秒钟，相比之下，AWS Sagemaker实例每次分配资源需要大约2分钟。<br/>-GPU加速虚拟机的成本通常低于托管平台的成本，从长远来看，这可能会带来巨大的收益。<br/> -无供应商锁定:通过将docker映像推送到私有注册表并将其拉至不同云供应商的虚拟机，自由移动您的工作负载。</p><p id="1615" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">反对意见:<br/> -不了解基础设施的数据科学家可能更喜欢托管平台，而不是管理基础设施来进行试验和部署。<br/> -设置警报、服务和基础设施监控需要额外的工作，这对任何生产系统都是必不可少的。</p><h1 id="8f85" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">设置独立于供应商的设置的步骤:</h1><p id="a569" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">步骤1 : <br/>根据各自的工作负载需求，为任何基于Linux的虚拟机提供所需的资源。例如，我们将假设我们想要建立一个GPU加速实验平台。我们将需要一台配置了GPU的虚拟机，例如AWS或NCasT4_v3系列上的G4系列，或者一台配有Nvidia t4、16 GB内存、4个vCPUs和125 GB永久存储的GCP计算引擎。</p><p id="9129" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">步骤2 : <br/>安装并更新操作系统正常运行所需的所有开发依赖项。</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">安装和更新操作系统的开发依赖项</figcaption></figure><p id="841f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第3步:<br/>安装CUDA工具包以访问驱动程序，从而高效地使用GPU<br/>请跳过这一步以防:<br/> -虚拟机没有配备Nvidia GPU<br/>-基础映像操作系统是定制的，并且已经支持CUDA</p><p id="1950" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">前往<a class="ae kc" href="https://developer.nvidia.com/cuda-downloads" rel="noopener ugc nofollow" target="_blank">https://developer.nvidia.com/cuda-downloads</a>，选择合适的选项</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><a href="https://developer.nvidia.com/cuda-downloads"><div class="gh gi mk"><img src="../Images/96ad0e188280bad80cafbd8919351cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zOb8j5RhAKel4EVVA2jsgw.png"/></div></a><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">选择适当的基础架构配置来获取安装CUDA驱动程序的命令(来源:<a class="ae kc" href="https://developer.nvidia.com/cuda-downloads" rel="noopener ugc nofollow" target="_blank">https://developer.nvidia.com/cuda-downloads</a></figcaption></figure><p id="ae20" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于高级用户来说，如果有任何依赖或特定的用例，可以随意进入文档库更改CUDA版本并安装相同的版本。如果对CUDA版本没有偏好，上述过程应该可以正常工作。<br/>运行Nvidia-smi命令验证Cuda安装是否正确</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/9a644e9b7222fae364441de4a4eae8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R31IslIpepgNleMB5ylvpw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">nvidia-smi输出(来源:作者)</figcaption></figure><p id="63f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">CUDA安装故障排除:<br/>确保将CUDA_HOME和LD_LIBRARY_PATH添加到正确调用nvidia-smi命令的路径中</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">CUDA安装常见错误配置的故障排除</figcaption></figure><p id="9a27" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">安装Nvidia cuDNN(可选):<br/> <a class="ae kc" href="https://developer.nvidia.com/cudnn" rel="noopener ugc nofollow" target="_blank"> NVIDIA cuDNN </a>是一个用于深度神经网络的GPU加速原语库。如果我们需要运行深度学习和基于神经网络的工作负载，建议在基本虚拟机上安装cuDNN。<br/>前往https://developer.nvidia.com/rdp/cudnn-download<a class="ae kc" href="https://developer.nvidia.com/rdp/cudnn-download" rel="noopener ugc nofollow" target="_blank"/>接受条款和条件。下载最新版本，以防工作负载没有冻结需求。<br/>注意:cuDNN的安装遵循文档:<a class="ae kc" href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-deb" rel="noopener ugc nofollow" target="_blank">链接</a> <br/>安装依赖关系<br/> 8.x.x.x-1+cudaX时，确保匹配从网站下载的CUDA和cuDNN包deb的版本。Y &lt; - 8.x.x.x-1指下载的cuDNN版本和cudaX。y指以前安装的CUDA版本。要确认Cuda版本，请运行Nvidia-smi命令。(以上版本为11.6) <br/>运行以下命令，确保cuDNN安装正确:</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mm"><img src="../Images/956274a786eca7cbf1eb6f5320510f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJCY_Fx4dyt0-6BZujIHHQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">cuDNN安装的验证(来源:作者)</figcaption></figure><p id="95bc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第四步:<br/>安装Docker和Docker-compose -</p><pre class="me mf mg mh gt mn mo mp mq aw mr bi"><span id="ea24" class="ms lc iq mo b gy mt mu l mv mw">#Docker installation<br/>curl <a class="ae kc" href="https://get.docker.com" rel="noopener ugc nofollow" target="_blank">https://get.docker.com</a> | sh \<br/>  &amp;&amp; sudo systemctl --now enable docker</span><span id="e3ff" class="ms lc iq mo b gy mx mu l mv mw">#Docker-compose installation<br/>cd /usr/local/bin &amp;&amp; sudo rm -rf docker-compose<br/>sudo curl -L "<a class="ae kc" href="https://github.com/docker/compose/releases/download/v2.2.3/docker-compose-linux-x86_64" rel="noopener ugc nofollow" target="_blank">https://github.com/docker/compose/releases/download/v2.2.3/docker-compose-linux-x86_64</a>" -o /usr/local/bin/docker-compose<br/>sudo chmod +x docker-compose</span></pre><p id="2906" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以使用docker-compose为我们的训练容器设置多个依赖项，如mock DB。这在使用docker-compose通过快速原型制作评估新解决方案时会派上用场。</p><p id="c4ca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">安装Nvidia-Docker2 : <br/>在有GPU加速工作负载的情况下，我们需要系统满足以下配置- <br/> 1。Docker引擎需要额外的驱动程序来运行GPU上的工作负载。<br/> 2。加速工作负载容器需要在容器上安装Cuda驱动程序，以利用GPU处理。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><a href="https://github.com/NVIDIA/nvidia-docker"><div class="gh gi my"><img src="../Images/5cd42c0fd48a45abc555865d18f40583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*0Ugi8kcY0MMi3fCkLJdslA.png"/></div></a><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">Nvidia-docker2使用情况(来源:<a class="ae kc" href="https://github.com/NVIDIA/nvidia-docker" rel="noopener ugc nofollow" target="_blank">https://github.com/NVIDIA/nvidia-docker</a></figcaption></figure><p id="14a3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行以下命令，在虚拟机实例上安装nvidia-docker2。</p><pre class="me mf mg mh gt mn mo mp mq aw mr bi"><span id="f012" class="ms lc iq mo b gy mt mu l mv mw">#adds the distribution to OS for installing drivers for docker<br/>distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \<br/>&amp;&amp; curl -s -L<a class="ae kc" href="https://nvidia.github.io/nvidia-docker/gpgkey" rel="noopener ugc nofollow" target="_blank"> https://nvidia.github.io/nvidia-docker/gpgkey</a> | sudo apt-key add — \<br/>&amp;&amp; curl -s -L<a class="ae kc" href="https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list" rel="noopener ugc nofollow" target="_blank"> https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list</a> | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><span id="83e1" class="ms lc iq mo b gy mx mu l mv mw">#Update and install nvidia-docker2<br/>sudo apt-get update<br/>sudo apt-get install -y nvidia-docker2</span><span id="2e78" class="ms lc iq mo b gy mx mu l mv mw">#restart docker engine to load new config<br/>sudo systemctl restart docker</span></pre><p id="6a01" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以通过以下命令来验证安装:</p><pre class="me mf mg mh gt mn mo mp mq aw mr bi"><span id="d144" class="ms lc iq mo b gy mt mu l mv mw">sudo docker run -rm -gpus all nvidia/cuda:11.0-base nvidia-smi</span></pre><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mz"><img src="../Images/69319637641cce6d14c9e6f6d2da9984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DpVgxLIG3kJhC_CiXJQewQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">Nvidia-Docker2安装的验证(来源:作者)</figcaption></figure><p id="06bc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">至此，我们已经完成了在平台无关的虚拟机上运行和管理GPU工作负载所需的基本设置。</p><h1 id="b31a" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">根据要求创建自定义docker图像:</h1><p id="c2f5" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">如上所述，如果我们想要运行GPU加速的工作负载，容器必须有Cuda工具包和驱动程序来与GPU通信。</p><p id="de85" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可以通过获得docker版本的基础映像来实现，该版本已经预装了Cuda驱动程序，我们可以通过为python和OS安装软件包来进一步定制。要了解更多关于使用容器处理数据科学工作负载的信息，请查看博客:<a class="ae kc" href="https://blog.devgenius.io/breaking-down-docker-for-data-science-6fabb752b087" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="cb7c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:对于在虚拟机上运行使用GPU进行加速的容器化工作负载，docker run命令如下所示</p><pre class="me mf mg mh gt mn mo mp mq aw mr bi"><span id="9df7" class="ms lc iq mo b gy mt mu l mv mw"># Here — gpus all specify that all GPUs available are to be used for # accelerating workload<br/>sudo docker run — gpus all tensorflow/tensorflow:latest-gpu-jupyter</span></pre><h1 id="93e7" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">现在我们有了基本的设置，我们要去哪里？</h1><p id="1cd9" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">现在，您能够仅使用docker容器中的虚拟机运行平台无关的加速工作负载，它最终可以发展为基于定制需求和用例的更加自以为是的系统。请记住，如果需要，这些可以再次与托管开源产品集成，甚至与特定于云的工具集成。</p><p id="c1fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如(见图1)，容器可以通过为虚拟机或Kubernetes集群网络启用的私有VPC访问驻留在任何结构化或非结构化数据库中的数据。我们可以有一个具有GPU加速的单个虚拟机来运行多个较小的工作负载，或者一个强大的多GPU实例来运行大量数据的分布式培训。然后，这些可以与MLFlow集成，用于存储和管理ML工件及其培训和评估指标。基于首选的云环境，我们可以进一步将工件存储配置为s3存储桶、azure blob存储桶或GCP存储桶。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/6796f1b628d0f814cdb2277eb2df643b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ybv_WP2kXwAzzdsCIJVRmQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图1-平台无关的ML训练系统(来源:作者)</figcaption></figure><p id="23fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在部署模型后进一步(见图2 ),使用whylogs、nannyml等MLOps解决方案，我们可以基于模型推断指标折旧和漂移指标触发kubeflow管道。可以将管道配置为在Kubernetes上生成spot实例pod(比按需实例pod便宜),然后可以使用它在Ray集群上的相同环境中使用新的或修改的数据重复训练过程，以进行分布式训练。培训完成后，资源被释放出来，这有助于降低长期成本。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/d2a708e975c0826be6533436ef5e9a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvYd_bK8i-xlrPvX20hxrA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图2-平台无关的ML培训管道(来源:作者)</figcaption></figure><p id="959c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有这些系统都需要一次性安装。还需要配置监控和自定义警报来提高管道的透明度。</p><h1 id="d58e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">总和与实质</h1><p id="b50f" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">ML架构师必须评估典型的“构建vs购买”场景的情况。如果我们计划在短期内进行更小规模的试验，购买和使用托管服务将提供更好的投资回报，因为构建定制平台必须证明其努力的合理性。在需要定制解决方案的长期基础上运行更大和更长的实验的情况下；在进行了成本和工作量分析之后，构建一个平台可能更适合于培训、部署、监控和长期维护生产中的ML模型。</p></div></div>    
</body>
</html>