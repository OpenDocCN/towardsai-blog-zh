<html>
<head>
<title>This AI takes a video and fills the missing pixels behind an object!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这个AI拍摄一个视频，并填充一个对象后面缺失的像素！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/this-ai-takes-a-video-and-fills-the-missing-pixels-behind-an-object-video-inpainting-9be38e141f46?source=collection_archive---------1-----------------------#2020-07-25">https://pub.towardsai.net/this-ai-takes-a-video-and-fills-the-missing-pixels-behind-an-object-video-inpainting-9be38e141f46?source=collection_archive---------1-----------------------#2020-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e388" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="40f1" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">视频修复—微软研究院</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/2b2be17f281bc83927f3bb3a639059b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nmoBe8fFgKvA9gtfkbS5bA.png"/></div></div></figure><p id="ef31" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这种人工智能可以填充移动物体背后的缺失像素，并以比当前最先进的方法更准确、更少模糊的方式重建整个视频！让我们看看这个人工智能到底有什么能力，以及研究人员是如何实现这一点的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/e2ebb50cc74651174b534bc1e0d26701.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*OP_AjIVaR0LThVW0_wXoyg.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">本文涉及的<a class="ae me" href="https://arxiv.org/abs/2007.10247" rel="noopener ugc nofollow" target="_blank">论文</a></figcaption></figure><p id="d1f1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">中国中山大学的研究人员最近发表了一篇被ECCV2020大会接受的论文，涵盖了一种新的视频修复技术，他们称之为“学习联合时空变换”。ECCV，或欧洲计算机视觉会议，是图像分析领域的顶级欧洲会议。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/8c74b60ba636fe5d4063502c0b24314b.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*B-a2vvedr_vhaCB4Lv1vyw.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/02037ebe6923970950876e77de1a4ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*_lIiACTuRDhOWmztnU2aOQ.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">图片来自<a class="ae me" href="https://arxiv.org/abs/2007.10247" rel="noopener ugc nofollow" target="_blank">报</a></figcaption></figure><p id="3547" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在图像分析中，视频修补是完成视频帧中缺失区域的过程。这是机器学习中一项具有挑战性的任务。当前最先进的方法使用注意力模型，以便通过从参考帧中搜索内容来找到帧的丢失像素。<br/>然后，逐帧重建整个视频。使用这种注意力技巧，我在<a class="ae me" href="https://www.youtube.com/watch?v=sMCHC7XFynM" rel="noopener ugc nofollow" target="_blank">之前的视频</a>中提到过，有一些问题。正如他们在论文中解释的那样，它可能会受到空间和时间维度上不一致的注意结果的影响。这通常会导致视频模糊和伪像，就像你在这张图片中看到的那样。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/74178cbbf4950b843655965a1017cb92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*XjmPS4Fm8LB0LU-JdUHZxg.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">图片来自<a class="ae me" href="https://arxiv.org/abs/2007.10247" rel="noopener ugc nofollow" target="_blank">论文</a></figcaption></figure><p id="230d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">他们提议使用一种叫做时空转换网络(STTN)的方法来代替注意力工作。简而言之，他们使用自我关注来同时填充所有输入帧中的缺失区域，在这些区域中前景的对象已经被移除。<br/>我在之前提到的的<a class="ae me" href="https://www.youtube.com/watch?v=sMCHC7XFynM" rel="noopener ugc nofollow" target="_blank">视频中也解释了注意力和自我注意力的区别。<br/>如果你有兴趣，下面的描述框里有这个视频的链接。然后，他们使用这种STTN的优化版本，该版本将邻近和远处的帧作为输入，并使用对抗训练同时填充所有输入帧中的缺失区域。</a></p><p id="18bf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个网络由三个主要部分组成:</p><p id="7090" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，有一个帧级编码器，顾名思义，它将视频的所有帧编码成三个1×1维的值，下一步将详细介绍。该编码器只是堆叠在一起的几个二维卷积层，旨在对每帧的低层像素的深层特征进行编码。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/74178cbbf4950b843655965a1017cb92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*XjmPS4Fm8LB0LU-JdUHZxg.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">图片来自<a class="ae me" href="https://arxiv.org/abs/2007.10247" rel="noopener ugc nofollow" target="_blank">纸</a></figcaption></figure><p id="53d6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后，这些输出被发送到一个多层、多头时空转换器，这些转换器被设计用来填充前面提到的所有输入帧中的空洞。但是，详细地说，单个变换器使用多个头部在各种各样的尺度上匹配查询(Q)和关键字(K ),使得检测由孔覆盖的相关区域的值(V)成为可能。在这些多头变压器中使用的各种各样的比例允许网络使用大尺寸的小块来完成静止的背景，以及捕捉视频中任何位置的深度对应，以便从较小的小块移动前景对象。所述小块都是通过在第一步中从所有帧中提取的空间小块中匹配查询和关键字来计算的。这是网络从所有输入帧中搜索连贯内容的关键部分。这一关键步骤中解释的一切都可以用这个短语来概括:学习深度编码空间中所有缺失区域的联合时空变换。</p><p id="815a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">而且这些变压器的头都是并行运行的，使得这个网络速度极快。最后一步是将转换器找到的多层特征发送到帧级解码器，以便根据这些特征重建视频帧。现在我们对它的工作原理有了更好的了解，让我们来看一些使用这种技术制作的惊人的例子。</p><div class="ks kt ku kv gt ab cb"><figure class="mi kw mj mk ml mm mn paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/732cd4cc1c5892699fe1a2e0a94c604a.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*eTRgqgaCT7ExwqBIIUxdPg.png"/></div></figure><figure class="mi kw mo mk ml mm mn paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/f985111123d03a4de19e9d90f97907df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*wFzzhXnaYs11DatSmY-ToA.png"/></div></figure><figure class="mi kw mp mk ml mm mn paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/991aacfb9de5d31749fc9b0082ffdf37.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*2CLE7BAVlxTct11ihEGnlA.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk mq di mr ms translated">图片来自<a class="ae me" href="https://arxiv.org/abs/2007.10247" rel="noopener ugc nofollow" target="_blank">纸</a></figcaption></figure></div><p id="5c90" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如上所述，它能够从视频中移除对象，并使用背景和时间信息填充其下的区域。只要看看下面的视频，看看在他们的方法中模糊有多不明显。他们还可以用同样的信息填充整个视频中固定的孔洞。</p><p id="f057" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">看看视频中的这些例子:</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mt mu l"/></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="3548" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果你喜欢我的工作并想支持我，我会非常感谢你在我的社交媒体频道上关注我:</p><ul class=""><li id="c478" class="nc nd it lf b lg lh lj lk lm ne lq nf lu ng ly nh ni nj nk bi translated">支持我最好的方式就是在<a class="ae me" href="https://medium.com/@whats_ai" rel="noopener"> <strong class="lf jd">中</strong> </a>关注我。</li><li id="599a" class="nc nd it lf b lg nl lj nm lm nn lq no lu np ly nh ni nj nk bi translated">订阅我的<a class="ae me" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> YouTube频道</strong> </a>。</li><li id="153b" class="nc nd it lf b lg nl lj nm lm nn lq no lu np ly nh ni nj nk bi translated">在<a class="ae me" href="https://www.linkedin.com/company/what-is-artificial-intelligence" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> LinkedIn </strong> </a>上关注我的项目</li><li id="1f6f" class="nc nd it lf b lg nl lj nm lm nn lq no lu np ly nh ni nj nk bi translated">一起学习AI，加入我们的<a class="ae me" href="https://discord.gg/SVse4Sr" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd"> Discord社区</strong> </a>，<em class="nq">分享你的项目、论文、最佳课程、寻找Kaggle队友等等！</em></li></ul></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="a214" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">参考文献</strong></p><p id="3bff" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">关注&amp;自我关注视频:</strong><a class="ae me" href="https://www.youtube.com/watch?v=sMCHC7XFynM" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=sMCHC7XFynM</a><br/><strong class="lf jd">论文:</strong><a class="ae me" href="https://arxiv.org/abs/2007.10247" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2007.10247</a></p></div></div>    
</body>
</html>