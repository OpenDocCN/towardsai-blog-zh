<html>
<head>
<title>BERT for QuestionAnswering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回答问题的伯特</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/bert-for-questionanswering-bfff793a4cd8?source=collection_archive---------0-----------------------#2020-03-29">https://pub.towardsai.net/bert-for-questionanswering-bfff793a4cd8?source=collection_archive---------0-----------------------#2020-03-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="io ip gp gr iq ir gh gi paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="gh gi ca"><img src="../Images/ae4b22834da97e30a60dc3d5c4c9c858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*caBgIfw7Veci38W1FY9Feg.jpeg"/></div></div><figcaption class="iy iz gj gh gi ja jb bd b be z dk translated">照片由<a class="ae jc" href="https://jhuo831alex.github.io/" rel="noopener ugc nofollow" target="_blank"> Alex(嘉豪)霍</a></figcaption></figure><h2 id="4f09" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><p id="4a2f" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">几个月前，我写了一篇关于<a class="ae jc" href="https://medium.com/@shwetabaranwal20/understanding-bert-b69ce7ad03c1" rel="noopener"> BERT </a>的中型文章，讨论了它的功能和用例，以及它通过Transformers的实现。在这篇文章中，我们将看看如何使用BERT根据给定的上下文使用拥抱脸中的<a class="ae jc" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">变形金刚</a>来回答我们的问题。</p><p id="0e9d" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">假设问的问题是:<strong class="kn jp"> <em class="lj">虚构的《肖邦》是谁写的？</em>《肖邦》是贾科莫·奥雷菲西创作的，1901年在米兰制作。所有的音乐都源自肖邦的作品。 </strong>那么伯特所期待的答案是<strong class="kn jp"> <em class="lj">贾科莫·奥利佛。</em> </strong></p><p id="c886" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">斯坦福问答数据集可以从这里下载，用于微调伯特:<a class="ae jc" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">https://rajpurkar.github.io/SQuAD-explorer/</a>。数据集采用JSON格式，需要转换为pandas数据框。转换后，我们需要三列。<code class="fe lk ll lm ln b">question</code>、<code class="fe lk ll lm ln b">context</code>和<code class="fe lk ll lm ln b">text</code>。<code class="fe lk ll lm ln b">answer_start</code>列给出了<code class="fe lk ll lm ln b">context</code>中答案的字符级开始位置，而我们需要答案的令牌级开始位置，因为输入是在令牌级进入模型的。例如:如果上下文是<em class="lj">“她叫亚历克斯。”</em>答案是<em class="lj">“Alex”，</em>则<code class="fe lk ll lm ln b">answer_start</code>的值为12，而我们需要从4开始定位。<code class="fe lk ll lm ln b">text</code>列中有答案文本，我们将使用该列来获取答案在上下文中的开始位置。</p><figure class="lp lq lr ls gt ir gh gi paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="gh gi lo"><img src="../Images/5912c8e2d9adb1fad07a6e408c982369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XgiB7WICjaO0yTSwb-WzxQ.png"/></div></div><figcaption class="iy iz gj gh gi ja jb bd b be z dk translated">SQAUD数据集转换为Pandas数据框</figcaption></figure><h2 id="2440" class="lt lu jf bd lv lw lx dn ly lz ma dp mb kw mc md me la mf mg mh le mi mj mk jl bi translated">输入和输出向量</h2><p id="fa7e" class="pw-post-body-paragraph kl km jf kn b ko ml kq kr ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li ij bi translated">问题和上下文被连接到一个由<code class="fe lk ll lm ln b">[SEP]</code>标记(标记id: 102)分隔的标记化向量中，并与标记类型id向量一起被馈送到BERT中，其中我们将0分配给问题向量，然后将1分配给上下文向量。这类似于两个文本的标记化，其中第一个文本是问题，第二个文本是上下文。使用的记号赋予器是来自<code class="fe lk ll lm ln b">bert-base-uncased</code>的预先训练的Bert记号赋予器。输出向量是来自输入标记化向量的答案的开始和结束位置。</p><p id="fecc" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">要获得答案的开始和结束位置，对<code class="fe lk ll lm ln b">text</code>进行标记化，并在输入标记向量中查找文本标记id向量，并注意输入标记向量的开始和结束索引，它与文本标记id向量相匹配。</p><p id="e7e1" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">下面是torch Dataloader的代码，它输出输入令牌向量、令牌类型、ids以及答案的开始和结束位置。</p><figure class="lp lq lr ls gt ir"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h2 id="2b2e" class="lt lu jf bd lv lw lx dn ly lz ma dp mb kw mc md me la mf mg mh le mi mj mk jl bi translated">模型</h2><p id="274d" class="pw-post-body-paragraph kl km jf kn b ko ml kq kr ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li ij bi translated">使用的模型是来自<code class="fe lk ll lm ln b">bert-base-uncased</code>的预训练伯特模型，后面是形状的线性密集层<code class="fe lk ll lm ln b">(hidden_size, 2)</code>。输入是问题上下文连接的标记id向量和标记类型id向量，输出是来自输入标记id的答案的开始和结束位置。使用的损失函数是<code class="fe lk ll lm ln b">nn.CrossEntropyLoss()</code>。总损失是答案的开始和结束位置的损失的总和。</p><figure class="lp lq lr ls gt ir"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h2 id="430a" class="lt lu jf bd lv lw lx dn ly lz ma dp mb kw mc md me la mf mg mh le mi mj mk jl bi translated">培养</h2><p id="8031" class="pw-post-body-paragraph kl km jf kn b ko ml kq kr ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li ij bi translated">所用令牌id的最大序列长度为512，批处理大小为8，在Google colab GPU上运行了2个时期。</p><figure class="lp lq lr ls gt ir"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h2 id="1d40" class="lt lu jf bd lv lw lx dn ly lz ma dp mb kw mc md me la mf mg mh le mi mj mk jl bi translated">估价</h2><p id="db8e" class="pw-post-body-paragraph kl km jf kn b ko ml kq kr ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li ij bi translated">来自小队的dev数据集被转换成pandas df并通过训练的模型运行。预测的开始和结束位置用于从输入令牌id向量中获取预测的答案文本，并与数据的<code class="fe lk ll lm ln b">text</code>(实际答案)列进行比较。</p><p id="9313" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果预测的开始和结束位置位于实际的开始和结束位置内，则认为是成功的。使用的dev数据集样本有454个数据点，其中349个数据点位于实际答案范围内，模型的验证准确率约为77%。</p><figure class="lp lq lr ls gt ir gh gi paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="gh gi ms"><img src="../Images/16b7fa3d3d5329e5d1287fb7e8b82673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57xwCDGVzUhcSln96746Lw.png"/></div></div><figcaption class="iy iz gj gh gi ja jb bd b be z dk translated">开发组中正确识别的答案示例</figcaption></figure><figure class="lp lq lr ls gt ir gh gi paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="gh gi mt"><img src="../Images/a640484d27c9dc63fd8ce5fd08f24ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yBvUwIqW_A7eK-q-xssQ2A.png"/></div></div><figcaption class="iy iz gj gh gi ja jb bd b be z dk translated">来自开发集的错误识别答案示例</figcaption></figure><h2 id="60bc" class="lt lu jf bd lv lw lx dn ly lz ma dp mb kw mc md me la mf mg mh le mi mj mk jl bi translated">代码Git repo</h2><div class="io ip gp gr iq mu"><a href="https://github.com/ShwetaBaranwal/BERT-for-QuestionAnswering" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jp gy z fp mz fr fs na fu fw jo bi translated">ShwetaBaranwal/回答问题的伯特</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">在GitHub上创建一个帐户，为ShwetaBaranwal/BERT-for-question-answering开发做贡献。</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">github.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni iw mu"/></div></div></a></div><h2 id="abc0" class="lt lu jf bd lv lw lx dn ly lz ma dp mb kw mc md me la mf mg mh le mi mj mk jl bi translated">参考</h2><div class="io ip gp gr iq mu"><a href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jp gy z fp mz fr fs na fu fw jo bi translated">BERT -变压器2.6.0文档</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">BERT模型是在BERT:用于语言理解的深度双向转换器的预训练中提出的…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">huggingface.co</p></div></div></div></a></div></div></div>    
</body>
</html>