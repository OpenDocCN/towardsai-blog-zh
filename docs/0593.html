<html>
<head>
<title>Evaluation Metrics for Textual Problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本问题的评估标准</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/evaluation-metrics-for-textual-problems-6e881feef5ad?source=collection_archive---------0-----------------------#2020-06-16">https://pub.towardsai.net/evaluation-metrics-for-textual-problems-6e881feef5ad?source=collection_archive---------0-----------------------#2020-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b860" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a></h2><div class=""/><div class=""><h2 id="92e7" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">为什么一开始就需要定义指标</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/49941a956c20275887824117f5142e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mQ8zYZIj7a6GuCw4"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><blockquote class="li lj lk"><p id="df22" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">如果你不知道如何证明一个模型是好是坏，这就好比你想得到某样东西却不知道它是什么。作为一名数据科学家工作了几年后，我坚信在早期阶段定义指标是非常重要的事情。</p></blockquote><p id="b915" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这个故事将涵盖几个文本指标。您也可以查看以下案例，了解其他评估指标</p><ul class=""><li id="076f" class="ml mm it lo b lp lq ls lt mi mn mj mo mk mp mh mq mr ms mt bi translated"><a class="ae lh" href="https://medium.com/towards-artificial-intelligence/evaluation-metrics-are-what-you-need-to-define-in-the-earlier-stage-99dbfae51472" rel="noopener">内部和外部评估指标</a></li><li id="0a54" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh mq mr ms mt bi translated"><a class="ae lh" href="https://medium.com/towards-artificial-intelligence/evaluation-metrics-for-classification-problems-e7442092bc5" rel="noopener">分类问题的度量</a></li><li id="b468" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh mq mr ms mt bi translated"><a class="ae lh" href="https://medium.com/towards-artificial-intelligence/evaluation-metrics-for-regression-problems-fff2ac8e3f43" rel="noopener">回归问题的度量</a></li></ul><h1 id="b41b" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">文本评估指标</h1><p id="a54c" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">在自然语言处理(NLP)领域，我们有许多下游任务，如翻译、文本识别和翻译。在这个故事中，我们将讲述:</p><ul class=""><li id="08b7" class="ml mm it lo b lp lq ls lt mi mn mj mo mk mp mh mq mr ms mt bi translated">文本生成:困惑</li><li id="1d4b" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh mq mr ms mt bi translated">翻译:BLEU</li><li id="c9a8" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh mq mr ms mt bi translated">文本识别:CER和WER</li><li id="acbf" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh mq mr ms mt bi translated">语言理解:胶水</li></ul><h2 id="1520" class="nw na it bd nb nx ny dn nf nz oa dp nj mi ob oc nl mj od oe nn mk of og np iz bi translated">困惑</h2><p id="654e" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">可能的任务:语言模型</p><p id="1cef" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">语言模型是指机器生成的文本如何类似于人类编写的文本。换句话说，给定k个前一个单词和正确的分数，生成k+1个令牌。你的困惑越低，你的模型就越好。</p><p id="8194" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">以“我爱NLP”为例。我们要计算得到“我”的概率，给定前一个词是“我”得到“爱”的概率，给定前一个词是“我”和“爱”得到“NLP”的概率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/766441386452e92394bbe3e55750090d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SBVjafsE1HdKl-pslxWQ_w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">“我爱NLP”的困惑</figcaption></figure><h2 id="30d9" class="nw na it bd nb nx ny dn nf nz oa dp nj mi ob oc nl mj od oe nn mk of og np iz bi translated">双语评估替角</h2><p id="8baf" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">可能的任务:神经机器翻译(NMT)，字幕生成(即Image2Text问题)</p><p id="7229" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">由于专业翻译的人工标注非常昂贵，因此引入BLEU来衡量候选文本(由机器翻译)和参考文本(由人类翻译)之间的差异。该值介于0和1之间，1是最好的分数。BLEU的计算涉及到n-gram精度和句子简洁惩罚的概念。</p><p id="2035" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">下面的例子将使用这个典型的例子来演示计算。候选是由机器学习模型翻译的文本，而参考1和2是由人类翻译的文本。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/9da4973b13b200bf1ad2a7f222859590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*jja09uLqnfSETH0s_tZIqQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">BLEU演示的典型示例(Papineni等人，2002年)</figcaption></figure><p id="284f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">首先，BLEU不是用uni-gram，而是用n-gram来计算分数。高的单字分数代表译文的完整性，而单字分数代表译文的流畅性。</p><p id="e46f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">下面的公式计算出j个参考字中存在的第I个字。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/6754fcc65f53f24bd50d7c4e0fc7a0c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e2rAUFnJXA9a9nmlm1NkFQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">Count wi:候选词中第I个单词的个数。Ref j_Count wi:第j个引用中的字数w</figcaption></figure><p id="7ea0" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在上面的例子中，对于参考文献1，我们可以得到“the”的答案是min(7，2) = 2。参考文献2的答案是min(7，1) = 1。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/911929942efec51033f10de3b16182a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AaV6W7Txqjqtr2zpXIeCHw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">参考中存在w的最大数量</figcaption></figure><p id="3fcb" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在上面的例子中，我们得到的答案是2，而公式是max(1，2)。在实际计算中，它将不仅是一元语法，而是多元语法。计算是一样的，但只是分别比较“猫”、“猫是”等等，而不是“这个”、“猫”、“是”。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/0bd58313cb4b7c22110f83f3af04d047.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*zf-XeewWSNkdXPpS2ejGRw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">修正的N-gram精度(Papineni等人，2002年)</figcaption></figure><p id="b0f4" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">由于公式计算，一个短句很容易得高分。因此Papineni等人提出对候选句子中调用BP的那些较少的词进行包含惩罚。</p><p id="730e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">如果候选词中的单词数与参考词相同，那么就没有罚分1。如果候选人是“猫”，而参考是“猫在垫子上”。所以r是6而c是2。以上面的例子为例，BP是e^(1- 6 / 2) = 7.3891。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/4b402209a916325e9c3e9b24b2f1c9fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*l4UMNL2qtbOoNprymTEpow.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">BP公式(Papineni等人，2002年)</figcaption></figure><p id="f857" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">最后，BLEU分数通过下式计算</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/b9e65d1c60b596bd63b910bc7f7ec373.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*2-X3ag3Pcx_yGfniLLuEXA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">蓝色配方奶粉(Papineni等人，2002年)</figcaption></figure><p id="2a22" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我们可以利用NLTK包来计算它，得分是0.5026。</p><pre class="ks kt ku kv gt oo op oq or aw os bi"><span id="27e0" class="nw na it op b gy ot ou l ov ow">from nltk.translate.bleu_score import sentence_bleu</span><span id="ab61" class="nw na it op b gy ox ou l ov ow">candidate = ['The', 'cat', 'sat', 'on', 'the', 'mat']<br/>reference = [['The', 'cat', 'is', 'on', 'the', 'mat']]<br/>weights = (0.34,0.33,0.33,0)</span><span id="6ab6" class="nw na it op b gy ox ou l ov ow">score = sentence_bleu(reference, candidate, weights)</span></pre><p id="50f5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">缺点是:</p><ul class=""><li id="6489" class="ml mm it lo b lp lq ls lt mi mn mj mo mk mp mh mq mr ms mt bi translated">不要考虑同义词。由于它只计算单词精确匹配，如果参考文本中不存在这些同义词，同义词将被视为不正确。</li><li id="fdb4" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh mq mr ms mt bi translated"><a class="ae lh" href="https://medium.com/@makcedward/nlp-pipeline-stop-words-part-5-d6770df8a936" rel="noopener">停用词</a>也有助于准确性。停用词(如a，an，the)没有太多意义，会在文本中反复出现。它可以提高精确度。</li></ul><h2 id="2c95" class="nw na it bd nb nx ny dn nf nz oa dp nj mi ob oc nl mj od oe nn mk of og np iz bi translated">字符错误率(CER)</h2><p id="0cfd" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">可能的任务:语音识别、光学字符识别(OCR)、手写识别。</p><p id="1ce1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">CER计算将一个单词转换成另一个单词的最小操作数<a class="ae lh" href="https://towardsdatascience.com/measure-distance-between-2-words-by-simple-calculation-a97cf4993305" rel="noopener" target="_blank"> Levenshtein距离</a>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/20baf981b753a430b6a991dae4df75fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*Pc0x99f6eRTpKxDmT_-3Sw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">CER公式</figcaption></figure><h2 id="f92c" class="nw na it bd nb nx ny dn nf nz oa dp nj mi ob oc nl mj od oe nn mk of og np iz bi translated">单词错误率(WER)</h2><p id="014b" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">可能的任务:语音识别、光学字符识别(OCR)、手写识别。</p><p id="9903" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">WER已经从<a class="ae lh" href="https://towardsdatascience.com/measure-distance-between-2-words-by-simple-calculation-a97cf4993305" rel="noopener" target="_blank">得出了Levenshtein距离</a>的实际值。它用参考单词计算最小距离。替换、删除和插入属于误差，WER是基于误差计算的。根据误差的编辑距离，公式将选择这些误差中的最小距离。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/8fc09f006eb0cd79a44f62054139598a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*qhipMWCdYaJ0bIsd6aD-Nw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">WER公式</figcaption></figure><p id="97e1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">缺点是它假设不同误差的影响是相同的。有时，插入错误可能比删除有更大的影响。另一个限制是，这种度量不能将替换错误与删除和插入错误区分开来。</p><h2 id="aeb4" class="nw na it bd nb nx ny dn nf nz oa dp nj mi ob oc nl mj od oe nn mk of og np iz bi translated">通用语言理解评估(GLUE)</h2><p id="b753" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">胶水是由NYU的一个小组提出的。它包括九个英语理解任务。它还带有一个人类基线分数，供从业者评估他们的模型有多好。最后的分数只是简单的累加了九个任务的分数。越高越好。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/6abb9ecb2a20828b7dcee3567589cc0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uBFXYrCuv2di972dTOKKdw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">任务描述与统计(王等，2018)</figcaption></figure><p id="a0b9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">从下面的排行榜中，你可能会注意到人类基线(排名第12)低于许多著名模特，如<a class="ae lh" href="https://medium.com/dataseries/text-to-text-transfer-transformer-e35dc28bae14" rel="noopener"> T5 </a>、<a class="ae lh" href="https://towardsdatascience.com/when-multi-task-learning-meet-with-bert-d1c49cc40a0c" rel="noopener" target="_blank"> MT-DNN </a>和<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6" rel="noopener">罗伯塔</a>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/c75d4eac47aaf1466477a97db5cd480e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_JoAd2eJSSyJN5t1Nizeg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">GLUE的排行榜(来自<a class="ae lh" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank"> GLUE </a></figcaption></figure><h1 id="abab" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">关于我</h1><p id="73e0" class="pw-post-body-paragraph ll lm it lo b lp nr kd lr ls ns kg lu mi nt lx ly mj nu mb mc mk nv mf mg mh im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。你可以通过<a class="ae lh" href="https://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae lh" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae lh" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="6220" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">延伸阅读</h1><ul class=""><li id="335b" class="ml mm it lo b lp nr ls ns mi pc mj pd mk pe mh mq mr ms mt bi translated"><a class="ae lh" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">胶水</a></li></ul><h1 id="c345" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">参考</h1><ul class=""><li id="80e9" class="ml mm it lo b lp nr ls ns mi pc mj pd mk pe mh mq mr ms mt bi translated">K.帕皮尼尼、s .鲁科斯、t .沃德和朱伟杰。<a class="ae lh" href="https://www.aclweb.org/anthology/P02-1040.pdf" rel="noopener ugc nofollow" target="_blank"> BLEU:一种自动评估机器翻译的方法</a>。2002</li><li id="d494" class="ml mm it lo b lp mu ls mv mi mw mj mx mk my mh mq mr ms mt bi translated">A.王、辛格、迈克尔、希尔、利维和鲍曼。GLUE:自然语言理解的多任务基准和分析平台。2018.</li></ul></div></div>    
</body>
</html>