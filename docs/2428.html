<html>
<head>
<title>This Google Brain Technique Streamlines ML Interpretability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这项谷歌大脑技术简化了ML的可解释性</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/this-google-brain-technique-streamlines-ml-interpretability-1ff616284730?source=collection_archive---------1-----------------------#2021-12-17">https://pub.towardsai.net/this-google-brain-technique-streamlines-ml-interpretability-1ff616284730?source=collection_archive---------1-----------------------#2021-12-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6157" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="229c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">概念激活向量是理解ML模型行为的一个有趣的想法。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c6bf935cea4544e052d26849c12227ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bSRrm9Zf0m0AELMG.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:https://blog.ml.cmu.edu/tag/interpretability/<a class="ae lh" href="https://blog.ml.cmu.edu/tag/interpretability/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><blockquote class="li lj lk"><p id="a30f" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过10万名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列|子堆栈</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到110，000+的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="7e47" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">可解释性仍然是现代深度学习应用的最大挑战之一。计算模型和深度学习研究的最新进展使得能够创建高度复杂的模型，这些模型可以包括数千个隐藏层和数千万个神经元。虽然创建令人难以置信的高级深度神经网络模型相对简单，但它对这些模型如何创建和使用知识的理解仍然是一个挑战。几年前，来自谷歌大脑团队<a class="ae lh" href="https://arxiv.org/pdf/1711.11279.pdf" rel="noopener ugc nofollow" target="_blank">的研究人员发表了一篇论文，提出了一种称为概念激活向量</a> (CAVs)的新方法，从一个新的角度研究深度学习模型的可解释性。</p><h1 id="5be7" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">可解释性与准确性</h1><p id="6936" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">为了理解CAV技术，理解深度学习模型中可解释性挑战的本质是很重要的。在当前这一代深度学习技术中，模型的准确性和我们解释其知识的能力之间存在永久的摩擦。可解释性和准确性之间的摩擦是能够完成复杂的知识任务和理解这些任务是如何完成的之间的摩擦。知识与控制、绩效与责任、效率与简单……选择你最喜欢的困境，它们都可以通过平衡准确性和可解释性来解释。</p><p id="ec68" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">你关心获得最好的结果，还是关心理解那些结果是如何产生的？这是数据科学家在每个深度学习场景中都需要回答的问题。许多深度学习技术本质上是复杂的，尽管它们在许多情况下结果非常准确，但它们可能变得难以置信地难以解释。如果我们可以在一张关联准确性和可解释性的图表中绘制一些最知名的深度学习模型，我们将得到如下内容:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/eae6f285e08c15350a435babc1692268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfXE2sVB0J1gxal2UHaVrw.png"/></div></div></figure><p id="0634" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">深度学习模型中的可解释性不是一个单一的概念，可以在多个层面上看到:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/3f0a660df318dafe14b0c2430304abd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XaSP_IVTu1wYM_ftMedy5w.png"/></div></div></figure><p id="ada2" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">实现上图中定义的每一层的可解释性需要几个基本的构建块。在最近的一篇论文中，来自谷歌的研究人员概述了他们认为的可解释性的一些基础构件。</p><p id="21fd" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">Google将可解释性的原则总结如下:</p><p id="b8f9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd"> —了解隐藏层的作用:</strong>深度学习模型中的大部分知识都是在隐藏层中形成的。在宏观水平上理解不同隐藏层的功能对于能够解释深度学习模型是至关重要的。</p><p id="bae4" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd"> —理解节点是如何激活的:</strong>可解释性的关键不是理解网络中单个神经元的功能，而是理解在同一空间位置一起放电的相互连接的神经元群。将一个网络分割成相互连接的神经元群将提供一个更简单的抽象层次来理解它的功能。</p><p id="df77" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd"> —理解概念是如何形成的:</strong>理解深度神经网络如何形成单个概念，然后这些概念可以被组合成最终输出，这是可解释性的另一个关键组成部分。</p><p id="c9f7" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">这些原则是谷歌新CAV技术背后的理论基础。</p><h1 id="2f9b" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">概念激活向量</h1><p id="0f4a" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">按照上一节讨论的想法，可解释性的自然方法应该是根据深度学习模型考虑的输入特征来描述深度学习模型的预测。一个典型的例子是逻辑回归分类器，其中系数权重通常被解释为每个特征的重要性。然而，大多数深度学习模型对像素值等特征进行操作，这些特征并不对应于人类容易理解的高级概念。此外，模型的内部值(例如，神经激活)可能看起来不可理解。虽然诸如显著图的技术在测量特定像素区域的重要性方面是有效的，但是它们不能与更高级别的概念相关联。</p><p id="9bd9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">CAV背后的核心思想是测量模型输出中概念的相关性。一个概念的CAV仅仅是一个在该概念的一组例子的值(例如，激活)方向上的向量。在他们的论文中，谷歌研究团队概述了一种新的线性可解释性方法，称为CAV测试(TCAV)，使用方向导数来量化模型预测对CAV学习的底层高级概念的敏感性。从概念上讲，TCAV有四个目标:</p><p id="d65b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd"> —可访问性:</strong>几乎不需要用户的ML专业知识。</p><p id="df23" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd"> —定制:</strong>适应任何概念(如性别)，不限于训练时考虑的概念。</p><p id="a114" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd"> —插件就绪:</strong>无需对ML模型进行任何重新训练或修改即可工作。</p><p id="8a67" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd"> —全局量化</strong>:可以用一个量化指标解释整个类别或一组实例，而不仅仅是解释单个数据输入。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/956327494e140aad7671ef1f6fea7938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2LLtd__nFjPvHo_H9av2LA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:谷歌研究</figcaption></figure><p id="08bc" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">为了实现上述目标，TCAV方法分为三个基本步骤:</p><p id="8eab" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">1)定义模型的相关概念。</p><p id="f7af" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">2)理解预测对那些概念的敏感性。</p><p id="7215" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">3)外推每个概念对每个模型预测类的相对重要性的全局定量解释。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/b55c2b061c4bdd45b46cf9054bb9d627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y1q7zuzZ7OSurlm4Df6GxQ.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:谷歌研究</figcaption></figure><p id="68d7" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">TCAV方法的第一步是定义一个感兴趣的概念(CAV)。TCAV简单地通过选择一组代表这个概念的例子或者找到一个带有这个概念标签的独立数据集来实现这个目标。CAV是通过训练一个线性分类器来区分由一个概念的例子和任何层中的例子产生的激活来学习的。</p><p id="a59a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">第二步是生成TCAV分数，该分数量化了预测对特定概念的敏感度。TCAV通过使用方向导数来实现这一点，方向导数在神经激活层测量最大似然预测对概念方向输入变化的灵敏度。</p><p id="b2f7" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">最后一步试图评估学习到的CAV的全球相关性，以避免依赖不相关的CAV。毕竟，TCAV技术的一个缺陷是学习一个无意义的CAV的可能性。毕竟，使用随机选择的一组图像仍然会产生CAV。基于这种随机概念的测试不太可能有意义。为了应对这一挑战，TCAV引入了一种统计显著性测试，根据随机的训练次数(通常为500次)来评估CAV。这个想法是，一个有意义的概念应该导致TCAV分数在训练中表现一致。</p><h1 id="c91b" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">TCAV在行动</h1><p id="ef87" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">谷歌大脑团队进行了几次实验，以评估与其他可解释性方法相比，TCAV的效率。在最引人注目的测试之一中，该团队使用了一个显著图，试图预测一个标题或图像的相关性，以理解出租车的概念。显著图的输出如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/5250b68064529a5451016e358814b498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vrE_0LFN_rHlOSb4ldlKSg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:谷歌研究</figcaption></figure><p id="d117" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">使用这些图像作为测试数据集，谷歌大脑团队在亚马逊Mechanical Turk上用50个人进行了实验。每个工人做一系列的六个任务(3个对象类x 2s aliency贴图类型)，都是为了一个模型。任务顺序是随机的。在每项任务中，工作人员首先看到四幅图像及其对应的显著性遮罩。然后，他们评估图像对模型的重要性(10分制)，标题对模型的重要性(10分制)，以及他们对自己答案的自信程度(5分制)。总体而言，turkers对60幅独特的图像进行了评级(120幅独特的显著性图)。</p><p id="bfc3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">实验的基本事实是图像概念比标题概念更相关。然而，当查看显著性图时，人类认为字幕概念更重要(0%噪声的模型)，或者看不出差异(100%噪声的模型)。相比之下，TCAV的结果正确地表明，意象概念更为重要。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/62615eae00b53fadff9fbfcde1e292c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSqmpdjc6zcRaAzF5k3otQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:谷歌研究</figcaption></figure><p id="d083" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">TCAV是近几年来研究神经网络可解释性的最具创新性的方法之一。初始技术的代码在GitHub 上<a class="ae lh" href="https://github.com/tensorflow/tcav" rel="noopener ugc nofollow" target="_blank">可用，我们应该会看到主流深度学习框架采用其中的一些想法。</a></p></div></div>    
</body>
</html>