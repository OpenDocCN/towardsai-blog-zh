<html>
<head>
<title>Detecting Bad Posture With Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习检测不良姿势</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/detecting-bad-posture-with-machine-learning-be4b9de763d0?source=collection_archive---------1-----------------------#2022-11-10">https://pub.towardsai.net/detecting-bad-posture-with-machine-learning-be4b9de763d0?source=collection_archive---------1-----------------------#2022-11-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="078e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用计算机视觉和机器学习的实时人机工程学反馈介绍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/41d52d9bbac22c70960284d6df640701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*415nIg5W5tkNACt-"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">朱利安·特朗瑟在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="1467" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">答</span>根据世界卫生组织的报告，肌肉骨骼疾病是全球范围内导致残疾的主要原因，其中腰痛是主要原因[1]。</p><p id="6c0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用最先进的机器学习和计算机视觉，现在可以自动检测不良姿势并实时给出反馈，这可以显著降低肌肉骨骼疾病的风险[2]。</p><p id="01db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，您将首先了解评估人体工程学的相关观察方法。然后，我们将在实践中应用这些知识，使用网络摄像头和来自Google media pipe的机器学习模型，用Python创建一个工作示例。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="ce2a" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">评价人类工程学的方法</h1><p id="db45" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在观察性工效学评估方法中，专家观察人们的工作表现。同时，他们填写标准化工作表，如快速上肢评估(RULA)、快速全身评估(REBA)、Ovako工作姿势分析系统(OWAS)或人体工程学评估工作表(EAWS)。</p><p id="5c8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将重点关注RULA，因为它在实践中是常用的，快速且易于应用，其结果与相关的肌肉骨骼状况密切相关[3]。</p><p id="29b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1993年，McAtamney和Corlett在科学杂志<em class="ni">应用人体工程学</em>上发表了《RULA 》,主要关注上肢的姿势。RULA为颈部、躯干、手臂、手腕和腿部定义关节角度范围。姿势越差，分数越多。最后，将所有的分数相加，给出一个总分，而总分的范围从“可接受的姿态”到“需要立即改变”[4]。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/530dd00b762bf41aa6b8aea9c90fd07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DpDvyOHsdpxb-bK41y0iCg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">可接受姿势的RULA简化示例。来源:图片由作者提供。</figcaption></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="8a86" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">使用MediaPipe和OpenCV进行自动后角评估</h1><p id="288a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在下一节中，我们将使用Python编写一个脚本，通过网络摄像头实时评估躯干弯曲角度，如下面的演示GIF所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/0c0bd8327b75f3a7daa2848420720887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/1*sCUvSgaCHK0iTKUXP2Qplg.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">展示结果的演示GIF。来源:图片由作者提供。</figcaption></figure><p id="0577" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要一个预训练的机器学习模型，它可以检测图像/视频中的身体关节。这里我们将使用<a class="ae ky" href="https://google.github.io/mediapipe/solutions/pose.html" rel="noopener ugc nofollow" target="_blank"> Google的MediaPipe Pose </a>，它是基于卷积神经网络blaze Pose【5】。</p><p id="e010" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MediaPipe估计33个被称为姿势标志的身体关键点，只给定一个RGB图像作为输入，不需要GPU。每个地标都有从臀部开始的以米为单位的<code class="fe nl nm nn no b">x</code>、<code class="fe nl nm nn no b">y</code>和<code class="fe nl nm nn no b">z</code>现实世界坐标。</p><p id="c90b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使用Pip安装MediaPipe，我们只需执行命令<code class="fe nl nm nn no b">pip install mediapipe</code>。<a class="ae ky" href="https://google.github.io/mediapipe/solutions/pose#python-solution-api" rel="noopener ugc nofollow" target="_blank">文档提供了下面使用网络摄像头的最小Python示例</a>，我们将使用并扩展它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="ea40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最重要的一行是<code class="fe nl nm nn no b">results = pose.process(image)</code>，包含了33个身体关键点。我们可以用<code class="fe nl nm nn no b">landmarks_3d = results.pose_world_landmarks</code>来访问它们。</p><p id="48ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个标志索引对应于一个身体关键点。有关所有可用关键点的列表，请参见姿势标志模型的<a class="ae ky" href="https://google.github.io/mediapipe/solutions/pose.html#pose-landmark-model-blazepose-ghum-3d" rel="noopener ugc nofollow" target="_blank">文档。例如，我们可以用下面几行代码访问鼻子的<code class="fe nl nm nn no b">x</code>、<code class="fe nl nm nn no b">y</code>和<code class="fe nl nm nn no b">z</code>坐标:</a></p><pre class="kj kk kl km gt nr no ns nt aw nu bi"><span id="ffb6" class="nv mm it no b gy nw nx l ny nz"># get key points from mediapipe results<br/>results = pose.process(image)<br/>landmarks_3d = results.pose_world_landmarks</span><span id="e433" class="nv mm it no b gy oa nx l ny nz"># nose equals index 0<br/>x_nose = landmarks_3d.landmark[0].x<br/>y_nose = landmarks_3d.landmark[0].y<br/>z_nose = landmarks_3d.landmark[0].z</span></pre></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="937f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如下图所示，使用MediaPipe姿势，我们现在可以得到肩膀、臀部和膝盖的关节。通过平均左右关节使用中点，我们得到以下三个点A、B和C，中间有角度<em class="ni"> θ </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/7ff1df20760507bf92a0feb8cc891365.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*GqLzm4VTjOaLJpabtTvQLA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">背部屈曲角度可以使用中间管体式中肩部、臀部和膝盖的三个中点来近似。来源:图片由作者提供。</figcaption></figure><p id="0b4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">角度<em class="ni"> θ </em>可以使用基本几何图形计算:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc nq l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">计算A、B和C三点之间的角度</figcaption></figure><p id="c46b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据RULA、REBA和EAWS，躯干屈曲角度可分为三类:0-20°、20-60°和&gt; 60°。我们将使用这三个类别来区分低、中、高风险背部姿势。</p><p id="6729" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们的角度<em class="ni"> θ </em>在直立状态下从180°开始，而评估人体工程学的方法将这个位置定义为0°。因此，我们必须通过减去180°来调整我们的角度。</p><p id="5113" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的Python代码创建了一个人机工程学类，其中<code class="fe nl nm nn no b">update_joints()</code>从MediaPipe中读取3D界标，使用<code class="fe nl nm nn no b">get_angle()</code>通过我们的三个点A、B和C计算躯干弯曲角度，并使用<code class="fe nl nm nn no b">get_trunk_color()</code>以绿色、黄色和红色显示三个类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="ffb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网络摄像头演示的完整工作代码可在我的<a class="ae ky" href="https://github.com/leoneversberg/ergonomy_demo" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。随意试验和构建代码，例如，添加对颈部、手臂和腿部的评估。</p><p id="b00e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了用于工作场所的人体工程学之外，另一个应用领域是在健身行业，例如，在蹲下或按压运动期间检查姿势。</p><p id="0751" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的演示视频来自该大学的主持人，<em class="ni">工业级网络和来自柏林工业大学的云</em>，展示了使用微软Azure Kinect摄像头基于RULA的详细人机工程学评估[6]。根据本文描述的方法进行自动人机工程学评估。类似的研究工作最近已经发表，例如，ErgoSentinel工具[7]基于RULA，而ErgoExplorer [8]基于REBA。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od nq l"/></div></figure><h1 id="da37" class="ml mm it bd mn mo oe mq mr ms of mu mv jz og ka mx kc oh kd mz kf oi kg nb nc bi translated">结论</h1><p id="77a2" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">使用现代机器学习和计算机视觉进行身体跟踪，您可以轻松构建实时人体工程学反馈的应用程序。可能的应用领域是工作场所，或者例如健身行业。</p><h1 id="459e" class="ml mm it bd mn mo oe mq mr ms of mu mv jz og ka mx kc oh kd mz kf oi kg nb nc bi translated">额外资源</h1><p id="f262" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">来自ErgoPlus的<em class="ni">RULA评估工具的分步指南</em>:<a class="ae ky" href="https://ergo-plus.com/rula-assessment-tool-guide/" rel="noopener ugc nofollow" target="_blank">https://ergo-plus.com/rula-assessment-tool-guide/</a></p><p id="8251" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自ErgoPlus的<em class="ni"> RULA员工评估工作表</em>:<a class="ae ky" href="https://ergo-plus.com/wp-content/uploads/RULA.pdf" rel="noopener ugc nofollow" target="_blank">https://ergo-plus.com/wp-content/uploads/RULA.pdf</a></p><h1 id="e14f" class="ml mm it bd mn mo oe mq mr ms of mu mv jz og ka mx kc oh kd mz kf oi kg nb nc bi translated">参考</h1><p id="6e2a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">[1]世界卫生组织，肌肉骨骼健康<em class="ni"> </em> (2022)，<a class="ae ky" href="https://www.who.int/news-room/fact-sheets/detail/musculoskeletal-conditions" rel="noopener ugc nofollow" target="_blank">https://www . who . int/news-room/fact-sheets/detail/muscle skeletal-conditions</a></p><p id="ef94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] N. Vignais等人，工业制造中实时人机工程学反馈的创新系统(2013)，<a class="ae ky" href="https://doi.org/10.1016/j.apergo.2012.11.008" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1016/j.apergo.2012.11.008</a></p><p id="a7dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] D. Kee，基于文献综述的OWAS、RULA和REBA的系统比较(2022)，<a class="ae ky" href="https://doi.org/10.3390/ijerph19010595" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3390/ijerph19010595</a></p><p id="440b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] L. McAtamney和E. N. Corlett，RULA:调查与工作有关的上肢疾病的调查方法(1993年)，<a class="ae ky" href="https://doi.org/10.1016/0003-6870(93)90080-S" rel="noopener ugc nofollow" target="_blank">https://doi . org/10.1016/0003-6870(93)90080-S</a></p><p id="3287" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] V. Bazarevsky等人，BlazePose: On-device实时身体姿态跟踪(2020)，<a class="ae ky" href="https://arxiv.org/abs/2006.10204" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2006.10204</a></p><p id="2869" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] L. Eversberg，C. Sohst，J. Lambrecht，<em class="ni"/>assistensystem zur Verbesserung der Ergonomie/assisten system改善人机工程学——用人工智能预防制造业中的肌肉骨骼障碍(2022)【https://doi.org/10.37544/1436-4980-2022-09-68】T4</p><p id="a13d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7] V. M. Manghisi等人，工厂车间的自动人体工程学姿势风险监控ERGOSENTINEL工具(2020年)<a class="ae ky" href="https://doi.org/10.1016/j.promfg.2020.02.091" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1016/j.promfg.2020.02.091</a></p><p id="04cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[8] M. M. Fernandez等人，ErgoExplorer:来自视频集合的交互式人机工程学风险评估(2022年)，<a class="ae ky" href="https://arxiv.org/abs/2209.05252" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2209.05252</a></p></div></div>    
</body>
</html>