<html>
<head>
<title>Calculus in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的微积分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/calculus-in-machine-learning-2e7cddafa21f?source=collection_archive---------3-----------------------#2020-08-24">https://pub.towardsai.net/calculus-in-machine-learning-2e7cddafa21f?source=collection_archive---------3-----------------------#2020-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3d6e52cfa2190db3cbce49857a8ca7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_NQxeSkngPU4iZwYgCLlwg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">Benjamin O. Tayo的图片</figcaption></figure><h2 id="1086" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/mathematics" rel="noopener ugc nofollow" target="_blank">数学</a></h2><div class=""/><div class=""><h2 id="23bd" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">每个机器学习模型的背后都有一个高度依赖微积分的优化算法</h2></div><h1 id="021c" class="lh li jj bd jg lj lk ll lm ln lo lp lq ky lr kz ls lb lt lc lu le lv lf lw lx bi translated">一.导言</h1><p id="2e65" class="pw-post-body-paragraph ly lz jj ma b mb mc kt md me mf kw mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">机器学习算法(如分类、聚类或回归)使用训练数据集来确定权重因子，这些权重因子可应用于未知数据以实现预测目的。<em class="mu">每一个机器学习模型背后都是一个严重依赖微积分的优化算法</em>。在本文中，我们将讨论一种这样的优化算法，即梯度下降近似(GDA ),我们将展示如何使用它来构建一个简单的回归估计量。</p><h1 id="cf65" class="lh li jj bd jg lj lk ll lm ln lo lp lq ky lr kz ls lb lt lc lu le lv lf lw lx bi translated">二。使用梯度下降算法的优化</h1><h2 id="05be" class="mv li jj bd jg mw mx dn lm my mz dp lq mh na nb ls ml nc nd lu mp ne nf lw jp bi translated">II.1衍生产品和梯度</h2><p id="3906" class="pw-post-body-paragraph ly lz jj ma b mb mc kt md me mf kw mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在一维情况下，我们可以利用导数找到函数的最大值和最小值。让我们考虑一个简单的二次函数<em class="mu"> f(x) </em>如下所示。</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/72079bac9ce04a9bf7d86bae890106a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*s97N4WDmQ4qHC2tZBKrbbQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd jg">使用梯度下降算法求简单函数的最小值。Benjamin O. Tayo拍摄的图片</strong></figcaption></figure><p id="8725" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">假设我们想求函数<em class="mu"> f(x) </em>的最小值。使用带有一些初始猜测的梯度下降法，<em class="mu"> X </em>根据以下等式进行更新:</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f192d95a7c8889889d4613a80bf6e5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*9ZP8zf3fOjUnY_loMoZqew.png"/></div></figure><p id="11e1" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">其中常数<em class="mu">η</em>是一个小的正常数，称为学习率。请注意以下几点:</p><ul class=""><li id="ddda" class="nr ns jj ma b mb nl me nm mh nt ml nu mp nv mt nw nx ny nz bi translated"><strong class="ma jt"> <em class="mu">当X_n &gt; X_min，f’(X_n)&gt;0时:这确保X_n+1小于X _ n。因此，我们在向左的方向上采取步骤以达到最小值。</em> </strong></li><li id="2543" class="nr ns jj ma b mb oa me ob mh oc ml od mp oe mt nw nx ny nz bi translated"><strong class="ma jt"> <em class="mu">当X_n &lt; X_min，f’(X_n)&lt;0时:这确保X_n+1大于X _ n。因此，我们正朝着正确的方向前进，以达到X_min。</em>T25】</strong></li></ul><p id="a85c" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">上述观察表明，不管初始猜测是什么，梯度下降算法总是会找到最小值。到达<em class="mu"> X_min </em>需要多少优化步骤取决于最初的猜测有多好。有时，如果初始猜测或学习率没有仔细选择，算法可能会完全错过最小值。这通常被称为“<strong class="ma jt">过冲</strong>”。通常，可以通过添加收敛标准来确保收敛，例如:</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/e7aee59964ccd25b1e02da722b2db763.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*QHKRS1Q6RVbOFX0bwAtgHQ.png"/></div></div></figure><p id="66c7" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">其中<em class="mu">ε</em>是一个小正数。</p><p id="a312" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">在更高维度中，也可以使用梯度下降算法来优化(最小化)几个变量的函数。在这种情况下，我们使用渐变来更新矢量X的<strong class="ma jt"><em class="mu"/></strong>:</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/67aba89c5fd3207cce424b63fd0174fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*sBCzhqQzmoMSfqS74marMA.png"/></div></figure><p id="f14f" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">如同在一维中一样，可以通过添加收敛标准来确保收敛，例如:</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/07f006d756970cfa9380d409b781ada8.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*9DW8RZaTTWxXDme1SiHn0Q.png"/></div></figure><h2 id="6f8e" class="mv li jj bd jg mw mx dn lm my mz dp lq mh na nb ls ml nc nd lu mp ne nf lw jp bi translated">II.2案例研究:构建简单的回归估计器</h2><p id="61fc" class="pw-post-body-paragraph ly lz jj ma b mb mc kt md me mf kw mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在这一小节中，我们将描述如何构建一个简单的python估算器来使用梯度下降法执行线性回归。假设我们有一个包含单个特征(<em class="mu"> X </em>)和结果(<em class="mu"> y </em>)的一维数据集，假设数据集中有N个观测值:</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d9e26cc565b7034768d749ff3f8f7c33.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/0*7HKoQP6mr5zrhSuy.png"/></div></figure><p id="cd6b" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">拟合数据的线性模型如下所示:</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/00a15cbbac49abda3eb54fde2fe3c03e.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/0*ZUcRYDTMUKucyn_l.png"/></div></figure><p id="7aa3" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">其中<em class="mu"> w0 </em>和<em class="mu"> w1 </em>是算法在训练过程中学习的权重。</p><h2 id="5da0" class="mv li jj bd jg mw mx dn lm my mz dp lq mh na nb ls ml nc nd lu mp ne nf lw jp bi translated">II.3梯度下降算法</h2><p id="b02d" class="pw-post-body-paragraph ly lz jj ma b mb mc kt md me mf kw mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">如果我们假设模型中的误差是独立的且呈正态分布，则似然函数如下所示:</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/173758dd0376ee674928152683579a06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/0*xUTrj21U_smKwAs4.png"/></div></figure><p id="9ae0" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">为了最大化似然函数，我们最小化相对于<em class="mu"> w0 </em>和<em class="mu"> w1 </em>的误差平方和(<em class="mu"> SSE </em>):</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/84660a7ecd4b7d6ef3f57071942b779b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*EH5UJnnSmIxBF4k6.png"/></div></figure><p id="6e9f" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><strong class="ma jt">目标函数</strong>或我们的<strong class="ma jt"> SSE函数</strong>通常使用梯度下降近似(GDA)算法最小化。在GDA方法中，权重根据以下程序进行更新:</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/823a614beb4901d7568a1a93c30f9f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/0*fNIgZQZ_twciKb53.png"/></div></figure><p id="2fbb" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated">即在与梯度相反的方向上。这里，<em class="mu">η</em>是被称为学习率的小的正常数。该等式可以用分量形式写成:</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/951f0828c1ab95431d6d2b0a4700df75.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/0*C8HmhfMTWQfIBZGR.png"/></div></figure><h2 id="4b19" class="mv li jj bd jg mw mx dn lm my mz dp lq mh na nb ls ml nc nd lu mp ne nf lw jp bi translated">II.4 Python实现</h2><pre class="nh ni nj nk gt oo op oq or aw os bi"><span id="6a0d" class="mv li jj op b gy ot ou l ov ow">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>class GradientDescent(object):<br/>    """Gradient descent optimizer.<br/>    Parameters<br/>    ------------<br/>    eta : float<br/>        Learning rate (between 0.0 and 1.0)<br/>    n_iter : int<br/>        Passes over the training dataset.<br/>        <br/>    Attributes<br/>    -----------<br/>    w_ : 1d-array<br/>        Weights after fitting.<br/>    errors_ : list<br/>        Error in every epoch.<br/>    """    def __init__(self, eta=0.01, n_iter=10):<br/>        self.eta = eta<br/>        self.n_iter = n_iter<br/>        <br/>    def fit(self, X, y):<br/>        """Fit the data.<br/>        <br/>        Parameters<br/>        ----------<br/>        X : {array-like}, shape = [n_points]<br/>        Independent variable or predictor.<br/>        y : array-like, shape = [n_points]<br/>        Outcome of prediction.<br/>        Returns<br/>        -------<br/>        self : object<br/>        """<br/>        self.w_ = np.zeros(2)<br/>        self.errors_ = []<br/>        <br/>        for i in range(self.n_iter):<br/>            errors = 0<br/>            for j in range(X.shape[0]):<br/>                self.w_[1:] += self.eta*X[j]*(y[j] - self.w_[0] -                     self.w_[1]*X[j])<br/>                self.w_[0] += self.eta*(y[j] - self.w_[0] - self.w_[1]*X[j])<br/>                errors += 0.5*(y[j] - self.w_[0] - self.w_[1]*X[j])**2<br/>            self.errors_.append(errors)<br/>        return self    </span><span id="30ae" class="mv li jj op b gy ox ou l ov ow">def predict(self, X):<br/>        """Return predicted y values"""<br/>        return self.w_[0] + self.w_[1]*X</span></pre><h2 id="ffaa" class="mv li jj bd jg mw mx dn lm my mz dp lq mh na nb ls ml nc nd lu mp ne nf lw jp bi translated">二. 5基本回归模型的应用</h2><p id="6842" class="pw-post-body-paragraph ly lz jj ma b mb mc kt md me mf kw mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated"><strong class="ma jt"> a)创建数据集</strong></p><pre class="nh ni nj nk gt oo op oq or aw os bi"><span id="b8fc" class="mv li jj op b gy ot ou l ov ow">np.random.seed(1)<br/>X=np.linspace(0,1,10)<br/>y = 2*X + 1<br/>y = y + np.random.normal(0,0.05,X.shape[0])</span></pre><p id="a6a6" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><strong class="ma jt"> b)拟合和预测</strong></p><pre class="nh ni nj nk gt oo op oq or aw os bi"><span id="690d" class="mv li jj op b gy ot ou l ov ow">gda = GradientDescent(eta=0.1, n_iter=100)<br/>gda.fit(X,y)<br/>y_hat=gda.predict(X)</span></pre><p id="af8e" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><strong class="ma jt"> c)绘图输出</strong></p><pre class="nh ni nj nk gt oo op oq or aw os bi"><span id="badc" class="mv li jj op b gy ot ou l ov ow">plt.figure()<br/>plt.scatter(X,y, marker='x',c='r',alpha=0.5,label='data')<br/>plt.plot(X,y_hat, marker='s',c='b',alpha=0.5,label='fit')<br/>plt.xlabel('x')<br/>plt.ylabel('y')<br/>plt.legend()</span></pre><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/75e11953c26b260807d0475509213ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/0*w-Exy9ledftq0CkW.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">Benjamin O. Tayo拍摄的图片</figcaption></figure><p id="5fd5" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><strong class="ma jt"> d)计算R平方值</strong></p><pre class="nh ni nj nk gt oo op oq or aw os bi"><span id="ed9a" class="mv li jj op b gy ot ou l ov ow">R_sq = 1-((y_hat - y)**2).sum()/((y-np.mean(y))**2).sum()<br/>R_sq<br/>0.991281901588877</span></pre><h1 id="1e87" class="lh li jj bd jg lj lk ll lm ln lo lp lq ky lr kz ls lb lt lc lu le lv lf lw lx bi translated"><strong class="ak">三世。总结和结论</strong></h1><p id="074b" class="pw-post-body-paragraph ly lz jj ma b mb mc kt md me mf kw mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">总之，我们已经展示了如何使用GDA算法在Python中构建和实现一个简单的线性回归估计器。每一个机器学习模型的背后都是一个严重依赖微积分的优化算法。如果你想看看GDA算法在一个真实的机器学习分类算法中是如何使用的，请看下面的<a class="ae oz" href="https://github.com/bot13956/LogisticRegression_gradient_descent" rel="noopener ugc nofollow" target="_blank"> Github资源库</a>。</p><h1 id="83cb" class="lh li jj bd jg lj lk ll lm ln lo lp lq ky lr kz ls lb lt lc lu le lv lf lw lx bi translated">其他数据科学/机器学习资源</h1><p id="b835" class="pw-post-body-paragraph ly lz jj ma b mb mc kt md me mf kw mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated"><a class="ae oz" href="https://medium.com/towards-artificial-intelligence/how-much-math-do-i-need-in-data-science-d05d83f8cb19" rel="noopener">数据科学需要多少数学？</a></p><p id="d073" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><a class="ae oz" href="https://medium.com/towards-artificial-intelligence/data-science-curriculum-bf3bb6805576" rel="noopener">数据科学课程</a></p><p id="8f0d" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><a class="ae oz" href="https://towardsdatascience.com/5-best-degrees-for-getting-into-data-science-c3eb067883b1" rel="noopener" target="_blank">进入数据科学的5个最佳学位</a></p><p id="16bd" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><a class="ae oz" href="https://towardsdatascience.com/theoretical-foundations-of-data-science-should-i-care-or-simply-focus-on-hands-on-skills-c53fb0caba66" rel="noopener" target="_blank">数据科学的理论基础——我应该关心还是仅仅关注实践技能？</a></p><p id="5727" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><a class="ae oz" href="https://towardsdatascience.com/machine-learning-project-planning-71bdb3a44349" rel="noopener" target="_blank">机器学习项目规划</a></p><p id="e9fa" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><a class="ae oz" href="https://towardsdatascience.com/how-to-organize-your-data-science-project-dd6599cf000a" rel="noopener" target="_blank">如何组织你的数据科学项目</a></p><p id="601f" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><a class="ae oz" href="https://medium.com/towards-artificial-intelligence/productivity-tools-for-large-scale-data-science-projects-64810dfbb971" rel="noopener">大型数据科学项目的生产力工具</a></p><p id="3389" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><a class="ae oz" href="https://towardsdatascience.com/a-data-science-portfolio-is-more-valuable-than-a-resume-2d031d6ce518" rel="noopener" target="_blank">数据科学作品集比简历更有价值</a></p><p id="46fd" class="pw-post-body-paragraph ly lz jj ma b mb nl kt md me nm kw mg mh nn mj mk ml no mn mo mp np mr ms mt im bi translated"><strong class="ma jt"> <em class="mu">如有疑问，请发邮件给我</em></strong>:benjaminobi@gmail.com</p></div></div>    
</body>
</html>