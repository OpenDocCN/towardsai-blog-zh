<html>
<head>
<title>Big Data Is Not the Way to Go</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大数据不是出路</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/big-data-is-not-the-way-to-go-bc68f6d3ad67?source=collection_archive---------2-----------------------#2022-09-24">https://pub.towardsai.net/big-data-is-not-the-way-to-go-bc68f6d3ad67?source=collection_archive---------2-----------------------#2022-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9bfc771f9bea24c04a60c627b3707597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JT3QVVBnha7G1oa7ApWv-Q.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">一拉<a class="ae jg" href="https://unsplash.com/photos/dBI_My696Rk" rel="noopener ugc nofollow" target="_blank"> unsplash </a></figcaption></figure><div class=""/><div class=""><h2 id="0b90" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">数据分布的重要性</h2></div><p id="ec79" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">围绕深度学习的大量论述会让你相信数据越多越好。这是一个相当直观的想法。数据在某种程度上代表了“真实世界”，基于这种数据的模型训练学习了这种“真实世界”的采样表示。因此，采样数据越多，模型的理解就越接近“真实世界”。</p><p id="f680" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个命题似乎被深度学习的整个历史所证实。“好的研究需要好的资源，”<a class="ae jg" href="https://www.image-net.org/about.php" rel="noopener ugc nofollow" target="_blank"> ImageNet网站</a>写道。“要大规模解决这个问题……为研究人员提供大规模图像数据库至关重要。”GPT-2论文在第2.1节“训练数据集”中报告说，“大多数先前的工作在单个文本域上训练语言模型……我们的方法鼓励建立尽可能大和多样化的数据集，以便收集尽可能多的域和上下文中的任务的自然语言演示。”</p><p id="4ca1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不可否认的是，ImageNet和GPT都是计算机视觉和语言建模的重要一步。衡量标准的实质性改进证实了这一点。暴露于较大数据集的模型表现更好，只要它们的内部容量也相应地调整以适应规模的增加。那么，寻求推进模型性能前沿的组织和研究人员是否应该赶紧积累尽可能大的数据集呢？</p><p id="de88" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">嗯，是也不是。特别是在如此大的数据集规模的领域中，我们不能再用宽泛的符号来推理数据集的动态性(“更大的数据集”、“更小的数据集”、“更低的损失”、“更高的准确性”等)。).这些符号掩盖了错综复杂的<em class="lu">数据分布</em>。</p><p id="516c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据分布是一个理论概念，它涉及如何在整个数据集中表示数据样本的质量。有许多实现(测量)数据分布的方法，但这些方法总是不全面的——比如获取三维区域的二维横截面。数据分布的一个明显的度量是分类问题的类(im)平衡。有些阶层比其他阶层更常被代表，还是他们被平等地代表？如果类被平等地表示，如果它们在验证集中没有被平等地表示，或者不会在部署环境中，它们应该被平等地表示吗？一个更具挑战性的衡量标准是样本复杂性:一些样本比其他样本“简单”。例如，在神经机器翻译中，翻译一个原始的名词序列比复杂和模糊的名词、动词、形容词、次动词的排列更容易。在语音到文本转换中，转录具有清晰背景的语音比在纽约市街道中间的语音更容易。我们也许能把它量化为“信噪比”。另一个衡量标准可能是“学习难度”。假设数据集由99%的“复杂”样本和1%的“简单”样本组成:直觉表明，99%的“复杂”样本比1%的“简单”样本更容易学习，因为它们具有更高的代表性。当然，事情要比这复杂得多。这就是说，有许多方法可以表征数据集的分布。</p><p id="984f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们考虑对应于“模型有用性”的数据分布的维度呢？我们期望所有的样本对模型同样有用吗？显然不是。但是，透过数据分布而不是数据集的抽象属性来思考，让我们提出了几个令人困扰的问题:</p><ul class=""><li id="7dc9" class="lv lw jj la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">如果较小数据集的数据分布具有更高浓度的有用样本，则较小数据集是否会比较大数据集产生更好的性能？</li><li id="3ead" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">大型数据集中有多少冗余是“自然”产生的？</li><li id="01df" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">在不损失性能的情况下，我们可以删除多少数据？</li><li id="8b56" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">事实上，我们能通过删除数据分布中冗余或无用的部分来体验性能提升吗？</li></ul><p id="84cb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，ImageNet通过提供前所未有的图像类别多样性取得了成就。每班有500张训练图像。但是它需要那么多吗？也许每班100张训练图像就足够了。或者可能一些类别比其他类别需要更少的训练图像。同样，用于训练GPT-2的大规模数据集跨越了令人难以置信的领域范围，这是其无监督多任务属性的原始来源。但是，我们可以用更小的数据集获得相同范围的属性吗？当我们考虑几乎任何背景下几乎任何“横截面”的数据分布的自然非均匀分布时，答案很可能是肯定的。</p><p id="1eee" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，大数据是未来的发展方向吗？</p><p id="75c2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是的，在一个非常具体的意义上:我们需要一个大的数据池，以便确定哪个子集是最有用的。同样的，如果我们想要更大绝对量的黄金，我们必须筛选更大绝对量的灰尘。坦率地说，很大一部分数据集是“垃圾”，而不是“黄金”。</p><p id="d3ac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不，从更一般的意义上来说。最明显的是，在不必要的大数据集上训练是对计算资源的巨大消耗。这种流失的规模是不可低估的。根据Meta的说法，“额外的20亿个预训练数据点(从10亿个开始)会导致ImageNet上的准确率提高几个百分点”。这遵循了论文<a class="ae jg" href="https://arxiv.org/abs/2001.08361" rel="noopener ugc nofollow" target="_blank">“神经语言模型的标度法则”</a>中研究的<em class="lu">幂律</em>关系:保持计算能力和模型大小不变，持续提高性能所需的样本数量成倍增加。用不太专业的术语来说，所有足够大规模的模型都经历了严重的垂死回报。完成其中的80%可能需要一天的计算时间，剩下的20%可能需要六天的时间。删除冗余样本减轻了死亡回报的影响，并允许更快的迭代。这使得大规模人工智能可以在高计算研究实验室之外实践，并使人工智能的前沿民主化(目前由少数机构垄断)。但是冗余削减也能提高性能，不管是普通的还是不普通的。在某种意义上，与完整数据<em class="lu"> D </em>具有相同“值”的数据子集<em class="lu"> d </em>对于相同的计算时间将能够比<em class="lu"> D </em>迭代更多次。我们最终都会被实际计算的有限性所束缚。重要的是，根据验证方法，更多的数据会极大地损害泛化能力。敌对样本可以用来阻挠模型预测，但也有不同种类的敌对样本:那些通过放置在特征空间的特别分裂的区域来阻挠学习的样本。</p><p id="1c2f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上，如何确定数据的最佳子集呢？</p><p id="46c7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其他人说得比我好得多。我会在这里链接几篇关于这个话题的值得一读的论文。了解我们塞进模型的数据的分布不仅可以让我们体验速度和可能的性能提升，还可以为深度学习理论做出重要贡献。</p><ul class=""><li id="a801" class="lv lw jj la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><a class="ae jg" href="https://arxiv.org/pdf/2001.08361.pdf" rel="noopener ugc nofollow" target="_blank">“神经语言模型的标度律”</a>。理解大规模建模培训的垂死回报制度的基础论文。</li><li id="1a89" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae jg" href="https://arxiv.org/pdf/2206.14486.pdf" rel="noopener ugc nofollow" target="_blank">“超越神经标度律:通过数据剪枝实现拍幂律标度”</a>。英雄，幂律的杀手:提供了数据修剪的分析理论，以及如何用它来击败幂律缩放。总结—让<em class="lu"> a </em>表示数据集大小和模型大小之间的比率，或者相对数据集大小。如果<em class="lu"> a </em>很小(例如<em class="lu">a</em>T24】1)，则需要更大数量的简单样本来建立强可概括的决策边界。如果<em class="lu"> a </em>很大(例如<em class="lu">a</em>T25】1)，则需要更多的困难样本来计算出决策边界的具体细微差别。‘困难样本’是低利润的<em class="lu"/>，表示它们接近决策边界，因此不会明显地落在一边或另一边(‘简单样本’是高利润的<em class="lu"/>)。作者建议构建小型信息包装的“基础数据集”(一种面向数据的模拟“基础模型”)，可用于有效地训练模型。</li><li id="7204" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae jg" href="https://arxiv.org/pdf/2010.13166.pdf" rel="noopener ugc nofollow" target="_blank">《课程学习调查》</a>。课程学习包括沿时间轴的数据修剪:也就是说，某些样本在某个时间不呈现给模型，但在稍后重新引入。在这里，样本对模型的“有用性”取决于模型的状态:例如，在模型很好地掌握了较容易的样本之后，一个困难的样本可能有价值。<a class="ae jg" href="https://arxiv.org/pdf/2101.10382.pdf" rel="noopener ugc nofollow" target="_blank">这个</a>也是很好的调查子领域。阅读Yoshua Bengio等人2009年的“原始”课程学习论文<a class="ae jg" href="https://ronan.collobert.com/pub/2009_curriculum_icml.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>。</li><li id="4bed" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1904.03626.pdf" rel="noopener ugc nofollow" target="_blank">《论课程学习在深度神经网络训练深度学习中的力量》</a>。作者将课程学习应用于CIFAR，并获得了显著的收益(加速高达70%)。使用ERM提供了一个分析理论来解释为什么课程学习有效:局部解决方案保留了它们的属性(即，局部最小值仍然是局部最小值)，但是损失情况变得“更平滑”。一个有趣的练习是将这个理论与“超越神经标度律”中提供的框架相调和。</li><li id="1cac" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1911.05248.pdf" rel="noopener ugc nofollow" target="_blank">“压缩深度神经网络忘记了什么？”</a>当我们压缩一个模型时(即“模型修剪”而不是“数据修剪”)，模型对数据分布的性能有什么变化？此外，一个更普遍的问题是，在数据剪枝中，我们是否丢弃了“困难的”或“无用的”样本，以提高性能并加快阻止分解成通用叙述的不方便的荆棘？他们是“数据社会”的“弃儿”吗？将他们排除在外会给公平问题带来什么？相关论文— <a class="ae jg" href="https://arxiv.org/pdf/2010.03058.pdf" rel="noopener ugc nofollow" target="_blank">“压缩模型中偏差的表征”</a>。</li><li id="49e2" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae jg" href="https://arxiv.org/pdf/2110.09485.pdf" rel="noopener ugc nofollow" target="_blank">“高维度的学习总是等于外推”</a>。一篇充满争议但从另一个角度来看仍然很有趣的论文:证明随着数据集维度的增加，验证样本几乎总是落在训练样本的凸包之外。作者认为这“总是相当于外推”。抛开这个问题不谈，它为我们理解数据的分布提供了另一个有趣的视角。例如,“接近”凸包是否意味着困难？这里的作者在原始特征空间(例如，图像的像素空间)中运行凸包计算，但正如其他人，特别是Francois Chollet所争论的那样，在低得多的维度潜在空间中这样做更有意义。</li><li id="bd59" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1611.03530.pdf" rel="noopener ugc nofollow" target="_blank">“理解深度学习需要重新思考概括”</a>。一篇更切题，但仍然相关的论文:证明深度神经网络可以很容易地适应随机标签。神经网络有能力记忆整个数据集，以考虑特征空间中学习的最终反常(即，尽可能接近“高利润”样本，真的)。一般化等问题与数据分布问题密切相关。</li></ul><p id="1774" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">开创性的论文<a class="ae jg" href="https://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">“彩票假说”</a>试图解释模型修剪的成功——通过使用适当的选择标准，网络中80%或更高的权重可以被删除(设置为零)而性能下降最小的方法——提出网络是子网的大盆地，每个子网都被初始化为“彩票”。在训练过程中，一个子网被“选择”并“成长”为网络中最活跃和最相关的部分。执行修剪后留下的就是这个子网。从这个意义上说，神经网络是一大堆灰尘，我们可以从中筛选出黄金。(这个有印象吗？)你可以阅读更多关于彩票假说和相关不可思议的结果(如无需任何训练，只需在初始化时进行修剪即可获得顶级神经网络性能)<a class="ae jg" href="https://medium.com/mlearning-ai/obtaining-top-neural-network-performance-without-any-training-5af0af464c59" rel="noopener">此处</a>。</p><p id="08ee" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们已经看到，“参数分布”的不均匀性在模型压缩和神经网络理论中产生了实质性的结果。我们应该期待不均匀的“数据分布”，已经观察到遵循非常相似的模式。关注深度学习的研究确实是一个令人兴奋的时刻。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="ad39" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p><p id="ac62" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你对最新的文章感兴趣，可以考虑<a class="ae jg" href="https://andre-ye.medium.com/subscribe" rel="noopener">订阅</a>。如果你想支持我的写作，通过我的<a class="ae jg" href="https://andre-ye.medium.com/membership" rel="noopener">推荐链接</a>加入Medium是一个很好的方式。干杯！</p></div></div>    
</body>
</html>