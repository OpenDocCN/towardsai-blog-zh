<html>
<head>
<title>Leveling Up Your Travel Agent Skills Through NLP (Part II)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过NLP提升你的旅行社技能(第二部分)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/leveling-up-your-travel-agent-skills-through-nlp-part-ii-9c66194ebdde?source=collection_archive---------2-----------------------#2021-05-10">https://pub.towardsai.net/leveling-up-your-travel-agent-skills-through-nlp-part-ii-9c66194ebdde?source=collection_archive---------2-----------------------#2021-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c4d1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><p id="a17e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你是在看完《T2》第一部后找到这篇文章的吗？你想继续读书是多么令人兴奋啊！</p><p id="7767" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果您不是来自本文的第一部分，您可以在这里阅读<a class="ae ku" rel="noopener ugc nofollow" target="_blank" href="/leveling-up-your-travel-agent-skills-through-nlp-part-i-45957845973d"/>，它首先讨论了为什么NLP预处理是必要的，预处理文本数据的一些最常见的方法，以及基于您的自然语言处理(NLP)用例应用什么预处理技术的直观感觉。它还介绍了我的具体使用案例——一个基于旅游偏好的推荐系统，使用Tripadvisor上人们写的评论构建。为了缩小范围，我把注意力集中在约塞米蒂国家公园。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/4715c34517696922f68eca4f29ae90be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OrviDVzCOvblStjf"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">约塞米蒂国家公园。照片由<a class="ae ku" href="https://unsplash.com/@postebymach?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">德拉霍姆在</a><a class="ae ku" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上发布</figcaption></figure><h1 id="fd18" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">继续了解主题建模！</strong></h1><p id="6454" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">NLP的基本思想是想出不同的方法将文本数据转换成数字数据(同时保留文本数据本身的基本含义)，然后计算机可以根据这些数据进行各种下游处理。最简单的方法是使用词频或TF-IDF向量制作一个单词包。这两种情况下的列都是由语料库中的每一个n元单词(又名token)组成的。如果您有1000个文档，一个unigram向量(每个唯一的单词是一列)可以轻松包含5000列(每列一个单词)，一个bigram向量(每个唯一的单词对一列)可以包含100，000列！！在大多数情况下，这太多了，没有任何实际用途。</p><p id="b6fd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">主题建模</strong>将大量的列减少到几列(也称为维度)，通常是5到30列。它通过使用降维技术来实现这一点，该技术在文档集合中捕获“共现术语的重复模式”。目标是能够使用几个小主题来表示文档和标记列，同时信息损失最小。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mo"><img src="../Images/e8326cb42e00a4f74f1b175fe1f1cd3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XtVSSq1tyRNghggWJzdbRQ.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">主题示例</figcaption></figure><p id="cd6d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">假设我们有30个话题。主题建模过程给出了30个主题中的每一个与每个文档相关程度的分数(在我的例子中是评论)。每个主题本身将由原始单词(记号)的集合来表示，伴随着这些单词中的每一个与主题相关的程度的分数。在技术术语中，它们分别被称为文档-主题矩阵和单词-主题矩阵。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mp"><img src="../Images/a55c42d046f89fb6b42f7b7037215372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3DNe_azbZ1k8wKLgPZu-0Q.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">主题的单词分数和文档的主题分数</figcaption></figure><p id="486e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">主题建模是一个高度复杂的话题(一语双关)，如果你想更深入地了解它的技术方面，我建议你先从这篇文章开始更深入地了解<a class="ae ku" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank">降维，然后再看</a><a class="ae ku" href="https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/" rel="noopener ugc nofollow" target="_blank">这篇关于主题建模的文章</a>。</p><h1 id="8bae" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">问:那么，有什么特定的算法可以用于主题建模吗？</strong></h1><p id="5e18" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">NMF和LDA是两种比较著名的无监督主题建模算法。有了这两个，结果主题的数量是您可以控制的最有影响力的参数。但除此之外，他们对自己的结果相当“无人监督”。它们产生的话题是对是错没有答案——只有在它们对你的目标有意义的情况下。</p><p id="603d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">典型地，NMF已经显示出当数据更少时(小文档大小，更少的文档)产生更好的结果，而LDA在有更多数据时开始产生更合理的主题。</p><blockquote class="mq"><p id="6752" class="mr ms iq bd mt mu mv mw mx my mz kt dk translated">我们可以使用每个主题中得分最高的文档和单词来确定这些主题是否有意义。</p></blockquote><p id="550d" class="pw-post-body-paragraph jw jx iq jy b jz na kb kc kd nb kf kg kh nc kj kk kl nd kn ko kp ne kr ks kt ij bi translated">如果给定主题下所有得分前5名(也可以检查前10名——没有硬性数字)的文档都有一个一致的主题，并且都围绕着您关心的内容，那么这些主题就有意义。同样的逻辑也适用于检查得分最高的单词。</p><p id="2ca0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">基于领域直觉，在我的例子中，我期望在国家公园的评论中反复提到一个地方是容易还是难徒步旅行，你是否能从那个景点得到好的照片，等等。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nf"><img src="../Images/c62d0a2fe27dce7a7ebcfa72070acecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ImwSzUup4TaS1p69AIL_w.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">谈论惊人观点的主题下的评论示例</figcaption></figure><p id="adeb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">下面是您在主题建模过程中可能遇到的一些挑战，以便获得一些“合理”的东西:</p><ol class=""><li id="ffc2" class="ng nh iq jy b jz ka kd ke kh ni kl nj kp nk kt nl nm nn no bi translated">该算法正在挑选某些重复出现的单词，这些单词耗尽了您在用例中需要的上下文。在这种情况下，您需要继续预处理您的数据，直到它对预期目标有意义。这可能意味着添加更多的自定义停用词，因为主题围绕着某些导致无用主题的词，或者将词分成二元/三元符号，或者基于词性的过滤。在我的例子中，景点的名称在原始文本数据中被突出使用，主题一直围绕着它。它们必须在预处理步骤中被周期性地去除，以获得期望的输出。</li><li id="f2d0" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">用于初始化主题建模过程本身的参数可能需要调整。要么允许模型发现更多(或更少)的主题，要么需要更多的迭代次数来提取更合理的模式。</li><li id="57a2" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">选择的算法不是最适合所提供的上下文的算法，必须使用不同的算法。</li></ol><blockquote class="mq"><p id="381b" class="mr ms iq bd mt mu nu nv nw nx ny kt dk translated">最重要的是，记住主题建模是一个迭代过程，有时会令人沮丧。你的话题可能毫无意义；尤其是刚开始。</p></blockquote><p id="97ab" class="pw-post-body-paragraph jw jx iq jy b jz na kb kc kd nb kf kg kh nc kj kk kl nd kn ko kp ne kr ks kt ij bi translated">但是您必须迭代预处理步骤和主题建模步骤，看看什么对您起作用，以获得最终的“合理的”主题集合。</p><h1 id="a7b1" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">问:<strong class="ak">你用了什么算法？你有没有找到什么“合理的”话题？</strong></h1><p id="c932" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">我最后使用了Corex算法，因为我喜欢它的半监督方法。这意味着你可以指定“锚词”来帮助算法创建以这些词为中心的主题。每个想要的主题下可以指定多个锚词。它的重点是找到这些词和其他出现频率接近的词，以发现最终的主题集，以及与这些主题密切相关的文档。</p><p id="80d8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你甚至可以指定锚词来明确地分离出合并在一起的主题。</p><p id="85df" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">指定锚词也是一个迭代的过程，直到你得到合理的结果(是的，更多的迭代！).</p><p id="e84b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">最后到了12组锚词(下面可以看到一部分)，18个话题。与锚词列表的数量相比，建议保留一些额外的主题，以便该算法可以捕获没有通过锚词本身明确指定的任何其他循环模式。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nz"><img src="../Images/8fc14709e5fef3d7480596a4a849a595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VA6vxOc1tCjBS5dOSQGfrA.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">锚词示例列表</figcaption></figure><p id="e65a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">根据经验，您希望确保每个主题占全部文档的5%或更多(一个文档通常位于多个主题下)。当一个主题只有很少的文档时，你可以通过删除与其明确对应的锚词或者减少主题的总数来删除这个主题。</p><p id="1d8d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Corex也有一个被称为TC(总相关度)的分数，它是在总体和个体主题水平上测量的。你可以在这里阅读更多关于Corex <a class="ae ku" href="https://github.com/gregversteeg/corex_topic" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><h1 id="ce28" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">问:你推荐哪些景点<strong class="ak">？</strong></h1><p id="c858" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">看你喜欢什么了！</p><p id="cb61" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">对于这个项目的最后一部分，我首先给出标签，通过查看每个主题的锚词、语料库中得分最高的词/评论来捕捉每个主题的含义。</p><p id="b1a6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">其中，我集中使用12个主题来创建推荐系统。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi oa"><img src="../Images/7e81bb5cd90731ebeb32dfd02714ea23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2gqPPH9cPyCJHGg3EUNQdw.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">带标签的主题</figcaption></figure><p id="9eca" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为了创建推荐系统，我对景点名称级别的所有评论的主题分数进行了平均，这样每个景点名称在“令人惊叹的景色”、“轻松的远足”等方面都有相应的分数。所以我为每个吸引力准备了一个12维的向量。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ob"><img src="../Images/b7c6b160c78ed2d556e82e4f2781606f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAfWPrCElVzzksN8AAzFGw.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">在给定的吸引点下，每篇评论的主题分数</figcaption></figure><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1382c5ab0ec755958a1dac4988fcdbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*f1nDJa4Vmb8G99IE8gZZpQ.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">每个吸引点下所有主题得分的平均值</figcaption></figure><p id="1432" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这里，我建立了一个推荐系统，用户可以输入他们对旅行的前三个偏好。这些偏好被赋予值1，而其他9个偏好被赋予值0，以创建12维的“用户输入”向量。</p><p id="e155" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用余弦相似性，将用户输入向量与每个景点的平均主题得分向量进行比较，以找到最高得分相似性。返回得分最高的3个结果(景点名称)，按余弦值降序排列。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi od"><img src="../Images/411314e62668a847a98e2263c46340dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*5GoqwqPKTz4zhy-vVykdcw.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">每个景点的分数(排名1-3)输出给用户</figcaption></figure><p id="ff41" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在说了和做了一切之后，我对NLP留下了深深的钦佩。当我开始涉足数据科学时，我从未被它的想法迷住，但我现在明白了为什么许多人迷恋它。尤其是考虑到NLP的最新进展仅仅是开创性的。一个惊人的例子可以从我的另一个项目中看出，这个项目名为“<a class="ae ku" href="https://medium.com/swlh/creating-unbiased-news-using-data-science-dd01b52c109c" rel="noopener"> <strong class="jy ja">利用数据科学</strong> </a>”创造“无偏见的新闻”。</p><h1 id="787e" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak"> Waiiit！我能得到一些推荐吗？</strong></h1><p id="b444" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">是的，当然！我使用streamlit在线部署了推荐系统，您可以单击此处的<a class="ae ku" href="https://share.streamlit.io/navish92/personalized_trip_advisor/main/streamlit_attractions_recommender.py" rel="noopener ugc nofollow" target="_blank">来使用它。</a></p><p id="e48f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在下面评论你的旅行吧！如果您想就任何问题或讨论与我联系，可以通过<a class="ae ku" href="https://www.linkedin.com/in/navishofficial/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。项目代码可以在我的<a class="ae ku" href="https://github.com/navish92/Trip_Advisor_National_Parks" rel="noopener ugc nofollow" target="_blank"> Github repo </a>上找到。</p></div></div>    
</body>
</html>