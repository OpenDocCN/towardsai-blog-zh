<html>
<head>
<title>Why Choose Random Forest and Not Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么选择随机森林而不是决策树</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/why-choose-random-forest-and-not-decision-trees-a28278daa5d?source=collection_archive---------0-----------------------#2020-07-02">https://pub.towardsai.net/why-choose-random-forest-and-not-decision-trees-a28278daa5d?source=collection_archive---------0-----------------------#2020-07-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ad1a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="1005" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">决策树和随机森林简明指南。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/4d038cc84d4cb38d529b068bcfefe0f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UcTadQyEoJRpGioV.png"/></div></div></figure><p id="ab75" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">决策树</em> </strong>属于<strong class="lc ja"> <em class="lw">家族的监督分类算法</em> </strong>。它们在<em class="lw">分类</em>问题上表现相当好，决策路径相对容易解释，算法快速简单。</p><p id="9180" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">决策树的集合版本是随机森林。</strong></p><h1 id="40fa" class="lx ly iq bd lz ma mb mc md me mf mg mh kf mi kg mj ki mk kj ml kl mm km mn mo bi translated">目录</h1><ol class=""><li id="24a0" class="mp mq iq lc b ld mr lg ms lj mt ln mu lr mv lv mw mx my mz bi translated"><strong class="lc ja">决策树</strong></li></ol><ul class=""><li id="745f" class="mp mq iq lc b ld le lg lh lj na ln nb lr nc lv nd mx my mz bi translated"><em class="lw">决策树介绍。</em></li><li id="cb16" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated"><em class="lw">决策树是如何工作的？</em></li><li id="8220" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated"><em class="lw">决策树从零开始实现。</em></li><li id="350e" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated"><em class="lw">决策树的优点&amp;缺点。</em></li></ul><p id="9bd5" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> 2。随机森林</strong></p><ul class=""><li id="618c" class="mp mq iq lc b ld le lg lh lj na ln nb lr nc lv nd mx my mz bi translated"><em class="lw">随机森林简介</em></li><li id="5d1a" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated"><em class="lw">随机森林是如何运作的？</em></li><li id="9e5a" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated"><em class="lw">随机森林的Sci-kit实施</em></li><li id="1201" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated"><em class="lw">优点&amp;缺点随机森林。</em></li></ul></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="c94e" class="lx ly iq bd lz ma nq mc md me nr mg mh kf ns kg mj ki nt kj ml kl nu km mn mo bi translated">决策树</h1><h2 id="c0cb" class="nv ly iq bd lz nw nx dn md ny nz dp mh lj oa ob mj ln oc od ml lr oe of mn iw bi translated">决策树介绍</h2><p id="6445" class="pw-post-body-paragraph la lb iq lc b ld mr ka lf lg ms kd li lj og ll lm ln oh lp lq lr oi lt lu lv ij bi translated">决策树是由节点和分支组成的简单的<strong class="lc ja">树状结构</strong>。在每个节点上，数据根据任何输入要素进行分割，生成两个或更多分支作为输出。这个迭代过程增加了生成分支的数量，并对原始数据进行了分区。这种情况一直持续到生成一个节点，其中所有或几乎所有的数据都属于同一个类，并且不再可能进行进一步的拆分或分支。</p><p id="d5b0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这整个过程生成一个<strong class="lc ja">树状结构。</strong><em class="lw">第一个拆分节点</em>称为<em class="lw">根节点。</em>末端节点被称为叶子，并且与类别标签相关联。从根到叶的路径产生分类规则。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/451df972bdbf2d2119b80b32c9913847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/0*AokvsLnti9x3ZBue.png"/></div></figure><p id="90af" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">假设，你是一名员工，你想吃加工食品。</p><p id="47be" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">你的行动将取决于几种情况。</p><p id="f578" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">如果你不饿，你就不会在瘾君子身上花钱。但是如果你饿了，那么选择就会改变，你的下一步行动取决于你的下一个环境，也就是说，你有没有买午餐？</p><p id="97fb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在，如果你不吃午饭，你的行动将完全取决于你的下一次选择，即它是不是月底？如果是这个月的最后几天，你会考虑不吃饭，否则你不会把它作为一种偏好。</p><p id="d008" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">当做出任何决定涉及到几个选择时，决策树就发挥作用了。现在你必须做出相应的选择，以获得一个有利的结果。</p><p id="758a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">基于树的学习算法被认为是最好的和最常用的<strong class="lc ja">监督学习</strong>方法之一。基于树的方法使预测模型合法化，具有更好的<em class="lw">准确性、稳定性和易于解释。</em>与同代人不同，他们也擅长处理非线性关系。决策树算法被称为<strong class="lc ja"> CART </strong> <strong class="lc ja">(分类和回归树)</strong>。</p><h2 id="351a" class="nv ly iq bd lz nw nx dn md ny nz dp mh lj oa ob mj ln oc od ml lr oe of mn iw bi translated">决策树是如何工作的？</h2><p id="bdfe" class="pw-post-body-paragraph la lb iq lc b ld mr ka lf lg ms kd li lj og ll lm ln oh lp lq lr oi lt lu lv ij bi translated">决策树有两个组成部分:</p><ul class=""><li id="bbbb" class="mp mq iq lc b ld le lg lh lj na ln nb lr nc lv nd mx my mz bi translated"><strong class="lc ja">熵</strong> —它被认为是系统的<strong class="lc ja">随机性。它是节点纯度或杂质的度量。</strong></li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/02840b6d3f28def0bf3a198b5ae08714.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*8jaO_c7lXG2UF1a_.png"/></div></figure><p id="6715" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">当p = 0.5时熵最大，即两个结果具有相同的偏好。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/61dce71bf14a99351359079c7b6e2dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/0*aalu1GmTxwuHOLXM"/></div></figure><ul class=""><li id="6e21" class="mp mq iq lc b ld le lg lh lj na ln nb lr nc lv nd mx my mz bi translated"><strong class="lc ja">信息增益— </strong>是熵的<strong class="lc ja">减少。</strong>是起始节点的不确定性和两个子节点的加权杂质之差。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/4531848f8bf8a4ce1851a3e1179d1ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F93DrQE5SsqkKdKA.png"/></div></div></figure><p id="e45e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">它帮助我们找到决策树的根节点，具有最大<strong class="lc ja">信息增益的节点被视为根节点，因为它具有最大的不确定性。</strong></p><p id="199a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们将首先分割具有<strong class="lc ja">最高信息增益</strong>的特征。这是一个递归过程，直到所有子节点都是纯的，或者直到信息增益为零。</p><blockquote class="on oo op"><p id="96b8" class="la lb lw lc b ld le ka lf lg lh kd li oq lk ll lm or lo lp lq os ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="iq">决策树的目标</em> </strong> <em class="iq">:最大化信息增益，最小化熵</em></p></blockquote><p id="9632" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">假设我们有一个60名学生的样本，有三个变量<em class="lw">性别</em>(男孩/女孩)<em class="lw">班级</em> (XI/十二)，W <em class="lw">八</em> (50到100公斤)。这60人中有15人在闲暇时间踢足球。现在，我们想创建一个模型来预测谁会在空闲时间踢足球？在这个问题中，我们需要根据三者之间的一个非常重要的输入变量来划分闲暇时间踢足球的学生。</p><p id="cd48" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这就是决策树发挥作用的地方，它将根据三个变量的所有值对学生进行分类，并确定变量，从而创建最佳的同质学生集。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ot"><img src="../Images/b96712f7617d82ab0e849b777385a9f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1Mq5s0P5cLqqt8En.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/a9cc0a92eb52dabb9bc155f8c7acc6ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*f6djB5AbJ9AsGAcR.jpeg"/></div></figure><p id="8f40" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">使用决策树，我们可以很容易地解决我们的问题，并根据学生的特点对他们进行分类，如他们是否喜欢在闲暇时间踢足球？</p><h2 id="000f" class="nv ly iq bd lz nw nx dn md ny nz dp mh lj oa ob mj ln oc od ml lr oe of mn iw bi translated">从零开始实现决策树</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/7fe5310f729042eeac6b3fcdcea8fa79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3fl6ZGltBz9EswRW.png"/></div></div><figcaption class="ow ox gj gh gi oy oz bd b be z dk translated">将数据分成不同的部分</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/6173746f028b53a37532e17f957dc931.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/0*Bdtty85lf8ngJlFS.png"/></div><figcaption class="ow ox gj gh gi oy oz bd b be z dk translated">计算熵</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/0b3aee0f28e40922ed2f9826dc0da920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IXGfppX-HtngCaOL.png"/></div></div><figcaption class="ow ox gj gh gi oy oz bd b be z dk translated">计算信息增益</figcaption></figure><p id="03aa" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> Sci-kit学习实现</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/5d1678d9012c291487043beaddf64cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e0FwW3pJ7CSwxV6x.png"/></div></div></figure><p id="cc91" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">可视化你的决策树</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pd"><img src="../Images/5765cfd5dd597bf424993d91e1587460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0MOmzCQRe9kTnMEG.png"/></div></div></figure></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h2 id="d41c" class="nv ly iq bd lz nw nx dn md ny nz dp mh lj oa ob mj ln oc od ml lr oe of mn iw bi translated">决策树的利与弊</h2><p id="07d8" class="pw-post-body-paragraph la lb iq lc b ld mr ka lf lg ms kd li lj og ll lm ln oh lp lq lr oi lt lu lv ij bi translated"><strong class="lc ja">优点</strong></p><ul class=""><li id="c355" class="mp mq iq lc b ld le lg lh lj na ln nb lr nc lv nd mx my mz bi translated">容易理解</li><li id="d417" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">很好地处理分类数据和连续数据。</li><li id="c632" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">适用于大型数据集。</li><li id="838a" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">对异常值不敏感。</li><li id="7f99" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">本质上是非参数的。</li></ul><p id="1505" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">缺点</strong></p><ul class=""><li id="b245" class="mp mq iq lc b ld le lg lh lj na ln nb lr nc lv nd mx my mz bi translated">这些容易过度拟合。</li><li id="f716" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">它可能非常大，因此需要修剪。</li><li id="47ed" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">不能保证最优的树。</li><li id="288b" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">与其他机器学习算法相比，它对数据集的预测精度较低。</li><li id="9c98" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">当有许多类变量时，计算会变得复杂。</li><li id="0beb" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">高方差(模型会随着训练数据的变化而快速变化)</li></ul></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="4b7a" class="lx ly iq bd lz ma nq mc md me nr mg mh kf ns kg mj ki nt kj ml kl nu km mn mo bi translated">随机森林</h1><h2 id="77bc" class="nv ly iq bd lz nw nx dn md ny nz dp mh lj oa ob mj ln oc od ml lr oe of mn iw bi translated">随机森林简介</h2><p id="9005" class="pw-post-body-paragraph la lb iq lc b ld mr ka lf lg ms kd li lj og ll lm ln oh lp lq lr oi lt lu lv ij bi translated">随机森林是另一种强大且最常用的<strong class="lc ja">监督学习</strong>算法。它允许从<strong class="lc ja">超大型数据集中快速识别重要信息。</strong>随机森林的最大优势是，它依靠<strong class="lc ja">各种决策树的集合</strong>来得出任何解决方案。</p><p id="eecd" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这是一个<strong class="lc ja">集成算法</strong>，其中<strong class="lc ja"> </strong>考虑了一个以上相同或不同种类的分类算法的结果。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pe"><img src="../Images/a8b6673aa95e535cc9da82f8006e3e47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DrAmS5L2ekBlAXJy.png"/></div></div><figcaption class="ow ox gj gh gi oy oz bd b be z dk translated"><a class="ae pf" href="https://www.oreilly.com/library/view/tensorflow-machine-learning/9781789132212/d3d388ea-3e0b-4095-b01e-a0fe8cb3e575.xhtml" rel="noopener ugc nofollow" target="_blank">随机森林，来源</a></figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/aad6d233271ee9b79e621af612d80c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rRevZu-6Hv1ZTh3E"/></div></div></figure><p id="e94f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">假设你想去度假，但对目的地感到困惑。所以你决定向你的朋友<em class="lw">拉克沙伊</em>寻求建议。拉克沙伊会问你上一次的假期，你是否喜欢，你在那里做了什么。为了得到精确的结果，他甚至会询问你的偏好，并根据你的评论向你提供建议。在这里，Lakshay使用决策树技术为你提供基于你的回答的反馈。</p><p id="1e66" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是你认为Lakshay的建议有点偏颇，你问了Meghna(你的另一个朋友)，同样的问题。她也提出了建议，但你再次认为这是一个冒险的选择。你重复了这个过程，问了“n”个朋友，同样的问题，现在你到了朋友推荐的一些常见的地方。你收集所有的投票并汇总。你决定去票数最多的地方。这里，你使用了随机森林技术。</p><p id="bf14" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在决策树中，<strong class="lc ja">你越深入，就越容易过度拟合，因为你对数据集越指定。因此，Random Forest通过向您展示决策树通过随机性实现的简单性和准确性来解决这个问题。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2b101e44d7ecd86ce6f674f4fd5043af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C8FFR-vIpcTm7rXJ.png"/></div></div><figcaption class="ow ox gj gh gi oy oz bd b be z dk translated">随机森林=决策树的简单性*随机性带来的准确性</figcaption></figure><h2 id="241d" class="nv ly iq bd lz nw nx dn md ny nz dp mh lj oa ob mj ln oc od ml lr oe of mn iw bi translated">随机森林是如何工作的？</h2><p id="5abd" class="pw-post-body-paragraph la lb iq lc b ld mr ka lf lg ms kd li lj og ll lm ln oh lp lq lr oi lt lu lv ij bi translated">假设我们的数据集中有“<em class="lw"> m </em>特征:</p><ol class=""><li id="95ce" class="mp mq iq lc b ld le lg lh lj na ln nb lr nc lv mw mx my mz bi translated">随机选择"<em class="lw"> k" </em>特征满足条件<em class="lw"> k </em> &lt; <em class="lw"> m. </em></li><li id="2c45" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">在<em class="lw"> k </em>个特征中，通过选择具有<em class="lw">最高信息增益的节点来计算根节点。</em></li><li id="f2d8" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">将节点拆分成子节点。</li><li id="5e5e" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">重复前面的步骤<em class="lw"> n次</em>。</li><li id="03bc" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">你最终会拥有一个由n棵树组成的森林。</li><li id="5a2e" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">执行<em class="lw">引导</em>，即将所有决策树的结果组合在一起。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pg"><img src="../Images/dabacef6e2033a785cc85dc3a972945e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LXBat0WeAywLLHje.png"/></div></div><figcaption class="ow ox gj gh gi oy oz bd b be z dk translated"><a class="ae pf" href="https://www.mql5.com/en/articles/3856" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="436d" class="nv ly iq bd lz nw nx dn md ny nz dp mh lj oa ob mj ln oc od ml lr oe of mn iw bi translated">随机森林的Sci-kit实现</h2><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ph pi l"/></div></figure><h2 id="eeae" class="nv ly iq bd lz nw nx dn md ny nz dp mh lj oa ob mj ln oc od ml lr oe of mn iw bi translated">随机森林的利与弊</h2><p id="2c37" class="pw-post-body-paragraph la lb iq lc b ld mr ka lf lg ms kd li lj og ll lm ln oh lp lq lr oi lt lu lv ij bi translated"><strong class="lc ja">优点:</strong></p><ul class=""><li id="2802" class="mp mq iq lc b ld le lg lh lj na ln nb lr nc lv nd mx my mz bi translated">对异常值稳健。</li><li id="ce0a" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">适用于非线性数据。</li><li id="4def" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">降低过度拟合的风险。</li><li id="f45e" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">在大型数据集上高效运行。</li><li id="5d71" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">比其他分类算法更准确。</li></ul><p id="653d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">缺点:</strong></p><ul class=""><li id="c1b9" class="mp mq iq lc b ld le lg lh lj na ln nb lr nc lv nd mx my mz bi translated">发现随机森林在处理分类变量时是有偏差的。</li><li id="fc58" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">缓慢的训练。</li><li id="fa71" class="mp mq iq lc b ld ne lg nf lj ng ln nh lr ni lv nd mx my mz bi translated">不适合具有大量稀疏特征的线性方法</li></ul></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="288e" class="lx ly iq bd lz ma nq mc md me nr mg mh kf ns kg mj ki nt kj ml kl nu km mn mo bi translated">结论</h1><p id="13da" class="pw-post-body-paragraph la lb iq lc b ld mr ka lf lg ms kd li lj og ll lm ln oh lp lq lr oi lt lu lv ij bi translated">希望这篇文章能帮助你以最好的方式理解决策树和随机森林，并帮助你实际使用它。</p><p id="2f2e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一如既往，非常感谢你的阅读，如果你觉得这篇文章有用，请分享！</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="a800" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">请随意连接:</p><blockquote class="on oo op"><p id="980e" class="la lb lw lc b ld le ka lf lg lh kd li oq lk ll lm or lo lp lq os ls lt lu lv ij bi translated"><em class="iq">LinkedIn ~</em><a class="ae pf" href="https://www.linkedin.com/in/dakshtrehan/" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://www.linkedin.com/in/dakshtrehan/</em></a></p><p id="cacf" class="la lb lw lc b ld le ka lf lg lh kd li oq lk ll lm or lo lp lq os ls lt lu lv ij bi translated"><em class="iq">insta gram ~</em><a class="ae pf" href="https://www.instagram.com/_daksh_trehan_/" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://www.instagram.com/_daksh_trehan_/</em></a></p><p id="d65a" class="la lb lw lc b ld le ka lf lg lh kd li oq lk ll lm or lo lp lq os ls lt lu lv ij bi translated"><em class="iq">Github ~</em><a class="ae pf" href="https://github.com/dakshtrehan" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://github.com/dakshtrehan</em></a></p></blockquote><p id="a04c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">关注更多机器学习/深度学习博客。</p><blockquote class="on oo op"><p id="f429" class="la lb lw lc b ld le ka lf lg lh kd li oq lk ll lm or lo lp lq os ls lt lu lv ij bi translated"><em class="iq">中等~</em><a class="ae pf" href="https://medium.com/@dakshtrehan" rel="noopener"><em class="iq">https://medium.com/@dakshtrehan</em></a></p></blockquote><h1 id="1231" class="lx ly iq bd lz ma mb mc md me mf mg mh kf mi kg mj ki mk kj ml kl mm km mn mo bi translated">想了解更多？</h1><p id="72a7" class="pw-post-body-paragraph la lb iq lc b ld mr ka lf lg ms kd li lj og ll lm ln oh lp lq lr oi lt lu lv ij bi translated"><a class="ae pf" href="https://towardsdatascience.com/detecting-covid-19-using-deep-learning-262956b6f981" rel="noopener" target="_blank">利用深度学习检测新冠肺炎</a></p><p id="7e9f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://towardsdatascience.com/the-inescapable-ai-algorithm-tiktok-ad4c6fd981b8" rel="noopener" target="_blank">无法逃脱的人工智能算法:抖音</a></p><p id="252a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">为什么你要为乔治·弗洛伊德的谋杀和德里的骚乱负责？</p><p id="d78a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://medium.com/@dakshtrehan/clustering-what-it-is-when-to-use-it-a612bbe95881" rel="noopener">聚类:它是什么？什么时候用？</a></p><p id="6abf" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://medium.com/@dakshtrehan/start-off-your-ml-journey-with-k-nearest-neighbors-f72a122f428" rel="noopener">从k个最近邻居开始你的ML之旅</a></p><p id="0024" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://medium.com/swlh/things-you-never-knew-about-naive-bayes-eb84b6ee039a" rel="noopener">朴素贝叶斯解释</a></p><p id="dab2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://medium.com/analytics-vidhya/activation-functions-explained-8690ea7bdec9" rel="noopener">激活功能说明</a></p><p id="8384" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://towardsdatascience.com/parameters-optimization-explained-876561853de0" rel="noopener" target="_blank">参数优化解释</a></p><p id="4e1b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c" rel="noopener" target="_blank">梯度下降解释</a></p><p id="c918" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://towardsdatascience.com/logistic-regression-explained-ef1d816ea85a" rel="noopener" target="_blank">逻辑回归解释</a></p><p id="f52c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://medium.com/towards-artificial-intelligence/linear-regression-explained-f5cc85ae2c5c" rel="noopener">线性回归解释</a></p><p id="eeeb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae pf" href="https://medium.com/datadriveninvestor/determining-perfect-fit-for-your-ml-model-339459eef670" rel="noopener">确定最适合您的ML模型</a></p><blockquote class="on oo op"><p id="93d4" class="la lb lw lc b ld le ka lf lg lh kd li oq lk ll lm or lo lp lq os ls lt lu lv ij bi translated"><em class="iq">干杯！</em></p></blockquote></div></div>    
</body>
</html>