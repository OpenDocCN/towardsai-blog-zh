<html>
<head>
<title>Create Indonesian Recipe Generator by Fine-tuning T5, BART, and GPT-2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过微调T5、BART和GPT-2创建印度尼西亚配方生成器</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/create-indonesian-recipe-generator-by-fine-tuning-t5-bart-and-gpt-2-a7fc0551190e?source=collection_archive---------4-----------------------#2022-05-02">https://pub.towardsai.net/create-indonesian-recipe-generator-by-fine-tuning-t5-bart-and-gpt-2-a7fc0551190e?source=collection_archive---------4-----------------------#2022-05-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f6a0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">通过微调T5、BART、GPT-2等预训练模型训练的印尼菜谱生成器深度学习模型</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/27a4085a0d7e539633d7c709449d74fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oojD2594FgHIsC8B"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">预览图片来自</em> <a class="ae kw" href="https://unsplash.com/photos/HlNcigvUi4Q" rel="noopener ugc nofollow" target="_blank"> <em class="kf"> Unsplash </em> </a> <em class="kf">由布鲁克百灵</em></figcaption></figure><p id="4077" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">大家好！欢迎来到我个人博客上的第一篇技术文章！在这篇文章中，我将写我的一个有趣的项目，印度尼西亚食谱生成器。这是我上一篇<a class="ae kw" rel="noopener ugc nofollow" target="_blank" href="/recibrew-find-out-the-foods-ingredients-dbc2a4e37383">中型文章</a>的延续，采用了更现代的方法。</p><p id="3bef" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这篇文章将告诉你我创建印度尼西亚食谱生成器实验的细节。</p><p id="6356" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">此贴转贴自:<a class="ae kw" href="https://haryoa.github.io/posts/id-recipe-generator/" rel="noopener ugc nofollow" target="_blank">https://haryoa.github.io/posts/id-recipe-generator/</a></p><p id="a59a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">你可以去参观一下，以获得良好的阅读体验。</p><h1 id="e156" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">存储库和演示</h1><p id="3d1a" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我会提供最好的模型，也是训练它的代码。</p><p id="f15c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">放心尝试下载吧:)。</p><ul class=""><li id="a9fb" class="mq mr iq kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">🤗拥抱面部空间(演示):<a class="ae kw" href="https://huggingface.co/spaces/haryoaw/id-recigen" rel="noopener ugc nofollow" target="_blank">空间</a></li><li id="1407" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">🤗拥抱脸模型(下载模型):<a class="ae kw" href="https://huggingface.co/haryoaw/id-recigen-bart" rel="noopener ugc nofollow" target="_blank">模型</a></li><li id="a25e" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">✨库(训练模型):<a class="ae kw" href="https://github.com/haryoa/idrecibrew2" rel="noopener ugc nofollow" target="_blank"> GitHub库</a></li></ul><h1 id="7eb4" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">介绍</h1><p id="fd10" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">过去，我通过使用seq2seq深度学习方法，如门控循环单元(GRU)和变压器，在我的<a class="ae kw" rel="noopener ugc nofollow" target="_blank" href="/recibrew-find-out-the-foods-ingredients-dbc2a4e37383">中期帖子</a>中创建了一个印度尼西亚配方生成器。我想重温我过去的工作，并加以改进。所以，这篇文章将会对前一篇文章做一些改进。</p><p id="2502" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我上一篇文章中的模型是原始的，没有经过预先训练。所以，它在训练过程中没有任何先验知识可以利用。我在以前的博客中提到的可以提高模型质量的改进之一是使用预先训练的模型。许多先进的研究工作实现它，以提高非预训练模型的质量。此外，目前是深度学习中预训练模型的时代。如今，当模型在目标数据集上再次训练(我们称之为<code class="fe ne nf ng nh b">fine-tune</code>)时，许多预训练模型出现并具有突出的结果。因此，在我的recipe generator项目中尝试它很有意思。</p><p id="004c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这个实验中，我将使用现成的公开可用的预训练模型。由于数据是印度尼西亚语言，我需要使用用印度尼西亚数据预先训练的模型。他们还需要处理序列生成问题，因为我要解决的问题是文本生成问题。我搜索了一下，只找到了T5、巴特和GPT型号。因此，我决定使用这些模型进行实验。</p><h1 id="12c1" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">数据，预处理，勘探数据分析</h1><p id="bfa8" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">由于这是我之前项目的延续，我使用了之前使用的相同数据。你可以通过我下面的文章来阅读更多的细节。</p><div class="ni nj gp gr nk nl"><a rel="noopener  ugc nofollow" target="_blank" href="/recibrew-find-out-the-foods-ingredients-dbc2a4e37383"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ir gy z fp nq fr fs nr fu fw ip bi translated">🍖🍲Recibrew！用深度学习预测食物成分！！🍲🍖</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">▶️▶️一步一步地讲述了我在深度学习中使用seq2seq预测食物成分的有趣自我项目…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">pub.towardsai.net</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz kq nl"/></div></div></a></div><h1 id="d442" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">方法</h1><p id="911a" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在这一节中，我将描述我已经尝试过并在上面提到的模型。我使用的所有模型都是基于变压器的模型。我将简要描述它们。</p><h1 id="7df7" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">巴特</h1><p id="79e9" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">BART是一个基于转换器的模型，通过学习损坏的输入进行预训练。它有一个类似于Transformer模型的编码器-解码器架构，并做了一些修改，比如替换了激活函数。BART的作者尝试了几个被破坏的场景。发布的最终系统是通过<em class="oa">句子洗牌</em>和<em class="oa">令牌屏蔽</em>训练出来的模型。这个想法是让BART学习扰动，并具有进行因果语言建模的能力。为了将其应用于系统，我根据数据对模型进行了微调。这是一个预先训练好的BART是如何建造的图示。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8cf5eaf28d652285c67d03f4a7389663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*fa8b9CfVQbypwQ6K.jpg"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">巴特预训。它使用损坏的输入(句子重排和标记屏蔽)来预测未损坏的输入</em></figcaption></figure><p id="bf1e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这个实验中，我使用IndoBART作为本文中发布的预训练模型。该模型通过Indo4B数据集、通用抓取和维基百科进行预训练。数据包含印度尼西亚语、巽他语和爪哇语。它在Huggingface模型中公开提供。</p><h1 id="9783" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">T5</h1><p id="8b9e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">T5也是一个基于变压器的模型，使用损坏的输入进行预训练。T5通过进行<em class="oa">令牌屏蔽</em>进行预训练。与使用数据进行因果语言模型训练的BART不同，T5使用训练数据作为seq2seq问题。数据可能包含翻译、问题回答和分类问题。该模型通过学习这些任务以及学习被破坏的输入来进行反馈。这是一个如何预先训练模型的例子。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oc"><img src="../Images/ea109d217e8cb258beb184d341e63ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O2H-8YRxpuoaoXi_.jpg"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf"> T5前期训练。它使用几个具有提示风格的任务作为模型的输入。</em></figcaption></figure><p id="353b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我用的是T5型号，在HuggingFace中有售。通过使用印度尼西亚mC4数据集对其进行预训练。</p><h1 id="6ccc" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">通用终端</h1><p id="5522" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">GPT-2是一种自动回归预训练模型，通过因果语言建模进行预训练，对输入没有干扰(不同于BART和T5)。然后，根据我们的数据对预训练模型进行微调。我使用了IndoGPT模型，该模型也与IndoBART一起在同一篇论文中发布。该模型也使用与IndoBART相同的数据进行预训练。</p><p id="c2aa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">由于模型不是编码器-解码器架构，我们需要重塑我们的输入，使之成为一个语言建模问题。</p><h1 id="6c6b" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">设置</h1><p id="abea" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我将把这一部分分为代码技术设置、模型设置和超参数设置。</p><h1 id="b4b6" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">代码技术设置</h1><p id="1418" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">为了制作训练脚本，我使用Pytorch作为深度学习框架。我用Pytorch Lightning <a class="ae kw" href="https://haryoa.github.io/posts/id-recipe-generator/#fn:pl-lit" rel="noopener ugc nofollow" target="_blank"> 6 </a>把它们包起来。我使用了Pytorch Lightning的模型检查点、提前停止和16位精度的实现。</p><p id="e256" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于度量计算，我使用BLEU评分。BLEU分数是序列间问题的一个流行的度量。我使用来自<code class="fe ne nf ng nh b">sacrebleu</code> Python包的现成BLEU score实现。</p><h1 id="4d10" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">模型设置</h1><p id="e0ac" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我对模型的输入进行了一些修改。对于架构，我使用了Huggingface提供的现成实现。</p><p id="1ae9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于<strong class="kz ir"> GPT </strong>，由于它需要一个输入，我用一个特殊的符号<code class="fe ne nf ng nh b">&gt;&gt;&gt;</code>将食物名称和食谱联系成一个输入。</p><pre class="kh ki kj kk gt od nh oe of aw og bi"><span id="7e64" class="oh lu iq nh b gy oi oj l ok ol">Input: &lt;FOOD&gt; &gt;&gt;&gt; &lt;INGREDIENTS&gt;<br/>Output: Shift the input (e.g.: Input: `Apple Fruit end_of_token`, Output: `Fruit end_of_token`)</span></pre><p id="542f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> T5 </strong>有seq2seq架构，所以我对输入做了小修改。据我所知，T5预先训练了“提示”式输入。例如:<code class="fe ne nf ng nh b">input: summarize: &lt;ARTICLE&gt;</code>。所以，我跟着它，把数据改成那样。下面是我如何呈现模型的输入输出</p><pre class="kh ki kj kk gt od nh oe of aw og bi"><span id="2385" class="oh lu iq nh b gy oi oj l ok ol">Input: resep: &lt;FOOD&gt;<br/>Output: &lt;INGREDIENTS&gt;</span></pre><p id="9b6b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我没有对<strong class="kz ir"> BART </strong>模型做任何修改，所以我按原样提供输入和输出。</p><pre class="kh ki kj kk gt od nh oe of aw og bi"><span id="accb" class="oh lu iq nh b gy oi oj l ok ol">Input: &lt;FOOD&gt;<br/>Output: &lt;INGREDIENTS&gt;</span></pre><h1 id="161f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">超参数设置</h1><p id="9d43" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我用亚当作为优化技术。学习速度因架构而异。我根据几个资源精心挑选了几个学习率值，并尝试了其中一些值。我分别选择<code class="fe ne nf ng nh b">1e-4</code>、<code class="fe ne nf ng nh b">1e-5</code>和<code class="fe ne nf ng nh b">1e-4</code>作为GPT、巴特和T5模型的学习率。我使用早期停止标准来避免模型过度拟合。如果验证损失在5个时期内没有增加，它将停止训练。为了挑选最佳模型，我使用了验证损失最低的模型。我使用AdamW作为模型的优化器。</p><p id="5cc9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了使训练更快，我使用Pytorch提供的自动混合精度(AMP)。可惜T5不能用AMP。所以，我在微调T5车型的时候没有用AMP。</p><p id="fc0b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">按照我以前的文章，为了进行公平的比较，我使用贪婪解码器作为解码策略来预测每个模型的输出。你可以在我过去的博客中看到关于贪婪解码器如何工作的细节。</p><h1 id="8492" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">实验结果</h1><p id="ee9d" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">下面是我实验的结果。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi om"><img src="../Images/23eace339fb9019375b5990dfca00442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*aZNTF8BNi2Ioc-Oq6HibjA.png"/></div></figure><p id="e177" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在我的设置下，IndoBART优于其他型号。T5、IndoBART和IndoGPT的BLEU分数高于transformer vanilla。这表明预训练的seq2seq模型可能有助于提高模型的性能。根据Indobenchmark的数据训练的所有模型都优于根据C4模型(T5)训练的模型。看到每个预训练模型的潜力是很有趣的。</p><h1 id="8366" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">分析</h1><p id="be81" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">请访问我的博客，以获得最佳体验来看到这一部分。</p><h1 id="3531" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi on"><img src="../Images/55e6492f6b52a0d804652f33eeba347f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PFRS8COl4M2QHuYB"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">随机猫~。照片由</em> <a class="ae kw" href="https://unsplash.com/@madhatterzone?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <em class="kf">曼雅</em> </a> <em class="kf">上</em><a class="ae kw" href="https://unsplash.com/s/photos/cat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"><em class="kf">Unsplash</em></a><em class="kf">。</em></figcaption></figure><p id="4978" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这篇文章中，我使用预先训练好的模型试验了一个印度尼西亚食谱生成器。根据BLEU评分，IndoBART的表现优于其他车型。我们还可以得出结论，微调预训练的模型通常比非预训练的模型更好。有趣的是看到它真的有效！</p><p id="7f7a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">其实这里有很多东西可以探讨。例如，观察BART、T5和GPT的预训练与非预训练的效果是很有趣的。我还需要对训练好的模型做一些严格的分析。遗憾的是，由于我的资源有限，我现在还做不到。</p><p id="cd83" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">以后打算写seq2seq模型目前的进展。2022年机器学习会议有很多新的有趣的发表论文。我会研究并在我的博客中写下它。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oo"><img src="../Images/c4aaddd0b29bba9a12b4e24820d8e3ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*z7-v7CXbb7fO9rxK.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk translated">来源:<a class="ae kw" href="https://pixabay.com/illustrations/thank-you-polaroid-letters-2490552/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>作者:<a class="ae kw" href="https://pixabay.com/illustrations/thank-you-polaroid-letters-2490552/" rel="noopener ugc nofollow" target="_blank">杰洛特</a></figcaption></figure></div></div>    
</body>
</html>