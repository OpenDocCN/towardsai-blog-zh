<html>
<head>
<title>Understanding Multiple Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解多元线性回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understanding-multiple-linear-regression-1b4a5b939f5a?source=collection_archive---------0-----------------------#2019-06-19">https://pub.towardsai.net/understanding-multiple-linear-regression-1b4a5b939f5a?source=collection_archive---------0-----------------------#2019-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="72e5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="bc79" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用矩阵代数…</h2></div><p id="ff38" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi ln translated"><span class="l lo lp lq bm lr ls lt lu lv di">H</span><strong class="kt jd">T5】ello World</strong>，有没有想过你最喜欢的机器学习算法实际上是如何工作的？让我们看看多元线性回归(<strong class="kt jd"> MLR </strong>)模型如何计算理想参数，给定特征矩阵(<strong class="kt jd"><em class="lw">【X】)</em></strong>和目标变量(<strong class="kt jd"> <em class="lw"> y)，</em> </strong>使用<strong class="kt jd"> <em class="lw">线性代数。</em> </strong></p><p id="4b58" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="lw"> X </em> </strong>是an ( <strong class="kt jd"> <em class="lw"> m * n特征矩阵)</em> </strong>和<em class="lw"/>y是an( <strong class="kt jd"> <em class="lw"> m* 1列向量</em> </strong>)</p><p id="f60f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="lw">其中:</em> </strong></p><p id="2fd3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="lw"> m </em> </strong>是数据集中观察值或行数</p><p id="8032" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="lw"> n </em> </strong>是为预测选择的属性或变量的数量</p><p id="6ca6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="lw"> X </em> </strong>是<strong class="kt jd"> <em class="lw"> m * n </em> </strong>特征矩阵的自变量</p><p id="45bf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="lw"> y </em> </strong>是目标或因变量</p><p id="42fc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我假设您知道线性回归的基础知识，并且能够轻松地完成整个简单线性回归(SLR)过程。</p><p id="756d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我<strong class="kt jd">强烈</strong>推荐你通过这个<a class="ae lx" href="https://medium.com/towards-artificial-intelligence/understanding-the-simple-maths-behind-simple-linear-regression-3ce4a30e7602" rel="noopener"> <strong class="kt jd"> <em class="lw">链接</em> </strong> </a>到我关于单反的文章。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/c3ee167e5bef83bbfdf74e3498bdf75c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRr14SjfGfL3cGMYBVKEwA.jpeg"/></div></div></figure><p id="f79f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是一种中间方法，需要对<em class="lw">线性代数</em>和向量或/和矩阵乘法、矩阵的逆和转置……也称为<strong class="kt jd"> <em class="lw">矩阵代数有所了解。</em>T75】</strong></p><p id="ffd6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">作为复习，我的朋友<strong class="kt jd"><em class="lw">H</em></strong><em class="lw">arvey</em><strong class="kt jd"><em class="lw">B</em></strong><em class="lw">erman</em>at<a class="ae lx" href="https://stattrek.com/" rel="noopener ugc nofollow" target="_blank"><strong class="kt jd"><em class="lw">stattrek.com</em></strong></a>有丰富的教程来指导你哦！他也回复他的电子邮件。</p><blockquote class="mk ml mm"><p id="679c" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated">好了，我们开始吧！</p></blockquote><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mq"><img src="../Images/c913d28efc2ea9f8abf836912205126c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxlx4rfOOw1Z61Irb3EXmQ.jpeg"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">开始吧…|<a class="ae lx" href="https://artfiles.alphacoders.com/334/33416.jpg" rel="noopener ugc nofollow" target="_blank"><em class="mv">img _ credit</em></a></figcaption></figure><h1 id="cf85" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">几个关键点…</h1><ol class=""><li id="af7d" class="no np it kt b ku nq kx nr la ns le nt li nu lm nv nw nx ny bi translated"><strong class="kt jd"> <em class="lw">目标</em> </strong>或<strong class="kt jd"> <em class="lw">因变量</em> </strong> ( <strong class="kt jd"> <em class="lw"> y </em> </strong>)必须是连续的，但<strong class="kt jd"> <em class="lw">特征</em> </strong>或<strong class="kt jd"> <em class="lw">自变量</em></strong>(<strong class="kt jd"><em class="lw">x1…xn</em></strong>)可以是分类的，也可以是连续的。</li><li id="84d0" class="no np it kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated">每个<em class="lw">特征</em>必须与<em class="lw">目标</em>成线性关系。</li><li id="fd90" class="no np it kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated">多元线性回归(<strong class="kt jd"> MLR </strong>)方程的形式如下</li></ol><p id="da8c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"><em class="lw">y _ hat = B0+b1x 1+B2 x2+b3x 3+…bnxn</em></strong></p><h2 id="0b92" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated">其中:</h2><p id="2b7b" class="pw-post-body-paragraph kr ks it kt b ku nq kd kw kx nr kg kz la op lc ld le oq lg lh li or lk ll lm im bi translated"><strong class="kt jd"> <em class="lw"> y_hat </em> </strong>是模型预测</p><p id="2afb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="lw"> b0 …bn </em> </strong>为模型的系数，以<strong class="kt jd"> <em class="lw"> b0 </em> </strong>为截距或偏差单位，<strong class="kt jd"> <em class="lw"> b1 …bn </em> </strong>为自变量的梯度或斜率(<strong class="kt jd"><em class="lw">x1</em></strong>…<strong class="kt jd"><em class="lw">【xn】</em></strong>)。</p><p id="e7ab" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，每个特征(<strong class="kt jd"> <em class="lw"> x1…xn </em> </strong>)都有其伴随的梯度或斜率(<strong class="kt jd"> <em class="lw"> b1…bn </em> </strong>)。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi os"><img src="../Images/39cfebceaa21c6fdd723085cf126a078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9t8xAeT7SEjPu304kCchlg.jpeg"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">矩阵代数…|<a class="ae lx" href="http://www.ma.man.ac.uk/" rel="noopener ugc nofollow" target="_blank"><em class="mv">img _ credit</em></a></figcaption></figure><h1 id="2fb2" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">探索真实世界的数据</h1><p id="0145" class="pw-post-body-paragraph kr ks it kt b ku nq kd kw kx nr kg kz la op lc ld le oq lg lh li or lk ll lm im bi translated">我们将使用加拿大汽车销售的燃油消耗率数据集。<br/> <a class="ae lx" href="https://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="lw">(原厂油耗评级2000–2014)</em></strong></a>。</p><p id="a2cf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以在这个<a class="ae lx" href="https://raw.githubusercontent.com/Blackman9t/Machine_Learning/master/Original_2000_2014_Fuel_Consumption_Ratings.csv" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>上看到原始文件。</p><p id="67e2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">导入所需的库</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="2079" class="oe mx it ou b gy oy oz l pa pb"># importing required libraries</span><span id="bae6" class="oe mx it ou b gy pc oz l pa pb">import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import seaborn as sns<br/>import pylab as pl<br/>import numpy as np</span></pre><p id="2bc6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">理解数据</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="da33" class="oe mx it ou b gy oy oz l pa pb"><strong class="ou jd">FuelConsumption.csv</strong><strong class="ou jd">:</strong></span><span id="3609" class="oe mx it ou b gy pc oz l pa pb">We have downloaded a fuel consumption dataset, <strong class="ou jd">FuelConsumption.csv</strong>, which contains model-specific fuel consumption ratings and estimated carbon dioxide emissions for new light-duty vehicles for retail sale in Canada.<br/><strong class="ou jd">MODELYEAR</strong> e.g. 2014<br/><strong class="ou jd">MAKE</strong> e.g. Acura<br/><strong class="ou jd">MODEL</strong> e.g. ILX<br/><strong class="ou jd">VEHICLE CLASS</strong> e.g. SUV<br/><strong class="ou jd">ENGINE SIZE</strong> e.g. 4.7<br/><strong class="ou jd">CYLINDERS</strong> e.g 6<br/><strong class="ou jd">TRANSMISSION</strong> e.g. A6<br/><strong class="ou jd">Fuel Type<br/>FUEL CONSUMPTION in CITY(L/100 km)</strong> e.g. 9.9<br/><strong class="ou jd">FUEL CONSUMPTION in HWY (L/100 km)</strong> e.g. 8.9<br/><strong class="ou jd">FUEL CONSUMPTION COMB (L/100 km)</strong> e.g. 9.2<br/><strong class="ou jd">COMB_(mpg)<br/>CO2 EMISSIONS (g/km)</strong> e.g. 182 → low → 0</span></pre><p id="076d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们从GitHub下载数据集</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="fa31" class="oe mx it ou b gy oy oz l pa pb"># First Let's import our Data set<br/>data = "<a class="ae lx" href="https://raw.githubusercontent.com/Blackman9t/Machine_Learning/master/Original_2000_2014_Fuel_Consumption_Ratings.csv" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/Blackman9t/Machine_Learning/master/Original_2000_2014_Fuel_Consumption_Ratings.csv</a>"</span><span id="818c" class="oe mx it ou b gy pc oz l pa pb"># Next let's define additional representation of null values <br/># To help us select all possible null values in case they exist</span><span id="a6ec" class="oe mx it ou b gy pc oz l pa pb">missing_data = [’n/a’,’na’,’--’,’?’,’non’,’Non’,’None’]</span></pre><p id="77b8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们读取一个pandas数据框并打印出前5行。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="98d0" class="oe mx it ou b gy oy oz l pa pb">fuel_df = pd.read_csv(data, na_values=missing_data)</span><span id="9fc6" class="oe mx it ou b gy pc oz l pa pb">#Next let's print out the first five rows</span><span id="6091" class="oe mx it ou b gy pc oz l pa pb">fuel_df.head()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pd"><img src="../Images/80aa2de8b94ce6ac9f41c38a45328833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zklNb3yFdbRXgHC5THVX7w.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">数据集的头部，显示属性(列标题)和特征(列数据内容)</figcaption></figure><p id="f344" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们检查一下形状:</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="ccb7" class="oe mx it ou b gy oy oz l pa pb">fuel_df.shape</span><span id="738a" class="oe mx it ou b gy pc oz l pa pb">&gt;&gt;<br/>  (14343, 13)</span><span id="857f" class="oe mx it ou b gy pc oz l pa pb"># 14343 rows, 13 cols</span></pre><p id="8080" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们检查缺失的值。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="b369" class="oe mx it ou b gy oy oz l pa pb">fuel_df.isna().any().all()</span><span id="994d" class="oe mx it ou b gy pc oz l pa pb">&gt;&gt;<br/>False</span></pre><p id="b081" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了简洁起见，让我们重命名一些属性。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="208b" class="oe mx it ou b gy oy oz l pa pb">fuel_df.rename(columns={'FUEL_CONSUMPTION_CITY(L/100km)':'FUEL_CONS_CITY', <br/>                        'ENGINE_SIZE(L)':'ENGINE_SIZE',<br/>                       'HWY_(L/100km)':'HWY_L100km',<br/>                       'COMB_(L/100km)':'COMB_L100km',<br/>                       'COMB_(mpg)':'COMB_MPG',<br/>                       'CO2_EMISSIONS(g/km)':'CO2_EMISSIONS'},<br/>                       inplace=True)</span></pre><p id="35be" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们来看看变化</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="121b" class="oe mx it ou b gy oy oz l pa pb">fuel_df.head()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pe"><img src="../Images/a3ea051f98bad549617da29b287ac627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XCKJ1ybdhiXpvsMFF7oNTQ.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">重命名的属性</figcaption></figure><h2 id="5592" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated">计算MLR模型的理想系数。</h2><p id="25af" class="pw-post-body-paragraph kr ks it kt b ku nq kd kw kx nr kg kz la op lc ld le oq lg lh li or lk ll lm im bi translated">要手动计算多元线性回归模型的理想系数，我们必须定义三个矩阵。参见<a class="ae lx" href="https://stattrek.com/multiple-regression/regression-coefficients.aspx?Tutorial=reg" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="lw">链接</em> </strong> </a></p><blockquote class="mk ml mm"><p id="1ac8" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated"><strong class="kt jd">注</strong>:一个<a class="ae lx" href="https://newonlinecourses.science.psu.edu/stat501/node/382/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">列向量</strong> </a>是一个<strong class="kt jd"> m </strong> × 1矩阵，也就是只有一列的矩阵</p></blockquote><p id="c334" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以矩阵是:-</p><ol class=""><li id="6496" class="no np it kt b ku kv kx ky la pf le pg li ph lm nv nw nx ny bi translated"><strong class="kt jd"><em class="lw">【X】，</em> </strong>包含我们的预测变量，实际上是一个(<strong class="kt jd"><em class="lw">m</em></strong>*(1+<strong class="kt jd"><em class="lw">n</em></strong>)特征矩阵)…因为截距或偏差单位而添加的一列1，<strong class="kt jd"> <em class="lw"> b0 </em> </strong>。</li><li id="821b" class="no np it kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated"><strong class="kt jd"> <em class="lw"> b </em> </strong>我们的模型系数是一个((<strong class="kt jd"> <em class="lw"> n + </em> </strong> 1)* 1列向量)…为<strong class="kt jd"> <em class="lw"> b0增加了1个元素。</em> </strong></li><li id="c9cf" class="no np it kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated"><strong class="kt jd"> <em class="lw"> y </em> </strong>我们的目标或因变量是哪一个(<strong class="kt jd"> <em class="lw"> m </em> </strong> * 1列向量)。</li></ol><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/8a260b480313ad508e7450ea6ff207b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*9FCGhv7E3HJ5j8vknUN-BA.jpeg"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">别担心，伙计，跟我在一起…这有点直觉。</figcaption></figure><p id="ccd7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae lx" href="https://i0.wp.com/www.planetclaire.tv/wp-content/uploads/2015/04/the-simpsons-season-17.jpg?resize=350%2C200&amp;ssl=1" rel="noopener ugc nofollow" target="_blank"> <em class="lw"> img_credit </em> </a></p><p id="cfcf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">不要太担心符号，这是代数，lol… <em class="lw">和我呆几分钟</em>。记住这个…</p><blockquote class="mk ml mm"><p id="1b4b" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated"><strong class="kt jd">y _ hat = B0+b1x 1+B2 x2+b3x 3+…bnxn</strong></p><p id="be8f" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated">其中:</p><p id="59b6" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated"><strong class="kt jd"> y_hat </strong>是模型预测</p><p id="f134" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated"><strong class="kt jd"> b0 …bn </strong>是模型的系数或参数，</p><p id="2dc6" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated"><strong class="kt jd"> b0 </strong>是截距或偏差单位，因此矩阵<strong class="kt jd"> X、</strong>中增加了一列1，这样当模型编译时，<em class="it"> 1 </em>将乘以<strong class="kt jd"> b0 </strong>的值，以产生截距或偏差单位。参见<a class="ae lx" href="https://stattrek.com/multiple-regression/regression-coefficients.aspx?Tutorial=reg" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">链接</strong> </a></p><p id="0d2b" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated">然后，每个特征(<strong class="kt jd"> x1…xn </strong>)都有其伴随的梯度或斜率(<strong class="kt jd"> b1…bn </strong>)。</p></blockquote><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/d99bf073a8f26aebda7e878d367285da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C1tppqMGt04SwAMZwtzFLA.jpeg"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">一步一步来...| <a class="ae lx" href="https://i.ytimg.com/vi/MhLQQ0cAbSo/maxresdefault.jpg" rel="noopener ugc nofollow" target="_blank"> <em class="mv"> img_credit </em> </a></figcaption></figure><p id="8bdb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在多元线性回归中，我们试图使用两个或更多的特征或自变量来预测一个连续的因变量。</p><p id="7bc6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这种情况下，我们希望使用数据集中的两个或更多特征来预测CO2 _排放量。特征可以是发动机尺寸、气缸、品牌、型号等</p><p id="5103" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，为了定义我们的特征矩阵<strong class="kt jd"><em class="lw">【X】，</em> </strong>，我们需要从数据集中选择理想的特征，即<a class="ae lx" href="https://cognitiveclass.ai/courses/data-analysis-python/" rel="noopener ugc nofollow" target="_blank"><strong class="kt jd"><em class="lw"/></strong></a>与<strong class="kt jd"><em class="lw"/></strong>CO2 _排放相关。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pj"><img src="../Images/bbc3f6cb97e01e68595b60bee93a6496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yz_b7ZiFky34N4FvWJI5NQ.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">相关性可以判断两个或多个变量是否相互依赖，以及相互依赖的程度。</figcaption></figure><p id="1cd5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">相关性产生一个介于-1和1之间的数字。如果数字接近-1，表示强负相关，如果接近1，表示强正相关，如果接近0，表示变量间弱相关。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/7933de90d4bebe7f487a000c6eaf5c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*xkBDTQYmB3qvjemv0XCqFw.jpeg"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">负相关向下倾斜，正相关向上倾斜，弱相关很少或没有倾斜</figcaption></figure><p id="efbd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">查看这个<a class="ae lx" href="https://www.coursera.org/learn/data-analysis-with-python/lecture/lb1Hl/correlation" rel="noopener ugc nofollow" target="_blank"> Coursera课程</a>了解更多详情。</p><h2 id="5d01" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated">定义特征矩阵(X)和目标矩阵(y)</h2><p id="c2e8" class="pw-post-body-paragraph kr ks it kt b ku nq kd kw kx nr kg kz la op lc ld le oq lg lh li or lk ll lm im bi translated">首先让我们看看我们的变量是如何相关的…</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="990f" class="oe mx it ou b gy oy oz l pa pb"># Let's plot the correlation of the data set using a heatmap from seaborn library</span><span id="92ef" class="oe mx it ou b gy pc oz l pa pb">corr_data = fuel_df.corr()</span><span id="bb82" class="oe mx it ou b gy pc oz l pa pb">plt.figure(figsize=(10, 6))<br/>sns.set_style('ticks')</span><span id="20c7" class="oe mx it ou b gy pc oz l pa pb">sns.heatmap(corr_data, annot=True)</span><span id="2659" class="oe mx it ou b gy pc oz l pa pb">plt.show()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pl"><img src="../Images/c8e3aa2a6b81db81c1c6c6d5d1a1a2d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dItbTNEi2KTR2-QzgJgtcQ.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">使用seaborn library的热图进行变量关联…较浅的颜色表示较强的相关性，反之亦然。</figcaption></figure><p id="9f0b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以看到，六(6)个变量与CO2排放密切相关。这些如下所示</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pm"><img src="../Images/3bd4ab4ee6c87ba784e857c81d67fb81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4o2vBOwtAcyBBSoZlkCrA.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">与Co2排放密切相关的六个变量以及对每种关系类型的描述。</figcaption></figure><p id="b799" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">只要看看上面的相关热图，我们就可以看到一些预测变量与其他预测变量高度相关。例如<em class="lw">燃料消耗城市</em>和<em class="lw">梳齿L/100 </em>。这种情况叫做<a class="ae lx" href="https://stattrek.com/multiple-regression/multicollinearity.aspx" rel="noopener ugc nofollow" target="_blank"><strong class="kt jd"><em class="lw"/></strong></a><strong class="kt jd"><em class="lw">。</em>T29】</strong></p><p id="7735" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">中度多重共线性可能不会造成问题，但严重的情况可能会导致:-</p><ul class=""><li id="6b66" class="no np it kt b ku kv kx ky la pf le pg li ph lm pn nw nx ny bi translated">回归系数的估计不可靠。<a class="ae lx" href="https://stattrek.com/multiple-regression/multicollinearity.aspx" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="lw">链接</em> </strong> </a></li><li id="1d03" class="no np it kt b ku nz kx oa la ob le oc li od lm pn nw nx ny bi translated"><a class="ae lx" href="https://blog.minitab.com/blog/adventures-in-statistics-2/what-are-the-effects-of-multicollinearity-and-when-can-i-ignore-them" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="lw">多重共线性会削弱分析的统计能力</em> </strong> </a>会导致系数转换符号，扭曲正确的模型。</li></ul><p id="9d7d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们通过丢弃冗余的(<em class="lw">高度相关的</em>)预测变量来解决这种情况</p><p id="2a31" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，我们需要使用一种叫做<a class="ae lx" href="https://etav.github.io/python/vif_factor_python.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="lw">方差-膨胀-因子</em></strong></a><strong class="kt jd"><em class="lw">【VIF】</em></strong>的技术来测量多重共线的程度。VIF是多元回归中预测变量之间共线性的一种度量。如果结果是1，没问题。如果它在1到5之间，它显示低到平均共线性，高于5通常意味着高度冗余，变量应该被删除。</p><p id="17f9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">计算VIF: </strong></p><p id="07a3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，让我们选择6个与二氧化碳排放量高度相关的变量。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="3002" class="oe mx it ou b gy oy oz l pa pb">data = fuel_df[['ENGINE_SIZE','CYLINDERS','FUEL_CONS_CITY','COMB_MPG', 'HWY_L100km', 'COMB_L100km']]</span><span id="bb54" class="oe mx it ou b gy pc oz l pa pb">data.head()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi po"><img src="../Images/ab21f010024ee1304556eb06c529ddec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u7pNwFhun0_ExN93Qi5t1A.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">显示前5行最高相关变量</figcaption></figure><p id="deb3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，让我们定义一个简单的方法来标准化数据</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="77ce" class="oe mx it ou b gy oy oz l pa pb">def standardize(data_features):<br/>    data_features = (data_features - data_features.mean()) /   data_features.std()<br/>    return data_features</span></pre><p id="88fa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们对每个特性应用标准化方法，并将其保存到一个新变量中。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="2ec1" class="oe mx it ou b gy oy oz l pa pb">std_data = data.apply(standardize, axis=0)<br/>std_data.head()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pp"><img src="../Images/835e62bce71fdd6260d4087b5dd4bdfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gnSEr9djOKVJcptJVGP9PA.png"/></div></div></figure><p id="1b9e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，我们计算VIF</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="be62" class="oe mx it ou b gy oy oz l pa pb">from statsmodels.stats.outliers_influence import variance_inflation_factor</span><span id="6f04" class="oe mx it ou b gy pc oz l pa pb">vif = pd.DataFrame()<br/>vif["VIF_Factor"] = [variance_inflation_factor(std_data.values, i) for i in range(std_data.shape[1])]<br/>vif["features"] = std_data.columns</span><span id="eb25" class="oe mx it ou b gy pc oz l pa pb">vif<br/># This displays the following:-</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pq"><img src="../Images/2c84cf2f7484c10716aac96406b67be6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-K_osZJYWaRMsvWM45QnA.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">显示要素或自变量多重共线性得分的VIF</figcaption></figure><p id="3ccf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如所怀疑的，三个变量(<strong class="kt jd"><em class="lw">FUEL _ CONS _城市，HWY_L/100km，COMB_L100Km </em> </strong>)具有极高的共线性值，应删除。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="c11d" class="oe mx it ou b gy oy oz l pa pb"># Dropping variables with extreme colinearity scores</span><span id="bf38" class="oe mx it ou b gy pc oz l pa pb">std_data.drop(['COMB_L100km','HWY_L100km','FUEL_CONS_CITY'], axis=1, inplace=True)</span></pre><p id="af11" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们计算剩余三个变量的VIF</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="3f9c" class="oe mx it ou b gy oy oz l pa pb">vif = pd.DataFrame()<br/>vif["VIF_Factor"] = [variance_inflation_factor(std_data.values, i) for i in range(std_data.shape[1])]<br/>vif["features"] = std_data.columns</span><span id="d954" class="oe mx it ou b gy pc oz l pa pb">vif<br/># This outputs the following:-</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pr"><img src="../Images/6c5b200d2f8caa5724c33858a24f05ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Omf1kQpzGjL3YrWVQfBmxw.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">选定独立变量的中等VIF分数</figcaption></figure><p id="ac2a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">相对适中的VIF评分分别为6.56、5.57和2.47，我们可以选择<strong class="kt jd"> <em class="lw">发动机_大小、</em> </strong>和<strong class="kt jd"> <em class="lw"> COMB_MPG </em> </strong>作为我们的自变量。</p><p id="ef7b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">定义特征矩阵(<em class="lw"> X </em>)和目标矩阵(<em class="lw"> y </em> ): </strong></p><p id="8997" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">记住<strong class="kt jd"> <em class="lw"> X </em> </strong>是一个<strong class="kt jd"><em class="lw">m</em></strong>*(1+<strong class="kt jd"><em class="lw">n</em></strong>)矩阵。所以我们需要插入一列1与偏置单位<strong class="kt jd"> <em class="lw"> b0相乘。</em>T45】</strong></p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="1ed1" class="oe mx it ou b gy oy oz l pa pb"># Save the un-standardized data with moderate VIF attributes as X<br/>X = data[['ENGINE_SIZE','CYLINDERS','COMB_MPG']]</span><span id="d7fc" class="oe mx it ou b gy pc oz l pa pb"># Define a column of ones, the length of X<br/>ones = np.ones(len(X), dtype= 'int8')</span><span id="0861" class="oe mx it ou b gy pc oz l pa pb"># Insert The column of ones to the first position of X<br/>X.insert(0, 'Ones', ones)</span><span id="f812" class="oe mx it ou b gy pc oz l pa pb"># Display first 5 rows of X<br/>X.head()</span><span id="eccd" class="oe mx it ou b gy pc oz l pa pb"># Now we define target matrix y(an m * 1 column vector)<br/>y = fuel_df.CO2_EMISSIONS<br/>y.head()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/85c35e6f3087ead5c0f8fef85f332a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*MfE8kTtwxBY5onabw2LnkQ.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">显示前五行特征矩阵(X)和目标列向量(y)</figcaption></figure><p id="78bb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">定义好我们的<strong class="kt jd"> <em class="lw"> X </em> </strong>和<strong class="kt jd"> <em class="lw"> y </em> </strong>矩阵后，让我们将它们分成训练和测试数据集，然后我们可以开始处理最终的矩阵<strong class="kt jd"> <em class="lw"> b </em> </strong></p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="bc52" class="oe mx it ou b gy oy oz l pa pb"># Import train_test_split for splitting the data set<br/>from sklearn.model_selection import train_test_split</span><span id="132a" class="oe mx it ou b gy pc oz l pa pb"># split the data set into training and testing sets.<br/>X, X_test, y, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)</span><span id="01c7" class="oe mx it ou b gy pc oz l pa pb"># Display the shapes of each split<br/>print('X shape is:',X.shape)<br/>print('y shape is:',y.shape)<br/>print('X_test shape is:',X_test.shape)<br/>print('y_test shape is:',y_test.shape)</span><span id="15e6" class="oe mx it ou b gy pc oz l pa pb">&gt;&gt;</span><span id="da5c" class="oe mx it ou b gy pc oz l pa pb">X shape is: (10757, 4) <br/>y shape is: (10757,) <br/>X_test shape is: (3586, 4) <br/>y_test shape is: (3586,)</span></pre><h2 id="a0e0" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated"><strong class="ak">定义系数矩阵(<em class="mv"> b </em> ) </strong></h2><p id="dd77" class="pw-post-body-paragraph kr ks it kt b ku nq kd kw kx nr kg kz la op lc ld le oq lg lh li or lk ll lm im bi translated">这个矩阵由斜率或梯度(<strong class="kt jd"> <em class="lw"> b1…bn </em> </strong>)和截距或偏差单元(<strong class="kt jd"> <em class="lw"> b0 </em> </strong>)组成</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pj"><img src="../Images/5c9446fcbf20152632de34e385920231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3BpGQtUvDfWMdP5trbMvDg.jpeg"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">斜率或梯度是简单的运行除以上升…</figcaption></figure><h2 id="bd44" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated"><strong class="ak">最小二乘估计:</strong></h2><p id="c6af" class="pw-post-body-paragraph kr ks it kt b ku nq kd kw kx nr kg kz la op lc ld le oq lg lh li or lk ll lm im bi translated">为了定义矩阵<strong class="kt jd"> <em class="lw"> b、</em> </strong>，我们将使用一个简单的公式，称为<a class="ae lx" href="https://newonlinecourses.science.psu.edu/stat501/node/382/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="lw">最小二乘估计(LSE)。</em></strong></a><strong class="kt jd"><em class="lw"/></strong>LSE非常适合计算线性回归的系数，无论是简单线性回归还是多元线性回归。</p><p id="919b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">LSE的唯一缺点是独立变量是线性相关的。</p><blockquote class="mk ml mm"><p id="314e" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated">也就是说，一个<em class="it">或多个</em>变量可以写成另一个变量的线性组合。例如，如果变量1是变量2乘以变量3的结果…这是我们在上一步中试图减少多重共线性的部分原因。</p></blockquote><p id="7b25" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们为什么要关心线性相关性呢？<a class="ae lx" href="https://newonlinecourses.science.psu.edu/stat501/node/382/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="lw">因为方阵的逆矩阵只有当列线性无关时才存在</em> </strong> </a>。LSE涉及计算方阵的逆矩阵。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pt"><img src="../Images/2f25bb06430b2afd2e41362a9b04dd28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUFzIZycREAqFUITP49EZw.jpeg"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">最小二乘估计公式</figcaption></figure><blockquote class="mk ml mm"><p id="6af7" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated"><strong class="kt jd">提示:</strong></p><p id="6f21" class="kr ks lw kt b ku kv kd kw kx ky kg kz mn lb lc ld mo lf lg lh mp lj lk ll lm im bi translated"><a class="ae lx" href="https://www.khanacademy.org/math/precalculus/precalc-matrices/properties-of-matrix-multiplication/v/commutative-property-matrix-multiplication" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">矩阵乘法不可交换</strong> </a>。除了<a class="ae lx" href="https://martin-thoma.com/when-is-matrix-multiplication-commutative/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">特例</strong> </a>。因此我们将矩阵<strong class="kt jd"> x </strong>乘以矩阵<strong class="kt jd"> y </strong> ( <strong class="kt jd"> xy </strong>)定义为矩阵x后乘以矩阵y，或者可以定义为矩阵<strong class="kt jd"> y </strong>前乘以矩阵<strong class="kt jd"> x </strong></p></blockquote><p id="448c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，LSE公式指出:</p><p id="3eae" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="lw"> b </em> </strong> = ((矩阵<strong class="kt jd"><em class="lw">X-转置</em> </strong>后置乘以矩阵<strong class="kt jd"> <em class="lw"> X </em> </strong>)前置乘以矩阵<strong class="kt jd"><em class="lw">X-转置</em> </strong>前置乘以矩阵<strong class="kt jd"> <em class="lw"> y. </em> </strong></p><p id="7c89" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们一步一步来…</p><p id="4d69" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">第一步:</strong>我们来定义一下<strong class="kt jd"><em class="lw">X-转置</em> </strong>矩阵(<strong class="kt jd"> <em class="lw"> X' </em> </strong>)</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="eab5" class="oe mx it ou b gy oy oz l pa pb"># First we convert X and X_test, y, y_test to Numpy arrays for ease of computing<br/>X = X.values<br/>X_test = X_test.values<br/>y = y.values<br/>y_test = y_test.values</span><span id="b111" class="oe mx it ou b gy pc oz l pa pb"># Next we define X_trans, the Transpose of X matrix<br/>X_trans = X.transpose()</span><span id="d680" class="oe mx it ou b gy pc oz l pa pb">print('X-Trans shape:',X_trans.shape)<br/>&gt;&gt;<br/>  X-Trans shape: (4, 10757)</span></pre><p id="7991" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">第二步:</strong>接下来我们定义一下<strong class="kt jd"><em class="lw">X-转置</em> </strong>后置乘矩阵<strong class="kt jd"> <em class="lw"> X: (X'X) </em> </strong></p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="3743" class="oe mx it ou b gy oy oz l pa pb"># Post-multiply X_trans by X<br/>X_trans_X = np.matmul(X_trans, X)</span><span id="5b4a" class="oe mx it ou b gy pc oz l pa pb"># Print out the shape of X_trans_X<br/>print('X_Trans_X shape:',X_trans_X.shape)<br/>&gt;&gt;<br/>  X_Trans_X shape: (4, 4)</span></pre><p id="ee53" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">第三步:</strong>接下来我们来定义<strong class="kt jd"> <em class="lw"> X的逆——转置</em> </strong>后乘矩阵<strong class="kt jd"> <em class="lw"> X: (X'X)^-1 </em> </strong></p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="27f9" class="oe mx it ou b gy oy oz l pa pb"># Import inv from numpy.linear algebra<br/>from numpy.linalg import inv</span><span id="7335" class="oe mx it ou b gy pc oz l pa pb"># define X_trans_X_inv<br/>X_trans_X_inv = inv(X_trans_X)</span><span id="3f99" class="oe mx it ou b gy pc oz l pa pb"># Print the shape of X_trans_X_inv<br/>print("Shape of X_trans_X_inv is",X_trans_X_inv.shape)<br/>&gt;&gt;<br/>  Shape of X_trans_X_inv is (4, 4)</span></pre><p id="4c4e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">步骤4: </strong>接下来我们将<strong class="kt jd"> <em class="lw">的逆x-转置-X </em> </strong>与<strong class="kt jd"><em class="lw">【x-transposed:(x'x)^-1x'】</em></strong>后乘</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="867b" class="oe mx it ou b gy oy oz l pa pb"># Let's Post-multiply X_trans_X_inv by X_trans<br/>X_trans_X_inv_X_trans = np.matmul(X_trans_X_inv, X_trans)</span><span id="dcfe" class="oe mx it ou b gy pc oz l pa pb"># Let's print the shape<br/>print("Shape of X_trans_X_inv_X_trans is",X_trans_X_inv_X_trans.shape)<br/>&gt;&gt;<br/>  Shape of X_trans_X_inv_X_trans is (4, 10757)</span></pre><p id="6492" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">第五步:</strong>最后我们将<strong class="kt jd"> <em class="lw"> (X'X)^-1X' </em> </strong>乘以矩阵<strong class="kt jd"> <em class="lw"> y </em> </strong>得出完整公式<strong class="kt jd"> <em class="lw"> (X'X)^-1X'Y </em> </strong></p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="0fd2" class="oe mx it ou b gy oy oz l pa pb"># Matrix b or coefficients defined as:<br/>b = np.matmul(X_trans_X_inv_X_trans, y)</span><span id="6a4f" class="oe mx it ou b gy pc oz l pa pb"># Let's print the shape of b<br/>print('Shape of matrix b or coefficients is:', b.shape)</span><span id="f6a3" class="oe mx it ou b gy pc oz l pa pb"># Print the values of b<br/>print(b)<br/>&gt;&gt;<br/>  Shape of matrix b or coefficients is: (4,)</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pu"><img src="../Images/73dc25ee9b291e28279c0db125a02a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RQgyvjABhCK2fVNso8skPA.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">矩阵b或系数矩阵值</figcaption></figure><p id="f408" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，矩阵代数模型系数为:-</p><p id="acd9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">b[0] =&gt; <strong class="kt jd"> <em class="lw">偏置单位或截距</em></strong>= 23861 . 486368636366</p><p id="6da7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">b[1] =&gt; <strong class="kt jd"> <em class="lw">发动机斜率_尺寸</em> </strong> = 7.85865697</p><p id="49d6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">b<strong class="kt jd">= &gt;<em class="lw"/></strong>= 5.99925421</p><p id="73f1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">b[3] =&gt; <strong class="kt jd"> <em class="lw">斜率梳_MPG </em> </strong> = -5.06530699</p><p id="9c69" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此我们可以看到，对于任何给定的车辆，<em class="lw">发动机_尺寸</em> (7.85)比<em class="lw">气缸</em> (5.99)或<em class="lw"> Comb_mpg </em> (-5.06)对<em class="lw">CO2 _排放</em>的影响更大。</p><p id="2f9e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，如果我们想预测一辆发动机尺寸为<strong class="kt jd"> 3.2 </strong>升、有<strong class="kt jd"> 6个</strong>气缸且comb_MPG值为<strong class="kt jd"> 28 </strong>的汽车的二氧化碳排放量(<strong class="kt jd">T59】y _ haty _ hat</strong>),我们只需代入:</p><p id="6bb2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"><em class="lw">y _ hat = B0+b1x 1+B2 x2+b3x 3</em></strong></p><p id="ca17" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这意味着…</p><p id="33a0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"><em class="lw">y _ hat</em></strong>= 323.97614481+(7.85865697 *<strong class="kt jd"><em class="lw">ENGINE _ SIZE</em></strong><em class="lw">)</em><strong class="kt jd"><em class="lw">+</em></strong>(5.99925421 *<strong class="kt jd"><em class="lw">气缸</em></strong>+(-5.0655)</p><p id="6888" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后让我们用3.2，6和28代替<strong class="kt jd"> <em class="lw"> x1 </em> </strong>，<strong class="kt jd"> <em class="lw"> x2 </em> </strong>和<strong class="kt jd"> <em class="lw"> x3 </em> </strong></p><p id="377d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"><em class="lw">y _ hat</em></strong>= 323.97614481+(7.85865697 * 3.2<em class="lw">)</em><strong class="kt jd"><em class="lw">+</em></strong>(5.99925421 * 6)+(-5.06530699 * 28)</p><p id="b01b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"><em class="lw">y _ hat</em></strong>= 243(四舍五入)</p><h2 id="6d89" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated"><strong class="ak">使用矩阵代数模型的预测:</strong></h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pv"><img src="../Images/dd9204c80920d2f5441628dabfdb42c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EG-Tid2k3P7ClL2ZgxDi0w.jpeg"/></div></div></figure><p id="e03b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们使用矩阵代数模型系数来预测测试数据集。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="7d23" class="oe mx it ou b gy oy oz l pa pb"># Let's define a simple method for prediction<br/>def predict(x):<br/>    """ takes a row of test data and predicts it<br/>    and returns the summed value of the observation"""<br/>    x = list(x)<br/>    <br/>    x = x[0]*intercept + x[1]*b1 + x[2]*b2 + x[3]*b3<br/>    <br/>    return x</span></pre><p id="6325" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，让我们将测试数据转换为数据框，以便我们可以对其应用预测方法。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="e39f" class="oe mx it ou b gy oy oz l pa pb">X_eval = pd.DataFrame(X_test)</span></pre><p id="f5d7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，让我们应用predict方法并传递axis=1，这样就沿着列完成了。然后我们使用。值，并将其全部赋给变量y_hat。y_hat现在是我们的模型对样本外或测试数据集的预测。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="e40c" class="oe mx it ou b gy oy oz l pa pb">y_hat = np.ceil(X_eval.apply(predict, axis=1).values)</span></pre><h2 id="069d" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated"><strong class="ak">评估矩阵代数模型:</strong></h2><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pw"><img src="../Images/a9418a2ff023a09f2b5f06b474c078f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G4toi8TRuTpSARdLhL1o6w.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated"><a class="ae lx" href="http://lynda.com" rel="noopener ugc nofollow" target="_blank"> img_credit </a></figcaption></figure><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="4b18" class="oe mx it ou b gy oy oz l pa pb"># First import metrics from sklearn<br/>from sklearn.metrics import mean_squared_error, r2_score</span></pre><p id="d466" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们计算均方误差和均方根误差。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="b202" class="oe mx it ou b gy oy oz l pa pb">MSE = mean_squared_error(y_test, y_hat)<br/>RMSE = MSE**0.5</span><span id="09bf" class="oe mx it ou b gy pc oz l pa pb"># Let's print out the MSE and RMSE<br/>print(MSE)<br/>print(RMSE)<br/>&gt;&gt;<br/>    502.12827663134414<br/>    22.408218952682162</span></pre><p id="e31a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们计算误差幅度和r2_score(决定系数)。r2_score告诉我们，我们的模型预测有多有把握。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="44cc" class="oe mx it ou b gy oy oz l pa pb"># First we find the range of the target variable, y<br/>y_range = y_test.max() - y_test.min()</span><span id="38cb" class="oe mx it ou b gy pc oz l pa pb"># Then we compute the RMSE as a percentage of the range of y<br/>error_margin = error_margin = (RMSE / y_range) * 100</span><span id="e7be" class="oe mx it ou b gy pc oz l pa pb"># Next we compute the r2_score<br/>r2_score = r2_score(y_test, y_hat)</span><span id="97bd" class="oe mx it ou b gy pc oz l pa pb"># Let's print out the error_margin and r2_score<br/>print(error_margin)<br/>print(r2_score)<br/>&gt;&gt;<br/>    4.6012769923371994<br/>    0.8535767069727507</span></pre><p id="4c6c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这意味着我们的矩阵代数模型对每个预测都有85%的信心，并且误差在目标变量范围的4.6%以内……非常好。</p><p id="7207" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">对比:</strong></p><p id="3610" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们使用相同的数据集将我们的矩阵代数模型与Sklearn库中的模型进行比较。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="a06e" class="oe mx it ou b gy oy oz l pa pb"># first import sklearn linear regression<br/>from sklearn.linear_model import LinearRegression</span><span id="18b6" class="oe mx it ou b gy pc oz l pa pb"># Instantiate a Linear regression model<br/>model = LinearRegression()</span></pre><p id="2724" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来训练Sklearn模型。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="9930" class="oe mx it ou b gy oy oz l pa pb">model.fit(X,y)<br/>&gt;&gt;<br/>  LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)</span></pre><p id="a546" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">打印Sklearn模型的系数。</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="d8b5" class="oe mx it ou b gy oy oz l pa pb"># The Slope or gradient<br/>model_slope = model.coef_</span><span id="bfe9" class="oe mx it ou b gy pc oz l pa pb"># the intercept or bias unit<br/>model_intercept = model.intercept_</span><span id="5f59" class="oe mx it ou b gy pc oz l pa pb">print(’Slope =’, slope,’\nIntercept = ',intercept)<br/>&gt;&gt;<br/>      Slope = [ 0.          7.9148692   5.89255091 -5.00194268]  Intercept =  322.5539237961783</span></pre><p id="3166" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们使用Sklearn模型来预测测试数据集</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="862e" class="oe mx it ou b gy oy oz l pa pb">y_pred = model.predict(X_test)</span></pre><p id="3902" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们来评价一下Sklearn模型的性能</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="0538" class="oe mx it ou b gy oy oz l pa pb">model_mse = mean_squared_error(y_test, y_pred)<br/>model_rmse = model_mse ** 0.5<br/>model_error_margin = (model_rmse / y_range) * 100<br/>model_r2_score = r2_score(y_test, y_pred)</span></pre><p id="485f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们来看看矩阵代数和Sklearn模型的总结</p><pre class="lz ma mb mc gt ot ou ov ow aw ox bi"><span id="af3c" class="oe mx it ou b gy oy oz l pa pb">summary_dict = {'algebra_model':[intercept, b1, b2, b3, MSE, RMSE, error_margin, matrix_r2_score],<br/>               'sklearn_model':[model_intercept, model_slope[1], model_slope[2], model_slope[3], model_mse, model_rmse, model_error_margin, model_r2_score]}</span><span id="374f" class="oe mx it ou b gy pc oz l pa pb">summary_df = pd.DataFrame(summary_dict, index=['Intercept','b1','b2','b3','MSE','RMSE','Error_margin','R2_score'])<br/>summary_df</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi px"><img src="../Images/104baf2610f59c299c36e075e744ef98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*W6xPLtbs7zHxaDUHwsHY1A.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">矩阵代数和Sklearn模型实际上具有相同的参数</figcaption></figure><p id="30e9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以看到，这两个模型在所有参数(截距、斜率、MSE、RMSE、误差容限和r2分数)上都有实际上精确的值</p><p id="b4ee" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，Sklearn和矩阵代数模型对每个预测都有85%的置信度，并且误差范围为y的4.6%。</p><h2 id="db59" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated"><strong class="ak">可视化:</strong></h2><p id="ea83" class="pw-post-body-paragraph kr ks it kt b ku nq kd kw kx nr kg kz la op lc ld le oq lg lh li or lk ll lm im bi translated">让我们看看矩阵代数模型预测的距离图和正则图</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi py"><img src="../Images/62e429549b615e6ca4dba731e8b9ce0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUbPeNincxjhSGf431rFug.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">测试数据集上矩阵代数模型预测的distplot和regplot</figcaption></figure><p id="5cb9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们来看看Sklearn模型预测的distplot和regplot</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pz"><img src="../Images/be2b12a08684b0295275dbec4ae94069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BOBifq_9z_Gk4MS-l2ue6w.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">测试数据集上Sklearn模型预测的distplot和regplot</figcaption></figure><p id="99e1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这两种型号惊人地相同。</p><h2 id="9230" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated"><strong class="ak">总结:</strong></h2><p id="5e76" class="pw-post-body-paragraph kr ks it kt b ku nq kd kw kx nr kg kz la op lc ld le oq lg lh li or lk ll lm im bi translated">我们已经看到多元线性回归(MLR)如何使用矩阵代数一步一步地工作。</p><p id="f229" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们从零开始建立了一个MLR模型，首先确定相关变量，确定多重共线性，并选择3个适度线性独立变量。<br/>然后，我们使用符合<em class="lw">最小二乘估计公式</em>的矩阵乘法，使用训练数据集训练模型，并使用模型对模型未知的测试数据集(样本数据集之外)进行预测。<br/>然后，我们使用MSE、RMSE和r2_score对该模型进行评估，并将其与Sklearn库中的模型进行比较。</p><h2 id="ed4d" class="oe mx it bd my of og dn nc oh oi dp ng la oj ok ni le ol om nk li on oo nm iz bi translated">最后:</h2><p id="5153" class="pw-post-body-paragraph kr ks it kt b ku nq kd kw kx nr kg kz la op lc ld le oq lg lh li or lk ll lm im bi translated">我希望我已经把你的直觉放大到多元线性回归，给你计算线性回归的能力，不管有没有库…看到本质，超越抽象。</p><p id="5806" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以在<a class="ae lx" href="https://cognitiveclass.ai/courses/machine-learning-with-python/" rel="noopener ugc nofollow" target="_blank"><strong class="kt jd"><em class="lw">cognitive class . ai</em></strong></a>上查看这个免费课程</p><p id="b91f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">参见Github上的  此处的<a class="ae lx" href="https://github.com/Lawrence-Krukrubo/Understanding_Multiple_Linear_Regression" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="lw">代码单元格</em></strong></a></p><p id="f839" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">干杯！</strong></p></div></div>    
</body>
</html>