<html>
<head>
<title>Top 5 Machine learning models 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年前5大机器学习模型</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/top-5-machine-learning-models-2021-1e03ce01dfe8?source=collection_archive---------1-----------------------#2022-01-04">https://pub.towardsai.net/top-5-machine-learning-models-2021-1e03ce01dfe8?source=collection_archive---------1-----------------------#2022-01-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="fe2a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="e89c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">过去一年中最值得注意的机器学习模型的集合</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/7adc3f34b8403c44aeb76c2a450dbe9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C-9UBs-QRU5h6mJc"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@pawel_czerwinski?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">帕韦尔·切尔文斯基</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="aa4c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今年有很多优秀的模特。在这篇文章中，我希望强调10个最值得注意的模型。这一年来，我一直在定期审查论文并解释它们，我认为我得到了相当多的好评。免责声明:可能有其他好的模型在这里没有提到，我并不声称自己是评估机器学习模型质量的最终专家！</p><p id="301c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另外，请注意，这个列表不是有序的！</p><ol class=""><li id="0f12" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><a class="ae lh" href="https://towardsdatascience.com/deepmind-releases-a-new-state-of-the-art-image-classification-model-nfnets-75c0b3f37312" rel="noopener" target="_blank"><strong class="lk jd">deep mind nfnet</strong></a></li></ol><blockquote class="mn mo mp"><p id="c4cc" class="li lj mq lk b ll lm kd ln lo lp kg lq mr ls lt lu ms lw lx ly mt ma mb mc md im bi translated"><em class="it">我们的小型模型与ImageNet上的EfficientNet-B7的测试精度相当，同时训练速度提高了8.7倍，我们的大型模型达到了86.5%的最新顶级精度。</em></p></blockquote><p id="3c29" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">来源:<a class="ae lh" href="https://arxiv.org/abs/2102.06171" rel="noopener ugc nofollow" target="_blank"> arxiv </a></p><p id="a493" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Deepmind于2021年2月发布了无规格化器网络，并获得了很多认可。我当时甚至写了一篇解释它们的文章，它获得了41000次浏览！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mu"><img src="../Images/72a8b9de45180507a836fc76856dcd4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E1fXDYeMAI3rl3XZYOsgbw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">我自己的媒体统计</figcaption></figure><p id="e9f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇论文最有趣的一点是他们对深度学习中现有的优化技术(如批处理规范化)所做的分析。他们的分析结果使他们得出这样的结论:它是不需要的，实际上删除它可以加快训练过程，而不会导致性能下降，因此得名“无归一化器网络”。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/955af1a4b04c82da3342e8519a53d0dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2EzuOKulYkvUIsQn.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://arxiv.org/abs/2102.06171" rel="noopener ugc nofollow" target="_blank"> arxiv </a></figcaption></figure><p id="71b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你有兴趣了解更多，请点击这里查看我的文章:</p><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/deepmind-releases-a-new-state-of-the-art-image-classification-model-nfnets-75c0b3f37312" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">Deepmind发布了一个新的最先进的图像分类模型——NFNets</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">NFNets比EfficientNets快，而且它们不使用规范化</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn lb mz"/></div></div></a></div><p id="7691" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2。</strong> <a class="ae lh" href="https://towardsdatascience.com/open-ai-clip-learning-visual-concepts-from-natural-language-supervision-d02644969278" rel="noopener" target="_blank"> <strong class="lk jd"> OpenAI夹</strong> </a></p><p id="fe46" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">CLIP是一个基于转换的神经网络，它使用对比语言-图像预训练来对图像进行分类。CLIP通过将图像分类转化为文本相似性问题来对非常广泛的图像进行分类。当前图像分类网络的问题是，它们是在固定数量的类别上训练的，CLIP不是这样工作的，它直接从关于图像的原始文本中学习，因此它不受标签和监督的限制。</p><p id="703f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你正在寻找一篇关于变形金刚图像的有趣论文，请查看剪辑:</p><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/open-ai-clip-learning-visual-concepts-from-natural-language-supervision-d02644969278" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">开放AI剪辑:从自然语言监督中学习视觉概念</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">一种基于变换的神经网络，使用对比语言-图像预训练对图像进行分类</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="no l nk nl nm ni nn lb mz"/></div></div></a></div><p id="d924" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3。</strong> <a class="ae lh" href="https://towardsdatascience.com/google-switch-transformers-scaling-to-trillion-parameter-models-with-constant-computational-costs-806fd145923d" rel="noopener" target="_blank"> <strong class="lk jd">谷歌开关变形金刚</strong> </a></p><p id="9875" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文介绍了一种新的方法，在保持每秒浮点运算次数(ML计算成本标准指标)的同时，大幅增加参数数量。</p><p id="7a49" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">众所周知，增加参数的数量会增加模型的复杂性和学习能力(当然在一定程度上)。正如预期的那样，该车型比<a class="ae lh" href="https://huggingface.co/google/t5-xxl-ssm" rel="noopener ugc nofollow" target="_blank"> T5-XXL </a>提高了4倍，比T5-Base和<a class="ae lh" href="https://huggingface.co/t5-large" rel="noopener ugc nofollow" target="_blank"> T5-Large </a>提高了7倍。</p><p id="2b3d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇论文使用了各种各样的ML概念，这使得它非常有趣，例如<a class="ae lh" href="https://machinelearningmastery.com/mixture-of-experts/" rel="noopener ugc nofollow" target="_blank">专家混合</a>、<a class="ae lh" href="https://en.wikipedia.org/wiki/Knowledge_distillation#:~:text=In%20machine%20learning%2C%20knowledge%20distillation,might%20not%20be%20fully%20utilized." rel="noopener ugc nofollow" target="_blank">蒸馏</a>和模型分片。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mv"><img src="../Images/6dcb11129fd6e4dbcd464413d9b71e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a0PgQhPbLSNsq8sG.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://arxiv.org/abs/2101.03961" rel="noopener ugc nofollow" target="_blank"> arxiv </a>(乳胶转载表)</figcaption></figure><p id="13ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用这种技术，他们设法在性能上比非常强大的现有基于变压器的模型提高了3-30%。我们还可以看到，参数的数量相对较少。</p><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/google-switch-transformers-scaling-to-trillion-parameter-models-with-constant-computational-costs-806fd145923d" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">谷歌开关变压器:在恒定计算成本下扩展到万亿参数模型</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">在强烈调整的T5-XXL基线(谷歌的顶级变压器)上实现了4倍的预训练加速。</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="np l nk nl nm ni nn lb mz"/></div></div></a></div><p id="58fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 4。</strong><a class="ae lh" href="https://towardsdatascience.com/deepmind-alphafold2-highly-accurate-protein-structure-prediction-7ef0946f60c0" rel="noopener" target="_blank"><strong class="lk jd">deep mind alpha fold 2</strong></a></p><p id="921d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这可能是我今年最喜欢的模型，因为我是药物发现和医疗领域深度学习的忠实粉丝。AlphaFold2是深度思维为解决蛋白质折叠问题而构建的高级模型。蛋白质折叠问题是生物信息学领域最大的问题之一，如果你想了解更多，请点击这里查看我的文章<a class="ae lh" href="https://ai.plainenglish.io/what-solving-the-protein-folding-problem-actually-means-a94ee2121a98" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="da0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">AlphaFold2是一个非常复杂的模型，有很多细节和技巧，如果你想了解更多关于它的工作原理和他们使用的“Evoformer”，请随意查看我在这里的文章:</p><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/deepmind-alphafold2-highly-accurate-protein-structure-prediction-7ef0946f60c0" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">Deepmind AlphaFold2:高度精确的蛋白质结构预测</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">深入研究AlphaFold2的文章，它是如何工作的，为什么，以及一切</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="nq l nk nl nm ni nn lb mz"/></div></div></a></div><p id="d64c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 5。</strong><a class="ae lh" href="https://towardsdatascience.com/google-releases-efficientnetv2-a-smaller-faster-and-better-efficientnet-673a77bdd43c" rel="noopener" target="_blank"><strong class="lk jd">Google efficient net v2</strong></a></p><p id="8aba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">高效网络在图像识别任务中非常流行。今年，谷歌发布了第二个版本，其性能略高于最先进的图像模型，而训练速度是5-10倍。这是今年一个值得注意的论文模式，相当多的论文关注于更快地训练模型，而不是获得更高的性能。此外，我认为相当多的论文集中在基于图像的问题上。</p><p id="c253" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">EfficientNetV2中使用的一个有趣的技术是渐进式学习，这意味着尽管训练开始时图像尺寸最初很小，但它们的尺寸会逐渐增加。这种解决方案源于这样一个事实，即EfficientNets的训练速度在高图像大小时开始受到影响。</p><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/google-releases-efficientnetv2-a-smaller-faster-and-better-efficientnet-673a77bdd43c" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">谷歌发布efficient net v2——一个更小、更快、更高效的网络</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">与最先进的技术相比，性能更高，同时训练速度提高5-10倍</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="nr l nk nl nm ni nn lb mz"/></div></div></a></div><h2 id="87d5" class="ns nt it bd nu nv nw dn nx ny nz dp oa lr ob oc od lv oe of og lz oh oi oj iz bi translated">荣誉奖:</h2><ul class=""><li id="69d2" class="me mf it lk b ll ok lo ol lr om lv on lz oo md op mk ml mm bi translated"><a class="ae lh" href="https://towardsdatascience.com/google-releases-mlp-mixer-an-all-mlp-architecture-for-vision-824fac3e788c" rel="noopener" target="_blank"> <strong class="lk jd">谷歌MLP混合器</strong> </a> <strong class="lk jd"> : </strong>使用简单多层感知器的高性能图像模型</li><li id="1f20" class="me mf it lk b ll oq lo or lr os lv ot lz ou md op mk ml mm bi translated"><a class="ae lh" href="https://towardsdatascience.com/varifocalnet-vf-net-new-state-of-the-art-object-detection-network-a1d54e0f7c1e" rel="noopener" target="_blank"> <strong class="lk jd">变焦距网络</strong> </a> <strong class="lk jd"> : </strong>一个强大的对象检测模型，实现了与YoloV5相似或更高的性能</li><li id="9263" class="me mf it lk b ll oq lo or lr os lv ot lz ou md op mk ml mm bi translated"><a class="ae lh" href="https://openai.com/blog/dall-e/" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">open ai DALL-E</strong></a><strong class="lk jd">:</strong>一个有趣的从文本创建图像的模型</li></ul><h2 id="ed17" class="ns nt it bd nu nv nw dn nx ny nz dp oa lr ob oc od lv oe of og lz oh oi oj iz bi translated"><strong class="ak">结论</strong></h2><p id="3cb0" class="pw-post-body-paragraph li lj it lk b ll ok kd ln lo ol kg lq lr ov lt lu lv ow lx ly lz ox mb mc md im bi translated">我知道这些型号中的大多数都是由大公司发布的，我很确定还有其他在2021年发布的伟大型号没有被列入这个列表，只是因为它们没有那么受欢迎，没有达到更多的观众。令人难过的事实是，这些大公司的报纸总会有更大的影响力。只是说，我只是因为这些模型是大公司发布的而没有提到它们，我实际上是看了论文并在中型帖子中解释了它们。如果你有任何其他伟大的论文，请务必在下面的评论中留下它们！(当然，我已经错过了一些好的)</p><p id="85be" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想定期收到关于人工智能和机器学习的最新论文的评论，请在这里添加你的电子邮件并订阅！</p><p id="bc4d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://artisanal-motivator-8249.ck.page/5524b8f934" rel="noopener ugc nofollow" target="_blank">https://artisanal-motivator-8249.ck.page/5524b8f934</a></p></div></div>    
</body>
</html>