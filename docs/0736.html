<html>
<head>
<title>Recurrent Neural Networks for Dummies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于假人的递归神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/recurrent-neural-networks-for-dummies-8d2c4c725fbe?source=collection_archive---------0-----------------------#2020-07-30">https://pub.towardsai.net/recurrent-neural-networks-for-dummies-8d2c4c725fbe?source=collection_archive---------0-----------------------#2020-07-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0db6" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="35dc" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">递归神经网络完美指南</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ec3b5fc47cb02f6e7caf6ad2393183c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u_-BN49pBojgLKwAbapRgg.png"/></div></div></figure><p id="c474" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你向Siri询问今天的天气，它出色地解决了你的疑问。</p><p id="521c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但是，这是怎么发生的呢？它是如何把你的演讲转换成文本，然后输入搜索引擎的？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lz"><img src="../Images/8a86af760a6e01bcad12985c43b09eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*90W3f0duz72eord7"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">照片由<a class="ae me" href="https://unsplash.com/@morningbrew?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">晨酿</a>在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">破浪</a></figcaption></figure><p id="1750" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这就是<strong class="lf jd">递归神经网络的神奇之处。</strong></p><p id="fe73" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">递归神经网络(RNN) </strong>隶属于<strong class="lf jd"> </strong> <a class="ae me" href="https://medium.com/towards-artificial-intelligence/diving-deep-into-deep-learning-f34497c18f11" rel="noopener"> <strong class="lf jd">深度学习</strong> </a> <strong class="lf jd">。</strong>它们被用于涉及自然语言处理的操作中。如今，由于人工智能的范围正在极大地扩展，我们可以很容易地定位我们周围的重复操作。从语音翻译、音乐创作到预测手机键盘上的下一个单词，这些都扮演着重要的角色。</p><p id="be20" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">RNN迎合的问题类型有:</p><ul class=""><li id="bc42" class="mg mh it lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated">输出取决于先前的输入。(顺序数据)</li><li id="6186" class="mg mh it lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">输入的长度不固定。</li></ul><h1 id="4182" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">顺序数据</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/7c76b28310a4cdaa9b1684c60d6f4295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4IZmMvsYpwbqcc24"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">照片由<a class="ae me" href="https://unsplash.com/@introspectivedsgn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">埃里克·麦克林</a>在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="c9d1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了理解序列数据，让我们假设你有一只静止不动的狗。</p><p id="81a8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，你应该预测他会朝哪个方向移动？那么，只有这些有限的信息传授给你，你会怎么做呢？好吧，你可以毫无疑问地猜一猜，但在我看来，你想出来的将是一个随机的猜测。不知道狗去过哪里，你就没有足够的数据来预测它会去哪里。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/d2256e7be6d9e66256e5fdf18e18f836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZOXMqEMzjuEQZVfF"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated"><a class="ae me" href="https://unsplash.com/@mbx?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马库斯·贝内迪克斯</a>在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="0a1b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但是，现在如果狗开始朝一个特定的方向跑，如果你试着记录狗的运动，你会很确定它会选择哪个方向。因为在这一瞬间你有足够的信息来做出更好的预测。</p><p id="8469" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所以一个序列是一个事物跟随另一个事物的特殊顺序。有了这些信息，你现在可以看到狗正向你走来。</p><p id="d759" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">文本、音频也是序列数据的示例。</p><p id="cd4e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当你和某人说话时，你说出的单词是有顺序的。同样，当你给某人发电子邮件时，根据你的短信，你肯定会说出下一句话。</p><h1 id="0f9c" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">时序存储器</h1><p id="78f8" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm nq lo lp lq nr ls lt lu ns lw lx ly im bi translated">如前所述，RNNs解决了涉及输出和先前输入之间相互依赖的问题。这间接意味着，有一些记忆附属于这类神经网络。</p><p id="4a8a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">顺序记忆是帮助RNN实现其目标的东西。</p><p id="66cf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了更好地理解，我想请你回忆一下你脑中的字母表。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi lz"><img src="../Images/3d4c631091254b79a3fc09669503f917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*o7GBZLbRagzp45dM"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">杰西卡在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的</figcaption></figure><p id="36e4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是一个简单的任务，如果你学会了这个特定的顺序，你应该很快就会明白。</p><p id="9902" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，如果我让你以相反的方式回忆字母表。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/3aac59c1b6e7388c2d523589f5123e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v5bSjUV0mdDJ7mb0jFRBIA.png"/></div></div></figure><p id="1199" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我打赌这个任务很可靠。而且在我看来，会让你不好过。</p><p id="adaf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所以，前一个任务之所以被证明是有弹性的，是因为你已经按顺序学习了字母。顺序记忆让你的大脑更容易识别模式。</p><h1 id="8627" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">递归神经网络与神经网络有何不同？</h1><p id="3664" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm nq lo lp lq nr ls lt lu ns lw lx ly im bi translated">如前所述，递归神经网络属于深度学习，但神经网络也是如此。但是由于缺乏内部状态，人工神经网络不是我们用来处理序列数据的东西。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d23c0694a1f3dc764da1ac7a0f7746ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*CHGkeG2bWE8ONOmK3VZvaA.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">前馈网络</figcaption></figure><p id="0858" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了开发一个对序列数据稳健的神经网络，我们给前馈神经网络添加了一个内部状态，为我们提供了内部记忆。或者简单地说，递归神经网络是具有内部记忆的前馈神经网络的推广。<strong class="lf jd"> RNN实现了顺序记忆的抽象概念</strong>，通过提供以前的经验来帮助他们，从而允许它更好地预测顺序数据。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c7ec83b6897e77f4e872c3df6ae615f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*HkY8xENB37eQHi8pSyVGaA.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">递归神经网络</figcaption></figure><p id="9322" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">RNN通过对每个输入执行相同的函数来证明它的递归性质，而当前输入的输出取决于过去的输入。与前馈神经网络相比，在RNN，所有的输入都是相互依赖的，不像传统的形式。</p><h1 id="3989" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">RNN的工作</h1><p id="1d08" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm nq lo lp lq nr ls lt lu ns lw lx ly im bi translated">好吧，但是RNN是如何复制这些内部记忆并实际运作的呢？</p><p id="0f4c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">假设，一个用户问，“<em class="mf">你叫什么名字？</em>”</p><p id="d55e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于RNN完全依赖于顺序记忆，我们希望我们的模型能把句子分解成单个的单词。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/43075be81657df7f6526faeff2e47937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*dIUMEQmeGfW7iyHOHASSVA.png"/></div></figure><p id="3002" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">起初，“什么”被输入RNN。然后，我们的模型对其进行编码，并向我们呈现一个输出。</p><p id="60ac" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于下一部分，我们输入单词“is”和我们从单词“What”得到的前一个输出。RNN现在可以获得“是什么”和“是什么”这两个词所传递的信息。</p><p id="0359" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">相同的过程将被重复，直到我们到达序列的末尾。最后，我们可以预期RNN已经从我们的序列中的所有单词中编码了信息。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ebdf1b4e92809d39c840f4c4a62639aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*-s2Ic6SjOlRqI5YkQga_CQ.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">你叫什么名字？</figcaption></figure><p id="2d12" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于最后的输出是由前面的输出和最后的输入组合而成的，所以我们可以将最后的输出传递给前馈层来实现我们的目标。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="6dc1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了创建上下文，让我们模拟x 输入的<strong class="lf jd">；<strong class="lf jd">由y </strong>输出；和<strong class="lf jd">状态向量由a. </strong></strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/db5936e6c4041e536db23536e1d2a0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*ueVq66aReMLHclHFX8DfoA.png"/></div></figure><p id="e677" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当我们传递我们的第一个输入，即x0(“什么”)，我们被提供输出y1和状态向量a1，其被传递到下一个函数s1以适应x0的过去输出。</p><p id="3af4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个过程不断重复，直到我们到达序列的末尾。最后，我们剩下状态向量a5，它向我们保证所有的输入<x1 x2="" x3="" x4="" x5="">已经被馈送到我们的模型，并且产生一个由所有输出贡献的输出。</x1></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/3144c8bafc27e657a16ce4aca87bceb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*3o2TI7EWm5VV_XfVFdO97g.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">状态向量</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/efe4008aaaea180df3df3f21686ba461.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*B5QzAZtz9a2Dnw3yrEcuwQ.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">单RNN细胞</figcaption></figure><h1 id="17ef" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">RNN的伪代码</h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="a87f" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">RNN建筑的类型</h1><ul class=""><li id="2ba8" class="mg mh it lf b lg no lj np lm ok lq ol lu om ly ml mm mn mo bi translated"><strong class="lf jd">一对一</strong></li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/66640be4f1c757e7256044ad1b33bdc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*pqkuO10FskOzGM32av8RFQ.png"/></div></figure><ul class=""><li id="5da1" class="mg mh it lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated"><strong class="lf jd">一对多</strong> —这类RNN架构通常用于图像字幕/故事字幕。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/6c7a5bf881efb0993fbc2136962c309f.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*Dn33qbXnmYVe4Y9llqRwbw.png"/></div></figure><ul class=""><li id="55f8" class="mg mh it lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated"><strong class="lf jd">多对一</strong> —这类RNN架构用于情感分析。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/812410d1b14bd12b5a21f55f2b58cc1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*QtPPom5SihPhgfD0jI6NfA.png"/></div></figure><ul class=""><li id="ce2e" class="mg mh it lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated"><strong class="lf jd">多对多</strong> —这些类型的RNN体系结构在词性中使用，也就是说，我们期望在词性中找到每个单词的属性。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/0174dc88f845a3da59c6d3e7c357b8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*nqmlhBoCNgEDGMEHiMe7qg.png"/></div></figure><ul class=""><li id="28a3" class="mg mh it lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated"><strong class="lf jd">编码器-解码器</strong> —这些类型的RNN是最复杂的，用于语言翻译。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/aea55eb51f55ca537dc398061d0cf849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6GCOS1ko1tY8bfBp"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated"><a class="ae me" href="https://6chaoran.wordpress.com/2019/01/15/build-a-machine-translator-using-keras-part-1-seq2seq-with-lstm/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="4b3a" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">RNN的缺点</h1><h2 id="aa85" class="os mv it bd mw ot ou dn na ov ow dp ne lm ox oy ng lq oz pa ni lu pb pc nk iz bi translated">短期记忆</h2><p id="d060" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm nq lo lp lq nr ls lt lu ns lw lx ly im bi translated">我希望你已经思考了我们最后的RNN细胞奇怪的颜色分布。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/207190fe8e8edc967672a29c40a783ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/0*xwyqjAU7bB8VCtH5.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">RNN的最终产量</figcaption></figure><p id="67b7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是对短期记忆的一种解释。在RNN，在每个新的时间戳(新的输入)，旧的信息被当前的输入所改变。人们可以想象，在“<em class="mf"> t </em>”时间戳之后，在时间步长<em class="mf"> (t-k) </em>存储的信息会完全变形。</p><p id="6ccc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因此，RNNs不能用于很长的序列。</p><h2 id="642d" class="os mv it bd mw ot ou dn na ov ow dp ne lm ox oy ng lq oz pa ni lu pb pc nk iz bi translated">消失梯度</h2><p id="887d" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm nq lo lp lq nr ls lt lu ns lw lx ly im bi translated">这就是短期记忆的原因。由于<a class="ae me" href="https://medium.com/towards-artificial-intelligence/diving-deep-into-deep-learning-f34497c18f11" rel="noopener">反向传播</a>的性质，消失梯度存在于每种类型的神经网络中。</p><p id="9f7b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当我们训练一个神经网络时，有三个主要步骤与我们的训练相关联。首先，进行正向传递以进行预测。之后，它将预测值与理论值进行比较，产生一个损失函数。最后，我们的目标是使我们的预测更好，因此，我们实现了反向传播，修改每个节点的值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/b1d6a5cbb1b79b0239dd7e432e5baf4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*BbakPt4KwnRomYl5eMGzCw.png"/></div></figure><p id="0ce8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">“在计算损失函数后，我们非常确定我们的模型有问题，我们需要检查，但是，检查每个神经元实际上是不可能的。但是，我们挽救模型的唯一可能的方法就是逆行。</p><p id="4a4f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">反向传播步骤</strong></p><ul class=""><li id="626c" class="mg mh it lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated">我们计算输出端的某些损耗，并试图找出哪个节点造成了低效率。</li><li id="1cf1" class="mg mh it lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">为此，我们将回溯整个网络。</li><li id="b86b" class="mg mh it lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">假设，我们发现第二层(w3h2+b2)对我们的损失负有责任，我们会努力改变它。但是如果我们仔细考虑我们的网络，w3和b2是独立的实体，但是h2依赖于w2、b1和h1，h1进一步依赖于我们的输入，即x1、x2、x3……，xn。但是由于我们无法控制输入，我们将尝试修改w1和b1。</li></ul><p id="2a22" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了计算我们的变化，我们将使用链式法则。"</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/86b64dd6cce62e8105c92cbf77991f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mn1TxLx8xtOZQy7N.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">反向传播的链式法则，<a class="ae me" href="https://dzone.com/articles/the-very-basic-introduction-to-feed-forward-neural" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="3bf7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当我们执行反向传播时，我们计算每个节点的权重和偏差。但是，如果前几层的改进很少，那么对当前层的调整就会小得多。这导致梯度急剧减小，从而导致我们的模型几乎没有变化，由于这一点，我们的模型不再学习，不再改进。</p><h1 id="1517" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">LSTMs和GRUs</h1><p id="15c2" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm nq lo lp lq nr ls lt lu ns lw lx ly im bi translated">为了克服RNNs的缺点，我们有<strong class="lf jd"> LSTM </strong> ( <em class="mf">长短期记忆</em>)和<strong class="lf jd"> GRU </strong> ( <em class="mf">门控循环单元</em>)。LSTMs和GRUs基本上是RNNs的高级版本，稍加调整以克服梯度消失的问题，并使用称为“门”的组件学习长期依赖性。门是一种张量运算，可以学习信息流，因此短期记忆对他们来说不是问题。</p><ul class=""><li id="9dc2" class="mg mh it lf b lg lh lj lk lm mi lq mj lu mk ly ml mm mn mo bi translated">在<strong class="lf jd">正向传播</strong>期间，门控制信息流。从而防止任何不相关的信息被写入状态。</li><li id="f1ee" class="mg mh it lf b lg mp lj mq lm mr lq ms lu mt ly ml mm mn mo bi translated">在<strong class="lf jd">反向传播</strong>过程中，门控制梯度的流动，这些门能够倍增梯度以避免梯度消失。</li></ul><p id="9586" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">要了解更多关于LSTM和格鲁的信息，您可以查看:</p><div class="pg ph gp gr pi pj"><a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">LSTM和GRU的图解指南:一步一步的解释</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">嗨，欢迎来到长短期记忆(LSTM)和门控循环单位(GRU)的图解指南。我是迈克尔…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px lb pj"/></div></div></a></div><blockquote class="py pz qa"><p id="0fce" class="ld le mf lf b lg lh kd li lj lk kg ll qb ln lo lp qc lr ls lt qd lv lw lx ly im bi translated">LSTM没有解决爆炸梯度的问题，因此，我们倾向于在实现lstm时使用梯度裁剪。</p></blockquote><h1 id="e51d" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">结论</h1><p id="3c08" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm nq lo lp lq nr ls lt lu ns lw lx ly im bi translated">希望这篇文章能帮助你以最好的方式理解递归神经网络，并帮助你实际使用它。</p><p id="0a59" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一如既往，非常感谢您的阅读，如果您觉得这篇文章有用，请分享！</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="4389" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请随意连接:</p><blockquote class="py pz qa"><p id="7a1a" class="ld le mf lf b lg lh kd li lj lk kg ll qb ln lo lp qc lr ls lt qd lv lw lx ly im bi translated"><em class="it">领英~</em><a class="ae me" href="https://www.linkedin.com/in/dakshtrehan/" rel="noopener ugc nofollow" target="_blank">T3】https://www.linkedin.com/in/dakshtrehan/T5】</a></p><p id="5157" class="ld le mf lf b lg lh kd li lj lk kg ll qb ln lo lp qc lr ls lt qd lv lw lx ly im bi translated"><em class="it">insta gram ~</em><a class="ae me" href="https://www.instagram.com/_daksh_trehan_/" rel="noopener ugc nofollow" target="_blank"><em class="it">https://www.instagram.com/_daksh_trehan_/</em></a></p><p id="b91e" class="ld le mf lf b lg lh kd li lj lk kg ll qb ln lo lp qc lr ls lt qd lv lw lx ly im bi translated"><em class="it">Github ~</em><a class="ae me" href="https://github.com/dakshtrehan" rel="noopener ugc nofollow" target="_blank">T15】https://github.com/dakshtrehan</a></p></blockquote><p id="6fae" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">关注更多机器学习/深度学习博客。</p><blockquote class="py pz qa"><p id="ccb8" class="ld le mf lf b lg lh kd li lj lk kg ll qb ln lo lp qc lr ls lt qd lv lw lx ly im bi translated"><em class="it">中等~</em><a class="ae me" href="https://medium.com/@dakshtrehan" rel="noopener">【https://medium.com/@dakshtrehan】T21</a></p></blockquote><h1 id="0b07" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">想了解更多？</h1><p id="49a2" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm nq lo lp lq nr ls lt lu ns lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/detecting-covid-19-using-deep-learning-262956b6f981" rel="noopener" target="_blank">利用深度学习检测新冠肺炎</a></p><p id="7811" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/the-inescapable-ai-algorithm-tiktok-ad4c6fd981b8" rel="noopener" target="_blank">无法逃脱的人工智能算法:抖音</a></p><p id="e69e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/an-insiders-guide-to-cartoonization-using-machine-learning-ce3648adfe8" rel="noopener">使用机器学习的卡通化内幕指南</a></p><p id="afa7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为什么你要为乔治·弗洛伊德的谋杀和德里的骚乱负责？</p><p id="1219" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/convolutional-neural-networks-for-dummies-afd7166cd9e" rel="noopener">虚拟卷积神经网络</a></p><p id="a948" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/diving-deep-into-deep-learning-f34497c18f11" rel="noopener">深入钻研深度学习</a></p><p id="1649" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/why-choose-random-forest-and-not-decision-trees-a28278daa5d" rel="noopener">为什么选择随机森林而不是决策树</a></p><p id="cff4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/@dakshtrehan/clustering-what-it-is-when-to-use-it-a612bbe95881" rel="noopener">聚类:它是什么？什么时候用？</a></p><p id="56d7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/@dakshtrehan/start-off-your-ml-journey-with-k-nearest-neighbors-f72a122f428" rel="noopener">从k个最近邻居开始你的ML之旅</a></p><p id="ee63" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/swlh/things-you-never-knew-about-naive-bayes-eb84b6ee039a" rel="noopener">朴素贝叶斯解释</a></p><p id="4aee" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/analytics-vidhya/activation-functions-explained-8690ea7bdec9" rel="noopener">激活功能说明</a></p><p id="98a8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/parameters-optimization-explained-876561853de0" rel="noopener" target="_blank">参数优化解释</a></p><p id="9ef8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c" rel="noopener" target="_blank">梯度下降解释</a></p><p id="2f81" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://towardsdatascience.com/logistic-regression-explained-ef1d816ea85a" rel="noopener" target="_blank">逻辑回归解释</a></p><p id="e3ed" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/towards-artificial-intelligence/linear-regression-explained-f5cc85ae2c5c" rel="noopener">线性回归解释</a></p><p id="ba64" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae me" href="https://medium.com/datadriveninvestor/determining-perfect-fit-for-your-ml-model-339459eef670" rel="noopener">确定最适合您的ML模型</a></p><blockquote class="py pz qa"><p id="c7c7" class="ld le mf lf b lg lh kd li lj lk kg ll qb ln lo lp qc lr ls lt qd lv lw lx ly im bi translated"><em class="it">干杯！</em></p></blockquote></div></div>    
</body>
</html>