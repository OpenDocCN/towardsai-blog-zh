<html>
<head>
<title>Two Ways to Learn Audio Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习音频嵌入的两种方法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/two-ways-to-learn-audio-embeddings-9dfcaab10ba6?source=collection_archive---------0-----------------------#2019-09-04">https://pub.towardsai.net/two-ways-to-learn-audio-embeddings-9dfcaab10ba6?source=collection_archive---------0-----------------------#2019-09-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9d5c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">音频特征工程| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向人工智能</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/63a1502123c8e40e61eb6e4f00139e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FrbZhfAP93w7cWD1"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@abn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Á·阿尔瓦罗·伯纳尔</a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="ed98" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae ko" href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum" rel="noopener ugc nofollow" target="_blank">梅尔倒谱系数</a>(MFCC)<a class="ae ko" href="https://en.wikipedia.org/wiki/Zero-crossing_rate" rel="noopener ugc nofollow" target="_blank">、过零率</a>是音频的一些经典特征。可以很容易地通过库提取它。然而，它可能无法为当今的深度学习模型提供高质量的信号或输入。</p><p id="1fe9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">两个研究小组提出了一种不同的方法来学习音频嵌入，但没有利用那些经典的特征。Chung和Glass (2018)建议学习基于单词的嵌入，而Haque等人(2019)建议学习基于句子的嵌入。</p><h1 id="2817" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">Speech2Vec</h1><p id="73fa" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">Chung和Glass受到了word2vec的启发，提出了一种学习音频嵌入的不同方法。word2vec利用skip-gram或连续词袋(CBOW)来学习单词嵌入。简而言之，单词嵌入是通过邻近单词学习的。如果你不熟悉它，你可以访问<a class="ae ko" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a>或<a class="ae ko" href="https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a" rel="noopener" target="_blank">这个故事</a>。</p><p id="dc89" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Speech2Vec采用了类似的概念，即通过相邻的声学区域学习声学嵌入。音频片段被按单词分割，并被馈送给模型以学习对应于单词的音频片段的固定嵌入。</p><h2 id="f722" class="mq lo it bd lp mr ms dn lt mt mu dp lx la mv mw mb le mx my mf li mz na mj iz bi translated">带Skip-gram和CBOW的Speech2Vec</h2><p id="bf2f" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">与word2vec的skip-gram方法相同，目标单词预测目标单词之前和之后预定义范围为k的周围单词。Speech2Vec的连续词袋(CBOW)以不同的方式前进。它不是使用目标词来预测周围的词，而是使用周围的词来预测目标词。</p><figure class="nc nd ne nf gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nb"><img src="../Images/0a436eb32e2aaf15e431ed44fb857d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9UseVT02HNEMqzekuO8QHQ.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">左:Speech2Vec，带跳格。右:与CBOW的Speech2Vec(钟和格拉斯，2018)</figcaption></figure><h1 id="ce64" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">句子级嵌入</h1><p id="2532" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">Haque等人介绍了用于音频输入的句子级嵌入，而字符级、单词级或音素级音频嵌入是经典的嵌入方法。观点是经典的嵌入太短，而句子层次足够长以捕捉更高层次的意思。</p><figure class="nc nd ne nf gt kd gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/bb93bad69c4cd16fecb324eadcc4cd54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*xgvaEcc_10XV1AjFnYnTYA.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">学习口语句子嵌入的架构(Haque等人，2019年)</figcaption></figure><p id="a669" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">作者将学习过程表述为学习口语句子嵌入的多任务学习。任务包括语音识别、情感识别和说话人识别。</p><figure class="nc nd ne nf gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nh"><img src="../Images/8ee4e7443ebfd1748b64fc66c6dbaa8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQGFbReyHov2Hw1L_R26qg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">各种嵌入的可视化(哈克等人，2019年)</figcaption></figure><h1 id="57fa" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">拿走</h1><ul class=""><li id="b0aa" class="ni nj it kr b ks ml kw mm la nk le nl li nm lm nn no np nq bi translated">在线预测期间，按单词和句子分割音频可能不会更容易</li><li id="7af7" class="ni nj it kr b ks nr kw ns la nt le nu li nv lm nn no np nq bi translated">对于Speech2Vec，由于不同的说话人、通道和背景噪声，同一个词有不同的嵌入。为了便于实验，相同单词的嵌入将被平均以呈现特定的单词。</li></ul><h1 id="94ec" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。在<a class="ae ko" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ko" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上随时联系<a class="ae ko" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>。</p><h1 id="be7f" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">延伸阅读</h1><ul class=""><li id="a617" class="ni nj it kr b ks ml kw mm la nk le nl li nm lm nn no np nq bi translated"><a class="ae ko" href="https://medium.com/hackernoon/how-can-you-apply-unsupervised-learning-on-audio-data-be95153c5860" rel="noopener">如何对音频数据应用无监督学习</a></li><li id="f109" class="ni nj it kr b ks nr kw ns la nt le nu li nv lm nn no np nq bi translated"><a class="ae ko" href="https://becominghuman.ai/how-does-your-assistant-device-work-based-on-text-to-speech-technology-5f31e56eae7e" rel="noopener ugc nofollow" target="_blank">您的助理设备如何基于文本到语音转换技术工作</a></li></ul><h1 id="1470" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">参考</h1><ul class=""><li id="dd0d" class="ni nj it kr b ks ml kw mm la nk le nl li nm lm nn no np nq bi translated">T.米科洛夫、g .科拉多、k .陈和杰弗里·迪恩。<a class="ae ko" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计</a>。2013.</li><li id="4d97" class="ni nj it kr b ks nr kw ns la nt le nu li nv lm nn no np nq bi translated">Y.钟和j .格拉斯。<a class="ae ko" href="https://arxiv.org/pdf/1803.08976.pdf" rel="noopener ugc nofollow" target="_blank"> Speech2Vec:从语音中学习单词嵌入的序列到序列框架</a>。2018</li><li id="8200" class="ni nj it kr b ks nr kw ns la nt le nu li nv lm nn no np nq bi translated">A.哈克，郭，，李。<a class="ae ko" href="https://arxiv.org/pdf/1902.07817.pdf" rel="noopener ugc nofollow" target="_blank">口语句子的音频语言嵌入</a>。2019</li></ul></div></div>    
</body>
</html>