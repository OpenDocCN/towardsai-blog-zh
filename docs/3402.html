<html>
<head>
<title>Practical Guide to Boosting Algorithms In Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的助推算法实用指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/practical-guide-to-boosting-algorithms-in-machine-learning-61c023107e12?source=collection_archive---------0-----------------------#2022-12-15">https://pub.towardsai.net/practical-guide-to-boosting-algorithms-in-machine-learning-61c023107e12?source=collection_archive---------0-----------------------#2022-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b488" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用弱学习者创造强学习者</h2></div><p id="e08c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di"> B </span> oosting(最初称为假设提升)是指任何能够将几个弱学习者组合成一个强学习者的集成方法。大多数boosting方法的一般思想是顺序训练预测器，每个预测器都试图纠正其前任。首先，根据训练数据建立模型。然后建立第二个模型来纠正第一个模型中存在的错误。继续该过程，并且添加模型，直到正确预测了完整的训练数据集，或者添加了最大数量的模型。通过组合所有单个模型的预测来做出最终预测。Boosting可用于回归和分类任务，是处理非线性数据的强大工具。它还相对抗过拟合，这意味着它通常可以在不牺牲泛化能力的情况下实现高水平的准确性。提高预测模型的准确性也是众所周知的。因此，Boosting是许多实际机器学习应用程序的流行选择。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/60edb8d4b98fd7e62e3aae6a14e3f1b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5fsnjicAXLDfHIH7"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">由<a class="ae md" href="https://unsplash.com/@hannahbusing?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">汉娜·布斯</a>在<a class="ae md" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="3f8b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有许多可用的增强方法，但在本文中，我们将涵盖最流行的增强算法:自适应增强(AdaBoost)、梯度增强和极端梯度增强(XGBoost)。</p><h2 id="89d1" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">目录:</h2><ol class=""><li id="3ec6" class="mx my it kk b kl mz ko na kr nb kv nc kz nd ld ne nf ng nh bi translated">自适应增压(AdaBoost) <br/> 1.1。自适应Boosting算法讲解<br/> 1.2。Python <br/> 1.3中的AdaBoost。用于分类的AdaBoost<br/>1.4。Adaboost回归</li><li id="29ae" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">梯度升压<br/> 2.1。梯度提升算法讲解<br/> 2.2。Python <br/> 2.3中的渐变提升。提前停止的梯度推进</li><li id="d83b" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">极端梯度推进(XGBoost) <br/> 3.1。XGBoost &amp;的主要优点是什么？<br/> 3.2。Python <br/> 3.3中的XGBoost。XGBoost超参数</li></ol></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="31bd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">如果你想免费学习数据科学和机器学习，看看这些资源:</strong></p><ul class=""><li id="6d73" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">免费互动路线图，自学数据科学和机器学习。从这里开始:<a class="ae md" href="https://aigents.co/learn/roadmaps/intro" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn/roadmaps/intro</a></li><li id="5ec4" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">数据科学学习资源搜索引擎(免费)。将你最喜欢的资源加入书签，将文章标记为完整，并添加学习笔记。<a class="ae md" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></li><li id="d26e" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">想要在导师和学习社区的支持下从头开始学习数据科学吗？免费加入这个学习圈:<a class="ae md" href="https://community.aigents.co/spaces/9010170/" rel="noopener ugc nofollow" target="_blank">https://community.aigents.co/spaces/9010170/</a></li></ul></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="c01f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">如果你想在数据科学&amp;人工智能领域开始职业生涯，但不知道如何开始。我提供数据科学指导课程和长期职业指导:</strong></p><ul class=""><li id="922f" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">长期师徒:【https://lnkd.in/dtdUYBrM】T4</li><li id="c794" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">辅导课程:<a class="ae md" href="https://lnkd.in/dXeg3KPW" rel="noopener ugc nofollow" target="_blank">https://lnkd.in/dXeg3KPW</a></li></ul><p id="d774" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ny">加入</em> </strong> <a class="ae md" href="https://youssefraafat57.medium.com/membership" rel="noopener"> <strong class="kk iu"> <em class="ny">中等会员</em> </strong> </a> <strong class="kk iu"> <em class="ny">计划继续无限制学习。如果你使用下面的链接，我会收到一小部分会员费，不需要你额外付费。</em> </strong></p><div class="nz oa gp gr ob oc"><a href="https://youssefraafat57.medium.com/membership" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">加入我的介绍链接媒体-优素福胡斯尼</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">阅读Youssef Hosni(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">youssefraafat57.medium.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq lx oc"/></div></div></a></div></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="4982" class="or mf it bd mg os ot ou mj ov ow ox mm jz oy ka mp kc oz kd ms kf pa kg mv pb bi translated">1.自适应增强(AdaBoost)</h1><h2 id="9393" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">1.1.自适应升压算法解释</h2><p id="4c8a" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">我们要讨论的第一个boosting算法是<strong class="kk iu">自适应Boosting </strong> (AdaBoost)。新预测器校正其前任的一种方法是对前任拟合不足的训练实例多加注意。这是通过给每个实例分配权重来实现的，对错误分类的实例分配较高的权重。这导致新的预测者越来越关注错误分类的病例。</p><p id="ada7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，为了构建AdaBoost分类器，第一基本分类器被训练并用于对训练集进行预测。然后，错误分类的训练实例的相对权重增加。使用更新的权重训练第二个分类器，并且再次对训练集进行预测，权重被更新，等等。</p><p id="8b64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦训练了所有的预测器，集成就使用所有训练的预测器进行预测，并使用硬投票或软投票来组合它们。在硬投票中，我们考虑每个分类器的类别预测，然后根据特定类别的最大投票数对输入进行分类。在软投票中，我们考虑每个分类器对每个类别的概率预测，然后根据该类别的平均概率(分类器概率的平均值)将输入分类到具有最大概率的类别。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/313f1cf8d2b58861a79d7f50254cc5b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*L0cdMsfTgkchIY2B3zwX9Q.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图一。Adaboost算法概述。</figcaption></figure><p id="b12a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们仔细看看AdaBoost算法背后的数学原理，以便更好地理解它的工作原理。最初，将每个实例权重w(i)设置为1\m。然后训练第一个预测器，并在训练集上计算其加权误差率e₁。等式(1)是jᵗʰ预测器的加权误差率eⱼ。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pg"><img src="../Images/ce4b8e3f2182806549d1285a8a7a5233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ogo9mcLOxGMpve9P2ByVZA.png"/></div></div></figure><p id="687d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后使用等式2计算预测值的权重αⱼ:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/1a99f3e37473324212c28e5b2d303059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*8SnLZL3YI-lRIGHdCrEyKg.png"/></div></figure><p id="3f1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中η是学习率超参数(默认为1)。预测器越精确，其权重就越高。如果只是随机猜测，那么它的权重将接近于零。然而，如果它经常是错误的(即，不如随机猜测准确)，那么它的权重将是负的。接下来，使用等式3更新实例权重，其中错误分类的实例被提升。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pi"><img src="../Images/94365d3565234d3e105eac6b8fbd3c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQZjhQaSqFzs5I_LMaK_Qg.png"/></div></div></figure><p id="585b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，通过将所有实例权重除以w的和来归一化所有实例权重。最后，使用更新后的权重来训练新的预测器，并重复整个过程(计算新预测器的权重，更新实例权重，然后训练另一个预测器，依此类推)。当达到期望的预测器数量或找到理想的预测器时，算法停止。为了进行预测，AdaBoost只需计算所有预测值的预测值，并使用预测值权重αⱼ.对它们进行加权如等式4所示，预测类别是接收大多数加权投票的类别。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pj"><img src="../Images/c810465fff10a5ba0ad58dc1b8c4c6ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sG54FOVescoFG29tZnX_ew.png"/></div></div></figure><h2 id="e968" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">1.2.Python中的AdaBoost</h2><p id="b16c" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">Scikit-Learn使用AdaBoost的多类版本，称为<strong class="kk iu"> SAMME </strong>(使用多类指数损失函数的分阶段加法建模)。当只有两个类时，<strong class="kk iu"> SAMME </strong>相当于AdaBoost。此外，如果预测器可以估计类概率(即，如果它们有predict_proba()方法)，Scikit-Learn可以使用SAMME的一个变体，称为SAMME。R(其中R代表“实数”)，它依赖于类别概率而不是预测，通常性能更好。</p><h2 id="2030" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated"><strong class="ak">1.3<em class="pk">。Adaboost为分类</em> </strong></h2><p id="1cfc" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">下面的代码使用Scikit-Learn的<strong class="kk iu"> AdaBoostClassifier </strong>类基于500个决策树桩训练了一个AdaBoost分类器(如下所示，还有一个用于回归任务的<strong class="kk iu"> AdaBoostRegressor </strong>类)。决策树桩是max_depth=1的决策树，换句话说，是由一个决策节点和两个叶节点组成的树。这是AdaBoostClassifier类的默认基本估计量:</p><p id="b16f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先创建一个月球数据集并绘制它:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/26f42cda4d5a5a0a0a2698d05acb7862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*k3CWcK8jZZzPaYa-0rhHoQ.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图二。</figcaption></figure><p id="554d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们画出决策界限。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi po"><img src="../Images/ec5089cc64bf32f9b4c38a7baab46b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*Kx_BAWtYQnDvXV50bp1U9Q.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图3。</figcaption></figure><h2 id="3acf" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">1.4.Adaboost回归</h2><p id="b609" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">我们还可以使用Scikit-Learn的<strong class="kk iu"> AdaBoostRegressor </strong>类来使用Adaboost进行回归。首先，我们将使用下面的代码创建一个二次数据集，用于回归任务:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/f716c01a3b586b8534bb841fd89c248d.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*XjZZ0Qx45iMJT9sMhXvZBw.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图4。用于回归任务的二次数据集</figcaption></figure><p id="d91e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们在训练数据集上训练一个<strong class="kk iu"> AdaBoost回归器</strong>，预测验证数据集的标签，并使用以下代码计算均方根误差:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/cfff948d35b8b3254ad8e7d295c62e88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*5TDrt-xF2cSSaAwII-4fnQ.png"/></div></figure><p id="de59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">均方根误差(RMSE)为0.003，这是一个非常好的结果。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="51c8" class="or mf it bd mg os ot ou mj ov ow ox mm jz oy ka mp kc oz kd ms kf pa kg mv pb bi translated">2.梯度推进</h1><h2 id="f2e6" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">2.1.梯度推进算法解释</h2><p id="73d4" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">另一个非常流行的提升算法是<strong class="kk iu">梯度提升</strong>。就像AdaBoost一样，梯度增强的工作原理是将预测器顺序添加到集合中，每个预测器都校正其前任。然而，这种方法不是像Adaboost那样在每次迭代中增加误分类的实例权重，而是尝试用新的预测器来拟合前一个预测器产生的残差。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/f075619c7bb6b0443cc54987f73122f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*0KqYN--E0iWg50EAoQdc1A.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图5。</figcaption></figure><p id="81ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个使用决策树作为基本预测器的简单回归示例。这被称为梯度树提升或梯度提升回归树(GBRT)。首先，让我们为训练集(例如，一个有噪声的二次训练集)拟合一个DecisionTreeRegressor:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="3418" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在根据第一个预测器产生的残差训练第二个决策树回归器:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="072b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们根据第二个预测器产生的残差训练第三个回归量:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="cf2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们有一个包含三棵树的系综。它可以简单地通过将所有树的预测相加来对新实例进行预测:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/f33e2896c159b66602f8667c6b1dd12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*X-QHDJx2JJNbKPmn2Zbh_g.png"/></div></figure><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/fe255394c49c8a5b0f502959616b6387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*yI12IQOc6jS8aIsekqjJWw.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图6。实践中的梯度推进。</figcaption></figure><p id="55f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图5在左栏中表示这三棵树的预测，在右栏中表示集合的预测。在第一行中，集合只有一棵树，因此它的预测与第一棵树的预测完全相同。在第二行中，根据第一棵树的残差训练新的树。在右侧，您可以看到集合的预测等于前两棵树的预测之和。类似地，在第三行中，根据第二棵树的残差训练另一棵树。你可以看到，随着集合中加入树木，集合的预测逐渐变得更好。</p><h2 id="306c" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">2.2.Python中的梯度增强</h2><p id="75b2" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">训练GBRT合奏的一个更简单的方法是使用Scikit-Learn的<strong class="kk iu">GradientBoostingClassifier&amp;GradientBoostingRegressor</strong>类。与RandomForestRegressor类非常相似，它具有控制决策树增长的超参数(例如，max_depth、min_samples_leaf等)，以及控制整体训练的超参数，例如树的数量(n_estimators)。</p><p id="213e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将从上面创建的月球数据集上的梯度增强分类器开始:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pt"><img src="../Images/628c94d3e38cab27a794e95d1de2a729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*WvAVDssCPSuDAUl0Y3a5IQ.png"/></div></div></figure><p id="cd1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还可以使用之前创建的<strong class="kk iu"> plot_decision_boundary </strong>函数绘制决策边界:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi po"><img src="../Images/77491e265a76bb27fb3e7cd1ce134165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*wCIXv9_FvDki15PtiwPPjQ.png"/></div></figure><p id="9b4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，该模型成功地以一种完美的方式分离了两个类，而没有导致任何过度拟合。</p><p id="18a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还可以使用<strong class="kk iu">GradientBoostingRegressor</strong>类将梯度推进用于回归，如下所示:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="c5d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> learning_rate </strong>超参数缩放每棵树的贡献。如果将其设置为较低的值，如0.1，则集合中将需要更多的树来适应训练集，但预测通常会更好地进行归纳。这是一种被称为<strong class="kk iu">收缩</strong>的正则化技术。让我们训练另一个GBRT，它具有较低的学习率= 0.1，但是具有较高数量的估计器= 200棵树。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/65a8e639f75498ca01c3efa601603334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*I_7oZFTJMf6zktGaRs68dQ.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图7。</figcaption></figure><p id="d35f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图7显示了以低学习率训练的两个GBRT系综:左边的系综没有足够的树来适应训练集，而右边的系综有太多的树，超过了训练集。</p><h2 id="6ba3" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">2.3.提前停止的梯度推进</h2><p id="0071" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">为了找到最佳的树数，你可以使用<strong class="kk iu">提前停止</strong>。实现这一点的一个简单方法是使用<strong class="kk iu"> staged_predict() </strong>方法:它返回集合在每个训练阶段所做预测的迭代器(用一棵树、两棵树等)。).</p><p id="8fef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下代码用120棵树训练一个GBRT系综，然后在训练的每个阶段测量验证误差以找到最佳的树数，最后使用最佳的树数训练另一个GBRT系综:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pu"><img src="../Images/9db26101f5ddf579825091dcc668fd19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rnhcewncmatopzt6-WND0w.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图8。</figcaption></figure><p id="846b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">流行的python库XGBoost中有一个梯度增强的优化实现，它代表极端梯度增强。这个包最初是由陈天琦作为分布式(深度)机器学习社区(DMLC)的一部分开发的，它的目标是非常快速、可伸缩和可移植。事实上，XGBoost往往是ML比赛中获奖作品的重要组成部分。我们将在下一节更深入地了解它。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="5f10" class="or mf it bd mg os ot ou mj ov ow ox mm jz oy ka mp kc oz kd ms kf pa kg mv pb bi translated">3.极端梯度增强(XGBoost)</h1><h2 id="0ddc" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">3.1.XGBoost &amp;主要优点是什么？</h2><p id="7454" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">XGBoost是梯度增强方法的高级版本。字面意思是极端的梯度推进。它提供了一个并行树提升，并且是用于回归、分类和排序问题的领先机器学习库。XGBoost是由陈天琦开发的，在许多开发人员的贡献下，它现在属于分布式机器学习社区(DMLC)的范畴。</p><p id="6197" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">XGBoost是梯度提升的可扩展和高度精确的实现，推动了提升树算法的计算能力的极限，主要是为了增强机器学习模型性能和计算速度而构建的。在XGBoost中，树是并行构建的，而不是顺序构建的，就像梯度增强一样。它遵循逐层策略，扫描梯度值，并使用这些部分和来评估训练集中每个可能分裂的分裂质量。这旨在提高计算的速度和效率。</p><p id="978e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">XGBoost是一个软件库，您可以下载并安装到您的机器上，然后从各种界面进行访问。具体来说，XGBoost支持以下主要接口:</p><ul class=""><li id="5cb6" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">命令行界面(CLI)。</li><li id="c29c" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">C++(编写库的语言)。</li><li id="57ec" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">Python接口，以及scikit-learn中的一个模型。</li><li id="b204" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">r接口以及caret包中的一个模型。</li><li id="bdae" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">朱莉娅。</li><li id="6733" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">像Scala这样的Java和JVM语言以及Hadoop这样的平台。</li></ul><p id="7eec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">XGBoost在过去几年中获得了巨大的声望，因为它帮助团队赢得了几乎每一场Kaggle结构化数据竞赛。在这些比赛中，公司和研究人员发布数据，之后统计学家和数据挖掘者竞争产生预测和描述数据的最佳模型。</p><h2 id="c97a" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated"><strong class="ak">系统优化:</strong></h2><ul class=""><li id="1d98" class="mx my it kk b kl mz ko na kr nb kv nc kz nd ld nx nf ng nh bi translated"><strong class="kk iu">正则化</strong>:因为决策树的集合有时会非常复杂。XGBoost使用Lasso和岭回归正则化来惩罚高度复杂的模型。</li><li id="32d4" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><strong class="kk iu">并行化和缓存块:</strong>在，XGBoost中，我们不能并行训练多棵树，但是可以并行生成树的不同节点。为此，数据需要按顺序排序。为了降低排序的成本，它将数据存储在块中。它以压缩的列格式存储数据，每一列按相应的特征值排序。这种切换通过抵消计算中的任何并行化开销来提高算法性能。</li><li id="9318" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><strong class="kk iu">树修剪:</strong> XGBoost使用<strong class="kk iu"> max_depth </strong>参数，该参数由分支的停止标准指定，并开始向后修剪树。这种深度优先的方法显著提高了计算性能。</li><li id="31ad" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><strong class="kk iu">缓存感知和分数外计算:</strong>该算法被设计为有效地利用硬件资源。这是通过在每个线程中分配内部缓冲区来存储梯度统计数据的缓存感知来实现的。“核外计算”等进一步的增强优化了可用磁盘空间，同时处理了不适合内存的大数据帧。在核外计算中，XGBoost试图通过压缩来最小化数据集。</li><li id="d837" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><strong class="kk iu">稀疏感知:</strong> XGBoost可以处理预处理步骤或缺失值可能产生的稀疏数据。它使用一种特殊的分裂查找算法，这种算法可以处理不同类型的稀疏模式。</li><li id="07e8" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><strong class="kk iu">加权分位数草图:</strong> XGBoost内置了分布式加权分位数草图算法，可以更轻松有效地在加权数据集之间找到最佳分割点。</li><li id="9581" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><strong class="kk iu">交叉验证:</strong> XGBoost实现附带了一个内置的交叉验证方法。当数据集不是很大时，这有助于算法防止过度拟合。</li></ul><h2 id="4dd7" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">3.2.Python中的XGBoost</h2><p id="3137" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">让我们首先使用下面的代码安装XGBoost库:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="53bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">和以前一样，我们将使用它在月球数据集上进行一次分类，在二次数据集上进行一次回归。让我们从分类任务开始:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/02bafdce2efcbb3fbf1165499df8b601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*ztbnFR--o3jkH20XT7-tEQ.png"/></div></figure><p id="26bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看起来测试精度很小，可能是因为模型过度拟合了训练数据。我们可以使用之前定义的<strong class="kk iu"> plot_decision_boundary </strong>函数绘制决策边界:</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="pl pm l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi po"><img src="../Images/157ca4817b56524b1fbe1b84586d62a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*Q1tw1ZdVXILpCXLe5WkuEw.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图9。</figcaption></figure><p id="aca8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如所料，模型过度拟合了训练数据，因此我们可以调整模型超参数来克服这种过度拟合。</p><h2 id="5e61" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">3.3.XGBoost超参数</h2><p id="10f5" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">模型参数可分为以下四个参数:<strong class="kk iu">通用参数</strong>、<strong class="kk iu">助推器参数</strong>、<strong class="kk iu">学习任务参数</strong>和<strong class="kk iu">命令行参数。</strong>为了提高性能，我们可以调整前三类参数——<strong class="kk iu">通用参数</strong>、<strong class="kk iu">助推器参数、</strong>和<strong class="kk iu">任务参数</strong>。第四类参数是<strong class="kk iu">命令行参数</strong>。它们只在XGBoost的控制台版本中使用。</p><p id="ab17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 1。一般参数:</strong>这些参数指导XGBoost模型的整体功能，它们是:</p><ul class=""><li id="a502" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated"><code class="fe pw px py pz b">booster [default = gbtree] </code> -这是用于提高模型性能的算法类型。它有三种选择<strong class="kk iu"> gbtree </strong>、<strong class="kk iu"> gblinear、</strong>或<strong class="kk iu"> dart。</strong><strong class="kk iu">GB tree</strong>和<strong class="kk iu"> dart </strong> -使用基于树的模型，而<strong class="kk iu"> gblinear </strong>使用线性模型。</li><li id="9142" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">verbosity [default = 1]</code> -它测量数据集中单词的详细程度。</li><li id="4c8d" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">nthread [default = max]</code> <strong class="kk iu"> </strong> - <strong class="kk iu"> </strong>这是用来运行XGBoost的并行线程数。</li></ul><p id="1e43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 2。助推器参数:</strong>我们有两种助推器——<strong class="kk iu">树状助推器</strong>和<strong class="kk iu">直线助推器</strong>。我们将把我们的讨论限制在<strong class="kk iu">树形助推器</strong>上，因为它总是优于<strong class="kk iu">线性助推器</strong>，因此后者很少被使用。</p><ul class=""><li id="2a3a" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated"><code class="fe pw px py pz b">eta [default = 0.3]</code> -类似于GBM中的学习率。其范围在[0，1]之间</li><li id="fc8e" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">gamma [default = 0] </code> - Gamma指定进行分割所需的最小损失减少量。这用于减少校正模型误差时的损失。</li><li id="66dd" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">max_depth [default = 6]</code> -这是XGBoost分类器的最大深度。</li><li id="0756" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">min_child_weight [default = 1]</code> -这是我们被允许划分树的叶节点的最小尺寸。它用于控制过度拟合。较高的值会阻止模型学习可能高度特定于为树选择的特定样本的关系。过高的值会导致拟合不足</li><li id="5124" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">max_delta_step [default = 0]</code> -用于在训练过程中更新模型类。通常，这个参数是不需要的，但是当类极度不平衡时，它可能有助于逻辑回归</li><li id="56dd" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">subsample [default = 1]</code> -我们用来对模型的训练阶段进行抽样的比率。将其设置为0.5意味着XGBoost将在生成树之前随机采样一半的训练数据，这将防止过度拟合。</li><li id="fd54" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">colsample_bylevel [default = 1]</code> -这显示了树中不同的分支级别是如何被分开的。</li><li id="5215" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">colsample_bynode [default = 1]</code> -显示不同的节点是如何分割的。</li><li id="f0b7" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">colsample_bytree [default = 1]</code> -展示了XGBoost中不同的树是如何被分开的。</li><li id="31e3" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">reg_lambda</code> -这是用于增加模型权重的参数。</li><li id="817d" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">reg_alpha</code> -这是用于减少模型重量的参数</li><li id="7954" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">n_estimators</code> -这是在模型训练期间添加的估计器的总数。</li></ul><p id="f4ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3。学习任务参数</strong></p><ul class=""><li id="f061" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated"><code class="fe pw px py pz b">objective</code> -它指定了用于构建模型的算法类型，在这种情况下，它使用逻辑回归。</li><li id="fe94" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">seed</code> -模型使用的种子。</li><li id="a2f0" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">random_state</code> -该播种编号由模型使用。</li><li id="7771" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">base_score</code> -这是对初始模型的预测。它的默认分数是<code class="fe pw px py pz b">0.5</code>。</li><li id="fa27" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><code class="fe pw px py pz b">n_jobs</code> -这是模型处理的作业总数。</li></ul></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="24ae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ny">喜爱文章？成为</em> </strong> <a class="ae md" href="https://youssefraafat57.medium.com/membership" rel="noopener"> <strong class="kk iu"> <em class="ny">中等会员</em> </strong> </a> <strong class="kk iu"> <em class="ny">继续无限制学习。如果你使用下面的链接，我会收到一小部分会员费，不需要你额外付费。</em> </strong></p><div class="nz oa gp gr ob oc"><a href="https://youssefraafat57.medium.com/membership" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">加入我的介绍链接媒体-优素福胡斯尼</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">阅读Youssef Hosni(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">youssefraafat57.medium.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq lx oc"/></div></div></a></div><p id="3f7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ny">感谢阅读！如果你喜欢这篇文章，一定要鼓掌(高达50！)并在</em></strong><a class="ae md" href="https://www.linkedin.com/in/youssef-hosni-b2960b135/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="ny">LinkedIn</em></strong></a><strong class="kk iu"><em class="ny">上与我联系，并在</em> </strong> <a class="ae md" href="https://youssefraafat57.medium.com/" rel="noopener"> <strong class="kk iu"> <em class="ny">上关注我的【中型】</em> </strong> </a> <strong class="kk iu"> <em class="ny">以保持更新我的新文章</em> </strong></p></div></div>    
</body>
</html>