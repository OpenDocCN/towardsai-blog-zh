<html>
<head>
<title>VQ-GAN &amp; Transformer — Taming Transformers for High-Resolution Image Synthesis: Synopsis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">甘和变形金刚——驯服变形金刚进行高分辨率图像合成:简介</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/synopsis-taming-transformers-for-high-resolution-image-synthesis-vq-gan-transformer-15c29b2d529d?source=collection_archive---------2-----------------------#2022-03-13">https://pub.towardsai.net/synopsis-taming-transformers-for-high-resolution-image-synthesis-vq-gan-transformer-15c29b2d529d?source=collection_archive---------2-----------------------#2022-03-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="199e" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">摘要</h1><p id="d055" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这篇文章总结了由<em class="lk"> Patrick Esser，Robin Rombach和bjrn Ommer</em>所做的作品“<a class="ae lj" href="https://arxiv.org/abs/2012.09841" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">”驯服高分辨率图像合成的变形金刚</strong> </a>”。它强调了关键的带回家的信息，改进的范围，以及这项工作的应用。本文有助于有兴趣了解卷积神经网络(CNN)[ <a class="ae lj" href="https://dl.acm.org/doi/10.5555/109230.109279" rel="noopener ugc nofollow" target="_blank"> 1 </a>、变压器[ <a class="ae lj" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]、自动编码器[ <a class="ae lj" href="https://www.science.org/doi/10.1126/science.1127647" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]、GAN[ <a class="ae lj" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]和矢量量化码本[ <a class="ae lj" href="https://arxiv.org/abs/1711.00937" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]等最先进的神经架构和技术如何结合用于图像合成的读者，而无需深入研究它们中的每一个。下面展示的思维导图是一个先决条件的知识金字塔，一个人必须拥有或随后获得，以理解在讨论的论文中定义的技术和正在<em class="lk">转变</em>生成性人工智能领域的类似方法。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/0411307bd4f5bcd467339b035c51b78a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HcUhH3m8vOmqs_RnowSqzA.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">本文中使用的概念思维导图(图片由作者提供)</figcaption></figure><p id="c76b" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">这篇文章的结构如下:</p><ul class=""><li id="59f2" class="mg mh iq kn b ko mb ks mc kw mi la mj le mk li ml mm mn mo bi translated"><a class="ae lj" href="#e05c" rel="noopener ugc nofollow">问题陈述</a></li><li id="3368" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated">方法论背后的直觉</li><li id="0b0b" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated"><a class="ae lj" href="#e651" rel="noopener ugc nofollow">结论</a></li><li id="1fff" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated"><a class="ae lj" href="#5b07" rel="noopener ugc nofollow">机遇</a></li><li id="ee07" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated"><a class="ae lj" href="#ffbd" rel="noopener ugc nofollow">应用</a></li><li id="ad75" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated"><a class="ae lj" href="#ff26" rel="noopener ugc nofollow">参考文献</a></li></ul><h2 id="e05c" class="mu jo iq bd jp mv mw dn jt mx my dp jx kw mz na kb la nb nc kf le nd ne kj nf bi translated">问题陈述</h2><p id="c19c" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">证明将细胞神经网络的感应偏差与变压器的表现力相结合，以无条件或受控的方式合成高分辨率和感知丰富的图像的有效性。</p><h2 id="f323" class="mu jo iq bd jp mv mw dn jt mx my dp jx kw mz na kb la nb nc kf le nd ne kj nf bi translated">方法论背后的直觉</h2><p id="d502" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了理解驯服变压器如何促进高分辨率合成，我们需要从头开始，矢量量化——变分自动编码器(VQ-VAE)[ <a class="ae lj" href="https://arxiv.org/abs/1711.00937" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]。VQ-VAE是一种变分自动编码器网络，将图像信息编码到一个离散的潜在空间。它将编码器产生的潜在向量映射到一个最接近它的向量，该向量属于一个固定的向量列表，称为向量码本。使用像欧几里德距离(<strong class="kn ir"> 𝘓₂范数</strong>)这样的度量来计算亲密度。为了确保生成的输出的多样性，编码器输出潜在向量的网格而不是单个潜在向量，并且每个潜在向量被映射到属于同一码本的向量之一。这个过程被称为潜在空间的<em class="lk">量化。因此，我们获得了表示潜在空间的整数的2D网格，其中每个网格值对应于码本列表内的向量的索引。最后，潜在空间的量化后选择的向量被馈送到解码器以生成输出图像。下面的体系结构图展示了这一点:</em></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ng"><img src="../Images/555cb7064dcc363a317e636fb779855c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kdgz2qnEy88OXD6XiA0BxQ.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">整体架构:VQ-GAN +变压器(来源:<a class="ae lj" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank"> Esser等人</a>)。)</figcaption></figure><p id="e51e" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">在数学上，VQ-VAE模型模拟了训练期间的图像重建问题。编码器子网络模拟函数<strong class="kn ir"> E(x) </strong>，该函数输出图像<strong class="kn ir"> x </strong>的潜在向量网格。使用函数<strong class="kn ir"> q(ẑ) </strong>对网格中的每个潜在向量进行量化，该函数将其映射到来自可用向量码本的向量<strong class="kn ir"> (ẑ) </strong>。最后，解码器模拟函数<strong class="kn ir"> G(zᵩ) </strong>，该函数从码本向量网格<strong class="kn ir"> (zᵩ) </strong>中重构图像<strong class="kn ir">(x’)</strong>。<em class="lk">(此处ᵩ = q) </em></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nh"><img src="../Images/0916e821d4545e9552fc11b0eccd8965.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*dnjAwwvu_5vATp4uhCpilQ.png"/></div></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/402570fbd39b1c49ef5f7eea020cfc66.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*vmLYybzIpUoOmHcgHzShKA.png"/></div></figure><p id="858f" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">其中<strong class="kn ir"> ẑᵤᵥ </strong>表示网格中<strong class="kn ir"> (u，v) </strong>位置的编码器预测潜向量，<strong class="kn ir"> zᵢ </strong>表示码本向量，<strong class="kn ir"> h </strong>表示网格高度，<strong class="kn ir"> w </strong>表示网格，<strong class="kn ir"> 𝒹 </strong>为每个码本向量的维数长度。</p><p id="79a7" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">通过优化以下目标函数来联合学习VQ-VAE和码本向量:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/245911310a64c96c50ca9dd80c536505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*9kcpNwDdfVInvNSm_kM94A.png"/></div></figure><p id="4ae2" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">其中<strong class="kn ir"> 𝘓ᵥ </strong>是用于一起训练模型和码本的<strong class="kn ir">向量量化损失</strong>，第一项是<strong class="kn ir">重构损失(𝘓ᵣ) </strong>，第二项也称为<strong class="kn ir">码本对齐损失，</strong>帮助将码本向量与编码器输出对齐，第三项也称为<strong class="kn ir">承诺损失，</strong>帮助反向对齐，即编码器输出与码本向量对齐，其对总损失的重要性由可调超参数<strong class="kn ir"> β缩放。sg[] </strong>代表<strong class="kn ir">停止梯度</strong>，即通过[]内的变量不会发生梯度流动或重量更新。对于这些概念的详细和直观的理解，你可以参考这些帖子<a class="ae lj" href="https://www.jeremyjordan.me/variational-autoencoders/" rel="noopener ugc nofollow" target="_blank"> VAE </a>和<a class="ae lj" href="https://ml.berkeley.edu/blog/posts/vq-vae/" rel="noopener ugc nofollow" target="_blank"> VQ-VAE </a>。</p><p id="bc14" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">现在是讨论文件中提出的对VQ-VAE法案的一系列改进。用感知损失[ <a class="ae lj" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank"> 6 </a> ]代替了<strong class="kn ir"> 𝘓₂重建损失</strong>，并引入了一个带有基于补丁的鉴别器[ <a class="ae lj" href="https://arxiv.org/abs/1611.07004" rel="noopener ugc nofollow" target="_blank"> 7 </a>的对抗训练程序来区分真实图像和重建图像。<em class="lk">这一改变确保了压缩率提高后的良好感知质量</em>。我将解释压缩的相关性，同时讨论变形金刚如何帮助图像合成过程。作者将这种方法命名为向量量化生成对抗网络(VQ-GAN)。以下是总体目标函数:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/eb99d896337ddb36564b1aa44ecd9be3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*W4z0g6n8S0uvBv9bfcTsVA.png"/></div></figure><p id="0961" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">这里<strong class="kn ir"> 𝘓g </strong>是<strong class="kn ir"> </strong>生成性对抗网络损耗，<strong class="kn ir"> λ </strong>是由下式给出的自适应权重，其中<strong class="kn ir"> 𝛿 </strong>是数值稳定性。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8282d63ee9ca4e66cbbd184529d212ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*58MFhwgN3FaojP8KNahy7w.png"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/5a7bc31d8af07ee25b3a4c6579276745.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*s8SWQzPbVn-mwBQ3do93gg.png"/></div></figure><p id="262a" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">让我们暂时离题，从概率的角度来看看VQ-甘框架。我们在潜在空间上有一个先验<strong class="kn ir"> p(z) </strong>，编码器近似潜在空间的后验分布<strong class="kn ir"> p(z|x) </strong>，解码器近似潜在空间经由分布<strong class="kn ir"> p(x|z)的重建。</strong>为了定义先验，我们展平形成量化潜在空间的码本矢量索引的2D网格，并获得索引序列<strong class="kn ir"> s. </strong>到目前为止，在所有潜在代码上假设一致的先验，使得它们在序列中的步骤<strong class="kn ir"> i </strong>中的选择同样可能并且独立于先前的步骤。但对于模型试图学习的给定数据集，这可能不成立。换句话说，应该从数据中学习潜在代码的分布。这将有两个好处。首先，我们通过从新的训练过的先验中采样潜在值而生成的数据将更好地代表底层数据集。第二，如果潜在值的分布不均匀，那么代表潜在值序列的比特可以通过应用标准霍夫曼或算术编码来进一步压缩。这就是我在帖子前面提到的压缩。</p><p id="ed61" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">学习潜在先验会产生自回归问题。最近，变压器已成为自回归和顺序建模任务的首选架构，在低分辨率图像合成方面优于卷积型(CNN)。在这篇<a class="ae lj" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">博客</a>中，我们用一个NLP例子很好地解释了转换器的工作原理。传统上，图像转换器处理<em class="lk">离散值</em>并在像素级执行顺序学习，其成本与图像分辨率成平方比例。虽然它们非常适合需要模型来学习局部真实感和理解全局构图的高分辨率图像合成，但计算成本一直是抑制因素。然而，通过将图像表示为一系列码本向量索引<strong class="kn ir">，</strong>，使用变换器模型在计算上变得容易处理。在选择了<strong class="kn ir"> s </strong>中指数的一些排序之后，学习先验可以公式化为自回归下一指数预测(<strong class="kn ir"> sᵢ </strong>)问题。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/066e5d47799f8d2a737a96224d85aa03.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*Db--V8iLWZgx_4i41PzDjg.png"/></div></figure><p id="ceb3" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">其中p(s)是完整图像表示的可能性，</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi no"><img src="../Images/208ff50e5b5873b27f1116d1b65c6dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*5BKweF4Qn7LUyCcBwpAzjQ.png"/></div></figure><p id="1439" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated">最后，注意机制是转换器架构的本质，它为输入到转换器的序列<strong class="kn ir"> s </strong>的长度设置了一个计算上限。为了缓解这一问题并生成百万像素级别的图像，在滑动窗口方法的帮助下，将transformer应用于图像补丁，如下图所示。VQ-甘确保可用的背景仍然足以忠实地模拟图像，只要数据集的统计数据近似空间不变或空间条件信息可用。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5188a87e00aae93911408fa09d961757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*AvMQy2hT0m-V1LDXtgxb2A.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">滑动注意窗(来源:<a class="ae lj" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank">埃塞尔等人</a>。)</figcaption></figure><h2 id="e651" class="mu jo iq bd jp mv mw dn jt mx my dp jx kw mz na kb la nb nc kf le nd ne kj nf bi translated">结论</h2><ul class=""><li id="d507" class="mg mh iq kn b ko kp ks kt kw nq la nr le ns li ml mm mn mo bi translated">所提出的方法能够通过CNN架构对图像成分进行建模，并通过通用和通用的transformer架构对它们的组成进行建模，从而充分挖掘它们在图像合成中互补优势的潜力。</li><li id="7a1d" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated">对于高分辨率图像合成，它优于最先进的卷积方法，对于条件生成任务，它接近最先进的方法。一些结果如下所示，</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c2ec90b71a580694e32fef53ce03131d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*9uAM8KTv2xS9S1p0YVSEMA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">从语义布局生成的高分辨率样本(来源:<a class="ae lj" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank">埃塞尔等人</a>)。)</figcaption></figure><ul class=""><li id="d2de" class="mg mh iq kn b ko mb ks mc kw mi la mj le mk li ml mm mn mo bi translated">VQ-GAN的性能优于最接近的可比网络VQ-VAE-2[ <a class="ae lj" href="https://arxiv.org/abs/1906.00446" rel="noopener ugc nofollow" target="_blank"> 8 </a> ]，同时提供显著更高的压缩率，有助于降低计算复杂性。</li></ul><h2 id="5b07" class="mu jo iq bd jp mv mw dn jt mx my dp jx kw mz na kb la nb nc kf le nd ne kj nf bi translated">机会</h2><ul class=""><li id="b9ff" class="mg mh iq kn b ko kp ks kt kw nq la nr le ns li ml mm mn mo bi translated">方法来增强这种方法的实时可用性，因为基于普通变压器的技术计算量很大。</li><li id="3b9c" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated">扩展给定的多模态图像合成过程，如文本到图像的生成。</li></ul><h2 id="ffbd" class="mu jo iq bd jp mv mw dn jt mx my dp jx kw mz na kb la nb nc kf le nd ne kj nf bi translated">应用程序</h2><ul class=""><li id="91d6" class="mg mh iq kn b ko kp ks kt kw nq la nr le ns li ml mm mn mo bi translated"><strong class="kn ir">高分辨率图像合成</strong></li><li id="fc60" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated"><strong class="kn ir">语义图像合成</strong>，以语义分割掩膜为条件。</li><li id="7366" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated"><strong class="kn ir">结构到图像的合成</strong>，其以深度或边缘信息为条件。</li><li id="d4a4" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated"><strong class="kn ir">姿势引导合成</strong>，其以人类主体的姿势信息为条件，如时装造型。</li><li id="6578" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated"><strong class="kn ir">随机超分辨率合成</strong>，以低分辨率图像为条件</li><li id="97f1" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li ml mm mn mo bi translated"><strong class="kn ir">类别条件合成</strong>，以定义感兴趣类别(物体、动物、人类等)的单一索引值为条件。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/fdeb0530603ba514ece847932223606d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*WzLBo_qGQbZKOTMLWcGqLg.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">顶行:ImageNet上无条件培训的完成情况。第二行:深度到图像合成。第三行:语义引导的合成。第四行:姿势引导的人物生成。底部一行:类条件合成。(来源:<a class="ae lj" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank">埃塞尔等人</a>。)</figcaption></figure><h2 id="ff26" class="mu jo iq bd jp mv mw dn jt mx my dp jx kw mz na kb la nb nc kf le nd ne kj nf bi translated">参考</h2><ol class=""><li id="9668" class="mg mh iq kn b ko kp ks kt kw nq la nr le ns li nv mm mn mo bi translated">Y.LeCun等人，“<a class="ae lj" href="https://dl.acm.org/doi/10.5555/109230.109279" rel="noopener ugc nofollow" target="_blank">用反向传播网络</a>进行手写数字识别”，在Proc .神经Inf。过程。系统。，1990年，第396-404页。</li><li id="7b78" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li nv mm mn mo bi translated">A.Vaswani，N. Shazeer，N. Parmar，J. Uszko- reit，L. Jones，A. N. Gomez，L. Kaiser和I. Polosukhin，“<a class="ae lj" href="http://iv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>，”神经信息处理系统进展30:神经信息处理系统年度会议，NeurIPS，2017年。</li><li id="212b" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li nv mm mn mo bi translated">G.辛顿和r .萨拉胡季诺夫。"<a class="ae lj" href="https://www.science.org/doi/10.1126/science.1127647" rel="noopener ugc nofollow" target="_blank">用神经网络降低数据的维度</a>"科学313.5786(2006):504–507。</li><li id="69f3" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li nv mm mn mo bi translated">I. Goodfellow，J. Pouget-Abadie，M. Mirza，B. Xu，D. Warde-Farley，S. Ozair，a .，Y. Bengio，“<a class="ae lj" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">生成对抗网络</a>”，载于《神经信息处理系统进展》，2014年，第2672-2680页。</li><li id="d5e3" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li nv mm mn mo bi translated">A.作者声明:Steven k .<a class="ae lj" href="https://arxiv.org/abs/1711.00937" rel="noopener ugc nofollow" target="_blank">神经离散表示学习</a>。神经信息处理系统进展30 (2017)。</li><li id="d98d" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li nv mm mn mo bi translated">J.约翰逊，亚历山大和李。<a class="ae lj" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a>欧洲计算机视觉会议。施普林格，查姆，2016。</li><li id="536e" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li nv mm mn mo bi translated">页（page的缩写）Isola，J. Zhu，T. Zhou，A. Efros，“条件对抗网络下的<a class="ae lj" href="https://arxiv.org/abs/1611.07004" rel="noopener ugc nofollow" target="_blank">图像到图像翻译</a>”，2017<em class="lk">IEEE计算机视觉与模式识别会议<em class="lk">，</em>，2017。</em></li><li id="6563" class="mg mh iq kn b ko mp ks mq kw mr la ms le mt li nv mm mn mo bi translated">A.Razavi，A. Oord和O. Vinyals，“<a class="ae lj" href="https://arxiv.org/abs/1906.00446" rel="noopener ugc nofollow" target="_blank">使用vq-vae-2 </a>生成多样化的高保真图像”，2019年。</li></ol></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="c97d" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated"><em class="lk">感谢您阅读这篇文章！如果你觉得这篇文章增加了你的知识，请点击拍手图标来表达你的感激，并与你认为可能从中受益的人分享。如果您有任何问题或发现可能出现的错误，请在下面留下评论。</em></p><p id="707d" class="pw-post-body-paragraph kl km iq kn b ko mb kq kr ks mc ku kv kw md ky kz la me lc ld le mf lg lh li ij bi translated"><a class="ae lj" href="https://medium.com/@rohanwadhawan" rel="noopener"> <em class="lk">跟随我</em> </a> <em class="lk">在我开发AI研究心智地图的旅程中及其影响，在</em><a class="ae lj" href="https://www.rohanwadhawan.com/" rel="noopener ugc nofollow" target="_blank"><em class="lk">【www.rohanwadhawan.com】</em></a><em class="lk">了解我更多，在</em><a class="ae lj" href="https://www.linkedin.com/in/rohan-wadhawan/" rel="noopener ugc nofollow" target="_blank"><em class="lk">LinkedIn</em></a><em class="lk">联系我！</em></p></div></div>    
</body>
</html>