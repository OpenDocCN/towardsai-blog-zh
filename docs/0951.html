<html>
<head>
<title>Reconstruct Photorealistic Scenes from Tourists’ Public Photos on the Internet!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从游客在网上公开的照片中重建出真实感场景！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/reconstruct-photorealistic-scenes-from-tourists-public-photos-on-the-internet-bb9ad39c96f3?source=collection_archive---------0-----------------------#2020-09-20">https://pub.towardsai.net/reconstruct-photorealistic-scenes-from-tourists-public-photos-on-the-internet-bb9ad39c96f3?source=collection_archive---------0-----------------------#2020-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="dd38" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div class="gh gi jz"><img src="../Images/9057be30085b79ebefba337c548adf0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*4H0N7VKyqb-k2nHgMIBcXA.gif"/></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk translated">使用https://research.cs.cornell.edu/crowdplenoptic/<a class="ae kk" href="https://research.cs.cornell.edu/crowdplenoptic/" rel="noopener ugc nofollow" target="_blank">制作的场景</a></figcaption></figure><p id="09d1" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">使用游客从互联网上公开的照片，他们能够重建一个场景的多个视点，保存真实的阴影和照明！<br/>这是真实感场景渲染技术的巨大进步，他们的效果简直令人惊叹。<br/>让我们看看他们是如何做到的，以及更多的例子。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><h2 id="4c25" class="lq lr it bd ls lt lu dn lv lw lx dp ly kw lz ma mb la mc md me le mf mg mh iz bi translated">论文简介</h2><figure class="mj mk ml mm gt kd gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/bf310476cb7622a341c91d5dc72a0dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*ICi3mGpYRLhFrB3dkKgdSw.png"/></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk translated"><a class="ae kk" href="https://research.cs.cornell.edu/crowdplenoptic/templates/comparison_i2i.html" rel="noopener ugc nofollow" target="_blank">https://research . cs . Cornell . edu/crowd plen optic/templates/comparison _ i2i . html</a></figcaption></figure><p id="f82e" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">康奈尔大学的研究人员介绍了一种新的方法，使用游客拍摄的在线公开照片来构建一组连续的光场，并合成捕捉全天场景外观的新颖视图。这项任务的复杂性在于，所有的照片都是在一天的不同时间、不同季节和不同方位拍摄的。为了回答这个问题，他们引入了DeepMPI。这是一种新的多平面图像表示法，正好满足了他们的需求。他们的方法是完全无人监管的，除了来自互联网的照片本身之外，不需要任何信息，并允许实时合成在空间和光照上都连续的照片级真实感视图。</p><div class="mj mk ml mm gt ab cb"><figure class="mn kd mo mp mq mr ms paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><img src="../Images/d64167164afc218e290db4644d0d1270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*olJ6VRnrW5MgbUEl83ajrg.gif"/></div></figure><figure class="mn kd mo mp mq mr ms paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><img src="../Images/d87fb4420f1603d6859297a0a5effe3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*nVZYKul44Q5cRWAYdqibnQ.gif"/></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk mx di my mz translated">左，最先进的弹药方法。对，他们的。场景来自<a class="ae kk" href="https://research.cs.cornell.edu/crowdplenoptic/templates/comparison_i2i.html" rel="noopener ugc nofollow" target="_blank">https://research . cs . Cornell . edu/crowd plen optic/templates/comparison _ i2i . html</a></figcaption></figure></div><p id="9f4d" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">你可以看到他们的结果比以前最先进的模型好得多。</p><figure class="mj mk ml mm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi na"><img src="../Images/707adc1592ac1b572e5a7c2626039bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2lEmwdjSlRG5ISnMd_5BhQ.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自<a class="ae kk" href="https://research.cs.cornell.edu/crowdplenoptic/templates/comparison_i2i.html" rel="noopener ugc nofollow" target="_blank">https://research . cs . Cornell . edu/crowd plen optic/templates/comparison _ i2i . html</a></figcaption></figure><p id="0f34" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">既然我们已经介绍了他们所做的事情以及为什么会如此令人印象深刻，那么让我们来看看他们是如何实现的以及更多的成果。</p><p id="1b13" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">简而言之，他们通过使用来自多个照明和角度源的互联网的图片，合成具有连续观看条件(如照明)的场景的任意视图。<br/>它拍摄特定地点的非结构化互联网照片，并学习如何重建尊重现实世界阴影物理的光场表示。</p><div class="mj mk ml mm gt ab cb"><figure class="mn kd mo mp mq mr ms paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><img src="../Images/d64167164afc218e290db4644d0d1270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*olJ6VRnrW5MgbUEl83ajrg.gif"/></div></figure><figure class="mn kd mo mp mq mr ms paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><img src="../Images/d87fb4420f1603d6859297a0a5effe3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*nVZYKul44Q5cRWAYdqibnQ.gif"/></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk mx di my mz translated">左，最先进的弹药方法。对，他们的。场景来自<a class="ae kk" href="https://research.cs.cornell.edu/crowdplenoptic/templates/comparison_i2i.html" rel="noopener ugc nofollow" target="_blank">https://research . cs . Cornell . edu/crowd plen optic/templates/comparison _ i2i . html</a></figcaption></figure></div><p id="16b6" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">正如你刚才看到的，之前作品的光场通过场景是不一致的，这是论文最大的贡献。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><h2 id="7856" class="lq lr it bd ls lt lu dn lv lw lx dp ly kw lz ma mb la mc md me le mf mg mh iz bi translated">他们是如何做到的？</h2><p id="2c1a" class="pw-post-body-paragraph kl km it kn b ko nb kq kr ks nc ku kv kw nd ky kz la ne lc ld le nf lg lh li im bi translated">这是通过两阶段模型的架构实现的。</p><figure class="mj mk ml mm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ng"><img src="../Images/d6479b3655e7d345b1111e92c271d2b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-2SxkjTYz0CZ2YTPcDI_g.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自<a class="ae kk" href="https://research.cs.cornell.edu/crowdplenoptic/templates/comparison_i2i.html" rel="noopener ugc nofollow" target="_blank">https://research . cs . Cornell . edu/crowd plen optic/templates/comparison _ i2i . html</a></figcaption></figure><p id="92b9" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">首先，他们使用新的DeepMPI表示。他们首先将每个图像重新投影到参考视点，并在每个深度平面上对所有这些重新投影的图像进行平均，从而创建平均RGB平面扫描体积(PSV ),这是一组在给定范围内具有视差的扭曲视图。由于这意味着RGB PSV无法准确地模拟参考视图中被遮挡的场景内容，因此他们引入了网络的第二阶段。</p><p id="8722" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">第二部分使用编码器和渲染网络优化了DeepMPI表示中的潜在特性。它能够捕捉并重新呈现随时间变化的外观。</p><p id="3fef" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">编码器的作用是从样本图像和辅助深度缓冲区产生“外观向量”，包含场景的语义和深度信息。深度缓冲器允许编码器通过使用在DeepMPI表示中编码的场景内在属性来对准样本图像中的照明信息，从而学习复杂的外观。如果没有这种比对，结果将和我们之前看到的一样不一致。这种对齐的深度缓冲是渲染场景中真实阴影和照明的主要原因。</p><figure class="mj mk ml mm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ng"><img src="../Images/d6479b3655e7d345b1111e92c271d2b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-2SxkjTYz0CZ2YTPcDI_g.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自<a class="ae kk" href="https://research.cs.cornell.edu/crowdplenoptic/templates/comparison_i2i.html" rel="noopener ugc nofollow" target="_blank">https://research . cs . Cornell . edu/crowd plen optic/templates/comparison _ i2i . html</a></figcaption></figure><p id="fa00" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">然后，在该模型的架构中由G表示的渲染网络获取投影到特定目标视点的DeepMPI及其从编码器产生的外观向量，并预测相应的RGB颜色层。</p><figure class="mj mk ml mm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nh"><img src="../Images/14af65911a77fa5e1bb611282e71c602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M72h-T9vTWq5l_tRmBgo3w.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk translated">摘自阿丹的结果<a class="ae kk" href="https://arxiv.org/abs/1703.06868" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1703.06868</a></figcaption></figure><p id="d4a7" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">该渲染网络是U-Net架构的变体，具有称为AdaIN的编码器-解码器架构，用于风格传递应用。该模型在稳定训练的同时产生自然的场景外观。<br/>保留样本图像的颜色和风格。为了获得更多信息，我在本文末尾链接了AdaIN的架构论文。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><h2 id="ce87" class="lq lr it bd ls lt lu dn lv lw lx dp ly kw lz ma mb la mc md me le mf mg mh iz bi translated">结果</h2><p id="95df" class="pw-post-body-paragraph kl km it kn b ko nb kq kr ks nc ku kv kw nd ky kz la ne lc ld le nf lg lh li im bi translated">简而言之，给定一张特定的样本照片，他们能够合成接近参考视点的新视图，同时保留样本的外观。这是惊人的准确，只需花一分钟来看看他们在这个短视频中使用多种照明获得的结果:</p><figure class="mj mk ml mm gt kd"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="7718" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">项目网站链接如下，代码和数据集将很快由作者提供。</p><p id="ee82" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">当然，这只是对这项新技术的简单概述。我强烈推荐阅读下面链接的文章以获取更多信息。</p><blockquote class="nk nl nm"><p id="ae8b" class="kl km nn kn b ko kp kq kr ks kt ku kv no kx ky kz np lb lc ld nq lf lg lh li im bi translated"><strong class="kn jd">项目页面、代码和论文</strong>:<a class="ae kk" href="https://research.cs.cornell.edu/crowdplenoptic/" rel="noopener ugc nofollow" target="_blank">https://research.cs.cornell.edu/crowdplenoptic/</a><br/><strong class="kn jd">阿丹的架构论文</strong>:<a class="ae kk" href="https://arxiv.org/abs/1703.06868" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1703.06868</a></p></blockquote></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><p id="f1ac" class="pw-post-body-paragraph kl km it kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li im bi translated">如果你喜欢我的工作并想支持我，我会非常感谢你在我的社交媒体频道上关注我:</p><ul class=""><li id="564e" class="nr ns it kn b ko kp ks kt kw nt la nu le nv li nw nx ny nz bi translated">支持我的最好方式就是在<a class="ae kk" href="https://medium.com/@whats_ai" rel="noopener"> <strong class="kn jd">中</strong> </a>关注我。</li><li id="4bab" class="nr ns it kn b ko oa ks ob kw oc la od le oe li nw nx ny nz bi translated">订阅我的<a class="ae kk" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"> <strong class="kn jd"> YouTube频道</strong> </a>。</li><li id="b299" class="nr ns it kn b ko oa ks ob kw oc la od le oe li nw nx ny nz bi translated">在<a class="ae kk" href="https://www.linkedin.com/company/what-is-artificial-intelligence" rel="noopener ugc nofollow" target="_blank"> <strong class="kn jd"> LinkedIn </strong> </a>上关注我的项目</li><li id="75e9" class="nr ns it kn b ko oa ks ob kw oc la od le oe li nw nx ny nz bi translated">一起学AI，加入我们的<a class="ae kk" href="https://discord.gg/SVse4Sr" rel="noopener ugc nofollow" target="_blank"> <strong class="kn jd">不和谐社区</strong> </a>，<em class="nn">分享你的项目、论文、最佳课程、寻找kaggle队友等等！</em></li></ul></div></div>    
</body>
</html>