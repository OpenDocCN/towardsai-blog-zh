<html>
<head>
<title>This Open Source Framework was Created by LinkedIn to Simplify the Interoperability Between TensorFlow and Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这个开源框架由LinkedIn创建，旨在简化TensorFlow和Spark之间的互操作性</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/this-open-source-framework-was-created-by-linkedin-to-simplify-the-interoperability-between-e4f327f24442?source=collection_archive---------3-----------------------#2021-06-01">https://pub.towardsai.net/this-open-source-framework-was-created-by-linkedin-to-simplify-the-interoperability-between-e4f327f24442?source=collection_archive---------3-----------------------#2021-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ae0c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="ee55" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Spark-TFRecord支持在Apache Spark中处理TensorFlow的TFRecord结构。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3820013fdd88d0f41495559c0ce5ce38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DjZAzYMSXwMMnicq.jpg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://www.kdnuggets.com/2020/05/linkedin-open-sources-small-component-tensorflow-spark-interoperability.html" rel="noopener ugc nofollow" target="_blank"> K </a> DNuggets</figcaption></figure><blockquote class="li lj lk"><p id="3442" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过80，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到85，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="06d3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">TensorFlow和Apache Spark的互操作是现实世界机器学习场景中的常见挑战。TensorFlow可以说是市场上最受欢迎的深度学习框架，而Apache Spark仍然是最广泛采用的数据计算平台之一，在大型企业和初创公司中有大量安装。很自然，公司会尝试将两者结合起来。虽然有一些框架可以使TensorFlow适应Spark，但是互操作性挑战的根源通常在数据层面。TFRecord是TensorFlow中的原生数据结构，在Apache Spark中不完全受支持。去年，LinkedIn的工程师开源了<a class="ae lh" href="https://github.com/linkedin/spark-tfrecord" rel="noopener ugc nofollow" target="_blank"> Spark-TFRecord </a>，这是Spark基于TensorFlow TFRecord的一个新的原生数据源。</p><p id="6d6a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">LinkedIn决定解决这个问题并不奇怪。这家互联网巨头长期以来一直是Spark技术的广泛采用者，并且一直是TensorFlow和机器学习开源社区的积极贡献者。在内部，LinkedIn的工程团队经常试图在TensorFlow的原生TFRecord格式和Spark的内部格式(如Avro或Parquet)之间实现转换。Spark-TFRecord项目的目标是在Spark管道中提供TFRecord结构的原生功能。</p><h1 id="3e6f" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">先前的尝试</h1><p id="3ef1" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated"><a class="ae lh" href="https://github.com/linkedin/spark-tfrecord" rel="noopener ugc nofollow" target="_blank"> Spark-TFRecord </a>并不是第一个试图解决Spark和TensorFlow之间数据互操作性挑战的项目。其中最受欢迎的项目是由Spark的创造者Databricks推广的<a class="ae lh" href="https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector" rel="noopener ugc nofollow" target="_blank">Spark-tensor flow-Connector</a>。我们已经多次使用Spark-TensorFlow-Connector，取得了不同程度的成功。从架构上来说，连接器是TFRecord格式到Spark SQL数据帧的改编。知道了这一点，Spark-TensorFlow-Connector在关系数据访问场景中非常有效，但在其他用例中仍然非常有限就不足为奇了。</p><p id="6c97" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">仔细想想，TensorFlow工作流的一个重要部分与磁盘I/O操作有关，而不是数据库访问。在这些场景中，当使用Spark-TensorFlow-Connector时，开发人员最终会编写大量的代码。此外，当前版本的Spark-TensorFlow-Connector仍然缺少重要的功能，例如在TensorFlow计算中经常使用的<em class="ln">分区</em>。最后，连接器更像是处理Spark SQL数据帧中TensorFlow记录的桥梁，而不是原生文件格式。</p><p id="69cd" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">考虑到这些限制，LinkedIn工程团队决定从稍微不同的角度解决Spark-TensorFlow互操作性挑战。</p><h1 id="4204" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">火花TFRecord</h1><p id="dcd3" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">Spark-TFRecord是Apache Spark的原生TensorFlow TFRecord。具体来说，Spark-TFRecord提供了从/向Apache Spark读取和写入TFRecord数据的例程。Spark-TFRecord不是构建一个连接器来处理TFRecord结构，而是像Avro、JSON或Parquet一样构建一个原生Spark数据集。这意味着所有Spark的DataSet和DataFrame I/O例程在Spark-TFRecord中都是自动可用的。</p><p id="9eed" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">一个显而易见的值得探讨的问题是，为什么要构建一个新的数据结构，而不是简单地对开源的Spark-TensorFlow连接器进行版本控制？看起来，使连接器适应磁盘I/O操作需要一个基本的重新设计。</p><p id="1267" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">LinkedIn工程团队没有遵循这条路线，而是决定实现一个新的<a class="ae lh" href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala" rel="noopener ugc nofollow" target="_blank"> Spark FileFormat接口</a>，它的基本设计是为了支持磁盘I/O操作。新接口将使TFRecord本机操作适应任何Spark数据帧。在体系结构上，Spark-TFRecord由一系列抽象读/写和序列化/反序列化例程的基本构建块组成:</p><p id="2d88" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><em class="ln"> </em> <strong class="lo jd"> <em class="ln">模式推理器:</em> </strong> <em class="ln">这是离Spark-TensorFlow-Connector最近的组件。该接口将TFRecords表示映射到本机Spark数据类型。</em></p><p id="7e36" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><em class="ln"/><strong class="lo jd"><em class="ln">TFRecord Reader:</em></strong><em class="ln">该组件读取TF record结构，并将它们传递给反序列化器。</em></p><p id="c099" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><em class="ln"> </em> <strong class="lo jd"> <em class="ln"> TFRecord编写器:</em> </strong> <em class="ln">该组件从序列化器接收一个TFRecord结构，并将其写入磁盘。</em></p><p id="9cff" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><em class="ln"> </em> <strong class="lo jd"> <em class="ln"> TFRecord序列化器:</em> </strong> <em class="ln">该组件将Spark InternalRow转换为TFRecord结构。</em></p><p id="e085" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><em class="ln"> </em> <strong class="lo jd"> <em class="ln"> TFRecord反序列化器:</em> </strong> <em class="ln">该组件将TFRecord转换为Spark InternalRow结构。</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/9dc011e6d1d9771d3a04f67110400fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OWFp_wLK2lHMBpie.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://engineering.linkedin.com/blog/2020/spark-tfrecord" rel="noopener ugc nofollow" target="_blank">https://engineering.linkedin.com/blog/2020/spark-tfrecord</a></figcaption></figure><p id="4fff" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">使用LinkedIn的Spark-TFRecord与其他Spark原生数据集并无不同。开发人员只需包含spark-tfrecord jar库，并使用传统的DataFrame API来读写tfrecord，如以下代码所示:</p><pre class="ks kt ku kv gt ob oc od oe aw of bi"><span id="c649" class="og ne it oc b gy oh oi l oj ok">import org.apache.commons.io.FileUtils<br/>import org.apache.spark.sql.{ DataFrame, Row }<br/>import org.apache.spark.sql.catalyst.expressions.GenericRow<br/>import org.apache.spark.sql.types._</span><span id="7a19" class="og ne it oc b gy ol oi l oj ok">val path = "test-output.tfrecord"<br/>val testRows: Array[Row] = Array(<br/>new GenericRow(Array[Any](11, 1, 23L, 10.0F, 14.0, List(1.0, 2.0), "r1")),<br/>new GenericRow(Array[Any](21, 2, 24L, 12.0F, 15.0, List(2.0, 2.0), "r2")))<br/>val schema = StructType(List(StructField("id", IntegerType),<br/>                             StructField("IntegerCol", IntegerType),<br/>                             StructField("LongCol", LongType),<br/>                             StructField("FloatCol", FloatType),<br/>                             StructField("DoubleCol", DoubleType),<br/>                             StructField("VectorCol", ArrayType(DoubleType, true)),<br/>                             StructField("StringCol", StringType)))</span><span id="464c" class="og ne it oc b gy ol oi l oj ok">val rdd = spark.sparkContext.parallelize(testRows)</span><span id="9b81" class="og ne it oc b gy ol oi l oj ok">//Save DataFrame as TFRecords<br/>val df: DataFrame = spark.createDataFrame(rdd, schema)<br/>df.write.format("tfrecord").option("recordType", "Example").save(path)</span><span id="c122" class="og ne it oc b gy ol oi l oj ok">//Read TFRecords into DataFrame.<br/>//The DataFrame schema is inferred from the TFRecords if no custom schema is provided.<br/>val importedDf1: DataFrame = spark.read.format("tfrecord").option("recordType", "Example").load(path)<br/>importedDf1.show()</span><span id="8942" class="og ne it oc b gy ol oi l oj ok">//Read TFRecords into DataFrame using custom schema<br/>val importedDf2: DataFrame = spark.read.format("tfrecord").schema(schema).load(path)<br/>importedDf2.show()</span></pre><p id="f277" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">Spark和深度学习框架(如TensorFlow)之间的互操作性可能会继续成为大多数组织面临的一个挑战。然而，像LinkedIn的Spark-TFRecord这样经过大规模测试的项目肯定有助于简化这两种技术之间的桥梁，而这两种技术对许多现代机器学习架构来说是必不可少的。</p></div></div>    
</body>
</html>