<html>
<head>
<title>Natural Language Clustering — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言聚类—第1部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/natural-language-clustering-part-1-32301e3125e5?source=collection_archive---------2-----------------------#2021-03-13">https://pub.towardsai.net/natural-language-clustering-part-1-32301e3125e5?source=collection_archive---------2-----------------------#2021-03-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5ddc" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="61d7" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">帮助聊天机器人处理常见问题，使用Python代码进行标记化，GloVe和TF-IDF</h2></div><p id="93cc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对事物进行分类对我们来说很自然:我们的书籍、电影和音乐都有流派；我们学习的东西被分成不同的科目，甚至我们吃的食物也属于不同的菜系！<br/>近年来，我们已经能够开发出越来越好的算法来对文本进行分类:像伯特-ITPT-FiT (BERT +任务内预训练+微调)或XL-NET这样的模型似乎是这一领域的卫冕冠军，至少在PapersWithCode上提供的<a class="ae lk" href="https://paperswithcode.com/task/text-classification" rel="noopener ugc nofollow" target="_blank"> 29个基准数据集</a>中是如此。</p><p id="d701" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">近年来，我们已经能够开发出越来越好的算法来对文本进行分类:像伯特-ITPT-FiT (BERT +任务内预训练+微调)或XL-NET这样的模型似乎是这一类别中的卫冕冠军，至少在PapersWithCode上提供的<a class="ae lk" href="https://paperswithcode.com/task/text-classification" rel="noopener ugc nofollow" target="_blank"> 29个基准数据集</a>中是如此。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/4d2ccdd0d4b87fa1e875424ead047d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zcrR9TOBHTmY_JEqanRcvA.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">作者图片</figcaption></figure><p id="d42f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是如果我们不知道我们想要分析的文本的可用类别呢？例如，一组对话或一组书籍或文章都属于同一主题的不同专业:标签并不总是像垃圾邮件/非垃圾邮件一样清晰，我们可能不知道会有多少或什么类型的标签，或者正常的预先训练的分类方法不具备将它们分类为相同类别所需的深入领域知识，同时没有足够的材料、时间或计算能力可用于<a class="ae lk" href="https://huggingface.co/transformers/custom_datasets.html" rel="noopener ugc nofollow" target="_blank">微调</a>转换模型。</p><p id="6044" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">也许最明显的例子之一是将收到的消息分类到FAQ中:不是每条消息都可以通过FAQ得到回答，但是我们希望将尽可能多的正确答案关联起来，以最小化我们留下的未回答或手动解决的数量，并动态地改进我们的FAQ，添加变得相关的新问题。</p><p id="aac7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们看看如何！</p><h1 id="0d64" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">从单词到数值:标记化和嵌入</h1><p id="827b" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">我们要做的第一件事，就像处理自然语言时一样，是将文本转换成我们的算法可以理解的东西。段落和句子在一个叫做记号化的过程中被分解成它们的基本成分，单词。但是单词本身不能被输入计算模型，因为数学运算不能应用于它们，这解释了将它们映射到实数向量的需要:基本上转换成在数学空间中表示它们的数字序列，还保留它们之间的关系(这个过程被称为<a class="ae lk" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">单词嵌入</a>，这个术语来自<a class="ae lk" href="https://en.wikipedia.org/wiki/Embedding" rel="noopener ugc nofollow" target="_blank">数学</a>以识别内射和结构保留映射)。</p><p id="6f5c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这听起来很复杂，但有一些简单的方法可以做到这一点，如下所示:从三个句子(左侧)开始，提取它们的标记，并通过计算每个标记(单词)在原始文本中的出现次数来执行非常简单的嵌入(右侧)</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi my"><img src="../Images/cf9250c9f59dba0bc52f71208e91e72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gchn9mzJKXS9Q0J1X3BEsg.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">三篇短文本的标记化和简单计数向量嵌入。</figcaption></figure><p id="8222" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在上面的示例中，单词startup被嵌入为[1，0，1]，因为它在第一个和第三个文本中出现过一次，但在第二个文本中没有出现，而单词“In”被嵌入为[0，2，0]，因为它只在第二个句子中出现了两次。</p><p id="9d47" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请注意，这不是一个非常实用的示例:由于输入数量很少，不同的单词最终具有相同的嵌入(例如，“specialized”和“in”都只出现在第二个文本中，因此它们共享相同的嵌入[0，1，0])，但我们希望它能帮助您更好地理解该过程是如何工作的。</p><p id="180b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">还要注意，记号都是小写的，标点符号被去掉了:这是常见的做法，尽管这取决于我们要处理的文本类型。</p><p id="ccf7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上例中没有显示的另一个常见做法是删除停用词，这些词太常见了，与我们的目的不相关，而且很可能在我们遇到的大多数文本中大量出现(如“a”、“the”或“is”)。</p><p id="ac01" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，这种方法提供了在嵌入单词的同时嵌入文档本身的优势:垂直阅读表格，Text1可以读作[1，1，1，1，1，0，0，0]。这使得通过引入距离度量来面对不同的源变得容易。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h2 id="f64d" class="ng mc iq bd md nh ni dn mh nj nk dp ml kx nl nm mn lb nn no mp lf np nq mr iw bi translated">使用nltk的Python中的标记化代码示例:</h2><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="f54c" class="ng mc iq ns b gy nw nx l ny nz"># !pip install nltk</span><span id="e8e6" class="ng mc iq ns b gy oa nx l ny nz">from nltk.tokenize import word_tokenize</span><span id="a67c" class="ng mc iq ns b gy oa nx l ny nz">text = "Digitiamo is a Startup from Italy"</span><span id="0288" class="ng mc iq ns b gy oa nx l ny nz">tokenized_text = nltk.word_tokenize(text)</span><span id="cf36" class="ng mc iq ns b gy oa nx l ny nz">print(tokenized_text)</span><span id="fb4b" class="ng mc iq ns b gy oa nx l ny nz"># OUTPUT:<br/># ['Digitiamo', 'is', 'a', 'Startup', 'from', 'Italy']</span></pre><h2 id="87e4" class="ng mc iq bd md nh ni dn mh nj nk dp ml kx nl nm mn lb nn no mp lf np nq mr iw bi translated">使用gensim的Word2Vec在Python中嵌入代码示例</h2><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="1b1c" class="ng mc iq ns b gy nw nx l ny nz"># !pip install --upgrade gensim</span><span id="7769" class="ng mc iq ns b gy oa nx l ny nz">from gensim.models import Word2Vec<br/>model = Word2Vec([tokenized_text], min_count=1)<br/>"""<br/> <strong class="ns ja">min_count</strong> (<em class="ob">int, optional</em>) – Ignores all words with total frequency lower than this.</span><span id="1078" class="ng mc iq ns b gy oa nx l ny nz">Word2Vec is optimized to work with multiple texts at the same time in the form of a list of texts. When working with a single text, it should be put into a list, which explains the square brackets around tokenized_text <br/>"""</span><span id="2566" class="ng mc iq ns b gy oa nx l ny nz">print(list(model.wv.vocab))</span><span id="4d48" class="ng mc iq ns b gy oa nx l ny nz"># OUTPUT:<br/># ['Digitiamo', 'is', 'a', 'Startup', 'from', 'Italy']</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oc"><img src="../Images/34b7943b0c4ea29cff91ec76edc60ed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fg94NFzdMGFngKjYS1Y-TQ.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">显示单词的嵌入。Word2Vec的默认参数使用100维嵌入</figcaption></figure></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="c3b3" class="mb mc iq bd md me od mg mh mi oe mk ml kf of kg mn ki og kj mp kl oh km mr ms bi translated">常用嵌入:TF-IDF和GloVe</h1><ul class=""><li id="cabb" class="oi oj iq kq b kr mt ku mu kx ok lb ol lf om lj on oo op oq bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> TF-IDF </strong> </a>代表<strong class="kq ja">T</strong>erm<strong class="kq ja">F</strong>frequency—<strong class="kq ja">I</strong>n reverse<strong class="kq ja">D</strong>document<strong class="kq ja">F</strong>frequency，它根据每个术语在语料库的其他文本中出现的次数来强调其重要性。出于集群化的目的，将文本嵌入专门的领域是非常有用的。出现在每个文档中的单词，无论它们在日常使用中多么不频繁，都将被赋予很小的权重，而只出现在某些文档中的单词将被赋予更大的权重，从而更容易识别可能的聚类。<br/>例如，如果我们试图嵌入一堆关于内分泌学的论文，单词“endocrinology”本身可能会在每篇论文中被多次提及，这使得它的存在对于区分它们的目的没有太大帮助。</li><li id="3608" class="oi oj iq kq b kr or ku os kx ot lb ou lf ov lj on oo op oq bi translated"><a class="ae lk" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja">GloVe</strong></a>(<strong class="kq ja">Glo</strong>bal<strong class="kq ja">Ve</strong>ctors for word representation)是一种无监督算法，它根据单词出现的频率来链接单词，试图映射潜在的含义。你可以在下面的图片中看到一些通过GloVe获得的单词之间关系的例子:“男人”与“女人”的距离大约与“国王”与“王后”的距离相同，因为它们对不同性别具有相同的含义(当然反之亦然)。同样的情况也发生在第四个面板中的比较级和最高级上:将你从“清晰”带到“更清晰”的向量，如果应用于“柔和”，会将你带到“更柔和”。<br/>根据你的语料库的大小，<a class="ae lk" href="https://github.com/roamanalytics/mittens" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja">Mittens</strong>library</a>可能是GloVe的一个很好的补充:根据他们的GitHub，它“对于需要专门表示但缺乏足够数据来从头训练它们的领域很有用。Mittens从通用的预训练表示开始，并将它们调整到专门的领域”，它也与Numpy和TensorFlow兼容。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ow"><img src="../Images/9563614ab559396b3e00d16ecaefb313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LbJ51zKoWxNF47fCeEFaw.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">(图片来自官方手套来源:<a class="ae lk" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>)</figcaption></figure><p id="7242" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">太棒了。既然我们已经将单词转换成数字向量，并将它们映射到空间上，如果我们使用的嵌入只适用于单词而不适用于全文，那么是时候回到句子了:还记得我们最初的目标吗？我们不只是想映射单词，而是试图将它们所在的文本或句子聚集在一起。</p><p id="5ba1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有许多方法可以做到这一点，根据具体情况，有些方法可能比其他方法更有效。也许最简单的方法，也是对短文本非常有效的方法，是对文本中每个单词的嵌入进行平均，以生成整个文本的嵌入。</p><p id="fc9c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于较长的材料，另一种方法是提取关键字，然后对这些关键字的嵌入进行平均，以确保您的平均值不会被无数的常见单词稀释，但这需要一种关键字提取算法，该算法可能并不总是可用。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h2 id="2ede" class="ng mc iq bd md nh ni dn mh nj nk dp ml kx nl nm mn lb nn no mp lf np nq mr iw bi translated">使用sklearn的Python中的Tf-Idf代码示例</h2><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="11fa" class="ng mc iq ns b gy nw nx l ny nz">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="05aa" class="ng mc iq ns b gy oa nx l ny nz">corpus = [<br/>    'Digitiamo is a Startup from Italy',<br/>    'Digitiamo is specialized in NLP in Italy',<br/>    'Italy is not a Startup'<br/>]</span><span id="38c4" class="ng mc iq ns b gy oa nx l ny nz">vectorizer = TfidfVectorizer()</span><span id="3523" class="ng mc iq ns b gy oa nx l ny nz">X = vectorizer.fit_transform(corpus)</span><span id="33f9" class="ng mc iq ns b gy oa nx l ny nz">print(vectorizer.get_feature_names())<br/>print(X.shape)</span><span id="0cec" class="ng mc iq ns b gy oa nx l ny nz"># OUTPUT: <br/># ['digitiamo', 'from', 'in', 'is', 'italy', 'nlp', 'not',  'specialized', 'startup']<br/># (3, 9)</span><span id="55f6" class="ng mc iq ns b gy oa nx l ny nz"># get_feature_names returns the dictionary of words, whereas X.shape # is 3x9: 3 documents, 9 different words. The word ‘a’ has been stopped.</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ox"><img src="../Images/2ec05d946759356bd52defe5301ddbf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-cZV660mAjr9eNdx0pUzjw.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">得到的训练模型。Word是倒着打印的，可以在上面的get_feature_names列表中看到，所以first row (0，4)表示第一个文本的最后一个单词，也就是列表中的第五个(4+1，我们从零开始数):“italy”。(0，1)是“从”；(0，8)“启动”等等。</figcaption></figure></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h2 id="ae4d" class="ng mc iq bd md nh ni dn mh nj nk dp ml kx nl nm mn lb nn no mp lf np nq mr iw bi translated">在Python中使用手套(<a class="ae lk" href="https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python" rel="noopener ugc nofollow" target="_blank">来源</a>)</h2><p id="8080" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">一旦你从<a class="ae lk" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">官方来源</a>下载了一个手套嵌入，你就可以如下所示使用它(归功于<a class="ae lk" href="https://stackoverflow.com/users/2857133/karishma-malkan" rel="noopener ugc nofollow" target="_blank">卡里什马·马尔坎</a></p><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="36fd" class="ng mc iq ns b gy nw nx l ny nz">import numpy as np<br/><br/>def loadGloveModel(File):<br/>    print("Loading Glove Model")<br/>    f = open(File,'r')<br/>    gloveModel = {}<br/>    for line in f:<br/>        splitLines = line.split()<br/>        word = splitLines[0]<br/>        wordEmbedding = np.array([float(value) for value in splitLines[1:]])<br/>        gloveModel[word] = wordEmbedding<br/>    print(len(gloveModel)," words loaded!")<br/>    return gloveModel</span></pre><p id="7b2f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，您可以通过简单地使用gloveModel变量来访问单词vectors。</p><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="cbe8" class="ng mc iq ns b gy nw nx l ny nz">print gloveModel['hello']</span></pre></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="46fe" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">或者，您可以使用如下所示的<strong class="kq ja"> Pandas </strong>加载文件(归功于<a class="ae lk" href="https://stackoverflow.com/users/116186/petter" rel="noopener ugc nofollow" target="_blank"> Petter </a>)</p><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="e32d" class="ng mc iq ns b gy nw nx l ny nz">import pandas as pd<br/>import csv</span><span id="6ecd" class="ng mc iq ns b gy oa nx l ny nz">words = pd.read_table(glove_data_file, sep=" ", index_col=0, header=None, quoting=csv.QUOTE_NONE)</span></pre><p id="1f7d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后得到一个单词的向量:</p><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="f372" class="ng mc iq ns b gy nw nx l ny nz">def vec(w):<br/>  return words.loc[w].as_matrix()</span></pre><p id="f42d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了找到最接近矢量的单词:</p><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="ff79" class="ng mc iq ns b gy nw nx l ny nz">words_matrix = words.as_matrix()<br/><br/>def find_closest_word(v):<br/>  diff = words_matrix - v<br/>  delta = np.sum(diff * diff, axis=1)<br/>  i = np.argmin(delta)<br/>  return words.iloc[i].name</span></pre></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="2bb9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该文件也可以使用如下所示的<strong class="kq ja"> gensim </strong>打开(归功于<a class="ae lk" href="https://stackoverflow.com/users/8514366/ben" rel="noopener ugc nofollow" target="_blank"> Ben </a>)，这可能是最好的方法，因为它与我们在本文中之前看到的其他方法一致，并且允许您使用gensim <a class="ae lk" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> word2vec </a>方法(例如，相似性)</p><p id="cbc6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用<a class="ae lk" href="https://radimrehurek.com/gensim/scripts/glove2word2vec.html" rel="noopener ugc nofollow" target="_blank"> glove2word2vec </a>将文本格式的手套向量转换为word2vec文本格式:</p><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="1d16" class="ng mc iq ns b gy nw nx l ny nz">from gensim.scripts.glove2word2vec import glove2word2vec<br/>glove2word2vec(glove_input_file="vectors.txt", word2vec_output_file="gensim_glove_vectors.txt")</span></pre><p id="7b9b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，使用<a class="ae lk" href="https://radimrehurek.com/gensim/models/keyedvectors.html" rel="noopener ugc nofollow" target="_blank"> KeyedVectors </a>将word2vec txt读取到gensim模型中:</p><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="1554" class="ng mc iq ns b gy nw nx l ny nz">from gensim.models.keyedvectors import KeyedVectors<br/>glove_model = KeyedVectors.load_word2vec_format("gensim_glove_vectors.txt", binary=False)</span></pre></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h2 id="e113" class="ng mc iq bd md nh ni dn mh nj nk dp ml kx nl nm mn lb nn no mp lf np nq mr iw bi translated">平均单词嵌入以获得文本嵌入</h2><pre class="lm ln lo lp gt nr ns nt nu aw nv bi"><span id="9a9d" class="ng mc iq ns b gy nw nx l ny nz">import re<br/>from gensim.models import Word2Vec</span><span id="6cc7" class="ng mc iq ns b gy oa nx l ny nz">text = 'Digitiamo is a Startup from Italy'</span><span id="f73f" class="ng mc iq ns b gy oa nx l ny nz">tokenized_text = nltk.word_tokenize(text)</span><span id="3370" class="ng mc iq ns b gy oa nx l ny nz">model = Word2Vec([tokenized_text], min_count=1)</span><span id="0e89" class="ng mc iq ns b gy oa nx l ny nz">modelledText = []<br/>for word in text.split():<br/>    word = re.sub(r'[^\w\s]','',word)<br/>    # remove punctuation<br/>    modelledText.append(model[word])</span><span id="3c96" class="ng mc iq ns b gy oa nx l ny nz">embedText = sum(modelledText) / len(modelledText)<br/># The embeddings are vectors, so the can be added and divided</span><span id="5cdc" class="ng mc iq ns b gy oa nx l ny nz">print(embedText)</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oy"><img src="../Images/4ebc635d0c6788f2c3c784e7262cbc97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8pDqbh841Xwh3k572ynbxg.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">整个句子的结果(平均)嵌入向量</figcaption></figure></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="8fb4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在我们终于得到了文本的嵌入，我们可以将它们聚集起来，就像它们是普通的数字数据条目一样！</p><p id="79c4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">要了解关于一些可用的集群技术的更多信息，请阅读本文的第二部分，即将推出！如果你想在它出版时得到通知，通过滑动下面的工具订阅我们的时事通讯(就像iPhone解锁一样)</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="oz pa l"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">想了解我们最新发布的产品吗？上面订阅！别担心，我们会谨慎使用:)</figcaption></figure><p id="8ec1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通过利用和完善本文中解释的技术，我们开发了<a class="ae lk" href="https://it.digitiamo.com/aiknowyou" rel="noopener ugc nofollow" target="_blank"> AiKnowYou </a>，这是一款分析和改进聊天机器人性能的产品。如果你想更好地了解它是如何工作的，或者如果你想提高你自己的聊天机器人的性能，请随时联系我们</p><p id="f888" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="ob">感谢您的阅读！</em></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/265d4476ea141a0364007cf4a2a8cef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*g_28_A_8jCH-eUqPI4BBpQ.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk translated">Digitiamo徽标</figcaption></figure><h2 id="e2e7" class="ng mc iq bd md nh ni dn mh nj nk dp ml kx nl nm mn lb nn no mp lf np nq mr iw bi translated">关于Digitiamo</h2><p id="4110" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">Digitiamo是一家来自意大利的初创公司，专注于使用人工智能来帮助公司管理和利用他们的知识。要了解更多信息，<a class="ae lk" href="https://www.digitiamo.com/" rel="noopener ugc nofollow" target="_blank">请访问我们的</a>。</p><h2 id="4119" class="ng mc iq bd md nh ni dn mh nj nk dp ml kx nl nm mn lb nn no mp lf np nq mr iw bi translated">关于作者</h2><p id="da31" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated"><a class="ae lk" href="https://medium.com/u/56f43ec01c1e?source=post_page-----576ed5f7988b--------------------------------" rel="noopener"> <em class="ob">法比奥·丘萨诺</em> </a> <em class="ob">是</em><a class="ae lk" href="https://www.digitiamo.com/" rel="noopener ugc nofollow" target="_blank"><em class="ob">Digitiamo</em></a><em class="ob">的数据科学负责人；</em> <a class="ae lk" href="https://medium.com/u/ec9f76d504e0?source=post_page-----576ed5f7988b--------------------------------" rel="noopener"> <em class="ob">弗朗切斯科·福马加利</em></a>T22】是一名有抱负的数据科学家，正在做R &amp; D实习。</p></div></div>    
</body>
</html>