<html>
<head>
<title>Top 10 Computer Vision Papers of 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年十大计算机视觉论文</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/top-10-computer-vision-papers-of-2021-3ce1ed2e6f59?source=collection_archive---------0-----------------------#2022-01-04">https://pub.towardsai.net/top-10-computer-vision-papers-of-2021-3ce1ed2e6f59?source=collection_archive---------0-----------------------#2022-01-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="48b2" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="b4ca" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">2021年十大计算机视觉论文，包括视频演示、文章、代码和论文参考。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/88efe45727965ee085d5e9d219ad47b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c01QsZN_ip2H8mhREOmCaQ.png"/></div></div></figure><p id="7914" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">虽然世界仍在复苏，但研究并没有放缓其疯狂的步伐，尤其是在人工智能领域。此外，今年还强调了许多重要方面，如道德方面、重要偏见、治理、透明度等等。人工智能和我们对人脑及其与人工智能的联系的理解正在不断发展，显示出在不久的将来改善我们生活质量的有前途的应用。然而，我们应该小心选择应用哪种技术。</p><blockquote class="lz ma mb"><p id="55f0" class="ld le mc lf b lg lh kd li lj lk kg ll md ln lo lp me lr ls lt mf lv lw lx ly im bi translated">"科学不能告诉我们应该做什么，只能告诉我们能做什么。"<br/><em class="it">——萨特《存在与虚无》</em></p></blockquote><p id="f8d9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这里是我今年在计算机视觉领域最有趣的10篇研究论文，以防你错过其中的任何一篇。简而言之，它基本上是人工智能和CV的最新突破的精选列表，有清晰的视频解释，更深入的文章链接和代码(如果适用)。享受阅读，如果我错过了任何重要的论文，请在评论中告诉我，或者直接在LinkedIn上联系我！</p><p id="9e9e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">本文末尾列出了每篇论文的完整参考资料。</p><p id="9019" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">订阅我的<a class="ae mg" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">时事通讯</strong></a>—AI的最新更新每周都有解释，如果我错过了什么有趣的论文，请随时<a class="ae mg" href="https://www.louisbouchard.ai/contact/" rel="noopener ugc nofollow" target="_blank">给我发消息</a>！</p><p id="0c8c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mc">在</em> <strong class="lf jd"> <em class="mc">上给我加标签Twitter</em></strong><em class="mc"/><a class="ae mg" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"><em class="mc">@ Whats _ AI</em></a><em class="mc">或</em><strong class="lf jd"><em class="mc">LinkedIn</em></strong><em class="mc"/><a class="ae mg" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"><em class="mc">@ Louis(什么是AI) Bouchard </em> </a> <em class="mc">如果分享一下名单！</em></p><h2 id="3a2c" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">观看2021 CV倒带</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="520a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">去年错过了？看看这个:<a class="ae mg" href="https://github.com/louisfb01/best_AI_papers_2021" rel="noopener ugc nofollow" target="_blank"> 2020年:充满令人惊叹的人工智能论文的一年——一篇评论</a></p><p id="2331" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">👀如果你想支持我的工作，并使用W&amp;B(免费)来跟踪你的ML实验，使你的工作可重复或与团队合作，你可以通过遵循<a class="ae mg" href="https://colab.research.google.com/github/louisfb01/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb" rel="noopener ugc nofollow" target="_blank">这个指南</a>来尝试一下！由于这里的大部分代码都是基于PyTorch的，我们认为分享一个在PyTorch上使用W &amp; B的<a class="ae mg" href="https://colab.research.google.com/github/louisfb01/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb" rel="noopener ugc nofollow" target="_blank">快速入门指南</a>会很有趣。</p><p id="052b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">👉遵循<a class="ae mg" href="https://colab.research.google.com/github/louisfb01/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb" rel="noopener ugc nofollow" target="_blank">这个快速指南</a>，在你的代码或下面的任何回复中使用相同的W &amp; B行，让你所有的实验在你的w &amp; b账户中自动跟踪！它不需要超过5分钟的时间来设置，并将改变你的生活，就像我改变你的生活一样！<a class="ae mg" href="https://colab.research.google.com/github/louisfb01/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb" rel="noopener ugc nofollow" target="_blank">这里有一个更高级的使用超参数扫描的指南</a>，如果你感兴趣的话:)</p><p id="c720" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">🙌感谢<a class="ae mg" href="https://wandb.ai/" rel="noopener ugc nofollow" target="_blank">Weights&amp;bias</a>赞助这个库和我一直在做的工作，感谢你们中的任何人使用这个链接并尝试W &amp; B！</p><p id="21c1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae mg" href="https://github.com/louisfb01/top-10-cv-papers-2021" rel="noopener ugc nofollow" target="_blank">访问GitHub存储库中的完整列表</a></p><h1 id="759e" class="nb mi it bd mj nc nd ne mm nf ng nh mp ki ni kj ms kl nj km mv ko nk kp my nl bi translated">目录</h1><ul class=""><li id="c052" class="nm nn it lf b lg no lj np lm nq lq nr lu ns ly nt nu nv nw bi translated">DALL E:open ai的零镜头文本到图像生成[1]</li><li id="2878" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">驯服高分辨率图像合成的变压器[2]</li><li id="7b11" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">Swin Transformer:使用移位窗口的分层视觉转换器[3]</li><li id="dc73" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">深网:他们为视觉做过什么？[奖金]</li><li id="aeb6" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">无限自然:从单一图像生成自然场景的永久视图[4]</li><li id="56a2" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">完全重新照明:学习为背景替换重新照明肖像[5]</li><li id="fc33" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">用欧拉运动场制作动画[6]</li><li id="bf13" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">CVPR 2021最佳论文奖:长颈鹿——可控图像生成[7]</li><li id="a945" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">TimeLens:基于事件的视频帧插值[8]</li><li id="b190" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">(Style)CLIPDraw:文本到绘图合成中内容和样式的耦合[9]</li><li id="a29c" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">城市NeRF:城市尺度的建筑NeRF[10]</li><li id="75b1" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">论文参考</li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="5e2f" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">DALL E:open ai的零镜头文本到图像生成[1]</h2><p id="a6da" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">OpenAI成功训练了一个能够从文本字幕生成图像的网络。它与GPT 3号和GPT图像非常相似，产生了惊人的效果。</p><p id="bd4f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="e128" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/openais-dall-e-text-to-image-generation-explained-1f6fb4bb5a0a"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">OpenAI的DALL E:解释文本到图像的生成</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">OpenAI刚刚发布了解释DALL-E如何工作的论文！它被称为“零镜头文本到图像的生成”。</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="b60c" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">论文:<a class="ae mg" href="https://arxiv.org/pdf/2102.12092.pdf" rel="noopener ugc nofollow" target="_blank">零投文本转图像生成</a></li><li id="4ed5" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">代码:<a class="ae mg" href="https://github.com/openai/DALL-E" rel="noopener ugc nofollow" target="_blank">代码&amp;用于DALL E </a>的离散VAE的更多信息</li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="e531" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">驯服高分辨率图像合成的变压器[2]</h2><p id="a113" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">TL；DR:他们将GANs和卷积方法的效率与transformers的表达能力结合起来，为语义指导的高质量图像合成提供了一种强大而省时的方法。</p><p id="0acf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="0fb1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/combining-the-transformers-expressivity-with-the-cnns-efficiency-for-high-resolution-image-synthesis-31c6767547da"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">结合变形金刚的表现力和CNN的高分辨率图像的效率…</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">TL；DR:他们将GANs和卷积方法的效率与变压器的表达能力结合起来…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="ph l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="9b2d" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">论文:<a class="ae mg" href="https://compvis.github.io/taming-transformers/" rel="noopener ugc nofollow" target="_blank">驯服高分辨率图像合成的变形金刚</a></li><li id="8dfd" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">代号:<a class="ae mg" href="https://github.com/CompVis/taming-transformers" rel="noopener ugc nofollow" target="_blank">驯服变形金刚</a></li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="3e15" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">Swin Transformer:使用移位窗口的分层视觉转换器[3]</h2><p id="4a5b" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">变形金刚会取代计算机视觉中的CNN吗？在不到5分钟的时间内，您将通过一篇名为Swin transformer的新论文了解如何将Transformer架构应用于计算机视觉。</p><p id="7d58" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="bc6e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/will-transformers-replace-cnns-in-computer-vision-55657a196833"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">变形金刚会取代计算机视觉中的CNN吗？</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">几分钟后，您将了解如何通过一种新的方式将transformer架构应用于计算机视觉</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="pi l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="2d17" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">论文:<a class="ae mg" href="https://arxiv.org/abs/2103.14030v1" rel="noopener ugc nofollow" target="_blank"> Swin转换器:使用移位窗口的分层视觉转换器</a></li><li id="a734" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><a class="ae mg" href="https://github.com/microsoft/Swin-Transformer" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="0f84" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">深网:他们为视觉做过什么？[奖金]</h2><p id="3ea4" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">“我将公开分享关于视觉应用深度网络的一切，它们的成功，以及我们必须解决的局限性。”</p><p id="d870" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="3d4c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/what-has-ai-done-for-computer-vision-3748f5958e07"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">AI为计算机视觉做了什么？</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">关于当前深度网络的一切，他们为视觉应用做了什么。他们的成功和局限。</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="pj l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="0d59" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">论文:<a class="ae mg" href="https://arxiv.org/abs/1805.04025" rel="noopener ugc nofollow" target="_blank">深网:他们为视觉做过什么？</a></li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="d467" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">无限自然:从单一图像生成自然场景的永久视图[4]</h2><p id="2283" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">视图合成的下一步:永久视图生成，目标是拍摄一幅图像，然后飞进去探索风景！</p><p id="a8cc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="da08" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/infinite-nature-fly-into-an-image-and-explore-it-like-controlling-a-drone-541cab44b8f5"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">无限自然:飞入一个影像，像控制无人机一样探索它！</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">视图合成的下一步:永久视图生成，其目标是获取一幅图像，然后…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="pk l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="0d6b" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">论文:<a class="ae mg" href="https://arxiv.org/pdf/2012.09855.pdf" rel="noopener ugc nofollow" target="_blank">无限自然:从单一图像生成自然场景的永久视图</a></li><li id="4ee7" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><a class="ae mg" href="https://github.com/google-research/google-research/tree/master/infinite_nature" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li><li id="28d6" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><a class="ae mg" href="https://colab.research.google.com/github/google-research/google-research/blob/master/infinite_nature/infinite_nature_demo.ipynb#scrollTo=sCuRX1liUEVM" rel="noopener ugc nofollow" target="_blank"> Colab演示</a></li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="8123" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">完全重新照明:学习为背景替换重新照明肖像[5]</h2><p id="b092" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">根据您添加的新背景的照明，正确地重新照亮任何肖像。你是否曾经想改变一张图片的背景，但却让它看起来很真实？如果你已经尝试过，你就会知道这并不简单。你不能在家里给自己拍张照片，然后给海滩换个背景。只是看起来很糟糕，不现实。任何人都会马上说“那是PS过的”。对于电影和专业视频，你需要完美的灯光和艺术家来再现高质量的图像，而这是超级昂贵的。你不可能用自己的照片做到这一点。还是可以？</p><p id="c608" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="e3e2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/change-your-portraits-backgrounds-with-realistic-lighting-b6f2ebeb1a85"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">用真实的灯光改变你的肖像背景</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">根据您添加的新背景的照明，正确地重新照亮任何肖像。</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="pl l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="76f6" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">Paper: <a class="ae mg" href="https://augmentedperception.github.io/total_relighting/total_relighting_paper.pdf" rel="noopener ugc nofollow" target="_blank">全面重新照明:学习为背景替换重新照亮肖像</a></li></ul><blockquote class="lz ma mb"><p id="2751" class="ld le mc lf b lg lh kd li lj lk kg ll md ln lo lp me lr ls lt mf lv lw lx ly im bi translated">如果你也想阅读更多的研究论文，我推荐你阅读<a class="ae mg" rel="noopener ugc nofollow" target="_blank" href="/how-to-read-more-research-papers-7737e3770d7f">我的文章</a>，在那里我分享了寻找和阅读更多研究论文的最佳技巧。</p></blockquote></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="8624" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">用欧拉运动场制作动画[6]</h2><p id="dcde" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">该模型拍摄一张照片，了解哪些粒子应该在移动，并在无限循环中逼真地动画化它们，同时完全保留照片的其余部分，仍然创建像这样看起来令人惊叹的视频…</p><p id="a3b9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="92e5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/create-realistic-animated-looping-videos-from-pictures-58debf6f139"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">从图片创建逼真的动画循环视频</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">这个模型拍一张照片，了解哪些粒子应该在移动，并逼真地将它们动画化…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="pm l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="9450" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">论文:<a class="ae mg" href="https://arxiv.org/abs/2011.15128" rel="noopener ugc nofollow" target="_blank">用欧拉运动场制作图片动画</a></li><li id="f2aa" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><a class="ae mg" href="https://eulerian.cs.washington.edu/" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="54d9" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">CVPR 2021最佳论文奖:长颈鹿——可控图像生成[7]</h2><p id="0709" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">使用改进的GAN架构，他们可以移动图像中的对象，而不会影响背景或其他对象！</p><p id="8814" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="3485" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/cvpr-2021-best-paper-award-giraffe-controllable-image-generation-24eac0001ca4"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">CVPR 2021年最佳论文奖:长颈鹿——可控图像生成</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">使用改良的氮化镓架构，他们甚至可以移动图像中的物体，而不影响背景或…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="pn l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="3339" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">论文:<a class="ae mg" href="http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf" rel="noopener ugc nofollow" target="_blank">长颈鹿:将场景表示为合成生成神经特征场</a></li><li id="abe6" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><a class="ae mg" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="ee90" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">TimeLens:基于事件的视频帧插值[8]</h2><p id="bad4" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">TimeLens可以理解视频帧之间的粒子运动，以我们肉眼无法看到的速度重建真实发生的事情。事实上，它实现了我们的智能手机和其他型号之前无法达到的效果！</p><p id="0968" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="9b46" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/change-video-into-slow-motion-with-ai-timelens-explained-4281d97c9b9d"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">用AI把视频换成慢动作！TimeLens解释道</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">时间镜头可以理解视频帧之间的粒子运动，以重建真正的…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="po l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="b927" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">论文:<a class="ae mg" href="http://rpg.ifi.uzh.ch/docs/CVPR21_Gehrig.pdf" rel="noopener ugc nofollow" target="_blank"> TimeLens:基于事件的视频帧插值</a></li><li id="7fc6" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><a class="ae mg" href="https://github.com/uzh-rpg/rpg_timelens" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul><blockquote class="lz ma mb"><p id="3f7c" class="ld le mc lf b lg lh kd li lj lk kg ll md ln lo lp me lr ls lt mf lv lw lx ly im bi translated">订阅我的每周<a class="ae mg" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank">时事通讯</a>，了解2022年AI的最新出版物！</p></blockquote></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="3eae" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">(Style)CLIPDraw:文本到绘图合成中内容和样式的耦合[9]</h2><p id="a9fd" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">你有没有梦想过采用图片的风格，比如左边这个很酷的抖音绘画风格，并将其应用到你选择的新图片中？是的，我做到了，而且从来没有这么容易做到。事实上，你甚至可以只通过文本来实现，现在就可以用这种新方法和他们为每个人提供的Google Colab笔记本来尝试。只需拍下你想要复制的样式的图片，输入你想要生成的文本，这个算法就会从中生成一张新的图片！回头看看上面的结果就知道了，这么大的进步！结果非常令人印象深刻，尤其是如果您考虑到它们是由单行文本构成的！</p><p id="3b0f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="054f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/text-to-drawing-synthesis-with-artistic-control-clipdraw-styleclipdraw-dd56fa208bea"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">具有艺术控制的文本到绘图合成| CLIPDraw &amp; StyleCLIPDraw</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">给你想要复制的风格拍张照，输入文字，算法会从中生成一张新的图片！</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="pp l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="08cd" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">Paper (CLIPDraw): <a class="ae mg" href="https://arxiv.org/abs/2106.14843" rel="noopener ugc nofollow" target="_blank"> CLIPDraw:通过语言图像编码器探索文本到绘图的合成</a></li><li id="e870" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">paper(StyleCLIPDraw):<a class="ae mg" href="https://arxiv.org/abs/2111.03133" rel="noopener ugc nofollow" target="_blank">StyleCLIPDraw:文本到图形合成中内容和样式的耦合</a></li><li id="16ca" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><a class="ae mg" href="https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb" rel="noopener ugc nofollow" target="_blank"> CLIPDraw Colab演示</a></li><li id="f969" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><a class="ae mg" href="https://colab.research.google.com/github/pschaldenbrand/StyleCLIPDraw/blob/master/Style_ClipDraw.ipynb" rel="noopener ugc nofollow" target="_blank"> StyleCLIPDraw Colab演示</a></li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="80b3" class="mh mi it bd mj mk ml dn mm mn mo dp mp lm mq mr ms lq mt mu mv lu mw mx my iz bi translated">城市NeRF:城市尺度的建筑NeRF[10]</h2><p id="f3da" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">这个模型被称为CityNeRF，它是从NeRF发展而来的，我之前在我的频道中介绍过。NeRF是首批使用辐射场和机器学习从图像中构建3D模型的模型之一。但是NeRF并不是那么有效，而且只适用于单一规模。在这里，CityNeRF同时应用于卫星和地面图像，为任何视点生成各种3D模型比例。简而言之，他们将NeRF带到了城市规模。但是怎么做呢？</p><p id="2093" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">短视频讲解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="7412" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">简短阅读</p><div class="om on gp gr oo op"><a rel="noopener  ugc nofollow" target="_blank" href="/technology-fcb0fbfa9c00"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">CityNeRF:城市比例的3D渲染！</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">以任何比例生成具有高质量细节的城市级3D场景！</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">pub.towardsai.net</p></div></div><div class="oy l"><div class="pq l pa pb pc oy pd lb op"/></div></div></a></div><ul class=""><li id="8de2" class="nm nn it lf b lg lh lj lk lm pe lq pf lu pg ly nt nu nv nw bi translated">论文:<a class="ae mg" href="https://arxiv.org/pdf/2112.05504.pdf" rel="noopener ugc nofollow" target="_blank">城市NeRF:城市规模的建筑NeRF</a></li><li id="ccde" class="nm nn it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><a class="ae mg" href="https://city-super.github.io/citynerf/" rel="noopener ugc nofollow" target="_blank">点击此处获取代码(即将发布)</a></li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><blockquote class="lz ma mb"><p id="5d36" class="ld le mc lf b lg lh kd li lj lk kg ll md ln lo lp me lr ls lt mf lv lw lx ly im bi translated">如果你想阅读更多的论文并有更广阔的视野，这里有另一个涵盖2020年的伟大知识库:<a class="ae mg" href="https://github.com/louisfb01/Best_AI_paper_2020" rel="noopener ugc nofollow" target="_blank"> 2020:充满令人惊叹的人工智能论文的一年——综述</a>并随时订阅我的每周<a class="ae mg" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank">时事通讯</a>，了解2022年人工智能的最新出版物！</p></blockquote><p id="a92b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mc">在</em> <strong class="lf jd"> <em class="mc">上给我贴标签</em></strong><em class="mc"/><a class="ae mg" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"><em class="mc">@ Whats _ AI</em></a><em class="mc">或</em><strong class="lf jd"><em class="mc">LinkedIn</em></strong><em class="mc"/><a class="ae mg" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"><em class="mc">@ Louis(什么是AI) Bouchard </em> </a> <em class="mc">如果分享一下名单！</em></p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="6d62" class="nb mi it bd mj nc pr ne mm nf ps nh mp ki pt kj ms kl pu km mv ko pv kp my nl bi translated">论文参考</h1><p id="e7c3" class="pw-post-body-paragraph ld le it lf b lg no kd li lj np kg ll lm oj lo lp lq ok ls lt lu ol lw lx ly im bi translated">[1] A. Ramesh等，零拍文本到图像的生成，2021。arXiv:2102.12092</p><p id="3394" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[2]驯服高分辨率图像合成的变压器，Esser等人，2020年。</p><p id="1d22" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[3]刘，z .等，2021，“Swin变压器:使用移位窗口的分层视觉变压器”，arXiv预印本<a class="ae mg" href="https://arxiv.org/abs/2103.14030v1" rel="noopener ugc nofollow" target="_blank">，</a></p><p id="2436" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[奖金]尤耶和刘，2021年。深网:他们为视觉做过什么？。《国际计算机视觉杂志》，129(3)，第781–802页，【https://arxiv.org/abs/1805.04025】T2。</p><p id="bd2c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[4]刘，a .，塔克，r .，贾帕尼，v .，马卡迪亚，a .，斯内夫利，n .，金泽，a .，2020。无限自然:从单一图像生成自然场景的永久视图，【https://arxiv.org/pdf/2012.09855.pdf T4】</p><p id="e920" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[5] Pandey等人，2021，Total Relighting:学习为背景替换重新照亮人像，doi: 10.1145/3450626.3459872，<a class="ae mg" href="https://augmentedperception.github.io/total_relighting/total_relighting_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://augmented perception . github . io/Total _ re lighting/Total _ re lighting _ paper . pdf</a>。</p><p id="3cd2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[6] Holynski，Aleksander等人，“用欧拉运动场制作动画”IEEE/CVF计算机视觉和模式识别会议录。2021.</p><p id="68ca" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[7] Michael Niemeyer和Andreas Geiger，(2021)，“长颈鹿:将场景表示为合成生成神经特征场”，发表于CVPR 2021年。</p><p id="bbde" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[8]斯捷潘·图利亚科夫*、丹尼尔·格赫里希*、斯塔马蒂亚斯·乔戈里斯、朱利叶斯·埃尔巴希、马蒂亚斯·格赫里希、李、大卫·斯卡拉穆扎，TimeLens:基于事件的视频帧内插，IEEE计算机视觉和模式识别会议(CVPR)，纳什维尔，2021年，<a class="ae mg" href="http://rpg.ifi.uzh.ch/docs/CVPR21_Gehrig.pdf" rel="noopener ugc nofollow" target="_blank">http://rpg.ifi.uzh.ch/docs/CVPR21_Gehrig.pdf</a></p><p id="b085" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[9] a) CLIPDraw:通过语言图像编码器探索文本到绘图的合成<br/>b)StyleCLIPDraw:Schaldenbrand，p .，Liu，z .和Oh，j .，2021。StyleCLIPDraw:文本到绘图合成中内容和样式的耦合。</p><p id="6dc1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[10]，杨，徐，李，潘，徐，赵，饶，李，戴，林，2021 .城市NeRF:在城市尺度上建造NeRF。</p></div></div>    
</body>
</html>