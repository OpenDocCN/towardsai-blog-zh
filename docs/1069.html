<html>
<head>
<title>Solving the Vanishing Gradient Problem with Self-Normalizing Neural Networks using Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Keras解决自归一化神经网络的消失梯度问题</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/solving-the-vanishing-gradient-problem-with-self-normalizing-neural-networks-using-keras-59a1398b779f?source=collection_archive---------1-----------------------#2020-10-21">https://pub.towardsai.net/solving-the-vanishing-gradient-problem-with-self-normalizing-neural-networks-using-keras-59a1398b779f?source=collection_archive---------1-----------------------#2020-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c259" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="6d48" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何通过简单的模型配置改善深度前馈神经网络的收敛性和性能</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/1eaf1b5fb6b7c5c21b580d98433f6045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ec-tmowLxG8A8ysV"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@martinsanchez?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马丁·桑切斯</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="7b3d" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">问题陈述</h1><p id="ac04" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">训练深度神经网络可能是一项具有挑战性的任务，尤其是对于非常深度的模型。这个困难的主要部分是由于通过<a class="ae le" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>计算的梯度的不稳定性。在本帖中，我们将学习<strong class="lz ja">如何使用Keras创建一个自标准化深度前馈神经网络。</strong>这将解决梯度不稳定问题，加快训练收敛，并提高模型性能。</p><blockquote class="mt mu mv"><p id="076d" class="lx ly mw lz b ma mx ka mc md my kd mf mz na mi mj nb nc mm mn nd ne mq mr ms ij bi translated">免责声明:本文是一个简短的总结，重点是实现。请阅读引用的论文以获得完整的细节和数学论证(参考资料部分的链接)。</p></blockquote></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="4647" class="lf lg iq bd lh li nm lk ll lm nn lo lp kf no kg lr ki np kj lt kl nq km lv lw bi translated">背景</h1><p id="89b7" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在他们2010年的里程碑式论文中，Xavier Glorot和Yoshua Bengio提供了关于训练深度神经网络的难度的宝贵见解。</p><p id="3238" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">原来，当时流行的激活函数和权重初始化技术的选择直接导致了所谓的<a class="ae le" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失/爆炸梯度问题</a>。<a class="ae le" href="https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem" rel="noopener ugc nofollow" target="_blank">本文</a>深入探讨了消失/爆炸渐变和渐变裁剪的主题。</p><p id="82f7" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">简而言之，这是梯度开始收缩或增加太多以至于无法训练的时候。</p><h2 id="a687" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">饱和激活函数</h2><p id="0a85" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在广泛采用现在无处不在的ReLU函数及其变体之前，<strong class="lz ja"> sigmoid </strong>函数(S形)是激活函数最受欢迎的选择。乙状结肠激活的一个这样的例子是<a class="ae le" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank"> <strong class="lz ja">逻辑功能</strong> </a> <strong class="lz ja"> : </strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ff445706e8fadb6ab1fbe4f166c979cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*HYhWqnm32J_dmhET.gif"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:https://www.mhnederlof.nl/logistic.html</figcaption></figure><p id="14a5" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">sigmoid函数的一个主要缺点是它们会饱和<strong class="lz ja">。</strong>在逻辑函数的情况下，对于负输入和正输入，输出分别饱和为0或1。这导致随着输入 的幅度增加，梯度越来越小(<em class="mw">非常接近0) <strong class="lz ja"> <em class="mw">)。</em></strong></em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/d60824d0da76ca3fe89f2594cf651360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m9qXyi7FBI8cD6kSPHk-4w.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">逻辑和ELU激活函数及其相应导数的可视化。由于饱和，逻辑函数的导数将趋于收缩。相反，对于正输入，ELU函数的导数将是常数。</figcaption></figure><p id="4796" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">由于ReLU及其变体不饱和，它们减轻了这种消失梯度现象。ReLU的改进变体，如eLU函数(如上所示),具有平滑的导数:</p><ul class=""><li id="6703" class="oe of iq lz b ma mx md my mg og mk oh mo oi ms oj ok ol om bi translated">对于任何实际输入，导数将始终为1</li><li id="9657" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">对于小的负数，导数不会接近于零</li><li id="c257" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">所有区域的平滑导数</li></ul><blockquote class="mt mu mv"><p id="5cb8" class="lx ly mw lz b ma mx ka mc md my kd mf mz na mi mj nb nc mm mn nd ne mq mr ms ij bi translated">注:因此，期望值为0的输入以及较小的方差是有益的。这将有助于在整个网络中保持强梯度。</p></blockquote><h2 id="2b92" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">权重初始化选择不当</h2><p id="fe68" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">论文中发现的另一个重要见解是使用平均值为0、标准差为1的正态分布<a class="ae le" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">进行权重初始化的效果，这是作者发现之前广泛流行的选择。</a></p><p id="0f27" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">作者表明，sigmoid激活和权重初始化的特定组合(均值为0，标准差为1的正态分布)使得<strong class="lz ja">输出比</strong>输入具有更大的方差。这种<strong class="lz ja"> <em class="mw">效应在整个网络</em> </strong>中复合，使得较深层的输入相对于较浅(较早)层的输入具有大得多的<strong class="lz ja"><em class="mw"/></strong>。这种现象后来在2015年由Sergey Ioffe和Christian Szegedy在一篇里程碑式的论文中被命名为<strong class="lz ja">内部协变量转移</strong>。</p><p id="6d6f" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">正如我们在上面看到的，当使用sigmoid激活时，这转化为越来越小的梯度。</p><blockquote class="mt mu mv"><p id="98ca" class="lx ly mw lz b ma mx ka mc md my kd mf mz na mi mj nb nc mm mn nd ne mq mr ms ij bi translated">由于逻辑函数的期望值是0.5 ，而不是0，这个问题在<strong class="lz ja">逻辑函数</strong>中变得更加突出。双曲正切sigmoid函数的期望值为0，因此在实践中表现更好(但也会饱和)。</p></blockquote><p id="d6ac" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">作者认为，为了在训练期间保持梯度稳定，所有层的输入和输出必须在整个网络 中保持或多或少相同的方差<strong class="lz ja"> <em class="mw">。这将防止信号在前向传播时消失或爆炸，以及在反向传播期间梯度消失或爆炸。</em></strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/b93cc5a196f7b07594eb33d2cfbd8942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5WazU0JrKBdgfBx12U33DA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">用LeCun正态初始化生成的分布导致更多以0为中心的概率质量，并且具有更小的方差。这与使用正态初始化(平均值为0，标准偏差为1)生成的分布相反，在正态初始化中，值的分布范围更广(方差更大)。</figcaption></figure><p id="12f2" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">为了实现这一点，他们提出了一种权重初始化技术，以论文第一作者的名字命名为<strong class="lz ja"> Glorot(或Xavier)初始化</strong>。结果是，对Glorot技术稍加修改，我们得到了<strong class="lz ja"> LeCun初始化，</strong>以Yann LeCun命名。</p><blockquote class="mt mu mv"><p id="1d42" class="lx ly mw lz b ma mx ka mc md my kd mf mz na mi mj nb nc mm mn nd ne mq mr ms ij bi translated"><strong class="lz ja"> Yann LeCun在20世纪90年代提出了他的LeCun初始化，参考了Springer出版物</strong> <a class="ae le" href="https://link.springer.com/book/10.1007/978-3-642-35289-8" rel="noopener ugc nofollow" target="_blank"> <strong class="lz ja">神经网络:贸易的诀窍(1998) </strong> </a> <strong class="lz ja">。</strong></p></blockquote></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h1 id="df22" class="lf lg iq bd lh li nm lk ll lm nn lo lp kf no kg lr ki np kj lt kl nq km lv lw bi translated">自归一化前馈神经网络</h1><p id="194c" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">2017年，君特·克兰鲍尔等人推出了<a class="ae le" href="https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lz ja">自归一化神经网络</strong> ( <strong class="lz ja"> SNNs </strong> ) </a>。通过确保满足某些条件，这些网络能够在所有层 上保持接近0平均值和1 <strong class="lz ja"> <em class="mw">标准偏差的输出。这意味着snn没有消失/爆炸梯度问题，因此比没有这种自归一化属性的网络收敛得更快。根据作者的说法，在论文报道的所有学习任务中，SNNs明显优于其他变体(没有自我规范化)。下面是创建SNN所需条件的更详细描述。</em></strong></p><h2 id="a16d" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">架构和层</h2><p id="276b" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">SNN必须是仅由全连接层组成的<strong class="lz ja">顺序模型</strong>。</p><blockquote class="mt mu mv"><p id="70e5" class="lx ly mw lz b ma mx ka mc md my kd mf mz na mi mj nb nc mm mn nd ne mq mr ms ij bi translated">注意:根据任务的不同，某些类型的网络比其他类型的网络更合适。例如，卷积神经网络通常用于计算机视觉任务，主要是由于它们的参数效率。确保一个全连接的层足以完成你的任务。如果是这种情况，那么考虑使用SNN。否则，<a class="ae le" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank"> <strong class="lz ja">批量规范化</strong> </a>是确保整个网络正常化的极好方法。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/7d884b8a8e78fc14c2103481b148942c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ahi8vTnOx7RATtO0.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">深度、顺序、全连接神经网络的示例。图片来源:<a class="ae le" href="https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html" rel="noopener ugc nofollow" target="_blank">https://www . or eilly . com/library/view/tensor flow-for-deep/9781491980446/ch04 . html</a></figcaption></figure><p id="9e63" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">在这种情况下，顺序模型是指各层严格按照顺序排列的模型。换句话说，对于每个隐藏层<em class="mw"> l </em>，层<em class="mw"> l </em>接收的唯一输入严格来说是层<em class="mw"> l-1的输出。</em>在第一个隐藏层的情况下，它只接收输入特征。在Keras中，这种类型的模型实际上被称为<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential" rel="noopener ugc nofollow" target="_blank">顺序模型</a>。</p><p id="02c4" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">全连接层是指层中的每个单元都与每个单独的输入相连。在Keras中，这种类型的层被称为<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense" rel="noopener ugc nofollow" target="_blank">致密层</a>。</p><h2 id="c9f5" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">输入标准化</h2><p id="b2b1" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">输入要素必须标准化。这意味着所有特征的训练数据应该具有0平均值和1标准偏差。</p><h2 id="84a8" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">重量初始化</h2><p id="27c7" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">SNN中的所有图层都必须使用LeCun正常初始化进行初始化。正如我们之前看到的，这将确保权重值的范围更接近于0。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/5b93ac933418aa5952cfb0ba9f0cd8ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jSQQEGd7eYXR39ZsW2nbPQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">将权重可视化为矩阵。标准正常初始化权重的值的范围远大于LeCun正常初始化权重的范围。</figcaption></figure><h2 id="75c1" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">SELU激活函数</h2><p id="e353" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">作者引入了<strong class="lz ja">标度ELU </strong> ( <strong class="lz ja"> SELU </strong>)函数作为SNNs的激活函数。只要上述条件得到满足，SELU就提供了自我正常化的保证。</p><h1 id="32da" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">Keras实施</h1><p id="9635" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">以下示例显示了如何为10类分类任务定义SNN:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ot ou l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">Keras中自规范化神经网络的实现</figcaption></figure><h1 id="f383" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">实验结果</h1><p id="1e4e" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">下面是常规前馈神经网络和SNN在三个不同任务上的比较:</p><ul class=""><li id="7dc3" class="oe of iq lz b ma mx md my mg og mk oh mo oi ms oj ok ol om bi translated">形象分类(<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist" rel="noopener ugc nofollow" target="_blank">时尚MNIST </a>，<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10" rel="noopener ugc nofollow" target="_blank"> CIFAR10 </a>)</li><li id="f9cb" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">回归(<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing" rel="noopener ugc nofollow" target="_blank">波士顿住房数据集</a>)</li></ul><p id="395b" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">两个网络共享以下配置:</p><ul class=""><li id="cd4e" class="oe of iq lz b ma mx md my mg og mk oh mo oi ms oj ok ol om bi translated">20个隐藏层</li><li id="7c89" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">每个隐藏层100个单位</li><li id="d70c" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">那达慕优化器</li><li id="79ee" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">7e-4的学习率</li><li id="d745" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">50个时代</li></ul><p id="ca87" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">对于这两个模型，学习曲线在达到最佳性能度量的时期停止</p><h2 id="d0d1" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">时尚MNIST</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/ef2b7c642d7c60ec66ab89316a00cbb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJVGV_63Y8aSSqW4-aWwRw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">与常规模型相比，SNN在少28%迭代次数的<strong class="bd lh">中达到最佳验证精度。</strong></figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/1a7a6aeda7a2e420c41ae8ba28db93c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjUaiC5vcI0DuhFLGiRDgQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">SNN最后一层权重的时间分布。</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/a8a4ab7bdb768bf58f8f2f88ab091d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScLZhpPcv3fI4Agq5ACNCA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">常规模型最后一层权重的时间分布。</figcaption></figure><h2 id="267d" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">CIFAR10</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/7774776ada21f748e9c5937f96372a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWX7RwDyZIbeNmRkCRFrKw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">与常规模型相比，SNN的验证损失和准确性始终更好。</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/1417b69ec1bf9bb175d0c903cd284223.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*60f_tgeqxQQj-dze7wOVLg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">与普通车型相比，SNN在测试台上的F1成绩<strong class="bd lh">提高了</strong>12%。</figcaption></figure><h2 id="7662" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">波士顿住房公司</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/82018815d31108b31036371efe816ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DiirQCnz9ApXCsrUJ6sYOg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">与常规模型相比，SNN在<strong class="bd lh">少32%的时期</strong>中实现了最佳验证准确性(SNN为34个时期，而常规模型为全部50个时期)。</figcaption></figure><h1 id="6019" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">结论</h1><p id="c017" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">通过确保我们的前馈神经网络配置满足一组条件，我们可以让它自动标准化。所需的条件是:</p><ul class=""><li id="84fd" class="oe of iq lz b ma mx md my mg og mk oh mo oi ms oj ok ol om bi translated">模型必须是一系列完全连接的层</li><li id="49c7" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">权重用LeCun正常初始化技术初始化</li><li id="ff86" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">该模型使用SELU激活函数</li><li id="6a94" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">输入是标准化的</li></ul><p id="2129" class="pw-post-body-paragraph lx ly iq lz b ma mx ka mc md my kd mf mg na mi mj mk nc mm mn mo ne mq mr ms ij bi translated">与没有自规范化的模型相比，这几乎总是会导致<strong class="lz ja">性能的提高和收敛</strong>。如果你的任务需要一个常规的前馈神经网络，考虑使用SNN变体。否则，批处理规范化是一个优秀的(但更多的时间和计算成本)规范化策略。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><h2 id="9474" class="nr lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">来源</h2><ul class=""><li id="7a9f" class="oe of iq lz b ma mb md me mg oz mk pa mo pb ms oj ok ol om bi translated">格洛特、泽维尔、约舒阿·本吉奥。"理解训练深度前馈神经网络的困难."第十三届人工智能与统计国际会议论文集。2010.</li><li id="9687" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">约菲、谢尔盖和克里斯蒂安·塞格迪。"批量标准化:通过减少内部协变量转移加速深度网络训练."<em class="mw"> arXiv预印本arXiv:1502.03167 </em> (2015)。</li><li id="d7fd" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated">Klambauer，Günter，等,“自规范化神经网络”<em class="mw">神经信息处理系统的进展</em>。2017.</li><li id="d7ef" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated"><a class="ae le" href="https://link.springer.com/book/10.1007/978-3-642-35289-8" rel="noopener ugc nofollow" target="_blank">蒙塔冯、格雷瓜尔、奥尔和克劳斯-罗伯特·米勒。"神经网络-商业诀窍第二版."施普林格，多伊10(2012):978–3。</a></li><li id="eaa3" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated"><a class="ae le" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习</a></li><li id="67cb" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Logistic_function</a></li><li id="d79a" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Vanishing_gradient_problem</a></li><li id="f551" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Backpropagation</a></li><li id="4f06" class="oe of iq lz b ma on md oo mg op mk oq mo or ms oj ok ol om bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Normal_distribution</a></li></ul></div></div>    
</body>
</html>