# 让艾给你写故事！！(使用 GPT J)

> 原文：<https://pub.towardsai.net/let-ai-write-stories-for-you-using-gpt-j-fc71b414daa8?source=collection_archive---------0----------------------->

## [自然语言处理](https://towardsai.net/p/category/nlp)

*GPT 模型家族不断提高生成文本的质量，以至于他们现在被用来写博客。*

![](img/2a37a64cc861d44e3d970f10c573af8e.png)

照片由[亚伦·伯顿](https://unsplash.com/@aaronburden?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

人工智能(AI)的进步让我们的许多任务变得更容易、更易于管理。也许是时候使用人工智能来为我们写博客了*？有很多在线服务可以分析你的写作，给你反馈如何改进。这些服务中的大部分集中在检测一个句子是否有意义。他们中的大多数人也关注于鉴别作者的语法是否正确。

此外，我知道一些初创公司试图使用 1750 亿美元的 GPT-3 模型来做文案，并能够获得大量资金！但是，这有可能吗？回到上一段，在第二句的末尾找到开头。在提到的星星之后出现的一切都是来自生成模型的不变输出。我认为这些句子的质量相当好，它们甚至相互关联而不重复。

**我用的是什么型号？—** 谈到预先训练的生成模型，有两个主要的竞争者:GPT-3 [1]和 GPT-J [2]！它们都是基于变压器的模型。GPT J 要小得多，只有大约 60 亿个参数。这两者的主要区别是他们的许可证。虽然你必须为每个令牌支付 OpenAI 才能使用 GPT-3，但你可以免费使用由 [EleutherAI](https://www.eleuther.ai/) 集团发布的 GPT-J 模型，并且可以在 Huggingface 图书馆上获得，这是一个巨大的优势。他们的表现如何？自从 GPT-3 测试版发布以来，我就一直在玩它，并试图对它们进行比较。GPT-3 可能在生成任务上稍好一点，但整体文本质量非常接近！

> ⚠️:我不想展示 GPT-3 的例子，因为它们有严格的指导原则。

那么，让我们和 GPT J 一起去吧！什么阻止了我们？— 嗯，模特的尺寸！该模型的检查点大小约为 25GB……要么使用您的 PC，要么使用免费的在线资源(如 Google Colab、Kaggle 或 SageMaker Lab)。必须有至少 45GB(或者 34GB，如果你使用 float16 半精度模型)的可用 RAM 来使用 Huggingface 库加载模型，除非你有足够的 RAM 来使用 CPU 或房间里的巨型 GPU。假设您需要支付一些费用来访问这样的资源(从亚马逊或谷歌云服务)是安全的。所以，它可能终究不是免费的！

# 如何使用模型？

Hugginface 库可以轻松加载预训练的模型，并使用它来生成文本。在加载模型及其标记器之后，您将需要向模型传递一个提示，并使用 generate()函数来完成提示。下面是我用来生成这个故事第一段的代码。

代码 1。使用 GPT-J 模型的示例代码。

如您所见，我将这两个句子作为上下文(提示)传递给了模型，并得到了故事的介绍作为输出。示例代码中没有什么特别的地方。你可以阅读 GPT J 文档[这里](https://huggingface.co/docs/transformers/model_doc/gptj)了解更多关于该模型的信息。

# 最后的话，

我相信用这些模型做文案是有可能的。基于给定上下文生成的文本质量令人印象深刻。我们可能还没有达到通过一个主题并获得一个完整故事的地步，但是有可能通过一些好的想法来利用这样的模型。主要问题是这种模型的可访问性，以及它们太大以至于需要多个 GPU 来加载它们的事实。好消息是，你可以在有足够内存的 CPU 上使用这些型号，这比 GPU 便宜得多。尽管如此，它们并不是每个人都能轻易获得的。

> 我每周给 NLP 的书呆子发一份时事通讯。如果您想了解自然语言处理的最新发展，可以考虑订阅。
> [阅读更多，订阅](https://nlpiation.github.io/) —加入酷孩子俱乐部，立即报名！

## 参考

[1] Brown，Tom B .等人，“语言模型是一次性学习者。”arXiv 预印本 arXiv:2005.14165 (2020)。
[2]J-6B:一个 60 亿参数的自回归语言模型，由王，本和小松崎，阿兰