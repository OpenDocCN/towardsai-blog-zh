<html>
<head>
<title>Image classification with neural network Part — 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用神经网络进行图像分类(二)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/image-classification-with-neural-network-part-2-83de33afa926?source=collection_archive---------1-----------------------#2022-01-06">https://pub.towardsai.net/image-classification-with-neural-network-part-2-83de33afa926?source=collection_archive---------1-----------------------#2022-01-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c730" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><p id="756e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果你错过了我之前的博客，看看这里的<a class="ae ku" rel="noopener ugc nofollow" target="_blank" href="/image-classification-with-neural-network-21a75cebb067"/>。我之前的博客很少涉及卷积层。</p><p id="cf1b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在图像上使用前馈网络的缺点之一是许多学习参数。卷积神经网络使用称为<strong class="jy ja">参数共享的概念或方案。</strong>使用它我们可以假设在单个坐标学习的权重可以用于相同深度的其他权重。所以每个深度的神经元使用相同的权重和偏差。</p><p id="73e0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我用一个例子来解释，输入图像大小是64x64x3，过滤器是3x3，过滤器的数量是32。输出将是62x62x32。</p><p id="83ac" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">计算公式请参见<a class="ae ku" rel="noopener ugc nofollow" target="_blank" href="/image-classification-with-neural-network-21a75cebb067">此处</a>。因此，如果不使用参数共享，该层中的可学习参数的数量将是，</p><p id="01de" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">与每个神经元相关联的参数= 3x3x3 + 1(用于偏置)= 28个参数。</p><p id="db62" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们有32x62x62 = 123008层。这样组合起来，我们有123008 x 28 = 3444224，这对单层来说是非常高的。</p><p id="4ade" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在当使用参数共享时，我们得到28 x 32 = 896 &lt;&lt;&lt;&lt; 3 million.</p><p id="ffce" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">In some cases, parameter sharing can be relaxed such as an object can be found only in the center or corners of the image, where we do not search for that object in the entire image and search in a particular location alone.</p><p id="d919" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">There are other layers that help CNN to extract data from images. Few are,</p><ul class=""><li id="a62f" class="kv kw iq jy b jz ka kd ke kh kx kl ky kp kz kt la lb lc ld bi translated">Pooling layer</li><li id="5bd8" class="kv kw iq jy b jz le kd lf kh lg kl lh kp li kt la lb lc ld bi translated">Activation layer</li><li id="7dec" class="kv kw iq jy b jz le kd lf kh lg kl lh kp li kt la lb lc ld bi translated">Dropout layer</li><li id="aa41" class="kv kw iq jy b jz le kd lf kh lg kl lh kp li kt la lb lc ld bi translated">Batch Normalization layer</li><li id="3649" class="kv kw iq jy b jz le kd lf kh lg kl lh kp li kt la lb lc ld bi translated">Fully Connected layer</li></ul><p id="cc0b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Let's discuss one of the important layers here,</p><p id="1228" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">池层:</strong></p><p id="5c8e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">通常在CNN中，当我们在层中深入时，我们需要减少空间维度并从图像中提取信息。CNN越深入，我们可以从图像中提取更复杂的模式。</p><p id="ca75" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，池图层有助于对制图表达进行缩减采样。像卷积层一样，池层也有固定的形状矩阵，它在输入上滑动，并根据步幅计算输出，但这些层没有任何可学习的参数。</p><p id="10a6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">存在各种类型的池层，但是最流行的是最大池层和平均池层。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/5acb7b4760de1d9ef5231a0ee0244ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjKThBDh_8h49zOyoZaJnQ.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">最大池:按作者分类的图像</figcaption></figure><p id="d21b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">以下是python中池化层的基本实现</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lz ma l"/></div></figure><p id="82b1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用python中的TensorFlow包可以获得类似的结果</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lz ma l"/></div></figure><p id="9ddd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">激活层</strong></p><p id="1cbc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这用于在神经网络中引入非线性。激活功能的一些实现可以在<a class="ae ku" href="https://mugunthanramesh.medium.com/neural-network-36e5e287ecc5" rel="noopener">这里</a>找到。</p><p id="f177" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">激活函数的Tensorflow实现是</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lz ma l"/></div></figure><p id="3d94" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">其他层可以在以后的博客中讨论</p><p id="b28c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>