<html>
<head>
<title>Selfie Background Remove or Blur With Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python移除或模糊自拍背景</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/selfie-background-remove-or-blur-with-python-cfba827e3c78?source=collection_archive---------1-----------------------#2022-09-29">https://pub.towardsai.net/selfie-background-remove-or-blur-with-python-cfba827e3c78?source=collection_archive---------1-----------------------#2022-09-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bcbe" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我将演示如何使用Python模糊或更改我们的自拍背景，就像Zoom、MS Teams、Google Meet或Skype一样。</h2></div><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="kn ko l"/></div></figure><p id="66cc" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">您见过的最先进的数据科学路线图！附带数以千计的免费学习资源和ChatGPT集成！</strong><a class="ae ll" href="https://87v9.short.gy/K93jZA" rel="noopener ugc nofollow" target="_blank"><strong class="kr iu"/></a></p><p id="b1a5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">你有没有想过Zoom、MS Teams、Google Meet或Skype中的“自定义背景”或“模糊背景”功能是如何工作的？没有深度捕捉技术或绿色屏幕，机器如何理解人和背景之间的差异？</p><p id="5519" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这就是图像分割发挥作用的地方。图像分割是机器学习目前为我们提供的最有价值的功能之一。图像分割仍在积极研究中，因此，我们有许多<a class="ae ll" href="https://paperswithcode.com/task/semantic-segmentation" rel="noopener ugc nofollow" target="_blank"> SOTA机器学习方法</a>来提供精确的像素级分割。</p><p id="a7c0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这些模型可以有各种大小，有数百万个参数。在电池寿命和计算资源有限的边缘设备上实时运行这些模型可能具有挑战性。当我们想要测试一些新东西或尝试新东西以获得乐趣时，预先训练的模型可以将我们从繁琐的优化中拯救出来，我们需要执行这些优化来使这些模型变得精简和超快。</p><p id="77d8" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">有许多预训练的自拍分割模型，但我选择了本教程中的<a class="ae ll" href="https://google.github.io/mediapipe/solutions/selfie_segmentation.html" rel="noopener ugc nofollow" target="_blank"> MediaPipe自拍分割</a>解决方案。以下是他们描述中的引文:</p><blockquote class="lm ln lo"><p id="9636" class="kp kq lp kr b ks kt ju ku kv kw jx kx lq kz la lb lr ld le lf ls lh li lj lk im bi translated">MediaPipe自拍分割分割场景中的突出人物。它可以在智能手机和笔记本电脑上实时运行。预期的使用案例包括自拍效果和视频会议，其中人很近(&lt; 2m) to the camera.</p></blockquote><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="ab gu cl lt"><img src="../Images/c36247328cd9b2b729b1834f8bdae1c7.png" data-original-src="https://miro.medium.com/v2/1*syL5alAFSfmYSMycLwBhIw.gif"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">Source: <a class="ae ll" href="https://ai.googleblog.com/2020/10/background-features-in-google-meet.html" rel="noopener ugc nofollow" target="_blank"> Google AI博客:Google Meet中的背景功能，由Web ML(googleblog.com)提供支持</a></figcaption></figure><p id="a149" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">该MediaPipe解决方案为我们提供了基于MobileNetV3的ML模型，有两种模型(通用和横向):</p><ul class=""><li id="2f84" class="ma mb it kr b ks kt kv kw ky mc lc md lg me lk mf mg mh mi bi translated">通用模型对256×256×3(HWC)张量进行操作，并输出表示分割掩模的256×256×1张量；</li><li id="17cf" class="ma mb it kr b ks mj kv mk ky ml lc mm lg mn lk mf mg mh mi bi translated">景观模型类似于一般模型，但是在144x256x3 (HWC)张量上操作。它比普通型号的FLOPs少，运行速度更快。</li></ul><blockquote class="lm ln lo"><p id="169c" class="kp kq lp kr b ks kt ju ku kv kw jx kx lq kz la lb lr ld le lf ls lh li lj lk im bi translated">注意。通用车型也为<a class="ae ll" href="https://developers.google.com/ml-kit/vision/selfie-segmentation" rel="noopener ugc nofollow" target="_blank"> ML套件</a>提供动力，风景车型的一个变种为<a class="ae ll" href="https://ai.googleblog.com/2020/10/background-features-in-google-meet.html" rel="noopener ugc nofollow" target="_blank"> Google Meet </a>提供动力。请在<a class="ae ll" href="https://google.github.io/mediapipe/solutions/models.html#selfie-segmentation" rel="noopener ugc nofollow" target="_blank">型号卡</a>中找到更多有关型号的详细信息。</p></blockquote><h2 id="6d95" class="mo mp it bd mq mr ms dn mt mu mv dp mw ky mx my mz lc na nb nc lg nd ne nf ng bi translated">设置项目(windows):</h2><p id="a1a0" class="pw-post-body-paragraph kp kq it kr b ks nh ju ku kv ni jx kx ky nj la lb lc nk le lf lg nl li lj lk im bi translated">所以，如果你看过我的YouTube视频，你可能会注意到如何使用这个模型非常简单。但是为了简化事情，我在<a class="ae ll" href="https://github.com/pythonlessons/background_removal" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上做了一个“背景_移除”的项目。你需要做的第一件事是从以下链接克隆或下载我的项目:<a class="ae ll" href="https://github.com/pythonlessons/background_removal.git" rel="noopener ugc nofollow" target="_blank">https://github.com/pythonlessons/background_removal.git</a>。</p><p id="8eb9" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我给出了如何在windows上快速测试它的说明，因为如果你使用的是Linux，我想你应该足够了解如何安装所有的要求:</p><ul class=""><li id="7d66" class="ma mb it kr b ks kt kv kw ky mc lc md lg me lk mf mg mh mi bi translated">安装Python虚拟环境:<code class="fe nm nn no np b">python -m venv venv</code>；</li><li id="cb66" class="ma mb it kr b ks mj kv mk ky ml lc mm lg mn lk mf mg mh mi bi translated">激活Python虚拟环境:<code class="fe nm nn no np b">venv\Script\activate</code>；</li><li id="290a" class="ma mb it kr b ks mj kv mk ky ml lc mm lg mn lk mf mg mh mi bi translated">安装所有要求:<code class="fe nm nn no np b">pip install -r requirements.txt</code>；</li><li id="1f52" class="ma mb it kr b ks mj kv mk ky ml lc mm lg mn lk mf mg mh mi bi translated">(如果有Nvidia GPU可选):安装有GPU支持的<code class="fe nm nn no np b">onnxruntime</code>:<code class="fe nm nn no np b">pip install onnxruntime-gpu</code>；</li></ul><p id="46c6" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">此时，当你阅读这篇文章时，我可能已经用更多的特性更新了这个项目，但是如果你只想在你的网络摄像头上运行一个快速测试，用下面的代码替换<code class="fe nm nn no np b">main.py</code>:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nq ko l"/></div></figure><p id="17fd" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">您可以通过在终端中键入<code class="fe nm nn no np b">python main.py</code>来运行它。</p><p id="812a" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">您应该会看到类似于“我的网络摄像头”视图的模糊背景:</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="ab gu cl lt"><img src="../Images/cb78b68d0e998fdb1af14196a4ef0bb8.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Zfa0X8WR5SITekboHUxuvg.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">来源:作者图片</figcaption></figure><p id="b354" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">简而言之，为了简单起见，我们使用三个(FPSmetric，MPSegmentation，Engine)不同的对象；FPSmetric并不重要，但很有价值。简而言之，我们用MPSegmentation对象定义了MediaPipe分段模型，我们只需向它提供帧就可以接收结果。FPSmetric对象为自己说话—计算每次调用迭代的FPS。而做所有艰苦工作的主要对象是发动机；有了它，我们可以处理图像、视频和网络摄像头输入。有关更多信息，请查看下面详细的代码块概述。</p><h1 id="27d6" class="nr mp it bd mq ns nt nu mt nv nw nx mw jz ny ka mz kc nz kd nc kf oa kg nf ob bi translated">详细的代码块概述:</h1><h2 id="d024" class="mo mp it bd mq mr ms dn mt mu mv dp mw ky mx my mz lc na nb nc lg nd ne nf ng bi translated">发动机:</h2><p id="dbb0" class="pw-post-body-paragraph kp kq it kr b ks nh ju ku kv ni jx kx ky nj la lb lc nk le lf lg nl li lj lk im bi translated">如您所见，引擎对象是完成所有繁重工作的主要对象。这个想法是，我们可以在不改变引擎代码的情况下添加自定义对象。应将附加处理添加到“<code class="fe nm nn no np b">custom_objects</code>”列表参数中。这些custom_objects在每个帧迭代中被调用；这意味着对象必须有一个“<code class="fe nm nn no np b">__call__</code>”功能。以下是完整的引擎代码:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nq ko l"/></div></figure><p id="87fe" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了避免解释代码的每一行，我记录了代码的关键部分。但是，仍然有一些你可能不清楚的功能。例如，“<code class="fe nm nn no np b">custom_processing</code>，这个函数被调用每一帧。在这里，我们循环遍历给定的"<code class="fe nm nn no np b">self.custom_objects</code>"列表对象，并将这些对象应用于框架。这里的想法是，我们可以提供一个图像路径，视频路径，或网络摄像头_id，引擎将开始处理它，当我们调用“<code class="fe nm nn no np b">run()</code>”函数。</p><p id="2352" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">还有一个是“<code class="fe nm nn no np b">display</code>”功能；这更适合在玩网络摄像机并希望看到结果时使用。此外，还编写了一些自定义代码来处理带有“<code class="fe nm nn no np b">a</code>”和“<code class="fe nm nn no np b">d</code>”键盘按钮的背景图像。</p><p id="7cff" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">主要功能有:</p><ul class=""><li id="fd0f" class="ma mb it kr b ks kt kv kw ky mc lc md lg me lk mf mg mh mi bi translated">“<code class="fe nm nn no np b">process_image</code>”-仅当我们在创建引擎对象时定义了一个“<code class="fe nm nn no np b">image_path</code>”时，才会调用该函数。它将处理后的图像保存在与原始图像相同的路径中；</li><li id="e2dc" class="ma mb it kr b ks mj kv mk ky ml lc mm lg mn lk mf mg mh mi bi translated">“<code class="fe nm nn no np b">process_webcam</code>”——这个函数每次都会被调用，并试图从我们用“<code class="fe nm nn no np b">webcam_id</code>”参数定义的网络摄像头中读取帧。它没有创造任何视频；它只是处理来自网络摄像头的画面并显示出来。您可以根据自己的需要随意更改此功能；</li><li id="5a22" class="ma mb it kr b ks mj kv mk ky ml lc mm lg mn lk mf mg mh mi bi translated">“<code class="fe nm nn no np b">process_video</code>”-这个函数处理视频文件，路径由“<code class="fe nm nn no np b">video_path</code>”参数定义。这个功能我用的最多。甚至我的YouTube视频也是在这个功能的帮助下，在去除背景的同时进行修改。</li></ul><h2 id="15c3" class="mo mp it bd mq mr ms dn mt mu mv dp mw ky mx my mz lc na nb nc lg nd ne nf ng bi translated">FPSmetric:</h2><p id="658e" class="pw-post-body-paragraph kp kq it kr b ks nh ju ku kv ni jx kx ky nj la lb lc nk le lf lg nl li lj lk im bi translated">另一个有用的对象用于测量推理速度，无论我们是处理保存的视频还是实时视频流，如网络摄像头流。下面的FPSmetric对象就是为此而使用的:</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nq ko l"/></div></figure><p id="59e8" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">如你所见，我们很少尝试计算单帧推断的FPS。通常，我们用range_average参数为这个对象中定义的某个范围执行此操作。其他的"<code class="fe nm nn no np b">__init__</code>"参数有待定义。"<code class="fe nm nn no np b">cv2.putText</code>"函数参数，用于样式框架中的文本。乍一看，您可以理解如果我们用“框架”输入调用这个对象，它是否会添加带有对象定义参数的文本。否则，该对象将返回计算出的FPS浮点值。我喜欢这个对象，因为我们不需要在代码内部手动实现任何自定义函数来计算FPS，每次迭代调用这个对象就足够了。</p><h2 id="2478" class="mo mp it bd mq mr ms dn mt mu mv dp mw ky mx my mz lc na nb nc lg nd ne nf ng bi translated">MP分段:</h2><p id="2b31" class="pw-post-body-paragraph kp kq it kr b ks nh ju ku kv ni jx kx ky nj la lb lc nk le lf lg nl li lj lk im bi translated">这个物体负责自拍图像分割。具体来说，我为<a class="ae ll" href="https://google.github.io/mediapipe/solutions/selfie_segmentation.html" rel="noopener ugc nofollow" target="_blank">媒体管道</a>解决方案创建了它。但是，如果您想要使用不同的自拍分段模型或实现，请修改或创建此对象的副本，并根据需要重新实现它。</p><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="nq ko l"/></div></figure><p id="b78e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">创建这个对象时，我们可以定义几个不同的参数，比如我们是使用通用推理模型还是景观类型。我们可以指定使用什么样的背景图像，甚至是图像列表，而不是模糊背景。阈值是一个边界参数，在分离背景和前景时给我们信心。为了获得最佳效果，有必要调整这个值。</p><p id="2eb4" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">主要逻辑保存在<code class="fe nm nn no np b">__call__</code>功能中。首先，我们将帧馈送给这个对象，其中定义的模型返回一个自拍分割1D遮罩。因为我们的图像是3D (RGB)的，所以我们将遮罩转换成相同的形状。接下来，我们检查我们是否已经定义了背景图像或背景颜色；如果没有，我们将使用模糊的背景，否则-给定的图像。在最后一步中，我们将原始图像中与分割掩模重叠的部分放在背景图像上。</p><h1 id="5298" class="nr mp it bd mq ns nt nu mt nv nw nx mw jz ny ka mz kc nz kd nc kf oa kg nf ob bi translated">结论:</h1><p id="035f" class="pw-post-body-paragraph kp kq it kr b ks nh ju ku kv ni jx kx ky nj la lb lc nk le lf lg nl li lj lk im bi translated">我希望你喜欢这个简短的自拍分割教程，并且明白把我们从背景中分离出来并不是火箭科学；当我们知道使用什么工具以及如何应用它们时，事情就更容易管理了。在这一部分中，我写了一个引擎框架，我们可以很容易地使用它和自定义对象来处理图像、视频或网络视频流！</p><p id="a39d" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我向您展示了如何创建对象来测量推理速度，最重要的是，如何使用媒体管道自拍分割模型，获得自拍分割结果并应用它们来模糊或改变我们的背景！</p><p id="5328" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在下一个教程中，我将创建另一个媒体管道人脸检测对象，我们将使用它来实时检测人脸。</p><p id="9dae" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">感谢阅读！一如既往，本教程给出的所有代码都可以在我的<a class="ae ll" href="https://github.com/pythonlessons/background_removal" rel="noopener ugc nofollow" target="_blank"> GitHub </a>页面找到，并且免费使用！</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="4518" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><em class="lp">原载于</em>【https://pylessons.com/remove-background】<em class="lp"/></p></div></div>    
</body>
</html>