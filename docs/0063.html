<html>
<head>
<title>One LEGO at a Time: Explaining the Math of how Neural Networks Learn with Implementation from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一次一个乐高:解释神经网络如何从零开始学习和实现的数学</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/one-lego-at-a-time-explaining-the-math-of-how-neural-networks-learn-with-implementation-from-scratch-39144a1cf80?source=collection_archive---------0-----------------------#2019-06-01">https://pub.towardsai.net/one-lego-at-a-time-explaining-the-math-of-how-neural-networks-learn-with-implementation-from-scratch-39144a1cf80?source=collection_archive---------0-----------------------#2019-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="835b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/mathematics" rel="noopener ugc nofollow" target="_blank">数学</a></h2><div class=""/><blockquote class="jz ka kb"><p id="f466" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la im bi translated"><em class="it">一个</em> <strong class="kf jd"> <em class="it">神经网络</em> </strong> <em class="it">是线性和非线性模块的巧妙排列。当我们明智地选择和连接它们时，我们就有了一个强大的工具来逼近任何数学函数。例如，</em> <strong class="kf jd"> <em class="it">用非线性决策边界</em> </strong> <em class="it">来分隔类别。</em></p></blockquote><p id="8238" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">运行代码的步骤:</p><ul class=""><li id="be94" class="le lf it kf b kg kh kk kl lb lg lc lh ld li la lj lk ll lm bi translated">git克隆<a class="ae ln" href="https://github.com/omar-florez/scratch_mlp/" rel="noopener ugc nofollow" target="_blank">https://github.com/omar-florez/scratch_mlp/</a></li><li id="1b06" class="le lf it kf b kg lo kk lp lb lq lc lr ld ls la lj lk ll lm bi translated">python scratch _ MLP/scratch _ MLP . py</li></ul><p id="7aa2" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">尽管具有直观和模块化的性质，但并不总是深入解释的一个主题是负责更新可训练参数的<strong class="kf jd">反向传播技术</strong>。让我们从零开始构建一个神经网络，以查看使用<strong class="kf jd">乐高积木作为模块化类比</strong>的神经网络的内部功能，一次一个砖块。</p><h1 id="248c" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">作为片段组合的神经网络</h1><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/39c0f0e5b383f54483695031ee2eb8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*t1v0WGPmfzwOAl26.png"/></div></div></figure><p id="b22b" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">上图描述了用于训练神经网络的一些数学方法。我们将在本文中解释这一点。读者可能会感兴趣的是，神经网络是具有不同目的的模块的堆叠:</p><ul class=""><li id="c5e6" class="le lf it kf b kg kh kk kl lb lg lc lh ld li la lj lk ll lm bi translated"><strong class="kf jd">输入X </strong>向神经网络输入原始数据，原始数据存储在矩阵中，矩阵中的观察值为行，维度为列</li><li id="6b8d" class="le lf it kf b kg lo kk lp lb lq lc lr ld ls la lj lk ll lm bi translated"><strong class="kf jd">权重W1 </strong>将输入X映射到第一个隐藏层h1。权重W1然后作为线性核工作</li><li id="9567" class="le lf it kf b kg lo kk lp lb lq lc lr ld ls la lj lk ll lm bi translated">一个<strong class="kf jd"> Sigmoid函数</strong>通过将隐藏层中的数字缩放到0-1来防止它们超出范围。结果是神经激活的<strong class="kf jd">阵列</strong> h1 = Sigmoid(WX)</li></ul><p id="25c3" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">在这一点上，这些操作只计算了一个<strong class="kf jd">一般线性系统</strong>，它不具备模拟非线性相互作用的能力。当我们再堆叠一层，增加这种模块化结构的深度时，这种情况就会改变。网络越深，我们就能学习到越微妙的非线性互动，也能解决越复杂的问题，这或许可以部分解释深度神经模型的兴起。</p><h1 id="6f3d" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">我为什么要看这个？</h1><blockquote class="jz ka kb"><p id="b48a" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la im bi translated"><em class="it">如果你了解神经网络的内部组成部分，你会很快知道当事情不奏效时首先要改变什么</em><strong class="kf jd"><em class="it"/></strong><em class="it">并定义一个策略来测试不变量</em>  <em class="it">和</em> <strong class="kf jd"> <em class="it">你所知道的预期行为</em> </strong> <em class="it">是算法的一部分。当您想要</em> <strong class="kf jd"> <em class="it">创建您正在使用的ML库</em> </strong> <em class="it">中当前没有实现的新功能时，这也会很有帮助。</em></p></blockquote><p id="0aa9" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated"><strong class="kf jd">因为调试机器学习模型是一项复杂的任务</strong>。根据经验，数学模型在第一次尝试时不会像预期的那样工作。它们可能会给你新数据的低准确率，花费很长的训练时间或太多的内存，返回大量的假阴性或NaN预测等。当知道算法如何工作变得很方便时，让我展示一些例子:</p><ul class=""><li id="b716" class="le lf it kf b kg kh kk kl lb lg lc lh ld li la lj lk ll lm bi translated">如果<strong class="kf jd">需要这么多时间来训练</strong>，那么增加迷你批次的大小来减少观察值的方差，从而帮助算法收敛，这可能是一个好主意</li><li id="0f62" class="le lf it kf b kg lo kk lp lb lq lc lr ld ls la lj lk ll lm bi translated">如果您观察到<strong class="kf jd"> NaN预测</strong>，该算法可能已经接收到产生内存溢出的大梯度。可以把这想象成多次迭代后爆发的连续矩阵乘法。降低学习率会降低这些值。减少层数将减少乘法次数。剪切渐变将明确地控制这个问题</li></ul><h1 id="f9d8" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">具体例子:学习异或函数</h1><blockquote class="jz ka kb"><p id="1383" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la im bi translated">让我们打开黑盒。我们现在将从头构建一个学习 <strong class="kf jd"> <em class="it">异或函数</em> </strong> <em class="it">的神经网络。这种</em> <strong class="kf jd"> <em class="it">非线性函数</em> </strong> <em class="it">的选择绝非偶然。如果没有反向传播，就很难学会用一条</em> <strong class="kf jd"> <em class="it">直线</em> </strong> <em class="it">来分隔类。</em></p></blockquote><p id="246f" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">为了说明这个重要的概念，请注意下面一条直线如何不能分隔异或函数的输出0和1。<strong class="kf jd">现实生活中的问题也是非线性可分的</strong>。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/72455b12af96bd8d1e8b75c2b20c2829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pDIM_869wPmuV322.png"/></div></div></figure><p id="9ed8" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">网络的拓扑结构很简单:</p><ul class=""><li id="c2da" class="le lf it kf b kg kh kk kl lb lg lc lh ld li la lj lk ll lm bi translated"><strong class="kf jd">输入X </strong>是一个二维向量</li><li id="3229" class="le lf it kf b kg lo kk lp lb lq lc lr ld ls la lj lk ll lm bi translated"><strong class="kf jd">权重W1 </strong>是一个具有随机初始化值的2×3矩阵</li><li id="2f56" class="le lf it kf b kg lo kk lp lb lq lc lr ld ls la lj lk ll lm bi translated"><strong class="kf jd">隐层h1 </strong>由三个神经元组成。每个神经元接收观察值的加权和作为输入，这是下图中用绿色突出显示的内积:<strong class="kf jd"> z1 = [x1，x2][w1，w2] </strong></li><li id="0235" class="le lf it kf b kg lo kk lp lb lq lc lr ld ls la lj lk ll lm bi translated"><strong class="kf jd">权重W2 </strong>是具有随机初始化值的3x2矩阵，并且</li><li id="b56b" class="le lf it kf b kg lo kk lp lb lq lc lr ld ls la lj lk ll lm bi translated"><strong class="kf jd">输出层h2 </strong>由两个神经元组成，因为XOR函数返回0 (y1=[0，1])或1 (y2 = [1，0])</li></ul><p id="9629" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">更直观:</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/92bb6b2ac5c24ab0089bf03009990311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GqP0Jq9vXBwtQ2qP.png"/></div></div></figure><p id="9948" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">现在让我们训练模型。在我们的简单示例中，可训练参数是权重，但是要知道，当前的研究正在探索更多类型的参数来进行优化。例如层间捷径、正则化分布、拓扑、残差、学习率等。</p><p id="d50c" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated"><strong class="kf jd">反向传播</strong>是一种向方向(<strong class="kf jd">梯度</strong>)更新权重的方法，该方法在给定一批标记的观察值的情况下，最小化被称为<strong class="kf jd">损失函数的预定义误差度量。这种算法已经被反复重新发现，并且是一种更一般的技术的特例，这种技术被称为反向累加模式中的<a class="ae ln" href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="noopener ugc nofollow" target="_blank">自动微分</a>。</strong></p><h1 id="132e" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">网络初始化</h1><blockquote class="jz ka kb"><p id="cec0" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la im bi translated"><em class="it">让我们用随机数初始化网络权重</em><strong class="kf jd"><em class="it"/></strong><em class="it">。</em></p></blockquote><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/d7e7c0e1c224dd45529295eabaa03f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vv66I7deNQp9i0uf.png"/></div></div></figure><h1 id="e6a6" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">向前一步:</h1><blockquote class="jz ka kb"><p id="67d5" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la im bi translated"><em class="it">这一步的目标是将</em> <strong class="kf jd"> <em class="it">正向传播</em> </strong> <em class="it">输入的X到网络的每一层，直到计算出输出层h2中的一个矢量。</em></p></blockquote><p id="7fdd" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">事情是这样发生的:</p><ul class=""><li id="f41c" class="le lf it kf b kg kh kk kl lb lg lc lh ld li la lj lk ll lm bi translated">使用权重W1作为内核来线性映射输入数据X:</li></ul><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nd"><img src="../Images/fb53c38211568617b1e873e642597bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fGnuKqFAsx8BHVDl.png"/></div></div></figure><ul class=""><li id="dc02" class="le lf it kf b kg kh kk kl lb lg lc lh ld li la lj lk ll lm bi translated">用Sigmoid函数缩放该加权和z1，以获得第一隐藏层h1的值。<strong class="kf jd">注意，原来的2D矢量现在被映射到一个三维空间</strong>。</li></ul><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/014b2e140368789afe4610041fd8038e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/0*0L6ymE4ZQlGQCaa1.png"/></div></figure><ul class=""><li id="fc9a" class="le lf it kf b kg kh kk kl lb lg lc lh ld li la lj lk ll lm bi translated">对于第二层h2，发生类似的过程。让我们首先计算第一个隐藏层的<strong class="kf jd">加权和</strong> z2，它现在是输入数据。</li></ul><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nf"><img src="../Images/c6dd8151a00265f48304c4c0d25c8030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ypy99tJyXovbIbF7.png"/></div></div></figure><ul class=""><li id="af4b" class="le lf it kf b kg kh kk kl lb lg lc lh ld li la lj lk ll lm bi translated">然后计算他们的Sigmoid激活函数。这个向量[0.37166596 0.45414264]代表网络给定输入x计算的<strong class="kf jd">对数概率</strong>或<strong class="kf jd">预测向量</strong></li></ul><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d44778da6f61d2b696f1d3ef3156dd38.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/0*aeM--sxYZxIgIiwx.png"/></div></figure><h1 id="2184" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">计算总损失</h1><blockquote class="jz ka kb"><p id="3689" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la im bi translated"><em class="it">又称“实际减去预测”，损失函数的目标是</em> <strong class="kf jd"> <em class="it">量化预测向量h2与人类提供的实际标签y </em> </strong> <em class="it">之间的距离。</em></p></blockquote><p id="3668" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">请注意，损失函数包含一个<strong class="kf jd">正则化组件</strong>，它像在岭回归中一样惩罚大的权重值。换句话说，大的平方权重值将增加损失函数<strong class="kf jd">，这是我们确实想要最小化的误差度量</strong>。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nh"><img src="../Images/cc6895535468d4a0553651525cfd33c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Nmp3gcVkS3U3zsCC.png"/></div></div></figure><h1 id="0f84" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">后退一步:</h1><blockquote class="jz ka kb"><p id="8e1c" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la im bi translated"><em class="it">这一步的目标是</em> <strong class="kf jd"> <em class="it">以最小化其损失函数的方向更新神经网络</em> </strong> <em class="it">的权重。正如我们将看到的，这是一个</em> <strong class="kf jd"> <em class="it">递归算法</em> </strong> <em class="it">，它可以重用先前计算的梯度，并且严重依赖于</em> <strong class="kf jd"> <em class="it">可微函数</em> </strong> <em class="it">。由于这些更新减少了损失函数，网络“学习”用已知类来近似观察的标签。一个叫做</em> <strong class="kf jd"> <em class="it">的属性概括了</em> </strong> <em class="it">。</em></p></blockquote><p id="39ba" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">这一步的顺序<strong class="kf jd">比前一步的顺序</strong>靠后。它首先计算损失函数相对于输出层权重(dLoss/dW2)的偏导数，然后计算隐藏层权重(dLoss/dW1)。让我们详细解释每一个。</p><h2 id="a4e2" class="ni lu it bd lv nj nk dn lz nl nm dp md lb nn no mh lc np nq ml ld nr ns mp iz bi translated">dLoss/dW2:</h2><p id="4e2f" class="pw-post-body-paragraph kc kd it kf b kg nt ki kj kk nu km kn lb nv kq kr lc nw ku kv ld nx ky kz la im bi translated">链式法则表示，我们可以将神经网络梯度的计算分解成<strong class="kf jd">个可微分的部分</strong>:</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ny"><img src="../Images/a1a8e00da59a9cc0a10df42782f76b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ogDsuGdZtyocFZ_X.png"/></div></div></figure><p id="20bb" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">作为内存助手，这些是上面使用的<strong class="kf jd">函数定义</strong>和它们的<strong class="kf jd">一阶导数</strong>:</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/696ddbfdfecc5f4de0b9d9bd5b107e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*krn9KKx2k5aVAZfuDVIiRw.png"/></div></figure><p id="590e" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">更直观地说，我们的目标是更新下图中的权重W2(蓝色)。为此，我们需要沿着链计算三个<strong class="kf jd">偏导数。</strong></p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oa"><img src="../Images/09ffff148194d8ac405573d4cb12af13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z1hefRZ3gk5i8ONO.png"/></div></div></figure><p id="b277" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">将值代入这些偏导数中，允许我们计算相对于权重W2的梯度，如下所示。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ob"><img src="../Images/8ad63f59f304ed6c6af6f76139811d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GUx4w1cUz6S4sKbl.png"/></div></div></figure><p id="7f62" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">结果是一个3×2矩阵dLoss/dW2，它将在最小化损失函数的方向上更新原始W2值。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/088c9de992d7191faa9d859db3836507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RVDs5blidcvSbz5n.png"/></div></div></figure><h2 id="0a1c" class="ni lu it bd lv nj nk dn lz nl nm dp md lb nn no mh lc np nq ml ld nr ns mp iz bi translated">dLoss/dW1:</h2><p id="1720" class="pw-post-body-paragraph kc kd it kf b kg nt ki kj kk nu km kn lb nv kq kr lc nw ku kv ld nx ky kz la im bi translated">计算用于更新第一隐藏层W1的权重的<strong class="kf jd">链规则</strong>展现了<strong class="kf jd">重用现有计算</strong>的可能性。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oc"><img src="../Images/0200484fcc9e8163259ca9ea6a7730e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FLhxovJrgw5U0BHs.png"/></div></div></figure><p id="5905" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">更直观地说，从输出层到权重W1 的<strong class="kf jd">路径接触到已经在<strong class="kf jd">后面的层</strong>中计算的偏导数。</strong></p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi od"><img src="../Images/c0b6e6dfebf039757d586c174f7a3967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qWesfR7Mv7M6dBeK.png"/></div></div></figure><p id="6d13" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">例如，在前面的部分中，偏导数dLoss/dh2和dh2/dz2已经被计算为用于学习输出层dLoss/dW2的权重的依赖性。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oe"><img src="../Images/9515658dfd0ab2d2451a47f254497181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O2H72iUIhupTm74z.png"/></div></div></figure><p id="25db" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">将所有导数放在一起，我们可以再次执行<strong class="kf jd">链规则</strong>来更新隐藏层W1的权重:</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/111e571add630f9e9d46259411f02b56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wOEh7Yp91qQlCTX-.png"/></div></div></figure><p id="e202" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">最后，我们将新值分配给权重，并完成了网络的训练步骤。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f5b76562b4fd4a903d100399d43e449d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/0*sa29N-Wq-6LZBsP5.png"/></div></figure><h1 id="3980" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">履行</h1><p id="d5d0" class="pw-post-body-paragraph kc kd it kf b kg nt ki kj kk nu km kn lb nv kq kr lc nw ku kv ld nx ky kz la im bi translated">让我们仅使用<a class="ae ln" href="http://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy </a>作为我们的<strong class="kf jd">线性代数引擎</strong>，将上述数学方程翻译成代码。神经网络在一个循环中被训练，其中每次迭代向网络呈现已经<strong class="kf jd">校准的输入数据</strong>。在这个小例子中，让我们只考虑每次迭代中的整个数据集。由于我们在每个周期中用相应的<strong class="kf jd">梯度</strong>(矩阵dL_dw1和dL_dw2)更新<strong class="kf jd">可训练参数</strong>(代码中的矩阵w1和w2)，因此<strong class="kf jd">向前步长</strong>、<strong class="kf jd">损失</strong>和<strong class="kf jd">向后步长</strong>的计算导致了良好的通用性。代码存储在这个库中:<a class="ae ln" href="https://github.com/omar-florez/scratch_mlp" rel="noopener ugc nofollow" target="_blank">https://github.com/omar-florez/scratch_mlp</a></p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi og"><img src="../Images/83181ffeea268f711db17329266e175f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*khNIaUP3GC70YqDh.png"/></div></div></figure><h1 id="56fd" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">让我们运行这个！</h1><p id="a573" class="pw-post-body-paragraph kc kd it kf b kg nt ki kj kk nu km kn lb nv kq kr lc nw ku kv ld nx ky kz la im bi translated">见下文<strong class="kf jd">一些神经网络</strong>被训练成在多次迭代中逼近<strong class="kf jd">异或函数。</strong></p><p id="321f" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated"><strong class="kf jd">左图:</strong>精度。<strong class="kf jd">中心情节:</strong>习得决定边界。<strong class="kf jd">右图:</strong>损失函数。</p><p id="a062" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">先来看看一个隐层有<strong class="kf jd"> 3个神经元</strong>的神经网络容量有多大。这个模型学习用一个简单的决策边界<strong class="kf jd">来区分两个类，这个简单的决策边界</strong>开始是一条直线，但随后显示出非线性行为。随着训练的继续，右图中的损失函数变得很低。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/2a7ff9fe09c803fe85d4bd578f372f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fw4FoDPrkpEQhgn3.gif"/></div></div></figure><p id="cc74" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">在隐藏层中拥有50个神经元显著增加了模型学习更多复杂决策边界的能力。这不仅可以产生更精确的结果，而且<strong class="kf jd">还可以分解梯度</strong>，这是训练神经网络时的一个显著问题。当非常大的梯度在反向传播期间乘以权重并因此产生大的更新权重时，会发生这种情况。这就是在训练的最后步骤(步骤&gt; 90)中<strong class="kf jd">损失值突然增加</strong>的原因。损失函数的<strong class="kf jd">正则化组件</strong>计算已经非常大的权重的<strong class="kf jd">平方值</strong>(sum(W)/2N)。</p><p id="242a" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn lb kp kq kr lc kt ku kv ld kx ky kz la im bi translated">这个问题可以通过<strong class="kf jd">降低学习率</strong>来避免，如下图所示。或者通过实施随着时间推移降低学习率的策略。或者通过加强监管，也许是L1而不是L2。<strong class="kf jd">爆炸</strong>和<strong class="kf jd">消失渐变</strong>是有趣的现象，我们将在后面进行完整的分析。</p></div></div>    
</body>
</html>