<html>
<head>
<title>OpenAI Threw Resources at Book Summarization Task (Paper Review/Explained)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI在图书摘要任务上投入资源(论文综述/解释)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/openai-threw-resources-on-book-summarization-task-paper-review-explained-70a7c755fb56?source=collection_archive---------1-----------------------#2021-10-08">https://pub.towardsai.net/openai-threw-resources-on-book-summarization-task-paper-review-explained-70a7c755fb56?source=collection_archive---------1-----------------------#2021-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7843" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><p id="656a" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">解释“用人类反馈递归总结书籍”这篇论文及其有效性。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi kx"><img src="../Images/bb7e0e8ee2c8a7d68642099d3a2f306f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z2UVX2dk4-eA7gmq"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">由<a class="ae ln" href="https://unsplash.com/@qmikola?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">miko aj</a>在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="2a7e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">你可能熟悉OpenAI的GPT家族模型和他们的第三代(GPT-3)具有1740亿个参数的预训练网络。众所周知，与(当时)其他研究相比，他们发表了大量预先训练好的模型。他们最新的论文专注于微调他们最新的文本摘要模型。先看看效果如何，更重要的是效果如何？</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi lo"><img src="../Images/c36198e203a4c3d21f358fc311d097da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aoodQlK2o6uSS47wbfqhVg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图一。摘自[1]</figcaption></figure><p id="f96c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">这篇论文[1]大量引用了OpenAI之前的一项研究，名为“从人类反馈中学习总结”[2]。他们似乎试图通过在网络中添加一个循环方案来处理长文本，从而扩展这项研究。他们使用<strong class="kb jd">任务分解</strong>的思想将大序列分解成更小的序列以形成一棵树，其中叶节点(高度0)是来自书的块，根节点(高度2)将是我们的最终摘要，如图1所示。高度为0的每个块将被汇总(由模型或人汇总)、连接并传递到下一个高度，在下一个高度，相同的过程发生以生成汇总的<strong class="kb jd">汇总，直到达到最终高度(例如，在这种情况下的高度2)。该论文提到，叶节点(高度0)通常由书中的600个标记组成，10-13个摘要在高度1处连接，而当我们在树中更高时，连接的摘要的数量可以减少到2个。此外，还有<strong class="kb jd">前一个上下文</strong>的概念，它发生在高度&gt; 0处，并从与生成下一个摘要的上下文相同的高度追加先前生成的摘要。(图1中颜色为蓝色)论文中的解释有点模糊。不过，我的理解是，只要不超过模型的输入限制，它会在每个高度和路径上发生。</strong></p><p id="4c47" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">使用任务分解方法有两个好处:1)模型在每个节点中执行相同的任务，这使得能够共享模型。2)人类将更容易对小块而不是整本书进行评估和反馈。这就把我们带到了论文的<strong class="kb jd">人类反馈</strong>部分。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi lp"><img src="../Images/a9f612d52c77dedae13edd72cc144343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T7GisKRv8_VbaCPnAtCo0Q.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图二。摘自[2]</figcaption></figure><p id="103c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">人工反馈步骤(如论文中所述)“严格遵循[2]中的程序”，它由图2所示的三个步骤组成。它基本上意味着从人类标签员那里收集数据，以训练一个奖励模型，在训练时用来模仿人类的偏好。同样，解释并不简单，但似乎他们只使用第一个子树(在图1中用黄色突出显示)来进行行为克隆。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h1 id="7905" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">结果和讨论</h1><p id="a35c" class="pw-post-body-paragraph jz ka it kb b kc mv ke kf kg mw ki kj kk mx km kn ko my kq kr ks mz ku kv kw im bi translated">他们将他们提出的架构与T5进行了比较，结果显示它可以将ROUGE得分全面提高3-4分。很难超越Oracle的提取基准，但这甚至不是一个公平的比较。(Abstractive vs. Extractive)让我们更深入地看看结果。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi na"><img src="../Images/6856b1ff21e61c4fdae0e407008101d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ad-85nZ-16rO8zoswknQmA.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图3。该模型的结果取自[1]</figcaption></figure><p id="3ac6" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">看到结果后，我想到的第一个问题是:将一个1750亿参数的模型与一个110亿参数的模型进行比较有意义吗？毫无疑问，如果你使用16倍大的模型，分数会提高！如果你仔细想想，考虑到投入了多少资源，3-4分可能并不令人印象深刻。</p><p id="8805" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">当你看到论文中没有有意义的消融研究时，就有点令人怀疑了！是的，有一个表格比较了不同的训练策略。但是，我们怎么知道提出的方法实际上更好呢！？最初的GPT-3[3]论文没有提出任何总结基准！该模型甚至没有公开供我们评估！如果单独使用GPT-3并将文章分成几块会更好呢？</p><p id="f457" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">现在，让我们看看这篇论文的两个局限性:[1]</p><ul class=""><li id="b406" class="nb nc it kb b kc kd kg kh kk nd ko ne ks nf kw ng nh ni nj bi translated"><strong class="kb jd">缺乏连贯性:</strong>引入512令牌输入限制的BERT [4]后，这是一个众所周知的问题。每个人都试图将序列分割成512个块，并分别输入到模型中。自然，生成的摘要根本不连贯。因此，经常计划无助于解决这个问题。</li><li id="6859" class="nb nc it kb b kc nk kg nl kk nm ko nn ks no kw ng nh ni nj bi translated"><strong class="kb jd">任务分解可能会从根本上限制</strong>:这个只是搞笑！当单词在文本中相距很远时，该算法无法找到它们之间的联系。嗯，处理长序列的主要问题仍然存在！</li></ul><p id="b694" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">此外，OpenAI似乎也对他们的实验有些怀疑。令人惊讶的是，他们甚至在他们的<a class="ae ln" href="https://openai.com/blog/summarizing-books/" rel="noopener ugc nofollow" target="_blank">博客文章</a>中使用了不同的标题和抽象概念【与论文相比】，专注于他们一年前发表的上一篇论文中提出的人类反馈方面，而不是当前论文的新颖性:<strong class="kb jd">是递归的</strong>！</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h1 id="4643" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">结论</h1><p id="7671" class="pw-post-body-paragraph jz ka it kb b kc mv ke kf kg mw ki kj kk mx km kn ko my kq kr ks mz ku kv kw im bi translated">这篇论文提出了一个可能行得通的新想法！但是，没有足够的证据支持它。请记住，我们甚至不确定是<strong class="kb jd">递归方案</strong>还是<strong class="kb jd">人类反馈</strong>增加了分数，或者只是更高的参数计数。当现实中仍有许多未解的问题时，看到对这篇论文的大肆宣传有点令人羞愧。</p><blockquote class="np"><p id="45db" class="nq nr it bd ns nt nu nv nw nx ny kw dk translated">我每周给NLP的书呆子发一份时事通讯。如果您想了解自然语言处理的最新发展，可以考虑订阅。<br/> <a class="ae ln" href="https://nlpiation.github.io/" rel="noopener ugc nofollow" target="_blank">阅读更多，订阅</a> —加入酷孩子俱乐部，立即报名！</p></blockquote><h1 id="6fa1" class="lx ly it bd lz ma nz mc md me oa mg mh mi ob mk ml mm oc mo mp mq od ms mt mu bi translated">参考</h1><p id="f26c" class="pw-post-body-paragraph jz ka it kb b kc mv ke kf kg mw ki kj kk mx km kn ko my kq kr ks mz ku kv kw im bi translated">[1]吴，j .，欧阳，l .，齐格勒，D. M .，斯蒂农，n .，洛，r .，j .，&amp;克里斯蒂亚诺，P. (2021)。递归总结带有人类反馈的书籍。<em class="oe"> arXiv预印本arXiv:2109.10862 </em>。<br/>【2】stien non，n .，欧阳，l .，吴，j .，齐格勒，D. M .，Lowe，r .，Voss，c .，… &amp; Christiano，P. (2020)。学会从人类反馈中总结。<em class="oe"> arXiv预印本arXiv:2009.01325 </em>。<br/>【3】布朗、T. B .、曼恩、b .、莱德、n .、苏比亚、m .、卡普兰、j .、达里瓦尔、p .……&amp;阿莫代伊，D. (2020)。语言模型是一次性学习者。arXiv预印本arXiv:2005.14165 。<br/>【4】Devlin，j .，Chang，M. W .，Lee，k .，&amp; Toutanova，K. (2018)。Bert:用于语言理解的深度双向转换器的预训练。<em class="oe"> arXiv预印本arXiv:1810.04805 </em>。</p></div></div>    
</body>
</html>