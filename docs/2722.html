<html>
<head>
<title>Step by Step Guide on Web Scraping Using Scrapy In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中使用Scrapy的Web抓取分步指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/web-scraping-in-scrapy-c2d87796f677?source=collection_archive---------0-----------------------#2022-05-02">https://pub.towardsai.net/web-scraping-in-scrapy-c2d87796f677?source=collection_archive---------0-----------------------#2022-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="577c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何检索新加坡的二手车信息</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6c6fb2f558c07533eb0c8b7c7dd51949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pMbOxsUfLROT4L4dcaadYg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">泰勒·弗兰塔在<a class="ae ky" href="https://unsplash.com/s/photos/web?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="1302" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我以前的一篇文章中，我介绍了使用请求&amp; BeautifulSoup/website API的web抓取策略。在这篇文章中，我想介绍一个更先进的应用程序框架，用于抓取网站内容和提取结构化数据，称为Scrapy。</p><h1 id="f8d0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Scrapy的利弊</h1><p id="4af8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">与美人汤等其他刮痧方式相比，刺痒有以下<strong class="lb iu">优势:</strong></p><ol class=""><li id="bafa" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">代码更加<strong class="lb iu">可扩展和灵活</strong>，更容易改变为更大规模的抓取或者迎合网站结构的变化。</li><li id="60fb" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">Scrapy有一个标准的代码结构和可以遵循的最佳实践</li><li id="8027" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">刺儿头有一个<strong class="lb iu">快</strong>的爬行速度</li></ol><p id="efc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ol class=""><li id="4d46" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">对于初学者来说，Scrapy的学习曲线稍微陡峭一些</li><li id="9e6b" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">您不能像普通的Jupyter笔记本解决方案那样看到每一步的输出(但是，有一个终端测试功能可用)</li></ol><blockquote class="ng nh ni"><p id="88ff" class="kz la nj lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated">正如<!-- -->你所看到的，Scrapy解决方案的唯一主要缺点是陡峭的学习，但是一旦你实现了这个解决方案，你就可以很容易地将它应用到任何其他的Scrapy网站。</p></blockquote><h1 id="5319" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">网站的哪个部分可以抓取？</h1><p id="a8c9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们不允许自由抓取所有网站。这是因为过多的网站流量可能会导致服务器崩溃，公司可能希望保护自己的数据。怎么知道能不能刮到这个网站？只需在<strong class="lb iu">主目录</strong>后添加<strong class="lb iu"> /robots.txt </strong>即可查看该信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/43335a02e240421cedce728c4496deb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*kdGwesuxXovJoQ7yz9S1NA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">sgcarmart.com的robots.txt文件—<a class="ae ky" href="https://www.sgcarmart.com/robots.txt" rel="noopener ugc nofollow" target="_blank">https://www.sgcarmart.com/robots.txt</a></figcaption></figure><p id="1517" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你从上面的截图中看到的，以sgcarmart.com为例，有些用户代理不允许抓取这个网站的任何子目录，如“Googlebot-Image”。但是，如果您使用“Applebot”作为您的用户代理，您可以使用超时5 来<strong class="lb iu">抓取某些子目录(不允许抓取那些不允许的目录)。如果你使用grapeshot，你可以在没有超时的情况下抓取子目录(更快的抓取速度)。</strong></p><h1 id="7544" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使用Scrapy进行网页抓取的步骤</h1><p id="9abd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Scrapy的官方文档/教程可以在https://docs.scrapy.org/en/latest/的<a class="ae ky" href="https://docs.scrapy.org/en/latest/" rel="noopener ugc nofollow" target="_blank">找到</a></p><p id="365c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一步:安装Scrapy包</strong></p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="107e" class="nt lw it np b gy nu nv l nw nx"># install at terminal<br/>pip install Scrapy</span></pre><p id="77ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者可以参考安装指南:【https://docs.scrapy.org/en/latest/intro/install.html T2】</p><p id="2eb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二步:创建Scrapy项目</strong></p><p id="591b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在终端中，找到要存储抓取代码的文件夹，然后键入</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="43f6" class="nt lw it np b gy nu nv l nw nx">scrapy startproject &lt;project_name&gt;</span></pre><p id="4627" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，您应该用您的项目名称替换<project_name>。在这里，我创建了一个名为“scraping_demo”的新项目</project_name></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/5d86309ba5802b5a18f4da1d1aa4e1be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qiZ3ZbBEybSf_-r4XmPi4w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">Scrapy startproject</figcaption></figure><p id="9799" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它将创建一个如下所示结构的文件夹</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/d3b282cca4fe32fa9a11428e1ce838bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EnF6pekIqvDwadR9j1uXfw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">Scrapy项目的文件夹结构</figcaption></figure><p id="c7f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的大多数文件都是预先配置好的，你不需要接触它们。您可以签出的第一个文件是settings.py</p><div class="kj kk kl km gt ab cb"><figure class="oa kn ob oc od oe of paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/9c2cb81f5ec085389795e50f06f411da.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*o621QWaPfMZz6cJFOa4HJw.png"/></div></figure><figure class="oa kn og oc od oe of paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/39f8ab447d38f8a399ee8d377e432747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*iKc4y9o79RL82M6mcmk8-Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk oh di oi oj translated">左(原始settings.py) —右(我的修改版settings.py)</figcaption></figure></div><p id="5437" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的截图可以看出，最初创建的settings.py将只是遵循robots.txt规则，没有任何设置或用户代理。您可以通过定义您的用户代理和设置download_delay 来修改它(根据网站规则的规定)</p><p id="fac8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该修改的另一个文件是<strong class="lb iu">在spiders文件夹</strong>中创建scraper python代码。目前，该文件夹中只有一个init.py文件，您可以在该文件夹中创建多个scrapers文件，并在不同的场景中使用它们，这将是我们的第3步。</p><h2 id="8f04" class="nt lw it bd lx ok ol dn mb om on dp mf li oo op mh lm oq or mj lq os ot ml ou bi translated">步骤3:在spiders文件夹下创建scraper代码</h2><p id="824d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这一步是你写刮刀的主要步骤。首先需要在spiders文件夹下创建一个py文件。之后，你可以参考下面的例子(来自Scrapy主网站)获得一个基本的代码结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/79a7499e9efe1c5eb7ba27fd5c16fe4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Rxt0-JA3p91wvHWMJZjyg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">Scrapy代码示例—修改自https://docs.scrapy.org/en/latest/intro/tutorial.html<a class="ae ky" href="https://docs.scrapy.org/en/latest/intro/tutorial.html" rel="noopener ugc nofollow" target="_blank">的示例</a></figcaption></figure><p id="e8df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在您创建的新py文件中，您需要定义scraper类，然后定义Scrapy scraper的名称。这是因为您可以在spiders文件夹中创建多个刮刀。当你运行刮刀时，你应该使用名称来区分，所以你需要确保不同刮刀中的<strong class="lb iu">名称是不同的</strong>。</p><p id="08e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之后，基本结构需要两个函数‘start _ requests’和‘parse’。“start_requests”类似于Python中请求库的角色，它会向您定义的网站URL发出请求。请注意两件事，这里你没有在函数中返回，而是<strong class="lb iu"> yield，这意味着你继续这个步骤</strong>。在请求函数中，您需要定义URL和<strong class="lb iu">回调函数</strong>，该函数将网站信息转换为结构化内容并返回到该函数。</p><p id="d2d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一个函数parse函数类似于BeautifulSoup的作用，就是解析网站内容。您可以通过CSS或XPath解析内容。你可以参考更多关于https://docs.scrapy.org/en/latest/topics/selectors.html的选择器的信息。在这里，您可以直接输出数据，或者稍后使用命令行输出。</p><blockquote class="ng nh ni"><p id="7303" class="kz la nj lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated">你可以使用scrapy shell函数来测试刮刀选择器。首先，你应该在终端中定位到你的项目文件夹，放入<strong class="lb iu">‘scrapy shell&lt;你想要抓取的URL&gt;’</strong>。</p><p id="8ec6" class="kz la nj lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated">之后，如果状态为成功，可以使用response对象来测试您的解析，比如' response.xpath('//title/text()')。get()'是否获得了您期望的正确输出。更多细节可以参考<a class="ae ky" href="https://docs.scrapy.org/en/latest/topics/shell.html" rel="noopener ugc nofollow" target="_blank">https://docs.scrapy.org/en/latest/topics/shell.html</a></p></blockquote><h2 id="8f08" class="nt lw it bd lx ok ol dn mb om on dp mf li oo op mh lm oq or mj lq os ot ml ou bi translated">最后一步:在终端运行Scrapy刮刀</h2><p id="eff3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最后一步很简单，只需在终端中找到你的项目文件夹，并写下你为scraper &gt; ' 定义的名字<strong class="lb iu"> 'scrapy crawl &lt;。如果您正在抓取大量页面，该步骤可能会花费相当长的时间，并且抓取速度也<strong class="lb iu">很大程度上由您在settings.py文件中定义的超时</strong>决定。</strong></p><h1 id="a82e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">案例研究:抓取新加坡汽车信息</h1><h2 id="b3cd" class="nt lw it bd lx ok ol dn mb om on dp mf li oo op mh lm oq or mj lq os ot ml ou bi translated">数据源</h2><p id="bbdf" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们使用sgcarmart.com作为数据源进行抓取。正如我之前展示的，sgCarMart允许一些用户代理抓取网站的某些部分。</p><h2 id="fcaf" class="nt lw it bd lx ok ol dn mb om on dp mf li oo op mh lm oq or mj lq os ot ml ou bi translated">问题陈述</h2><p id="6646" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们想找到目前平台上卖的二手车有哪些。它们的价格、标题和其他信息是什么？根据这些信息，我们可以发现目前影响二手车价格的重要因素是什么，以及市场上现有汽车的总体趋势是什么。</p><h2 id="1a37" class="nt lw it bd lx ok ol dn mb om on dp mf li oo op mh lm oq or mj lq os ot ml ou bi translated">解决办法</h2><ol class=""><li id="98bb" class="ms mt it lb b lc mn lf mo li ow lm ox lq oy lu mx my mz na bi translated">首先，我们将使用“scrapy startproject sgcarmart”命令来设置项目文件夹</li><li id="0b4d" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">我们查看settings.py文件，并根据robots.txt相应地更改user_agent和download_delay。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/241f9cef4e3bb6682bd5a8d56c19b171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FpME5dleK4kEmmZKG0_z3Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">sgcarmart的settings.py</figcaption></figure><p id="8126" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.让我们打开网页，看看我们如何抓取</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/f9d69e43923a8f39cddedaf07a00b151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ip1fuu4MnmRb90eauccGjQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">sgcarmart.com所有可用的汽车</figcaption></figure><p id="3159" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上是所有可用二手车的sgcarmart列表页面。在检查页面之前，我们可以获得一些有用的信息。</p><ul class=""><li id="15b2" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu pb my mz na bi translated">URL会告诉你什么是汽车的过滤标准，以及它正在爬哪个页面。例如，BRSR表示当前页面索引，RPG表示该页面上汽车列表的数量，我们可以迭代地将BRSR增加20来抓取所有页面。你可以在不同页面的一些探索之后得到这个。</li><li id="77c4" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu pb my mz na bi translated">有14k个汽车列表和730个页面要抓取，这意味着我们将基于BRSR值迭代检索所有730个页面的URL</li></ul><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="9e5e" class="nt lw it np b gy nu nv l nw nx">def generate_urls(self):<br/>  urls = []<br/>  #730 pages to crawl<br/>  for i in range(0, 731):<br/>    urls.append(f'https://www.sgcarmart.com/used_cars/listing.php?AVL=2&amp;BRSR={i * 20}&amp;RPG=20&amp;VEH=0')<br/>    return urls</span><span id="96ff" class="nt lw it np b gy pc nv l nw nx">#making requests to all 730 page url<br/>def start_requests(self):<br/>  for url in self.generate_urls():<br/>    yield scrapy.Request(url=url, callback=self.parse)</span></pre><ul class=""><li id="5ec8" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu pb my mz na bi translated">每个汽车列表在主页上都有一些信息，如价格或折旧，但我们需要反复点击每个汽车列表以了解更多细节。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/9d5f9a67b08687c8beb743b0017dcf95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ufIIs2Fcg3-F-cp-eD_mRA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">检查每个列表url的页面</figcaption></figure><p id="5127" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您检查页面，您可以看到单个URL存储在强括号的href属性中。让我们用Scrapy shell方法来测试我们是否能得到正确的元素。</p><p id="ddd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将输入到下面的终端，记住在您想要测试的URL之外添加“”</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="4787" class="nt lw it np b gy nu nv l nw nx">scrapy shell '<a class="ae ky" href="https://www.sgcarmart.com/used_cars/listing.php?BRSR=40&amp;RPG=20&amp;AVL=2&amp;VEH=0" rel="noopener ugc nofollow" target="_blank">https://www.sgcarmart.com/used_cars/listing.php?BRSR=40&amp;RPG=20&amp;AVL=2&amp;VEH=0</a>'</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/540d7dba9c2f92f052ce47b6754c313b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4cA2SEeQOHrtWBPl-tb0Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">scrapy shell后的成功页面</figcaption></figure><p id="ecb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果成功，您可以看到上面的页面，您可以看到状态为200。之后可以尝试使用CSS或者XPath来获取想要的信息。在我的例子中，我想获得所有需要的URL。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/990e42b49fe14fcfe8a37d000f4381b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s0a-7mZiCnOYTeS0egQtPQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">应用CSS选择器后的结果</figcaption></figure><p id="59ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以从上面的截图中看到，一旦你应用了CSS，你就可以得到所有的URL。由于广告列表或过期的URL，会有一些干扰数据，但是您可以使用一个简单的if语句来删除它们。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="37ad" class="nt lw it np b gy nu nv l nw nx">#parse all main page information and go to each car detail page<br/>def parse(self, response):<br/>  # follow links to car listings<br/>  for href in response.css('strong a::attr(href)'):<br/>    # filter out the advertisement links<br/>    if 'info.php' in href.extract():<br/>      url = response.urljoin(href.extract())<br/>      yield scrapy.Request(url, callback=self.parse_car)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/eb4d520e2f13006aed5041820f90c616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEccP_E7K6lwPNkd1YEbBw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">汽车详细信息页面</figcaption></figure><p id="0038" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您打开任何汽车详细信息页面，您可以看到所有信息都以表格格式存储，您可以使用相同的scrapy shell方法测试并获得正确的数据元素。这里我使用一个字典来存储所有需要的数据字段，并一起输出整个字典。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="082a" class="nt lw it np b gy nu nv l nw nx">#parse each car detail page information and retrieve all necessary information<br/>def parse_car(self, response):<br/>  result = dict()<br/>  result['title'] = response.css('#toMap a::text').extract()[0].strip()<br/>  result['price'] = response.css('.font_red a strong::text').extract()<br/>  result['depreciation']= response.css('#carInfo tr')[1].css('td')[1].css('::text')[0].extract().strip()<br/>  result['mileage']=response.css('#carInfo tr')[2].css('td')[0].css('div.eachInfo')[0].css('div.row_info::text').extract()[0].strip()<br/>  result['road_tax']=response.css('#carInfo tr')[2].css('td')[0].css('div.eachInfo')[1].css('div.row_info::text').extract()[0].strip()<br/>  result['dereg_value']=response.css('#carInfo tr')[2].css('td')[0].css('div.eachInfo')[2].css('div.row_info::text').extract()[0].strip()<br/>  result['coe']=response.css('#carInfo tr')[2].css('td')[0].css('div.eachInfo')[3].css('div.row_info::text').extract()[0].strip()<br/>  result['engine_cap']=response.css('#carInfo tr')[2].css('td')[0].css('div.eachInfo')[4].css('div.row_info::text').extract()[0].strip()<br/>  result['curb_weight']=response.css('#carInfo tr')[2].css('td')[0].css('div.eachInfo')[5].css('div.row_info::text').extract()[0].strip()<br/>  if len(response.css('#carInfo tr')[2].css('td')[0].css('div.eachInfo'))&gt;6:<br/>    result['vehicle_type']=response.css('#carInfo tr')[2].css('td')[0].css('div.eachInfo')[6].css('div.row_info a::text').extract()[0].strip()<br/>  else:<br/>    result['vehicle_type']=response.css('#carInfo tr')[3].css('td a::text').extract()[0].strip()<br/>  result['url'] = response.url<br/>  yield result</span></pre><p id="12a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.在我们理解了页面结构和元素之后，让我们享受编写scraper的乐趣</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/0a9b413432c21a40a01271339e2d0618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gSfNc6pL9mn7tTwo4Rut4Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">spiders文件夹下sgcarmart.py的代码</figcaption></figure><p id="bb37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.最后，在终端中，我们定位到项目文件夹，然后键入</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="a087" class="nt lw it np b gy nu nv l nw nx">scrapy crawl new_car -t csv -O ./output/result.csv</span></pre><p id="c82c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们希望将所有结果保存到同一目录下output文件夹下的CSV文件中。“t csv”表示我们希望将结果转换为csv格式，-o会覆盖现有文件，而-O只会将数据追加到底部。你也可以直接在scraper代码中定义输出文件格式。</p><p id="2321" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你抓取了所有的页面，运行起来将会花费相当长的时间。在这里，我只是测试运行抓取2个页面，它将生成结构化的CSV文件如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/e7f0786efc1dc88d99a3bd5d1e27a331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6crWlSFvmPglGUTtYBnuzw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">result.csv —搜索结果</figcaption></figure><h1 id="bff0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">最后建议和结论</h1><p id="9a41" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这篇文章旨在介绍Scrapy作为网络爬行的最佳实践之一。爬行代码格式通常很标准，但更重要的是，在开始编码之前，我们应该理解网站的结构。该结构包括诸如要抓取哪些URL、要抓取哪些数据字段、不同页面上的网站布局是否有变化等信息。</p><p id="9e67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在网站元素发生变化的情况下，您需要使用case when语句或其他一些方法来确保最终的输出是结构化的和准确的。此外，<strong class="lb iu">大多数数据字段还需要经过另一轮数据清理，然后才能真正使用</strong>，就像您需要将道路税$959/yr转换为数值959。</p><p id="9232" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nj">吨感谢朋友们</em> <a class="pj pk ep" href="https://medium.com/u/b9b791def441?source=post_page-----c2d87796f677--------------------------------" rel="noopener" target="_blank"> <em class="nj">叶</em> </a> <em class="nj">和</em> <a class="pj pk ep" href="https://medium.com/u/9ae06bd367f?source=post_page-----c2d87796f677--------------------------------" rel="noopener" target="_blank"> <em class="nj">刘政</em> </a> <em class="nj">对代码的帮助！</em></p><p id="7054" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你是网络抓取的新手，想学习更多关于CSS选择器或XPath选择器的知识，你可以从一些在线课程如datacamp学习，或者直接参考下面的w3school链接:</p><p id="f538" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.w3schools.com/cssref/css_selectors.asp" rel="noopener ugc nofollow" target="_blank">https://www.w3schools.com/cssref/css_selectors.asp</a></p><p id="1585" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.w3schools.com/xml/xpath_intro.asp" rel="noopener ugc nofollow" target="_blank">https://www.w3schools.com/xml/xpath_intro.asp</a></p><p id="6416" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对使用BeautifulSoup或direct API(如果有的话)进行网络抓取感兴趣，你也可以参考我下面的另一篇文章。感谢您的阅读，祝您有美好的一天！</p><div class="pl pm gp gr pn po"><a href="https://towardsdatascience.com/web-scraping-basics-82f8b5acd45c" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">网页抓取基础知识</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">如何用Python从网站上抓取数据</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc ks po"/></div></div></a></div></div></div>    
</body>
</html>