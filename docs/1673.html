<html>
<head>
<title>Teaching AI to See Like a Human</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教人工智能像人类一样看东西</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/teaching-ai-to-see-like-human-36304b0fa642?source=collection_archive---------3-----------------------#2021-03-15">https://pub.towardsai.net/teaching-ai-to-see-like-human-36304b0fa642?source=collection_archive---------3-----------------------#2021-03-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0b50" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="ed1f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">DeepMind生成查询网络可以在导航视觉环境时推断知识。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f4e650e305f643102bced7fda88d73e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*siMc9lIRjwb4pVHp"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://www.weareworldquant.com/en/thought-leadership/understanding-images-computer-vision-in-flux/" rel="noopener ugc nofollow" target="_blank"> WORLDQUANT </a></figcaption></figure><blockquote class="li lj lk"><p id="6baa" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过70，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">该序列解释了主要的机器学习概念，让你与最相关的项目和最新的…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="7125" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">一幅图像胜过千言万语，这句古老的名言道出了视觉分析在人类学习过程中的重要性。每当我们看到一个视觉场景，我们的大脑就会对其中的物体及其背景性质做出成千上万的推断。例如，如果我们看到一个人坐着，我们会推断他下面有一把椅子。即使我们看不到物体，视觉推理也能起作用。例如，如果我们在卧室里看到一个壁橱，我们会假设里面有衣物，即使我们看不到它们。作为一种认知技能，视觉推理是记忆、计划或想象等其他能力的基础。</p><p id="0db5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">这些视觉认知任务对人脑来说毫不费力，但在人工智能(AI)代理中却难以复制。今天，视觉识别模型依赖于大数据集的标记图像来完成基本任务，如物体识别。生成这些带标签的数据集是一个劳动密集型的过程，并且通常情况下，它无法捕捉场景的许多上下文方面，如灯光位置、观看者的视角或对象之间的关系。人工智能视觉识别模型的下一个前沿是让人工智能代理从视觉场景中学习，进行类似于人类的推理。这是DeepMind在科学杂志上发表的2018年<a class="ae lh" href="http://science.sciencemag.org/content/360/6394/1204.full?ijkey=kpkRRXA1ckHD6&amp;keytype=ref&amp;siteid=sci" rel="noopener ugc nofollow" target="_blank">研究论文</a>的主题。</p><p id="b71a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在标题<a class="ae lh" href="http://science.sciencemag.org/content/360/6394/1204.full?ijkey=kpkRRXA1ckHD6&amp;keytype=ref&amp;siteid=sci" rel="noopener ugc nofollow" target="_blank">“自然场景表示和渲染”</a>下，DeepMind引入了生成查询网络(GQN)的概念，这是一种能够创建AI智能体的模型，当它们在视觉场景中导航时，可以从周围环境中学习。GQN模型由两个神经网络组成:一个表示网络和一个生成网络。表示网络将代理的观察结果作为输入，并产生一个神经场景表示，该表示对关于底层场景的信息进行编码。每一个额外的观察都在同一表现中积累了关于场景内容的进一步证据。然后，生成网络从任意查询角度预测场景，必要时使用随机潜在变量在其输出中创建可变性。这两个网络以端到端的方式被联合训练，以最大化生成将从查询视点观察到的地面实况图像的可能性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/81806afbe4e51e7d86df4489eafd358b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ueL0R3GOSnxlI1LPo7wt1g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://science.sciencemag.org/content/360/6394/1204.full?ijkey=kpkRRXA1ckHD6&amp;keytype=ref&amp;siteid=sci" rel="noopener ugc nofollow" target="_blank">科学</a></figcaption></figure><p id="041e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">GQN模型的主要贡献是基于这样的事实，即表示网络不知道生成网络将被查询来预测哪些视点。因此，表示网络将产生包含所有信息(例如，对象身份、位置、颜色、数量和房间布局)的场景表示，这些信息是生成器网络做出准确预测所必需的。在训练过程中，生成器网络学习环境中典型对象之间的关系，这允许它从表示网络接收描述并填充细节。例如，表示网络将简洁地将“蓝立方”表示为一小组数字，并且生成网络将知道如何从特定视点将其自身表示为像素。下图显示了一个GQN模型，该模型分析了一个带有特定对象的空间房间。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/fb71b466487a8014a425ebd14a638f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xAkq442B7p4ucUXSBwpeTw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://science.sciencemag.org/content/360/6394/1204.full?ijkey=kpkRRXA1ckHD6&amp;keytype=ref&amp;siteid=sci" rel="noopener ugc nofollow" target="_blank">科学</a></figcaption></figure><p id="d83d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">为了测试GQNs的有效性，DeepMind团队使用生成的3D环境进行了一系列实验，其中多个对象具有任意位置、颜色、形状和纹理。实验突出了GQNs的一些显著特性。例如，GQN模型表明，当视觉场景中的某些内容不是真正可见时，它们能够解释视觉场景中的不确定性。例如，下面的GQN模型通过其预测的可变性来表达其不确定性，这种可变性随着其在迷宫中的移动而逐渐减少。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nf"><img src="../Images/9a4b0999f267bdcde799665663fc8ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*WFwtdqt2btU8GQ9o-Njnuw.gif"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://science.sciencemag.org/content/360/6394/1204.full?ijkey=kpkRRXA1ckHD6&amp;keytype=ref&amp;siteid=sci" rel="noopener ugc nofollow" target="_blank">科学</a></figcaption></figure><p id="b94d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">类似地，GQN模型能够从新的视角“想象”未被观察到的场景。如下图所示，当给定场景表示和新的相机视点时，GQN模型能够生成清晰的图像，而无需事先指定任何透视法则。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ng"><img src="../Images/c9e5f9f5359c3d96c7daf3cdebef0cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wvMiqkaySyXNZSDIhtkqAg.gif"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://science.sciencemag.org/content/360/6394/1204.full?ijkey=kpkRRXA1ckHD6&amp;keytype=ref&amp;siteid=sci" rel="noopener ugc nofollow" target="_blank">科学</a></figcaption></figure><p id="5f99" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">GQNs是一种非常新颖的技术，与传统的深度学习图像分析方法相比，它仍然有许多局限性。然而，GQNs表明，在没有任何人类标记这些场景内容的情况下，人工智能代理可以感知、解释和表示合成场景，这代表了图像分析方法演变的一个重大突破。</p></div></div>    
</body>
</html>