<html>
<head>
<title>Introduction to Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入简介</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/introduction-to-word-embedding-5ba5cf97d296?source=collection_archive---------1-----------------------#2020-08-04">https://pub.towardsai.net/introduction-to-word-embedding-5ba5cf97d296?source=collection_archive---------1-----------------------#2020-08-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0ed1" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="53e3" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">利用深度学习实现自然语言处理中的单词嵌入。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/be9283e9f626cb809a96d1b4827d72d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bju1447SMWNgUbpglNi5yQ.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来自Kaggle的单词云</figcaption></figure><p id="1e4b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">单词嵌入是一种通过低维向量获取单词“含义”的方法，可以用于自然语言处理的各种任务中。</p><p id="80b0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在开始单词嵌入教程之前，我们应该了解一下<strong class="lj jd">向量空间</strong>和<strong class="lj jd">相似度矩阵</strong>。</p><h1 id="efe7" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">向量空间</h1><p id="33f8" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">用于识别空间中一个点的数字序列被称为<strong class="lj jd">向量</strong>，如果我们有一整串向量都属于同一个数据集，它将被称为<strong class="lj jd">向量空间</strong>。</p><p id="3f39" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">文本中的单词也可以在向量空间的更高维度中表示，其中具有相同含义的单词将具有相似的表示。举个例子，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/80dbfb8d5c854a4f8f735c76be2690b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*LzE0D3ISbwJl3uwAPEDXjg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片来自Github</figcaption></figure><p id="09a9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上图显示了动物可爱程度和大小的矢量表示。我们可以看到，在相似性质的基础上，词与词之间存在着语义关系。很难描述单词之间的高维度关系，但是背后的数学原理是一样的，所以在高维度中也是一样的。</p><h1 id="7d61" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">相似度矩阵</strong></h1><p id="958b" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">它用于计算向量空间中向量之间的距离。它测量向量空间中两个数据点之间的相似性或距离。这使我们能够捕捉到以相似方式使用的单词，从而使相似的表达自然地捕捉到它们的意思。有许多相似性矩阵可用，但我们将讨论<strong class="lj jd">欧几里德距离</strong>和<strong class="lj jd">余弦相似性。</strong></p><h2 id="fca3" class="nb me it bd mf nc nd dn mj ne nf dp mn lq ng nh mp lu ni nj mr ly nk nl mt iz bi translated">欧几里得距离</h2><blockquote class="nm nn no"><p id="89a2" class="lh li np lj b lk ll kd lm ln lo kg lp nq lr ls lt nr lv lw lx ns lz ma mb mc im bi translated">计算两个数据点在向量空间中有多远的一种方法是计算欧几里德距离。</p></blockquote><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="f928" class="nb me it nu b gy ny nz l oa ob"><strong class="nu jd">import</strong> <strong class="nu jd">math</strong><br/><strong class="nu jd">def</strong> distance2d(x1, y1, x2, y2):<br/>    <strong class="nu jd">return</strong> math.sqrt((x1 - x2)**2 + (y1 - y2)**2)</span></pre><p id="4b20" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，上面图像示例中“水豚”(70，30)和“熊猫”(74，40)之间的距离:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/74df11866d545a337b871135b5ca8240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xp34mR3evhDaep0uRxPP1A.png"/></div></div></figure><p id="c121" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">…小于上图示例中“狼蛛”和“大象”之间的距离:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/2aad008f1f963a99f949fd4129af28ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwrszUgMOvsNZtV6-cOHjg.png"/></div></div></figure><p id="e932" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这表明“熊猫”和“水豚”比“狼蛛”和“大象”更相似。</p><h2 id="b411" class="nb me it bd mf nc nd dn mj ne nf dp mn lq ng nh mp lu ni nj mr ly nk nl mt iz bi translated">余弦相似性</h2><blockquote class="nm nn no"><p id="719c" class="lh li np lj b lk ll kd lm ln lo kg lp nq lr ls lt nr lv lw lx ns lz ma mb mc im bi translated">它是内积空间的两个非零向量之间相似性的度量，度量它们之间角度的余弦。</p></blockquote><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="bde9" class="nb me it nu b gy ny nz l oa ob">from numpy import dot<br/>from numpy.linalg import norm<br/><br/>cos_sim = dot(a, b)/(norm(a)*norm(b))</span></pre><h1 id="293c" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">现在的问题是什么是单词嵌入，我们为什么要使用它们？</h1><p id="4317" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">简单来说，它们是句子、文档等中单词的向量表示。,</p><p id="0527" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">单词嵌入是数字向量形式的单词的学习表示。它从文本语料库中学习预定义的固定大小词汇的密集分布表示。单词嵌入表示能够揭示单词之间的许多隐藏关系。例如，vector(“国王”)— vector(“领主”)类似于vector(“女王”)— vector(“公主”)</p><p id="3d47" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是对表示单词的传统方法的改进，例如<strong class="lj jd">单词袋</strong>模型，其产生大的稀疏向量，这在计算上不切实际地表示整个词汇。由于其庞大的词汇表，这些表示是稀疏的，并且给定的单词或文档将由主要由零值组成的大向量来表示，稀疏表示。</p><blockquote class="oe"><p id="bd16" class="of og it bd oh oi oj ok ol om on mc dk translated">从文本中学习单词嵌入的两种流行方法包括:</p><p id="1d89" class="of og it bd oh oi oj ok ol om on mc dk translated"><strong class="ak"> 1。Word2Vec </strong>。</p><p id="3b0b" class="of og it bd oh oi oj ok ol om on mc dk translated"><strong class="ak"> 2。手套</strong>。</p></blockquote><p id="78e7" class="pw-post-body-paragraph lh li it lj b lk oo kd lm ln op kg lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">有一些预先训练好的模型是在一个大的文本语料库上训练出来的。我们可以在用例中使用它们。</p><p id="6bd9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">除了这些方法，还可以使用深度学习模型来学习单词嵌入。这可能是一种较慢的方法，但我们可以为自己的用例设计它。该模型将根据我们自己的要求在特定的训练数据集上进行训练。Keras提供了一个非常简单灵活的<a class="ae ot" href="https://keras.io/api/layers/core_layers/embedding/#embedding" rel="noopener ugc nofollow" target="_blank">嵌入</a>层，可以用于文本数据上的神经网络。</p><blockquote class="nm nn no"><p id="3950" class="lh li np lj b lk ll kd lm ln lo kg lp nq lr ls lt nr lv lw lx ns lz ma mb mc im bi translated">在本教程中，我们将使用Keras来训练我们自己的单词嵌入模型，并可进一步用于情感分析、机器翻译、语言建模和各种其他自然语言处理任务。</p></blockquote><h2 id="a4a1" class="nb me it bd mf nc nd dn mj ne nf dp mn lq ng nh mp lu ni nj mr ly nk nl mt iz bi translated">导入模块</h2><p id="0eb7" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">让我们开始导入数据集、模块并检查它的头部。我从卡格尔·IMBD电影评论中获取了一组数据。</p><div class="ou ov gp gr ow ox"><a href="https://www.kaggle.com/krystalliu152/imbd-movie-reviewnpl" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd jd gy z fp pc fr fs pd fu fw jc bi translated">IMBD电影评论</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">卡格尔</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">www.kaggle.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl lb ox"/></div></div></a></div><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="0b15" class="nb me it nu b gy ny nz l oa ob">import pandas as pd<br/>import numpy as np<br/>from numpy import array<br/>from keras.preprocessing.text import one_hot, Tokenizer<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import Flatten<br/>from keras.layers.embeddings import Embedding</span></pre><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><p id="157d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将使用<a class="ae ot" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>将数据集分为训练集和测试集。我们将在70%的数据上训练单词embedding，并在30%的数据上进行测试。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><h2 id="118a" class="nb me it bd mf nc nd dn mj ne nf dp mn lq ng nh mp lu ni nj mr ly nk nl mt iz bi translated">对所有文件进行整数编码</h2><p id="596d" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">在此之后，所有唯一的单词将由一个整数表示。为此，我们使用Keras中可用的<strong class="lj jd"> one_hot </strong>函数。请注意，<strong class="lj jd"> vocab_size </strong>被指定为唯一字的总数，以确保每个字的<strong class="lj jd">唯一整数编码</strong>。</p><p id="f29b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">注意一件重要的事情，单词的整数编码在不同的文本中保持不变。在每一份文件中,“年”都用23518表示。</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><p id="adf6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在让我们来看看其中的一篇评论。在接下来的步骤中，我们将把这个句子和它的变形进行比较。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="0e52" class="nb me it nu b gy ny nz l oa ob">I really didn't like this movie because it didn't really bring across the messages and ideas L'Engle brought out in her novel. We had read the novel in our English class and i absolutely loved it, i'm afraid i can't say the same for the film. There were some serious differences between the novel and the adapted version and it just didn't do any credit to the imaginative genius that is Madeleine L'Engle! This is the reason i gave it such a poor rating. Don't see this movie if you are a big fan of L'Engle's texts because you will be sorely disappointed. However, if you are watching the movie for entertainment purposes (or educational as was my case) then it is an alright movie!</span></pre><p id="a49c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该评论将被转换成整数表示，其中每个数字代表一个唯一的单词。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="ec37" class="nb me it nu b gy ny nz l oa ob">[24608, 32542, 30289, 58025, 50966, 19624, 43296, 35850, 30289, 32542, 31519, 11569, 30465, 7968, 12928, 34105, 8750, 49668, 38039, 40264, 3503, 45016, 63074, 41404, 53275, 30465, 45016, 40264, 28666, 47101, 44909, 12928, 24608, 62202, 46727, 35850, 24425, 5515, 24608, 25601, 35725, 30465, 10577, 55918, 30465, 13875, 62286, 22967, 5067, 9001, 33291, 1247, 30465, 45016, 12928, 30465, 23555, 44142, 12928, 35850, 41976, 30289, 20229, 15687, 7845, 50705, 30465, 58301, 14031, 11556, 1495, 26143, 8750, 50966, 1495, 30465, 63056, 24608, 39847, 35850, 30936, 54227, 33469, 55622, 8193, 3111, 50966, 19624, 9403, 51670, 40033, 54227, 42254, 52367, 44935, 63226, 17625, 43296, 51670, 65642, 30053, 42863, 34757, 32894, 9403, 51670, 40033, 1112, 30465, 19624, 55918, 55169, 57666, 10193, 50176, 59413, 10480, 63135, 56156, 64520, 35850, 1495, 49938, 59074, 19624]</span></pre><h2 id="403c" class="nb me it bd mf nc nd dn mj ne nf dp mn lq ng nh mp lu ni nj mr ly nk nl mt iz bi translated">填充文本(使文本具有相同的长度)</h2><p id="22f2" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated"><strong class="lj jd">Keras嵌入层要求所有单个文档长度相同。</strong>因此，我们现在用0填充较短的文档。因此现在在Keras嵌入层中，<strong class="lj jd">‘input _ length’</strong>将等于具有最大长度或最大字数的文档的长度(即字数)。</p><p id="9bd1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了填充较短的文档，我使用了Keras库中的<strong class="lj jd"> pad_sequences </strong>函数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="f82e" class="nb me it nu b gy ny nz l oa ob">The maximum number of words in any document is :  1719</span></pre><p id="4482" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这里，我们发现一个句子最多可以容纳1719个单词。所以我们将根据它填充。在填充中，我们将在比max_length更短的句子中添加零(0)。在较短的句子中,“0”将被加在句子的开头。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><p id="d390" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="ee00" class="nb me it nu b gy ny nz l oa ob">array([    0,     0,     0, ..., 32875, 18129, 60728])</span></pre><h2 id="e634" class="nb me it bd mf nc nd dn mj ne nf dp mn lq ng nh mp lu ni nj mr ly nk nl mt iz bi translated">我们将使用KERAS嵌入层创建嵌入</h2><p id="5151" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">现在所有的文本长度相同(填充后)。所以现在我们准备创建和使用嵌入层。</p><p id="b12d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">嵌入层的参数—</strong></p><p id="7cb1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 'Input_dim' =我们将选择的vocab大小</strong>。它是词汇表中唯一单词的数量。</p><p id="7958" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 'Output_dim' =我们希望嵌入到</strong>中的维数。每个单词可以用一个相同维数的向量来表示。</p><p id="c189" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">'输入长度' =最大文本长度</strong>。它存储在示例中的<code class="fe po pp pq nu b">maxlen</code>变量中。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="e7ea" class="nb me it nu b gy ny nz l oa ob">Model: "sequential_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_1 (Embedding)      (None, 1719, 8)           527680    <br/>_________________________________________________________________<br/>flatten_1 (Flatten)          (None, 13752)             0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 1)                 13753     <br/>=================================================================<br/>Total params: 541,433<br/>Trainable params: 541,433<br/>Non-trainable params: 0<br/>_________________________________________________________________<br/>None</span></pre><p id="2119" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们在训练集上检查模型的准确性。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="231c" class="nb me it nu b gy ny nz l oa ob">6000/6000 [==============================] - 1s 170us/step<br/>Training Accuracy is 100.0</span></pre><p id="e3ce" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下一步我们可以在测试集上检查它的准确性。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="da90" class="nb me it nu b gy ny nz l oa ob">4000/4000 [==============================] - 1s 179us/step<br/>Testing Accuracy is 86.57500147819519</span></pre><p id="3b55" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们得到了100%的训练准确率，因为在我们训练嵌入的数据上，但是对于测试数据，使用了一些看不见的词，所以我们得到的准确率有点低。</p><p id="bd47" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在实践中，我建议使用固定的预训练嵌入来执行单词嵌入，并尝试在预训练嵌入的基础上执行学习。这肯定会提高测试数据的性能。</p></div><div class="ab cl pr ps hx pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="im in io ip iq"><h2 id="2de6" class="nb me it bd mf nc nd dn mj ne nf dp mn lq ng nh mp lu ni nj mr ly nk nl mt iz bi translated">下一步是什么</h2><p id="f1da" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">现在我们已经学会了如何用连续数字的形式来表示单词。与诸如词袋或TF-IDF(术语频率-逆文档频率)等其他形式的文本表示相比。单词嵌入给出了单词之间更好的语义关系。它可以显著提高自然语言处理(NLP)任务的性能。</p><p id="f48d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，我建议你在自己的NLP任务中尝试单词嵌入，你会发现性能有了显著的提高。您还可以通过使用预先训练的单词嵌入(如Word2Vec)在同一个数据集上尝试实现单词嵌入，并在此基础上执行学习。</p><p id="f5cc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通常，您会注意到预训练模型在测试集上具有更高的准确性，原因是它已经在大量的NLP数据集上进行了训练。但是如果你有足够的数据，并且想要执行一个特定的任务，那么训练你自己的单词嵌入将是一个更好的选择。</p><p id="246d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae ot" href="https://github.com/bharatc9530/Machine-Learning/tree/master/Embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd"> GitHub </strong> </a> <strong class="lj jd">上有嵌入文字的代码。</strong></p></div><div class="ab cl pr ps hx pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="im in io ip iq"><h1 id="af8b" class="md me it bd mf mg py mi mj mk pz mm mn ki qa kj mp kl qb km mr ko qc kp mt mu bi translated">参考</h1><ol class=""><li id="16d4" class="qd qe it lj b lk mv ln mw lq qf lu qg ly qh mc qi qj qk ql bi translated">这是一个很好的基本理解单词向量的教程。</li></ol><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pm pn l"/></div></figure><p id="5932" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi">2.</p><div class="ou ov gp gr ow ox"><a href="https://keras.io/api/layers/core_layers/embedding/#embedding" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd jd gy z fp pc fr fs pd fu fw jc bi translated">Keras文档:嵌入层</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">将正整数(索引)转换为固定大小的密集向量。例如[[4]，[20]]--&gt;[[0.25，0.1]，[0.6，-0.2]]这个…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">keras.io</p></div></div><div class="pg l"><div class="qm l pi pj pk pg pl lb ox"/></div></div></a></div><p id="475a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">3.把GloVe和Word2Vec理解为预先训练好的单词嵌入。</p><div class="ou ov gp gr ow ox"><a href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd jd gy z fp pc fr fs pd fu fw jc bi translated">如何用Keras使用单词嵌入层进行深度学习——机器学习掌握</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">单词嵌入提供了单词及其相关含义的密集表示。它们比…有所改进</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">machinelearningmastery.com</p></div></div><div class="pg l"><div class="qn l pi pj pk pg pl lb ox"/></div></div></a></div></div><div class="ab cl pr ps hx pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="im in io ip iq"><p id="7f69" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">谢谢你的阅读。我希望这有助于您理解单词嵌入及其在自然语言处理(NLP)中的重要性。</p><p id="2693" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">跟我上<a class="ae ot" href="https://medium.com/@bharatchoudhary817" rel="noopener"> <strong class="lj jd">中</strong> </a>。一如既往，我欢迎反馈和建设性的批评，您可以通过<a class="ae ot" href="http://www.linkedin.com/in/bharat-choudhary-9530" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd"> Linkedin </strong> </a>联系我。</p></div></div>    
</body>
</html>