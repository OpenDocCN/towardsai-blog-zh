<html>
<head>
<title>PyTorch Tutorial for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch初学者教程</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pytorch-tutorial-for-beginners-8331afc552c4?source=collection_archive---------0-----------------------#2021-02-03">https://pub.towardsai.net/pytorch-tutorial-for-beginners-8331afc552c4?source=collection_archive---------0-----------------------#2021-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="78df" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="b9f2" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Pytorch神经网络设计的基本概念综述</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/ccb194747703d24bef6383d60673cb2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*Gqvk5YvOMqBpX9HJBtE5_g.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">作者制作的图像</figcaption></figure><p id="ddd1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">这篇文章是Pytorch构建深度学习模型系列指南的第一篇。下面，是全系列:</em></p><p id="2ce6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">第一部分:Pytorch初学者教程(本帖)</em></p><p id="6b13" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">第二部分:</em> <a class="ae ma" href="https://medium.com/mlearning-ai/manipulating-pytorch-datasets-c58487ab113f?sk=5d4cf7bd62d527d7c968b8db696b633f" rel="noopener"> <em class="lz">操纵Pytorch数据集</em> </a></p><p id="4c51" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">第三部分:</em> <a class="ae ma" rel="noopener ugc nofollow" target="_blank" href="/understanding-tensor-dimensions-in-deep-learning-models-with-pytorch-4ee828693826"> <em class="lz">了解DL模型中的张量维度</em> </a></p><p id="91ea" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">第四部分:</em> <a class="ae ma" href="https://medium.com/dataseries/visualizing-the-feature-maps-and-filters-by-convolutional-neural-networks-e1462340518e" rel="noopener"> <em class="lz"> CNN &amp;特征可视化</em> </a></p><p id="ef6e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">第五部分:</em> <a class="ae ma" rel="noopener ugc nofollow" target="_blank" href="/tuning-pytorch-hyperparameters-with-optuna-470edcfd4dc"> <em class="lz">超参数调谐用Optuna </em> </a></p><p id="2ae3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">第六部分:</em> <a class="ae ma" href="https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f?sk=2466aaedc4e454b89f880a32604a2e0a" rel="noopener"> <em class="lz"> K折交叉验证</em> </a></p><p id="3c6d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">第七部分:</em> <a class="ae ma" href="https://medium.com/dataseries/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac" rel="noopener"> <em class="lz">卷积自动编码器</em> </a></p><p id="6523" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">第八部分:</em> <a class="ae ma" href="https://ai.plainenglish.io/denoising-autoencoder-in-pytorch-on-mnist-dataset-a76b8824e57e" rel="noopener ugc nofollow" target="_blank"> <em class="lz">去噪</em> </a></p><p id="3e28" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="lz">第九部分:</em> <a class="ae ma" href="https://medium.com/dataseries/variational-autoencoder-with-pytorch-2d359cbf027b" rel="noopener"> <em class="lz">变型自动编码器</em> </a></p><p id="7d57" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">本系列的目标是通过实现示例尽可能使Pytorch更直观、更容易理解。互联网上有许多教程可以使用Pytorch构建多种类型的具有挑战性的模型，但同时也会令人困惑，因为当您从一个教程转到另一个教程时，总会有轻微的差异。在这个系列中，我想从最简单的主题开始，到更高级的主题</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="7cfa" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">介绍</h1><p id="3662" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">Pytorch是脸书人工智能研究实验室于2016年开发的深度学习框架。它以面向计算机视觉的应用而闻名。此外，它的特点是简单，强大的GPU支持，并实现了深度学习算法。由于这些特性，它也是学术研究中使用最多的图书馆之一。</p><p id="571e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">网上有很多Pytorch教程，Pytorch网站上也有很多文档。但是太多的信息会使人困惑，并且会浪费很多时间。我的目标是展示Pytorch中可用的基本函数和类的概述以及一些示例。在本教程中，我将简单直观地展示在开始构建神经网络之前需要了解的主题。</p><h1 id="7a20" class="mi mj it bd mk ml nf mn mo mp ng mr ms ki nh kj mu kl ni km mw ko nj kp my mz bi translated">内容:</h1><ol class=""><li id="5eb3" class="nk nl it lf b lg na lj nb lm nm lq nn lu no ly np nq nr ns bi translated"><a class="ae ma" href="#c495" rel="noopener ugc nofollow"> <strong class="lf jd">张量</strong> </a></li><li id="5cf2" class="nk nl it lf b lg nt lj nu lm nv lq nw lu nx ly np nq nr ns bi translated"><a class="ae ma" href="#d159" rel="noopener ugc nofollow"> <strong class="lf jd">矩阵乘法</strong> </a></li><li id="9c3e" class="nk nl it lf b lg nt lj nu lm nv lq nw lu nx ly np nq nr ns bi translated"><a class="ae ma" href="#f40a" rel="noopener ugc nofollow"> <strong class="lf jd">从Pytorch到NumPy反之亦然</strong> </a></li><li id="d9ab" class="nk nl it lf b lg nt lj nu lm nv lq nw lu nx ly np nq nr ns bi translated"><a class="ae ma" href="#72e7" rel="noopener ugc nofollow"> <strong class="lf jd">辨于亲笔</strong> </a></li><li id="1af9" class="nk nl it lf b lg nt lj nu lm nv lq nw lu nx ly np nq nr ns bi translated"><a class="ae ma" href="#e95c" rel="noopener ugc nofollow"> <strong class="lf jd">单层神经网络</strong> </a> <strong class="lf jd"> k </strong></li><li id="d7c0" class="nk nl it lf b lg nt lj nu lm nv lq nw lu nx ly np nq nr ns bi translated"><a class="ae ma" href="#2a7f" rel="noopener ugc nofollow"> <strong class="lf jd">简单神经网络</strong> </a></li></ol></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="c495" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">1.张量</h1><p id="f9ba" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">一个<strong class="lf jd">张量</strong>是一个Pytorch对象，表示一个多维数组。的确，它类似于NumPy的数组。为了更好地理解构建张量的主要函数，我也将使用Numpy来举例，以说明逻辑是相同的。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/548ab5720a0a7a76ec9d413f024756e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQRo4ACEj8pfM_uAjDK_eA.png"/></div></div></figure><p id="ab4e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Numpy和Pytorch使用许多相似的函数来创建矩阵:</p><ul class=""><li id="f57e" class="nk nl it lf b lg lh lj lk lm of lq og lu oh ly oi nq nr ns bi translated"><code class="fe oj ok ol om b">randn</code>创建随机矩阵</li><li id="52d2" class="nk nl it lf b lg nt lj nu lm nv lq nw lu nx ly oi nq nr ns bi translated"><code class="fe oj ok ol om b">zeros</code>构建充满零的矩阵</li><li id="588a" class="nk nl it lf b lg nt lj nu lm nv lq nw lu nx ly oi nq nr ns bi translated"><code class="fe oj ok ol om b">ones</code>返回带有1的矩阵</li></ul><p id="0d70" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">构建单位矩阵的函数有一个例外。Pytorch使用函数<code class="fe oj ok ol om b">eye</code>，而NumPy使用函数<code class="fe oj ok ol om b">identity</code>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e043856f74b9f49568cc48d1ba30540a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*wwgW0fdDMTK02KNCwt1XQQ.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/bca8e06adbfc07d6551ca1d5254ef550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*8tENTP8mgT1E0-JksvHIPQ.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/0f689e3b68ae7333da7f533832a42314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*6ZfriIp3wCwxyWHYV1CHEw.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/44e5e85b64f737d39ed6fdd54c0df7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*_cJ-vQJy0J_l3Aq8MUZEVw.png"/></div></figure><h1 id="d159" class="mi mj it bd mk ml nf mn mo mp ng mr ms ki nh kj mu kl ni km mw ko nj kp my mz bi translated">2.矩阵乘法</h1><p id="1737" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">当训练神经网络的模型时，有许多矩阵乘法。我将展示两种乘法。</p><p id="b1a1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在NumPy中，函数<code class="fe oj ok ol om b">dot</code>和<code class="fe oj ok ol om b">matmul</code>用于两个矩阵之间的乘积。如果您想避开这些函数，也可以使用@操作符。</p><p id="b523" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Pytorch的功能<code class="fe oj ok ol om b">matmul</code>和NumPy一样。另一种可能是<code class="fe oj ok ol om b">torch.mm</code>功能。两者的区别在于torch.mm不支持广播。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/5783a0c3c95cf14dd31094968f774853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*kCeqADYwMu6Kv6c5FIuw4A.png"/></div></figure><p id="626a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在逐元素乘法的情况下，NumPy使用<code class="fe oj ok ol om b">multiply</code>，而Pytorch使用简单的操作符<code class="fe oj ok ol om b">*</code></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/402e3fa39177c1b04dfef5f604237f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*4TncOJD38RfxT_bl17aHMw.png"/></div></figure><h1 id="f40a" class="mi mj it bd mk ml nf mn mo mp ng mr ms ki nh kj mu kl ni km mw ko nj kp my mz bi translated">3.从<strong class="ak"> Pytorch到NumPy，反之亦然</strong></h1><p id="126b" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">Nupy数组可以使用函数<code class="fe oj ok ol om b">from_numpy</code>转换成张量。通过功能<code class="fe oj ok ol om b">numpy</code>可以进行相反的操作。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/dbd41acbcc2c1e749d87b1ee3004dfba.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*zWYtLuUvq9ffySmMoZgBSQ.png"/></div></figure><h1 id="72e7" class="mi mj it bd mk ml nf mn mo mp ng mr ms ki nh kj mu kl ni km mw ko nj kp my mz bi translated">4.亲笔签名的差异</h1><p id="0697" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">导数构成了神经网络的一个基本方面。事实上，梯度下降算法使用导数来学习模型。该算法的目标是最小化损失函数j。然后，参数的值将改变，直到我们没有获得j的最优值。该方法的著名更新规则是:</p><blockquote class="ot"><p id="b9d1" class="ou ov it bd ow ox oy oz pa pb pc ly dk translated">w = w -学习率* dJ(w)/dw</p><p id="5618" class="ou ov it bd ow ox oy oz pa pb pc ly dk translated">b = b -学习率* dJ(b)/db</p></blockquote><p id="5dfe" class="pw-post-body-paragraph ld le it lf b lg pd kd li lj pe kg ll lm pf lo lp lq pg ls lt lu ph lw lx ly im bi translated">该规则将使学习更快或更慢，这取决于两个特征，即损失函数相对于所考虑的参数w或b的<strong class="lf jd">学习速率</strong>和<strong class="lf jd">导数</strong>。我想重点介绍导数的概念。</p><p id="2c2f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是什么意思？</p><p id="c9b3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">它代表一个函数的斜率。当导数的值很高时，函数快速变化，而当它接近0时，函数不变，这可能是神经网络环境中学习的一个问题。在这个例子中，我比较了直线、抛物线和双曲线。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi pi"><img src="../Images/00642b723a23beafb5d9be6fa2ecab13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OKWrysWlcqEMEcc3Nsib3A.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">作者制作的图像</figcaption></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/fa56616a0d104364a0577c06c2be6f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*KpQ08OydkCYh5hM_o1P2rg.png"/></div></figure><p id="cc7d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">与抛物线和双曲线相比，直线具有较小的导数，而抛物线和双曲线具有较高的值。可以观察到，抛物线的值是直线值的4倍，而双曲线的值是抛物线值的3倍，所以变化比抛物线快3倍。</p><h1 id="e95c" class="mi mj it bd mk ml nf mn mo mp ng mr ms ki nh kj mu kl ni km mw ko nj kp my mz bi translated">5.神经网络的单层</h1><p id="367c" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">在神经网络中，数据集中的每个特征列表示为一个输入神经元，而每个加权值表示为一个从特征列到输出神经元的箭头。我们将这些特征乘以权重并求和，然后添加一个偏差并将其传递给激活函数。这样，我们获得了网络的输出。</p><p id="851f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">例如，如果我们有一个1行10列的输入向量，我们将有10个神经元。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/aefc167648b2ce74f12bf7a404ee7a57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*CUlATdjydmnwGrefJwp4nQ.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">作者制作的图像</figcaption></figure><p id="0100" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">数学上，它看起来像:</p><blockquote class="ot"><p id="f3ae" class="ou ov it bd ow ox oy oz pa pb pc ly dk translated">z = w₁ x₁ + … + w₁₀ x₁₀ + b</p><p id="62c7" class="ou ov it bd ow ox oy oz pa pb pc ly dk translated">y = a(z) = a(w₁ x₁ + … + w₁₀ x₁₀ + b)</p></blockquote><p id="4ae3" class="pw-post-body-paragraph ld le it lf b lg pd kd li lj pe kg ll lm pf lo lp lq pg ls lt lu ph lw lx ly im bi translated">在代码中，我通过<code class="fe oj ok ol om b">matmul</code>函数创建了一个输入向量，它用于创建矩阵，矩阵中填充了均值为0、方差为1的正态分布中的随机数。</p><p id="8f6d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">加权矩阵将具有与输入相同的大小，但具有不同的值。偏差由正态分布的单个值组成。我们可以使用<code class="fe oj ok ol om b">matmul</code>在输入和权重之间进行矩阵乘法，然后我们可以应用激活函数<code class="fe oj ok ol om b">sigm</code>(称为sigmoid)来映射0和1之间的任何值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi pl"><img src="../Images/07d91f222537c8405170840f4c65a3bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kCA8iMa3YiCLWnEuUopzUA.png"/></div></div></figure><p id="a63a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">代码返回一个错误。当你训练一个神经网络时，出现这种错误是很常见的。这是因为<strong class="lf jd">输入的列数不等于w1 </strong>的行数。为了解决这个问题，我们需要改变重物的形状。Pytorch有三种方法可以做到这一点:</p><ul class=""><li id="9122" class="nk nl it lf b lg lh lj lk lm of lq og lu oh ly oi nq nr ns bi translated"><code class="fe oj ok ol om b">w1.reshape(10,1)</code> <strong class="lf jd"> </strong>返回一个新的张量，其数据与w1相同，但形状为(10，1)</li><li id="9c38" class="nk nl it lf b lg nt lj nu lm nv lq nw lu nx ly oi nq nr ns bi translated"><code class="fe oj ok ol om b">w1.view(10,1)</code> <strong class="lf jd"> </strong>返回一个与w1数据相同，形状不同的新张量(10，1)。</li><li id="d202" class="nk nl it lf b lg nt lj nu lm nv lq nw lu nx ly oi nq nr ns bi translated"><code class="fe oj ok ol om b">w1.resize_(10,1)</code>返回形状不同的<strong class="lf jd">相同的</strong>张量(10，1)</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/477950d9685231cd67b7556cb4b2c237.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*IgxxLw5Y_1kkh1iiosqeiQ.png"/></div></figure><h1 id="2a7f" class="mi mj it bd mk ml nf mn mo mp ng mr ms ki nh kj mu kl ni km mw ko nj kp my mz bi translated">6.简单神经网络</h1><p id="2626" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">现在，我们将研究神经网络的最简单模型，称为全连接网络。一层中的每个神经元都与下一层中的每个神经元相连。</p><p id="8001" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面考虑的架构有一个输出层、两个隐藏层和一个输出层。如前所述，我们使用函数<code class="fe oj ok ol om b">randn</code>获得输入、权重和偏差。我们不是只对一层进行计算，而是对每一层进行计算。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/d5164aff7abccc60d0fe665d25c71320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*EPPbRZr9zKi4I_0Fe8vcBw.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi po"><img src="../Images/2cb4a42ebb977f91661c5775db20ff9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I5dAK5ggCW65SRnzJ9TwtQ.png"/></div></div></figure><p id="1675" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面我将展示如何定义一个类，它定义了网络并继承自<code class="fe oj ok ol om b">nn.Module</code>。完全连接的层是通过<code class="fe oj ok ol om b">nn.Linear(in_features,ou_features)</code>构建的。第一个参数是输入单元的数量，而第二个参数是输出单元的数量。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi pl"><img src="../Images/a4ba2101226bb6601208091d1cde3a1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HtTpJLqF9oJnbFDvHjBUNg.png"/></div></div></figure></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="0e96" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated"><strong class="ak">最终想法:</strong></h1><p id="7163" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">我希望这篇教程通过一个概述帮助你更好地理解Pytorch。</p><p id="191a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有时候编程时很容易阻塞，可能是因为有些东西不清楚，或者看得太快，没有时间好好记忆。这里的GitHub代码是<a class="ae ma" href="https://github.com/eugeniaring/Medium-Articles/blob/main/Pytorch/pytorch-for-beginners.ipynb" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><p id="99ab" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">感谢阅读。祝您愉快！</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><p id="7e68" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你喜欢我的文章吗？ <a class="ae ma" href="https://eugenia-anello.medium.com/membership" rel="noopener"> <strong class="lf jd"> <em class="lz">成为会员</em> </strong> </a> <strong class="lf jd"> <em class="lz">每天无限获取数据科学新帖！这是一种间接的支持我的方式，不会给你带来任何额外的费用。如果您已经是会员，</em> </strong> <a class="ae ma" href="https://eugenia-anello.medium.com/subscribe" rel="noopener"> <strong class="lf jd"> <em class="lz">订阅</em> </strong> </a> <strong class="lf jd"> <em class="lz">每当我发布新的数据科学和python指南时，您都可以收到电子邮件！</em> </strong></p></div></div>    
</body>
</html>