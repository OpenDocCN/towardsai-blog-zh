<html>
<head>
<title>Objects that Sound: DeepMind’s Research Show How to Combine Vision and Audio in a Single Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">发出声音的物体:DeepMind的研究显示了如何在一个模型中结合视觉和听觉</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/objects-that-sound-deepminds-research-show-how-to-combine-vision-and-audio-in-a-single-model-c4051ea21495?source=collection_archive---------1-----------------------#2021-07-01">https://pub.towardsai.net/objects-that-sound-deepminds-research-show-how-to-combine-vision-and-audio-in-a-single-model-c4051ea21495?source=collection_archive---------1-----------------------#2021-07-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="936b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="1575" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">被称为AVE-Net的新架构仍然是多模态学习的一个重大突破。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/fa152b27d6fdbbbfaa90dc8ba0e95cab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RHC7LI2q8RY-zbAu.jpg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:https://www.infoq.com/news/2020/09/ai-created-foley/<a class="ae lh" href="https://www.infoq.com/news/2020/09/ai-created-foley/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><blockquote class="li lj lk"><p id="0ac0" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过90，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到85，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="bd41" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">因为我们是婴儿，我们直觉地发展了将来自不同认知传感器的输入相关联的能力，例如视觉、听觉和文本。当听交响乐时，我们会立即想象一个管弦乐队，或者当欣赏一幅风景画时，我们的大脑会将视觉与特定的声音联系起来。图像、声音和文本之间的关系是由负责分析特定认知输入的大脑不同部分之间的连接决定的。从这个意义上说，你可以说我们天生就能同时从多种认知信号中学习。尽管在图像、语言和声音分析等不同的深度学习领域取得了进步，但大多数神经网络仍然专注于单一的输入数据类型。几年前，Alphabet子公司DeepMind的研究人员发表了一篇<a class="ae lh" href="https://arxiv.org/abs/1712.06651" rel="noopener ugc nofollow" target="_blank">研究论文</a>，提出了一种方法，可以同时分析音频和视觉输入，并在一个共同的环境中学习物体和声音之间的关系。</p><p id="3881" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在标题<a class="ae lh" href="https://arxiv.org/abs/1712.06651" rel="noopener ugc nofollow" target="_blank">“发出声音的物体”</a>下，DeepMind的研究论文专注于一个被称为跨模态学习的分支学科，该分支学科专注于研究图像、声音和文本之间的隐藏关系。跨模态学习已经在图像-文本关系领域取得了一些成功，但是在可以将图像和声音相关联的模型方面几乎没有什么进展。对此的解释非常简单，文本比音频更接近于语义注释。当分析所提供的图像说明的文本形式时，对象是直接可用的，然后问题是提供名词和图像中的空间区域之间的对应关系。在音频的情况下，获取语义不太直接。思考一下根据图像是否包含狗来分类图像和根据音频剪辑是否包含狗的声音来分类音频剪辑之间的区别。</p><p id="c2cf" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">解决跨模态学习问题的传统方法是使用技术学生监督网络，其中的“教师”已经使用大量的人工注释进行了训练。例如，在ImageNet上训练的视觉网络可以用于将YouTube视频的帧注释为“原声吉他”，这向“学生”音频网络提供训练数据，用于学习“原声吉他”听起来像什么。师生方法的挑战是图像和音频不是以相同的时间和空间顺序处理的，这引入了大量的上下文差异。此外，众所周知，师生模型的大规模实施成本很高，因为需要大量精选的训练数据集。</p><h1 id="6bd0" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">AVC和AVE-Net</h1><p id="423e" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">为了解决师生模型的局限性，DeepMind团队依赖于一种被称为视听通信(AVC)的跨模式学习形式。AVC方法采用视频帧和1秒音频的输入对，并尝试确定它们是否一致。使用前面的类比，AVC模型将从头开始训练视觉和音频网络，使“原声吉他”的概念自然地出现在这两种模式中。</p><p id="b6b6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">DeepMind论文中介绍的特定AVC模型被称为视听嵌入网络(AVE-Net ),它采用由成对图像和1秒音频频谱图形成的输入数据集。该模型使用音频和视觉子网处理输入，然后是功能视觉层，该功能视觉层试图确定图像和声音之间是否存在关系。下图说明了AVE-Net神经网络体系结构。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/095fcacfc596b8e0ce02da4dea0e2a2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFzT9BNIL6FopN9tkch29w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:DeepMind</figcaption></figure><p id="9905" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">通过同时处理音频和视觉，AVE-Net模型可以使用简单的欧几里德距离技术来确定两个子网络(音频、图像)之间的嵌入关系。最初的测试表明，AVE-Net在检测物体和声音之间的双向相关性方面非常聪明，正如在以下视频中清楚看到的那样。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="cda8" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在第一波实验中，AVE-Net在不同环境下的表现远远优于传统的跨通道学习模型。</p><h1 id="76a1" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">你能告诉我是什么物体发出这种声音吗？</h1><p id="43c0" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">父母不断要求婴儿模仿不同物体或动物的声音。从认知上来说，这是一个发展婴儿跨模态学习能力的很好的练习。上一节中显示的AVE-Net架构在确定图像和声音域之间的相关性方面证明是有效的，但它仍然不能识别图像或视频帧中的哪些对象产生特定的声音。为了应对这一挑战，DeepMind团队创建了一个AVE-Net模型的变体，它在更深的粒度级别上尝试将特定声音中的区域/对象和图像关联起来。该模型被称为视听对象定位(AVOL-Net)，它采用图像和声音对，并试图找到图像中解释特定声音的区域，而其他区域不应与其相关，应属于背景。</p><p id="a277" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">AVOL-Net架构看起来类似于AVE-Net模型，只是在视觉网络上有所不同，它生成对应于图像向量中不同区域的视觉嵌入网格。音频和所有视觉嵌入之间的相似性揭示了发出声音的对象的位置，而最大相似性被用作对应得分。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1d3e351b8ace457417dd1886d3dc00c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/1*639ym4tZYyIBRjY4UzOblQ.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:DeepMind</figcaption></figure><p id="e769" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">以下视频展示了AVOL-Net模型识别图像中与目标声音相对应的特定对象的有效性。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="a52b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">跨模态学习仍处于起步阶段，但AVE-Net和AVOL-Net等方法代表了深度学习领域的主要里程碑。这两种技术都能够学习在相同环境中运行的图像和声音之间的语义关系，而AVOL-Net模型能够将声音与图像中的特定对象相关联。诸如AVE-Net和AVOL-Net之类的方法在暴露于真实世界环境的人工智能(AI)代理中变得极其相关。</p></div></div>    
</body>
</html>