<html>
<head>
<title>DeepMind’s AlphaTensor: Deepmind’s Alphatensor: The AI That Is Reinventing Math</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepMind的AlphaTensor: Deepmind的AlphaTensor:正在重塑数学的人工智能</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/deepminds-alphatensor-deepmind-s-alphatensor-the-ai-that-is-reinventing-math-4e282e25da35?source=collection_archive---------1-----------------------#2022-10-10">https://pub.towardsai.net/deepminds-alphatensor-deepmind-s-alphatensor-the-ai-that-is-reinventing-math-4e282e25da35?source=collection_archive---------1-----------------------#2022-10-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a9fb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">DeepMind的最新模型如何能彻底改变数学</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/145baaf5be4f96e918e7a21b759aac30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_BQ0OoxxJTMnlZfplwPNpg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">使用<a class="ae kv" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank"> OpenAI Dall-E 2 </a>生成的图像</figcaption></figure><p id="b34f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在没有意识到的情况下，我们的任何活动都以这样或那样的方式涉及矩阵乘法。整个计算都依赖于它们；能够提高效率才是根本。DeepMind(在<a class="ae kv" href="https://towardsdatascience.com/speaking-the-language-of-life-how-alphafold2-and-co-are-changing-biology-97cff7496221" rel="noopener" target="_blank">用AlphaFold2 </a>彻底改变生物学一年后)发表了一篇文章，其中使用强化学习，它设法提高矩阵乘法的效率。在本文中，我们将讨论它是如何以及为什么重要。</p><p id="e5b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我们仍然在矩阵中挣扎</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/ae92c7bed2cd9f697e5ae366f6247d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2hgbWtVAfkiNTKuW"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">unsplash.com的罗马法师拍摄的图片</figcaption></figure><p id="144b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">自有史以来，算法就是基础。希腊人和埃及人都发明了算法，使他们能够在伟大的工作中取得成功。算法也是现代文明的基础，在没有意识到的情况下，它们支撑着几乎每个知识领域及其应用。</p><p id="8b02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，发现新的算法也绝非易事(我们在研究算法和数据结构时都吃过苦，但发现新的就更难了)。当今最重要的算法之一是两个矩阵相乘。为什么？</p><p id="2ce1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为实际上所有类型的数据都可以用矩阵来表示。事实上，图像可以表示为矩阵，可以用于求解线性方程，用于图形视频游戏、天气模拟等。此外，大多数人工智能算法可以简化为矩阵乘法(然后由GPU有效处理)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/63c5570de533fe94c48fb40b6b566f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_ToDAyvjfaPJHll7zctvg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">矩阵乘法。图片来源:<a class="ae kv" href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor" rel="noopener ugc nofollow" target="_blank"> DeepMind blogpost </a></figcaption></figure><p id="55f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Matrix_multiplication" rel="noopener ugc nofollow" target="_blank">矩阵乘法</a>看起来是一个非常简单的概念，但是考虑到它的重要性，能够稍微提高它的效率将会节省大量的计算。几个世纪以来，数学家们认为已知矩阵乘法是一种有效的方法。1969年，正如Volker Strassen 所证明的那样，效率实际上是次优的，这一事实震惊了整个社区。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lu"><img src="../Images/21a8ba889564949ae3a0b2e47f7898cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFuzLa1RDMOu8ir0Ib7Wrg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">矩阵乘法:比较两种算法，Strassen的算法更有效，因为它少用了一个标量乘法。图片来源:<a class="ae kv" href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor" rel="noopener ugc nofollow" target="_blank"> DeepMind博文</a></figcaption></figure><p id="0c91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，少一个标量乘法似乎没什么大不了的。但是如果我们乘以10亿个矩阵，我们就节省了10亿个标量乘法。问题是Strassen的方法只适合两个2x2矩阵的乘法。</p><p id="108d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">deep mind如何解决问题</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lv lw l"/></div></figure><blockquote class="lx ly lz"><p id="abc1" class="kw kx ma ky b kz la jr lb lc ld ju le mb lg lh li mc lk ll lm md lo lp lq lr ij bi translated">Strassen的工作表明，矩阵乘法算法可以通过寻找新的方法来将称为矩阵乘法张量的3D数字阵列分解为基本构建块的总和来发现。——<a class="ae kv" href="https://www.nature.com/articles/d41586-022-03023-w" rel="noopener ugc nofollow" target="_blank">自然评论文章</a></p></blockquote><p id="1ebf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DeepMind的研究人员已经把矩阵乘法的问题变成了一种单人游戏(毕竟他们在<a class="ae kv" href="https://en.wikipedia.org/wiki/AlphaZero" rel="noopener ugc nofollow" target="_blank"> AlphaZero </a>和<a class="ae kv" href="https://en.wikipedia.org/wiki/AlphaGo" rel="noopener ugc nofollow" target="_blank"> AlphaGo </a>之后在该领域特别有经验)。事实上，在这种情况下，棋盘是一个三维张量(张量实际上是一个矩阵，3D张量是一个3D矩阵)，玩家四处移动，试图达到最优解(修改张量并将其条目清零)。如果玩家成功了，他的移动的结果就是正确的矩阵乘法算法(效率是由将张量归零所用的步骤数给出的)。因此，目标是最小化将张量归零的移动(步骤)数量。很聪明，对吧？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi me"><img src="../Images/00363260d2c09f356125f23365b44005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sbrIrKPrvwHJNM3R.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:<a class="ae kv" href="https://www.nature.com/articles/s41586-022-05172-4" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="3a45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">研究人员使用强化学习来“玩”。正如文章中所描述的，人们可以将这个系统视为AlphaZero的改编版本(其中代理人<a class="ae kv" href="https://www.deepmind.com/research/highlighted-research/alphago" rel="noopener ugc nofollow" target="_blank">的目标是在围棋、象棋和其他游戏</a>中获胜)。由于这个原因，这个模型被称为AlphaZero。</p><p id="4ec4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些术语描述的问题听起来很简单，但正如DeepMind研究人员所描述的那样，在现实中，有很多潜在的组合:</p><blockquote class="lx ly lz"><p id="f4b6" class="kw kx ma ky b kz la jr lb lc ld ju le mb lg lh li mc lk ll lm md lo lp lq lr ij bi translated">这个游戏具有难以置信的挑战性——要考虑的可能算法的数量远远大于宇宙中的原子数量，即使是矩阵乘法的小例子。与几十年来一直是人工智能挑战的围棋游戏相比，我们游戏中每一步可能的走法数量要多30个数量级(我们考虑的一个设置超过1033个)。— <a class="ae kv" href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor" rel="noopener ugc nofollow" target="_blank"> DeepMind博客</a></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/1164fd00e8ef2533a7f22b61b824e6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tq2tDSvty0AML73N"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自usplash.com的<a class="ae kv" href="https://unsplash.com/@felix_mittermeier" rel="noopener ugc nofollow" target="_blank">菲利克斯·米特迈尔</a></figcaption></figure><p id="fefc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，为了成功，作者使用了一种新的架构，结合了特定问题的归纳偏见；他们还使用了合成数据和一些关于问题的信息(对称性)。更具体地说，研究人员使用了基于变压器的架构(使用交叉注意、因果自我注意等。、<a class="ae kv" href="https://www.nature.com/articles/s41586-022-05172-4/figures/8" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae kv" href="https://www.nature.com/articles/s41586-022-05172-4/figures/9" rel="noopener ugc nofollow" target="_blank">这里</a>是结构的详细图像)。然后使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank">强化学习</a>训练该模型(输入实际上是当前状态和3D张量，以及之前的动作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/62b510f4b473a34c0a7fcbcf51b31b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wXc67FCl49PDGnq-.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">阿尔法张量的模型结构。图片来源:<a class="ae kv" href="https://www.nature.com/articles/s41586-022-05172-4" rel="noopener ugc nofollow" target="_blank">原纸</a></figcaption></figure><p id="9f50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练开始时，该模型不了解矩阵相乘的现有算法，但在训练过程中，它会变得更好。有趣的是，AlphaTensor首先重新发现已知的算法，然后找到未知的算法(实际上超越了人类的直觉)</p><blockquote class="lx ly lz"><p id="50a4" class="kw kx ma ky b kz la jr lb lc ld ju le mb lg lh li mc lk ll lm md lo lp lq lr ij bi translated">这导致了大矩阵乘法算法的发现，其速度比硬件上常用的算法快10-20%。— <a class="ae kv" href="https://www.nature.com/articles/d41586-022-03023-w" rel="noopener ugc nofollow" target="_blank">来源</a></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/bdf324d6fdb3b17b82191861a7cef26e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7iYIlcU56CQBpHD3.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">为GPU量身定制的阿尔法张量发现算法的加速。图片来源:<a class="ae kv" href="https://www.nature.com/articles/s41586-022-05172-4/figures/5" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="bccb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个有趣的结果是，实际上矩阵乘法算法的空间比以前认为的更丰富。现在，这听起来像是数学术语，但它实际上意味着作者能够根据需要的情况来调整AlphaTensor以寻找更有效的算法:即GPU或TPU是否需要矩阵乘法算法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mf"><img src="../Images/8f904cebb20781dd60fd22b609309b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSADxtQZMEd_DnYdHsVcBA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:<a class="ae kv" href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="8b99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">离别的思念</strong></p><p id="c238" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DeepMind用一条推文宣布了这篇文章的发表，科学界立即进行了炒作。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="9fe7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，这个结果本身是不可思议的，因为这是第一次，计算的基本算法之一变得更加有效(他们已经尝试了几个世纪)。此外，这一发现不是由于人类的直觉，而是由于一种算法(你可以在这里看到代码和算法<a class="ae kv" href="https://github.com/deepmind/alphatensor" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="0c85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者声称可以找到更快的算法。所以这只是一个开始，作者希望将研究扩展到其他相关问题，比如不涉及负元素的矩阵分解。</p><p id="59f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">无论如何，这似乎也是一个新时代的开始，数学研究人员将得到算法的帮助。此外，更有效的算法使计算更有效，允许更大的模型，从而处于一种积极的循环中。此外，降低模型的计算成本允许没有最先进的基础设施的其他人使用具有许多参数的模型。</p><h1 id="309b" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">如果你觉得有趣:</h1><p id="8381" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">你可以寻找我的其他文章，你也可以<a class="ae kv" href="https://salvatore-raieli.medium.com/subscribe" rel="noopener"> <strong class="ky ir">订阅</strong> </a>在我发表文章时获得通知，你也可以在<strong class="ky ir"/><a class="ae kv" href="https://www.linkedin.com/in/salvatore-raieli/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">LinkedIn</strong></a><strong class="ky ir">上连接或联系我。</strong>感谢您的支持！</p><p id="872d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我的GitHub知识库的链接，我计划在这里收集代码和许多与机器学习、人工智能等相关的资源。</p><div class="nd ne gp gr nf ng"><a href="https://github.com/SalvatoreRa/tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">GitHub - SalvatoreRa/tutorial:关于机器学习、人工智能、数据科学的教程…</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">关于机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码(python…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">github.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu kp ng"/></div></div></a></div><p id="bd28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者随意查看我在Medium上的其他文章:</p><div class="nd ne gp gr nf ng"><a href="https://medium.com/mlearning-ai/nobel-prize-cyberpunk-e1803aa0e087" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">诺贝尔奖赛博朋克</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">科学发现中人工智能最重要奖项的计算视角</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="nv l nr ns nt np nu kp ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://towardsdatascience.com/alphafold2-year-1-did-it-change-the-world-499a5a38130a" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">AlphaFold2第一年:它改变了世界吗？</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">DeepMind向我们承诺了一场革命，它发生了吗？</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="nw l nr ns nt np nu kp ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://towardsdatascience.com/speaking-the-language-of-life-how-alphafold2-and-co-are-changing-biology-97cff7496221" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">说生命的语言:AlphaFold2和公司如何改变生物学</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">人工智能正在重塑生物学研究，并开辟治疗的新领域</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="nx l nr ns nt np nu kp ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://towardsdatascience.com/a-critical-analysis-of-your-dataset-2b388e7ca01e" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">对数据集的批判性分析</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">停止微调你的模型:你的模型已经很好了，但不是你的数据</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">towardsdatascience.com</p></div></div><div class="np l"><div class="ny l nr ns nt np nu kp ng"/></div></div></a></div></div></div>    
</body>
</html>