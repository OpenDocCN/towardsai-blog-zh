<html>
<head>
<title>The Limits of Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的局限</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-limits-of-deep-learning-96511b36087b?source=collection_archive---------1-----------------------#2020-08-01">https://pub.towardsai.net/the-limits-of-deep-learning-96511b36087b?source=collection_archive---------1-----------------------#2020-08-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="47d1" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/opinion" rel="noopener ugc nofollow" target="_blank">观点</a></h2><div class=""/><div class=""><h2 id="c313" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">大型计算需要有限性能，要求更高的效率。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b317616a2a3d7ec84201a322bfceb1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vLe57EusNXc6cIHP"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@lucambro?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卢卡·安布罗西</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="1526" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> G </span> PT-3是深度学习领域的最新技术，在没有额外训练的情况下，在一系列语言任务中取得了令人难以置信的结果。这种型号和以前型号的主要区别在于尺寸。</p><p id="aa88" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GPT-3接受了数千亿个词的训练——几乎是整个互联网——产生了一个计算量巨大的1750亿个参数模型。</p><p id="d784" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">OpenAI的作者<a class="ae lh" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">注意到</a>我们不能永远缩放模型:</p><blockquote class="mn mo mp"><p id="eaab" class="li lj mq lk b ll lm kd ln lo lp kg lq mr ls lt lu ms lw lx ly mt ma mb mc md im bi translated">“本文中描述的一般方法(放大任何类似LM的模型，无论是自回归模型还是双向模型)的一个更基本的限制是，它最终可能会遇到(或可能已经遇到)预训练目标的限制。”</p></blockquote><p id="5d52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是收益递减规律在起作用。</p><h1 id="202b" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">收益递减</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/30eb311ec53f4b82f4098bbea59d7272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DjJadi0ekXWeo9Up8fSy2A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">按作者。</figcaption></figure><p id="6425" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你用小数据从零开始训练一个深度学习模型<em class="mq"/>(而不是从ResNet或ImageNet 或其他一些迁移学习基础开始的<a class="ae lh" href="https://towardsdatascience.com/does-deep-learning-really-require-big-data-no-13890b014ded" rel="noopener" target="_blank">)，你将实现更低的性能。如果你用更多的数据进行训练，你会取得更好的成绩。GPT-3表明，使用超级计算机在巨大的数据集上进行训练，可以获得最先进的结果。</a></p><p id="cac1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">每一个连续的GPT模型都在很大程度上通过缩放训练数据来改进上一个模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/e8a5f2a8fe3a7b93c4f6aec1d7cebadf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ji-3rPqbF83jwDm7_NvJw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者使用<a class="ae lh" href="https://imgflip.com/memegenerator/Expanding-Brain" rel="noopener ugc nofollow" target="_blank"> imgflip </a>创建的模因。</figcaption></figure><p id="2e91" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，不确定的是，再次扩大规模——比方说，10倍的数据和10倍的计算——会带来更多的准确性收益。论文“<a class="ae lh" href="https://arxiv.org/pdf/2007.05558.pdf" rel="noopener ugc nofollow" target="_blank">深度学习中的计算限制</a>”列出了这些问题——深度学习是不可持续的，就像现在这样:</p><blockquote class="mn mo mp"><p id="ae44" class="li lj mq lk b ll lm kd ln lo lp kg lq mr ls lt lu ms lw lx ly mt ma mb mc md im bi translated">"目前的进展在经济、技术和环境方面正迅速变得不可持续."</p></blockquote><p id="98c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个<a class="ae lh" href="https://arxiv.org/pdf/2007.05558.pdf" rel="noopener ugc nofollow" target="_blank">例子</a>完美地说明了收益递减:</p><blockquote class="mn mo mp"><p id="8db1" class="li lj mq lk b ll lm kd ln lo lp kg lq mr ls lt lu ms lw lx ly mt ma mb mc md im bi translated">“即使在更乐观的模型中，估计也需要额外的10⁵×更多计算才能使ImageNet的错误率达到5%。”</p></blockquote><p id="f3aa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">广受欢迎的Keras图书馆的作者Franç ois Chollet指出，我们已经接近了DL的极限:</p><blockquote class="mn mo mp"><p id="07cd" class="li lj mq lk b ll lm kd ln lo lp kg lq mr ls lt lu ms lw lx ly mt ma mb mc md im bi translated">“对于深度学习已经实现了变革性更好解决方案的大多数问题(视觉、言语)，我们在2016-2017年进入了回报递减的领域。”</p></blockquote><div class="no np gp gr nq nr"><a href="https://semiwiki.com/artificial-intelligence/7588-deep-learning-diminishing-returns/" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">深度学习:收益递减？-塞米维基</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">深度学习(DL)已经成为我们这个时代的神谕——我们用来寻找几乎任何问题答案的通用技术…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">semiwiki.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of lb nr"/></div></div></a></div><p id="2ab3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">事实上，虽然GPT-3比GPT-2大得多，但它仍然有严重的缺点，根据论文作者的说法:</p><blockquote class="mn mo mp"><p id="c5e0" class="li lj mq lk b ll lm kd ln lo lp kg lq mr ls lt lu ms lw lx ly mt ma mb mc md im bi translated">“尽管GPT-3在数量和质量上都有很大提高，特别是与它的直接前身GPT-2相比，它仍然有明显的弱点，”包括在敌对的NLI上“比机会好不了多少”的性能。</p></blockquote><p id="4fb6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自然语言推理已经被证明是深度学习的一个主要挑战，以至于在一个难以置信的大语料库上训练都无法解决它。</p><h2 id="5466" class="og mv it bd mw oh oi dn na oj ok dp ne lr ol om ng lv on oo ni lz op oq nk iz bi translated">“黑盒”人工智能——可解释性差</h2><p id="4fcc" class="pw-post-body-paragraph li lj it lk b ll or kd ln lo os kg lq lr ot lt lu lv ou lx ly lz ov mb mc md im bi translated">深度学习的另一个局限是可解释性差。对于像GPT 3号这样的庞大模型——回想一下它有1750亿个参数——解释几乎是不可能的。我们只能猜测为什么模型会做出某个决定，但是并不清楚。</p><p id="7901" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，如果GPT-3告诉我们，它<a class="ae lh" href="https://medium.com/dataseries/the-top-data-science-job-of-the-future-6dbdae7343d7" rel="noopener">更喜欢《我的世界》</a>而不是堡垒之夜，我们可以凭直觉认为这是因为“《我的世界》”这个词在它的训练数据中出现得更多。</p><p id="0213" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个问题与深度学习的低效率是分开的，如果你在寻找可解释性，最好的解决方案是简单地使用更多可解释的模型。</p><p id="a914" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，一些AutoML工具，如<a class="ae lh" href="http://apteo.co" rel="noopener ugc nofollow" target="_blank"> Apteo </a>通过在包括决策树和随机森林在内的模型中进行选择，从你的数据中获得洞察力，这些模型比深度神经网络具有更强的解释能力。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/db769c39e0e66c72c5f2948a709f9f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*15d8521TR_Cqdf2p2LxrHg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="http://apteo.co" rel="noopener ugc nofollow" target="_blank"> Apteo </a>截图。由作者捕获。</figcaption></figure><p id="deec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最终，你需要权衡用例中可解释性的相对重要性。</p><h1 id="7251" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">实现更高的深度学习效率</h1><p id="3076" class="pw-post-body-paragraph li lj it lk b ll or kd ln lo os kg lq lr ot lt lu lv ou lx ly lz ov mb mc md im bi translated">过去几年里，由于计算和数据的大幅增加，人工智能领域出现了一个又一个突破，但我们正在最大限度地利用这些机会。</p><p id="f855" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对话需要转向算法和硬件效率，这也将增加可持续性。</p><h2 id="c223" class="og mv it bd mw oh oi dn na oj ok dp ne lr ol om ng lv on oo ni lz op oq nk iz bi translated">量子计算</h2><p id="d0b7" class="pw-post-body-paragraph li lj it lk b ll or kd ln lo os kg lq lr ot lt lu lv ou lx ly lz ov mb mc md im bi translated">在过去十年中，DL的计算改进主要包括GPU和TPU实现，以及FPGA和其他ASICs。量子计算也许是最好的选择，<a class="ae lh" href="https://arxiv.org/pdf/2007.05558.pdf" rel="noopener ugc nofollow" target="_blank">作为</a>“它提供了计算能力持续指数增长的潜力。”</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/2e9f862370f8efc0fb9a090a443840be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eyBCn9C2eeIG87XktXJ6aw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">按作者。</figcaption></figure><p id="5bc9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当前尖端的量子计算机，如IBM的<em class="mq">罗利</em>，拥有大约<a class="ae lh" href="https://www.ibm.com/blogs/research/2020/01/quantum-volume-32/" rel="noopener ugc nofollow" target="_blank"> 32 </a>的量子体积，尽管霍尼韦尔声称最近已经制造了<a class="ae lh" href="https://www.honeywell.com/en-us/newsroom/news/2020/06/the-worlds-highest-performing-quantum-computer-is-here" rel="noopener ugc nofollow" target="_blank"> 64量子体积计算机</a>。IBM希望其量子量每年翻一番。</p><h2 id="e36e" class="og mv it bd mw oh oi dn na oj ok dp ne lr ol om ng lv on oo ni lz op oq nk iz bi translated">降低计算复杂性</h2><p id="2b7d" class="pw-post-body-paragraph li lj it lk b ll or kd ln lo os kg lq lr ot lt lu lv ou lx ly lz ov mb mc md im bi translated">GPT-3是一个非常复杂的模型，有1750亿个参数。人们可以通过压缩神经网络中的连接来降低计算复杂性，例如通过“修剪”掉权重、量化网络或使用低秩压缩。</p><p id="ce09" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">迄今为止，这些方法的结果仍有许多不足之处，但这是一个潜在的探索领域。</p><h2 id="e3b4" class="og mv it bd mw oh oi dn na oj ok dp ne lr ol om ng lv on oo ni lz op oq nk iz bi translated">高性能小型深度学习</h2><p id="12b8" class="pw-post-body-paragraph li lj it lk b ll or kd ln lo os kg lq lr ot lt lu lv ou lx ly lz ov mb mc md im bi translated">最后，人们可以使用优化来找到更有效的网络架构，以及元学习和迁移学习。然而，像元学习这样的方法会对准确性产生负面影响。</p><h1 id="90f3" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">结论</h1><p id="01be" class="pw-post-body-paragraph li lj it lk b ll or kd ln lo os kg lq lr ot lt lu lv ou lx ly lz ov mb mc md im bi translated">鉴于不断增长的数据和计算，深度学习近年来一直是令人难以置信的人工智能突破的来源，但摩尔定律不能永远持续下去。我们已经见证了规模模型的收益递减。潜在的解决方案包括更高的算法和硬件效率，特别是在量子计算方面。</p></div></div>    
</body>
</html>