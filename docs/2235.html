<html>
<head>
<title>6 Types of AI Bias Everyone Should Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个人都应该知道的6种人工智能偏见</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/6-types-of-ai-bias-everyone-should-know-e72b2259cb1a?source=collection_archive---------2-----------------------#2021-10-09">https://pub.towardsai.net/6-types-of-ai-bias-everyone-should-know-e72b2259cb1a?source=collection_archive---------2-----------------------#2021-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0c0e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence/fairness" rel="noopener ugc nofollow" target="_blank">公平</a></h2><div class=""/><p id="799b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在我之前的博客中，我们研究了<a class="ae ku" rel="noopener ugc nofollow" target="_blank" href="/bias-vs-fairness-vs-explainability-in-ai-5e0f37ffb22">在人工智能</a>中偏见、公平和可解释性之间的区别。我在文章中概括了什么是偏见，但这一次我们将深入探讨更多细节。</p><p id="8812" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">偏见以许多不同的形式出现在机器学习中。需要考虑的重要事情是，训练机器学习模型很像抚养孩子。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/dbb6b384ba8f36333488eef12bcd8f53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K5vsYNccZn0eU2QA"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">照片由<a class="ae ku" href="https://unsplash.com/@phammi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米PHAM </a>在<a class="ae ku" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="7054" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">当孩子成长时，他们使用听觉、视觉和触觉等感官来学习周围的世界。他们对世界的理解，他们的观点，以及他们最终做出的决定都受到他们的教养的严重影响。例如，一个在性别歧视社区中长大和生活的孩子可能永远不会意识到他们看待不同性别的方式有任何偏见。机器学习模型完全一样。他们不使用感官作为输入，而是使用数据——我们给他们的数据！这就是为什么在用于训练机器学习模型的数据中尝试和避免偏见如此重要。让我们仔细看看机器学习中一些最常见的偏见形式:</p><h1 id="0769" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">历史偏见</strong></h1><p id="1253" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">在为训练机器学习算法收集数据时，获取历史数据几乎总是最容易的起点。然而，如果我们不小心的话，历史数据中很容易包含偏见。</p><p id="1a46" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">以亚马逊为例；2014年，他们着手建立一个自动筛选求职者的系统。这个想法就是给系统输入数百份简历，然后自动挑选出最优秀的候选人。该系统根据10年的求职申请及其结果进行了训练。问题？亚马逊的大多数员工都是男性(尤其是技术职位)。该算法了解到，因为亚马逊的男性多于女性，所以男性是更合适的候选人，并积极歧视非男性申请人。到2015年，整个项目不得不被废弃。</p><h1 id="f8ed" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">样本偏差</strong></h1><p id="d8ef" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">当您的训练数据没有准确反映模型的真实使用情况时，就会出现样本偏差。通常一个群体要么代表人数过多，要么代表人数不足。</p><p id="5a6b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我最近看了大卫·基恩的演讲，他给出了一个样本偏差的很好的例子。</p><p id="b69f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">当训练一个语音到文本的系统时，你需要大量的音频片段和它们相应的转录。除了有声读物，还有什么地方能更好地获取这些数据呢？这种方法会有什么问题呢？</p><p id="4182" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">事实证明，绝大多数有声读物是由受过良好教育的中年白人男性讲述的。毫不奇怪，当用户来自不同的社会经济或种族背景时，使用这种方法训练的语音识别软件表现不佳。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mo"><img src="../Images/0f25e8c1b91778b7c2fcb1c70c5acced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kIBoKKc7yjlPKfr9"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">来源:https://www.pnas.org/content/117/14/7684</figcaption></figure><p id="eada" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">上图显示了大型科技公司语音识别系统的单词错误率(WER)。你可以清楚地看到，所有的算法对于黑人声音和白人声音都表现不佳。</p><h1 id="28ae" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">标签偏差</strong></h1><p id="786e" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">训练ML算法所需的大量数据在有用之前需要被标记。当你登录网站时，你实际上经常这样做。被要求识别包含交通灯的方块？你实际上是在确认该图像的一组标签，以帮助训练视觉识别模型。然而，我们标注数据的方式千差万别，标注的不一致会给系统带来偏差。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mp"><img src="../Images/fe47636db96b406cc51059a3e1c365e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8YmiQgZT8KSaVL_gBNvN_w.jpeg"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">作者图片</figcaption></figure><p id="c801" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">想象一下，你训练一个系统，用上图中的盒子给狮子贴上标签。然后向您的系统显示这个图像:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/5851b088ce361c5edc286afe9c497403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*UiJeTdihtk8CqdYf51qcSw.jpeg"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">作者图片</figcaption></figure><p id="a817" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">恼人的是，它无法识别图片中非常明显的狮子。通过只标记面部，你无意中使系统偏向正面的狮子图片！</p><h1 id="ea05" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">聚合偏差</strong></h1><p id="3263" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">有时我们聚集数据来简化它，或者以一种特殊的方式呈现它。这可能会导致偏差，不管它发生在创建模型之前还是之后。例如，看一下这张图表:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mr"><img src="../Images/3a11bdbd96e0032c0ac746cee496063c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fEU9QlWWc6anEO-s"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">作者图片</figcaption></figure><p id="2d4c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">它显示了工资是如何根据工作年限增长的。这里有一个很强的相关性，你工作的时间越长，你得到的报酬就越多。现在，让我们来看看用于创建该聚合的数据:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mr"><img src="../Images/620f5d6c7fac5b0c3a750cf98d213d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TbJiJJ4V3dHg-VC2"/></div></div></figure><p id="58d6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们看到，对于运动员来说，情况正好相反。在职业生涯的早期，当他们还处于体能巅峰时，他们能够获得高薪，但随着他们停止竞争，薪水就会下降。通过把他们和其他职业放在一起，我们让我们的算法对他们有偏见。</p><h1 id="8e33" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">确认偏差</strong></h1><p id="fbb3" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">简而言之，确认偏见是我们倾向于相信确认我们现有信念的信息，或者丢弃不确认的信息。理论上，我可以建立有史以来最准确的ML系统，在数据或建模方面都没有偏见，但如果你打算根据自己的“直觉”改变结果，那就没关系了。</p><p id="d850" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">确认偏差在机器学习的应用中尤其普遍，在这些应用中，在采取任何行动之前都需要人工审查。人工智能在医疗保健中的使用已经看到医生对算法诊断不屑一顾，因为它不符合他们自己的经验或理解。通常当调查时，结果是医生没有阅读最新的研究文献，这些文献指出了稍微不同的症状、技术或诊断结果。最终，一个医生可以阅读的研究期刊是有限的(特别是在全职拯救生命的时候)，但是ML系统可以将它们全部吸收。</p><h1 id="2915" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">评估偏差</strong></h1><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ms"><img src="../Images/0f9d4b98d1b101f44abeb618ee20de50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_AJd1mIjdLgxM4K-"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk translated">由<a class="ae ku" href="https://unsplash.com/@ajaegers?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿诺·杰格斯</a>在<a class="ae ku" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="1fbf" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们想象一下，你正在建立一个机器学习模型来预测大选期间全国的投票率。你希望通过年龄、职业、收入和政治倾向等一系列特征，可以准确预测某人是否会投票。你建立了你的模型，用你当地的选举来检验它，并且对你的结果非常满意。似乎你可以在95%的情况下正确预测某人是否会投票。</p><p id="1bc0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">随着大选的临近，你突然感到非常失望。你花了很长时间设计和测试的模型只有55%的正确率——表现只比随机猜测好一点点。糟糕的结果是评价偏差的一个例子。通过只对你所在地区的人评估你的模型，你无意中设计了一个只对他们有效的系统。这个国家的其他地区，具有完全不同的投票模式，还没有被适当地考虑进去，即使它们被包括在你的初始训练数据中。</p><h1 id="09a3" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">结论</strong></h1><p id="6e95" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">你现在已经看到了偏见影响机器学习的六种不同方式。虽然这不是一个详尽的列表，但它应该让您很好地理解ML系统最终变得有偏差的最常见的方式。如果你有兴趣进一步阅读，我推荐Mehrabi等人的这篇论文。</p></div></div>    
</body>
</html>