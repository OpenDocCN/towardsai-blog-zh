<html>
<head>
<title>Closed-form and Gradient Descent Regression Explained with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python解释封闭形式和梯度下降回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/closed-form-and-gradient-descent-regression-explained-with-python-1627c9eeb60e?source=collection_archive---------2-----------------------#2020-08-07">https://pub.towardsai.net/closed-form-and-gradient-descent-regression-explained-with-python-1627c9eeb60e?source=collection_archive---------2-----------------------#2020-08-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="98de" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><div class=""><h2 id="461b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">回归问题的简化及其在Python中的实现</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4e6062228d7d7bbfa108fc8188ceb97d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vwYQVmqPxUnP0z1kreYOrw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@jsnbrsc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">杰森·布里斯科</a>在<a class="ae lh" href="https://unsplash.com/s/photos/stock?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="644e" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="d819" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">回归是机器学习中的一种监督学习算法。这是一种对因变量(或目标、响应)、<strong class="mc jd"><em class="mw"/></strong>和解释变量(或输入、预测值)、<strong class="mc jd"><em class="mw">【X】</em></strong>之间的关系<strong class="mc jd">建模的方法。例如，它的目标是预测目标变量的数量；预测股票价格，这不同于分类问题，例如，我们要预测目标的标签；预测股票的方向(上涨或下跌)。</strong></p><p id="41b5" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">而且，一个回归可以用来回答几个变量是否和<strong class="mc jd">如何相关</strong>，或者相互影响，例如；确定<strong class="mc jd"> <em class="mw">如果</em> </strong>和<strong class="mc jd"><em class="mw"/></strong>工作经历或年龄在多大程度上影响薪水。</p><p id="0270" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">在本文中，我将主要关注线性回归及其方法。</p><h1 id="c1cc" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">线性回归的不同方法</h1><p id="d01c" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">OLS(普通最小二乘法)的目标是找到使垂直偏移最小化的最佳拟合线(超平面)，垂直偏移可以是目标变量和预测输出之间的均方误差(MSE)或其他误差度量(MAE，RMSE)。</p><p id="e83a" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">我们可以使用以下方法实现线性回归模型:</p><ol class=""><li id="749b" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated">求解模型参数(封闭型方程)</li><li id="7c76" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">使用优化算法(梯度下降、随机梯度等。)</li></ol><p id="b8bf" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">请注意，OLS回归估计量是<strong class="mc jd"> <em class="mw">最佳线性无偏估计量</em> </strong>(简称蓝色)。在其他形式的回归中，参数估计可能有偏差，例如；当数据中存在共线性时，岭回归有时用于减少估计值的方差。但偏倚和方差的讨论不在本文讨论范围内(请参考这篇<a class="ae lh" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">有关偏倚和方差的大文章</a>)。</p><h2 id="2263" class="nq lj it bd lk nr ns dn lo nt nu dp ls mj nv nw lu mn nx ny lw mr nz oa ly iz bi translated">封闭型方程</h2><p id="bc8e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">假设我们有X大小n的输入和一个目标变量，我们可以写出下面的等式来表示线性回归模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/ca682f4b9add7f0ded29f73d338dcc52.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*aenkI0FtEcWHSF5veygdkg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">线性回归的简单形式(其中i = 1，2，…，n)</figcaption></figure><p id="622c" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">该等式假设截距X0 = 1。还有一个没有截距的模型，其中B0 = 0，但这是基于一些假设，即它将始终通过原点(有很多关于这个主题的讨论，你可以在这里阅读更多<a class="ae lh" href="https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model" rel="noopener ugc nofollow" target="_blank"/>和<a class="ae lh" href="http://web.ist.utl.pt/~mcasquilho/compute/errtheory/,regression/regrthroughorigin.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>)。</p><p id="cb3b" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">根据上面的等式，我们可以基于下面的计算来计算回归参数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/04880b91da27ce89c9635ac9c1e047bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*Bt1LjOBnr9vkwPTotwM2yA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">多元回归模型的矩阵公式</figcaption></figure><p id="2882" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">现在，让我们用Python来实现它，有三种方法可以做到这一点；手动矩阵乘法，<strong class="mc jd"> <em class="mw"> statsmodels </em> </strong>库，以及<strong class="mc jd"> <em class="mw"> sklearn </em> </strong>库。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/02d3b45336a4fe3fe90d8601a91eb22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*WLBocK6CN2CJH8eOxsPS7g.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">模型权重</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/29f6fc6f9cb97b6522a855e109e093d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*abGYH2WMzvXLfqs5DRiNDw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">单一线性模型示例，一个输入(左)和模型预测(右)</figcaption></figure><p id="6043" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">您可以看到，所有三种解决方案给出了相同的结果，然后我们可以使用输出来编写模型方程(Y =0.7914715+1.38594198X <strong class="mc jd">)。</strong></p><p id="6b78" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">这种方法为较小的数据提供了一个更好的解决方案，简单，快速的解释模型。</p><h2 id="9e7d" class="nq lj it bd lk nr ns dn lo nt nu dp ls mj nv nw lu mn nx ny lw mr nz oa ly iz bi translated">梯度下降</h2><p id="0530" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">如果闭型方程可以解决回归问题，我们为什么需要梯度下降。有些情况下。</p><ul class=""><li id="7a8d" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv og ni nj nk bi translated">大多数非线性回归问题没有封闭解。</li><li id="e001" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv og ni nj nk bi translated">即使在线性回归中，在某些情况下使用该公式也是不切实际的。一个例子是当X是一个非常大的稀疏矩阵时。该解决方案的计算成本太高。</li></ul><p id="72cf" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">梯度下降是一种计算成本更低(更快)的解决方案。</p><p id="e439" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">梯度下降是一种优化算法，用于通过沿最陡下降方向重复移动来最小化某个成本函数。因此，在每个时期之后更新模型权重。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2f3a23909e82bf093fb9f7f5f6f523fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*vdaHdaSROpE4wHNTjtrU3Q.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">梯度下降的基本可视化-理想情况下，梯度下降试图向全局最小值收敛</figcaption></figure><p id="c3cd" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">在机器学习算法中使用的梯度下降有三种主要类型；</p><ol class=""><li id="2d49" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated">批量梯度下降</li><li id="cbc4" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">随机梯度下降</li><li id="0291" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated">小批量梯度下降</li></ol><p id="a6b3" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">让我们更详细地了解每种类型及其实现。</p><h2 id="4181" class="nq lj it bd lk nr ns dn lo nt nu dp ls mj nv nw lu mn nx ny lw mr nz oa ly iz bi translated">批量梯度下降</h2><p id="b9da" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">这种方法是最直接的。它计算训练集中每个观察值的误差。<strong class="mc jd">评估完所有训练观察值后，它将更新模型参数</strong>。这个过程可以被称为<em class="mw">训练时期</em>。</p><p id="77c9" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">这种方法的主要优点是计算效率高，并且产生稳定的误差梯度和稳定的收敛，然而，它需要存储器中的整个训练集，并且稳定的误差梯度有时会导致不是最佳的模型(收敛到局部最小值陷阱，而不是试图找到最佳的全局最小值)。</p><p id="6f9c" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">让我们观察一下回归问题的python实现。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/ca210a10c51f080f002aea547c7847f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*t7VufDuKIHm9ruMJqH6X-g.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">线性回归的成本函数</figcaption></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/a9c74753938bb2d1d801b6615f2d46a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wU_VnCXcxQZnV8wJuRKT_g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">批量梯度下降—每个时期的成本和MSE</figcaption></figure><p id="fa56" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">正如我们所看到的，成本正在稳步下降，并达到大约150-200个纪元的最小值。</p><p id="1624" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">在计算过程中，我们还使用了矢量化以获得更好的性能。但是，如果训练集非常大，性能会比较慢。</p><h2 id="0e95" class="nq lj it bd lk nr ns dn lo nt nu dp ls mj nv nw lu mn nx ny lw mr nz oa ly iz bi translated">随机梯度下降</h2><p id="b2b5" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在随机梯度下降中，SGD(或有时称为<em class="mw">迭代</em>或<em class="mw">在线</em> GD)。“随机”和“在线GD”的名称来自于这样一个事实，即基于梯度的<em class="mw">单次训练观察</em>是真实成本梯度的“随机近似”。然而，由于这个原因，通向全局成本最小化的路径不是直接的，并且在收敛到全局成本最小化之前可能上下起伏。</p><p id="4a38" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">因此；</p><ul class=""><li id="152f" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv og ni nj nk bi translated">这使得SGD比批处理GD更快(在大多数情况下)。</li><li id="9439" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv og ni nj nk bi translated">我们可以实时查看模型的洞察力和改进率。</li><li id="b07f" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv og ni nj nk bi translated">增加模型更新频率可以导致更快的学习。</li><li id="83df" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv og ni nj nk bi translated">随机性质的噪声更新有助于避免局部最小值。</li></ul><p id="a9dc" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">然而，一些缺点是:</p><ul class=""><li id="7c2e" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv og ni nj nk bi translated">由于更新频率，这可能是更昂贵的计算，这可能需要比另一种方法更长的时间来完成。</li><li id="c92d" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv og ni nj nk bi translated">频繁的更新将导致有噪声的梯度信号，这导致模型参数和误差跳跃，在训练时期上有较高的方差。</li></ul><p id="81a4" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">让我们看看如何用Python实现这一点。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/e7c8a58cecd2bfc6505cedff51a08209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vzKykDmVUrlMbzKAmL539w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">随机梯度下降—每个时期的性能</figcaption></figure><h2 id="8d99" class="nq lj it bd lk nr ns dn lo nt nu dp ls mj nv nw lu mn nx ny lw mr nz oa ly iz bi translated">小批量梯度下降</h2><p id="27af" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">小批量梯度下降(MB-GD)是更优选的方法，因为它折衷了批量梯度下降和随机梯度下降。它将训练集分成小批，并提供给算法。该模型将基于这些批次获得更新。该模型将比批量GD收敛得更快，因为权重更新得更频繁。</p><p id="241d" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">该方法结合了批量遗传算法的高效性和随机遗传算法的鲁棒性。一个(小的)缺点是这种方法引入了一个新的参数“批量”，这可能需要作为模型调整/优化的一部分进行微调。</p><p id="dfa8" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">我们可以把批量想象成学习过程中的一个滑块。</p><ul class=""><li id="7d70" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv og ni nj nk bi translated">小的值使学习过程以训练过程中的噪声为代价快速收敛</li><li id="5e9f" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv og ni nj nk bi translated">较大的值给出了学习过程收敛较慢的误差梯度的精确估计</li></ul><p id="c8a4" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">我们可以重用上面的函数，但是需要指定批量大小为<strong class="mc jd"> <em class="mw"> len(训练集)&gt; batch_size &gt; 1。</em>T9】</strong></p><p id="448f" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated"><code class="fe ol om on oo b">theta, _, mse_ = _sgd_regressor(X_, y, learning_rate=learning_rate, n_epochs=n_epochs, batch_size=50)</code></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/a72a97c79d6f145cb3eecbc3425a61b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKc1_sAfw4LErobl0pRpJg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">小批量梯度下降—一个时期内的性能</figcaption></figure><p id="6fd7" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">我们可以看到，只有前几个历元，模型能够立即收敛。</p><h2 id="5e60" class="nq lj it bd lk nr ns dn lo nt nu dp ls mj nv nw lu mn nx ny lw mr nz oa ly iz bi translated">新加坡元回归(scikit-learn)</h2><p id="7e33" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在python中，我们可以通过使用<code class="fe ol om on oo b">sklearn.linear_model.SGDRegressor</code>在回归问题上实现梯度下降方法。更多详情请参考<a class="ae lh" href="https://scikit-learn.org/stable/modules/sgd.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="1446" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">下面是我们如何实现随机和小批量梯度下降法。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/901eb46394f073af6beacb317de32bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*S0YlNOTBltdLQRVXWtFXSQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">sci kit-了解SGD型号的详细信息和性能</figcaption></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/f2aec453cf351ce2e9e428f66e90230d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*2bG2GJ8xPLFaU345h0noYg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">sci kit-了解SGD小批量模型的详细信息和性能</figcaption></figure><h1 id="ad50" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">尾注</h1><p id="3dd5" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在这篇文章中，我解释了线性回归的封闭形式方程和优化算法，梯度下降，从头实现它们，并使用内置的库。</p></div><div class="ab cl os ot hx ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="im in io ip iq"><h2 id="f10a" class="nq lj it bd lk nr ns dn lo nt nu dp ls mj nv nw lu mn nx ny lw mr nz oa ly iz bi translated">附加阅读和Github库:</h2><div class="oz pa gp gr pb pc"><a href="https://online.stat.psu.edu/stat462/node/132/" rel="noopener  ugc nofollow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">5.4-多元回归模型的矩阵公式</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">注意:这节课对那些在毕业后继续学习统计学的学生来说是最重要的</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">online.stat.psu.edu</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq lb pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a href="https://arxiv.org/abs/1206.5533" rel="noopener  ugc nofollow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">深度架构基于梯度训练的实用建议</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">与人工神经网络相关的学习算法，尤其是深度学习算法，可能会涉及许多…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="oz pa gp gr pb pc"><a href="https://github.com/netsatsawat/close-form-and-gradient-descent-regression-explained/blob/master/code/linear%20regression%20explained.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">netsatsawat/闭合形式和梯度下降回归-解释</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">github.com</p></div></div><div class="pl l"><div class="pr l pn po pp pl pq lb pc"/></div></div></a></div></div></div>    
</body>
</html>