<html>
<head>
<title>Easy Object Detection with Transformers: Simple Implementation of Pix2Seq Model in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用变压器实现简单的对象检测:PyTorch中Pix2Seq模型的简单实现</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/easy-object-detection-with-transformers-simple-implementation-of-pix2seq-model-in-pytorch-fde3e7162ce7?source=collection_archive---------1-----------------------#2022-08-30">https://pub.towardsai.net/easy-object-detection-with-transformers-simple-implementation-of-pix2seq-model-in-pytorch-fde3e7162ce7?source=collection_archive---------1-----------------------#2022-08-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/dd9894dbdc0a4a4b5739b6de8ae69862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T4a7BeMb9l3cCMkODJOJTg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">我对作者的Pix2Seq | Image的简单实现</figcaption></figure><h1 id="c0c1" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">介绍</h1><p id="84ad" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi ly translated">目标检测不一定是一项困难的任务！我清楚地记得我第一次从零开始实现YOLO的时候，理解它是如何工作的是一件痛苦的事情。对于计算机视觉应用的初学者来说，我相信<strong class="lc ir">物体检测是分类、分割等中最难理解的</strong>。</p><p id="5ebb" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">当我第一次听说来自<strong class="lc ir"> ICLR 2022 </strong>的论文“<a class="ae mm" href="https://arxiv.org/abs/2109.10852" rel="noopener ugc nofollow" target="_blank"> Pix2seq:用于对象检测的语言建模框架</a>”时，我变得相当兴奋，我确信我的下一篇博客文章将会是关于它的；所以，我写了这篇文章，希望你会喜欢，并发现pix2seq模型很容易理解和实现。</p><p id="2a54" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">在本教程结束时，您将学习实现一个简单的对象检测模型，它会产生以下结果:</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/3f291bc0043b7c51e18e55bc0d409ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F7eXT12Zp-W5FhUzdFuXPA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">我们的最终模型在验证集上的预测|作者图片</figcaption></figure><p id="a954" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">我把我所有的代码都作为<a class="ae mm" href="https://colab.research.google.com/drive/1UeYIZ6_GHNwCHSi8nNV5dVc3oUrM5-BA?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir"> Google Colab笔记本</strong> </a>和<a class="ae mm" href="https://www.kaggle.com/code/moeinshariatnia/object-detection-w-transformers-pix2seq-pytorch/notebook" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir"> Kaggle笔记本</strong> </a>提供。我还把整个项目和代码<a class="ae mm" href="https://github.com/moein-shariatnia/Pix2Seq" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir">放到了我的GitHub </strong> </a>上。</p><h2 id="ac2f" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">这篇论文的有趣之处在于</h2><p id="c47b" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这个想法非常简单:将对象检测问题<strong class="lc ir">重新定义为文本(令牌)生成任务</strong>！我们希望模型"<strong class="lc ir">告诉我们</strong>"图像中存在哪些对象，以及它们的边界框(bboxes)的(x，y)坐标，所有这些都以特定的格式在生成的序列中显示，就像文本生成一样！</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/d5752c1903e773cb4a6520cb2866e90d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O-Y26k10JSXBcHZbMhsbeA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">pix2seq的工作原理:它生成一系列标记，告诉每个对象在哪里(BOS =句首，EOS =句尾)|作者图片</figcaption></figure><p id="f643" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">正如你所看到的，<strong class="lc ir">物体检测</strong>任务被转换成类似<strong class="lc ir">图像字幕</strong>的任务:在文本(序列)中描述图像，但这次告诉我们物体的确切位置。</p><h1 id="694b" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Pix2Seq:简单实现</h1><h2 id="0a54" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">所需模块</h2><p id="a2b4" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">与Pix2Seq最接近的任务是图像字幕。因此，我们将需要一个<strong class="lc ir">图像编码器</strong>来将图像转换成隐藏表示的向量，然后需要一个<strong class="lc ir">解码器</strong>来获取图像表示和先前生成的标记的图像表示，并预测下一个标记。我们还需要一个记号化器来将对象类和坐标转换成形成它们的特殊词汇的记号，就像自然语言中的单词一样。</p><h2 id="bc8c" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">我对Pix2Seq的简单实现</h2><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/dd9894dbdc0a4a4b5739b6de8ae69862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T4a7BeMb9l3cCMkODJOJTg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">我对作者的Pix2Seq | Image的简单实现</figcaption></figure><p id="7fea" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">上图可以看到这个项目的高层管道。如你所见，我们需要一个图像数据集和它们的框，我们将使用<a class="ae mm" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir"> Pascal VOC 2012数据集</strong> </a>。接下来，我们将从头开始编写自己的<strong class="lc ir">记号赋予器</strong>，将bbox类和坐标转换成一系列记号。然后，我们将使用<strong class="lc ir"> DeiT </strong>(来自这篇<a class="ae mm" href="https://arxiv.org/abs/2012.12877" rel="noopener ugc nofollow" target="_blank">论文</a>)作为我们的<strong class="lc ir">图像编码器</strong>，并将图像嵌入馈送到一个普通的<strong class="lc ir">变压器解码器</strong>(来自这篇<a class="ae mm" href="https://arxiv.org/abs/1706.03762?context=cs" rel="noopener ugc nofollow" target="_blank">论文</a>)。解码器的任务是根据前一个标记预测下一个标记。解码器的输出被提供给语言建模损失函数。</p><p id="90b3" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">本教程的代码可在以下链接获得:<br/> - <a class="ae mm" href="https://colab.research.google.com/drive/1UeYIZ6_GHNwCHSi8nNV5dVc3oUrM5-BA?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir"> Google Colab笔记本</strong></a><br/>-<a class="ae mm" href="https://www.kaggle.com/code/moeinshariatnia/object-detection-w-transformers-pix2seq-pytorch/notebook" rel="noopener ugc nofollow" target="_blank"><strong class="lc ir">ka ggle笔记本</strong> </a> <br/> - <a class="ae mm" href="https://github.com/moein-shariatnia/Pix2Seq" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir">我的GitHub repo </strong> </a></p><h2 id="6d84" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated"><strong class="ak">数据集</strong></h2><p id="d673" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">正如我前面提到的，我们将使用VOC 2012数据集和来自<strong class="lc ir"> 20类</strong>的图像及其相应的对象。该论文使用COCO数据集，该数据集比VOC大一个数量级，并且他们还在COCO上训练之前在更大的数据集上预先训练模型。但是，为了简单起见，我将使用这个相当小的VOC数据集。</p><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">Pascal VOC 2012课程</figcaption></figure><p id="1267" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">我们需要一个PyTorch数据集类，它以序列的形式为我们提供图像及其bbox坐标和类。</p><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="7344" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">如您所见，这里的大部分代码都是您期望从简单的分类数据集得到的，但是也有一些小的差异。我们需要一个<strong class="lc ir">记号赋予器</strong>将我们的标签和bbox坐标(x和y)转换成一个序列，这样我们就可以为语言建模任务执行训练我们的模型(根据之前看到的记号预测下一个记号)。</p><h2 id="57ae" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">标记器</h2><p id="6ef1" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我们如何将这些信息转换成一个序列？嗯，没那么难。为了表示图像中的对象，我们需要5个数字:4个坐标数字和1个数字来表示它属于哪个类。</p><p id="f651" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">你实际上需要知道一个边界框的2个点的坐标，以便能够在图像中画出它；在<strong class="lc ir"> pascal格式</strong>中，我们用bbox的<strong class="lc ir">左上点</strong>和<strong class="lc ir">右下点</strong>作为那两个临界点，每个点用它的x和y值来表示→所以，我们总共需要4个数来画一个包围盒。你可以看到下面的替代格式来表示一个边界框。另外，看看x和y轴的起点在哪里(0，0点)。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/26f100c1700af3f6d5c83236022063f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OOkPjkPAoC_VlzbxJyK-5A.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">用不同的格式来表示边界框及其坐标|来自<a class="ae mm" href="https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/" rel="noopener ugc nofollow" target="_blank">相册文档</a>的图像</figcaption></figure><p id="0a00" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">正如您在数据集的代码中看到的，我们将bbox坐标和标签交给我们的标记器，并得到一个简单的标记列表。分词器需要完成以下任务:</p><ol class=""><li id="5586" class="nh ni iq lc b ld mh lh mi ll nj lp nk lt nl lx nm nn no np bi translated"><strong class="lc ir">标记w/e特殊令牌(BOS和EOS令牌)序列的开始和结束</strong>。</li><li id="365d" class="nh ni iq lc b ld nq lh nr ll ns lp nt lt nu lx nm nn no np bi translated"><strong class="lc ir">量化</strong>坐标的连续值(我们可以将x=34.7作为一个点的坐标，但是我们需要像34这样的离散值作为我们的记号，因为我们最终是在一组有限的记号上进行分类)</li><li id="2638" class="nh ni iq lc b ld nq lh nr ll ns lp nt lt nu lx nm nn no np bi translated"><strong class="lc ir">将对象的标签</strong>编码成相应的令牌</li><li id="c941" class="nh ni iq lc b ld nq lh nr ll ns lp nt lt nu lx nm nn no np bi translated"><strong class="lc ir">在最终序列中随机化对象的顺序</strong>(更多信息见下文)</li></ol><p id="8596" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">如果你熟悉NLP应用程序，这些步骤可能听起来很熟悉，因为我们在处理自然语言中的单词时也会用到它们；我们需要对它们进行标记，并将每个单词分配给它自己的离散标记，标记序列的开始和结束，等等。</p><p id="42bb" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">关于这个列表中的数字4，这就是这篇论文所做的，并且有一个关于它是否是一个好主意的广泛的消融研究。它说的是，每次我们向模型显示相同的图像(在不同的时期)，我们随机化对象在相应序列中出现的顺序，我们将该序列提供给模型(其中一个令牌被移位)和我们的损失函数。例如，如果图像中有“人”、“车”和“猫”，记号赋予器和数据集会将这些对象按随机顺序放入序列中:</p><ul class=""><li id="fc41" class="nh ni iq lc b ld mh lh mi ll nj lp nk lt nl lx nv nn no np bi translated">BOS，car_xmin，car_ymin，car_xmax，car_ymax，car_label，person_xmin，person_ymin，person_xmax，person_ymax，person_label，cat_xmin，cat_ymin，cat_xmax，cat_ymax，cat_label，EOS</li><li id="2bea" class="nh ni iq lc b ld nq lh nr ll ns lp nt lt nu lx nv nn no np bi translated">BOS，person_xmin，person_ymin，person_xmax，person_ymax，person_label，car_xmin，car_ymin，car_xmax，car_ymax，car_label，cat_xmin，cat_ymin，cat_xmax，cat_ymax，cat_label，EOS</li><li id="6b7c" class="nh ni iq lc b ld nq lh nr ll ns lp nt lt nu lx nv nn no np bi">…</li></ul><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="fe31" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">关于坐标连续值如何量化的另一个注意事项:假设<strong class="lc ir">图像大小为224 </strong>。这4个坐标(12.2，35.8，68.1，120.5)可以有一个bbox。</p><p id="8cca" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">你将需要至少<strong class="lc ir"> 224个令牌</strong> (num_bins)才能以1个像素的精度令牌化(量化)这4个数字(你将丢失1个像素以下的信息)。正如您在标记器代码中看到的，要将这些bbox坐标转换成它们的标记化版本，我们需要执行以下操作:</p><ol class=""><li id="bd27" class="nh ni iq lc b ld mh lh mi ll nj lp nk lt nl lx nm nn no np bi translated">标准化坐标(通过将它们除以最大值= 224，使它们在0和1之间)</li><li id="5e46" class="nh ni iq lc b ld nq lh nr ll ns lp nt lt nu lx nm nn no np bi translated">这样做:<em class="nw"> int(x * (num_bins-1)) </em></li></ol><p id="23ab" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">所以，转换后的版本将是:(12，35，67，119)。请记住，Python <strong class="lc ir">中的int()函数不会将数字四舍五入到最接近的整数</strong>，而是只保留数字的整数部分。如你所见，我们丢失了一些关于bbox确切位置的信息，但它仍然是一个非常好的近似值。我们可以使用更大数量的标记(如论文中所述的箱数)，并且我们将具有更精确的位置。我们的标记器也有decode()函数，我们将使用它将序列转换成bbox坐标和标签。</p><h2 id="9633" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">校对功能</h2><p id="9f57" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在这里，我们将实现一个自定义的collate_function来提供给我们的PyTorch数据加载器。这个函数将为我们处理填充:通过将PAD_IDX添加到较短的序列中，使所有的序列长度相同，以便能够用它们构建一个批处理。我们将填充序列到300个令牌的固定最大长度。</p><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><h2 id="f3ae" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">编码器</h2><p id="8125" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我终于到了对每个深度学习爱好者来说最酷的部分:<strong class="lc ir">模型😍</strong></p><p id="93df" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">我们再来看一下本教程的<a class="ae mm" href="https://drive.google.com/file/d/19zuuskVKj4GM_9uGcA8a-FBPr1ykuiK6/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">第一张图。首先，我们需要一个编码器来获取输入图像，并给我们一些嵌入(表示)。论文用的是ResNet50(其他实验也用ViT)，但我决定用</a><a class="ae mm" href="https://arxiv.org/abs/2012.12877" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir"> DeiT </strong> </a>。顾名思义，这是一个数据高效的视觉转换器，我认为它非常适合我们的小数据集。像ViT一样，它将图像分割成小块，并像处理句子中的单词一样处理它们，这对于我们的任务来说也是很好的，因为我们将为每个小块进行单独的嵌入，我们可以在下一部分将它们交给我们的解码器来预测目标序列(参见从英语到法语的<strong class="lc ir">翻译</strong>，其中我们的图像就像英语中的一个句子，而包含bboxes的坐标和标签的目标序列就像法语中的对应句子)。</p><p id="a3fb" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">我将使用timm库来实现一个预先训练好的DeiT模型。</p><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="e639" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">瓶颈层是将这些嵌入的特征数量减少到解码器的特征数量。本文使用了256的解码器dim，这就是我在这里使用<strong class="lc ir">平均池</strong>减少它的原因。此外，这个模型中的第一个令牌与CLS令牌相关，我将在forward方法中跳过它(features[:，1:])。</p><h2 id="1a90" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">解码器</h2><p id="3e4a" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我们的解码器采用输入图像的补丁嵌入，并学习预测包含bboxes的序列。这里我用的是PyTorch <strong class="lc ir"> nn。TransformerDecoder </strong>模块实现一个特征尺寸为256的6层解码器。我们还需要向嵌入中添加位置嵌入，以便模型知道每个标记在序列中的位置(我为编码器标记和解码器标记都添加了位置嵌入。虽然我们必须为解码器这样做，但我们可能不需要将它们添加到编码器标记中，因为DeiT模型知道补丁本身的顺序)。我就是靠那些<strong class="lc ir"> nn做到的。参数</strong>模块将在每个标记位置学习1个参数。最后，我们将使用一个<strong class="lc ir"> nn。线性</strong>层来预测我们词汇表中的下一个标记。</p><p id="ebbb" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated"><strong class="lc ir"> create_mask </strong>函数为我们提供了训练解码器所需的两个掩码:一个告诉模型忽略填充令牌并且不将它们合并到它的注意模块中，另一个屏蔽未来的令牌，以便使解码器仅通过查看当前令牌和先前令牌来预测令牌。</p><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><h2 id="8ffc" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">把它们放在一起</h2><p id="03bd" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这是一个封装了编码器和解码器的简单类。它还有一个<strong class="lc ir">预测</strong>函数，调用解码器的预测函数(上面没有显示，我们稍后会看到)来检测图像中的对象。</p><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><h2 id="36cf" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">培养</h2><p id="44bf" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">现在让我们看看如何训练这个模型。下面的大部分代码只是标准的PyTorch培训样板，但其中有一点很简单但很重要。如前所述，我们像训练语言模型一样训练模型(例如GPT)，它是这样工作的→模型只需要通过看到前面的标记(左边的标记)来预测下一个标记。开始时，它只看到BOS语句，它需要预测下一个标记，以此类推。这可以通过这部分来实现:</p><ul class=""><li id="8d2a" class="nh ni iq lc b ld mh lh mi ll nj lp nk lt nl lx nv nn no np bi translated">y_input = y[:，:-1]</li><li id="2f9f" class="nh ni iq lc b ld nq lh nr ll ns lp nt lt nu lx nv nn no np bi translated">y_expected = y[:，1:]</li><li id="3653" class="nh ni iq lc b ld nq lh nr ll ns lp nt lt nu lx nv nn no np bi translated">preds =模型(x，y _输入)</li></ul><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="2755" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">我在Kaggle上使用单个GPU训练了这个模型，仅在6小时内进行了25次。这并不多，但足以在评估指标上获得不错的表现。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/d0265a39e6f1d85093eadce3e1811257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bckLrB1gfrVFsDYC1CeR_A.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">:)</figcaption></figure><p id="5c0b" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">物体检测最常见的度量是<strong class="lc ir">平均精度(AP) </strong>你可以在这里阅读更多关于它的<a class="ae mm" href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener">。经过几个小时的大量数据训练，本文得到了一个43 w/ ResNet50骨干网的AP。用这个小模型和短训练时间，我可以在我的验证集上得到26.4的AP，这很酷，因为这是一个如何轻松实现本文的教程，我的目标不是用这个打败SOTA！</a></p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/ca70981b64ee1620e9f07fe8a566c631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pC6cIWqoiuTfTngkYKG0Yw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">我的模型在Pascal VOC数据集上的平均精度表现|图片由作者提供</figcaption></figure><h2 id="fb06" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">推理</h2><p id="1717" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">现在，让我们看看如何使用这个模型为测试图像生成检测序列。这是解码器类的预测方法。它获取先前生成的标记，将它们填充到max_length，并预测批中每个序列的下一个标记，然后返回这些新标记。</p><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="a701" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">下面的<strong class="lc ir"> generate() </strong>函数显示了整个序列生成管道的简化版本→首先，我们将使用shape (batch_size，1)创建一个批处理，该批处理只包含批处理中每个图像的BOS标记。该模型获取图像和这些BOS标记，然后预测每个图像的下一个标记。我们获取模型的预测，对其执行softmax和argmax以获得预测的标记，并将这个新预测的标记与之前的batch_preds张量连接，该张量具有BOS标记。然后我们重复这个循环max_len次。</p><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="812f" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">最后，我们通过标记器的decode()方法对预测的标记进行解码。你可以看看我的GitHub、<a class="ae mm" href="https://colab.research.google.com/drive/1UeYIZ6_GHNwCHSi8nNV5dVc3oUrM5-BA?usp=sharing" rel="noopener ugc nofollow" target="_blank">Colab笔记本</a>或者<a class="ae mm" href="https://www.kaggle.com/code/moeinshariatnia/object-detection-w-transformers-pix2seq-pytorch/notebook" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>上的具体实现。</p><h2 id="fe2b" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">结果</h2><p id="1c6c" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">最后，让我们看看我们的模型到目前为止的一些结果。模型并不总是这么好(这些都是精心挑选的！)，但这些结果表明，实现已经足够好了，如果有更大的数据集和模型以及更长的训练时间，您可以很容易地获得论文中提到的那些完美的结果。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/d48378b8b5bd04b49b54b4cdd4ae45e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GxFw1PQJixzIjVmSBLYHUw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">我们的最终模型在验证集上的预测|作者图片</figcaption></figure><p id="749a" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">当图像中有1到2个bboxes时，我们的模型工作得最好，当图像中有大量对象时，它的性能会下降。下图显示了我们模型的一些失败案例:</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/d6f17db769b26ca4883a5a78a08dd78b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_lXKiJcNTW8SPasa0_XBQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">我们模型的一些失败案例|作者图片</figcaption></figure><h2 id="889d" class="ms kd iq bd ke mt mu dn ki mv mw dp km ll mx my kq lp mz na ku lt nb nc ky nd bi translated">最后的话</h2><p id="334f" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">我希望你喜欢这个教程，并学到了一些新的东西。一如既往，我将很高兴听到你对本教程的评论，或者回答你对论文和模型的任何问题。</p><p id="7f1d" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">祝您愉快！</p><p id="e9fa" class="pw-post-body-paragraph la lb iq lc b ld mh lf lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx ij bi translated">我把我所有的代码都作为<a class="ae mm" href="https://colab.research.google.com/drive/1UeYIZ6_GHNwCHSi8nNV5dVc3oUrM5-BA?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir"> Google Colab笔记本</strong> </a>和一个<a class="ae mm" href="https://www.kaggle.com/code/moeinshariatnia/object-detection-w-transformers-pix2seq-pytorch/notebook" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir"> Kaggle笔记本</strong> </a>提供。我还把整个项目和代码<a class="ae mm" href="https://github.com/moein-shariatnia/Pix2Seq" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ir">放到了我的GitHub </strong> </a>上。</p><blockquote class="ob oc od"><p id="1532" class="la lb nw lc b ld mh lf lg lh mi lj lk oe mj ln lo of mk lr ls og ml lv lw lx ij bi translated"><strong class="lc ir"> <em class="iq">关于我</em> </strong></p><p id="11fb" class="la lb nw lc b ld mh lf lg lh mi lj lk oe mj ln lo of mk lr ls og ml lv lw lx ij bi translated">我是Moein Shariatnia，一名医科学生，机器学习开发人员和研究员。在我的研究中，我对深度学习模型的泛化和迁移学习性能感兴趣。我也喜欢实现最先进的深度模型，并理解它们是如何工作的。</p><p id="0192" class="la lb nw lc b ld mh lf lg lh mi lj lk oe mj ln lo of mk lr ls og ml lv lw lx ij bi translated"><strong class="lc ir"> <em class="iq">我的谷歌学术</em></strong>:<a class="ae mm" href="https://scholar.google.com/citations?user=YLHsTOUAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://scholar.google.com/citations?user=YLHsTOUAAAAJ&amp;HL = en</em></a><br/><em class="iq">我的GitHub</em>:<a class="ae mm" href="https://github.com/moein-shariatnia" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://github.com/moein-shariatnia</em></a></p></blockquote></div></div>    
</body>
</html>