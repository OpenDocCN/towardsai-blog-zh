<html>
<head>
<title>Cross-lingual Language Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">跨语言语言模型</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/cross-lingual-language-model-56a65dba9358?source=collection_archive---------1-----------------------#2019-07-15">https://pub.towardsai.net/cross-lingual-language-model-56a65dba9358?source=collection_archive---------1-----------------------#2019-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4318" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">XLMs预训练模型| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">朝向AI </a></h2><div class=""/><div class=""><h2 id="18a2" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过多语言神经语言模型讨论XLMs和无监督跨语言单词嵌入</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1d4913012d0be38aa371b798385c0c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rS5Bffcr_Ge6Cr-7"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="161b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">预训练模型被证明可以改善下游问题。Lample和Conneau提出了两个新的训练目标来训练跨语言语言模型(XLM)。这种方法在跨语言自然语言推理(XNLI)方面取得了最新的成果。另一方面，Wada和Iwata提出了另一种在没有平行数据的情况下学习跨语言文本表示的方法。他们将其命名为多语言神经语言模型。</p><p id="1526" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本故事将讨论<a class="ae lh" href="https://arxiv.org/pdf/1901.07291.pdf" rel="noopener ugc nofollow" target="_blank">跨语言语言模型预训练</a> (Lample和Conneau，2019)和<a class="ae lh" href="https://arxiv.org/pdf/1809.02306.pdf" rel="noopener ugc nofollow" target="_blank">多语言神经语言模型的无监督跨语言单词嵌入</a> (Wada和Iwata，2018)</p><p id="4d69" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将涵盖以下内容:</p><ul class=""><li id="daad" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">数据</li><li id="374a" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">跨语言语言模型架构</li><li id="7ab3" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">多语言神经语言模型架构</li><li id="b6b6" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">实验</li></ul><h1 id="df57" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">数据</h1><p id="858f" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">Lample和Conneau将维基百科转储用于单语数据，而跨语言数据来自:</p><ul class=""><li id="fa32" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">MultiUN (Ziemski等人，2016年):法文、西班牙文、俄文、阿拉伯文和中文</li><li id="cd6d" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">IIT孟买语料库(Anoop等人，2018年):印地语</li><li id="937c" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">OPUS (Tiedemann，2012):德语、希腊语、保加利亚语、土耳其语、越南语、泰语、乌尔都语、斯瓦希里语和斯瓦希里语</li></ul><p id="97a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Wada和Iwata对除芬兰语之外的每种语言使用新闻搜索2012单语语料库，而对芬兰语使用新闻搜索2014。</p><h1 id="3f41" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">跨语言语言模型架构</h1><h2 id="584a" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">输入表示</h2><p id="792f" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">为了处理词汇外(OOV)和跨语言，应用字节对编码(<a class="ae lh" href="https://towardsdatascience.com/how-subword-helps-on-your-nlp-model-83dd1b836f46" rel="noopener" target="_blank"> BPE </a>)子字算法将一个字分割成子字。不是每种语言使用不同的子词集，而是共享相同的字母、数字、特殊标记和专有名词，以改进跨语言嵌入空格的对齐。</p><p id="2ed1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了子词，XLM还将位置嵌入(表示句子的位置)和语言嵌入(表示不同的语言)输入到不同的语言模型(LM)中来学习文本表示。这些LM是:</p><ul class=""><li id="2958" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">因果语言建模(CLM)</li><li id="ff29" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">掩蔽语言建模(MLM)</li><li id="f3ff" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">翻译语言建模(TLM)</li></ul><h2 id="236b" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">因果语言建模(CLM)</h2><p id="eba2" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">CLM由一个<a class="ae lh" href="https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b" rel="noopener" target="_blank">转换器</a>组成，通过提供一组先前的特性来学习文本表示。给定当前批次的前一个隐藏状态，模型预测下一个单词。</p><h2 id="4e36" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">掩蔽语言建模(MLM)</h2><p id="fd80" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">Lample和Connea遵循Devlin等人(2018)的方法，随机选取15%的子词，80%的时间用保留字([掩码])替换，10%的时间用随机工作替换，10%的时间保持不变。</p><p id="ab3c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Devlin等人(2018)的不同之处在于:</p><ul class=""><li id="6f77" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">使用任意数量的句子，而不仅仅是句子对</li><li id="bab4" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><a class="ae lh" href="https://towardsdatascience.com/how-negative-sampling-work-on-word2vec-7bf8d545b116" rel="noopener" target="_blank">子样</a>高频子字</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/c20087bf751602f647ac2916e06f8a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dy0NE-k3XYHuuuX8nlY9oA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">MLM建筑(Lample和Conneau，2019)</figcaption></figure><h2 id="99e2" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">翻译语言建模(TLM)</h2><p id="3910" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">CLM和MLM是为单语数据设计的，而TLM的目标是跨语言数据。BERT使用段嵌入来表示单个输入序列中不同句子，同时用语言嵌入来代替它来表示不同的语言。</p><p id="2c08" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在两种语言数据中随机选取子词。两种语言子词都可以用来预测任何屏蔽词。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/b4f2568792730183d40bfef0225aecd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q1qXIpnJ-jNa27sd6tXkcg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">TLM建筑(Lample和Conneau，2019)</figcaption></figure><h1 id="894f" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">多语言神经语言模型架构</h1><p id="a88e" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">Wada和Iwata注意到并行数据不适合低资源语言。由于该模型不能从并行数据中学习文本表示，子词嵌入在不同的语言中是不同的。然而，他们共享双向LSTM来学习多语言的单词嵌入。由于架构是跨语言共享的，Wada和Iwata认为，如果令牌相同，模型可以学习相似的嵌入。</p><p id="1224" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下图显示了此模型的体系结构，同时:</p><ul class=""><li id="bdb9" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">正向和反向LSTM网络</li><li id="d7cd" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">EBOS:嵌入的初始输入</li><li id="25ff" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">WEOS:指出下一个单词作为句子结尾的可能性有多大</li><li id="41c6" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">El:语言l的单词嵌入</li><li id="0220" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">Wl:语言El的线性投影，用于计算下一个单词的概率分布</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/2179ad1892673ea6ce3bcbeef411f697.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*EOUyy26AXAPBZYM0m2BUFw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">多语言神经语言模型的架构(Wada和Iwata 2018)</figcaption></figure><h1 id="4bfe" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">实验</h1><p id="fb99" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">基本上，XLM (MLM + TLM)取得了跨语言的好成绩。由于作者注意到CLM在跨语言问题中没有标度，他们在下面的模型比较中没有包括CLM的训练对象。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/3a9f9787b79ab2ac52af5810beea3b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-xh0584R2WhkOEqbbpHiw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">模型中的XLM结果(Lample和Conneau，2019)</figcaption></figure><p id="8c00" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于Wada和Iwata专注于解决只有少量单语数据可用，或者单语语料库的领域在语言场景中是不同的。他们打算用不同的数据集大小来看性能。下图展示了在数据集很小的情况下，该模型优于其他模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/391a26c95bcaf917fcca75ab7e3d9697.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*olCydHq5Uenp08vJywtYeg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">多语言神经语言模型的比较结果(Wada和Iwata 2018)</figcaption></figure><h1 id="c77b" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">拿走</h1><ul class=""><li id="b0aa" class="me mf it lk b ll nk lo nl lr of lv og lz oh md mj mk ml mm bi translated">伯特使用语段嵌入(代表不同句子)，而XLM使用语言嵌入(代表不同的语言)。</li><li id="74c5" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">CLM不能扩展到跨语言场景。</li><li id="467a" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">XLM可能不适合低资源语言，好像需要并行数据(TML)来提升性能。同时，多语言神经语言模型被设计来克服这个限制。</li></ul><h1 id="94ec" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新工作。欢迎在<a class="ae lh" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与<a class="ae lh" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>联系，或者在<a class="ae lh" href="http://medium.com/@makcedward/" rel="noopener"> Medium </a>或<a class="ae lh" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="be7f" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">延伸阅读</h1><ul class=""><li id="a617" class="me mf it lk b ll nk lo nl lr of lv og lz oh md mj mk ml mm bi translated"><a class="ae lh" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">变压器的双向编码器表示(BERT) </a></li><li id="9dea" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><a class="ae lh" href="https://towardsdatascience.com/how-subword-helps-on-your-nlp-model-83dd1b836f46" rel="noopener" target="_blank"> 3个子字算法帮助拆分一个字</a></li><li id="7f4b" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><a class="ae lh" href="https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b" rel="noopener" target="_blank">创成式预培训</a></li><li id="f515" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">XLM实现(<a class="ae lh" href="https://github.com/facebookresearch/XLM" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>)</li></ul><h1 id="1470" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">参考</h1><ul class=""><li id="a5d0" class="me mf it lk b ll nk lo nl lr of lv og lz oh md mj mk ml mm bi translated">G.兰普尔和a .康诺。<a class="ae lh" href="https://arxiv.org/pdf/1901.07291.pdf" rel="noopener ugc nofollow" target="_blank">跨语言语言模型预训练</a>。2019</li><li id="48a4" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">J.戴弗林，张文伟，李，图塔诺娃。<a class="ae lh" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a>。2018</li><li id="d32d" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">T.和田和岩田聪。<a class="ae lh" href="https://arxiv.org/pdf/1809.02306.pdf" rel="noopener ugc nofollow" target="_blank">通过多语言神经语言模型进行无监督的跨语言单词嵌入</a>。2018</li></ul></div></div>    
</body>
</html>