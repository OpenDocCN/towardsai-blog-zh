<html>
<head>
<title>Three Types of Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">三种类型的递归神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/three-types-of-recurrent-neural-networks-567b4e9c4261?source=collection_archive---------0-----------------------#2022-01-21">https://pub.towardsai.net/three-types-of-recurrent-neural-networks-567b4e9c4261?source=collection_archive---------0-----------------------#2022-01-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a322" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="8fa1" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">让我们来看看RNNs、LSTMs和GRUs</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b648144d442266e518ccbbdc2a7a1669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tJbaQFnGiFkxVMG7.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg" rel="noopener ugc nofollow" target="_blank">图片来自维基百科</a></figcaption></figure><p id="9adb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">递归神经网络是为序列数据设计的神经网络。序列数据是以前面的数据点影响后面的数据点的形式出现的任何数据。RNNs可以应用于图像数据、时间序列数据，以及最常见的语言数据。在本帖中，我们将介绍三种最常见的递归神经网络，以及如何在TensorFlow上的Keras中实现它们。</p><h1 id="ca9f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">三种类型的递归神经网络</h1><p id="9cab" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">三种最著名的递归神经网络是简单RNN、长短期记忆神经网络和门控递归单元神经网络。RNN建筑归功于大卫鲁梅尔哈特1986年的论文。十多年后，Hochreiter和Schmidhuber在1997年的一篇论文中展示了长短期记忆细胞的准确性优势。近二十年后，KyungHyun Cho等人用门控循环单位显示了某些类型数据的改善。</p><h2 id="2f89" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated">简单递归神经网络</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e0d9c8b7482397eae4fa675fd53015b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*rxs19VvVblTySbFu.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">RNN，<a class="ae lh" href="https://i.stack.imgur.com/KmrmP.png" rel="noopener ugc nofollow" target="_blank">图片来自堆栈交换</a></figcaption></figure><p id="eb67" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们要研究的第一种RNN是“简单的”递归神经网络。简单意味着网络的节点(或单元)不被修改。在大多数情况下，它还表明我们正在使用一个标准的全连接前馈神经网络作为我们的基础架构。通过为至少<em class="nn">一层神经元</em>添加一个“反馈”回路，从基本前馈神经网络扩充rnn。</p><p id="92f3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个反馈回路产生了递归神经网络的“递归”。这些反馈环是使RNNs成为序列数据理想选择的特征。有了这种设置，使用一个数据点在网络中运行一次就可以利用上一次运行的结果。</p><h2 id="3c7e" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated">长短期记忆神经网络</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/be11100138f7a7d9662d4b3ecaaeb42f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H70aBP2r6TD2KVbq.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">LSTM，<a class="ae lh" href="https://upload.wikimedia.org/wikipedia/commons/6/63/Long_Short-Term_Memory.svg" rel="noopener ugc nofollow" target="_blank">图片来自维基百科</a></figcaption></figure><p id="5122" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">长短期记忆实际上是指RNN中的细胞类型。整个网络的架构保持不变。当然，架构也可以有变化，但这里的主要创新是细胞类型。LSTM单元为单元引入了额外的三个“门”。</p><p id="9cb2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些门是输入门、输出门和遗忘门。这些门的引入创造了一个更复杂的方法来确定我们是否应该使用递归数据。这对于图像、视频或文字等非正常序列数据尤其有用。LSTMs已经被证明在视频数据、音频数据和异常检测上实现了高得多的准确度。</p><p id="222b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">LSTMs解决的RNN问题之一是<a class="ae lh" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度</a>的问题。在迭代过程中，梯度呈指数下降，梯度告诉网络它犯了多大的错误。LSTM通过允许梯度不变地流过网络来解决这个问题。然而，LSTMs仍然可能成为爆炸梯度的受害者，这是相反的效果。</p><h2 id="bb9b" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated">门控递归单元神经网络</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/2b543626c6b3739bd9fdc14e65781a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3V29obIloNacwKra.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">GRU，<a class="ae lh" href="https://upload.wikimedia.org/wikipedia/commons/5/5f/Gated_Recurrent_Unit.svg" rel="noopener ugc nofollow" target="_blank">图片来自维基百科</a></figcaption></figure><p id="5467" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">像LSTMs一样，门控循环单位也指细胞类型。GRU电池和LSTM电池的区别在于GRU电池没有输出门。这降低了GRUs的训练复杂性。与参数数量为四倍的LSTMs相比，GRU层的参数数量是简单RNN的三倍。</p><p id="b4d9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GRUs在大多数系列数据上表现出与LSTMs相似的性能，尤其是音频和语言数据。然而，gru在一些分类频率较低的较小数据集上表现更好。</p><h1 id="3e0a" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">RNN、LSTM和GRU的Python实现</h1><p id="a351" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们可以使用TensorFlow上的Keras轻松创建RNN、LSTM和GRU模型。请记住，RNNs、LSTMs和GRUs之间的主要区别在于单元的类型。在天真的意义上，任何具有递归层的神经网络都是RNN，任何具有LSTM或GRU细胞的神经网络分别是这些类型的模型。</p><p id="a5a7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在Keras中创建神经网络很简单。我们需要做的就是初始化一个模型类型并添加层。在我们的例子中，我们正在创建<code class="fe no np nq nr b">Sequential</code>模型。对于下面的三个示例，我们将为每种类型的RNN创建三层神经网络。在下面的例子中，你会注意到代码几乎完全相同。这是因为它们之间唯一需要改变的是细胞类型。</p><p id="4006" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，我们可以随意改变第一层的形状。我们做<code class="fe no np nq nr b">28x28</code>的原因是因为我们一会儿将在MNIST数字数据集上训练这些。</p><h2 id="a742" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated">克拉斯·RNN</h2><pre class="ks kt ku kv gt ns nr nt nu aw nv bi"><span id="4894" class="nb mf it nr b gy nw nx l ny nz">import tensorflow as tf</span><span id="1c0e" class="nb mf it nr b gy oa nx l ny nz">from tensorflow import keras</span><span id="2661" class="nb mf it nr b gy oa nx l ny nz">from tensorflow.keras import layers</span><span id="93e5" class="nb mf it nr b gy oa nx l ny nz">model = keras.Sequential()</span><span id="96c9" class="nb mf it nr b gy oa nx l ny nz">model.add(layers.SimpleRNN(64, <em class="nn">input_shape</em>=(28, 28)))</span><span id="5b98" class="nb mf it nr b gy oa nx l ny nz">model.add(layers.BatchNormalization())</span><span id="52b3" class="nb mf it nr b gy oa nx l ny nz">model.add(layers.Dense(10))</span><span id="ed50" class="nb mf it nr b gy oa nx l ny nz">print(model.summary())</span></pre><h2 id="7662" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated">克拉斯·LSTM</h2><pre class="ks kt ku kv gt ns nr nt nu aw nv bi"><span id="e8d4" class="nb mf it nr b gy nw nx l ny nz">import tensorflow as tf</span><span id="0080" class="nb mf it nr b gy oa nx l ny nz">from tensorflow import keras</span><span id="e0d2" class="nb mf it nr b gy oa nx l ny nz">from tensorflow.keras import layers</span><span id="fa70" class="nb mf it nr b gy oa nx l ny nz">model = keras.Sequential()</span><span id="d6df" class="nb mf it nr b gy oa nx l ny nz">model.add(layers.LSTM(64, <em class="nn">input_shape</em>=(28, 28)))</span><span id="1b50" class="nb mf it nr b gy oa nx l ny nz">model.add(layers.BatchNormalization())</span><span id="0605" class="nb mf it nr b gy oa nx l ny nz">model.add(layers.Dense(10))</span><span id="8651" class="nb mf it nr b gy oa nx l ny nz">print(model.summary())</span></pre><h2 id="55aa" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated">克拉斯·GRU</h2><pre class="ks kt ku kv gt ns nr nt nu aw nv bi"><span id="75a8" class="nb mf it nr b gy nw nx l ny nz">import tensorflow as tf</span><span id="a5cf" class="nb mf it nr b gy oa nx l ny nz">from tensorflow import keras</span><span id="762b" class="nb mf it nr b gy oa nx l ny nz">from tensorflow.keras import layers</span><span id="0d50" class="nb mf it nr b gy oa nx l ny nz">model = keras.Sequential()</span><span id="f9cb" class="nb mf it nr b gy oa nx l ny nz">model.add(layers.GRU(64, <em class="nn">input_shape</em>=(28, 28)))</span><span id="48d2" class="nb mf it nr b gy oa nx l ny nz">model.add(layers.BatchNormalization())</span><span id="850e" class="nb mf it nr b gy oa nx l ny nz">model.add(layers.Dense(10))</span><span id="a67f" class="nb mf it nr b gy oa nx l ny nz">print(model.summary())</span></pre><h2 id="a646" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated">MNIST数字数据集上RNNs、LSTMs和GRUs的比较</h2><p id="ec6b" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们可以通过添加以下代码块，在MNIST数字数据集上训练上述神经网络:</p><pre class="ks kt ku kv gt ns nr nt nu aw nv bi"><span id="91c4" class="nb mf it nr b gy nw nx l ny nz">mnist = keras.datasets.mnist</span><span id="c01b" class="nb mf it nr b gy oa nx l ny nz">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><span id="26e9" class="nb mf it nr b gy oa nx l ny nz">x_train, x_test = x_train/255.0, x_test/255.0</span><span id="119f" class="nb mf it nr b gy oa nx l ny nz">x_validate, y_validate = x_test[:-10], y_test[:-10]</span><span id="93bc" class="nb mf it nr b gy oa nx l ny nz">x_test, y_test = x_test[-10:], y_test[-10:]</span><span id="dad1" class="nb mf it nr b gy oa nx l ny nz">model.compile(<em class="nn">loss</em>=keras.losses.SparseCategoricalCrossentropy(<em class="nn">from_logits</em>=True), <em class="nn">optimizer</em>="sgd", <em class="nn">metrics</em>=["accuracy"],)</span><span id="b8bc" class="nb mf it nr b gy oa nx l ny nz">model.fit(x_train, y_train, <em class="nn">validation_data</em>=(x_validate, y_validate), <em class="nn">batch_size</em>=64, <em class="nn">epochs</em>=10)</span></pre><p id="4aeb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在MNIST数字数据集上训练这些神经网络的结果如下。</p><p id="1e73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">10个时代的RNN。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/2798f627d1aa1c0e52353098eadae638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G-xqMAsaEL1F0CjM"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">RNN结果，作者图片</figcaption></figure><p id="0867" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">10个时代的LSTM。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/4caa86f491637f8b24401c6040493a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T6wFgUG-YV4_Nvb7"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">LSTM结果，作者图片</figcaption></figure><p id="6706" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">10个时代的GRU。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/df09b9df978598329b6588b6faf9e692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*16_k1LDnyxs_TCJB"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">GRU结果，作者图片</figcaption></figure><h1 id="82d1" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">rnn、LSTMs和gru的摘要</h1><p id="f07d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在本文中，我们了解了rnn、LSTMs和gru。我们回顾了每种技术的简史，从20世纪80年代的RNNs、20世纪90年代的LSTMs和21世纪10年代的GRUs开始。然后我们看了如何用Keras和TensorFlow库在Python中实现这些。最后，我们比较了它们在MNIST数字数据集上的表现。如需更深入的分析，请参见RNNs、LSTMs和GRUs 的<a class="ae lh" href="https://pythonalgos.com/the-best-rnn-for-image-classification-rnn-lstm-or-gru/" rel="noopener ugc nofollow" target="_blank">对比。</a></p><p id="f1a7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你喜欢这篇文章，请在Twitter上分享！为了无限制地访问媒体文章，今天就注册成为<a class="ae lh" href="https://www.medium.com/@ytang07/membership" rel="noopener">媒体会员</a>！别忘了关注我，<a class="ae lh" href="https://www.medium.com/@ytang07" rel="noopener">唐</a>，获取更多关于技术、<a class="ae lh" href="https://pythonalgos.com/2021/11/23/what-is-natural-language-processing-nlp/" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>，以及成长的文章！</p><h2 id="f97e" class="nb mf it bd mg nc nd dn mk ne nf dp mo lr ng nh mq lv ni nj ms lz nk nl mu iz bi translated">进一步阅读</h2><ul class=""><li id="121f" class="oe of it lk b ll mw lo mx lr og lv oh lz oi md oj ok ol om bi translated"><a class="ae lh" href="https://pythonalgos.com/build-your-own-ai-text-summarizer-in-python/" rel="noopener ugc nofollow" target="_blank">用Python构建你自己的文本摘要器</a></li><li id="b1ab" class="oe of it lk b ll on lo oo lr op lv oq lz or md oj ok ol om bi translated"><a class="ae lh" href="https://pythonalgos.com/build-a-recurrent-neural-network-from-scratch-in-python-3/" rel="noopener ugc nofollow" target="_blank">Python 3中的神经网络代码</a></li><li id="4c82" class="oe of it lk b ll on lo oo lr op lv oq lz or md oj ok ol om bi translated"><a class="ae lh" href="https://blog.devgenius.io/three-nlp-projects-you-need-in-your-portfolio-2a1a124f2570" rel="noopener ugc nofollow" target="_blank">你的投资组合中需要的三个NLP项目</a></li><li id="df7f" class="oe of it lk b ll on lo oo lr op lv oq lz or md oj ok ol om bi translated"><a class="ae lh" href="https://blog.devgenius.io/five-jobs-in-software-jobs-you-may-not-know-9712b26623b0" rel="noopener ugc nofollow" target="_blank">你可能不知道的五种软件工作</a></li><li id="c65b" class="oe of it lk b ll on lo oo lr op lv oq lz or md oj ok ol om bi translated"><a class="ae lh" href="https://blog.devgenius.io/why-programming-is-easy-but-software-engineering-is-hard-90019fd78ed5" rel="noopener ugc nofollow" target="_blank">为什么编程容易而软件工程难</a></li></ul></div></div>    
</body>
</html>