<html>
<head>
<title>Text Mining in Python: Steps and Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的文本挖掘:步骤和示例</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/text-mining-in-python-steps-and-examples-78b3f8fd913b?source=collection_archive---------0-----------------------#2019-08-22">https://pub.towardsai.net/text-mining-in-python-steps-and-examples-78b3f8fd913b?source=collection_archive---------0-----------------------#2019-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4dd4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-mining" rel="noopener ugc nofollow" target="_blank">数据挖掘</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming/python" rel="noopener ugc nofollow" target="_blank"> Python </a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/4e0cd379be3570fa6eec4dae6e496624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jchjqUcYwBu1FQta.jpg"/></div></div></figure><p id="3162" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">在今天的场景中，人们成功的一种方式取决于他们如何与他人交流和分享信息。这就是语言概念出现的原因。然而，世界上有许多种语言。每一个都有许多标准和字母，这些词有意义地组合起来就形成了一个句子。每种语言在发展这些句子时都有自己的规则，这些规则也被称为语法。</p><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi li"><img src="../Images/242bc624c7aee1bc07254b0206e3f765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iZbht7SBvznG6p3jWJUPJQ.png"/></div></div></figure><p id="14f9" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">根据行业估计，在当今世界，当我们说话、发推特、在WhatsApp、电子邮件、脸书、Instagram或任何短信上发送消息时，只有20%的数据是以结构化格式生成的。而且，这些数据的大部分以文本形式存在，这是一种高度非结构化的格式。为了从文本数据中产生有意义的见解，我们需要遵循一种叫做文本分析的方法。</p><h1 id="7fb6" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">什么是文本挖掘？</h1><blockquote class="ml mm mn"><p id="0cb2" class="kk kl mo km b kn ko kp kq kr ks kt ku mp kw kx ky mq la lb lc mr le lf lg lh im bi translated">文本挖掘是从自然语言文本中获取有意义信息的过程。</p></blockquote><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ms"><img src="../Images/16ed314e873209b01760a57846a365cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KNTY9V9VyT3qs4hm2GPLdg.png"/></div></div></figure><h1 id="eaf4" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated"><strong class="ak">什么是NLP？</strong></h1><blockquote class="ml mm mn"><p id="ac05" class="kk kl mo km b kn ko kp kq kr ks kt ku mp kw kx ky mq la lb lc mr le lf lg lh im bi translated"><strong class="km jd">自然语言处理(NLP)是处理人类语言的计算机科学和人工智能的一部分。</strong></p></blockquote><p id="940b" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">换句话说，NLP是文本挖掘的一个组成部分，它执行一种特殊的语言分析，本质上是帮助机器“阅读”文本。它使用不同的方法来破译人类语言中的歧义，包括以下内容:自动摘要、词性标注、消歧、组块，以及消歧和自然语言理解和识别。我们将一步一步地看到使用Python的所有过程。</p><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/ebbc6fdc2512d181c6b0ce56e371f10a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*cXlPOwhc6A3Skp9GQ1gVFQ.png"/></div></figure><p id="3936" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">首先，我们需要安装NLTK库，这是一个自然语言工具包，用于构建Python程序来处理人类语言数据，它还提供了易于使用的接口。</p><h1 id="d97d" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">自然语言处理术语</h1><h2 id="32a6" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">标记化</h2><p id="2a0a" class="pw-post-body-paragraph kk kl it km b kn nf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld nj lf lg lh im bi translated">标记化是自然语言处理的第一步。它是将字符串分解成记号的过程，这些记号又是小的结构或单元。标记化包括三个步骤:将复杂的句子分解成单词，理解每个单词相对于句子的重要性，以及最终对输入句子产生结构描述。</p><h2 id="0aaa" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated"><strong class="ak">代码:</strong></h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="926a" class="mu lo it nl b gy np nq l nr ns"># Importing necessary library<br/>import pandas as pd<br/>import numpy as np<br/>import nltk<br/>import os<br/>import nltk.corpus</span><span id="614d" class="mu lo it nl b gy nt nq l nr ns"># sample text for performing tokenization<br/>text = “In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern<br/>side of South America"</span><span id="fe4e" class="mu lo it nl b gy nt nq l nr ns"># importing word_tokenize from nltk<br/>from nltk.tokenize import word_tokenize</span><span id="a89a" class="mu lo it nl b gy nt nq l nr ns"># Passing the string text into word tokenize for breaking the sentences<br/>token = word_tokenize(text)<br/>token</span></pre><h2 id="73fc" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="c614" class="mu lo it nl b gy np nq l nr ns">['In','Brazil','they','drive', 'on','the', 'right-hand', 'side', 'of', 'the', 'road', '.', 'Brazil', 'has', 'a', 'large', 'coastline', 'on', 'the', 'eastern', 'side', 'of', 'South', 'America']</span></pre><p id="640d" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">从上面的输出中，我们可以看到文本被分割成标记。单词、逗号、标点符号称为记号。</p><h1 id="5a98" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">在文本中寻找不同的频率</h1><h2 id="52bc" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">代码1</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="2855" class="mu lo it nl b gy np nq l nr ns"># finding the frequency distinct in the tokens<br/># Importing FreqDist library from nltk and passing token into FreqDist<br/>from nltk.probability import FreqDist<br/>fdist = FreqDist(token)<br/>fdist</span></pre><h2 id="e540" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="d4c1" class="mu lo it nl b gy np nq l nr ns">FreqDist({'the': 3, 'Brazil': 2, 'on': 2, 'side': 2, 'of': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'road': 1, ...})</span></pre><p id="0758" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">“the”在文本中出现3次，“Brazil”在文本中出现2次，依此类推。</p><h2 id="5a69" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">代码2</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="a27f" class="mu lo it nl b gy np nq l nr ns"># To find the frequency of top 10 words<br/>fdist1 = fdist.most_common(10)<br/>fdist1</span></pre><h2 id="1fc1" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="897d" class="mu lo it nl b gy np nq l nr ns">[('the', 3),<br/> ('Brazil', 2),<br/> ('on', 2),<br/> ('side', 2),<br/> ('of', 2),<br/> ('In', 1),<br/> ('they', 1),<br/> ('drive', 1),<br/> ('right-hand', 1),<br/> ('road', 1)]</span></pre><h1 id="79d9" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">堵塞物</h1><blockquote class="ml mm mn"><p id="3921" class="kk kl mo km b kn ko kp kq kr ks kt ku mp kw kx ky mq la lb lc mr le lf lg lh im bi translated">词干通常是指将单词规范化为其基本形式或词根形式。</p></blockquote><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/45ce756000f2bc6be5c52136ee8a9154.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/0*TIYLCwlI6rundRHb"/></div></figure><p id="e960" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">在这里，我们等待着，等待着，等待着。这里的词根是wait。词干提取有两种方法，即波特词干提取(从单词中删除常见的词形和词尾变化)和兰开斯特词干提取(一种更积极的词干提取算法)。</p><h2 id="09b7" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">代码1</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="6f95" class="mu lo it nl b gy np nq l nr ns"># Importing Porterstemmer from nltk library<br/># Checking for the word ‘giving’ <br/>from nltk.stem import PorterStemmer<br/>pst = PorterStemmer()<br/>pst.stem(“waiting”)</span></pre><h2 id="a1e9" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="f14e" class="mu lo it nl b gy np nq l nr ns">'wait'</span></pre><h2 id="7941" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">代码2</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="e039" class="mu lo it nl b gy np nq l nr ns"># Checking for the list of words<br/>stm = ["waited", "waiting", "waits"]<br/>for word in stm :<br/>   print(word+ ":" +pst.stem(word))</span></pre><h2 id="90a4" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="c037" class="mu lo it nl b gy np nq l nr ns">waited:wait<br/>waiting:wait<br/>waits:wait</span></pre><h2 id="ddd3" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">代码3</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="91e1" class="mu lo it nl b gy np nq l nr ns"># Importing LancasterStemmer from nltk<br/>from nltk.stem import LancasterStemmer<br/>lst = LancasterStemmer()<br/>stm = [“giving”, “given”, “given”, “gave”]<br/>for word in stm :<br/> print(word+ “:” +lst.stem(word))</span></pre><h2 id="2b84" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="a8c0" class="mu lo it nl b gy np nq l nr ns">giving:giv<br/>given:giv<br/>given:giv<br/>gave:gav</span></pre><p id="3da8" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">兰卡斯特比波特·斯泰默更具侵略性</p><h1 id="efbd" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">词汇化</h1><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nv"><img src="../Images/d5b9cb0a079dfba5f8ce69c9bf1088ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kt9AbfaCIHCG2QjBckRVLg.png"/></div></div></figure><p id="1295" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">简单来说，就是把一个单词转换成它的基本形式的过程。词干化和词元化的区别在于，词元化考虑上下文并将单词转换为其有意义的基本形式，而词干化只是删除最后几个字符，通常会导致不正确的意思和拼写错误。</p><p id="fdaf" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">例如，词汇化将正确地识别“care”到“care”的基本形式，而词干化将截断“ing”部分并将其转换为car。</p><p id="33fc" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">可以使用Wordnet Lemmatizer、Spacy Lemmatizer、TextBlob、Stanford CoreNLP在python中实现Lemmatization</p><h2 id="92e2" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">密码</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="9afc" class="mu lo it nl b gy np nq l nr ns"># Importing Lemmatizer library from nltk<br/>from nltk.stem import WordNetLemmatizer<br/>lemmatizer = WordNetLemmatizer() <br/> <br/>print(“rocks :”, lemmatizer.lemmatize(“rocks”)) <br/>print(“corpora :”, lemmatizer.lemmatize(“corpora”))</span></pre><h2 id="1c7b" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="0e75" class="mu lo it nl b gy np nq l nr ns">rocks : rock<br/>corpora : corpus</span></pre><h1 id="e7a5" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">停止言语</h1><p id="32dc" class="pw-post-body-paragraph kk kl it km b kn nf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld nj lf lg lh im bi translated">“停用词”是语言中最常见的词，如“the”、“a”、“at”、“for”、“above”、“on”、“is”、“all”。这些词不提供任何意义，通常从文本中删除。我们可以使用nltk库删除这些停用词</p><h2 id="25ba" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">密码</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="623e" class="mu lo it nl b gy np nq l nr ns"># importing stopwors from nltk library<br/>from nltk import word_tokenize<br/>from nltk.corpus import stopwords<br/>a = set(stopwords.words(‘english’))</span><span id="b562" class="mu lo it nl b gy nt nq l nr ns">text = “Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.”<br/>text1 = word_tokenize(text.lower())<br/>print(text1)</span><span id="48b9" class="mu lo it nl b gy nt nq l nr ns">stopwords = [x for x in text1 if x not in a]<br/>print(stopwords)</span></pre><h2 id="a371" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="9638" class="mu lo it nl b gy np nq l nr ns">Output of text:<br/>['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']</span><span id="c67c" class="mu lo it nl b gy nt nq l nr ns">Output of stopwords:<br/>['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']</span></pre><h1 id="d076" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">词性标注</h1><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nw"><img src="../Images/9d7b6c9c1c76565a1b3dc902073e13db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JRej3tXz1qjoW_ZFSpfJNQ.png"/></div></div></figure><p id="f3f7" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">词性标注用于根据定义和上下文给给定文本的每个单词(如名词、动词、代词、副词、连词、形容词、感叹词)分配词性。有许多工具可用于POS标签，一些广泛使用的标签有NLTK、Spacy、TextBlob、Standford CoreNLP等。</p><h2 id="bf46" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">密码</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="db45" class="mu lo it nl b gy np nq l nr ns">text = “vote to choose a particular man or a group (party) to represent them in parliament”<br/>#Tokenize the text<br/>tex = word_tokenize(text)<br/>for token in tex:<br/>print(nltk.pos_tag([token]))</span></pre><h2 id="05d1" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="2dc1" class="mu lo it nl b gy np nq l nr ns">[('vote', 'NN')]<br/>[('to', 'TO')]<br/>[('choose', 'NN')]<br/>[('a', 'DT')]<br/>[('particular', 'JJ')]<br/>[('man', 'NN')]<br/>[('or', 'CC')]<br/>[('a', 'DT')]<br/>[('group', 'NN')]<br/>[('(', '(')]<br/>[('party', 'NN')]<br/>[(')', ')')]<br/>[('to', 'TO')]<br/>[('represent', 'NN')]<br/>[('them', 'PRP')]<br/>[('in', 'IN')]<br/>[('parliament', 'NN')]</span></pre><h1 id="f4ad" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">命名实体识别</h1><p id="67af" class="pw-post-body-paragraph kk kl it km b kn nf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld nj lf lg lh im bi translated">它是检测命名实体(如人名、地名、公司名、数量和货币价值)的过程。</p><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e997ec4a4d8b78c0108c5b8bc3d5e94c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*jdt9-0S1lJm-caVKXNdyfQ.png"/></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk translated">Ref: <a class="ae oc" href="https://www.slideshare.net/sujitpal/soda-v2-named-entity-recognition-from-streaming-test-106598233" rel="noopener ugc nofollow" target="_blank"> Sujit Pal </a></figcaption></figure><h2 id="619b" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">密码</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="216c" class="mu lo it nl b gy np nq l nr ns">text = “Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event”</span><span id="6279" class="mu lo it nl b gy nt nq l nr ns">#importing chunk library from nltk<br/>from nltk import ne_chunk</span><span id="4e23" class="mu lo it nl b gy nt nq l nr ns"># tokenize and POS Tagging before doing chunk<br/>token = word_tokenize(text)<br/>tags = nltk.pos_tag(token)<br/>chunk = ne_chunk(tags)<br/>chunk</span></pre><h2 id="f235" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated"><strong class="ak">输出</strong></h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="002d" class="mu lo it nl b gy np nq l nr ns">Tree('S', [Tree('GPE', [('Google', 'NNP')]), ("'s", 'POS'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])</span></pre><h1 id="2350" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">组块</h1><p id="a7f5" class="pw-post-body-paragraph kk kl it km b kn nf kp kq kr ng kt ku kv nh kx ky kz ni lb lc ld nj lf lg lh im bi translated">分块意味着挑选出单个的信息片段，并把它们组合成更大的片段。在自然语言处理和文本挖掘的背景下，组块意味着将单词或标记分组为组块。</p><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi od"><img src="../Images/44114398758bad1dcc5d4502f8b9d89d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0sID7SOM5aDE6Lwq.png"/></div></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk translated">参赛:nltk.org<a class="ae oc" href="https://www.nltk.org/book/ch07.html" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><h2 id="1a7d" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">密码</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="4c10" class="mu lo it nl b gy np nq l nr ns">text = “We saw the yellow dog”<br/>token = word_tokenize(text)<br/>tags = nltk.pos_tag(token)</span><span id="695a" class="mu lo it nl b gy nt nq l nr ns">reg = “NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}” <br/>a = nltk.RegexpParser(reg)<br/>result = a.parse(tags)<br/>print(result)</span></pre><h2 id="9b64" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">输出</h2><pre class="lj lk ll lm gt nk nl nm nn aw no bi"><span id="0fb0" class="mu lo it nl b gy np nq l nr ns">(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))</span></pre><p id="0b9f" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这篇博客总结了文本预处理，涵盖了NLTK步骤，包括标记化、词干化、词汇化、词性标注、命名实体识别和组块。</p><p id="05dd" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">感谢阅读。请继续学习，并关注更多内容！</p><p id="f48f" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">你也可以在<a class="ae oc" href="https://www.kdnuggets.com/2020/05/text-mining-python-steps-examples.html" rel="noopener ugc nofollow" target="_blank"> KDnuggets上阅读这篇文章。</a></p><h2 id="a4d8" class="mu lo it bd lp mv mw dn lt mx my dp lx kv mz na mb kz nb nc mf ld nd ne mj iz bi translated">参考:</h2><ol class=""><li id="17ad" class="oe of it km b kn nf kr ng kv og kz oh ld oi lh oj ok ol om bi translated"><a class="ae oc" href="https://www.expertsystem.com/natural-language-processing-and-text-mining/" rel="noopener ugc nofollow" target="_blank">https://www . expert system . com/natural-language-processing-and-text-mining/</a></li><li id="5d2e" class="oe of it km b kn on kr oo kv op kz oq ld or lh oj ok ol om bi translated">https://www.nltk.org<a class="ae oc" href="https://www.nltk.org" rel="noopener ugc nofollow" target="_blank"/></li><li id="2c11" class="oe of it km b kn on kr oo kv op kz oq ld or lh oj ok ol om bi translated">【https://www.edureka.co T4】</li><li id="79cf" class="oe of it km b kn on kr oo kv op kz oq ld or lh oj ok ol om bi translated"><a class="ae oc" href="https://www.geeksforgeeks.org/nlp-chunk-tree-to-text-and-chaining-chunk-transformation/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/NLP-chunk-tree-to-text-and-chaining-chunk-transformation/</a></li><li id="7719" class="oe of it km b kn on kr oo kv op kz oq ld or lh oj ok ol om bi translated"><a class="ae oc" href="https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/part-speech-tagging-stop-words-using-nltk-python/</a></li></ol></div></div>    
</body>
</html>