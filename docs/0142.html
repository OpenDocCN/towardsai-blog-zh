<html>
<head>
<title>How to Train MAML(Model-Agnostic Meta-Learning)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练MAML(模型不可知元学习)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-train-maml-model-agnostic-meta-learning-90aa093f8e46?source=collection_archive---------0-----------------------#2019-08-23">https://pub.towardsai.net/how-to-train-maml-model-agnostic-meta-learning-90aa093f8e46?source=collection_archive---------0-----------------------#2019-08-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2f50" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="f006" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">对MAML和更多的详细解释</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b57b120b626e7f5b30120386b78ca1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e2Odga58h5KRsrZD4M-YpA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:Pixabay</figcaption></figure><h1 id="6c7f" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">介绍</h1><p id="dffa" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">M</strong>odel-<strong class="mb jd">A</strong>gnostic<strong class="mb jd">M</strong>eta-<strong class="mb jd">L</strong>earning(MAML)自2017年由Finn等人首次提出以来，在元学习领域越来越受欢迎。它是一种简单、通用和有效的优化算法，不会对模型结构或损失函数施加任何约束。因此，它可以与任意网络和不同类型的损失函数相结合，这使得它适用于各种不同的学习过程。</p><p id="f87e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">这篇文章由两部分组成:我们首先解释MAML，提出一个详细的讨论和可视化的学习过程。然后，我们描述了原始MAML的一些潜在问题，并根据安托尼乌等人的工作解决了这些问题。</p><h1 id="4f5b" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">MAML</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/2dc2ec62affa3ef1ae60fd4acc251f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*UV54wwA-SapRJMeCKeq98w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">与模型无关的元学习算法(MAML)的示意图，该算法针对可以快速适应新任务的表示θ进行了优化。来源:Finn等人的《深度网络快速适应的模型不可知元学习》</figcaption></figure><p id="fb11" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">MAML背后的想法很简单:它优化一组参数，使得当针对特定任务<em class="nb"> i </em>采取梯度步骤时，参数<em class="nb"> θᵢ </em>接近任务<em class="nb"> i </em>的最优参数。因此，这种方法的目的是学习一种广泛适用于任务分配<em class="nb"> p(T) </em>中所有任务的内部特征，而不是单一任务。这是通过最小化从任务分布<em class="nb"> p(T) </em>中采样的任务的总损失来实现的</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/724c4541ea74c52440dccfba9a64a192.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*l8gbCYzFzAO9JwEXdhpLUw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">情商。(1).单步内循环学习的简单说明。我们也可以更新θᵢ'几次。</figcaption></figure><p id="8138" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">注意，我们实际上并没有在这里定义一组额外的变量<em class="nb"> θᵢ' </em>。<em class="nb"> θᵢ' </em>是通过从<em class="nb"> θ </em> w.r.t. task <em class="nb"> i </em>开始采取一个(或几个)梯度步骤来计算的——这一步骤通常被称为内环学习，与我们优化Eq的外环学习相反。(1).为了更好地理解，如果我们将内循环学习视为相对于任务<em class="nb"> i </em>的微调<em class="nb"> θ </em>，那么等式。(1)同样地说，我们优化一个目标，期望模型在各自的微调之后在每个任务上做得很好。</p><p id="a4df" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">还有一点值得注意的是，优化情商的时候。(1)我们最终将计算Hessian-矢量积，即使用共轭梯度法也是昂贵的。Finn等人在监督学习问题上使用MAML的一阶近似进行了一些实验，其中省略了这些二阶导数(这可以通过停止计算<em class="nb">【∇_θ(l_tᵢ(f(θ】</em>)的梯度来编程实现)。注意，所得到的方法仍然在更新后的参数值<em class="nb"> θᵢ' </em>处计算元梯度，这提供了有效的元学习。实验证明，该方法的性能与用全二阶导数获得的性能几乎相同，这表明MAML的大部分改进来自更新后参数值处的目标梯度，而不是通过梯度更新的微分的二阶更新。</p><h2 id="927c" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">MAML可视化</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/d7da66644ee4ee8d4b82d2c22f1b2a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ks0s2IHDumN5VaY7gwe0ZA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">MAML可视化。这里，我们将中间参数θᵢ'称为快速权重。内环学习采取n个梯度步骤来计算最终的快速权重，基于此，外环学习计算外层任务损失，<em class="np">l_tᵢ(f(θᵢ')】</em></figcaption></figure><p id="3c17" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">注意，如果内循环学习被重复<em class="nb"> N </em>次，MAML仅使用最终权重进行外循环学习。正如我们将在后面看到的，这可能会很麻烦，当<em class="nb"> N </em>很大时会导致不稳定的学习。</p><h2 id="0881" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">算法</h2><p id="d896" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">现在，应该很容易看到算法</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/f1f3ede8d4b7d1f84b3494ebeb6cbe75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fjck6UGUXR9m8IMP_XeAmA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">MAML的伪代码。来源:Finn等人的《深度网络快速适应的模型不可知元学习》</figcaption></figure><h1 id="463e" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">MAML++</h1><p id="3dba" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">在这一节中，我们将集中讨论原始MAML的几个问题，并提出相应的潜在解决方案和最终的MAML++算法。所有这些贡献最初是由安托尼乌等人[2]在ICLR 2019年提出的。</p><h2 id="f3ee" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">训练不稳定性</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/079b6354c12aa4f65399e07c6e11c093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P21KqCrQR7bLajhgTSgWiw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:安托尼乌等人《如何训练你的MAML》</figcaption></figure><p id="4ae1" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">MAML训练可能不稳定，这取决于神经网络架构和整体超参数设置。例如，安托尼乌等人发现，简单地用步长卷积层替换最大池层会使其变得不稳定，如图1所示。他们推测不稳定性是由梯度退化(梯度爆炸或梯度消失)引起的，而梯度退化又是由深层网络引起的。为了看到这一点，我们回头看看MAML的形象。假设网络是标准的4层卷积网络，后面是单个线性层，如果我们重复内循环学习<em class="nb"> N </em>次，那么推理图总共由<em class="nb"> 5N </em>层组成，没有任何跳跃连接。由于原始MAML仅使用最终权重进行外环学习，反向传播必须通过所有层，这使得梯度退化有意义。</p><p id="1238" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">解决方案:多步损耗优化(MSL) </strong></p><p id="718b" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们可以采用GoogLeNet的类似思想，通过计算每个内部步骤后的外部损失来缓解梯度退化问题。具体来说，我们有外环更新</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1b10cf600a7b8b673531a0c37a463c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*y4EmLsjQa5O1YMrRO6DGYw.png"/></div></figure><p id="fad5" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">其中<em class="nb"> β </em>是学习率，L_Tᵢ(f(θ_j^i)表示在<em class="nb"> j </em>内步更新之后使用基网络权重时任务<em class="nb"> i </em>的外部损失，并且<em class="nb"> wⱼ </em>表示在步骤<em class="nb"> j </em>的外部损失的重要性权重。为了更好的比较，我们也将这个过程可视化</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/ff491a962d2c365c9be4deba3dd4c25f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SRbpzKRHUWKkqR0W4-2T_w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">MAML++可视化。我们在每一个内部步骤之后计算外部损失，最后取它们的加权平均值</figcaption></figure><p id="4cef" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">在实践中，我们用对损失的相等贡献来初始化所有损失，但是随着迭代的增加，我们减少早期步骤的贡献，并慢慢增加后面步骤的贡献。这样做是为了确保随着训练的进行，最终的步骤损失得到优化器更多的关注，从而确保它达到尽可能低的损失。如果不进行退火，最终损耗可能会高于原始配方。</p><h2 id="adc0" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">二阶导数成本</h2><p id="a3d3" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">MAML通过完全忽略它来降低二阶导数成本。在某些情况下，这可能会损害最终的泛化性能。</p><p id="0446" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">解决方案:导数级退火(DA) </strong></p><p id="22e2" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">安托尼乌等人[2]建议在训练阶段的前50个时期使用一阶梯度，然后在训练阶段的剩余时间切换到二阶梯度。一个有趣的观察是，这种导数级退火没有显示出爆炸或递减梯度的事件，与更不稳定的仅二级MAML相反。在开始使用二阶导数之前使用一阶导数可以作为一种强有力的预训练方法，学习不太可能产生梯度爆炸/减弱问题的参数。</p><h2 id="98e5" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">缺少批量标准化统计累积</h2><p id="d219" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">MAML在批量标准化中不使用运行统计。而是使用当前批次的统计数据。这导致批量标准化效率较低，因为学习到的参数必须适应不同任务的各种不同平均值和标准偏差。</p><h2 id="63f5" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">解决方案:每步批量标准化运行统计数据、每步批量标准化权重和偏差(BNRS + BNWB)</h2><p id="ec0f" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">在MAML的上下文中，批处理规范化的一个简单实现将在内循环学习的所有更新步骤中累积运行的批处理统计数据。不幸的是，这将导致优化问题，并可能减慢或完全停止优化。这个问题源于一个错误的假设:当我们维护跨网络的所有内环更新共享的运行统计时，我们假设初始模型及其所有更新的迭代具有相似的特征分布。显然，这种假设是远远不正确的。一个更好的替代方法是存储每一步的运行统计数据，并为每个内循环迭代学习每一步的批处理规范化参数。</p><h2 id="8434" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">共享内循环学习率</h2><p id="0359" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">影响泛化和收敛速度的一个问题是对所有参数和所有更新步骤使用共享学习速率的问题。具有固定的学习率需要进行多次超参数搜索，以找到特定数据集的正确学习率，这可能在计算上是昂贵的，取决于如何进行搜索。此外，虽然梯度是数据拟合的有效方向，但是固定的学习速率可能容易导致在少触发机制下的过拟合。</p><p id="9062" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">解:学习每层每步的学习率(LSLR) </strong></p><p id="5c5e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">为了避免潜在的过度拟合，一种方法是以最大化泛化能力而不是数据拟合的方式来确定所有学习因子。李等人[3]提出学习的基础网络中每个参数的学习率。内环更新现在变成了</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/45b5e2a23d8097e72b5fba30c820a44c.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*c-lXsywCg0yd-kERMlSIjg.png"/></div></figure><p id="c365" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">其中<em class="nb"> α </em>是具有与L_Tᵢ(f(θ相同大小的可学习参数的向量)，而∘表示逐元素乘积。由此产生的方法，即Meta-SGD，已被证明实现了比MAML更好的泛化性能，但代价是增加了学习参数和计算开销。注意，我们没有把积极性的约束放在学习速率<em class="nb"> α </em>上。因此，我们不应该期望内部更新方向遵循梯度方向。</p><p id="26c4" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">考虑到Meta-SGD的诱导成本，安托尼乌等人[2]提出学习网络中每一层的学习率，以及学习基础网络在采取步骤时的每次适应的不同学习率。例如，假设基本网络具有<em class="nb"> L </em>层，并且内循环学习包括<em class="nb"> N </em>步更新，我们现在引入<em class="nb"> LN </em>用于内循环学习速率的附加可学习参数。</p><h2 id="096a" class="nd li it bd lj ne nf dn ln ng nh dp lr mi ni nj lt mm nk nl lv mq nm nn lx iz bi translated">固定外环学习速率</h2><p id="ec8c" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">MAML使用具有固定学习速率的亚当来优化元目标。一些文献表明，退火学习速率对泛化性能至关重要。此外，有一个固定的学习率可能意味着你必须花更多的时间来调整学习率。</p><p id="a7d6" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">解:元优化器学习速率(CA)的余弦退火</strong></p><p id="9f36" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">安托尼乌等人[2]提出在元优化器上应用余弦退火调度(Loshchilov &amp; Hutter[4])。余弦退火调度被定义为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/363afc58aac85b4bfa44ab1e30f951b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*svChue0GDJ1jf1RErh9q7w.png"/></div></div></figure><p id="829e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">其中<em class="nb"> β_{min} </em>表示最小学习率<em class="nb">，β_{max} </em>表示初始学习率，<em class="nb"> T </em>为当前迭代次数<em class="nb">，T_{max} </em>为最大迭代次数。当<em class="nb"> T=0 </em>时，学习率<em class="nb"> β=β_{max} </em>。曾经<em class="nb"> T=T_{max} </em>，<em class="nb"> β=β_{min} </em>。实际上，我们可能希望将<em class="nb"> T </em>绑定为<em class="nb"> T_{max} </em>以避免重启。</p><h1 id="9c11" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">MAML++的实验结果</h1><p id="bf98" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">最后，我们给出了一些完整的实验结果。</p><p id="32fe" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们首先展示了个人在20路Omniglot任务中对MAML的改进。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/8288289ea96b51faff8478920b90c94f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxhxTeq76BpmaAElGOYESQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">20-way表示每个任务有20个类。1-shot表示我们只使用1个样本进行内循环学习和测试，而5-shot使用5个样本。来源:安托尼乌等人《如何训练你的MAML》</figcaption></figure><p id="c0db" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们可以看到，每步批量归一化和运行统计(BNWB+BNRS)和学习每层每步学习率(LSLR)收获最多。</p><p id="281d" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们也在迷你图像网络任务中展示了MAML</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/fd2b36751ecef1ccf9c30317abf5989d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZavclATVdbbrEf8vvCJpw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:安托尼乌等人《如何训练你的MAML》</figcaption></figure><p id="1526" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们可以看到，随着内部步骤数量的增加，性能会有所提高。值得注意的是，即使是1步MAML++优于原来的5步MAML。</p><p id="065c" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">如图1所示(在我们讨论MAML++的开始)，与MAML相比，MAML++收敛到最佳泛化性能的速度也快得多</p><h1 id="e005" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">参考</h1><ol class=""><li id="1c27" class="ny nz it mb b mc md mf mg mi oa mm ob mq oc mu od oe of og bi translated">切尔西·芬恩，彼得·阿贝耳，谢尔盖·莱文。用于深度网络快速适应的模型不可知元学习</li><li id="a7b0" class="ny nz it mb b mc oh mf oi mi oj mm ok mq ol mu od oe of og bi translated">安特雷亚·安托尼乌，哈里森·爱德华兹和阿莫斯·斯托基。如何训练你的MAML</li><li id="c271" class="ny nz it mb b mc oh mf oi mi oj mm ok mq ol mu od oe of og bi translated">、周、、和。Meta-SGD:学会快速学习</li><li id="c127" class="ny nz it mb b mc oh mf oi mi oj mm ok mq ol mu od oe of og bi translated">伊利亚·洛希洛夫和弗兰克·哈特。SGDR:带有热重启的随机梯度下降</li></ol></div></div>    
</body>
</html>