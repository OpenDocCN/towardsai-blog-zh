<html>
<head>
<title>Understanding LSTMs and GRUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解LSTMs和gru</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understanding-lstms-and-gru-s-b69749acaa35?source=collection_archive---------0-----------------------#2020-08-06">https://pub.towardsai.net/understanding-lstms-and-gru-s-b69749acaa35?source=collection_archive---------0-----------------------#2020-08-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f5e3" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="8a3b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">长短期记忆和门控循环单元的完美指南。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/712704db1f6143b339be01cda5919052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UMv3apD3YCl8fYV69m3zXg.png"/></div></div></figure><p id="d33c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我的上一篇文章中，我介绍了递归神经网络及其带来的复杂性。为了克服这些缺点，我们使用LSTMs &amp; GRUs。</p><div class="lz ma gp gr mb mc"><a href="https://medium.com/towards-artificial-intelligence/recurrent-neural-networks-for-dummies-8d2c4c725fbe" rel="noopener follow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd jd gy z fp mh fr fs mi fu fw jc bi translated">用于假人的递归神经网络</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">递归神经网络完美指南</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">medium.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq lb mc"/></div></div></a></div><h1 id="acf0" class="mr ms it bd mt mu mv mw mx my mz na nb ki nc kj nd kl ne km nf ko ng kp nh ni bi translated">障碍是短期记忆</h1><p id="3603" class="pw-post-body-paragraph ld le it lf b lg nj kd li lj nk kg ll lm nl lo lp lq nm ls lt lu nn lw lx ly im bi translated">递归神经网络仅限于短期记忆。如果一个长序列被输入到网络中，他们将很难记住这些信息，还不如从一开始就忽略掉重要的信息。</p><p id="a65c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">此外，当反向传播开始起作用时，递归神经网络面临消失梯度问题。由于冲突，更新的梯度小得多，使得我们的模型没有变化，因此对学习没有太大贡献。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/13a545d4e6b3d063d476d7f86b1fd711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ua5VZXS20Cz2Z52VeGoPVw.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk translated">权重更新规则</figcaption></figure><p id="fd2b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">“当我们执行反向传播时，我们计算每个节点的权重和偏差。但是，如果前几层的改进很少，那么对当前层的调整就会小得多。这导致梯度显著减小，从而导致我们的模型几乎没有变化，因此我们的模型不再学习，不再改进。”</p><h1 id="33df" class="mr ms it bd mt mu mv mw mx my mz na nb ki nc kj nd kl ne km nf ko ng kp nh ni bi translated">为什么是LSTMs和GRUs？</h1><p id="f6ae" class="pw-post-body-paragraph ld le it lf b lg nj kd li lj nk kg ll lm nl lo lp lq nm ls lt lu nn lw lx ly im bi translated">比方说，你正在网上查看对<a class="ae nt" href="https://www.imdb.com/title/tt3526078/" rel="noopener ugc nofollow" target="_blank">的《小溪》</a>的评论，以决定你是否可以观看。基本方法是阅读评论并确定其观点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/8f65964f2b69b0124539aa57f934fe24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*tKE_5hcNC_PDBbPyAy-Qgg.png"/></div></figure><p id="d13d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当你寻找评论时，你的潜意识会试图记住决定性的关键词。你会试着记住更多有分量的词，如“Aaaaastonishing”、“永恒的”、“不可思议的”、“古怪的”和“反复无常的”，而不会关注像“认为”、“确切地”、“大多数”等常规词。</p><p id="5011" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下一次当你被要求回忆这篇评论时，你可能会有一段艰难的时间，但是，我敢打赌，你必须记住上面提到的观点和一些重要的决定性的词语。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4e5b67cfa7771a658e533d2b0860e4f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*A0JwICCWKGu-BNNNXtYGfA.png"/></div></figure><p id="7e3a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这正是LSTM和GRU想要做的。</p><blockquote class="nv"><p id="2dbc" class="nw nx it bd ny nz oa ob oc od oe ly dk translated">只学习和记住重要的信息，忘记所有其他的东西。</p></blockquote><h1 id="eeb8" class="mr ms it bd mt mu mv mw mx my mz na nb ki of kj nd kl og km nf ko oh kp nh ni bi translated">LSTM(长短期记忆)</h1><p id="c17b" class="pw-post-body-paragraph ld le it lf b lg nj kd li lj nk kg ll lm nl lo lp lq nm ls lt lu nn lw lx ly im bi translated">LSTMs是香草RNN的一种进步形式，被引入来克服其缺点。为了实现上述直觉并管理由于RNN有限大小的状态向量而产生的重要信息，我们选择性地使用读、写和遗忘门。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/5e5ffe06b96ddd9e6d7d5a39cd0a5fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SUidJeTqRg99GrBK.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk translated">LSTM细胞，<a class="ae nt" href="https://www.researchgate.net/profile/Xiaofeng_Yuan4/publication/331421650/figure/fig2/AS:771405641695233@1560928845927/The-structure-of-the-LSTM-unit.png" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="0603" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">抽象概念围绕细胞状态和各种门。细胞状态可以将相关信息传递给序列链，并能够在整个计算过程中携带相关信息，从而解决了短期记忆的问题。随着这个过程的继续，更多的相关信息通过gates被添加和删除。门是一种特殊类型的神经网络，它在训练过程中学习相关信息。</p><h2 id="6c43" class="oj ms it bd mt ok ol dn mx om on dp nb lm oo op nd lq oq or nf lu os ot nh iz bi translated">选择性写入</h2><p id="3970" class="pw-post-body-paragraph ld le it lf b lg nj kd li lj nk kg ll lm nl lo lp lq nm ls lt lu nn lw lx ly im bi translated">我们假设，隐藏状态(<strong class="lf jd"> sₜ </strong>)，先前隐藏状态(<strong class="lf jd"> sₜ₋₁ </strong>)，当前输入(<strong class="lf jd"> xₜ </strong>)，偏置(<strong class="lf jd"> b </strong>)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/640564c12097865d4e3f52a7b4e03b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*5b2Ac6rGEsTj4esN.png"/></div></figure><p id="1bea" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，我们正在累加前一状态<strong class="lf jd"> sₜ₋₁ </strong>的所有输出，并计算当前状态<strong class="lf jd"> s </strong> ₜ的输出</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/7e1b3586077bc8c13abe408f91ac2436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/0*4ZNve1Y9cMAYOrAZ.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk translated">香草RNN</figcaption></figure><p id="81ba" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">使用选择性写入，我们感兴趣的只是将相关信息传递给下一个状态<strong class="lf jd"> sₜ.</strong>为了实现该策略，我们可以为每个输入分配一个从0到1的值，以确定有多少信息将被传递到下一个隐藏状态。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/f25bb34da603d053c3333306eb78cac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6034TdBdw8ONY-Aj.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk translated">LSTM选择性写作</figcaption></figure><p id="6b3f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以把要传递的那部分信息存储在一个向量<strong class="lf jd"> hₜ₋₁ t </strong>中，这个向量可以通过把先前的状态向量<strong class="lf jd"> sₜ₋₁ </strong>和<strong class="lf jd"> oₜ₋₁ </strong>相乘来计算，前者为每个输入存储0和1之间的值。</p><p id="36ad" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们遇到的下一个问题是，如何得到<strong class="lf jd"> oₜ₋₁？</strong></p><p id="2173" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了计算oₜ₋₁，我们必须学习它，我们唯一能控制的向量是我们的参数。所以，为了继续计算，我们需要用参数的形式表示oₜ₋₁。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/b0049bbb3b6bd140cdd8455c046fe122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/0*Zwa3HIi5KlqWpkgI.png"/></div></figure><p id="92b2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在使用梯度下降学习了<strong class="lf jd"> Uo、Wo </strong>和<strong class="lf jd"> Bo </strong>之后，我们可以使用我们的输出门(<strong class="lf jd"> oₜ₋₁ </strong>)进行精确预测，该输出门控制将有多少信息传递到下一个门。</p><h2 id="b627" class="oj ms it bd mt ok ol dn mx om on dp nb lm oo op nd lq oq or nf lu os ot nh iz bi translated">选择性阅读</h2><p id="a8a5" class="pw-post-body-paragraph ld le it lf b lg nj kd li lj nk kg ll lm nl lo lp lq nm ls lt lu nn lw lx ly im bi translated">在传递了来自前一个门的相关信息后，我们引入一个新的隐藏状态向量<strong class="lf jd">šₜ</strong>(用绿色标记)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/73ac99672d0bed9bea5f956e9d67346b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Nu0TUzzDF_F273ko.png"/></div></div></figure><p id="2df2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">šₜ</strong>捕捉来自前一状态<strong class="lf jd"> hₜ₋₁ </strong>和当前输入<strong class="lf jd"> xₜ </strong>的所有信息。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/ed8512be3c54d8820c13ac1638fa48a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/0*BY54iTUxFnQQ286v.png"/></div></figure><p id="18f1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">但是，我们的目标是尽可能多地移除不重要的东西，并继续我们的想法，我们将选择性地从<strong class="lf jd">šₜ</strong>中读取，以构建新的细胞阶段。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/fb53da4058b8a97639bba34b84a54fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7lZRSzQMZgEjSuI6.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk translated">选择性阅读</figcaption></figure><p id="2773" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了存储所有重要的内容，我们将再次回滚到0–1策略，在该策略中，我们将为每个输入分配一个介于0–1之间的值，以定义我们希望阅读的比例。</p><p id="8222" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">向量<strong class="lf jd"> iₜ </strong>将存储每个输入的比例值，该比例值稍后将与<strong class="lf jd">šₜ</strong>相乘，以控制流经当前输入的信息，该输入被称为<strong class="lf jd">输入门。</strong></p><p id="9064" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了计算<strong class="lf jd"> iₜ </strong>，我们必须学习它，我们唯一能控制的向量是我们的参数。所以，为了继续计算，我们需要用参数的形式表达iₜ。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/657fc44f9081568e6b6d3dae996a0a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/0*y9TTwG69zQIYL-_Q.png"/></div></figure><p id="1726" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在使用梯度下降学习了<strong class="lf jd"> Ui、Wi、</strong>和<strong class="lf jd"> Bi </strong>之后，我们可以使用我们的输入门(<strong class="lf jd"> iₜ </strong>)进行精确的预测，输入门控制有多少信息将被提供给我们的模型。</p><p id="d0d5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">总结到目前为止学习到的参数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/9d4947c7bc31c94715468b9d49c1aa22.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/0*tKC-bBtgJxD3biP1.png"/></div></figure><h2 id="5e46" class="oj ms it bd mt ok ol dn mx om on dp nb lm oo op nd lq oq or nf lu os ot nh iz bi translated">选择性遗忘</h2><p id="450f" class="pw-post-body-paragraph ld le it lf b lg nj kd li lj nk kg ll lm nl lo lp lq nm ls lt lu nn lw lx ly im bi translated">在有选择地阅读和书写信息之后，现在我们的目标是忘记所有不相关的东西，这些东西可以帮助我们减少混乱。</p><p id="e586" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了丢弃所有来自sₜ₋₁的无用信息，我们使用遗忘门fₜ.</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/fb98867919775013a6bdd51cfed5228e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IF-SxAz3A7MTuqx7.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk translated">选择性遗忘</figcaption></figure><p id="9741" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">遵循上述传统，我们将引入遗忘门<strong class="lf jd"> fₜ </strong>，它将构成一个从0到1的值，用于确定每个输入的重要性。</p><p id="0e82" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了计算fₜ，我们必须学习它，我们唯一能控制的向量是我们的参数。因此，为了继续计算，我们需要以提供的参数的形式表示<strong class="lf jd"> fₜ </strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/ca7d2c5e5a7505ce4d90de3f11b949d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/0*GnJQKxUdfC-YrN-G.png"/></div></figure><p id="6415" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在使用梯度下降学习了<strong class="lf jd"> Uf、Wf </strong>和<strong class="lf jd"> Bf </strong>之后，我们可以使用我们的遗忘门(<strong class="lf jd"> fₜ </strong>)进行精确预测，遗忘门控制着将丢弃多少信息。</p><p id="59fd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">将遗忘门和输入门的信息相加，可以告诉我们当前的隐藏状态信息。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/199581dbd4dee2d17e5d00b96d4d40f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/0*R5twp0I9QV-EUmlk.png"/></div></figure><h2 id="75bb" class="oj ms it bd mt ok ol dn mx om on dp nb lm oo op nd lq oq or nf lu os ot nh iz bi translated">最终模型</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/f62c0b2ce4eb38426a2ce85e7b1f6ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gMdU6hZLvuMVulg-.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk translated">LSTM模型</figcaption></figure><p id="27e0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">完整的方程组看起来像:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/0b677545d039c5f5e6f81f1d9751c4d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AUYB_cZj-yGzkTRp.png"/></div></div></figure><p id="019a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">LSTM需要的参数比香草RNN需要的多得多。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/59aa06fcd34c8ec1ce515ea8ef5818ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfcH6KTw_PxkkbGpSJO6Pg.png"/></div></div></figure><p id="a308" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于大门的数量和它们的排列变化很大，LSTM可以有很多种类型。</p><h1 id="d0dc" class="mr ms it bd mt mu mv mw mx my mz na nb ki nc kj nd kl ne km nf ko ng kp nh ni bi translated">门控循环单元</h1><p id="596d" class="pw-post-body-paragraph ld le it lf b lg nj kd li lj nk kg ll lm nl lo lp lq nm ls lt lu nn lw lx ly im bi translated">如前所述，LSTM可以有许多变化，GRU是其中之一。不太可能LSTM，GRU试图实现更少的门，从而有助于降低计算成本。</p><p id="12a5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在门控循环单元中，我们有一个输出门，控制传递到下一个隐藏状态的信息比例，此外，我们有一个输入门，控制来自当前输入的信息流，与RNN不同，我们不使用遗忘门。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/ba75a78a5251d5b1bbe645600d3fa54d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NT2iTXfiWa7XeBZp.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk translated">门控循环单位</figcaption></figure><p id="2ceb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了降低计算时间，我们移除遗忘门并丢弃信息，我们使用输入门向量的补充，即(1- <strong class="lf jd"> iₜ </strong>)。</p><p id="5ed4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为GRU实现的方程是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/e99276f5ae27f9ff56007c4abb950c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Mn8K8lewFYpWgLAn.png"/></div></div></figure><h1 id="88ca" class="mr ms it bd mt mu mv mw mx my mz na nb ki nc kj nd kl ne km nf ko ng kp nh ni bi translated">要点</h1><ul class=""><li id="7a68" class="pl pm it lf b lg nj lj nk lm pn lq po lu pp ly pq pr ps pt bi translated">引入LSTM和GRU是为了避免RNN的短期记忆。</li><li id="bbef" class="pl pm it lf b lg pu lj pv lm pw lq px lu py ly pq pr ps pt bi translated">LSTM通过使用遗忘门来遗忘。</li><li id="3b8f" class="pl pm it lf b lg pu lj pv lm pw lq px lu py ly pq pr ps pt bi translated">LSTM记得使用输入门。</li><li id="10c0" class="pl pm it lf b lg pu lj pv lm pw lq px lu py ly pq pr ps pt bi translated">LSTM利用细胞状态保持长期记忆。</li><li id="04e8" class="pl pm it lf b lg pu lj pv lm pw lq px lu py ly pq pr ps pt bi translated">gru比LSTM速度更快，计算成本更低。</li><li id="d6ca" class="pl pm it lf b lg pu lj pv lm pw lq px lu py ly pq pr ps pt bi translated">在向前传播的情况下，LSTM的梯度仍然可以消失。</li><li id="8d63" class="pl pm it lf b lg pu lj pv lm pw lq px lu py ly pq pr ps pt bi translated">LSTM没有解决爆炸梯度的问题，因此我们使用梯度剪辑。</li></ul><h1 id="7b20" class="mr ms it bd mt mu mv mw mx my mz na nb ki nc kj nd kl ne km nf ko ng kp nh ni bi translated">实际使用案例</h1><ul class=""><li id="6264" class="pl pm it lf b lg nj lj nk lm pn lq po lu pp ly pq pr ps pt bi translated">使用RNN的情感分析</li></ul><div class="lz ma gp gr mb mc"><a href="https://github.com/dakshtrehan/IMDB-sentiment-analysis-NN/blob/master/IMDB%20Sentiment%20Analysis%20using%20Neural%20Network.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd jd gy z fp mh fr fs mi fu fw jc bi translated">dakshtrehan/IMDB-情感分析-NN</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">github.com</p></div></div><div class="ml l"><div class="pz l mn mo mp ml mq lb mc"/></div></div></a></div><ul class=""><li id="5d62" class="pl pm it lf b lg lh lj lk lm qa lq qb lu qc ly pq pr ps pt bi translated">使用LSTM的人工智能音乐生成</li></ul><div class="lz ma gp gr mb mc"><a href="https://github.com/dakshtrehan/AI-Music-Generation" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd jd gy z fp mh fr fs mi fu fw jc bi translated">dakshtrehan/AI-音乐-一代</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">加入我在www.dakshtrehan.com | | www.linkedin.com/in/dakshtrehan的目标是产生新的音乐使用LSTM和给定的…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">github.com</p></div></div><div class="ml l"><div class="qd l mn mo mp ml mq lb mc"/></div></div></a></div></div><div class="ab cl qe qf hx qg" role="separator"><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj"/></div><div class="im in io ip iq"><h1 id="37cb" class="mr ms it bd mt mu ql mw mx my qm na nb ki qn kj nd kl qo km nf ko qp kp nh ni bi translated">结论</h1><p id="df8c" class="pw-post-body-paragraph ld le it lf b lg nj kd li lj nk kg ll lm nl lo lp lq nm ls lt lu nn lw lx ly im bi translated">希望这篇文章能帮助你以最好的方式理解长短期记忆(LSTM)和门控循环单位(GRU ),并帮助你实际应用。</p><p id="4b10" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一如既往，非常感谢您的阅读，如果您觉得这篇文章有用，请分享！</p></div><div class="ab cl qe qf hx qg" role="separator"><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj"/></div><div class="im in io ip iq"><p id="fcd6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">请随意连接:</p><blockquote class="qq qr qs"><p id="004f" class="ld le qt lf b lg lh kd li lj lk kg ll qu ln lo lp qv lr ls lt qw lv lw lx ly im bi translated">领英~<a class="ae nt" href="https://www.linkedin.com/in/dakshtrehan/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/dakshtrehan/</a></p><p id="97ee" class="ld le qt lf b lg lh kd li lj lk kg ll qu ln lo lp qv lr ls lt qw lv lw lx ly im bi translated">https://www.instagram.com/_daksh_trehan_/的insta gram ~<a class="ae nt" href="https://www.instagram.com/_daksh_trehan_/" rel="noopener ugc nofollow" target="_blank"/></p><p id="53e1" class="ld le qt lf b lg lh kd li lj lk kg ll qu ln lo lp qv lr ls lt qw lv lw lx ly im bi translated">github ~【https://github.com/dakshtrehan T4】</p></blockquote><p id="b44b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">关注更多机器学习/深度学习博客。</p><blockquote class="qq qr qs"><p id="63ef" class="ld le qt lf b lg lh kd li lj lk kg ll qu ln lo lp qv lr ls lt qw lv lw lx ly im bi translated">中等~<a class="ae nt" href="https://medium.com/@dakshtrehan" rel="noopener">https://medium.com/@dakshtrehan</a></p></blockquote><h1 id="d8ee" class="mr ms it bd mt mu mv mw mx my mz na nb ki nc kj nd kl ne km nf ko ng kp nh ni bi translated">想了解更多？</h1><p id="751d" class="pw-post-body-paragraph ld le it lf b lg nj kd li lj nk kg ll lm nl lo lp lq nm ls lt lu nn lw lx ly im bi translated"><a class="ae nt" href="https://towardsdatascience.com/detecting-covid-19-using-deep-learning-262956b6f981" rel="noopener" target="_blank">利用深度学习检测新冠肺炎</a></p><p id="f06e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://towardsdatascience.com/the-inescapable-ai-algorithm-tiktok-ad4c6fd981b8" rel="noopener" target="_blank">无法逃脱的人工智能算法:抖音</a></p><p id="905a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/towards-artificial-intelligence/an-insiders-guide-to-cartoonization-using-machine-learning-ce3648adfe8" rel="noopener">使用机器学习的卡通化内部指南</a></p><p id="a526" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你为什么要为乔治·弗洛伊德的谋杀和德里的骚乱负责？</p><p id="f643" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/towards-artificial-intelligence/recurrent-neural-networks-for-dummies-8d2c4c725fbe" rel="noopener">用于假人的递归神经网络</a></p><p id="2c32" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/towards-artificial-intelligence/convolutional-neural-networks-for-dummies-afd7166cd9e" rel="noopener">虚拟卷积神经网络</a></p><p id="a8b1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/towards-artificial-intelligence/diving-deep-into-deep-learning-f34497c18f11" rel="noopener">深入深度学习</a></p><p id="925c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/towards-artificial-intelligence/why-choose-random-forest-and-not-decision-trees-a28278daa5d" rel="noopener">为什么选择随机森林而不是决策树</a></p><p id="5ccf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/@dakshtrehan/clustering-what-it-is-when-to-use-it-a612bbe95881" rel="noopener">集群是什么？什么时候用？</a></p><p id="1ede" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/@dakshtrehan/start-off-your-ml-journey-with-k-nearest-neighbors-f72a122f428" rel="noopener">从k个最近邻居开始你的ML之旅</a></p><p id="880a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/swlh/things-you-never-knew-about-naive-bayes-eb84b6ee039a" rel="noopener">朴素贝叶斯解释</a></p><p id="9a16" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/analytics-vidhya/activation-functions-explained-8690ea7bdec9" rel="noopener">激活功能说明</a></p><p id="6a1b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://towardsdatascience.com/parameters-optimization-explained-876561853de0" rel="noopener" target="_blank">参数优化说明</a></p><p id="1ba6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c" rel="noopener" target="_blank">梯度下降解释</a></p><p id="ecbc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://towardsdatascience.com/logistic-regression-explained-ef1d816ea85a" rel="noopener" target="_blank">逻辑回归解释</a></p><p id="01e0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/towards-artificial-intelligence/linear-regression-explained-f5cc85ae2c5c" rel="noopener">线性回归解释</a></p><p id="b780" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nt" href="https://medium.com/datadriveninvestor/determining-perfect-fit-for-your-ml-model-339459eef670" rel="noopener">确定最适合你的ML模型</a></p><blockquote class="qq qr qs"><p id="27c5" class="ld le qt lf b lg lh kd li lj lk kg ll qu ln lo lp qv lr ls lt qw lv lw lx ly im bi translated">干杯！</p></blockquote></div></div>    
</body>
</html>