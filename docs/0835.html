<html>
<head>
<title>Unlocking the Power of Text Analytics with Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用自然语言处理释放文本分析的力量</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/unlock-the-power-of-text-analytics-with-natural-language-processing-2e6d83b35f99?source=collection_archive---------1-----------------------#2020-08-22">https://pub.towardsai.net/unlock-the-power-of-text-analytics-with-natural-language-processing-2e6d83b35f99?source=collection_archive---------1-----------------------#2020-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3cfe" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="6040" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">主题建模的潜在狄利克雷分配和文本分类的朴素贝叶斯</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/8cef194954d585aee425af67b31df18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8qJJZ7vo62iSJ6YS.jpg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">资料来源:<a class="ae le" href="https://www.surveysensum.com/customer-experience/text-analytics/" rel="noopener ugc nofollow" target="_blank">调查统计</a></figcaption></figure><p id="9151" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">自然语言是一种用于人类日常交流的语言。它在本质上对于文本和语音都是高度非结构化的，因此很难被机器解析和理解。<strong class="lh ja">自然语言处理(“NLP”)涉及自然人类语言和计算机之间的交互。它是语言学、计算机科学和人工智能领域的交叉。</strong></p><p id="656c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据国际数据公司在其报告<a class="ae le" href="https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf" rel="noopener ugc nofollow" target="_blank">中的预测，世界的数字化:从边缘到核心</a>，数据总量</p><blockquote class="mk ml mm"><p id="4c7d" class="lf lg mn lh b li lj ka lk ll lm kd ln mo lp lq lr mp lt lu lv mq lx ly lz ma ij bi translated">将从2018年的33泽字节增长到2025年的175泽字节。</p></blockquote><p id="c39b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据2019年福布斯的统计，<a class="ae le" href="https://www.forbes.com/sites/bernardmarr/2019/10/16/what-is-unstructured-data-and-why-is-it-so-important-to-businesses-an-easy-explanation-for-anyone/#5c1abd3415f6" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">每天生成的90%的数据是非结构化数据</strong> </a>，其中大部分将是文本。这是产生的最大数据源，因此是企业中分析和部署AI应用程序的丰富数据源。然而，公司通常只习惯于管理和分析“结构化”数据，这些数据整齐地排列在数据库的行和列中。</p><p id="326b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了从非结构化数据中发掘洞察力，我们可以利用文本分析的力量，<em class="mn"> </em>并使用NLP将文档和数据库中的非结构化文本转换为适合分析的标准化结构化数据。</p><h1 id="d8b7" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak">NLP的两个主要组成部分</strong></h1><ul class=""><li id="c8eb" class="nj nk iq lh b li nl ll nm lo nn ls no lw np ma nq nr ns nt bi translated">自然语言理解帮助计算机理解和解释人类语言。NLP不要求用户通过编程代码与计算机交互，而是允许用户使用日常语言与计算机交互，计算机可以对此做出适当的响应。</li><li id="c342" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nq nr ns nt bi translated"><strong class="lh ja">自然语言生成</strong>是计算机将数据翻译成人类可读语言的过程。正在翻译的数据包括构成电脑屏幕上显示的照片和文本的位和字节。</li></ul><h1 id="7d34" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak">自然语言理解的主要应用</strong></h1><ol class=""><li id="3b6f" class="nj nk iq lh b li nl ll nm lo nn ls no lw np ma nz nr ns nt bi translated"><strong class="lh ja">主题建模</strong>通过识别重复出现的模式或主题从文本中提取意义，并解开每个单独文本背后的语义结构。</li><li id="14fe" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nz nr ns nt bi translated"><strong class="lh ja">文档分类</strong>帮助将离散的文本集合分类。示例包括垃圾邮件过滤、区分正面和负面产品评论以及客户评论。</li><li id="5303" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nz nr ns nt bi translated"><strong class="lh ja">文档推荐</strong>通过基于内容的<a class="ae le" href="https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering" rel="noopener ugc nofollow" target="_blank">推荐系统</a>根据给定的信息选择<strong class="lh ja"> </strong>最相关的文档。一个很好的例子是Google搜索引擎，它根据用户的查询显示最相关的网页。</li></ol><h1 id="8c58" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated"><strong class="ak">自然语言生成的主要应用</strong></h1><ol class=""><li id="1722" class="nj nk iq lh b li nl ll nm lo nn ls no lw np ma nz nr ns nt bi translated"><strong class="lh ja">文档摘要</strong>从不断增长的可用文本数据中生成文本摘要。随着远程办公的发展，从对话中获取关键想法和内容的能力越来越受欢迎。一个能够将语音转换成文本并从团队会议中生成摘要的<strong class="lh ja">语音摘要</strong>系统会很有趣。</li><li id="789f" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nz nr ns nt bi translated"><strong class="lh ja">机器翻译</strong>在不同语言之间翻译文本。谷歌翻译就是一个例子，它可能是迄今为止使用最多、最著名的机器翻译引擎。</li><li id="e81c" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nz nr ns nt bi translated">系统用自然语言回答人类提出的问题。想想苹果的Siri、亚马逊的Alexa等语音助手。</li></ol><p id="c293" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇文章中，我们将完成一个NLP工作流程，探索使用潜在的Dirichlet分配进行主题建模和使用朴素贝叶斯进行文本分类的基础。</p><h1 id="f2a8" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">问题陈述:对一组新闻语料进行主题建模，开发文本分类模型。</h1><h1 id="4fcf" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">开始编码吧！</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="4f64" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">数据准备</h1><p id="b425" class="pw-post-body-paragraph lf lg iq lh b li nl ka lk ll nm kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">加载<a class="ae le" href="https://www.nytimes.com/" rel="noopener ugc nofollow" target="_blank">新闻语料库</a>并预览数据集的片段。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/112fd20c432e2a3e773a61439a5d193a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hb4mO4GK4o7y9ZhRCpIXA.png"/></div></div></figure><p id="93cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">新闻语料库中存在URL链接，让我们利用正则表达式来移除URL链接。<strong class="lh ja"> <em class="mn"> re.compile() </em> </strong>方法将一个正则表达式模式组合成模式对象进行模式匹配。这使我们能够再次搜索一个模式，而不用重写它。然后，我们将逐行拆分单独的新闻文章，删除开头和结尾的空格。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/7f5fd0e479dee3cb5ece7c449cd603d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFzONJakvrtFB4HFkiVEJg.png"/></div></div></figure><p id="db39" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">上面展示的示例文章看起来不错。让我们将处理后的新闻<a class="ae le" href="https://dictionary.cambridge.org/dictionary/english/corpus" rel="noopener ugc nofollow" target="_blank">语料库</a>加载到dataframe中，并创建一个名为“content”的列。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/17e05caa9e0083baf177c0a47bb0698c.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*uW5MtX2FXXhSrmNtYefwSA.png"/></div></figure><p id="bb3f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">索引0下的第一行似乎是空的。在清理之前，让我们找出数据框中的物品总数。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/48fbff4c86190f04106a644476115ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:148/format:webp/1*4l8vysu_6NfhmpUAVTrAOg.png"/></div></figure><h1 id="ee2b" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">数据清理</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/22c2186e961833bc4aa966f6897c6614.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*HzKLDGvDZV5fjzbEJFhavQ.png"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/bf3c73cc13fd0072aa0a64299b92b62b.png" data-original-src="https://miro.medium.com/v2/resize:fit:112/format:webp/1*jkfHP6xLlpY9Uz2TZ6vOpw.png"/></div></figure><p id="0a4e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们已经成功地从数据帧中删除了5个空行。下一步，我们将使用正则表达式删除包含数字的单词，并用一个空格替换它们。标准化也是通过将所有的单词转换成小写来完成的。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/905c343db4c8028b3732b5a44c8e0a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*pwMhxr-9hqP6dAW3CXIdXw.png"/></div></figure><p id="177d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些数据对我来说看起来仍然有些奇怪，为什么连字符还在那里？去掉所有的标点符号是个好主意。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/ab882d0f9607a249a887f99e4e506520.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*PkowRGRZ7Nsufy6rQ0ygNA.png"/></div></div></figure><h1 id="1e52" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">探索性数据分析</h1><p id="051b" class="pw-post-body-paragraph lf lg iq lh b li nl ka lk ll nm kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated"><strong class="lh ja">单词云</strong>是一种数据可视化技术，用于表示文本数据，其中每个单词的大小表明其频率或重要性。使用单词云可以突出显示重要的文本数据点，从而有助于获得最常见单词的可视化表示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/6ac626a6859d96627abbb2b6f9704957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6OSCrKeYNC1q4gOl99sIw.png"/></div></div></figure><h1 id="85bc" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">特征工程</h1><p id="659c" class="pw-post-body-paragraph lf lg iq lh b li nl ka lk ll nm kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated"><a class="ae le" href="http://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">自然语言工具包</a>(“NLTK”)是一个为文本工作和建模而编写的Python库。它为加载和清理文本提供了很好的工具，以便为使用机器学习和深度学习算法准备好数据。以下是一些用于特征工程的常用工具:</p><ul class=""><li id="13c1" class="nj nk iq lh b li lj ll lm lo oo ls op lw oq ma nq nr ns nt bi translated"><strong class="lh ja">标记化</strong>包括将原始文本分割成小的、不可分割的单元进行处理。NLTK提供了一个函数- <strong class="lh ja"> <em class="mn"> word_tokenize()，</em> </strong>用于将字符串拆分成记号(名义上是单词)。根据空格和标点符号拆分标记。</li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/761814bbf235d56517a9237189f4bc41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*5IFNfPyyNJ3RPEhpCkC17g.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">转换成<strong class="bd mt">单词的单个记号的句子</strong></figcaption></figure><ul class=""><li id="795a" class="nj nk iq lh b li lj ll lm lo oo ls op lw oq ma nq nr ns nt bi translated"><strong class="lh ja">停用词</strong>是对短语的深层含义没有贡献的词。最常见的例子有"<em class="mn">、</em>、<em class="mn">、</em>、<em class="mn">、</em>、<em class="mn">、</em>。NLTK提供了各种语言(如英语)的常用停用词列表。</li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/9344b3d770cb39f7ba7d1df295453aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7fCSQ8C6D5vDh86qSHxGbg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd mt">删除停用词</strong>，如“the”、“in”、“it”和“was”。</figcaption></figure><ul class=""><li id="9393" class="nj nk iq lh b li lj ll lm lo oo ls op lw oq ma nq nr ns nt bi translated"><strong class="lh ja">引理化</strong>使用单词的词汇和形态分析，旨在仅移除<a class="ae le" href="https://sites.google.com/a/norridge80.net/ms-matocha-s-first-grade/phonemic-experts/inflected-endings" rel="noopener ugc nofollow" target="_blank">屈折词尾</a>，并返回单词的基础或字典形式，这被称为<a class="ae le" href="https://en.wikipedia.org/wiki/Lemma_(morphology)#:~:text=In%20morphology%20and%20lexicography%2C%20a,by%20which%20they%20are%20indexed." rel="noopener ugc nofollow" target="_blank">引理</a>。NLTK提供了<strong class="lh ja"><em class="mn">WordNet lemmatizer()</em></strong>，它使用<a class="ae le" href="https://wordnet.princeton.edu/" rel="noopener ugc nofollow" target="_blank"> WordNet数据库</a>来查找单词的词条。</li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ot"><img src="../Images/ccceae3a705a5b97ef56c69827174374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8aHI7WE2TbUkKS9JwhZbw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd mt">从“小时”到“小时”、“状态”到“状态”导出</strong>的引理。</figcaption></figure><ul class=""><li id="80b5" class="nj nk iq lh b li lj ll lm lo oo ls op lw oq ma nq nr ns nt bi translated"><strong class="lh ja">词性标注</strong>或<strong class="lh ja">词性标注</strong>是将单词按其词性分类并相应标注的过程。词性标注器用于指定句子中每个单词的语法信息。</li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/ad1ea6d4c4adfcbbe2c78f94c3dcb2f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pK9MApg9C9Vgx_7T_oKD2w.png"/></div></div></figure><p id="6593" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以上缩写的几个例子，连同它的意思:</p><p id="6dd5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mn"> NN:名词，单数或复数</em></p><p id="3736" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">NNS:名词，复数</p><p id="0ade" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mn"> IN:介词或从属连词</em></p><p id="aa9f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">JJ:形容词</p><p id="1873" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">RB:副词</p><p id="bc12" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">NLTK使用来自<a class="ae le" href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html" rel="noopener ugc nofollow" target="_blank"> Penn Treebank项目</a>的标签集，该项目为语言结构注释自然出现的文本。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/0d024748edc4a0892af2ddd0d10f67ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FX2vH-uVEIQ5_JjY1XiPxg.png"/></div></div></figure><p id="4da1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">接下来，我们将使用<a class="ae le" href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/#:~:text=A%20bag%2Dof%2Dwords%20is,the%20presence%20of%20known%20words." rel="noopener ugc nofollow" target="_blank">单词袋</a>方法从新的语料库中提取特征，并建立其中出现的所有独特单词的词汇表。新的语料库被转换成简单的矢量表示<em class="mn">。现在，我们能够根据向量列表绘制出十个最常见的单词。</em></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/211c41c08ed47c9cd26ae514a264d4e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RjwrTlJT-x3JuxMgus5kwQ.png"/></div></div></figure><h1 id="2e0d" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">使用潜在狄利克雷分配(“LDA”)的主题建模</h1><p id="0547" class="pw-post-body-paragraph lf lg iq lh b li nl ka lk ll nm kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">LDA背后的关键思想是将语料库中的文档表示为潜在主题的随机混合，其中每个主题由词的分布来表征。LDA是语料库的生成概率模型。我们将通过使用scikit-learn实现<a class="ae le" href="https://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html" rel="noopener ugc nofollow" target="_blank"> LDA </a>，scikit-learn是一个用于机器学习的开源Python库。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="899e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，如上所述为<strong class="lh ja">主题模型对象</strong>定义一个函数。接下来，我们将准备<strong class="lh ja">文档术语矩阵</strong>。LDA主题模型算法需要文档单词矩阵作为主要输入。使用计数矢量化工具对文档进行矢量化，因为我们只能使用LDA的原始术语计数，LDA是一种概率图形模型。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="61c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">接下来，执行<strong class="lh ja">参数选择，选择最佳数量的主题。</strong>我们可以使用困惑度和对数似然性等指标来确定良好的模型性能。具有较高对数似然性和较低复杂度的模型被认为是好的。首先指定值的初始范围。</p><p id="e168" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对所有可能的主题大小应用LDA，<em class="mn"> k </em>，并确定其对数似然和困惑分数。<strong class="lh ja">通过执行for循环来创建新的LDA对象，并将其与文档术语矩阵相匹配，从而遍历主题编号的可能范围</strong>。然后将结果插入到创建的topic_models列表中。获得具有最高对数似然和最低困惑分数的<em class="mn"> </em> LDA的最优主题大小，<em class="mn"> k，</em>。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/f7dfddd05514d37c1116d20461e4d670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*nANwf8sWuDGuDOPmcDq-1A.png"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/8fedc4e258911e7f095ca426a8e5561a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-FSKQFX5RlwFxi8yn2dikg.png"/></div></div></figure><p id="c9cd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">结果显示，<strong class="lh ja"> <em class="mn"> k个主题:24个</em> </strong>是给出最高对数似然和最低困惑分数的主题的<strong class="lh ja">最佳数量。使用获得的最佳主题数量评估主题和术语</strong>，并显示前15个单词，以了解每篇新闻文章的主题类别。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/3ace2700783e631fad6aee08554d539d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d8HABw_qRBNDi0rjg7k8fg.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/186f5302d10fe608e1d645652e638995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02NuHg_rvZyBnHR_KVi6YA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">使用LDA优化获得24个主题。</figcaption></figure><p id="82de" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过<strong class="lh ja">使用pyLDAvis可视化主题模型，解释符合文本数据语料库的主题模型中的主题。pyLDAvis是一个用于交互式主题模型可视化的python库。</strong></p><p id="cf0c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如卡森·希沃特和肯尼·雪莉的研究论文<a class="ae le" href="https://www.aclweb.org/anthology/W14-3110.pdf" rel="noopener ugc nofollow" target="_blank"> LDAvis:一种可视化和解释主题的方法</a>中所介绍的，</p><blockquote class="mk ml mm"><p id="f033" class="lf lg mn lh b li lj ka lk ll lm kd ln mo lp lq lr mp lt lu lv mq lx ly lz ma ij bi translated">LDAvis，一个基于web的交互式可视化主题，使用潜在的Dirichlet分配进行估计，它是使用R和D3的组合构建的。我们的可视化提供了主题的全局视图(以及它们之间的区别)，同时允许深入检查与每个主题最相关的术语。</p></blockquote><p id="a766" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一个好的主题模型应该没有重叠，每个主题都有一个相当大的区域。我们将向pyLDAvis传递三个参数，(1) LDA对象，(2)文档术语矩阵，以及(3)计数矢量器。相关性度量通过主题内的概率相对于整个语料库的概率来排序。如上述研究论文所述，推荐的最佳相关性度量是<strong class="lh ja"> λ=0.6。</strong></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ca"><img src="../Images/2335211981adb62091acb8c6cb331e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16CtdJgXM3vQTLTBLR3CMw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">pyLDAvis可视化工具显示与主题19最相关的术语。</figcaption></figure><p id="1baa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">主题间距离图允许我们了解主题如何相互关联，包括主题组之间潜在的更高层次的结构。圆圈之间的<strong class="lh ja">距离</strong>表示主题之间的分布相似度接近度。基于上述主题间距离图，主题#1、2、3和7彼此紧密相关。</p><p id="7dc2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">圆圈的<strong class="lh ja">面积与主题的总体流行度成正比。在这种情况下，圆圈#1具有出现频率较高的术语。此外，圆圈根据流行程度进行编号和分类。</strong></p><p id="e103" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要获得文档的主导主题，一种合理的方法是查看哪个主题对该文档的贡献最大，并将该主题作为标签分配给该文档。属性<strong class="lh ja"> <em class="mn"> n_components </em> </strong>包含主题总数，通过循环寻找<strong class="lh ja"> <em class="mn"> n_components </em> </strong>，为主题提供索引。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/1fa025cfb943e10ad8fb11b69e1d61a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q5j10SZZz5wa1FOBh6TGog.png"/></div></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/bebbf932db1bee2a01eeb887b961c242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_Hx1iV4x9WCnYW12XDluw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">以粗体和绿色字体表示的主导主题的最高贡献。</figcaption></figure><p id="5d89" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们对确定的主要主题进行直观检查。</p><p id="5b7b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">主题19 </strong>由上面pyLDAvis可视化工具中显示的术语组成。因此，通过视觉检查，我们可以确认文档19的主导主题确实是主题19。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pd"><img src="../Images/6bd97233fbb414f1b7a249bfb5cbebeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JZEIiMpMoxvEagOsHuKZyA.png"/></div></div></figure><h1 id="0ee1" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">使用朴素贝叶斯的文本分类</h1><p id="6cc9" class="pw-post-body-paragraph lf lg iq lh b li nl ka lk ll nm kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">文本分类是根据文本的内容给文本分配标签、标记或类别的过程。朴素贝叶斯是一个概率算法家族，利用<a class="ae le" href="https://en.wikipedia.org/wiki/Bayes'_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>来预测文本的标签。</p><p id="ee57" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">看看Kamran Kowsari等人的研究论文<a class="ae le" href="https://arxiv.org/abs/1904.08067" rel="noopener ugc nofollow" target="_blank">文本分类算法:调查</a>，</p><blockquote class="mk ml mm"><p id="6c50" class="lf lg mn lh b li lj ka lk ll lm kd ln mo lp lq lr mp lt lu lv mq lx ly lz ma ij bi translated">自20世纪50年代以来，朴素贝叶斯文本分类已被广泛用于文档分类任务。朴素贝叶斯分类方法的理论基础是贝叶斯定理，该定理是由托马斯·贝叶斯在1701-1761年间提出的。最近的研究已经广泛地解决了信息检索中的这种技术。这种技术是一种生成模型，是最传统的文本分类方法。</p></blockquote><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pe"><img src="../Images/ecdc1ba34a33c8fb41d5bf6e2eb1e351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_I6T5AzfOylsvEQ8iHHIw.png"/></div></div></figure><p id="3a5e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">主导主题被表示为整数，因此我们将替换它们并用最主导的主题标签来标记数据帧。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pf"><img src="../Images/dac6f5eb74802f40e473903f1af397cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ryM7LoWf1gKwmI1REIcs0Q.png"/></div></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pg"><img src="../Images/e6b19af019ebd65a1dab151e24b42acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YNnwphT7kkH9RcpiVQqLBQ.png"/></div></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/23f1708b1e22c123b0869491cb9bbf10.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*GmqtAoJ8DqAgnU7IyVyM7Q.png"/></div></figure><p id="fb08" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">数据帧现在可以进行训练了。将数据帧分为输入文本和输出标签。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/d10f02f1dc0632475a2bd69beee65b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*v5S7-4b5FdB2uk4kiT5x_Q.png"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ce82880cabb3cadf30ac978d3b101a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*43is9cA3a-TJd7KrDOVjfg.png"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="25a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过使用scikit-learn内置的英语停用词表，使用<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器</a>将文本文档集合转换为令牌计数矩阵。<strong class="lh ja"> <em class="mn"> ngram_range </em> </strong>表示要提取的不同单词n元文法或字符n元文法的n值范围的下限和上限。将使用所有n值，使得min_n &lt; = n &lt; = max_n。在这种情况下，默认的<strong class="lh ja"> ngram_range </strong>为(1，1)，即。使用了unigrams。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/1373529d77eea3db4ae9944c6622451b.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*L5end2v_4XgRVW-47mslQQ.png"/></div></figure><p id="e30d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">选择了<a class="ae le" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank">多项式朴素贝叶斯模型</a>来实现用于多项式分布数据的朴素贝叶斯算法。它是文本分类中使用的两个经典朴素贝叶斯变体之一，其中数据通常表示为词向量计数。对训练数据拟合模型，并应用拟合的模型来预测测试集的主题标签。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pk"><img src="../Images/1f84ca0b1cd6a24b083c36b7bd02b622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tN7szqNlw1TU7-9XhIXZbg.png"/></div></div></figure><h1 id="a9b7" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">结果评估</h1><div class="pl pm gp gr pn po"><a href="https://medium.com/towards-artificial-intelligence/use-of-decision-trees-and-random-forest-in-machine-learning-1e35e737b638" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd ja gy z fp pt fr fs pu fu fw iz bi translated">决策树和随机森林在机器学习中的应用</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">分类问题的监督学习研究</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">medium.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc ky po"/></div></div></a></div><p id="1193" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请务必查看我的上述帖子，以深入了解<strong class="lh ja">混淆矩阵</strong>和<strong class="lh ja">分类报告。</strong></p><h1 id="0cb8" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">1.混淆矩阵</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qd"><img src="../Images/e5284d297c86e2f2d9fd745b24eb15db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-2NgoUr8ogGCO2OLViAtA.png"/></div></div></figure><p id="3543" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">基于上述归一化的混淆矩阵，朴素贝叶斯分类器表现良好，预测了高百分比的真实标签。</p><h1 id="bd1f" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">2.分类报告和错误度量</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/3ea647792c2348b708435ebf3e3bb9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*ys1GMofKP1JsnTymo43Tvg.png"/></div></figure><p id="1084" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">分类报告</strong>显示了基于每个类别的主要分类度量<strong class="lh ja">的表示</strong>，并给出了对分类器行为的更深层次的直觉，而不是全局准确性。这些指标是根据真阳性和假阳性以及真阴性和假阴性来定义的。</p><p id="9cee" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">需要注意的一个重要方面是“教育”类别的分数。获得了1.00的精确度分数，这相当于被分类为阳性的所有实例的100%分数。然而，0.4的不良回忆分数(&lt; 0.5) was obtained with a slightly above average f1-score of 0.57. This indicates that <strong class="lh ja">对于所有实际上为阳性的实例，低百分比实际上被正确分类</strong>。对于数据集中25个实际出现的“教育”文章的低支持，也许在“教育”类别中训练更多的文章将有助于提高分数。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/68496eb2752fa482aea503b230506f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*OATyYoQPMW-PwmW7zxW6dw.png"/></div></figure><p id="5672" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">在全球基础上</strong>，我们还通过将测试标签与预测标签进行比较，计算了各种<strong class="lh ja">误差指标</strong>，如准确度、精确度、召回率和f1分数。总体而言，朴素贝叶斯分类器获得的分数远高于基线0.5。</p><h1 id="e44d" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">已训练模型的部署</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qf"><img src="../Images/24ba95fd233c2bfe1af8b947a81f102e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxwCbWBWFFpyTfIkWkPKDQ.png"/></div></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="223b" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">主题名称预测</h1><p id="4e09" class="pw-post-body-paragraph lf lg iq lh b li nl ka lk ll nm kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">使用训练好的模型，让我们请求用户输入新的文本。</p><p id="c7f1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面的例子接收一个新的文本，并调用上面定义的函数对标签进行预处理和预测。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qg"><img src="../Images/25cd7a6733713dc769c168e39558a284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yOCTjDQKjedOFBkXOguF1g.png"/></div></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qh"><img src="../Images/4e286dd456016abde0a522fe980ba9c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j3U4ZVfYwAyDTtA2Zik18g.png"/></div></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qi"><img src="../Images/32a60b7a3cdf319d21ecb3dc49508d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o830X2iLqh1628qfV9juvA.png"/></div></div></figure><h1 id="dff2" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">更多示例新闻片段的结果</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qj"><img src="../Images/bfc4ad1b3d1d8659ae561f67f767ef2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Epog2TaW2_jjCm4twKEyQ.png"/></div></div></figure></div><div class="ab cl qk ql hu qm" role="separator"><span class="qn bw bk qo qp qq"/><span class="qn bw bk qo qp qq"/><span class="qn bw bk qo qp"/></div><div class="ij ik il im in"><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qr"><img src="../Images/10166ed73cdf2d760e70ec8f604c5c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uVQpkwHba3_fHvVzhmda0Q.png"/></div></div></figure></div><div class="ab cl qk ql hu qm" role="separator"><span class="qn bw bk qo qp qq"/><span class="qn bw bk qo qp qq"/><span class="qn bw bk qo qp"/></div><div class="ij ik il im in"><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qs"><img src="../Images/40f3940c708ba483cd1a630c183bc03b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qCm33JSj-CuYUQakR2MleA.png"/></div></div></figure><h1 id="687b" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">结论</h1><p id="818c" class="pw-post-body-paragraph lf lg iq lh b li nl ka lk ll nm kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">LDA的建模目标已经成功实现。对于主题和术语的评估，从新的语料库中获得最佳的主题数量。这样，<strong class="lh ja">就能够</strong> <strong class="lh ja">确定各种主题类别，并解开隐藏的主题结构</strong>。</p><p id="2344" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，朴素贝叶斯分类器的使用再次被证明适用于文本分类。在大量数据用于训练的情况下，基于对结果的评估，主题建模和文本分类的结果是令人满意的。</p><p id="de84" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过使用从互联网上提取的各种新闻片段测试训练好的模型，<strong class="lh ja">结果被证明是积极的，并且在主题标签应该是什么的预期之内</strong>。</p></div><div class="ab cl qk ql hu qm" role="separator"><span class="qn bw bk qo qp qq"/><span class="qn bw bk qo qp qq"/><span class="qn bw bk qo qp"/></div><div class="ij ik il im in"><h1 id="30ea" class="mr ms iq bd mt mu qt mw mx my qu na nb kf qv kg nd ki qw kj nf kl qx km nh ni bi translated">参考</h1><ol class=""><li id="1bff" class="nj nk iq lh b li nl ll nm lo nn ls no lw np ma nz nr ns nt bi translated"><a class="ae le" href="https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf" rel="noopener ugc nofollow" target="_blank">世界的数字化:从边缘到核心</a></li><li id="48ae" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nz nr ns nt bi translated"><a class="ae le" href="https://www.linguamatics.com/what-text-mining-text-analytics-and-natural-language-processing" rel="noopener ugc nofollow" target="_blank">什么是文本挖掘、文本分析和自然语言处理</a></li><li id="0e06" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nz nr ns nt bi translated"><a class="ae le" href="https://www.peakindicators.com/blog/unlocking-insights-from-unstructured-data-with-text-mining" rel="noopener ugc nofollow" target="_blank">通过文本挖掘从非结构化数据中发掘洞察力</a></li><li id="f058" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nz nr ns nt bi translated"><a class="ae le" href="https://machinelearningmastery.com/clean-text-machine-learning-python/" rel="noopener ugc nofollow" target="_blank">如何用Python清理机器学习的文本</a></li><li id="bf6b" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nz nr ns nt bi translated"><a class="ae le" href="https://www.aclweb.org/anthology/W14-3110.pdf" rel="noopener ugc nofollow" target="_blank"> LDAvis:一种可视化和解释主题的方法</a></li><li id="7603" class="nj nk iq lh b li nu ll nv lo nw ls nx lw ny ma nz nr ns nt bi translated"><a class="ae le" href="https://arxiv.org/abs/1904.08067" rel="noopener ugc nofollow" target="_blank">文本分类算法综述</a></li></ol></div></div>    
</body>
</html>