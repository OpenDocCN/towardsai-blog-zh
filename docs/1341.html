<html>
<head>
<title>Neural Architecture Search (NAS) and Reinforcement Learning (RL)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经结构搜索(NAS)和强化学习(RL)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/neural-architecture-search-nas-and-reinforcement-learning-rl-5515f32135b9?source=collection_archive---------7-----------------------#2021-01-03">https://pub.towardsai.net/neural-architecture-search-nas-and-reinforcement-learning-rl-5515f32135b9?source=collection_archive---------7-----------------------#2021-01-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2291" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><p id="ecac" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">计算机视觉，更具体地说是分类任务，是最流行的深度学习技术之一。卷积神经网络(CNN)特别流行于机器学习的计算机视觉领域。CNN的特殊性在于每个隐层神经元与前一层神经元子集的选择性连接。这意味着在分类任务中，初始隐藏层可以显示边界，第二个隐藏层可以显示特定的形状，依此类推，直到识别特定对象的最后一个隐藏层。</p><p id="dc09" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">CNN架构包括几种层类型，包括卷积、池化和全连接。元建模中的许多研究努力试图在设计神经网络架构时最小化人工干预。由[1]提出的基于RL(强化学习)的方法在众所周知的分类基准上取得了最先进的结果。NAS方法增强了NAS的吸引力，并引发了关于这一研究主题的一系列激动人心的工作。我们的调查主要是出于检查架构搜索的背景，并集中于近年来获得最大兴趣的RL方法。</p><p id="efe5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">CNN的性能主要取决于模型结构、训练过程和数据表示。几个超参数用于控制所有这些变量，并对学习过程有重大影响。由于CNN结构的未知性，CNN参数设置被认为是一个黑箱问题。在这种情况下，自动设计解决方案是非常必要的，并启动了大量的研究。CNN超参数调整的任务已经通过元建模来处理。</p><p id="d72e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用强化学习(RL)的神经结构搜索(NAS)的一些论文:</p><h1 id="efd3" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak"> MetaQNN [2]: </strong></h1><p id="103e" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">该模型[2]依赖于Q学习在有限空间中顺序选择网络层及其参数。层组成它，如图1所示:卷积(C)、池化(P)、完全连接(FC)、全局平均池化和softmax。MetaQNN与相似和不同的手工制作的CNN架构进行了竞争性评估。给定一定数量的限制，代理动作空间被集成到代理可能移动到的可能层中(图2)。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/4942da7c90a03bbeae6b48f5eb9c62ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*ybL5U5etlArlKKqtb2X8PA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">图一。状态空间可能的参数[2]</figcaption></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/b15b1dd16dfe465e38b98603e7ac26cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*bXe2bWg74lq3lOhIZwL6YA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">图二。完整状态和动作空间(a)以及代理选择的路径(b)的图示[2]</figcaption></figure><h1 id="1df8" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">具有强化学习的神经架构搜索[1]: </strong></h1><p id="bf26" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">控制器递归神经网络(RNN)为CNN的卷积层依次选择参数[1]。softmax分类器预测每个序列输出，并用作后续序列的输入。参数是-过滤器的高度和宽度，步幅的高度和宽度，每层过滤器的数量。一旦层数超过预定义的值，建筑设计就停止。所建立的体系结构的准确性是对RNN控制器的RL训练的奖励。在CIFAR10和Penn Treebank数据集上取得了竞争结果(图3)。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/cc334c57cec0722a0802cc069e9fce52.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*-P2ejLurzlLOlbRTbBJCPQ.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">图3。说明了代理用来选择超参数的方式[1]。</figcaption></figure><h1 id="356d" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">高效架构搜索(EAS) [3] : </strong></h1><p id="63b7" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">在最近的工作中，有效的架构搜索(EAS)，[3]实现了允许重用预先存在的模型的技术。操作包括网络变换活动，如添加、扩展和删除图层。EAS解决方案的重点是受Net2Net技术启发的更深层次的学生网络。如图4所示，用双向递归神经网络[4]实现的编码器网络为具有给定架构的网络提供参与者。Net2Wider的动作网络共享同一个sigmoid分类器，并根据每个编码器的隐藏状态决定是否加宽层。Net2Deeper中的动作网络将最终的双LSTM隐藏层输入到递归网络中，递归网络决定在哪里插入该层以及插入层的相应参数。EAS提供了类似的最先进的模型，无论是手动的还是自动的，这提供了相对较小的计算资源。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/9eea4712d11b6c0ab02bf52fa4b6cd4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*jEsPa1yvH6JjodQglH41mA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">图4。网络改造的元控制器操作[3]。</figcaption></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/afe7c4549a96a33b2e2f14280629af47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*7GxYAdZlwTMB9DgS2owc6A.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated"><strong class="bd kw">图五。</strong>两个行动网络的内部结构:Net2Wider和Net2Deeper [3]。</figcaption></figure><h1 id="d434" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak"> BlockQNN [5]: </strong></h1><p id="6fc2" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">最近关于神经结构搜索的大部分工作是基于更复杂的模块化(多分支)结构。然后，这些多分支组件重复堆叠，通过跳跃连接创建深层架构。“逐块”架构通过加快搜索过程显著减少了搜索空间。BlockQNN是实现块式架构的第一种方法[5]。它使用Q学习强化技术自动构建卷积网络。Block架构接近现有网络，比如ResNet和Inception (GoogLeNet)。块搜索空间如图5所示，由五个参数组成。该参数由五层组成:索引(位于块中)、操作类型(通常从七种类型中选择)、内核大小和前趋层的两层索引。通过顺序堆叠n次，基于指定的块构建整个网络。图6描述了两个不同的块示例，一个具有多分支结构，另一个显示了跳过连接。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/e9d42664fe26032d791dcd36ed97e4d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*nhLWWme3Jk2_m6xZcL4Mrg.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">图5。网络结构代码空间[5]。</figcaption></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/98fd5e10c885f25a6bbf10a5d84a41a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*p0zeAaLyhKNhMrCXVUS1xw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">图6。具有代表性的块样本及其网络结构代码[5]</figcaption></figure><h1 id="b4f7" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">渐进式神经架构搜索(PNAS)【6】:</strong></h1><p id="88a7" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">渐进式神经架构搜索(PNAS) [6]提出探索模块化结构的空间，从简单的模型开始，然后发展到更复杂的模型。在这种方法下，模块化结构被称为单元，并且包括设定数量的块。在这种方法中，模块化结构称为单元，包含固定数量的块。每个块由八个选定运算符中的两个运算符组成。为了创建最终的CNN，首先学习细胞结构，然后将其堆叠N次。PNAS的主要贡献在于通过避免在单元的整个空间中直接搜索来优化搜索过程。最大像元大小为5个块，K等于256，PNAS快了5倍。性能预测比设计单元的完全训练花费更少的时间。最佳单元架构如图7所示。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/7a63ad145f1a2e685332598b48343e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*RUj4mVpKbNFw3tG-0C5msg.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">图7。PNAS的最佳选择细胞架构[6]。</figcaption></figure><p id="e98d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">当前RL方法在NAS领域的成功已被广泛证明。然而，这是通过高计算资源的成本来实现的。由于这个原因，目前的技术水平阻止了个体研究人员和小型研究实体(公司和实验室)充分利用这项创新技术。</p><p id="cf98" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">跟随作者<a class="mo mp ep" href="https://medium.com/u/add16b429026?source=post_page-----5515f32135b9--------------------------------" rel="noopener" target="_blank">阿琼戈什哈</a>T4来到这里。</p><p id="dcdb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">参考文献:</strong></p><p id="265f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[1] B. Zoph，Q.V. Le，具有强化学习的神经架构搜索，学习表征国际会议论文集(ICLR)，2017。</p><p id="ba77" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[2] B. Baker，O. Gupta，n .纳伊克，R. Raskar，利用强化学习设计神经网络架构，学习表征国际会议论文集(ICLR)，2017。</p><p id="2b58" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[3]蔡宏，陈婷，张伟，于，王军，基于网络变换的高效体系结构搜索，2018年第32届人工智能大会。</p><p id="9531" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[4] M. Schuster，K. Paliwal，双向递归神经网络，译。签名。继续。45 (11) 2673–2681, 1997.</p><p id="5c79" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[5]钟，严，吴，邵，刘，实用的块式神经网络体系结构生成，IEEE计算机视觉与模式识别会议，，2018 .</p><p id="af82" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[6]刘C，佐夫B，诺依曼M，施伦斯J，华W，李立杰，L，尤耶A，黄J，墨菲K，“渐进式神经架构搜索”，载于:欧洲计算机视觉会议，2018年。</p></div></div>    
</body>
</html>