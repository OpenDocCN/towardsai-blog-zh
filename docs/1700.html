<html>
<head>
<title>Grouping Classes Using K-Nearest Neighbors Algorithm — Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用K-最近邻算法对类进行分组Python</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/grouping-classes-using-k-nearest-neighbors-algorithm-python-81f04f0a1fba?source=collection_archive---------2-----------------------#2021-03-23">https://pub.towardsai.net/grouping-classes-using-k-nearest-neighbors-algorithm-python-81f04f0a1fba?source=collection_archive---------2-----------------------#2021-03-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="b9e9" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="a01a" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">KNN算法的理解和实现指南。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/6e66c83d81e72fae51e1b65534060a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*3KV4kKLDGF-ib0KjS6ERBw.jpeg"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">由<a class="ae la" href="https://unsplash.com/@anniespratt?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">安妮·斯普拉特</a>在<a class="ae la" href="/s/photos/groups?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="abde" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">什么是KNN算法？</h1><p id="b8f9" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">这是一种用于分类任务的算法，其工作原理非常简单。</p><h1 id="6b77" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">它是如何工作的？</h1><p id="2bd1" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">KNN算法是非常基本的。训练算法存储所有数据。并且预测算法计算数据点到数据中所有点的距离，按照离数据点距离的递增顺序对点进行排序，然后预测“k”个最近点的多数标签。</p><h1 id="3a20" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">这种算法的优点是什么？</h1><ol class=""><li id="d08b" class="mp mq iq lv b lw lx lz ma mc mr mg ms mk mt mo mu mv mw mx bi translated">它非常简单，易于理解和实现。</li><li id="8725" class="mp mq iq lv b lw my lz mz mc na mg nb mk nc mo mu mv mw mx bi translated">它只使用了两个参数:k和距离度量。</li><li id="4b00" class="mp mq iq lv b lw my lz mz mc na mg nb mk nc mo mu mv mw mx bi translated">它可以对任意数量的类进行分类。</li><li id="f6fb" class="mp mq iq lv b lw my lz mz mc na mg nb mk nc mo mu mv mw mx bi translated">训练步骤非常容易实现，并且可以在任何阶段添加更多数据。</li></ol><h1 id="458e" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">这种算法的缺点是什么？</h1><ol class=""><li id="9292" class="mp mq iq lv b lw lx lz ma mc mr mg ms mk mt mo mu mv mw mx bi translated">它只适用于数字数据。对于分类数据，它可能表现不好。</li><li id="7b65" class="mp mq iq lv b lw my lz mz mc na mg nb mk nc mo mu mv mw mx bi translated">预测的成本非常高。</li><li id="582f" class="mp mq iq lv b lw my lz mz mc na mg nb mk nc mo mu mv mw mx bi translated">它在处理高维数据时表现不佳。</li></ol><h1 id="2f3a" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">如何用Python实现KNN？</h1><p id="2fa3" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">使用人工数据集来执行分类。有两个类0和1。目标是将数据分为两个不同的类别。</p><p id="bf5c" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated"><strong class="lv ja"> →导入包</strong></p><p id="a9bc" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated">导入了帮助处理数据的库——pandas和numpy以及数据可视化包——matplotlib和seaborn。</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="eebc" class="nn lc iq nj b gy no np l nq nr"><strong class="nj ja">&gt;&gt;&gt; import</strong> pandas <strong class="nj ja">as</strong> pd<br/><strong class="nj ja">&gt;&gt;&gt; import</strong> numpy <strong class="nj ja">as</strong> np<br/><strong class="nj ja">&gt;&gt;&gt; import</strong> matplotlib.pyplot <strong class="nj ja">as</strong> plt<br/><strong class="nj ja">&gt;&gt;&gt; import</strong> seaborn <strong class="nj ja">as</strong> sns<br/><strong class="nj ja">&gt;&gt;&gt; %</strong>matplotlib inline</span></pre><p id="b30b" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated"><strong class="lv ja"> →读取数据</strong></p><p id="c857" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated">数据有一个取值为0或1的目标类。其他列是数字，但是没有意义，因为数据是人工的。</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="3d11" class="nn lc iq nj b gy no np l nq nr">&gt;&gt;&gt; df <strong class="nj ja">=</strong> pd.read_csv('datasets/dataset.csv')<br/>&gt;&gt;&gt; df.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi ns"><img src="../Images/9fb0ebbe39f665030ed5fd15e04b7807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-bc3bg6JNrY_unDCa1FTNQ.png"/></div></div></figure><p id="0675" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated"><strong class="lv ja"> →标准化数据</strong></p><p id="7027" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated">在KNN，重要的是将变量标准化。原因是KNN通过识别最接近的观察值来对测试观察值进行分类，并且如果有任何大范围的变量，那么它将对观察值之间的距离有更大的影响。这是使用sci-kit学习包完成的。导入标准标量函数并创建其实例。</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="fa09" class="nn lc iq nj b gy no np l nq nr"><strong class="nj ja">&gt;&gt;&gt; from</strong> sklearn.preprocessing <strong class="nj ja">import</strong> StandardScaler<br/>&gt;&gt;&gt; scaler <strong class="nj ja">=</strong> StandardScaler()</span></pre><p id="22de" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated">接下来，将scaler对象与要素(没有目标类的数据)相匹配，并使用transform()方法将要素转换为缩放版本。</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="48be" class="nn lc iq nj b gy no np l nq nr">&gt;&gt;&gt; scaler.fit(df.drop('TARGET CLASS',axis<strong class="nj ja">=</strong>1))<br/>&gt;&gt;&gt; scaled_features <strong class="nj ja">=</strong> scaler.transform(df.drop('TARGET CLASS',axis<strong class="nj ja">=</strong>1))</span></pre><p id="2b35" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated">可以以数据框的形式查看缩放后的特征，该数据框将用于模型构建。</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="7c73" class="nn lc iq nj b gy no np l nq nr">&gt;&gt;&gt; df_final <strong class="nj ja">=</strong> pd.DataFrame(scaled_features,columns<strong class="nj ja">=</strong>df.columns[:<strong class="nj ja">-</strong>1])<br/>&gt;&gt;&gt; df_final.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi nx"><img src="../Images/8bf3b851752a32fc11bf1419d88809c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IaDplD5mehhOUEiigqRTEQ.png"/></div></div></figure><p id="343a" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated"><strong class="lv ja"> →将数据分成训练和测试数据</strong></p><p id="6ea3" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated">将从scikit学习包中使用训练测试分割功能。</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="b0cd" class="nn lc iq nj b gy no np l nq nr"><strong class="nj ja">&gt;&gt;&gt; from</strong> sklearn.model_selection <strong class="nj ja">import</strong> train_test_split<br/>&gt;&gt;&gt; X_train, X_test, y_train, y_test <strong class="nj ja">=</strong> train_test_split(scaled_features,df['TARGET CLASS'],test_size<strong class="nj ja">=</strong>0.30)</span></pre><h1 id="8c8a" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">选择K值</h1><p id="df26" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">使用肘法，将选择K的值。因此K值将被循环，并且对于每个值，将计算误差率。</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="7b3a" class="nn lc iq nj b gy no np l nq nr"><strong class="nj ja">&gt;&gt;&gt; from</strong> sklearn.neighbors <strong class="nj ja">import</strong> KNeighborsClassifier</span><span id="5d37" class="nn lc iq nj b gy ny np l nq nr">&gt;&gt;&gt; error_rate <strong class="nj ja">=</strong> []</span><span id="055b" class="nn lc iq nj b gy ny np l nq nr"><strong class="nj ja">&gt;&gt;&gt; for</strong> i <strong class="nj ja">in</strong> range(1,40):<br/>        knn <strong class="nj ja">=</strong> KNeighborsClassifier(n_neighbors<strong class="nj ja">=</strong>i)<br/>        knn.fit(X_train,y_train)<br/>        pred_i <strong class="nj ja">=</strong> knn.predict(X_test)<br/>        error_rate.append(np.mean(pred_i <strong class="nj ja">!=</strong> y_test))</span><span id="b7d5" class="nn lc iq nj b gy ny np l nq nr">&gt;&gt;&gt; plt.figure(figsize<strong class="nj ja">=</strong>(10,6))<br/>&gt;&gt;&gt; plt.plot(range(1,40),error_rate,marker<strong class="nj ja">=</strong>'o',<br/>    markerfacecolor<strong class="nj ja">=</strong>'red')<br/>&gt;&gt;&gt; plt.title('Error Rate v/s K Value')<br/>&gt;&gt;&gt; plt.xlabel('K')<br/>&gt;&gt;&gt; plt.ylabel('Error Rate')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi nz"><img src="../Images/049e6af037013a6dbab43efc5899925e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0DAx0mrPBqSOTpYrknACLw.png"/></div></div></figure><p id="14e2" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated">使用上图，K值可以认为是30。</p><p id="3293" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated"><strong class="lv ja"> →建模</strong></p><p id="75bb" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated">该模型是通过使用从sci-kit learn导入的KNeighborsClassifier方法构建的。将创建KNN函数的对象，并且将提到等于上面获得的K值的邻居数量。然后根据训练数据拟合对象。</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="5847" class="nn lc iq nj b gy no np l nq nr">&gt;&gt;&gt; knn <strong class="nj ja">=</strong> KNeighborsClassifier(n_neighbors<strong class="nj ja">=</strong>30)<br/>&gt;&gt;&gt; knn.fit(X_train,y_train)</span><span id="7536" class="nn lc iq nj b gy ny np l nq nr">KNeighborsClassifier(n_neighbors=30)</span></pre><p id="e00d" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated"><strong class="lv ja"> →预测</strong></p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="00dd" class="nn lc iq nj b gy no np l nq nr">&gt;&gt;&gt; pred <strong class="nj ja">=</strong> knn.predict(X_test)</span></pre><p id="4612" class="pw-post-body-paragraph lt lu iq lv b lw nd ka ly lz ne kd mb mc nf me mf mg ng mi mj mk nh mm mn mo ij bi translated"><strong class="lv ja"> →评估</strong></p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="1e6c" class="nn lc iq nj b gy no np l nq nr"><strong class="nj ja">&gt;&gt;&gt; from</strong> sklearn.metrics <strong class="nj ja">import</strong> classification_report,confusion_matrix<br/>&gt;&gt;&gt; print(confusion_matrix(y_test,pred))</span><span id="85a8" class="nn lc iq nj b gy ny np l nq nr">[[133  34]<br/> [ 16 117]]</span><span id="767a" class="nn lc iq nj b gy ny np l nq nr">&gt;&gt;&gt; print(classification_report(y_test,pred))<br/>precision    recall  f1-score   support</span><span id="178b" class="nn lc iq nj b gy ny np l nq nr">           0       0.89      0.80      0.84       167<br/>           1       0.77      0.88      0.82       133</span><span id="af47" class="nn lc iq nj b gy ny np l nq nr">    accuracy                           0.83       300<br/>   macro avg       0.83      0.84      0.83       300<br/>weighted avg       0.84      0.83      0.83       300</span></pre></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><blockquote class="oh"><p id="b153" class="oi oj iq bd ok ol om on oo op oq mo dk translated"><em class="or">这里的</em>  <em class="or">指数据集和笔记本</em> <a class="ae la" href="https://github.com/jayashree8/Machine_learning_supervised_models/tree/master/Classification%20models" rel="noopener ugc nofollow" target="_blank"> <em class="or">。</em></a></p></blockquote><h2 id="ad36" class="nn lc iq bd ld os ot dn lh ou ov dp ll mc ow ox ln mg oy oz lp mk pa pb lr iw bi translated">初级机器学习书籍可以参考:</h2><div class="pc pd gp gr pe pf"><a href="https://amzn.to/3i3XU1A" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd ja gy z fp pk fr fs pl fu fw iz bi translated">Python机器学习:机器学习和深度学习的Python编程初学者指南</h2></div><div class="pm l"><div class="pn l po pp pq pm pr ku pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a href="https://amzn.to/3fQc6IW" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd ja gy z fp pk fr fs pl fu fw iz bi translated">一百页的机器学习书</h2></div><div class="pm l"><div class="ps l po pp pq pm pr ku pf"/></div></div></a></div><h2 id="ab42" class="nn lc iq bd ld os pt dn lh ou pu dp ll mc pv ox ln mg pw oz lp mk px pb lr iw bi translated">可以参考的高级机器学习书籍:</h2><div class="pc pd gp gr pe pf"><a href="https://amzn.to/2SxwQNw" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd ja gy z fp pk fr fs pl fu fw iz bi translated">用Scikit-Learn、Keras和张量流进行机器学习:概念、工具和技术…</h2></div><div class="pm l"><div class="py l po pp pq pm pr ku pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a href="https://amzn.to/3wz62eE" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd ja gy z fp pk fr fs pl fu fw iz bi translated">模式识别和机器学习(信息科学和统计学)</h2></div><div class="pm l"><div class="pz l po pp pq pm pr ku pf"/></div></div></a></div><blockquote class="qa qb qc"><p id="f0d5" class="lt lu qd lv b lw nd ka ly lz ne kd mb qe nf me mf qf ng mi mj qg nh mm mn mo ij bi translated"><em class="iq">联系我:</em><a class="ae la" href="https://www.linkedin.com/in/jayashree-domala8/" rel="noopener ugc nofollow" target="_blank"><em class="iq">LinkedIn</em></a></p><p id="013c" class="lt lu qd lv b lw nd ka ly lz ne kd mb qe nf me mf qf ng mi mj qg nh mm mn mo ij bi translated"><em class="iq">查看我的其他作品:</em> <a class="ae la" href="https://github.com/jayashree8" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> GitHub </em> </a></p></blockquote></div></div>    
</body>
</html>