<html>
<head>
<title>CVPR 2021 Best Paper Award: GIRAFFE — Controllable Image Generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CVPR 2021年最佳论文奖:长颈鹿——可控图像生成</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/cvpr-2021-best-paper-award-giraffe-controllable-image-generation-24eac0001ca4?source=collection_archive---------0-----------------------#2021-07-07">https://pub.towardsai.net/cvpr-2021-best-paper-award-giraffe-controllable-image-generation-24eac0001ca4?source=collection_archive---------0-----------------------#2021-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b7b0" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="aa99" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用改进的GAN架构，他们可以移动图像中的对象，而不会影响背景或其他对象！</h2></div><blockquote class="kr ks kt"><p id="6237" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/cvpr-2021-best-paper/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/cvpr-2021-best-paper/" rel="noopener ugc nofollow" target="_blank">我的博客上看到了！</a></p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/7dc1dbb6e0263d42d2ff9324c0395077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9G2u3B4Ugp152CiKHS5cmQ.png"/></div></div></figure><p id="80ac" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">听听这个故事…</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="988f" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">CVPR 2021年最佳论文奖授予了来自马普智能系统研究所和图宾根大学的Michael Niemeyer和Andreas Geiger，他们的论文名为Giraffe，研究了可控图像合成的任务。换句话说，他们着眼于生成新的图像，并控制什么会出现，物体及其位置和方向，背景等。使用改进的GAN架构，他们甚至可以在不影响背景或其他对象的情况下移动图像中的对象！CVPR是一个一年一度的会议，就在上周，大量计算机视觉领域的新研究论文专门为这次会议而发表。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/207efb1557789963f341d12c25902b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*JBMMq0n6nNFDl_Gl4mXxTw.gif"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">作者制作的预告片示例。迈克尔·尼迈耶和安德烈亚斯·盖格(2021年)</figcaption></figure><p id="cabd" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">正如你已经知道的，如果你经常阅读我的文章，传统的GAN架构与编码器和解码器设置一起工作，就像这样。在训练期间，编码器接收图像，将其编码为压缩表示，解码器采用该表示来创建改变风格的新图像。对于我们在训练数据集中的所有图像，这被重复多次，以便编码器和解码器学习如何在训练期间最大化我们想要实现的任务的结果。一旦训练完成，你可以发送一个图像到编码器，它会做同样的过程，根据你的需要生成一个新的看不见的图像。无论任务是什么，它都将非常相似地工作，无论是将一张脸的图像转换成另一种风格，如卡通化器，还是从快速草稿中创建一个美丽的风景。仅使用解码器，我们也称之为生成器，因为它是负责创建新图像的模型，我们可以在这个编码信息空间中行走，并对我们发送给生成器的信息进行采样，以生成无限量的新图像。这个编码信息空间通常被称为潜在空间，我们用来生成新图像的信息是潜在代码。我们基本上在这个最佳空间内随机选择一些潜在的代码，它会根据我们想要完成的任务生成一个新的随机图像，当然是根据这个生成器的训练过程。这是难以置信的酷，但正如我刚才所说的，图像是完全随机的，我们没有或很少知道它会是什么样子，这对创作者来说已经没什么用了。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mo"><img src="../Images/217084719705b3ca346bf2b509e31c3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7rplyuac_2oVcBwh.gif"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">甘举例。迈克尔·尼迈耶和安德烈亚斯·盖格(2021年)</figcaption></figure><p id="5b48" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这就是他们用这篇论文攻击的问题。事实上，通过获取物体形状和外观的潜在代码并将其发送到解码器或生成器，他们能够控制物体的姿势，这意味着他们可以四处移动它们，改变它们的外观，添加其他物体，改变背景，甚至改变相机的姿势。所有这些变换都可以在每个对象或背景上独立完成，而不会影响图像中的任何其他内容！</p><div class="lt lu lv lw gt ab cb"><figure class="mp lx mq mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/5f6bcfb510b0b45a304884b1dbb3245f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/0*GLmykwVCGyULIDsK.gif"/></div></figure><figure class="mp lx mv mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/a8255eb09be16cd79d7eb4f6c74397cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*gBMtswnycB2Ymfht.gif"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk mw di mx my translated">左边是长颈鹿，右边是普通的2D甘。<a class="ae lr" href="https://m-niemeyer.github.io/project-pages/giraffe/index.html" rel="noopener ugc nofollow" target="_blank">迈克尔·尼迈耶和安德里亚斯·盖格(2021) </a></figcaption></figure></div><p id="2e8f" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">正如您所看到的，它比其他基于GAN的方法要好得多，其他基于GAN的方法通常无法将对象彼此分开，并且都受到特定对象修改的影响。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="ab gu cl mz"><img src="../Images/e162d85589802cc2687c44ce1ce5877a.png" data-original-src="https://miro.medium.com/v2/format:webp/0*bT0-fsf88s-F8XzU.png"/></div></a></figure><p id="c18b" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">他们的方法与众不同之处在于，他们在三维场景表示中攻击这个问题，就像我们如何看待现实世界一样，而不是像其他GANs那样停留在二维图像世界中。但除此之外，这个过程非常相似。他们对信息进行编码，识别物体，在潜在空间内对其进行编辑，并对其进行解码以生成新的图像。在这里，在这个潜在的空间里还有一些步骤要做。我们可以将此视为经典GAN图像合成网络与神经渲染器的组合，神经渲染器用于从发送到网络的图像生成3D场景，正如我们将看到的那样。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi na"><img src="../Images/6f301578fa7a4f68c3a5b29f18fee0a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/0*3wl2yuPqUcO41RB3.PNG"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">生成对象的单个三维表示。迈克尔·尼迈耶和安德烈亚斯·盖格(2021年)</figcaption></figure><p id="8518" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">实现这一目标有三个主要步骤。对输入图像进行编码后，意味着我们已经处于潜在空间中，第一步是将图像转换为3D场景。但不仅仅是简单的3D场景，而是由3D元素组成的3D场景，3D元素是对象和背景。这种将图像视为由生成的体渲染组成的场景的方式允许他们在生成的图像中改变相机角度并独立控制对象。这是通过使用一个类似的模型来实现的，我之前报道的论文中的<a class="ae lr" href="https://youtu.be/ZkaTyBvS2w4" rel="noopener ugc nofollow" target="_blank">叫做NERV </a>，但是他们不是使用一个单一的模型来从输入图像中生成整个锁定的场景，而是使用两个独立的模型来独立生成对象和背景。这里称为采样特征字段。这个网络的参数也是在训练期间学习的。我不会进入细节，但它非常类似于我在另一篇文章中提到的NERF。如果你想了解更多关于这类网络的细节，你可以观看<a class="ae lr" href="https://youtu.be/ZkaTyBvS2w4" rel="noopener ugc nofollow" target="_blank">这个关于NERV的视频</a>，它也链接在下面的参考文献中。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nb"><img src="../Images/267de99699ab1d3c6bf726c2aca8be2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K41kzWPxMs9MCNSh.PNG"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">3D表示中的姿态转换。迈克尔·尼迈耶和安德烈亚斯·盖格(2021年)</figcaption></figure><p id="d78c" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这个场景中的元素没有纠缠在一起，我们可以单独编辑它们，而不会影响图像的其他部分。这是第二步。他们可以对对象做任何他们想做的事情，比如改变它的位置和方向。<br/>换句话说，它们改变物体或背景的姿势。在这一点上，他们甚至可以添加新的对象放在他们想要的任何地方。然后，他们通过将所有的特征字段加在一起，简单地将它们组合成包含所有对象和背景的最终3D场景。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nc"><img src="../Images/d2b1981bc6678285ac9913c9f182d0ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y5B7L2YOwa_Ng_Mg.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">完整的潜在空间计算。迈克尔·尼迈耶和安德烈亚斯·盖格(2021年)</figcaption></figure><p id="2b24" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">最后，我们必须回到自然图像的2D世界。所以最后一步是把这个3D场景渲染成一个普通的图像。因为我们仍然在3D世界中，我们可以改变相机的视点来决定我们将如何看待这个场景。然后，我们根据相机光线和其他参数(如alpha值和透射率)来评估每个像素。这给了我们他们所谓的特征图像，但这个特征图像是由每个像素的特征向量组成的图像。由于我们仍然处于潜在空间，这些特征需要被转换成RGB颜色和高分辨率图像。这是通过使用典型的解码器来实现的，就像其他GAN架构一样，将其放大到原始尺寸，同时学习RGB通道转换功能。瞧，您的新图像对生成的内容有了更多的控制！</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nd"><img src="../Images/175afa81a0250702d02c3ec5ff9a300d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A85P-3_unBV_Pk7j.PNG"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">编码器编码后的完整网络。<a class="ae lr" href="https://m-niemeyer.github.io/project-pages/giraffe/index.html" rel="noopener ugc nofollow" target="_blank">迈克尔·尼迈耶和安德里亚斯·盖格(2021) </a></figcaption></figure><p id="8c56" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">当然，正如您所看到的，当用于真实世界的数据时，它仍然不是完美的。尽管如此，它仍然令人印象深刻，是朝着正确方向迈出的重要一步，特别是考虑到这些是完全由GANs生成的合成图像，并且它是第一篇能够以这种精度控制生成图像的论文。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/a9777af8c58b5a7792cbc0d568532a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/1*SPr8RQYw7rFL3xATFIIttw.gif"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">瑕疵示例。<a class="ae lr" href="https://m-niemeyer.github.io/project-pages/giraffe/index.html" rel="noopener ugc nofollow" target="_blank">迈克尔·尼迈耶和安德里亚斯·盖格(2021) </a></figcaption></figure><p id="d4f0" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这篇论文真的很有趣，我推荐阅读它，以了解他们的模型是如何工作的。祝贺Michael Niemeyer和Andreas Geiger获得他们当之无愧的最佳论文奖。他们还在GitHub上发布了代码，如果你想玩的话。链接在下面的参考资料中</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="https://www.louisbouchard.ai/learnai/"><div class="gh gi nf"><img src="../Images/d6d4f598ae72cf7f2fb082a3e0a0d220.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*fqRa4yjYoXrzncvQ.png"/></div></a></figure><p id="dd93" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">感谢您的阅读！</p><h2 id="a186" class="ng nh it bd ni nj nk dn nl nm nn dp no me np nq nr mf ns nt nu mg nv nw nx iz bi translated">观看视频</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ny mi l"/></div></figure><p id="d7e5" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">来我们的<a class="ae lr" href="https://discord.gg/learnaitogether" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> Discord社区和我们聊天:</strong> <strong class="kx jd">一起学习AI</strong></a>和<em class="kw">分享你的项目、论文、最佳课程、寻找Kaggle队友等等！</em></p><p id="cbb7" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank">T5】简讯 </a>！</p><h1 id="a410" class="nz nh it bd ni oa ob oc nl od oe of no ki og kj nr kl oh km nu ko oi kp nx oj bi translated">支持我:</h1><ul class=""><li id="d99a" class="ok ol it kx b ky om lb on me oo mf op mg oq lq or os ot ou bi translated">支持我的最好方式是成为这个网站<strong class="kx jd"> </strong>的成员，或者如果你喜欢视频格式，在<a class="ae lr" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"><strong class="kx jd">YouTube</strong></a><strong class="kx jd"/>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="63a4" class="ok ol it kx b ky ov lb ow me ox mf oy mg oz lq or os ot ou bi translated">在经济上支持我在T21的工作</li><li id="8d21" class="ok ol it kx b ky ov lb ow me ox mf oy mg oz lq or os ot ou bi translated">跟我来这里上<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a></li></ul><h1 id="3007" class="nz nh it bd ni oa ob oc nl od oe of no ki og kj nr kl oh km nu ko oi kp nx oj bi translated">参考</h1><ul class=""><li id="9da1" class="ok ol it kx b ky om lb on me oo mf op mg oq lq or os ot ou bi translated">Michael Niemeyer和Andreas Geiger(2021)，“长颈鹿:将场景表示为合成生成神经特征场”，发表于CVPR 2021年。</li><li id="b2d9" class="ok ol it kx b ky ov lb ow me ox mf oy mg oz lq or os ot ou bi translated">论文等项目链接:<a class="ae lr" href="https://m-niemeyer.github.io/project-pages/giraffe/index.html" rel="noopener ugc nofollow" target="_blank">https://m-nie Meyer . github . io/project-pages/长颈鹿/index.html </a></li><li id="5920" class="ok ol it kx b ky ov lb ow me ox mf oy mg oz lq or os ot ou bi translated">代号:<a class="ae lr" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">https://github.com/autonomousvision/giraffe</a></li><li id="0156" class="ok ol it kx b ky ov lb ow me ox mf oy mg oz lq or os ot ou bi translated">NERF视频:<a class="ae lr" href="https://youtu.be/ZkaTyBvS2w4" rel="noopener ugc nofollow" target="_blank">https://youtu.be/ZkaTyBvS2w4</a></li></ul></div></div>    
</body>
</html>