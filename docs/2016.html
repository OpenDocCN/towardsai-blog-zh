<html>
<head>
<title>Decision Trees vs. Random Forests in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的决策树与随机森林</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/decision-trees-vs-random-forests-in-machine-learning-be56c093b0f?source=collection_archive---------0-----------------------#2021-07-24">https://pub.towardsai.net/decision-trees-vs-random-forests-in-machine-learning-be56c093b0f?source=collection_archive---------0-----------------------#2021-07-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="efe3" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="9a4d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">理解两种监督学习算法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/60d924446663c8f6ce3e0f4d95120dd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nR0ImIoPaj7gVBHr"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@dietmarbecker?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">迪特玛·贝克尔</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="601c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">众所周知，机器学习是人工智能的子集，允许用户训练模型并预测商业消费的数据。我们用训练数据集训练机器；数据不必总是被标记为1，即没有指定的特征。这是无监督的ML模型进入画面的时候。他们在未标记的数据中工作。有监督的工作在有标签的数据上，无监督的工作在无标签的数据上。</p><p id="b33c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们现在对什么是机器学习及其类型有了一个大致的了解。现在，在本文中，让我们进一步尝试理解两种监督学习算法决策树、随机森林，以及它们之间的不同点。</p><blockquote class="me mf mg"><p id="40fc" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">决策树</em> </strong></p></blockquote><p id="b2b6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">顾名思义，它是一个树状结构，主要用于条件控制语句中的决策。</p><p id="0b8c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是一个决策树的图示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/aea4e2eddc2426e46f00dafb4e80e67a.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*Wo7p6QEEd4tog973cEwbOQ.png"/></div></figure><p id="1f72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">机器学习中的决策树:</strong></p><p id="1298" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策树是一种受监督的机器学习模型，用于分类和回归。这里的数据是按照一定的规则或参数连续分割的。每个分支代表一个测试结果，每个叶节点持有一个标签。数据分割从根节点开始，在叶节点结束。所以，他们总是遵循自上而下的方法。在我们继续之前，让我们学习一些新的术语。</p><ul class=""><li id="a19b" class="mm mn it lk b ll lm lo lp lr mo lv mp lz mq md mr ms mt mu bi translated"><strong class="lk jd">根节点</strong>:代表整个群体。这是第一个节点。</li><li id="9d01" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated"><strong class="lk jd">分割:</strong>根据参数分割样本的过程。</li><li id="7a5b" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated"><strong class="lk jd">决策节点:</strong>节点分裂成更多的子节点。</li><li id="b388" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated"><strong class="lk jd">叶节点:</strong>最后一级节点或输出标签。</li><li id="c099" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated"><strong class="lk jd">修剪:</strong>与劈树相反，是为了缩小树的尺寸。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/b99e24bb597bb6bda182bc8bfac5035c.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*DitQpOJh4xhjQd1iNYSnog.png"/></div></figure><p id="767c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">数据拆分的依据是什么？</strong></p><p id="66aa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用基尼指数、熵和信息增益等方法将决策树中的数据分成节点和子节点。</p><p id="6213" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">ID3代表迭代二分法3。它是一种分类算法，选择产生最大信息增益或最小熵的最佳属性。</p><p id="4485" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">同样，CART算法主要使用基尼指数来选择最佳属性。</p><p id="14f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">熵:</strong></p><p id="bab5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用蹩脚的话来说，熵被定义为一种用于发现错误或杂质的度量。而是数据点的随机性。如果所有的数据点都属于同一个类别，它们被称为“纯的”,否则被称为“不纯的”。</p><p id="2e59" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">熵的范围从0到1。在某些情况下，它也可能不止一个，意味着数据更加不纯。它用“H”表示</p><p id="cb0f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，如果数据集包含同质的观测值子集，则数据集中不存在杂质或随机性，如果所有观测值都属于一个类，则该数据集的熵为零。</p><p id="bf08" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">信息增益:</strong></p><p id="c595" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">熵是寻找或度量信息增益的基础。它被称为熵的变化。信息增益用于确定最佳值，从而为我们提供关于数据的最大信息。</p><p id="7edf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">具有最大信息增益的特征应该被用作决策树的根节点。它用“IG”表示，并且随着树到达其末端，信息节点减少。</p><p id="4b43" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="mh">信息增益=分裂前的熵—分裂后的熵</em> </strong></p><p id="a911" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">基尼指数:</strong></p><p id="c67a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基尼指数主要用于经典的CART算法。CART的意思是分类和回归树，它解释了如何根据其他值预测结果变量的值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/fcebfee08adbc51155c893af003d1323.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*V99fCPtmyAKBMNLjdXWmvQ.png"/></div></figure><p id="39ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它适用于分类变量。结果不是“成功”就是“失败”，所以它只进行二进制拆分。基尼系数的程度也从0到1不等。</p><ul class=""><li id="dc28" class="mm mn it lk b ll lm lo lp lr mo lv mp lz mq md mr ms mt mu bi translated">如果为0，则数据分布在同一个类中。</li><li id="c69e" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated">如果为1，则数据是随机分布的。</li><li id="e141" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated">0.5表示数据均匀分布。</li></ul><p id="a4d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从1中减去概率总和的计算非常简单。</p><p id="c2ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面给出了一个说明决策树的例子。</p><p id="79d8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们以冰淇淋数据集为例，应用决策树算法。数据包含以下列。</p><p id="6533" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">用python代码实现决策树:</strong></p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="38df" class="nh ni it nd b gy nj nk l nl nm">#importing all the libraries needed for the process<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>%inline matplotlib</span></pre><p id="22bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用熊猫将csv文件读入冰激凌。查看数据集的前10个值</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="a725" class="nh ni it nd b gy nj nk l nl nm">ice_cream=pd.read_csv("IceCreamData.csv")<br/>ice_cream.head(10)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6e9705526a98fc28791a38caefde528f.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*x-i6IkauGDQxHbmmyAbR4Q.png"/></div></figure><p id="525d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用displot了解数据集中收入列的分布</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="e3fd" class="nh ni it nd b gy nj nk l nl nm">sns.displot(x='Revenue',data=ice_cream,kde=True)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/97201d15e13dee24215300e5f50952f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*12wQ90FLQezeab4EE1tjiQ.png"/></div></figure><p id="9eac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用散点图了解数据集中温度和收入之间的关系</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="8d14" class="nh ni it nd b gy nj nk l nl nm">plt.scatter(x='Temperature',y='Revenue',data=ice_cream)<br/>plt.title("Relation between Temp and rev")<br/>plt.xlabel('Temp')<br/>plt.ylabel('rev')<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/63b733bd43decedda612e4d98f6c9cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*fzyZV0d3pmjbGGvCQWhWig.png"/></div></figure><p id="b88b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">气温和收益正相关。随着温度的升高，收益也随之增加。因此，很明显，温度是独立的，收入取决于温度。将数据分为X和Y，分别独立和相关。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="bc97" class="nh ni it nd b gy nj nk l nl nm">X=pd.DataFrame(ice_cream['Temperature'])<br/>Y=pd.DataFrame(ice_cream['Revenue'])</span><span id="dffa" class="nh ni it nd b gy nq nk l nl nm">#importing train_test_split method from sklearn<br/>from sklearn.model_selection import train_test_split<br/>X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.1,<br/>                                               random_state=1)</span><span id="4127" class="nh ni it nd b gy nq nk l nl nm">#importing the DecisionTreeRegressor from the sklearn lib<br/>from sklearn.tree import DecisionTreeRegressor<br/>regressor = DecisionTreeRegressor(random_state=0)<br/>regressor.fit(X_train, Y_train)</span><span id="9394" class="nh ni it nd b gy nq nk l nl nm">#output:<br/>Training score:1.0<br/>Testing score:0.9739054111493022</span><span id="ba5b" class="nh ni it nd b gy nq nk l nl nm">#Predicting the values<br/>y_pred=regressor.predict(X_test)<br/>x_pred=regressor.predict(X_train)</span><span id="fdc4" class="nh ni it nd b gy nq nk l nl nm">#Evaluating the model based on the metrics<br/>from sklearn import metrics<br/>print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test,<br/>                                                         y_pred))</span><span id="499f" class="nh ni it nd b gy nq nk l nl nm">print('Root Mean Squared Error of train dataset:',<br/>             np.sqrt(metrics.mean_squared_error(Y_train,x_pred)))</span><span id="f5dd" class="nh ni it nd b gy nq nk l nl nm">print('Root Mean Squared Error of test dataset:', <br/>              np.sqrt(metrics.mean_squared_error(Y_test, y_pred)))</span><span id="c057" class="nh ni it nd b gy nq nk l nl nm">#output:<br/>Mean Absolute Error: 21.89481194999999<br/>Root Mean Squared Error of train dataset: 0.0<br/>Root Mean Squared Error of test dataset: 27.149437063596295</span></pre><p id="5280" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里平均绝对误差是21.9，即实际数据和预测数据之间的平均距离是21.9。预测数据的rmse值为27.1，这意味着实际数据与预测数据相差(大约)27.1个单位。</p><p id="7e3b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策树中的标准参数将基尼指数作为默认值。即<strong class="lk jd"> <em class="mh">【判据:{"gini "，"熵" }，默认=" gini "】。</em>T13】</strong></p><p id="d351" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">优点:</strong></p><ul class=""><li id="af6d" class="mm mn it lk b ll lm lo lp lr mo lv mp lz mq md mr ms mt mu bi translated">决策树不需要数据的标准化和缩放。</li><li id="b9fb" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated">它对丢失的值是健壮的。</li></ul><p id="531d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">缺点:</strong></p><ul class=""><li id="5ee4" class="mm mn it lk b ll lm lo lp lr mo lv mp lz mq md mr ms mt mu bi translated">决策树通常需要更多时间来训练数据。</li><li id="f152" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated">数据的微小变化会导致决策树结构的巨大变化，从而导致不稳定。</li><li id="aeaa" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated">倾向于过度拟合数据。</li></ul><p id="2711" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了克服数据的复杂性和不稳定性，我们使用随机森林分类器。现在，让我们了解什么是随机森林，以及它如何帮助减少上述问题。</p><div class="nr ns gp gr nt nu"><a rel="noopener  ugc nofollow" target="_blank" href="/fully-explained-decision-tree-classification-with-python-d90d3bd16836"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd jd gy z fp nz fr fs oa fu fw jc bi translated">用Python全面解释决策树分类</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">分类问题决策树的深入研究</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">pub.towardsai.net</p></div></div><div class="od l"><div class="oe l of og oh od oi lb nu"/></div></div></a></div><blockquote class="me mf mg"><p id="47d1" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">随机森林</em> </strong></p></blockquote><p id="b80f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随机森林是一种集成机器学习技术，它添加了几个决策树，目的是克服单个决策树的过拟合问题。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/1492f9f66d3cf1093550a6e653d622ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*oy8TdWzRXCmEXc0fEvRvDw.png"/></div></figure><p id="ee59" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随机森林是用于分类和回归问题的最常用的机器学习算法之一。但是在分类模型上表现良好。</p><p id="f037" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随机森林是另一种类型的集成学习。集成学习将几个决策树结合起来，比利用单个决策树产生更好的预测性能。集合模型背后的主要原理是一群弱学习者作为强学习者聚集在一起。</p><p id="7935" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随机森林增加了模型的随机性。分割节点时，它会在随机特征子集中搜索最佳特征。这导致了多样性，而多样性通常会导致更好的模型。</p><p id="22b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">功能重要性:</strong></p><p id="314b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mh">特征重要性</em>指的是根据输入特征在预测目标变量时的有用程度对其进行评分的技术</p><p id="a5e4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Sklearn提供了一些方法来衡量一个特性的重要性，方法是查看该特性在森林中的所有树中减少了多少杂质。它在训练后自动计算每个特征的分数，并试图使结果相等，使得所有重要性的总和等于1。</p><p id="b83a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">用python代码实现随机森林:</strong></p><p id="904f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">6.5级的应聘者之前的工资是16万。为了在我们的新公司雇用他，我们想确认他是否对他的上一份薪水诚实，我们将使用随机森林来预测这一点。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="dc33" class="nh ni it nd b gy nj nk l nl nm">#importing all the libraries needed for the process<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>%inline matplolib</span><span id="5e3a" class="nh ni it nd b gy nq nk l nl nm">#reading the csv data using pandas to salary<br/>salary=pd.read_csv("Position_Salaries.csv")<br/>salary</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/2a3950bb2d86986aad5d9de9abd13e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*O4qdplSptl6Kgnd2wUHtYQ.png"/></div></figure><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="a1ac" class="nh ni it nd b gy nj nk l nl nm">#Using the scatterplot to find the relation between level and salary<br/>sns.scatterplot(data=salary,x='Level',y='Salary')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a8a6ae92daee64e5a066dbe13cf991ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*8v1oJxeo_JMe8QC_v3Z4tg.png"/></div></figure><p id="317e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">变量之间有一个多项式关系，即如果级别增加，工资也会增加</p><p id="c46d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很明显，工资是自变量，水平是因变量。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="0a5c" class="nh ni it nd b gy nj nk l nl nm">#Now dividing the X and Y based on the variables.<br/>X=salary.iloc[:,1].values<br/>Y=salary.iloc[:,-1].values</span><span id="f84c" class="nh ni it nd b gy nq nk l nl nm">#importing the train_test_split method from sklearn<br/>from sklearn.model_selection import train_test_split</span><span id="2112" class="nh ni it nd b gy nq nk l nl nm">X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,<br/>                                               random_state=0)</span><span id="eaf3" class="nh ni it nd b gy nq nk l nl nm">#reshaping the train and test dataset to minimize the value error in the further process.</span><span id="c48c" class="nh ni it nd b gy nq nk l nl nm">X_train=np.array(X_train).reshape(-1, 1)<br/>Y_train=np.array(Y_train).reshape(-1, 1)<br/>X_test=np.array(X_test).reshape(-1, 1)<br/>Y_test=np.array(Y_test).reshape(-1, 1)</span><span id="66ed" class="nh ni it nd b gy nq nk l nl nm">#importing the Random tree regressor form the sklearn.ensemble<br/>from sklearn.ensemble import RandomForestRegressor<br/>rfr=RandomForestRegressor(n_estimators=100,random_state=1)<br/>rfr.fit(X_train,Y_train.ravel())</span></pre><p id="dbda" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们应该总是在测试分数大于训练数据集的数据集上工作。为了得到要求的分数，我们应该改变n估计量、测试规模和随机状态。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="3ecd" class="nh ni it nd b gy nj nk l nl nm">print("Training dataset Score:",rfr.score(X_train,Y_train))<br/>print("Testing dataset Score:",rfr.score(X_test,Y_test))</span><span id="01b3" class="nh ni it nd b gy nq nk l nl nm">#output:<br/>Training dataset Score: 0.883197840979863<br/>Testing dataset Score: 0.9909168388429752</span><span id="883d" class="nh ni it nd b gy nq nk l nl nm">#Predicting the values<br/>x_pred=rfr.predict(X_train)<br/>y_pred=rfr.predict(X_test)</span><span id="15f0" class="nh ni it nd b gy nq nk l nl nm"><br/>from sklearn import metrics<br/>print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test,<br/>                                                         y_pred))</span><span id="da66" class="nh ni it nd b gy nq nk l nl nm">print('Root Mean Squared Error of train dataset:', <br/>          np.sqrt(metrics.mean_squared_error(Y_train, x_pred)))</span><span id="82a5" class="nh ni it nd b gy nq nk l nl nm">print('Root Mean Squared Error of test:', <br/>           np.sqrt(metrics.mean_squared_error(Y_test, y_pred)))</span><span id="1c55" class="nh ni it nd b gy nq nk l nl nm">#output:<br/>Mean Absolute Error: 16250.0<br/>Root Mean Squared Error of train dataset: 101634.8626825461<br/>Root Mean Squared Error of test: 20967.236346261754</span></pre><p id="791c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里平均绝对误差是16250，即实际数据和预测数据之间的平均距离是16250。预测数据的rmse值为20967，这意味着实际数据与预测数据相差(大约)指定的单位。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="381f" class="nh ni it nd b gy nj nk l nl nm">#Now predicting if he is being honest about his last salary by predicting the sal for the level 6.5</span><span id="cfe4" class="nh ni it nd b gy nq nk l nl nm">new_val= np.array(6.5).reshape(-1,1)<br/>our_pred=rfr.predict(new_val)<br/>our_pred</span><span id="0c47" class="nh ni it nd b gy nq nk l nl nm">#output:<br/>array([164100.])</span></pre><p id="89cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">6.5级的候选人之前的工资是160000英镑。在这里，我们预测它是164100，这是大约相同的价值，所以我们得出结论，他是诚实的关于他的最后工资。</p><p id="f841" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">优点:</strong></p><ul class=""><li id="7ec1" class="mm mn it lk b ll lm lo lp lr mo lv mp lz mq md mr ms mt mu bi translated">因为随机森林减少了过拟合标准，所以提高了精确度。</li><li id="ff14" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated">它非常有用，适用于回归和分类。</li><li id="7d06" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated">要素是分类值还是连续值并不重要，两者都适用。</li></ul><p id="ac86" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">缺点:</strong></p><ul class=""><li id="146e" class="mm mn it lk b ll lm lo lp lr mo lv mp lz mq md mr ms mt mu bi translated">它需要大量的计算能力，因为它构建了大量的树来组合它们的输出。</li><li id="bf1f" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated">它还需要更多的训练时间，因为它结合了许多决策树来确定类别</li><li id="10fd" class="mm mn it lk b ll mv lo mw lr mx lv my lz mz md mr ms mt mu bi translated">它无法确定它使用集成学习的每个变量的重要性。</li></ul><div class="nr ns gp gr nt nu"><a rel="noopener  ugc nofollow" target="_blank" href="/fully-explained-ensemble-techniques-example-with-python-b83e50310841"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd jd gy z fp nz fr fs oa fu fw jc bi translated">用Python完整解释了整体技术示例</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">基于几种决策树的机器学习方法</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">pub.towardsai.net</p></div></div><div class="od l"><div class="om l of og oh od oi lb nu"/></div></div></a></div><p id="fdeb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">决策树和随机森林的区别:</strong></p><p id="eec7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与随机森林相比，决策树很容易。决策树结合了决策，但是随机森林结合了几个决策树。所以，这是一个漫长而缓慢的过程。</p><p id="5d11" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策树速度很快，在大型数据集上操作简单。随机森林模型需要严格的训练。根据不同的需求，如果你有更少的时间来处理一个模型，你一定会选择决策树。然而，稳定性和可靠的预测在随机森林中更准确。</p><p id="8d3f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">结论:</strong></p><p id="a7ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">希望你对有监督的机器学习模型有所了解，这些模型几乎是紧密工作的，决策树和随机森林。它们的重要性、优点、缺点以及最重要的区别。</p><p id="7b29" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我感谢Dhanasree在这篇文章中帮助我。</p></div><div class="ab cl op oq hx or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="im in io ip iq"><p id="c0dc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae lh" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="43b5" class="ow ni it bd ox oy oz pa pb pc pd pe pf ki pg kj ph kl pi km pj ko pk kp pl pm bi translated">推荐文章</h1><p id="0803" class="pw-post-body-paragraph li lj it lk b ll pn kd ln lo po kg lq lr pp lt lu lv pq lx ly lz pr mb mc md im bi translated"><a class="ae lh" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄与Python </a> <br/> 2。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a>T5】3 .<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/deep-learning-88e218b74a14?source=friends_link&amp;sk=540bf9088d31859d50dbddab7524ba35">为什么LSTM在深度学习方面比RNN更有用？</a> <br/> 5。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88?source=friends_link&amp;sk=6844935e3de14e478ce00f0b22e419eb">神经网络:递归神经网络的兴起</a> <br/> 6。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/differences-between-concat-merge-and-join-with-python-1a6541abc08d?source=friends_link&amp;sk=3b37b694fb90db16275059ea752fc16a">concat()、merge()和join()与Python </a> <br/>的区别9。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a> <br/> 10。<a class="ae lh" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>