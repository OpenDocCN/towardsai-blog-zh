<html>
<head>
<title>Model-Based Meta Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于模型的元强化学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/model-based-meta-reinforcement-learning-71f0e1b52593?source=collection_archive---------0-----------------------#2019-09-16">https://pub.towardsai.net/model-based-meta-reinforcement-learning-71f0e1b52593?source=collection_archive---------0-----------------------#2019-09-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="fbad" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">使用基于模型的Meta-RL | <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">实现对AI </a>的快速适应</h2><div class=""/><div class=""><h2 id="57ad" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">深入研究基于模型的meta-RL算法，实现快速自适应</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/173d26a5fc172a2e83d7edfeecf3cdc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qwpU-0hSadxqZr9U8AXBUA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来自Pixabay的mrthoif0</figcaption></figure><h1 id="2c57" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">介绍</h1><p id="b0ca" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">在<a class="ae mv" href="https://medium.com/towards-artificial-intelligence/pearl-probabilistic-embeddings-for-actor-critic-rl-42a5cbe05bfb?source=friends_link&amp;sk=27f49e0ec95092933d99bdb6997f9619" rel="noopener">的上一篇文章</a>中，关于无模型的meta-RL已经洒了很多墨水。在本文中，我们提出了一个基于模型的meta-RL框架，该框架由Nagabandi &amp; Clavera等人提出，能够快速适应环境动态的变化。长话短说，这种方法学习一个能快速适应环境变化的动态模型，并利用这个模型做模型预测控制(<a class="ae mv" href="https://en.wikipedia.org/wiki/Model_predictive_control#targetText=Model%20predictive%20control%20(MPC)%20is,oil%20refineries%20since%20the%201980s." rel="noopener ugc nofollow" target="_blank"> MPC </a>)采取行动。值得注意的是，学习的动态模型的自适应性质不仅在元学习中特别重要，而且在基于模型的RL中也特别重要，因为它减轻了对全局精确模型的要求，这在基于模型的RL中起着重要作用。</p><h1 id="328d" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">环境假设</h1><p id="1403" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">我们将环境分布<em class="mw"> ɛ </em>定义为<em class="mw">𝜌(ɛ</em>。我们放弃了MAML-RL采用的情节框架，在该框架中，任务被预先定义为不同的奖励或环境，并且任务只存在于轨迹层面。相反，我们认为每个时间步都可能是一个新的“任务”，其中任何细节或设置都可能在任何时间步发生变化。例如，一个真正的有腿毫机器人在向前移动时意外地失去了一条腿，如下图所示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mx"><img src="../Images/05ddcfe9916a73d496cfe30369a23c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6V3Ch2zMt0RJJAOxu5fEQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">一个真正有腿的毫机器人在第二帧意外地失去了一条腿。来源:通过元强化学习学习适应动态的真实世界环境</figcaption></figure><p id="5ffe" class="pw-post-body-paragraph lz ma it mb b mc my kd me mf mz kg mh mi na mk ml mm nb mo mp mq nc ms mt mu im bi translated">我们进一步假设环境是局部一致的，因为长度为<em class="mw"> i-j </em>的每个片段具有相同的环境。尽管这种假设并不总是正确的，但它允许我们在不知道环境何时发生变化的情况下，学会根据数据进行调整。由于适应的快速本质(不到一秒)，这个假设很少被违反。</p><h1 id="bc87" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">问题设置</h1><p id="56f6" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">我们将适应问题公式化为优化𝜓𝜃的学习过程的参数，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/e3879c1f6c10e16f2792a38804ba1295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qisDyzeOjT03-Te_G6cDlw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">情商。(1)适应问题的客观方面，如果你对这个客观方面感到不舒服，请查看我们<a class="ae mv" href="https://medium.com/towards-artificial-intelligence/how-to-train-maml-model-agnostic-meta-learning-90aa093f8e46" rel="noopener">以前关于MAML的文章</a></figcaption></figure><p id="ea04" class="pw-post-body-paragraph lz ma it mb b mc my kd me mf mz kg mh mi na mk ml mm nb mo mp mq nc ms mt mu im bi translated">其中<em class="mw"> 𝜏_ɛ(t-M，t+K) </em>对应于从以前的经验中采样的轨迹段，<em class="mw"> u_𝜓 </em>是我们后面要定义的适应过程，<em class="mw"> L </em>表示动力学损失函数，它是状态变化的均方误差(见官方实现<a class="ae mv" href="https://github.com/iclavera/learning_to_adapt/blob/bd7d99ba402521c96631e7d09714128f549db0f1/learning_to_adapt/dynamics/meta_mlp_dynamics.py#L133" rel="noopener ugc nofollow" target="_blank">此处</a>):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/4083e271911f8b9f817f1c74afe2e802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ss3oWq5b7WVhOHufPazMkw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">动态模型的损失函数</figcaption></figure><p id="3e51" class="pw-post-body-paragraph lz ma it mb b mc my kd me mf mz kg mh mi na mk ml mm nb mo mp mq nc ms mt mu im bi translated">直观来说，通过优化Eq。(1)，我们期望代理在根据过去的m个变迁适应模型之后，在接下来的k个步骤中做得很好(<em class="mw"> L(𝜏_ɛ(t，t+K))被最小化</em>。认识到什么是情商至关重要。(1)是为了理解这个算法而做的。如果你仍然对它感到不舒服，请在继续之前阅读本文的<a class="ae mv" href="https://medium.com/towards-artificial-intelligence/how-to-train-maml-model-agnostic-meta-learning-90aa093f8e46?source=friends_link&amp;sk=7352308d22be5367cffc17377a0e3b1e" rel="noopener">MAML部分。</a></p><p id="9eb7" class="pw-post-body-paragraph lz ma it mb b mc my kd me mf mz kg mh mi na mk ml mm nb mo mp mq nc ms mt mu im bi translated">此外，请注意，在等式。(1)我们将所有数据放在单个数据集𝐷中，而不是为每个任务维护一个数据集，因为我们想要进行快速适应，而不是这里的轨迹级元学习。</p><h1 id="a949" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">基于模型的元强化学习</h1><p id="edd9" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">Nagabandi&amp;Clavera等人介绍了两种求解方程的方法。(1).一种是基于梯度的元学习，另一种是基于递归模型。两者共享相同的框架，仅在网络架构和优化程序上有所不同。事实上，由于它们正交地强调了框架的不同部分，它们最终可能被组合起来形成一个更强大的方法。</p><h2 id="12dc" class="nf li it bd lj ng nh dn ln ni nj dp lr mi nk nl lt mm nm nn lv mq no np lx iz bi translated">基于梯度的自适应学习器</h2><p id="bcf2" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">基于梯度的自适应学习器(GrBAL)使用基于梯度的元学习来执行在线自适应；更新规则由梯度下降规定:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/4866d2ff6a07791320fa06aabed5b27c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r3bA163V5wPmiVBe723LyQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">动态模型的适应步骤</figcaption></figure><p id="46ca" class="pw-post-body-paragraph lz ma it mb b mc my kd me mf mz kg mh mi na mk ml mm nb mo mp mq nc ms mt mu im bi translated">这里，可学习参数𝜓表示自适应时的步长。这种方法与非常相似，只是改进了李等人[4]提出的学习步长的方法。</p><h2 id="665d" class="nf li it bd lj ng nh dn ln ni nj dp lr mi nk nl lt mm nm nn lv mq no np lx iz bi translated">基于递归的自适应学习器</h2><p id="eb54" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">基于递归的自适应学习器(ReBAL)利用递归模型，通过其内部结构学习自己的更新规则。在这种情况下，𝜓和u_𝜓对应于更新其隐藏状态的递归模型的权重。更多信息，请参考<a class="ae mv" href="https://arxiv.org/abs/1611.02779" rel="noopener ugc nofollow" target="_blank"> RL </a>。</p><h1 id="b1d5" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">算法</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/94071bf24eea27d178e2bc56632a0c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WNrhqKZaesRggU0OyuoQsQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">算法伪代码。来源:通过元强化学习学习适应动态的真实世界环境</figcaption></figure><p id="1fce" class="pw-post-body-paragraph lz ma it mb b mc my kd me mf mz kg mh mi na mk ml mm nb mo mp mq nc ms mt mu im bi translated">如果你已经熟悉模型不可知的元学习(<a class="ae mv" href="https://medium.com/towards-artificial-intelligence/how-to-train-maml-model-agnostic-meta-learning-90aa093f8e46?source=friends_link&amp;sk=7352308d22be5367cffc17377a0e3b1e" rel="noopener"> MAML </a>)和模型预测控制(MPC)，这里没有什么新的东西。我们通过算法1(第8–14行)学习自适应动态模型，然后在每个环境步骤中，代理首先调整模型(算法2中的第3行)，然后执行MPC(算法2中的第4行)以采取算法2中所示的行动。</p><h1 id="6e6a" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">实验结果</h1><p id="7139" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">以下视频演示了该算法的有效性</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ns nt l"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">实验结果。来源:https://sites.google.com/berkeley.edu/metaadaptivecontrol<a class="ae mv" href="https://sites.google.com/berkeley.edu/metaadaptivecontrol" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><h1 id="35fa" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">参考</h1><ol class=""><li id="083e" class="nu nv it mb b mc md mf mg mi nw mm nx mq ny mu nz oa ob oc bi translated">A.Nagabandi，I. Clavera，S. Liu，R. S. Fearing，P. Abbeel，S. Levine和C. Finn，“通过元强化学习学习适应动态的真实世界环境”，<em class="mw"/>，第1–17页，2019年。</li><li id="fa6c" class="nu nv it mb b mc od mf oe mi of mm og mq oh mu nz oa ob oc bi translated">C.Finn，P. Abbeel和S. Levine，“用于深度网络快速适应的模型不可知元学习”，<em class="mw">第34期。糖膏剂马赫。学习。ICML 2017 </em>，第3卷，第1856–1868页，2017。</li><li id="7865" class="nu nv it mb b mc od mf oe mi of mm og mq oh mu nz oa ob oc bi translated">Y.段、j .舒尔曼、x .陈、P. L. Bartlett、I. Sutskever、P. Abbeel，“RL:通过慢速强化学习实现快速强化学习”，<em class="mw">、</em>，第1–14页，2017。</li><li id="fdb1" class="nu nv it mb b mc od mf oe mi of mm og mq oh mu nz oa ob oc bi translated">Z.李，周芳芳，陈芳芳，李。Meta-SGD:学会快速学习</li></ol></div></div>    
</body>
</html>