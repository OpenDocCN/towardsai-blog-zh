<html>
<head>
<title>Let AI Write Stories for You!! (Using GPT-J)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让艾给你写故事！！(使用GPT J)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/let-ai-write-stories-for-you-using-gpt-j-fc71b414daa8?source=collection_archive---------0-----------------------#2021-12-15">https://pub.towardsai.net/let-ai-write-stories-for-you-using-gpt-j-fc71b414daa8?source=collection_archive---------0-----------------------#2021-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7b8d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><p id="5f25" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><em class="kx">GPT模型家族不断提高生成文本的质量，以至于他们现在被用来写博客。</em></p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi ky"><img src="../Images/2a37a64cc861d44e3d970f10c573af8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1uOAtJKAQWywGx08"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">照片由<a class="ae lo" href="https://unsplash.com/@aaronburden?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚伦·伯顿</a>在<a class="ae lo" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="4f29" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">人工智能(AI)的进步让我们的许多任务变得更容易、更易于管理。也许是时候使用人工智能来为我们写博客了*？有很多在线服务可以分析你的写作，给你反馈如何改进。这些服务中的大部分集中在检测一个句子是否有意义。他们中的大多数人也关注于鉴别作者的语法是否正确。</p><p id="4954" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">此外，我知道一些初创公司试图使用1750亿美元的GPT-3模型来做文案，并能够获得大量资金！但是，这有可能吗？回到上一段，在第二句的末尾找到开头。在提到的星星之后出现的一切都是来自生成模型的不变输出。我认为这些句子的质量相当好，它们甚至相互关联而不重复。</p><p id="54aa" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">我用的是什么型号？— </strong>谈到预先训练的生成模型，有两个主要的竞争者:GPT-3 [1]和GPT-J [2]！它们都是基于变压器的模型。GPT J要小得多，只有大约60亿个参数。这两者的主要区别是他们的许可证。虽然你必须为每个令牌支付OpenAI才能使用GPT-3，但你可以免费使用由<a class="ae lo" href="https://www.eleuther.ai/" rel="noopener ugc nofollow" target="_blank"> EleutherAI </a>集团发布的GPT-J模型，并且可以在Huggingface图书馆上获得，这是一个巨大的优势。他们的表现如何？自从GPT-3测试版发布以来，我就一直在玩它，并试图对它们进行比较。GPT-3可能在生成任务上稍好一点，但整体文本质量非常接近！</p><blockquote class="lp lq lr"><p id="9dbd" class="jz ka kx kb b kc kd ke kf kg kh ki kj ls kl km kn lt kp kq kr lu kt ku kv kw im bi translated">⚠️:我不想展示GPT-3的例子，因为它们有严格的指导原则。</p></blockquote><p id="8496" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">那么，让我们和GPT J一起去吧！什么阻止了我们？— 嗯，模特的尺寸！该模型的检查点大小约为25GB……要么使用您的PC，要么使用免费的在线资源(如Google Colab、Kaggle或SageMaker Lab)。必须有至少45GB(或者34GB，如果你使用float16半精度模型)的可用RAM来使用Huggingface库加载模型，除非你有足够的RAM来使用CPU或房间里的巨型GPU。假设您需要支付一些费用来访问这样的资源(从亚马逊或谷歌云服务)是安全的。所以，它可能终究不是免费的！</p><h1 id="5585" class="lv lw it bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">如何使用模型？</h1><p id="e05b" class="pw-post-body-paragraph jz ka it kb b kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw im bi translated">Hugginface库可以轻松加载预训练的模型，并使用它来生成文本。在加载模型及其标记器之后，您将需要向模型传递一个提示，并使用generate()函数来完成提示。下面是我用来生成这个故事第一段的代码。</p><figure class="kz la lb lc gt ld"><div class="bz fp l di"><div class="my mz l"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">代码1。使用GPT-J模型的示例代码。</figcaption></figure><p id="1fc8" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">如您所见，我将这两个句子作为上下文(提示)传递给了模型，并得到了故事的介绍作为输出。示例代码中没有什么特别的地方。你可以阅读GPT J文档<a class="ae lo" href="https://huggingface.co/docs/transformers/model_doc/gptj" rel="noopener ugc nofollow" target="_blank">这里</a>了解更多关于该模型的信息。</p><h1 id="95a1" class="lv lw it bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">最后的话，</h1><p id="c5e3" class="pw-post-body-paragraph jz ka it kb b kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw im bi translated">我相信用这些模型做文案是有可能的。基于给定上下文生成的文本质量令人印象深刻。我们可能还没有达到通过一个主题并获得一个完整故事的地步，但是有可能通过一些好的想法来利用这样的模型。主要问题是这种模型的可访问性，以及它们太大以至于需要多个GPU来加载它们的事实。好消息是，你可以在有足够内存的CPU上使用这些型号，这比GPU便宜得多。尽管如此，它们并不是每个人都能轻易获得的。</p><blockquote class="na"><p id="f6e8" class="nb nc it bd nd ne nf ng nh ni nj kw dk translated">我每周给NLP的书呆子发一份时事通讯。如果您想了解自然语言处理的最新发展，可以考虑订阅。<br/> <a class="ae lo" href="https://nlpiation.github.io/" rel="noopener ugc nofollow" target="_blank">阅读更多，订阅</a> —加入酷孩子俱乐部，立即报名！</p></blockquote><h2 id="cec1" class="nk lw it bd lx nl nm dn mb nn no dp mf kk np nq mj ko nr ns mn ks nt nu mr iz bi translated">参考</h2><p id="a46f" class="pw-post-body-paragraph jz ka it kb b kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw im bi translated">[1] Brown，Tom B .等人，“语言模型是一次性学习者。”arXiv预印本arXiv:2005.14165 (2020)。<br/>[2]J-6B:一个60亿参数的自回归语言模型，由王，本和小松崎，阿兰</p></div></div>    
</body>
</html>