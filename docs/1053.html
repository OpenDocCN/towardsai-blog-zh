<html>
<head>
<title>Curse of Dimensionality</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">维度的诅咒</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/curse-of-dimensionality-4335c09ed688?source=collection_archive---------1-----------------------#2020-10-17">https://pub.towardsai.net/curse-of-dimensionality-4335c09ed688?source=collection_archive---------1-----------------------#2020-10-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8c24" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><p id="8fe6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">维数灾难——这是数学家理查德·贝尔曼在1957年的著作《<strong class="jy ja">动态编程</strong>中提出的一个吸引人的术语，指的是在高维实例中问题会变得更加难以解决。</p><p id="148b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们从一个问题开始。什么是<strong class="jy ja">维度</strong>？</p><p id="eeb3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">维度</strong>简单来说就是给定数据集的属性或特征。</p><p id="bfdd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这听起来很简单。那么，为什么我们要使用这样一个与维度相关的负面词汇呢？这里的<strong class="jy ja">诅咒</strong>是什么？</p><h2 id="b6a1" class="ku kv iq bd kw kx ky dn kz la lb dp lc kh ld le lf kl lg lh li kp lj lk ll iw bi translated"><strong class="ak">让我们用一个一般的例子来学习维数灾。</strong></h2><p id="959c" class="pw-post-body-paragraph jw jx iq jy b jz lm kb kc kd ln kf kg kh lo kj kk kl lp kn ko kp lq kr ks kt ij bi translated">如果有人问我什么是机器学习模型？</p><p id="0fef" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">通俗地说，当我们给定数据集进行训练时，训练阶段的输出是一个<strong class="jy ja">模型</strong>。</p><p id="0aa7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">假设我们有7个模型，每个模型都有不同数量的维度，在所有7个模型中保持模型的动机相同:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/68e5589581958db8994d435cf38b388a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oWU10rdTjCTvWpA1YZpU6g.jpeg"/></div></div></figure><p id="5fc2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们在这里观察到的是，我们为训练阶段提供的用于生成模型的特征数量呈指数增长。</p><p id="324c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">所以问题来了，维度的数量和模型之间有什么关系？</p><p id="764c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们能说更多的特征会导致更好的模型吗？</p><p id="ffe9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">答案是肯定的，但是……哦，是的！这里有一个但是。</p><p id="19c4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们可以说更多的特征导致更好的模型，但这仅在一定程度上是正确的，让我们称这个程度为阈值。</p><p id="d7ff" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">假设</strong>:在我们的例子中，模型5是阈值。</p><p id="59c2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">模型5具有14个属性，并且在模型1至模型7中，相同属性的错误率最低。</p><p id="9005" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为什么会这样？7型不应该是最好的型号吗？</p><p id="b5db" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">其原因是，在模型6和模型7中，大多数特征是冗余的或稀疏的<strong class="jy ja"> </strong>，这意味着在一定数量的特征之后，所有其他特征都不重要。</p><h2 id="06ed" class="ku kv iq bd kw kx ky dn kz la lb dp lc kh ld le lf kl lg lh li kp lj lk ll iw bi translated">好吧，但是现在稀疏是什么？为什么这是一个问题？</h2><p id="f369" class="pw-post-body-paragraph jw jx iq jy b jz lm kb kc kd ln kf kg kh lo kj kk kl lp kn ko kp lq kr ks kt ij bi translated">考虑可以覆盖一维单位线的10个均匀间隔的点。</p><p id="82ef" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在二维1×1平面中，需要100个均匀间隔的样本点来覆盖整个区域。</p><p id="cafe" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">当涉及到一个三维的a立方体时，需要1000个点来覆盖整个区域，你可以想象只把100个点放入三维空间，它们会显得相当稀疏。</p><p id="a3ca" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">覆盖整个体积所需的数据点随着尺寸的增加呈指数增加。空间的容量增长非常快，因此数据跟不上，从而变得稀疏，如下图所示。低维中的数据结构和相关性在高维空间中不能很好地应用或推广。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi md"><img src="../Images/8358f3fe50d5f565a915d01d9e2567ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OdsvyiyWypJbNkKk1YE-pg.jpeg"/></div></div></figure><p id="43b8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">好复杂！二维，三维，多维？多维度的直觉是什么？</strong></p><p id="3bf5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因为我们都习惯于二维或最多三维，我们无法想象多维思考超出了我们想象的范围。让我们试着用概率来形象化维度的诅咒。</p><p id="6219" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">以一个分类问题为例。在分类问题中，目标是定义类别之间的明确界限。</p><p id="7be9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">从一条单位长度的线上随机选取一点。点在边上的概率小于&lt;0.001 from the border is:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/7c187d062b2105a96711c3f3d5b37d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*nKiJLEbJlQ3fDGKr_2Z0nw.jpeg"/></div></figure><p id="a01b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Similarly, in a two-dimensional space, consider a unit square. The probability of a point being in the edges is:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/7692a4bde6131c898e3b542faf0687bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*_JVj8N36Vxodo9Pp30oWDw.jpeg"/></div></figure><p id="eec3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">In a three dimensional space, in a unit cube, the probability of a point being in the edges is:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mg"><img src="../Images/231b74028084b44fa61ac3a1a38d9a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSQO4HttgcnQJ4kRZUQaCw.jpeg"/></div></div></figure><p id="79ce" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">similarly, in an n-dimensional space, the probability of a point being on the edges is:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/3dfe23a73304cab37099638cc012b853.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Nnrt7v7QeFVEh-jxLwVzWg.jpeg"/></div></figure><p id="c60b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">The point I am trying to make here is as “n” increases, the probability of a data point likely to be on the edges increases and the graph below gives a visual of the same.</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mi"><img src="../Images/47d19d25d804db95c4b68e18344c3dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*92sOkF6yrajDcCaeRK8rgg.jpeg"/></div></div></figure><h2 id="8d12" class="ku kv iq bd kw kx ky dn kz la lb dp lc kh ld le lf kl lg lh li kp lj lk ll iw bi translated"><strong class="ak">我们能逃脱这个诅咒吗？</strong></h2><p id="70cf" class="pw-post-body-paragraph jw jx iq jy b jz lm kb kc kd ln kf kg kh lo kj kk kl lp kn ko kp lq kr ks kt ij bi translated"><strong class="jy ja">哦，是的！！！降维。</strong></p><p id="d6ce" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">降维是一种将高维变量转换成低维变量而不丢失太多特征信息的方法。</p><p id="a7fe" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">降维中基本上有两种类型的组件，本文不讨论，因为降维本身是一个巨大且非常有趣的主题，但只是定义组件。</p><p id="dded" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">特征选择:</strong>该技术从原始数据集中提取最相关的变量，涉及三种方式；过滤器、包装器和嵌入式。</p><p id="0c58" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">特征提取:</strong>该技术用于将维度数据降低到更低维度的空间。</p><p id="5747" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">通过降低数据的维度，我们大大减少了计算工作量，减少了维度冗余，并获得了更有效的距离度量。</p><h2 id="fde5" class="ku kv iq bd kw kx ky dn kz la lb dp lc kh ld le lf kl lg lh li kp lj lk ll iw bi translated"><strong class="ak">距离度量？在高维空间中距离是如何被影响的？</strong></h2><p id="fb94" class="pw-post-body-paragraph jw jx iq jy b jz lm kb kc kd ln kf kg kh lo kj kk kl lp kn ko kp lq kr ks kt ij bi translated">高维数据影响距离度量，尤其是欧几里德距离。从数学上讲，欧几里德距离的公式是，</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/15b79b63ebadfb63b85e64ce60029705.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*mdLkAFHy_tkvclo1UZi_Ug.jpeg"/></div></figure><p id="35fb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使得，</p><p id="b3b0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> n </strong>为尺寸。</p><p id="53e9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> Xi </strong>是实数的n维向量</p><p id="8fc8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">x是一个向量空间，其中X₁，X₂，…Xₖ的Xi属于x</p><p id="6935" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"><em class="mk">distmax(Xi)——&gt;</em></strong><em class="mk">Xj属于X₁的Xi到XJ的最大距离，…，Xₖ &amp; Xj ≠Xi </em></p><p id="64dd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"><em class="mk">distmin(Xi)——&gt;</em></strong><em class="mk">Xj属于X₁的Xi到XJ的最小距离，…，Xₖ &amp; Xj ≠Xi </em></p><p id="b267" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在较低的维度中，distmax()会比distmin()大得多，因此比率会大于零(图)。</p><p id="964b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">然而，在更高的维度中，distmax()和distmin()都是相同的，所以比率实际上会变成零，这意味着在高维度中，所有点几乎是<strong class="jy ja">等距的(</strong>图<strong class="jy ja">)。</strong></p><p id="919b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，在高维空间中，由于所有的点都是等距的，所以应用欧几里德距离是没有意义的。这是机器学习在计算高维距离时的主要问题之一。下面是上述python数学证明模拟的输出，它验证了我们关于欧几里德距离和高维度的陈述。</p><p id="e7ab" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">如下图:</strong></p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi ml"><img src="../Images/54b781a2e84d1b84a616b652d35d7b6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YjYZH3TC8YQpigc_yxDsEA.jpeg"/></div></div></figure><h2 id="5f86" class="ku kv iq bd kw kx ky dn kz la lb dp lc kh ld le lf kl lg lh li kp lj lk ll iw bi translated">我们有结论了吗？</h2><p id="8401" class="pw-post-body-paragraph jw jx iq jy b jz lm kb kc kd ln kf kg kh lo kj kk kl lp kn ko kp lq kr ks kt ij bi translated">在具有大量特征的问题中，很大比例的数据将位于边缘。这被称为维数灾难，也是特征工程必不可少的原因。</p><p id="214c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在本文中，使用简单的例子研究了维数灾难的本质。很明显，更高的维度是有代价的。空间体积的爆炸性质是维度诅咒的首要原因。我们观察到维数灾难的影响在只有几十维的情况下很容易被察觉。</p><p id="b196" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">对于距离，如果我们增加维数，计算欧几里得距离就没有意义，因为在高维空间中，所有的向量几乎都是等距的。</p><p id="701a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">尽管存在诅咒，但机器学习的许多最大成功都来自高维领域。</p><h2 id="33ff" class="ku kv iq bd kw kx ky dn kz la lb dp lc kh ld le lf kl lg lh li kp lj lk ll iw bi translated"><strong class="ak">参考文献:</strong></h2><p id="1bac" class="pw-post-body-paragraph jw jx iq jy b jz lm kb kc kd ln kf kg kh lo kj kk kl lp kn ko kp lq kr ks kt ij bi translated">1.贝尔曼环(1961)。适应性控制过程。有导游的旅行。新泽西州普林斯顿普林斯顿大学出版社</p><p id="16f9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">2.<a class="ae mm" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn、Keras和tensor flow-aurélien géRon进行机器实践学习</a></p><p id="e47b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">3.<a class="ae mm" href="https://www.cs.princeton.edu/courses/archive/fall15/cos521/lecnotes/lec12.pdf" rel="noopener ugc nofollow" target="_blank">高维几何，维数灾难，降维</a></p><p id="18a0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">4.数据挖掘简介—庞·谭宁、迈克尔·斯坦巴克、维平·库马尔</p></div></div>    
</body>
</html>