<html>
<head>
<title>Multi-Object Tracking Metrics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多目标跟踪度量</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/multi-object-tracking-metrics-1e602f364c0c?source=collection_archive---------2-----------------------#2021-06-17">https://pub.towardsai.net/multi-object-tracking-metrics-1e602f364c0c?source=collection_archive---------2-----------------------#2021-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ecca" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><p id="c8d5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">评估过程是建立机器学习模型的最重要的步骤之一。尤其是实时检测加跟踪系统。计算机视觉在跟踪方面的应用在监控、体育分析、自动驾驶汽车等领域越来越受欢迎。因此，在部署之前，评估您的模型将是最困难的任务。今天，让我们来看一组评估您的跟踪系统的指标，让您更好地理解您的模型。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ku"><img src="../Images/e2290eb965b194454444763e41f6b11a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gY8e_hlE4ponYiT8"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">照片由<a class="ae lk" href="https://unsplash.com/@mbaumi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米卡·鲍梅斯特</a>在<a class="ae lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="ae0e" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">介绍</h1><p id="f9e9" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">现实环境中的多目标跟踪系统总是具有挑战性，并且没有很多度量标准来评估你的跟踪系统。这篇文章描述了一个叫做CLEAR MOT Metrics的指标的实现。任何跟踪评估系统的主要目标都是判断跟踪器的精度并检查运动物体的轨迹，而单个物体没有多条路线。</p><h2 id="a875" class="mo lm iq bd ln mp mq dn lr mr ms dp lv kh mt mu lz kl mv mw md kp mx my mh iw bi translated"><strong class="ak">清除运动指标</strong></h2><p id="9022" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">假设对于视频馈送中的每一帧，跟踪器系统输出“n”个假设，并且该帧具有“m”个基本事实对象。CLEAR MOT度量的评估过程如下:</p><ol class=""><li id="33d1" class="mz na iq jy b jz ka kd ke kh nb kl nc kp nd kt ne nf ng nh bi translated">基于它们的坐标并在不同匹配算法的帮助下，找到并配对假设和基本事实之间的最佳匹配。</li><li id="ef11" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">对于每一个配对，找出物体位置的误差。</li><li id="d748" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">计算多个误差的总和，例如:</li></ol><p id="c189" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">a)未命中，其中跟踪器未能为给定对象产生任何假设。</p><p id="4f70" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">b)假阳性，其中追踪器产生了假设，但是没有物体存在。</p><p id="dc80" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">c)不匹配误差，其中跟踪者对地面真实的假设改变了当前帧。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/afcb94fd941fcbaef1b86ec67965cd4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*yK_brOyDYk8ntyx6EInLBw.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">图:伯纳丁、妮可和莱纳·斯蒂费尔哈根。"<a class="ae lk" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.367.6279&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">评估多目标跟踪性能:清晰的运动指标</a>。"《欧洲图像与视频处理杂志》2008(2008):1–10。</figcaption></figure><p id="dda7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">基于上述过程，性能可以用两个指标来表示:</p><ol class=""><li id="b321" class="mz na iq jy b jz ka kd ke kh nb kl nc kp nd kt ne nf ng nh bi translated">MOTP(多目标跟踪精度)表示对目标精确位置的估计有多精确。它是所有帧中匹配的基础真值-假设对的估计位置的总误差，由匹配的总数平均。该指标不负责识别对象配置和评估对象轨迹。</li><li id="9434" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">MOTA(多目标跟踪精度)显示了跟踪系统在未命中、误报、不匹配错误等方面犯了多少错误。因此，它可以从三个错误率导出:缺失率、假阳性率和所有帧的不匹配率。</li></ol><h2 id="9843" class="mo lm iq bd ln mp mq dn lr mr ms dp lv kh mt mu lz kl mv mw md kp mx my mh iw bi translated"><strong class="ak">使用Python实现</strong></h2><p id="d1a6" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">如果你想要一个详细的数学解释，我建议你去看看下面链接的官方论文。现在，让我们深入研究使用python实现这些指标。</p><p id="fd94" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">py-motmetrics python库不仅为您提供了MOTA和MOTP，还提供了各种各样的值，您可以通过这些值来分析您的跟踪系统。</p><p id="1b15" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用pip命令安装库，</p><pre class="kv kw kx ky gt np nq nr ns aw nt bi"><span id="f733" class="mo lm iq nq b gy nu nv l nw nx">pip install motmetrics</span></pre><p id="d353" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">导入必要的包，并给出假设和地面真相坐标和对象id，并使用IOU方法获得距离矩阵。</p><pre class="kv kw kx ky gt np nq nr ns aw nt bi"><span id="fcba" class="mo lm iq nq b gy nu nv l nw nx">import motmetrics as mm<br/>import numpy as np<br/><br/>acc = mm.MOTAccumulator(auto_id=True)<br/>a = np.array([            <br/>   [121,458,187,551], #coordinates of three hypothesis in the frame<br/>   [ 316 ,414 ,373,492] # Format--&gt; X, Y, Width, Height<br/>])<br/>b = np.array([ <br/>    [600 , 612, 700, 712], #coordinates of three groundtruth objects<br/>    [134, 445, 173, 523],                                                  <br/>    [321 , 436, 366, 481]<br/>])<br/>distance_matrix = mm.distances.iou_matrix(a, b, max_iou=0.7)<br/>print(distance_matrix)</span><span id="a496" class="mo lm iq nq b gy ny nv l nw nx">OUTPUT --&gt;</span><span id="bdab" class="mo lm iq nq b gy ny nv l nw nx">[[       nan 0.16199685        nan]<br/>[       nan        nan 0.08276546]]</span></pre><p id="7fe4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在让我们用对象id更新累加器，并获得评估报告。</p><pre class="kv kw kx ky gt np nq nr ns aw nt bi"><span id="f289" class="mo lm iq nq b gy nu nv l nw nx">acc.update(<br/>[1, 2],                     # Ground truth object ID in this frame<br/>[0,3,4],                  # Detector hypothesis ID in this frame<br/>[distance_matrix])</span><span id="45b2" class="mo lm iq nq b gy ny nv l nw nx">mh = mm.metrics.create()</span><span id="eeb3" class="mo lm iq nq b gy ny nv l nw nx">report = mh.compute(acc, metrics=['num_frames', 'num_objects','num_matches' ,'mota','motp', 'num_misses','num_false_positives','num_switches','mostly_tracked','partially_tracked','mostly_lost'], name='acc')</span><span id="9901" class="mo lm iq nq b gy ny nv l nw nx">print(report)</span></pre><p id="809d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">查看文档，了解评估报告中包含的更多参数。您可以为视频的多个帧自动编写代码。</p><p id="9894" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">前往<a class="ae lk" href="https://motchallenge.net/" rel="noopener ugc nofollow" target="_blank"> MOT Challenge网站</a>下载样本视频，这些视频带有针对不同视频场景的逐帧注释数据集，以测试您的跟踪系统。MOT17和MOT20的地面实况数据解析示例:</p><pre class="kv kw kx ky gt np nq nr ns aw nt bi"><span id="73bf" class="mo lm iq nq b gy nu nv l nw nx">def gtparser(current_frame,end_frame,file,split):</span><span id="2e61" class="mo lm iq nq b gy ny nv l nw nx">'''<br/>current_frame and end_frame gives you a range of frames you wanna work with. <br/>file -&gt; location of the gt.txt file.<br/>split-&gt; how are the values in gt seperated(mostly it is a ',' or a space ' ').<br/>'''<br/>    dict1 = {}<br/>    while current_frame&lt;=end_frame:  <br/>        f_bb,idarr = [], []<br/>        f = open(file,'r')   <br/>        for line in f.readlines():<br/>            bb = []    <br/>            if int(line.split(split)[0])==current_frame and int(line.split(split)[7])==1:<br/>                x_scale = 960/1920<br/>                y_scale = 540/1080<br/>                (origLeft, origTop, origWidth, oriHeight) = (int(float(line.split(split)[2])),int(float(line.split(split)[3])),int(float(line.split(split)[4])),int(float(line.split(split)[5])))</span><span id="2b56" class="mo lm iq nq b gy ny nv l nw nx">    dict1 = {}<br/>    while current_frame&lt;=end_frame:  <br/>        f_bb,idarr = [], []<br/>        f = open(file,'r')   <br/>        for line in f.readlines():<br/>            bb = []    <br/>            if int(line.split(split)[0])==current_frame and int(line.split(split)[7])==1:<br/>                x_scale = 960/1920<br/>                y_scale = 540/1080<br/>                (origLeft, origTop, origWidth, oriHeight) = (int(float(line.split(split)[2])),int(float(line.split(split)[3])),int(float(line.split(split)[4])),int(float(line.split(split)[5])))</span><span id="ef45" class="mo lm iq nq b gy ny nv l nw nx">x = int(np.round(origLeft * x_scale))<br/>                y = int(np.round(origTop * y_scale))</span><span id="7453" class="mo lm iq nq b gy ny nv l nw nx">w = int(np.round(origWidth * x_scale)) <br/>                h = int(np.round(oriHeight * y_scale)) <br/>                bb.append(x)<br/>                bb.append(y)<br/>                bb.append(w)<br/>                bb.append(h)<br/>                idarr.append(int(line.split(split)[1]))<br/>                f_bb.append(bb)<br/>        f_bb = np.array(f_bb)  <br/>        dict1[current_frame] = (idarr,f_bb)<br/>        current_frame+=1<br/>        f.close()<br/>    #print(len(dict),current_frame)</span><span id="a9fc" class="mo lm iq nq b gy ny nv l nw nx">return dict1</span></pre><h2 id="abcc" class="mo lm iq bd ln mp mq dn lr mr ms dp lv kh mt mu lz kl mv mw md kp mx my mh iw bi translated"><strong class="ak">对py-motmetrics结果的解释</strong></h2><ol class=""><li id="3fea" class="mz na iq jy b jz mj kd mk kh nz kl oa kp ob kt ne nf ng nh bi translated">MOTP的范围是从0到1(如果你需要用百分比乘以100)。如果MOTP值是1，那么系统的精度很差。如果它接近于零，那么系统的精度是好的。</li><li id="4997" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">MOTA的范围从–INF到1。(如果需要MOTA的百分比，请乘以100)。如果MOTA是1，那么系统的精度是好的。如果MOTA在零附近或小于零，则系统的精度很差。</li><li id="7e85" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">num_objects，所有帧中唯一对象出现的总数。</li><li id="cd09" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">num_matches，假设和基本事实对象id之间匹配总数。</li><li id="e59c" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">未命中，未命中的总数(假阴性)。</li><li id="ef62" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">false_positives (FP)，误报(误报警)的总数。</li><li id="b268" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">道岔，轨道道岔的总数。</li><li id="5895" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">mostly _ tracked，在至少80%的生命周期内跟踪的对象数量</li><li id="d83c" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">partially_tracked，在生命周期的20%到80%之间跟踪的对象数量。</li><li id="908d" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">大多数情况下，跟踪的对象数量少于寿命的20%。</li></ol><h2 id="393c" class="mo lm iq bd ln mp mq dn lr mr ms dp lv kh mt mu lz kl mv mw md kp mx my mh iw bi translated"><strong class="ak">来源</strong></h2><ol class=""><li id="fc03" class="mz na iq jy b jz mj kd mk kh nz kl oa kp ob kt ne nf ng nh bi translated">伯纳丁，妮可和莱纳·斯蒂费尔哈根。"<a class="ae lk" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.367.6279&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">评估多目标跟踪性能:清晰的运动指标</a>。"<em class="oc"> EURASIP图像和视频处理杂志</em>2008(2008):1–10。</li><li id="c4e3" class="mz na iq jy b jz ni kd nj kh nk kl nl kp nm kt ne nf ng nh bi translated">py-motmetrics Github，<a class="ae lk" href="https://github.com/cheind/py-motmetrics/tree/6597e8a4ed398b9f14880fa76de26bc43d230836" rel="noopener ugc nofollow" target="_blank">https://Github . com/cheind/py-mot metrics/tree/6597 E8 a4 ed 398 B9 f 14880 fa 76 de 26 BC 43d 230836</a></li></ol></div></div>    
</body>
</html>