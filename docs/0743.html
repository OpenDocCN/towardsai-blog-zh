<html>
<head>
<title>Implement a Neural Network from Scratch with NumPy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用NumPy从头实现一个神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/implement-a-neural-network-from-scratch-with-numpy-67db290771b?source=collection_archive---------1-----------------------#2020-07-31">https://pub.towardsai.net/implement-a-neural-network-from-scratch-with-numpy-67db290771b?source=collection_archive---------1-----------------------#2020-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9ce1" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><div class=""><h2 id="804d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">…一件没有你想象的那么难的事情</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/87d0cd9046a470165e5ebd4102a41a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WJ57ZKta2HxlQhzxuWR5zw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">背景图片来源:<a class="ae lh" href="https://commons.wikimedia.org/wiki/File:Multi-Layer_Neural_Network-Vector-Blank.svg" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></figcaption></figure><p id="f707" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我认为真正理解神经网络如何工作的最好方法是从头开始实现一个神经网络。这正是我在这篇文章中要做的。我将创建一个神经网络类，我想以这样一种方式来设计它，以便更加灵活。我不想在其中硬编码特定的激活或损失函数，或优化器(即SGD，Adam，或其他基于梯度的方法)。我将把它设计成从类外部接收这些，这样人们就可以获取该类的代码，并向它传递他想要的任何激活/丢失/优化。因此，我将实现激活和损失函数，以及优化器类，我们希望在这里将它们作为独立于<code class="fe me mf mg mh b">NeuralNetwork</code>类的东西使用。我们需要激活/损失函数及其导数。</p><p id="20c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了允许批量大于1，我们的激活和损失函数应该处理矩阵输入。这些矩阵中的行将代表不同的数据点，列将是特征。我们的网络将允许2种激活功能:隐藏层和输出层。隐藏层激活应该对它们的输入向量进行元素化操作，因此它们的导数也是元素化的，为每个数据点返回一个向量。但是输出激活应该允许基于输入向量中的所有元素来计算输出向量中的每个元素。那就是能够使用softmax激活。正因为如此，它们的导数需要返回一个雅可比矩阵(一个由每个输出函数对每个输入分量的偏导数组成的矩阵；你可以在<a class="ae lh" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant" rel="noopener ugc nofollow" target="_blank">维基百科</a>上阅读更多关于每个数据点的信息。</p><p id="16a2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里我们将只使用ReLU作为隐藏激活；identity和softmax将用作输出激活。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="3a68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们使用EPS变量，它是float64类型的最小正表示数，以避免被0除。为了避免softmax函数中的溢出错误，我们从输入中减去了每个数据点的最大值。我们可以这样做，因为它不会改变函数的输出，因为它的效果与将该分数的两项除以相同的量相同。</p><p id="4147" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">损失函数应该将两个矩阵作为输入:预测的y和真实的y，这两个矩阵的形式与激活函数中的相同。这些损失函数应该为每个数据点输出一个数字。它们的导数应该为每个数据点输出一个行向量，所有数据点都堆叠成一个3维的数组。这个输出形状需要能够使用NumPy的<code class="fe me mf mg mh b">matmul()</code>函数与输出激活的导数相乘。注意下面的<code class="fe me mf mg mh b">expand_dims()</code>函数的使用，该函数用于返回所需的形状。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="fccd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里我们将只使用带动量的随机梯度下降作为优化方法，但是还有更多基于梯度的方法。一些流行的选择是亚当，RMSprop，阿达格拉德。为了让我们的神经网络类能够处理所有这些，我们将把优化器作为一个单独的类来实现，使用一个返回更新参数的<code class="fe me mf mg mh b">.update(old_params, gradient)</code>方法。神经网络类将接收一个优化器作为参数。所以，想要使用其他优化方法的人可以创建一个具有所需接口的类，并在实例化时将其传递给我们的神经网络类。</p><p id="a832" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是新币+动量优化器:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="59b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了将分类任务中的分类标签转换为一键编码，我们将使用<code class="fe me mf mg mh b">to_categorical()</code>实用函数:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="b818" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们从NeuralNetwork类的代码开始。实例化方法需要以下参数:</p><ul class=""><li id="f3e8" class="mk ml it lk b ll lm lo lp lr mm lv mn lz mo md mp mq mr ms bi translated"><strong class="lk jd">层</strong>:由每层(包括输入和输出层)的节点数组成的列表<br/>例如:【5，10，2】表示5个输入，10个隐层节点，2个输出节点</li><li id="4891" class="mk ml it lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated"><strong class="lk jd">隐藏_激活</strong>:激活隐藏层；一个元组形式(activation_function，its_derivative) <br/>这个激活函数和它的导数应该在输入数组<br/>上执行它们的任务，例如:(relu，d_relu)</li><li id="56b5" class="mk ml it lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated"><strong class="lk jd">输出_激活</strong>:输出层激活；一个形式元组(activation_function，its_derivative) <br/>这个激活函数将一个形状数组(n，m)作为输入；n个样本，m个神经元在输出层；并返回shape (n，m)数组；输出数组中一行上的每个元素都是输入数组中该行上所有元素的函数输出。<br/>它的导数将一个类似于激活所采用的数组作为输入，但它返回一个shape (n，m，m)数组，该数组是一堆雅可比矩阵，每个样本一个。</li><li id="5fda" class="mk ml it lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated"><strong class="lk jd"> loss </strong>:一个形式元组(loss_function，its _ derivative)<br/>loss函数取形状(n，m)的两个数组(预测y和真实y)作为输入；n个样本，输出层m个神经元；并返回shape (n，1)数组，其元素是每个样本的损失。<br/>它的导数将shape (n，m)的数组作为输入，并返回shape (n，1，m)中的一个，该数组是由m个输入变量中的每一个的导数w.r.t .组成的行向量的堆栈。<br/>例如:(分类_交叉熵，d _分类_交叉熵)</li><li id="1160" class="mk ml it lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated"><strong class="lk jd">优化器</strong>:有方法的对象。update(old_params，gradient)返回新的参数<br/>，例如:SGD()</li></ul><p id="4890" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，它使用Xavier初始化方法的变体初始化其权重和偏差。也就是说，我们从均值为0且标准差为的正态分布中得出权重和偏差:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1499ddec9ab4cea1570850c8a84436d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gCsCw3xbrkR42LhxW7QrLQ.png"/></div></div></figure><p id="7d9a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<code class="fe me mf mg mh b">fan_in</code>和<code class="fe me mf mg mh b">fan_out</code>分别是前一层的节点数和下一层的神经元数。权重矩阵中的行数与前一层中的节点数相匹配，列数与下一层中的节点数相匹配。偏差是行向量，其元素数量与下一层中的节点数量相匹配。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="524e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了方便地进行参数更新过程，我们将创建一个<code class="fe me mf mg mh b">.__flatten_params(weights, biases)</code>方法，将权重矩阵列表和作为输入接收的偏差向量转换为扁平向量。我们还需要一个<code class="fe me mf mg mh b">.__restore_params(params)</code>方法，将参数的扁平向量转换回权重和偏差列表。注意，方法名前面的2个下划线仅仅意味着该方法在OOP术语中是私有的。这只是意味着该方法应该只在类内部使用。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="7328" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe me mf mg mh b">.__forward(x)</code>方法通过网络传递输入数组x，在这样做的同时，它跟踪每层的输入和输出数组。然后，它将此作为一个列表返回，其中第I个元素是一个形式为[第I层的输入，第I层的输出]的列表。我们将需要这些数组来计算后向传递中的导数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="691c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe me mf mg mh b">.__backward(io_arrays, y_true)</code>方法计算梯度。它将一个由<code class="fe me mf mg mh b">.__forward(x)</code>方法返回的表单列表和一个包含基本事实y的数组作为输入。它使用反向传播算法计算权重和偏差的梯度。然后它返回一个元组(d_weights，d _ biases)。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="9fa7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">协调所有培训的方法是<code class="fe me mf mg mh b">.fit(x, y, batch_size, epochs, categorical)</code>，其中:</p><ul class=""><li id="6188" class="mk ml it lk b ll lm lo lp lr mm lv mn lz mo md mp mq mr ms bi translated"><code class="fe me mf mg mh b">x</code>是输入数据</li><li id="09fc" class="mk ml it lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated"><code class="fe me mf mg mh b">y</code>是地面真理</li><li id="70c1" class="mk ml it lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated"><code class="fe me mf mg mh b">batch_size</code>是一批数据的大小</li><li id="0357" class="mk ml it lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated"><code class="fe me mf mg mh b">epochs</code>是所有输入数据的迭代次数</li><li id="056b" class="mk ml it lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated"><code class="fe me mf mg mh b">categorical</code>是一个可选参数，当设置为true时，将y转换为一键编码</li></ul><p id="2333" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于每批数据，它使用<code class="fe me mf mg mh b">.__forward()</code>和<code class="fe me mf mg mh b">.__backward()</code>方法计算梯度，然后使用<code class="fe me mf mg mh b">.__flatten_params()</code>拉平网络的当前参数和梯度。之后，使用<code class="fe me mf mg mh b">self.optimizer.update()</code>计算新的参数，然后使用<code class="fe me mf mg mh b">.__restore_params()</code>将返回的向量恢复为正确的格式，并将其分配给<code class="fe me mf mg mh b">self.weights, self.biases</code>。每批结束时，打印进度和平均损失。维护并返回每个时期结束时所有损失值的列表。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="7c43" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">默认情况下，在输入x通过网络后，<code class="fe me mf mg mh b">.predict()</code>方法将返回输出节点中的精确值。如果labels参数设置为true，则返回预测的标签；这大概就是你在一个分类问题中想要的。</p><p id="8f22" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">默认情况下，<code class="fe me mf mg mh b">.score()</code>方法返回平均损失。如果accuracy设置为true，那么将返回精度。请注意，在分类问题中，如果您想要损失，那么y应该以一键编码格式提供，否则，如果您想要返回准确性，那么y应该是常规的类标签。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="1f41" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们希望能够在本地保存参数，这样我们就不必在每次想要进行预测时训练我们的模型。注意，下面的方法只能保存和加载权重和偏差，而不能保存和加载关于层、激活、损失函数和优化器的全部信息。因此，您还应该保存用于实例化神经网络的代码。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="4024" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是完整代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="0ac5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nf">我希望这些信息对您有用，感谢您的阅读！</em></p><p id="7e8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章也贴在我自己的网站<a class="ae lh" href="https://www.nablasquared.com/implement-a-neural-network-from-scratch-with-numpy/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>