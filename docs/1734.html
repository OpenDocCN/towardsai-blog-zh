<html>
<head>
<title>Into the Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">进入逻辑回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/into-the-logistic-regression-75847b0e6f1e?source=collection_archive---------0-----------------------#2021-04-04">https://pub.towardsai.net/into-the-logistic-regression-75847b0e6f1e?source=collection_archive---------0-----------------------#2021-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5b8c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="e119" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">打破逻辑回归的概念和多类分类的一对一和一对一方法</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/3c6f2bb426b958cbc3872a6dcf32fb64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CO1C_BbeMvZQ_nZYhGnFPg.png"/></div></div></figure><p id="410e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">之前我已经深入写过线性回归的封闭形式(方程)和梯度下降。你可以从下面的网址阅读它们。</p><div class="lw lx gp gr ly lz"><a rel="noopener  ugc nofollow" target="_blank" href="/closed-form-and-gradient-descent-regression-explained-with-python-1627c9eeb60e"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd ja gy z fp me fr fs mf fu fw iz bi translated">用Python解释封闭形式和梯度下降回归</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">回归问题简化和实施的封闭形式的方程和梯度下降从零开始和…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">pub.towardsai.net</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ky lz"/></div></div></a></div><p id="4270" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在本文中，我将重点介绍逻辑回归算法，分解概念，像机器一样思考，并了解使用逻辑回归的多类分类器背后的概念。</p><h1 id="d81a" class="mo mp iq bd mq mr ms mt mu mv mw mx my kf mz kg na ki nb kj nc kl nd km ne nf bi translated">线性回归再探…</h1><p id="aec5" class="pw-post-body-paragraph la lb iq lc b ld ng ka lf lg nh kd li lj ni ll lm ln nj lp lq lr nk lt lu lv ij bi translated">在开始逻辑回归之前，让我们快速回顾一下线性回归。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/7be3b6c7f2acf6781caea1b400690aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*KH661_7PrhuRC0DStPe6aQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">线性回归方程</figcaption></figure><p id="9e67" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">线性回归模型适用于因变量连续且分类失败的回归问题，因为它将类视为数字(0和1)并拟合使点和超平面之间的距离最小化的最佳超平面，因此减少了误差(为简单起见，您可以将超平面视为模型方程)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/8c4f0640f29f83787d96ad135478e288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eIKX-GFytNanTU0_kvmFtA.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">不同问题中的线性回归(a)左图是用回归(b)右图是用分类问题</figcaption></figure><p id="5556" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">可以看出，线性回归的预测可以走<strong class="lc ja"> (-∞，∞) </strong>。此外，它没有给出概率作为输出。</p><p id="64c8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这就是对数概率的来源。</p><h1 id="46cd" class="mo mp iq bd mq mr ms mt mu mv mw mx my kf mz kg na ki nb kj nc kl nd km ne nf bi translated">但是什么是赔率和对数赔率…</h1><p id="0053" class="pw-post-body-paragraph la lb iq lc b ld ng ka lf lg nh kd li lj ni ll lm ln nj lp lq lr nk lt lu lv ij bi translated">如前所述，线性回归无法计算概率。然而，如果我们观察概率，通过计算<strong class="lc ja">比值比</strong>的对数，它可以很容易地从对数比值转换而来。概率、优势比和对数优势都是相同的，但表达方式不同。</p><p id="7f32" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">概率</strong>是事件发生的可能性(即学生通过数学考试的几率为60%)。</p><p id="6e48" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">赔率</strong>，或成功的赔率或赔率比，是一种暴露和结果之间关联的度量，或者简单地说，是成功的概率与失败的概率之比(从上面的例子来看，那就是0.6/0.4 = 1.5)。</p><p id="bd8e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">对数赔率</strong>是赔率的对数(ln(1.5) ≅ 0.405)。</p><p id="a579" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="nr">备注:在正常计算中，可以使用任意底数的对数，但需要一致。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/1e9b07362111968ff87359720cd2e8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mwO_eEMm6z2p2grcwsXX-A.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">概率到比值比和对数比值比的方程和转换</figcaption></figure><p id="2372" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们可以想象出概率的比值比和比值比的对数比值比。观察比值比可以在[0，∞]之间变化，而对数比值可以在(-∞，∞)之间变化。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/c984854bd0df56dfc6fe9de9e8873177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eO7wzKzovSzqh13DqSO9-w.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">(a)赔率对概率和(b)对数赔率对赔率的可视化</figcaption></figure><p id="0196" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在让我们快速看一下如何解读比值比；</p><ul class=""><li id="0870" class="nu nv iq lc b ld le lg lh lj nw ln nx lr ny lv nz oa ob oc bi translated">OR = 1，暴露不影响结果的几率</li><li id="220a" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv nz oa ob oc bi translated">或&gt; 1，暴露与更高的结果几率相关</li><li id="159f" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv nz oa ob oc bi translated">或者&lt; 1, exposure associated with lower odds of the outcome</li></ul><p id="34b6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">Using the above example, the odds of students passed math exams are 1.5 times as large as the odds of students failed math exams. <strong class="lc ja">记住这并不等同于1.5倍的可能性！</strong></p><h1 id="8a14" class="mo mp iq bd mq mr ms mt mu mv mw mx my kf mz kg na ki nb kj nc kl nd km ne nf bi translated">回到逻辑回归</h1><p id="8908" class="pw-post-body-paragraph la lb iq lc b ld ng ka lf lg nh kd li lj ni ll lm ln nj lp lq lr nk lt lu lv ij bi translated">我假设我们都知道逻辑回归的定义，逻辑回归模型(或<em class="nr"> logit </em>模型)是对某一类或某一事件(即学生通过考试，客户流失)的概率进行建模的统计方法。在这个模型中，我们假设自变量和因变量的对数优势之间有一个<strong class="lc ja">线性关系</strong>，它可以表示为下面的等式。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/510e81ec275c06771c23f00bbb682744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7OMzBO6I1TIlderr9UH0A.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">逻辑斯蒂方程</figcaption></figure><p id="ffbb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">将对数优势转换为概率的函数是逻辑函数，对数优势标度的测量单位称为<strong class="lc ja"> logit </strong>，它来自<strong class="lc ja"> log </strong> istic un <strong class="lc ja"> it </strong>(因此得名)。</p><p id="0a6d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">所以从上面的等式，最终，我们尝试预测等式的左边路径(不是右边)，因为<strong class="lc ja"> <em class="nr"> p(y=1|x) </em> </strong>是我们想要的。所以我们可以取这个logit函数的逆，那么我们会得到类似于我们的东西。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/eded5ec59147167bbdf7fe9b269842d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*S5Gaw1cITvmDyHyr748loA.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">逆logit函数</figcaption></figure><p id="be33" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">上面的等式是常见的sigmoid函数，逻辑sigmoid，它从输入<strong class="lc ja"> <em class="nr"> b0 + b1x1 + … </em> </strong>中返回<strong class="lc ja"> <em class="nr"> p(y=1|x) </em> </strong>的类概率</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ca6e66549a0c7cb478de372cb249c0a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*w8OWKaMrpu8w73HGjQBhHQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">逻辑曲线</figcaption></figure><p id="0660" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">正如y轴中暗示的逻辑曲线，这种映射是概率(0，1)。最终，我们使用赔率的对数的原因是它可以取任何正值或负值(如前所述)。因此，逻辑回归是对数优势的线性模型。</p><p id="dd00" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">请注意，在逻辑方程式中，选择参数是为了最大化观察样本值的可能性(或MLE—<strong class="lc ja">M</strong>axi mize<strong class="lc ja">L</strong>ikelihood<strong class="lc ja">E</strong>估计)，而不是最小化误差平方和(或LSE—<strong class="lc ja">L</strong>east<strong class="lc ja">S</strong>quare<strong class="lc ja">E</strong>估计)。</p><h2 id="217d" class="ol mp iq bd mq om on dn mu oo op dp my lj oq or na ln os ot nc lr ou ov ne iw bi translated">回顾:线性回归和逻辑回归之间的比较</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/30ad3476cc33bd488a3d495c1ee8c0bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qfecIbMfYUJenrmQFnykxg.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">线性回归和逻辑回归的主要区别</figcaption></figure><h1 id="002f" class="mo mp iq bd mq mr ms mt mu mv mw mx my kf mz kg na ki nb kj nc kl nd km ne nf bi translated">如果我们有两个以上的类要预测…</h1><p id="eeba" class="pw-post-body-paragraph la lb iq lc b ld ng ka lf lg nh kd li lj ni ll lm ln nj lp lq lr nk lt lu lv ij bi translated">我到目前为止所讨论的只是针对二元逻辑回归(因变量只包含两类——是或否，0或1)。在本节中，我们将经历<strong class="lc ja">多项式</strong>逻辑回归，或者我们可以称之为多类逻辑回归。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/ef78f25cd4c9c58c5e253c22c5ad8429.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*Za8bM0U7VxwPd4ngZjUGVQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">4类分类的可视化</figcaption></figure><p id="5cc2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">为了打破这一点，我们已经有了一个二元分类模型(逻辑回归)，最终，我们可以将多类数据集分割成多个二元分类数据集，并使模型适合每个数据集。</p><h2 id="03fc" class="ol mp iq bd mq om on dn mu oo op dp my lj oq or na ln os ot nc lr ou ov ne iw bi translated">那么它是如何工作的呢…</h2><p id="6e64" class="pw-post-body-paragraph la lb iq lc b ld ng ka lf lg nh kd li lj ni ll lm ln nj lp lq lr nk lt lu lv ij bi translated">良好的..你有没有遇到过需要猜一猜的情况(让我们在考试中想象一下——选择题考试)。你读问题，比较选择，去掉没意义的，比较剩下的选择，挑一个。</p><p id="3709" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">实际上，这可以应用于二元分类模型如何针对多类问题工作。因此，这取决于您如何进行比较，假设我们有四个类别:A、B、C和D。您可以选择通过A与[B、C、D]进行比较，或者将A与每个单独的类别进行比较(即A与B、A与C)。这两种比较实际上是有名称的，它们是一对一策略。</p><h1 id="08f0" class="mo mp iq bd mq mr ms mt mu mv mw mx my kf mz kg na ki nb kj nc kl nd km ne nf bi translated">一对多策略</h1><p id="7119" class="pw-post-body-paragraph la lb iq lc b ld ng ka lf lg nh kd li lj ni ll lm ln nj lp lq lr nk lt lu lv ij bi translated">One-vs-All，OvA(或One-vs-Rest，OvR)是使用二元分类算法进行多类分类的策略之一。</p><p id="07c2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在这种方法中，我们将把一个类作为正面类，而其他类被假定为负面类(考虑将一个类与其他类进行比较)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/df1c05d9e0a816d9023f8d14d9a8ca33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ZI9OH4BGWP77JZCG_ZK7A.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">一对一战略的可视化</figcaption></figure><p id="2848" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">例如，给定一个具有四个类的多类问题(按照上面的可视化)，那么我们可以将这些分成四个二进制分类数据集；</p><ul class=""><li id="d597" class="nu nv iq lc b ld le lg lh lj nw ln nx lr ny lv nz oa ob oc bi translated"><strong class="lc ja"> 1: </strong>蓝色vs【绿色、黑色、红色】</li><li id="f7d4" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv nz oa ob oc bi translated"><strong class="lc ja"> 2: </strong>绿色vs【蓝色、黑色、红色】</li><li id="6e5f" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv nz oa ob oc bi translated"><strong class="lc ja"> 3: </strong>黑色vs【蓝色、绿色、红色】</li><li id="336b" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv nz oa ob oc bi translated"><strong class="lc ja"> 4: </strong>红色vs【蓝色、绿色、黑色】</li></ul><p id="19a7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这种策略要求每个模型预测一个类似概率的分数。这些分数的argmax随后被用于预测类别。这种策略通常用于以下算法:</p><ol class=""><li id="9d5d" class="nu nv iq lc b ld le lg lh lj nw ln nx lr ny lv oz oa ob oc bi translated">逻辑回归</li><li id="d3cc" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv oz oa ob oc bi translated">感觉</li><li id="cb09" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv oz oa ob oc bi translated">以<strong class="lc ja"> <em class="nr"> softmax函数</em> </strong>为输出层的深度神经网络</li></ol><p id="4ac1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，当使用这些算法解决多类问题时，scikit-learn库默认实现OvA / OvR。</p><p id="2cb8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">当处理非常大的数据集时，这种方法可能会有一些缺点。</p><h1 id="2c7b" class="mo mp iq bd mq mr ms mt mu mv mw mx my kf mz kg na ki nb kj nc kl nd km ne nf bi translated">一对一策略</h1><p id="7854" class="pw-post-body-paragraph la lb iq lc b ld ng ka lf lg nh kd li lj ni ll lm ln nj lp lq lr nk lt lu lv ij bi translated">与一对一策略类似，一对一策略是一种通过将数据集拆分为二元分类数据集来对多类问题进行二元分类的方法。与一对一策略不同，一对一策略将数据集分成两个特定的类，下面举例说明。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/bef16368a678887112ef8286225889ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*wKYi_-D-ENgmdVXBqI1Rhw.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">一对一策略的可视化</figcaption></figure><p id="f719" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">由于这种分裂，二进制分类模型比一对一的策略更多；</p><ol class=""><li id="cedb" class="nu nv iq lc b ld le lg lh lj nw ln nx lr ny lv oz oa ob oc bi translated">蓝色vs绿色</li><li id="eea2" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv oz oa ob oc bi translated">蓝色对红色</li><li id="3626" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv oz oa ob oc bi translated">蓝色vs黑色</li><li id="523e" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv oz oa ob oc bi translated">红色vs绿色</li><li id="7f59" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv oz oa ob oc bi translated">红色vs黑色</li><li id="dd2d" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv oz oa ob oc bi translated">绿色vs黑色</li></ol><p id="dd96" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">生成的模型数量=<em class="nr">(n _ classes *(n _ classes-1))/2</em></p><p id="b09d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一旦生成了所有模型，该点将被测试到所有模型，并记录一个类相对于其他类优先多少次。得票最多的班级获胜。</p><p id="95e6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">通常，这种策略被建议用于支持向量机和相关的基于核的算法，因为核方法的性能不与训练数据集的大小成比例，并且仅使用训练数据的子集可以抵消这种影响。</p><h1 id="9918" class="mo mp iq bd mq mr ms mt mu mv mw mx my kf mz kg na ki nb kj nc kl nd km ne nf bi translated">那么哪种策略更好呢？</h1><p id="dfb6" class="pw-post-body-paragraph la lb iq lc b ld ng ka lf lg nh kd li lj ni ll lm ln nj lp lq lr nk lt lu lv ij bi translated">如上所述，一个对所有的策略在处理大型数据集时可能具有挑战性，因为我们仍然会多次使用所有的数据。然而，一对一策略将整个数据集分割成每对类的二进制分类(参见上面的OvA和OvO的可视化)。</p><p id="7471" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一对一策略训练的分类器数量较少，因此速度更快。但是，一对一策略不太容易在数据集中造成不平衡。</p><h1 id="158a" class="mo mp iq bd mq mr ms mt mu mv mw mx my kf mz kg na ki nb kj nc kl nd km ne nf bi translated">结论</h1><p id="66ea" class="pw-post-body-paragraph la lb iq lc b ld ng ka lf lg nh kd li lj ni ll lm ln nj lp lq lr nk lt lu lv ij bi translated">在这篇文章中，我有一个演练；</p><ul class=""><li id="5cd1" class="nu nv iq lc b ld le lg lh lj nw ln nx lr ny lv nz oa ob oc bi translated">复习线性回归；</li><li id="f76a" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv nz oa ob oc bi translated">对数优势的基本概念及其使用原因；</li><li id="e5fa" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv nz oa ob oc bi translated">深入探究逻辑回归的概念；</li><li id="9bc6" class="nu nv iq lc b ld od lg oe lj of ln og lr oh lv nz oa ob oc bi translated">多类分类器的一对一和一对一策略。</li></ul><p id="cd1b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">希望你获得更多的知识和逻辑回归的背景知识来扩展你的思想并在此基础上构建(而不是仅仅导入逻辑回归库/包并使用它)。</p><h1 id="88ba" class="mo mp iq bd mq mr ms mt mu mv mw mx my kf mz kg na ki nb kj nc kl nd km ne nf bi translated">参考和外部链接</h1><div class="lw lx gp gr ly lz"><a href="https://christophm.github.io/interpretable-ml-book/logistic.html" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd ja gy z fp me fr fs mf fu fw iz bi translated">4.2逻辑回归|可解释的机器学习</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">逻辑回归对具有两种可能结果的分类问题的概率进行建模。这是一个扩展…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">christophm.github.io</p></div></div><div class="mi l"><div class="pb l mk ml mm mi mn ky lz"/></div></div></a></div><div class="lw lx gp gr ly lz"><a href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd ja gy z fp me fr fs mf fu fw iz bi translated">逻辑回归-维基百科</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">在统计学中，逻辑模型(或logit模型)用于模拟某一类或某一事件的概率…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">en.wikipedia.org</p></div></div><div class="mi l"><div class="pc l mk ml mm mi mn ky lz"/></div></div></a></div><div class="lw lx gp gr ly lz"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd ja gy z fp me fr fs mf fu fw iz bi translated">sk learn . multi class . onevsrestclassifier-sci kit-learn 0 . 24 . 1文档</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">也称为一对一，这种策略包括为每个类安装一个分类器。对于每个分类器，类是…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">scikit-learn.org</p></div></div><div class="mi l"><div class="pd l mk ml mm mi mn ky lz"/></div></div></a></div><div class="lw lx gp gr ly lz"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd ja gy z fp me fr fs mf fu fw iz bi translated">sk learn . multi class . onevsoneclassifier-sci kit-learn 0 . 24 . 1文档</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">该策略包括为每个类对安装一个分类器。在预测时间，收到最多的班级…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">scikit-learn.org</p></div></div><div class="mi l"><div class="pe l mk ml mm mi mn ky lz"/></div></div></a></div></div></div>    
</body>
</html>