<html>
<head>
<title>AI Writes Shakespearean Plays</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">艾写莎士比亚的戏剧</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/ai-writes-shakespearean-plays-e0d5f30c16b2?source=collection_archive---------0-----------------------#2020-03-09">https://pub.towardsai.net/ai-writes-shakespearean-plays-e0d5f30c16b2?source=collection_archive---------0-----------------------#2020-03-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/25edb2725ab6f62ffdbab8e0c741b30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yK_SN7M6SG-ovDwi.jpg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://vitadamuseo.files.wordpress.com/2016/03/shakespeare-fisrt-folio-1623-british-library.jpg" rel="noopener ugc nofollow" target="_blank">来源</a>。图像免费共享和商业使用。</figcaption></figure><div class=""/><div class=""><h2 id="8c2e" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">训练一个递归神经网络来模仿莎士比亚</h2></div><blockquote class="ky kz la"><p id="ce7c" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">弗洛里泽尔:<br/>她应该跪下吗？<br/>在不得泣受；把我<br/>和不尊敬的问候比住在，你，<br/>看着我，在天堂的儿子。</p></blockquote><p id="62fd" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">谁写的，莎士比亚还是机器学习模型？</p><p id="e696" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">如果你回答了前者就不会被指责了！上面这段话是一个用TensorFlow训练的递归神经网络的产物，经过30个历元的训练，给了一个‘flori zel:’的种子在这篇文章中，我将解释并给出代码，告诉你如何训练一个机器神经网络来写莎士比亚的戏剧或任何你想让它写的东西！</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="3166" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">导入和数据</h1><p id="ab2c" class="pw-post-body-paragraph lb lc jj le b lf na kk lh li nb kn lk ly nc ln lo lz nd lr ls ma ne lv lw lx im bi translated">首先，导入一些基本库:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="1b3b" class="no mj jj nk b gy np nq l nr ns">import tensorflow as tf<br/>import numpy as np<br/>import os<br/>import time</span></pre><p id="4a7e" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">TensorFlow内置了莎士比亚戏剧的访问权限。如果你在Kaggle这样的在线环境中工作，请确保互联网已启用。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="fb93" class="no mj jj nk b gy np nq l nr ns">path_to_file = tf.keras.utils.get_file('shakespeare.txt', '<a class="ae jg" href="https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'</a>)</span></pre><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/f6acb2369861b0bcf35395fb6d9c81c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTW4bKjw6RxMFBfBDUEaaQ.png"/></div></div></figure><p id="8c14" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">数据需要和utf-8一起解码。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="a06c" class="no mj jj nk b gy np nq l nr ns">text = open(path_to_file, 'rb').read().decode(encoding='utf-8')<br/># length of text is the number of characters in it<br/>print ('Length of text: {} characters'.format(len(text)))</span></pre><p id="9612" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">[输出]:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="107b" class="no mj jj nk b gy np nq l nr ns">Length of text: 1115394 characters</span></pre><p id="989a" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">有大量的数据要处理！</p><p id="92af" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">查看文本前250个字符的数据:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="48b3" class="no mj jj nk b gy np nq l nr ns">print(text[:250])</span></pre><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/d364f768f6c5822101cf3fdc0d764e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RKt5B5xpiiYtHUL1UBImvQ.png"/></div></div></figure></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="c5b5" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">矢量化</h1><p id="461d" class="pw-post-body-paragraph lb lc jj le b lf na kk lh li nb kn lk ly nc ln lo lz nd lr ls ma ne lv lw lx im bi translated">首先，让我们看看文件中有多少不同类型的字符:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="7db7" class="no mj jj nk b gy np nq l nr ns">vocab = sorted(set(text))<br/>print ('{} unique characters'.format(len(vocab)))</span></pre><p id="2291" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">[输出]:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="1c0a" class="no mj jj nk b gy np nq l nr ns">65 unique characters</span></pre><p id="6ce6" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">在训练之前，需要将字符串映射为数字表示。下面创建了两个表，一个将字符映射到数字，另一个将数字映射到字符。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="56a4" class="no mj jj nk b gy np nq l nr ns">char2idx = {u:i for i, u in enumerate(vocab)}<br/>idx2char = np.array(vocab)<br/>text_as_int = np.array([char2idx[c] for c in text])</span></pre><p id="2ceb" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">查看矢量化词典:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="00f3" class="no mj jj nk b gy np nq l nr ns">print('{')<br/>for char,_ in zip(char2idx, range(20)):<br/>    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))<br/>print('  ...\n}')</span></pre><p id="e7b7" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">[输出]:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="7580" class="no mj jj nk b gy np nq l nr ns">{   <br/>'\n':   0,   <br/>' ' :   1,   <br/>'!' :   2,   <br/>'$' :   3,   <br/>'&amp;' :   4,   <br/>"'" :   5,   <br/>',' :   6,   <br/>'-' :   7,   <br/>'.' :   8,   <br/>'3' :   9,   <br/>':' :  10,   <br/>';' :  11,   <br/>'?' :  12,   <br/>'A' :  13,   <br/>'B' :  14,   <br/>'C' :  15,   <br/>'D' :  16,   <br/>'E' :  17,   <br/>'F' :  18,   <br/>'G' :  19,   <br/>... <br/>}</span></pre><p id="3b2c" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">每个独特的字符都有自己的编号。</p><p id="6eec" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">让我们看看矢量器如何处理该剧的前两个词“第一公民”。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="ff95" class="no mj jj nk b gy np nq l nr ns">print ('{} ---- characters mapped to int ---- &gt; {}'.format(repr(text[:13]), text_as_int[:13]))</span></pre><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/46d2453727423bf52e0a5a1c25db759b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7wBZv_zxxW0uHqliqE_20A.png"/></div></div></figure><p id="dfb4" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">单词被转换成一个数字向量，这个向量可以很容易地用整数到字符的字典转换回文本。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="70c4" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">创建培训数据</h1><p id="6dc2" class="pw-post-body-paragraph lb lc jj le b lf na kk lh li nb kn lk ly nc ln lo lz nd lr ls ma ne lv lw lx im bi translated">给定一个字符序列，模型将理想地找到最可能的下一个字符。文本将被分成几个句子，每个输入句子将包含一个来自文本的可变seq_length字符。任何输入句子的输出将是输入句子，向右移动一个字符。</p><p id="4f8b" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">例如，给定一个输入“Hell”，输出将是“ello”，形成单词“Hello”。</p><p id="6c27" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">首先，我们可以使用TensorFlow的handy。from_tensor_slices函数将文本向量转换为字符索引。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="c8dc" class="no mj jj nk b gy np nq l nr ns"># The maximum length sentence we want for a single input in characters<br/>seq_length = 100<br/>examples_per_epoch = len(text)//(seq_length+1)</span><span id="7703" class="no mj jj nk b gy nw nq l nr ns"># Create training examples / targets<br/>char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)</span><span id="ac69" class="no mj jj nk b gy nw nq l nr ns">for i in char_dataset.take(5):<br/>  print(idx2char[i.numpy()])</span></pre><p id="27a8" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">[输出]:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="d050" class="no mj jj nk b gy np nq l nr ns">F<br/>i<br/>r<br/>s<br/>t</span></pre><p id="ced9" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">批处理方法允许这些单个字符变成确定大小的序列，形成段落片段。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="819d" class="no mj jj nk b gy np nq l nr ns">sequences = char_dataset.batch(seq_length+1, drop_remainder=True)</span><span id="1e56" class="no mj jj nk b gy nw nq l nr ns">for item in sequences.take(5):<br/>  print(repr(''.join(idx2char[item.numpy()])))</span></pre><p id="b7c7" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">[输出]:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="90bf" class="no mj jj nk b gy np nq l nr ns">'First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou ' 'are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you k' "now Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us ki" "ll him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be d" 'one: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citi'</span></pre><p id="cb79" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">对于每个序列，我们将复制它，并使用map方法将其转换为输入和目标。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="5e68" class="no mj jj nk b gy np nq l nr ns">def split_input_target(chunk):<br/>    input_text = chunk[:-1]<br/>    target_text = chunk[1:]<br/>    return input_text, target_text</span><span id="d100" class="no mj jj nk b gy nw nq l nr ns">dataset = sequences.map(split_input_target)</span></pre><p id="cee0" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">现在，数据集有了所需的输入和输出:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="69d8" class="no mj jj nk b gy np nq l nr ns"><strong class="nk jk">Input data: </strong> 'First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou' <br/><strong class="nk jk">Target data:</strong> 'irst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou '</span></pre><p id="643c" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">向量的每个索引被处理为一个一次性步骤；对于时间步长0处的输入，模型接收“F”的数字索引，并尝试将“I”预测为下一个字符。在下一个时间步，它做同样的事情，但是RNN不仅考虑前面的步骤，而且考虑它刚刚预测的字符。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="74a8" class="no mj jj nk b gy np nq l nr ns">for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):<br/>    print("Step {:4d}".format(i))<br/>    print("  input: {} ({:s})".format(input_idx, repr(idx2char[input_idx])))<br/>    print("  expected output: {} ({:s})".format(target_idx, repr(idx2char[target_idx])))</span></pre><p id="231d" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">[输出]:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="9ae7" class="no mj jj nk b gy np nq l nr ns">Step    0   <br/>	input: 18 ('F')   <br/>	expected output: 47 ('i') <br/>Step    1   <br/>	input: 47 ('i')   <br/>	expected output: 56 ('r') <br/>Step    2   <br/>	input: 56 ('r')   <br/>	expected output: 57 ('s') <br/>Step    3   <br/>	input: 57 ('s')   <br/>	expected output: 58 ('t') <br/>Step    4   <br/>	input: 58 ('t')   <br/>	expected output: 1 (' ')</span></pre><p id="9bb4" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">Tensorflow的tf.data可以用来将文本分割成更易于管理的序列——但首先，数据需要被打乱并打包成批。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="5180" class="no mj jj nk b gy np nq l nr ns"># Batch size<br/>BATCH_SIZE = 64</span><span id="f8cf" class="no mj jj nk b gy nw nq l nr ns"># Buffer size to shuffle the dataset<br/>BUFFER_SIZE = 10000</span><span id="d2f5" class="no mj jj nk b gy nw nq l nr ns">dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)</span><span id="0e70" class="no mj jj nk b gy nw nq l nr ns">dataset</span></pre><p id="ba80" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">[输出]:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="6477" class="no mj jj nk b gy np nq l nr ns">&lt;BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)&gt;</span></pre></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="e88e" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">构建模型</h1><p id="eb0c" class="pw-post-body-paragraph lb lc jj le b lf na kk lh li nb kn lk ly nc ln lo lz nd lr ls ma ne lv lw lx im bi translated">最后，我们可以建立模型。让我们首先设置一些重要的变量:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="67ee" class="no mj jj nk b gy np nq l nr ns"># Length of the vocabulary in chars<br/>vocab_size = len(vocab)</span><span id="2efd" class="no mj jj nk b gy nw nq l nr ns"># The embedding dimension<br/>embedding_dim = 256</span><span id="2ee7" class="no mj jj nk b gy nw nq l nr ns"># Number of RNN units<br/>rnn_units = 1024</span></pre><p id="e57b" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">该模型将有一个嵌入或输入层，它将每个字符的数量映射到一个具有可变embedding_dim维度的向量。它将有一个大小单位= rnn_units的GRU图层(可替换为LSTM图层)。最后，输出图层将是标准的密集图层，具有vocab_size输出。</p><p id="3853" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">下面的函数帮助我们快速清晰地创建一个模型。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="9a72" class="no mj jj nk b gy np nq l nr ns">def build_model(vocab_size, embedding_dim, rnn_units, batch_size):<br/>  model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(vocab_size, embedding_dim,<br/>                              batch_input_shape=[batch_size, None]),<br/>    tf.keras.layers.GRU(rnn_units,<br/>                        return_sequences=True,<br/>                        stateful=True,<br/>                        recurrent_initializer='glorot_uniform'),<br/>    tf.keras.layers.Dense(vocab_size)<br/>  ])<br/>  return model</span></pre><p id="3290" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">通过调用函数来组合模型架构。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="c31a" class="no mj jj nk b gy np nq l nr ns">model = build_model(<br/>  vocab_size = len(vocab),<br/>  embedding_dim=embedding_dim,<br/>  rnn_units=rnn_units,<br/>  batch_size=BATCH_SIZE)</span></pre><p id="a56e" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">让我们总结一下我们的模型，看看有多少个参数。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="f15d" class="no mj jj nk b gy np nq l nr ns">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding (Embedding)        (64, None, 256)           16640     <br/>_________________________________________________________________<br/>gru (GRU)                    (64, None, 1024)          3938304   <br/>_________________________________________________________________<br/>dense (Dense)                (64, None, 65)            66625     <br/>=================================================================<br/>Total params: 4,021,569<br/>Trainable params: 4,021,569<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="c11d" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">400万参数！我们可以期待一个很长的训练时间。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="fff9" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">收集</h1><p id="722d" class="pw-post-body-paragraph lb lc jj le b lf na kk lh li nb kn lk ly nc ln lo lz nd lr ls ma ne lv lw lx im bi translated">这个问题现在可以当作一个分类问题。给定前一个RNN状态和输入的时间步长，预测代表下一个字符的类。</p><p id="c499" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">因此，我们将附加一个稀疏分类交叉熵损失函数和Adam优化器。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="1fe2" class="no mj jj nk b gy np nq l nr ns">def loss(labels, logits):<br/>  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)</span><span id="abcf" class="no mj jj nk b gy nw nq l nr ns">example_batch_loss  = loss(target_example_batch, example_batch_predictions)<br/>print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")<br/>print("scalar_loss:      ", example_batch_loss.numpy().mean())<br/>model.compile(optimizer='adam', loss=loss)</span></pre><p id="b864" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">[输出]:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="ff76" class="no mj jj nk b gy np nq l nr ns">Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size) <br/>scalar_loss:       4.1746616</span></pre></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="963e" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">配置检查点</h1><p id="c402" class="pw-post-body-paragraph lb lc jj le b lf na kk lh li nb kn lk ly nc ln lo lz nd lr ls ma ne lv lw lx im bi translated">模型训练，尤其是对于像莎士比亚戏剧这样的庞大数据集，需要<em class="ld">很长的</em>时间。理想情况下，我们不想为了预测而重复训练它。TF . keras . callbacks . model check point有助于将训练期间特定检查点的权重保存到一个文件中，该文件可以在以后检索并在空白模型中实现。这在训练因任何原因中断的情况下也很方便。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="ea95" class="no mj jj nk b gy np nq l nr ns"># Directory where the checkpoints will be saved<br/>checkpoint_dir = './training_checkpoints'<br/># Name of the checkpoint files<br/>checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")</span><span id="8423" class="no mj jj nk b gy nw nq l nr ns">checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(<br/>    filepath=checkpoint_prefix,<br/>    save_weights_only=True)</span></pre></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="dfbd" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">最后，执行训练。</h1><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="5ce5" class="no mj jj nk b gy np nq l nr ns">EPOCHS=30<br/>history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])</span></pre><p id="5c99" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">这需要大约6个小时才能得到不太明显但更快的结果。纪元可以调整到10(任何小于5，这将是完全的垃圾)。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="30c6" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">生成文本</h1><p id="e29c" class="pw-post-body-paragraph lb lc jj le b lf na kk lh li nb kn lk ly nc ln lo lz nd lr ls ma ne lv lw lx im bi translated">从检查点恢复权重:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="71a8" class="no mj jj nk b gy np nq l nr ns">tf.train.latest_checkpoint(checkpoint_dir)</span></pre><p id="e5c4" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">我们可以用权重重建模型:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="47be" class="no mj jj nk b gy np nq l nr ns">model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)</span><span id="e9ab" class="no mj jj nk b gy nw nq l nr ns">model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))</span><span id="490c" class="no mj jj nk b gy nw nq l nr ns">model.build(tf.TensorShape([1, None]))</span></pre><p id="b7b3" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">生成文本的游戏计划:</p><ol class=""><li id="188e" class="nx ny jj le b lf lg li lj ly nz lz oa ma ob lx oc od oe of bi translated">首先选择一个种子字符串，初始化RNN状态，并设置要生成的字符数。</li><li id="52b1" class="nx ny jj le b lf og li oh ly oi lz oj ma ok lx oc od oe of bi translated">使用起始字符串和RNN状态获得下一个字符的预测分布。</li><li id="5609" class="nx ny jj le b lf og li oh ly oi lz oj ma ok lx oc od oe of bi translated">使用分类分布计算预测字符的索引，将其用作模型的下一个输入。</li><li id="2043" class="nx ny jj le b lf og li oh ly oi lz oj ma ok lx oc od oe of bi translated">模型返回的RNN状态反馈到自身。</li><li id="e10f" class="nx ny jj le b lf og li oh ly oi lz oj ma ok lx oc od oe of bi translated">重复步骤2和4，直到生成文本。</li></ol><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="bf52" class="no mj jj nk b gy np nq l nr ns">def generate_text(model, start_string):<br/>  # Evaluation step (generating text using the learned model)</span><span id="b1b9" class="no mj jj nk b gy nw nq l nr ns">  # Number of characters to generate<br/>  num_generate = 1000</span><span id="2096" class="no mj jj nk b gy nw nq l nr ns">  # Converting our start string to numbers (vectorizing)<br/>  input_eval = [char2idx[s] for s in start_string]<br/>  input_eval = tf.expand_dims(input_eval, 0)</span><span id="759b" class="no mj jj nk b gy nw nq l nr ns">  # Empty string to store our results<br/>  text_generated = []</span><span id="aae1" class="no mj jj nk b gy nw nq l nr ns">  # Low temperatures results in more predictable text.<br/>  # Higher temperatures results in more surprising text.<br/>  # Experiment to find the best setting.<br/>  temperature = 1.0</span><span id="5db0" class="no mj jj nk b gy nw nq l nr ns">  # Here batch size == 1<br/>  model.reset_states()<br/>  for i in range(num_generate):<br/>      predictions = model(input_eval)<br/>      # remove the batch dimension<br/>      predictions = tf.squeeze(predictions, 0)</span><span id="4a62" class="no mj jj nk b gy nw nq l nr ns">      # using a categorical distribution to predict the character returned by the model<br/>      predictions = predictions / temperature<br/>      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()</span><span id="0773" class="no mj jj nk b gy nw nq l nr ns">      # We pass the predicted character as the next input to the model<br/>      # along with the previous hidden state<br/>      input_eval = tf.expand_dims([predicted_id], 0)</span><span id="6ad2" class="no mj jj nk b gy nw nq l nr ns">      text_generated.append(idx2char[predicted_id])</span><span id="fa14" class="no mj jj nk b gy nw nq l nr ns">  return (start_string + ''.join(text_generated))</span></pre><p id="406c" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">最后，给定一个起始字符串，我们可以生成一些有趣的文本。</p><p id="9f5d" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">而现在，欣赏两个rnn写的剧，一个是10个时代后训练出来的，一个是30个时代后训练出来的。</p><h2 id="4db6" class="no mj jj bd mk ol om dn mo on oo dp ms ly op oq mu lz or os mw ma ot ou my ov bi translated">经过10个时期的训练后</h2><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="9db7" class="no mj jj nk b gy np nq l nr ns">print(generate_text(model, start_string=u"ROMEO: "))</span></pre><blockquote class="ky kz la"><p id="62ee" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">罗密欧我是多么的不值得你拥有，但却是爱。</p><p id="b873" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">朱丽叶:去吧，打吧，先生；我们说‘是’，就站着不去；把他带走。唉，我的年轻；一个男人从他的主人那里听到了你。</p><p id="ee21" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">国王理查德三世:来吧，停止。哦，兄弟，时光的变形！你会很安静。</p><p id="bc94" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">鲍莉娜:我想你说的是时间！啊，你公司的洞；但是，好我的主；我们有一个和平的国王？</p><p id="6ec3" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">卡杜尔和瓦希他能吗！我可以勾引她。</p><p id="da3a" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">格洛斯特:有你在这里，就能磨炼出这样的脾气；或者他的高贵的提议和速度，看起来你的装饰在一个虚弱的amidude借此给dother，dods公民。</p><p id="244d" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">第三个市民:<br/>甜甜夫人给了奖赏，让他们造反的消息不胫而走！采摘屈服:这标志着所有十次争吵中risess的事情，简单地菜他的finmers。</p><p id="9c7f" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">朱丽叶:<br/>先生们，上帝夏娃来了，就像它的妻子一样——凯旋的夜晚改变了你们的神，你们去了:<br/>将分散和法兰西。</p></blockquote><p id="9edc" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">哇！仅仅过了10个时代，就有了令人印象深刻的理解水平。单词拼写的准确性令人怀疑，但有明显的情节/冲突。写作肯定可以改进。希望30历元模型表现更好。</p><h2 id="29a9" class="no mj jj bd mk ol om dn mo on oo dp ms ly op oq mu lz or os mw ma ot ou my ov bi translated">经过30个时期的训练</h2><p id="7ced" class="pw-post-body-paragraph lb lc jj le b lf na kk lh li nb kn lk ly nc ln lo lz nd lr ls ma ne lv lw lx im bi translated">享受这个完全由RNN写的角色对角色的游戏吧！</p><blockquote class="ky kz la"><p id="d398" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">布鲁图斯:<br/>你会怕他吗，我们的两个，<br/>但是很害怕；为此我们在那不勒斯的树荫下。<br/>这里没有增加假给t，offorit又是白给的战争。这是女王，她的头是当之无愧的。<br/>但cere它是一个女巫，一些安慰。什么，奶妈，我说！<br/>加油哈默尔。</p><p id="b89d" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">她应该跪下吗？<br/>在不得哀哭受了；别忘了我<br/>和不尊敬的问候，而不是住在，你，<br/>看着我，在天上的儿子，<br/>永远你是我的父亲，但很严格；<br/>除非你想重新拥有他，否则你总是大声嚷嚷。真诚的，我可以像行为一样修复天堂狄尔斯<br/>的脾气只要另一个少女在这里，他被禁止在春天；在你的窥视下，我没有惊雷；还有我的好反派！消除彼此的睡意。<br/>傻瓜；如果这种公事公办的职责<br/>确实让这些叛徒们其他的悲哀。</p><p id="77ca" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">告诉我，他们是可敬的。</p><p id="5653" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">牧羊人:<br/>我知道，大人，要去伦敦，而你我搬来加入他的麾下，<br/>伟大的阿波罗·斯坦要作一本书，<br/>两者都还没定，我父亲就朝科文特走去了。啧啧，你还是地球人的主r明智的你的母亲吗？</p><p id="7350" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">去吧，威尔！<br/>我们还没有集合，因为你不会:你在狂怒中在你的同伴中得了好疯，我要你这样战斗，他的眼睛每天都这样，<br/>发誓这样的光束检测，<br/>克拉伦斯死了来召唤你我感谢你的恩典，我的父亲和我的父亲，还有你的胜利<br/>我的父亲，有一把剑在这里；也不是当你的心变得严肃。</p><p id="c7b5" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">玛格丽特女王:<br/>你是一个很好的住宿和感谢<br/>与他在一起。<br/>但是现在手头有了:<br/>所以罗密欧死了也是可能的。</p><p id="de0f" class="lb lc ld le b lf lg kk lh li lj kn lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">米尼涅斯:<br/>哈！小非常欢迎我女儿的剑，<br/>也许我的祈祷者的腿，就像他一样。我是班克斯，先生，我会让你说‘不；因为这里现在这样更好，派它来:它是陌生的。</p></blockquote><p id="d439" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">哇！引人入胜——在某些情况下，模特甚至学会了押韵(尤其是Florizel的台词)。想象一下RNN在50甚至100个纪元后会写下什么！</p><p id="60be" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">如果你想看更多RNN写的剧本，以及你可以派生、编辑和运行的内核，去我的Kaggle内核<a class="ae jg" href="https://www.kaggle.com/washingtongold/creating-a-shakespearean-play-writer" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="777a" class="mi mj jj bd mk ml mm mn mo mp mq mr ms kp mt kq mu ks mv kt mw kv mx kw my mz bi translated">嗯，我想我只是让作家失业了。</h1><p id="12a7" class="pw-post-body-paragraph lb lc jj le b lf na kk lh li nb kn lk ly nc ln lo lz nd lr ls ma ne lv lw lx im bi translated">不完全是——但我可以想象未来人工智能会发表旨在传播的文章。这里有一个挑战——收集与某个主题相关的顶级文章，比如说，关于人体器官或另一个类似出版物的文章，并训练一个人工智能撰写趋势文章。一个字符一个字符地张贴RNN输出的内容，看看它是如何完成的！注意——我不建议在更专业的出版物上培训RNN，如《走向数据科学》或《更好的编程》,因为这需要RNN在合理的时间内无法学习的技术知识。然而，在RNN目前的能力范围内，更多的哲学和非技术写作是正确的。</p><p id="86cb" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">随着文本生成变得越来越先进，它将有可能比人类写得更好，因为它关注什么内容会像病毒一样传播，什么措辞会让读者感觉良好，等等。令人震惊和惊奇的是，有一天，机器可能会在可以说是最像人类的事情——写作——上打败人类。没错，它将无法真正理解它写的东西——但是它已经掌握了人类的交流方式。</p><p id="80a9" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">我猜如果你不能打败他们，加入他们！</p><p id="4dc6" class="pw-post-body-paragraph lb lc jj le b lf lg kk lh li lj kn lk ly lm ln lo lz lq lr ls ma lu lv lw lx im bi translated">如果你喜欢这篇文章，请随意看看我的其他作品。<br/>如果这篇文章对你来说有点太专业了，看看这篇关于模仿乔·拜登和伯尼·桑德斯言论的更简单的教程。</p></div></div>    
</body>
</html>