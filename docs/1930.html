<html>
<head>
<title>K-Means Clustering: Techniques to Find the Optimal Clusters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类:寻找最佳聚类的技术</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/k-means-clustering-techniques-to-find-the-optimal-clusters-7eea5431a4fb?source=collection_archive---------0-----------------------#2021-06-20">https://pub.towardsai.net/k-means-clustering-techniques-to-find-the-optimal-clusters-7eea5431a4fb?source=collection_archive---------0-----------------------#2021-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b9c3" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="e089" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用肘方法、轮廓评分和间隙统计在K-Means聚类中查找最佳聚类。</h2></div><p id="9559" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，您将了解</p><ul class=""><li id="1220" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">什么是K-Means聚类？</li><li id="7d1f" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">K-Means是如何工作的？</li><li id="d3c2" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">K-均值聚类的应用</li><li id="1016" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">K-Means聚类在Python中的实现</li><li id="ef1a" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">使用肘方法、轮廓分数和间隙统计寻找最佳聚类</li></ul><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/05453feb16f79a8713d0ec06818844ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*dUdeLbEguDm3W6Abl5DA3g.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">作者图片</figcaption></figure><blockquote class="mn"><p id="61e0" class="mo mp it bd mq mr ms mt mu mv mw lm dk translated">K-Means聚类是一种简单、流行但功能强大的无监督机器学习算法。<strong class="ak">一种迭代算法，为一个未标记的数据集寻找具有相似特征的数据组，并将其分成簇</strong>。</p></blockquote><p id="251c" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated"><strong class="kt jd">K-Means算法旨在根据定义的聚类数K获得内聚的聚类。它通过最小化</strong>总的类内变化(称为<strong class="kt jd">类内平方和</strong> (WCSS)来创建内聚的紧凑聚类。</p><p id="7eaf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">K-Means算法<strong class="kt jd">从随机选择的指定簇数的质心</strong>开始。质心是群集的中心。必须有策略地选择<strong class="kt jd">质心</strong>，因为不同的位置会产生不同的结果。</p><p id="d21f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后<strong class="kt jd">算法通过将每个数据点分配给一个聚类，基于它与所有其他聚类相比与特定聚类的接近程度，迭代地改进质心位置</strong>。数据点与质心的接近程度基于它们的欧几里德平方距离。</p><p id="7c82" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">K-Means聚类可以使用以下任何一种距离度量</p><ul class=""><li id="9427" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">欧几里得距离</li><li id="8da6" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">曼哈顿距离</li><li id="e047" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">平方欧几里得距离</li><li id="e724" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">余弦距离</li></ul><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/3974d985619d6f32eaceb9949e4911d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p02g9UDbXXUHUnYkk_ThFA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">资料来源:https://stanford.edu/~cpiech/cs221/handouts/kmeans.html</figcaption></figure><p id="7857" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一旦所有的数据点都被分配到一个聚类中，计算新的质心并将其放入每个聚类中。基于新的质心，根据其与质心的接近程度，查找是否需要将任何数据点重新分配给新的聚类。</p><p id="b12d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> K-Means通过在</strong>之间交替来迭代地寻找一个聚类的最佳质心</p><ul class=""><li id="352e" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><strong class="kt jd">根据数据点与当前质心的接近程度将数据点分配给聚类，以及</strong></li><li id="964b" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">基于数据点到聚类的当前分配重新计算质心。</strong></li></ul><p id="5420" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当出现以下情况时，K-Means算法会停止聚类优化</p><ul class=""><li id="e4f7" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><strong class="kt jd">当聚类的数据点不再变化时，聚类质心已经稳定，或者</strong></li><li id="d75d" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">算法已经完成了指定次数的迭代。</strong></li></ul><p id="794b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">k-均值算法用于</p><ul class=""><li id="1e0f" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><strong class="kt jd">像欺诈检测一样的异常检测</strong></li><li id="2dd8" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">分割像客户分割、图像分割</strong></li><li id="6358" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">数据挖掘</strong></li></ul><h2 id="031f" class="ni nj it bd nk nl nm dn nn no np dp nq la nr ns nt le nu nv nw li nx ny nz iz bi translated"><strong class="ak">K-Means的实现</strong></h2><p id="e647" class="pw-post-body-paragraph kr ks it kt b ku oa kd kw kx ob kg kz la oc lc ld le od lg lh li oe lk ll lm im bi translated"><a class="ae nh" href="https://www.kaggle.com/shwetabh123/mall-customers" rel="noopener ugc nofollow" target="_blank">商城客户数据集</a></p><p id="e1e1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">读取数据框中的CSV文件，将分类变量转换为数值数据类型，然后使用PCA(主成分分析)降低维度</p><pre class="mc md me mf gt of og oh oi aw oj bi"><span id="be37" class="ni nj it og b gy ok ol l om on"># Import the required libraries<br/><strong class="og jd">import numpy as np <br/>import pandas as pd <br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.cluster import KMeans<br/>from sklearn.preprocessing import LabelBinarizer<br/>from sklearn.decomposition import PCA</strong></span><span id="7fd1" class="ni nj it og b gy oo ol l om on"># Read he dataset into a dataframe<br/><strong class="og jd">dataset = pd.read_csv('Mall_Customers.csv',index_col='CustomerID')</strong></span><span id="d157" class="ni nj it og b gy oo ol l om on"># Drop duplicates<br/><strong class="og jd">dataset.drop_duplicates(inplace=True)</strong></span><span id="0ab4" class="ni nj it og b gy oo ol l om on"># Converting categorical column Genre to onehot vector<br/><strong class="og jd">label_binarizer = LabelBinarizer()</strong><br/>  <br/>#use LabelBinarizer for gender<br/><strong class="og jd">label_binarizer_output = label_binarizer.fit_transform( dataset['Genre'])</strong></span><span id="5d70" class="ni nj it og b gy oo ol l om on">#Adding the categorical and the main dataframe into one dataframe<br/><strong class="og jd">result_df = pd.DataFrame(label_binarizer_output, columns=['Gender_1'])<br/>dataset_1= pd.concat([dataset, result_df], axis=1, join='inner')</strong></span><span id="4ea4" class="ni nj it og b gy oo ol l om on"># Creating the input variable<br/><strong class="og jd">X= dataset_1.iloc[:, [1,2,3,4]].values</strong></span><span id="3866" class="ni nj it og b gy oo ol l om on"># reducing the input dimension using PCA<br/><strong class="og jd">reduced_data = PCA(n_components=2).fit_transform(X)</strong></span></pre><p id="2c4a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当我们运行KMeans时，我们需要提供<strong class="kt jd"> n_clusters，它表示要形成的最佳聚类数</strong>和要为数据集生成的质心数。</p><pre class="mc md me mf gt of og oh oi aw oj bi"><span id="d183" class="ni nj it og b gy ok ol l om on"><strong class="og jd">kmeans = KMeans(init="k-means++", n_clusters=5, n_init=4)<br/>kmeans.fit(X)</strong></span></pre><p id="a69c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="op">寻找最优聚类的最好方法是什么？</em>T15】</strong></p><p id="8e94" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于KMeans聚类算法，有3种常用的方法来查找最佳聚类。</p><h2 id="903f" class="ni nj it bd nk nl nm dn nn no np dp nq la nr ns nt le nu nv nw li nx ny nz iz bi translated"><strong class="ak">肘法</strong></h2><p id="b35c" class="pw-post-body-paragraph kr ks it kt b ku oa kd kw kx ob kg kz la oc lc ld le od lg lh li oe lk ll lm im bi translated">在肘方法中，对不同数量的聚类运行K-Means算法，以找到每个数据点到聚类质心的平方和，也称为<strong class="kt jd">聚类内平方和。</strong></p><p id="e2f4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">绘制WCSS以及K个簇的数量。<strong class="kt jd">在WCSS开始变平或突然下降的图上拾取K值，再增加一个集群也没有明显改善，形成一个弯头</strong>。肘形被认为是数据集最佳聚类的指示符。</p><pre class="mc md me mf gt of og oh oi aw oj bi"><span id="e301" class="ni nj it og b gy ok ol l om on"># Using the elbow method to find the optimal number of clusters<br/><strong class="og jd">max_k=11 </strong># max no. of clusters to be evaluated<br/><strong class="og jd">wcss = []<br/>for i in range(1, max_k):<br/>    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)<br/>    kmeans.fit(reduced_data)</strong><br/>    # inertia method returns wcss for that model<br/>    <strong class="og jd">wcss.append(kmeans.inertia_)</strong></span><span id="4e8f" class="ni nj it og b gy oo ol l om on">#plotting the data <br/><strong class="og jd">plt.figure(figsize=(5,3))<br/>sns.lineplot(range(1, max_k), wcss,marker='o',color='red')<br/>plt.title('The Elbow Method')<br/>plt.xlabel('Number of clusters')<br/>plt.ylabel('WCSS')<br/>plt.show()</strong></span></pre><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/2c39407d9a089830a2fd9eaab0daa3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*UcMav0-T9vj7s7N8_CZ1aA.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">肘形法-圆圈显示数据集的最佳聚类数。</figcaption></figure><h2 id="361d" class="ni nj it bd nk nl nm dn nn no np dp nq la nr ns nt le nu nv nw li nx ny nz iz bi translated"><strong class="ak">剪影评分</strong></h2><p id="942d" class="pw-post-body-paragraph kr ks it kt b ku oa kd kw kx ob kg kz la oc lc ld le od lg lh li oe lk ll lm im bi translated"><strong class="kt jd">侧影得分是检查聚类紧密度的另一个指标，以确定聚类是否良好</strong>。</p><p id="9bfe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">轮廓分数<strong class="kt jd">像肘方法一样计算平均聚类内距离</strong>以及平均最近聚类。这表示与所有其他聚类质心相比，数据点与其自己的聚类有多相似。因此，<strong class="kt jd">轮廓分数测量同一个聚类的紧密度和同其他聚类的分离度</strong>。</p><p id="d110" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">如果所有数据点的轮廓得分的值相当高，则给出聚类的最佳值</strong>。剪影得分使用闵可夫斯基距离或欧几里德距离，其值的范围为[-1，1]</p><p id="b756" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">以下代码改编自<a class="ae nh" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/cluster/plot _ k means _ silhouette _ analysis . html #</a></p><p id="b118" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">代码被修改以使用通过PCA减少的数据维度</p><pre class="mc md me mf gt of og oh oi aw oj bi"><span id="53bd" class="ni nj it og b gy ok ol l om on"><strong class="og jd">from sklearn.metrics import silhouette_samples, silhouette_score<br/>import matplotlib.cm as cm<br/>for n_clusters in range(2, max_k):</strong><br/>    # Create a subplot with 1 row and 2 columns<br/>    <strong class="og jd">fig, (ax1, ax2) = plt.subplots(1, 2)<br/>    fig.set_size_inches(18, 7)</strong></span><span id="8870" class="ni nj it og b gy oo ol l om on"># The 1st subplot is the silhouette plot<br/>    # The silhouette coefficient can range from -1, 1 but in this example all<br/>    # lie within [-0.1, 1]<br/>    <strong class="og jd">ax1.set_xlim([-0.1, 1])</strong><br/>    # The (n_clusters+1)*10 is for inserting blank space between silhouette<br/>    # plots of individual clusters, to demarcate them clearly.<br/>    <strong class="og jd">ax1.set_ylim([0, len(reduced_data) + (n_clusters + 1) * 10])</strong></span><span id="499d" class="ni nj it og b gy oo ol l om on"># Initialize the clusterer with n_clusters value and a random generator<br/>    # seed of 10 for reproducibility.<br/>    <strong class="og jd">clusterer = KMeans(n_clusters=n_clusters, random_state=10)<br/>    cluster_labels = clusterer.fit_predict(reduced_data)</strong></span><span id="1031" class="ni nj it og b gy oo ol l om on"># The silhouette_score gives the average value for all the samples.<br/>    # This gives a perspective into the density and separation of the formed<br/>    # clusters<br/>    <strong class="og jd">silhouette_avg = silhouette_score(reduced_data, cluster_labels)</strong><br/>   <strong class="og jd"> print("For n_clusters =", n_clusters,"The average silhouette_score is :", silhouette_avg)</strong></span><span id="4c09" class="ni nj it og b gy oo ol l om on"># Compute the silhouette scores for each sample<br/>    <strong class="og jd">sample_silhouette_values = silhouette_samples(reduced_data, cluster_labels)</strong></span><span id="3c7e" class="ni nj it og b gy oo ol l om on"><strong class="og jd">y_lower = 10</strong><br/>    <strong class="og jd">for i in range(n_clusters):</strong><br/>        # Aggregate the silhouette scores for samples belonging to<br/>        # cluster i, and sort them<br/>        <strong class="og jd">ith_cluster_silhouette_values = \<br/>            sample_silhouette_values[cluster_labels == i]</strong></span><span id="f621" class="ni nj it og b gy oo ol l om on"><strong class="og jd">ith_cluster_silhouette_values.sort()</strong></span><span id="7201" class="ni nj it og b gy oo ol l om on"><strong class="og jd">size_cluster_i = ith_cluster_silhouette_values.shape[0]<br/>        y_upper = y_lower + size_cluster_i</strong></span><span id="5596" class="ni nj it og b gy oo ol l om on"><strong class="og jd">color = cm.nipy_spectral(float(i) / n_clusters)<br/>        ax1.fill_betweenx(np.arange(y_lower, y_upper),<br/>                          0, ith_cluster_silhouette_values,<br/>                          facecolor=color, edgecolor=color, alpha=0.7)</strong></span><span id="9c18" class="ni nj it og b gy oo ol l om on"># Label the silhouette plots with their cluster numbers at the middle<br/>       <strong class="og jd"> ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))</strong></span><span id="9112" class="ni nj it og b gy oo ol l om on"># Compute the new y_lower for next plot<br/>      <strong class="og jd">  y_lower = y_upper + 10</strong>  # 10 for the 0 samples</span><span id="adbe" class="ni nj it og b gy oo ol l om on"><strong class="og jd">ax1.set_title("The silhouette plot for the various clusters.")<br/>    ax1.set_xlabel("The silhouette coefficient values")<br/>    ax1.set_ylabel("Cluster label")</strong></span><span id="dfc9" class="ni nj it og b gy oo ol l om on"># The vertical line for average silhouette score of all the values<br/>   <strong class="og jd"> ax1.axvline(x=silhouette_avg, color="red", linestyle="--")</strong></span><span id="00b1" class="ni nj it og b gy oo ol l om on"><strong class="og jd">ax1.set_yticks([])  # Clear the yaxis labels / ticks<br/>    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])</strong></span><span id="fb17" class="ni nj it og b gy oo ol l om on"># 2nd Plot showing the actual clusters formed<br/>    <strong class="og jd">colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)<br/>    ax2.scatter(reduced_data[:, 0], reduced_data[:, 1], marker='.', s=30, lw=0, alpha=0.7,<br/>                c=colors, edgecolor='k')</strong></span><span id="6443" class="ni nj it og b gy oo ol l om on"># Labeling the clusters<br/>   <strong class="og jd"> centers = clusterer.cluster_centers_</strong><br/>    # Draw white circles at cluster centers<br/>   <strong class="og jd"> ax2.scatter(centers[:, 0], centers[:, 1], marker='o',<br/>                c="white", alpha=1, s=200, edgecolor='k')</strong></span><span id="7f1c" class="ni nj it og b gy oo ol l om on"><strong class="og jd">for i, c in enumerate(centers):<br/>        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,<br/>                    s=50, edgecolor='k')</strong></span><span id="39cc" class="ni nj it og b gy oo ol l om on"><strong class="og jd">ax2.set_title("The visualization of the clustered data.")<br/>    ax2.set_xlabel("Feature space for the 1st feature")<br/>    ax2.set_ylabel("Feature space for the 2nd feature")</strong></span><span id="2111" class="ni nj it og b gy oo ol l om on"><strong class="og jd">plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "<br/>                  "with n_clusters = %d" % n_clusters),<br/>                 fontsize=14, fontweight='bold')</strong></span><span id="c622" class="ni nj it og b gy oo ol l om on"><strong class="og jd">plt.show()</strong></span></pre><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi or"><img src="../Images/0b269e7db9573319db83a6ac6d26f7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*BoCJze8lLJ-JunLdA_SydQ.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">轮廓分数-显示5是聚类的最佳数量</figcaption></figure><p id="8996" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">侧影图推断，对于给定的数据，5是最佳聚类的良好选择。通过存在具有高于平均轮廓分数的聚类和轮廓图大小的适当波动来评估最佳聚类。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d0929c350160070abab9610925e5fbde.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*OVoUe8rJVf-1A9OvM9Lbtw.png"/></div></figure><p id="3571" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="op">肘法和剪影评分均给出最优聚类为5 </em> </strong></p><p id="1b60" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">以最佳分类为5运行KMeans，然后在reduced_data上使用最佳分类绘制散点图，以可视化分类</p><pre class="mc md me mf gt of og oh oi aw oj bi"><span id="6090" class="ni nj it og b gy ok ol l om on"># Fitting K-Means to the dataset<br/><strong class="og jd">kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)<br/>y_kmeans = kmeans.fit_predict(reduced_data)</strong></span><span id="9d7f" class="ni nj it og b gy oo ol l om on">#Visualising the clusters<br/><strong class="og jd">plt.figure(figsize=(15,7))<br/>sns.scatterplot(reduced_data[y_kmeans == 0, 0], reduced_data[y_kmeans == 0, 1], color = 'yellow', label = 'Cluster 1',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 1, 0], reduced_data[y_kmeans == 1, 1], color = 'blue', label = 'Cluster 2',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 2, 0], reduced_data[y_kmeans == 2, 1], color = 'green', label = 'Cluster 3',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 3, 0], reduced_data[y_kmeans == 3, 1], color = 'grey', label = 'Cluster 4',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 4, 0], reduced_data[y_kmeans == 4, 1], color = 'orange', label = 'Cluster 5',s=50)</strong></span><span id="7caa" class="ni nj it og b gy oo ol l om on"><strong class="og jd">sns.scatterplot(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color = 'red', <br/>                label = 'Centroids',s=300,marker=',')<br/>plt.grid(False)<br/>plt.title('Clusters of customers')<br/>plt.xlabel('PC-1')<br/>plt.ylabel('PC-2')<br/>plt.legend()<br/>plt.show()</strong></span></pre><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ot"><img src="../Images/8fff73ab3659b9e9a97e123891e7375e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0jqDOgu4M4g1MdHjVa80-Q.png"/></div></div></figure><h2 id="a349" class="ni nj it bd nk nl nm dn nn no np dp nq la nr ns nt le nu nv nw li nx ny nz iz bi translated"><strong class="ak">差距统计</strong></h2><p id="9097" class="pw-post-body-paragraph kr ks it kt b ku oa kd kw kx ob kg kz la oc lc ld le od lg lh li oe lk ll lm im bi translated">间隙统计是另一种确定最佳聚类数的流行技术，可以应用于任何聚类方法。</p><p id="44e8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">差距统计的概念是将数据上的聚类惯性与其在适当的空参考数据集下的期望值进行比较。</strong>零参考数据集可以从正态分布或均匀分布中采样。<strong class="kt jd">K的最佳选择是使数据集的类内惯性和空引用数据集之间的差距统计最大化的值。</strong></p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/1cdcc904f4a1abf8ee971a58cff41d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*OQHmo0_pDIkNNKfquYGM9w.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">来源:<a class="ae nh" href="https://statweb.stanford.edu/~gwalther/gap" rel="noopener ugc nofollow" target="_blank">https://statweb.stanford.edu/~gwalther/gap</a></figcaption></figure><p id="726c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"/></p><p id="cf13" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">以下代码灵感来源于<a class="ae nh" href="https://glowingpython.blogspot.com/2019/01/a-visual-introduction-to-gap-statistics.html" rel="noopener ugc nofollow" target="_blank">https://glowingpython . blogspot . com/2019/01/a-visual-introduction-to-gap-statistics . html</a></p><p id="c29c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">空引用数据集是一组均匀分布的点</p><pre class="mc md me mf gt of og oh oi aw oj bi"><span id="7777" class="ni nj it og b gy ok ol l om on"><strong class="og jd">from sklearn.datasets import make_blobs<br/>from sklearn.metrics import pairwise_distances</strong></span><span id="2077" class="ni nj it og b gy oo ol l om on"># creating a uniform distributed null refernced dataset<br/><strong class="og jd">reference = np.random.rand(100, 2)<br/>plt.figure(figsize=(12, 3))<br/>for k in range(1,6):<br/>    kmeans = KMeans(n_clusters=k)<br/>    a = kmeans.fit_predict(reference)<br/>    plt.subplot(1,5,k)<br/>    plt.scatter(reference[:, 0], reference[:, 1], c=a)<br/>    plt.xlabel('k='+str(k))<br/>plt.tight_layout()<br/>plt.show()</strong></span></pre><p id="a90a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">由于这些点是均匀分布的，KMeans算法会将这些点均匀地分成K个聚类，即使它们之间没有分离</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ov"><img src="../Images/d637328187e0314b4766c9bf1303327b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TVY2M5zIeVRpPd2SdKNLw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk translated">使用KMeans聚集的空引用统一分布数据集</figcaption></figure><pre class="mc md me mf gt of og oh oi aw oj bi"><span id="39fd" class="ni nj it og b gy ok ol l om on"><strong class="og jd">def compute_inertia(a, X):<br/>    W = [np.mean(pairwise_distances(X[a == c, :])) for c in np.unique(a)]<br/>    return np.mean(W)</strong></span><span id="665b" class="ni nj it og b gy oo ol l om on"><strong class="og jd">def compute_gap(clustering, data, k_max=5, n_references=5):<br/>    if len(data.shape) == 1:<br/>        data = data.reshape(-1, 1)</strong><br/>    # The Null Reference dataset that is uniformly distributed<br/>   <strong class="og jd"> reference = np.random.rand(*data.shape)<br/>    reference_inertia = []</strong><br/>    <br/>    # Calculate the WCSS for the null refrenced data<br/>   <strong class="og jd"> for k in range(1, k_max+1):<br/>        local_inertia = []<br/>        for _ in range(n_references):<br/>            clustering.n_clusters = k<br/>            assignments = clustering.fit_predict(reference)<br/>            local_inertia.append(compute_inertia(assignments, reference))<br/>        reference_inertia.append(np.mean(local_inertia))<br/>    </strong><br/>    # Calculate the WCSS for the data<br/>    <strong class="og jd">ondata_inertia = []<br/>    for k in range(1, k_max+1):<br/>        clustering.n_clusters = k<br/>        assignments = clustering.fit_predict(data)<br/>        ondata_inertia.append(compute_inertia(assignments, data))</strong><br/>        <br/>    # Calculate the gao statistics between the WCSS for the null referenced data and the WCSS for the data<br/>   <strong class="og jd"> gap = np.log(reference_inertia)-np.log(ondata_inertia)<br/>    return gap, np.log(reference_inertia), np.log(ondata_inertia)</strong></span><span id="3a3f" class="ni nj it og b gy oo ol l om on"><br/><strong class="og jd">gap, reference_inertia, ondata_inertia = compute_gap(KMeans(), reduced_data, k_max)</strong><br/><strong class="og jd">plt.plot(range(1, k_max+1), gap, '-o')<br/>plt.ylabel('gap')<br/>plt.xlabel('k')</strong></span></pre><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ow"><img src="../Images/1cb9c559e81d84531eda3288713e8fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*-mX8tF6PNH5F5jNvcwQ8mA.png"/></div></div></figure><p id="7eb9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">间隙统计根据数据上的<strong class="kt jd">群集惯性和空参考数据之间的最大间隙，给出群集的最佳数量为10。</strong></p><p id="93c2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对10个分类运行KMeans，然后在散点图中显示这些分类。</p><pre class="mc md me mf gt of og oh oi aw oj bi"><span id="b899" class="ni nj it og b gy ok ol l om on"># Visualising the clusters<br/><strong class="og jd">plt.figure(figsize=(15,7))<br/>sns.scatterplot(reduced_data[y_kmeans == 0, 0], reduced_data[y_kmeans == 0, 1], color = 'yellow', label = 'Cluster 1',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 1, 0], reduced_data[y_kmeans == 1, 1], color = 'blue', label = 'Cluster 2',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 2, 0], reduced_data[y_kmeans == 2, 1], color = 'green', label = 'Cluster 3',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 3, 0], reduced_data[y_kmeans == 3, 1], color = 'grey', label = 'Cluster 4',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 4, 0], reduced_data[y_kmeans == 4, 1], color = 'orange', label = 'Cluster 5',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 5, 0], reduced_data[y_kmeans == 5, 1], color = 'pink', label = 'Cluster 6',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 6, 0], reduced_data[y_kmeans == 6, 1], color = 'magenta', label = 'Cluster 7',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 7, 0], reduced_data[y_kmeans == 7, 1], color = 'cyan', label = 'Cluster 8',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 8, 0], reduced_data[y_kmeans == 8, 1], color = 'purple', label = 'Cluster 9',s=50)<br/>sns.scatterplot(reduced_data[y_kmeans == 9, 0], reduced_data[y_kmeans == 9, 1], color = 'brown', label = 'Cluster 10',s=50)<br/>sns.scatterplot(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color = 'red', <br/>                label = 'Centroids',s=300,marker=',')<br/>plt.grid(False)<br/>plt.title('Clusters of customers')<br/>plt.xlabel('PC-1')<br/>plt.ylabel('PC-2')<br/>plt.legend()<br/>plt.show()</strong></span></pre><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/15108ac28c522d416e3e872eefc3507b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*C0fKvkHJmZfgFQ7L3eZERQ.png"/></div></figure><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oy"><img src="../Images/f2119dadf6fb833e58a0390b0ceebf03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4dNNcjkzHeSUVKITzDO_xw.png"/></div></div></figure><h2 id="e1b8" class="ni nj it bd nk nl nm dn nn no np dp nq la nr ns nt le nu nv nw li nx ny nz iz bi translated">结论:</h2><p id="9114" class="pw-post-body-paragraph kr ks it kt b ku oa kd kw kx ob kg kz la oc lc ld le od lg lh li oe lk ll lm im bi translated">k-均值聚类是一种</p><ul class=""><li id="2b53" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><strong class="kt jd">一种无监督的机器学习算法</strong></li><li id="5a36" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">为无标签数据集寻找具有相似特征的数据组的迭代算法</strong></li></ul><p id="17e7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">确定最佳聚类的常用技术</p><ul class=""><li id="2b87" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><strong class="kt jd">肘形法</strong>:WCSS突然开始变平<strong class="kt jd"> </strong>再加一簇也没有明显改善，形成肘形。</li><li id="c8e1" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">轮廓得分</strong>:测量同一个聚类的紧密度和与其他聚类的分离度</li><li id="363b" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">差距统计:</strong>数据集和空引用数据集的类内惯性的最大差距是均匀分布的。</li></ul><h2 id="17d5" class="ni nj it bd nk nl nm dn nn no np dp nq la nr ns nt le nu nv nw li nx ny nz iz bi translated">参考资料:</h2><p id="b053" class="pw-post-body-paragraph kr ks it kt b ku oa kd kw kx ob kg kz la oc lc ld le od lg lh li oe lk ll lm im bi translated">【https://stanford.edu/~cpiech/cs221/handouts/kmeans.html T4】</p><div class="oz pa gp gr pb pc"><a href="https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/k-means-clustering" rel="noopener  ugc nofollow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">K-Means聚类:模块参考- Azure机器学习</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">本文描述了如何使用Azure机器学习设计器中的K-Means聚类模块来创建一个</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">docs.microsoft.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq mh pc"/></div></div></a></div><p id="9408" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae nh" href="https://arxiv.org/ftp/arxiv/papers/1912/1912.00643.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/ftp/arxiv/papers/1912/1912.00643.pdf</a></p><div class="oz pa gp gr pb pc"><a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html" rel="noopener  ugc nofollow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">在KMeans clustering - scikit-learn上使用剪影分析选择聚类数…</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">剪影分析可用于研究所得聚类之间的分离距离。剪影情节…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">scikit-learn.org</p></div></div><div class="pl l"><div class="pr l pn po pp pl pq mh pc"/></div></div></a></div><p id="a3f5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae nh" href="https://statweb.stanford.edu/~gwalther/gap" rel="noopener ugc nofollow" target="_blank">r . TiB shirani、G. Walther和T. Hastie(斯坦福大学，2001年)</a>通过gap statistics 估计数据集中的聚类数。</p><p id="0dd1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae nh" href="https://glowingpython.blogspot.com/2019/01/a-visual-introduction-to-gap-statistics.html" rel="noopener ugc nofollow" target="_blank">https://glowingpython . blogspot . com/2019/01/a-visual-introduction-to-gap-statistics . html</a></p></div></div>    
</body>
</html>