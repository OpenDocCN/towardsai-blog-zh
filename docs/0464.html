<html>
<head>
<title>Estimating a Classifier Performance Just by Looking at the Distributions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">仅通过查看分布来评估分类器性能</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/estimating-a-classifier-performance-just-by-looking-at-the-distributions-385d99cb2e13?source=collection_archive---------1-----------------------#2020-05-05">https://pub.towardsai.net/estimating-a-classifier-performance-just-by-looking-at-the-distributions-385d99cb2e13?source=collection_archive---------1-----------------------#2020-05-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="75eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多数时候，当我们的人工智能没有如我们所愿地表现时，我们可能会责怪模型背后的算法。然而，一个有足够经验的程序员知道，数据的质量对我们的人工智能的表现有很大的影响。</p><h1 id="de3e" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">在训练人工智能之前查看数据</h1><p id="cb90" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">要确切了解数据集的表现，唯一的方法就是对其进行训练。然而，从头开始查看数据可以节省我们大量的时间和痛苦。</p><h1 id="cec4" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">观察虹膜数据集的分布</h1><p id="cfeb" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在本文中，我将以Iris数据集为例:我是否能够在训练模型之前就确定数据的质量？</p><p id="572c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们首先用几行代码导入数据集:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="130a" class="lx km iq lt b gy ly lz l ma mb">import pandas as pd<br/>X = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')<br/>X.head()</span></pre><figure class="lo lp lq lr gt md gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/0d9996ab56516ab62ef3b7e0657cdeac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*EI6MjVDBmebZXAV2eb7PTQ.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk translated">虹膜数据集</figcaption></figure><p id="143a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为列和标签还没有命名，所以我将手动命名。X数据框将包含这些特征。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="dcf9" class="lx km iq lt b gy ly lz l ma mb">X.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']</span></pre><p id="77a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">y数据帧将包含标签。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="3667" class="lx km iq lt b gy ly lz l ma mb">y = X.pop('species')<br/>y = pd.DataFrame(y)</span></pre><p id="6362" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">* * *我决定将y格式的as系列转换成DataFrame供将来使用。</p><h1 id="edf5" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">我们如何表示数据？</h1><p id="6f15" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我们的目标不是训练模型，而是查看原始数据来估计准确性。</p><p id="257d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">之前，我将使用直方图绘制按标注分组的所有要素。对于每个特征，我将用不同的颜色表示所有三个不同的标签。我选择这种特定表示方法的原因是，我想看看分类器在选择每个标签时的选择是如何依赖于特征的分布的。</p><p id="0b06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，给定分布在单个直方图上的第一特征<strong class="jp ir">花瓣宽度</strong>、<strong class="jp ir">T3的所有值，我们可以尝试自己确定分类器的估计。在下图中，我代表了<strong class="jp ir">花瓣宽度的整列；每种</strong>颜色代表三种可能的输出之一:三个标签。</strong></p><p id="7db0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我必须用一个单独的值<strong class="jp ir">花瓣宽度</strong>来估计我的标签，如果这个值位于[.7，2.1]之间，我会100%准确。正如我们从分布中看到的，对于[.7，2.1]之间的数据集中的每个单独的<strong class="jp ir">花瓣_宽度</strong>，输出将是第一个标签(用蓝色表示)。从我们的数据来看，没有其他选择。</p><figure class="lo lp lq lr gt md gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/2e1b7d8d462b20e12d0fde1e7eada9cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMDA3UGXYD5oczFcTzLABw.png"/></div></div></figure><p id="9cd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相反，如果<strong class="jp ir">花瓣宽度</strong>值位于[3.7，5.8]之间，我们的估计可能是橙色或绿色标签:结果是不明确的。</p><h1 id="0d8c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">按标签分组的所有要素的列表</h1><p id="c9e0" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">到目前为止，我们在我的示例中看到的是，给定按标注分组的所有要素的图表分布，我们甚至可以通过快速浏览来估计哪些标注很容易以100%的准确度预测，哪些标注在正确估计时会有问题。</p><p id="9e6c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用此代码，您将在一个数组中分发按标签分组的数据帧中的所有要素:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="ad3e" class="lx km iq lt b gy ly lz l ma mb">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>a = list()<br/>for region, df_region in X.groupby(‘species’):<br/>a.append(df_region)</span><span id="887b" class="lx km iq lt b gy mp lz l ma mb">b = list()<br/>for c in X.columns[:-1]:<br/>for l in range(len(a)):<br/>b.append(pd.DataFrame(a[l][c]))<br/>for k in range(0,3): #len(X['species'].unique())<br/>  sns.distplot(b[k], hist = True, bins = 10, kde = True, kde_kws = {'linewidth': 1})</span></pre><h1 id="6ea8" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">绘制按标注分组的要素</h1><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="7959" class="lx km iq lt b gy ly lz l ma mb">fig, ax = plt.subplots()<br/>fig.set_size_inches(17.55, 11.4)</span><span id="f0d7" class="lx km iq lt b gy mp lz l ma mb">for k in range(0,3):<br/>  sns.distplot(b[k], hist = True, bins = 10, kde = True, kde_kws = {'linewidth': 1})</span></pre><figure class="lo lp lq lr gt md gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mq"><img src="../Images/e67dcf5f8645d992300160c3f52484a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rqynk9LhvtGzaMHYuC8wVA.png"/></div></div></figure><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="894e" class="lx km iq lt b gy ly lz l ma mb">fig, ax = plt.subplots()<br/>fig.set_size_inches(11.7, 8.27)</span><span id="ef35" class="lx km iq lt b gy mp lz l ma mb">for k in range(3,6):<br/>  sns.distplot(b[k], hist = True, bins = 10, kde = True, kde_kws = {‘linewidth’: 1})</span></pre><figure class="lo lp lq lr gt md gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mr"><img src="../Images/4b4d4e0735fc1c08185021089d336d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Vi0LYk5hBj7rp49EGtz9g.png"/></div></div></figure><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="5ddf" class="lx km iq lt b gy ly lz l ma mb">fig, ax = plt.subplots()<br/>fig.set_size_inches(17.55, 11.4)</span><span id="936d" class="lx km iq lt b gy mp lz l ma mb">for k in range(6,9):<br/>  sns.distplot(b[k], hist = True, bins = 10, kde = True, kde_kws = {‘linewidth’: 1})</span></pre><figure class="lo lp lq lr gt md gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/2e1b7d8d462b20e12d0fde1e7eada9cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMDA3UGXYD5oczFcTzLABw.png"/></div></div></figure><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="3c14" class="lx km iq lt b gy ly lz l ma mb">fig, ax = plt.subplots()<br/>fig.set_size_inches(17.55, 11.4)</span><span id="7f36" class="lx km iq lt b gy mp lz l ma mb">for k in range(9,12):<br/>  sns.distplot(b[k], hist = True, bins = 10, kde = True, kde_kws = {‘linewidth’: 1})</span></pre><figure class="lo lp lq lr gt md gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ms"><img src="../Images/361d59ea6368c46947443ea9d4e6ba05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NFJWI1UTb-65p0FHpow6Fw.png"/></div></div></figure><h1 id="16c4" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">得出结论</h1><p id="8d92" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">上面绘制的直方图非常清楚:指示前两个特征的前两个图显示了所有分布的重叠:基本上，前两个特征不允许我们准确地预测任何标签。</p><p id="f83b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相反，最后两个图表特征显示分布彼此远离，并且它们充当标签的几乎完美的估计。</p><h1 id="6f28" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">测试我们的假设</h1><p id="af29" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我将继续测试两个不同的假设:</p><blockquote class="mt mu mv"><p id="58ec" class="jn jo mw jp b jq jr js jt ju jv jw jx mx jz ka kb my kd ke kf mz kh ki kj kk ij bi translated">假设_1</p></blockquote><p id="7f15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我将去掉前两组分布，并在数据集上查看结果。剩下的两个特征彼此相距甚远:我假设即使只有这两个特征，数据集也会获得很高的精确度。</p><blockquote class="mt mu mv"><p id="c2f6" class="jn jo mw jp b jq jr js jt ju jv jw jx mx jz ka kb my kd ke kf mz kh ki kj kk ij bi translated">假设_2</p></blockquote><p id="71fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我的第二个假设中，我将去掉最后两组分布。因为剩余的分布几乎在每个点都是重叠的，我可以假设很难有一个精确的结论:这将导致低水平的准确性。</p><h1 id="864f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">测试假设_1</h1><p id="0151" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">如果我们重新加载并重命名初始数据集，我们只需进行以下更改:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="88c8" class="lx km iq lt b gy ly lz l ma mb">X.pop(‘sepal_length’)<br/>X.pop(‘sepal_width’)</span></pre><p id="bb75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我们剩下的:</p><figure class="lo lp lq lr gt md gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ee45fc07c04fd8ce4286215e484baad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*sStxxc6SXAe2jkwtEVgy4g.png"/></div></figure><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="fd9a" class="lx km iq lt b gy ly lz l ma mb">#splitting the dataset<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)<br/>print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)</span><span id="209b" class="lx km iq lt b gy mp lz l ma mb">import numpy as np<br/>from sklearn.naive_bayes import GaussianNB</span><span id="943c" class="lx km iq lt b gy mp lz l ma mb">#creating the model<br/>clf = GaussianNB()</span><span id="5741" class="lx km iq lt b gy mp lz l ma mb">#training the model<br/>clf.fit(X_train, y_train)<br/>print(clf.score(X_test, y_test))<br/>0.9666666666666667</span></pre><p id="5d8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所看到的，即使只使用最后两个特征，我们也获得了惊人的0.97的准确度。</p><h1 id="9edf" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">测试假设_2</h1><p id="5ff3" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">现在，我将做同样的事情，去掉最后两组分布。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="80f0" class="lx km iq lt b gy ly lz l ma mb">X.pop(‘petal_length’)<br/>X.pop(‘petal_width’)</span></pre><p id="da74" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行模型后，结果如预期的那样低:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="a513" class="lx km iq lt b gy ly lz l ma mb">0.7</span></pre><h1 id="cf4a" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结论</h1><p id="5c76" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我们可以得出一个简单的结论:如果在所有的图中，我们识别出任何只对应于一个标签的值，那么该预测将是100%准确的。因此，对于不与同一平面中的其他分布重叠的每个分布，该模型获得了准确性。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="111f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由<a class="ae ni" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向AI </a>发布</p></div></div>    
</body>
</html>