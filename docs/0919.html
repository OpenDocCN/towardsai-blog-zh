<html>
<head>
<title>The Pyramid Principle applied to Classification Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">金字塔原理在分类算法中的应用</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-pyramid-principle-applied-to-classification-algorithms-b8118e14f405?source=collection_archive---------2-----------------------#2020-09-12">https://pub.towardsai.net/the-pyramid-principle-applied-to-classification-algorithms-b8118e14f405?source=collection_archive---------2-----------------------#2020-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="42e4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="5923" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何更好地记忆和理解机器学习分类器</h2></div><p id="e619" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">明托金字塔原则旨在更好地组织和记忆具有金字塔和等级结构的想法。如果非要回答:有哪些不同的分类算法？你可能会想到，随机森林，KNN，朴素贝叶斯，逻辑回归，等等。但是两者之间有什么关系，又如何给出一个结构化的答案呢？</p><p id="600b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们应用明托金字塔原理。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/9d6531ecfdad5c9d9e08fbc40ecf4d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p-I_19-LSyPXcnrR"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">照片由<a class="ae md" href="https://unsplash.com/@filipovsky?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">菲利普·吉尔达</a>在<a class="ae md" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="dae8" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">3种基本方式</h1><p id="445d" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">3将数据分类的基本方法有:</p><ul class=""><li id="c20c" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><strong class="kt jd">线性分类器</strong>具有线性决策边界。或者更准确地说，决策边界是一个超平面。</li><li id="cbe8" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><strong class="kt jd">最近邻居</strong>通过分析k个最近观察值(在计算新观察值和所有训练数据之间的距离之后)</li><li id="95da" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">决策树构造<strong class="kt jd">(超)矩形</strong>通过最小化基尼或熵来重组观察值。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi np"><img src="../Images/3e9648556f1a1af40ad351cc6bb6efa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v5pGuVYUuMt5YRfOXemEKg.png"/></div></div></figure><h1 id="32dd" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">线性分类器</h1><p id="7160" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">构造一个超平面把空间分成2个，这就是线性分类器的原理。现在如何构造这个超平面呢？我们有几种不同的方法。</p><p id="a48c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们首先考虑二元分类的情况:</p><ul class=""><li id="6e39" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><strong class="kt jd">逻辑回归</strong>使用逻辑函数将数据转换为0类和1类。而0.5的值会给我们决策边界，这是一个超平面。</li><li id="058b" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><strong class="kt jd"> SVM </strong>通过最大化两个类的软边界找到超平面。</li><li id="c332" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><strong class="kt jd">线性判别分析</strong>认为两类数据服从多元正态分布。然后利用贝叶斯定理，计算每一类的后验概率。最后，最有可能的类获胜！通过查看空间中每个点的哪个类别获胜，我们得到一个超平面。(有一个条件:协方差矩阵被认为对所有类都是相同的)。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nq"><img src="../Images/c8fd9f606f2a6346f3094f2fcb2f70b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQA_iNiuQhiNfK2ZOd94tQ.png"/></div></div></figure><p id="9f52" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在进行多类分类</p><ul class=""><li id="ea01" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated">对于SVM，我们通常使用一对休息。所以算法的基本原理是不一样的。</li><li id="3d32" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">对于逻辑回归，我们推广逻辑函数以获得softmax函数，然后将分类器称为softmax分类器。</li><li id="0728" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">对于LDA，通过构造，我们可以通过计算每个类的多元分布来处理多类分类。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nr"><img src="../Images/ed533fd743ece132a099b9865312194d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eNQCKVhuPq46G_TvwI7ImQ.png"/></div></div></figure><p id="f3c8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SVM和逻辑回归之间的关系也可以通过它们损失函数的差异来解释:逻辑回归的损失函数为逻辑，SVM的损失函数为铰链。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ns"><img src="../Images/e59c308062bc43a981213131e1f26c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tbjCrl6Y9A7eJYlS.png"/></div></div></figure><h2 id="ae80" class="nt mf it bd mg nu nv dn mk nw nx dp mo la ny nz mq le oa ob ms li oc od mu iz bi translated">线性分类器和KNN</h2><p id="11db" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">我们还可以通过考虑彼此不同的距离概念来找到所有这些线性分类器和KNN之间的关系。对于一个新的观察，KNN的原理是分析最近邻，我们实际上不需要任何模型。这就是为什么我们说KNN是一个基于实例的分类器。</p><p id="db8d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于其他的，我们考虑一个模型，在线性模型的情况下，这个模型是一个超平面分离器。以及如何对一个新的观察进行分类？我们需要测量新观测值到超平面的距离。</p><ul class=""><li id="29de" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated">对于<strong class="kt jd"> KNN </strong>，来自训练数据的新观测值的距离就是<strong class="kt jd">欧氏距离</strong>。</li><li id="7c8a" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">对于<strong class="kt jd">线性判别分析</strong>，考虑的距离是来自不同类别中心的新观察的<strong class="kt jd"> Mahalanobis距离</strong>。</li><li id="edb9" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">对于SVM，我们考虑通过最小化软边所构造的超平面的距离。</li><li id="69af" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">对于逻辑回归，距离由逻辑函数转换得出。</li></ul><h1 id="d38e" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">基于树的模型</h1><p id="bcde" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">单个决策树在实践中不是很有效。但是基于这种构造树的原理，我们可以做出很多树。一般决策树中的集成方法。创建森林(由树组成)的不同方式为我们提供了不同的算法:</p><ul class=""><li id="9f92" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><strong class="kt jd"> Bagging </strong>:理论上，我们可以自举——聚合所有的算法，实际上，对于决策树来说确实是这样。</li><li id="0bf9" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><strong class="kt jd">随机森林</strong>:除了装袋，我们还可以随机选择每个节点的变量。而这个原理给了我们Random Forest(也是商标)。</li><li id="53a5" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated"><strong class="kt jd">梯度增强</strong>:梯度增强不是对所有树取平均值，而是通过逐步相加来聚合基于残差构建的树。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi oe"><img src="../Images/9ee58ad8de75b4d2e3f54afacd4fc07b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wok7bIMU2OXWL3OSuiJjIA.png"/></div></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi of"><img src="../Images/5df7eef86c9a2705f1ca61bd959ec8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VK4WEC2tyscrqaznMXKi-g.png"/></div></div></figure><h1 id="3163" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">线性分类器与非线性分类器</h1><p id="41be" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">通过构造，KNN和基于树的模型是非线性分类器。如果数据不是线性可分的，它们将比线性分类器更有效。</p><p id="ee1a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，线性分类器能做什么？可以做几个增强。</p><ul class=""><li id="d2d2" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated">对于LDA，协方差矩阵被认为对所有类都是相同的。但是如果我们计算一个不同的协方差矩阵，那么决策边界将不再是线性的。并且我们得到<strong class="kt jd">二次判别分析</strong>。而我们也可以考虑变量是独立的，那么我们得到<strong class="kt jd">(高斯)朴素贝叶斯</strong>。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi oe"><img src="../Images/d0a01dc0b312acefe438f58a3270c6dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jwAGFHvGqfTj2JbZhofVsQ.png"/></div></div></figure><ul class=""><li id="c169" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated">对于SVM来说，<strong class="kt jd">内核技巧</strong>用于将数据转换到更高维度的空间，在那里它们将被线性分离。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi og"><img src="../Images/5fbd398c8ea900f774752aebcef2f99a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uUI_8LhEsWzRmG5aUP-qOw.png"/></div></div></figure><ul class=""><li id="5313" class="nb nc it kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated">同样的核技巧也可以用于逻辑回归。</li><li id="cf1d" class="nb nc it kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">有了逻辑回归，我们还可以把它组合几次，得到一个<strong class="kt jd">神经网络</strong>。如果你对此不清楚，请阅读<a class="ae md" href="https://towardsdatascience.com/visualize-how-a-neural-network-works-from-scratch-3c04918a278" rel="noopener" target="_blank">想象神经网络如何从零开始工作</a>。</li></ul><h1 id="4378" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">分类器的金字塔</h1><p id="c09c" class="pw-post-body-paragraph kr ks it kt b ku mw kd kw kx mx kg kz la my lc ld le mz lg lh li na lk ll lm im bi translated">最后，我们有了这个金字塔，它是我们在机器学习中用于分类任务的主要分类器。绿色背景下，我们有线性分类器。在蓝色背景中，我们有非线性分类器。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi oh"><img src="../Images/f730cfaf11a1c0aa3e086b22bb92216d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bO-Q9UzuqwgerwpG8WhRKA.png"/></div></div></figure></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="fefe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你觉得这个金字塔对你有用吗？如果你认为我应该提到其他算法，请评论。</p></div></div>    
</body>
</html>