<html>
<head>
<title>PySpark process Multi char Delimiter Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark过程多字符分隔符数据集</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pyspark-process-multiple-delimited-data-ef99fa05c6f7?source=collection_archive---------0-----------------------#2020-09-15">https://pub.towardsai.net/pyspark-process-multiple-delimited-data-ef99fa05c6f7?source=collection_archive---------0-----------------------#2020-09-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="21f1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><p id="997f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">本文的目标是使用Apache spark和Python编程语言处理多个分隔文件。这是一个实时场景，应用程序可以共享多个分隔文件，开发团队必须处理相同的文件。我们将学习如何应对挑战。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ku"><img src="../Images/a61126a4a06af03c65f29f0cc3baa3c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*g_JG3UFQORmWsPsCvVlk1w.png"/></div></figure><p id="7557" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">输入数据集如下:</p><pre class="kv kw kx ky gt lc ld le lf aw lg bi"><span id="7374" class="lh li iq ld b gy lj lk l ll lm">Name@@#Age  &lt;--Header<br/>vivek, chaudhary@@#30  &lt;--row1<br/>john, morgan@@#28   &lt;--row2</span></pre><p id="6e02" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">方法1: </strong>让我们尝试使用<strong class="jy ja"> read.csv() </strong>读取文件，并查看输出:</p><pre class="kv kw kx ky gt lc ld le lf aw lg bi"><span id="90e3" class="lh li iq ld b gy lj lk l ll lm">from pyspark.sql import SparkSession</span><span id="d78b" class="lh li iq ld b gy ln lk l ll lm">from pyspark.sql import SparkSession<br/>spark= SparkSession.builder.appName(‘multiple_delimiter’).getOrCreate()</span><span id="607b" class="lh li iq ld b gy ln lk l ll lm">test_df=spark.read.csv(‘D:\python_coding\pyspark_tutorial\multiple_delimiter.csv’)<br/>test_df.show()</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/1fddb7e273bbc0a2605c706908744c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*5qSjjhr9zTCz-4REQahtxw.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk translated">输出</figcaption></figure><blockquote class="lt lu lv"><p id="1b70" class="jw jx lw jy b jz ka kb kc kd ke kf kg lx ki kj kk ly km kn ko lz kq kr ks kt ij bi translated"><strong class="jy ja">#注意:</strong>输出不是期望的输出，因此处理不会产生期望的结果</p></blockquote><p id="bd0a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> Approach2: </strong>接下来，使用带有<strong class="jy ja"> option() </strong>参数的<strong class="jy ja"> read.csv() </strong>读取文件，并将分隔符作为值为'<strong class="jy ja">@ @ # '【T15]的参数传递，并查看输出:</strong></p><pre class="kv kw kx ky gt lc ld le lf aw lg bi"><span id="8516" class="lh li iq ld b gy lj lk l ll lm">test_df=spark.read.option(‘delimiter’,’@@#’).csv(‘D:\python_coding\pyspark_tutorial\multiple_delimiter.csv’)</span><span id="1cb9" class="lh li iq ld b gy ln lk l ll lm">test_df.show(truncate=0)</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ma"><img src="../Images/ddc4f2fa04d19777d291f1f406f72b10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*edZoAimYtLCoBZ33z9aKgw.png"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk translated">错误</figcaption></figure><blockquote class="lt lu lv"><p id="d736" class="jw jx lw jy b jz ka kb kc kd ke kf kg lx ki kj kk ly km kn ko lz kq kr ks kt ij bi translated">#注意:当我们试图传递多于一个字符的分隔符时，spark抛出错误。</p></blockquote><p id="ff5f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">方法3: </strong>下一个方法是使用<strong class="jy ja">read . text()</strong>spark的方法。</p><pre class="kv kw kx ky gt lc ld le lf aw lg bi"><span id="a6dc" class="lh li iq ld b gy lj lk l ll lm">mult_df=spark.read.text(‘D:\python_coding\pyspark_tutorial\multiple_delimiter.csv’)<br/>mult_df.show(truncate=0)</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/1440d3fa74471eb63d347d59595b23ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*DZXGeMLTshurYODgQpULug.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk translated">火花.阅读.文本</figcaption></figure><blockquote class="lt lu lv"><p id="d88c" class="jw jx lw jy b jz ka kb kc kd ke kf kg lx ki kj kk ly km kn ko lz kq kr ks kt ij bi translated">#注意:spark.read.text返回一个DataFrame。</p></blockquote><p id="362f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">文本文件中的每一行代表DataFrame中只有一列“<strong class="jy ja">值”</strong>的一条记录。要转换成多个列，我们将使用映射转换和拆分方法来转换和拆分列值。</p><pre class="kv kw kx ky gt lc ld le lf aw lg bi"><span id="d453" class="lh li iq ld b gy lj lk l ll lm">#first() returns the first record of dataset<br/>header=mult_df.first()[0]<br/>print(header)<br/>Output: <br/>Name@@#Age</span><span id="9cae" class="lh li iq ld b gy ln lk l ll lm">#split('delimiter') the string on basis of the delimiter<br/>#define the schema of the Dataframe to be created</span><span id="1eed" class="lh li iq ld b gy ln lk l ll lm">schema=header.split(‘@@#’)<br/>print(schema)</span><span id="f728" class="lh li iq ld b gy ln lk l ll lm">Output:<br/>['Name', 'Age']</span></pre><p id="4f68" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">下一步是拆分行并创建单独的列:</p><pre class="kv kw kx ky gt lc ld le lf aw lg bi"><span id="cee6" class="lh li iq ld b gy lj lk l ll lm">#filter operation is removing the header<br/>#map operation is splitting each record as per delimiter<br/>#.rdd converts DF to rdd and toDF converts the rdd back to DF</span><span id="4816" class="lh li iq ld b gy ln lk l ll lm">mult_df.filter(mult_df[‘value’]!=header).rdd.map(lambda x:x[0].split(‘@@#’)).toDF(schema).show()</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/4a3dd45adfaea1f0ffde004ec34042a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*UMEt92B7qtG4Xh_2SoBpxg.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk translated">最终输出</figcaption></figure><p id="f790" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">万岁！！我们能够根据多个分隔符'<strong class="jy ja"> @@#' </strong>来拆分数据。</p><h1 id="4702" class="mh li iq bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">总结:</h1><p id="88cd" class="pw-post-body-paragraph jw jx iq jy b jz ne kb kc kd nf kf kg kh ng kj kk kl nh kn ko kp ni kr ks kt ij bi translated">使用spark.read.text()方法读取多个分隔的数据集</p><p id="599f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用map()、filter()转换</p><p id="264f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">感谢所有人阅读我的博客，如果你喜欢我的内容和解释，请在medium上关注我并分享你的反馈，这将永远帮助我们所有人提高我们的知识。</p></div></div>    
</body>
</html>