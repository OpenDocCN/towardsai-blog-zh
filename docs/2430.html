<html>
<head>
<title>Understanding Tree Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解树模型</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understanding-tree-models-d2a38a9dcd5b?source=collection_archive---------3-----------------------#2021-12-17">https://pub.towardsai.net/understanding-tree-models-d2a38a9dcd5b?source=collection_archive---------3-----------------------#2021-12-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="99bc" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><p id="84b5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">生活中充满了决策，最终，我们会通过一些基于逻辑的分析来衡量选择哪一个。在这一系列博客中，我们将让自己适应两个非常流行的机器学习模型— <strong class="jy ja">决策树</strong>和<strong class="jy ja">随机森林。</strong></p><p id="a506" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">对于这篇博客，我们将把自己限制在<strong class="jy ja">决策树</strong>上，然后继续讨论<strong class="jy ja">随机森林</strong>业界最常用的算法之一。我们还将通过解决一个真实世界的例子来理解业务的编码部分。那我们开始吧。</p><h1 id="ff0c" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">决策树背后的直觉</strong></h1><p id="da4c" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">凭借高度的可解释性和直观的算法，决策树模仿人类的决策过程，并擅长处理分类数据。与逻辑回归或支持向量机等其他算法不同，决策树不会发现自变量和目标变量之间的线性关系。相反，它们可以用于<strong class="jy ja">高度非线性数据的建模。</strong></p><p id="8966" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这是一种监督学习算法，你可以很容易地解释导致特定决策/预测的所有因素。因此，他们很容易被商人理解。让我们通过下面的<strong class="jy ja">贷款审批系统</strong>的例子来理解这一点</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/ec11526072197e2a49bcf089e9149db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*Vcxwlenkfza-Tt8_z47PLw.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">贷款数据集</figcaption></figure><p id="2f9b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果出现新的记录，我们需要根据这些历史数据做出决策，不管这个人是否有资格获得贷款。让我们将上述数据集的决策树可视化</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mj"><img src="../Images/0f0e76091bbe8e5f0878b97634406ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JM9H8lS0VRYTFqitPteNJw.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">贷款审批决策树</figcaption></figure><p id="44f0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你可以看到决策树使用一个非常自然的决策过程:在一个嵌套的if-then-else结构中问一系列问题。</p><p id="fd66" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在每个节点上，您提出一个问题来进一步拆分该节点保存的数据。如果测试通过，你向左走；否则，你走右边。我们在每个节点分裂，直到我们没有得到一个纯粹的子集或达成一个决定。</p><h1 id="9441" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">解释决策树</h1><p id="e320" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">让我们从决策树的角度来理解上面的内容。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mo"><img src="../Images/7a7cfa4c6d753d6d998a7bbb6a6c8fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tc-0kIpxM4UB-KU7o1Qqvg.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">决策图表</figcaption></figure><p id="c74f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在每个决策节点，都会进行与数据集中的一个要素相关的测试，如果测试通过，则测试会进行到树的一侧，否则会进行到另一侧。测试的结果将把我们带到树的一个分支，对于一个分类问题，每个叶子将包含一个类。</p><p id="89e2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在二叉决策树中，我们测试将数据集分成两部分，就像上面示例中根据雇佣类型进行决策一样。现在，如果有一个属性有4种值，我想根据每个值做出决定，那么它将是一个多路决策树。</p><p id="dae2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">决策树很容易解释。几乎总是，你能识别导致决定的各种因素。树通常被低估了将预测变量与预测联系起来的能力。根据经验，如果外行人的可解释性是你在模型中寻找的，那么决策树应该在你的列表的顶部。</p><h1 id="177e" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">决策树回归</h1><p id="53a4" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">在回归问题中，决策树将数据分成多个子集。决策树分类和决策树回归的区别在于，在回归中，<strong class="jy ja">每片叶子代表一个线性回归模型</strong>，而不是一个类标签。对于这个博客，我们将把自己限制在分类问题上，但是我也会在需要的地方提到回归的某些要点。</p><h1 id="4116" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">决策树构造算法</strong></h1><p id="f1fe" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">让我们理解一下，决策树是如何知道应该首先对哪个属性进行拆分的？</p><h2 id="dbfc" class="mp kv iq bd kw mq mr dn la ms mt dp le kh mu mv li kl mw mx lm kp my mz lq iw bi translated">同质性概念</h2><p id="d98e" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">一般来说，规则说我们应该尝试分割节点，使得产生的节点尽可能地<strong class="jy ja">，</strong>，即分割后所有的行都属于一个类。如果在上面的例子中节点是收入水平，尝试用一个规则分割它，使得所有通过规则的数据点有一个标签(即尽可能同质)，而那些不通过规则的数据点有另一个标签。</p><p id="e087" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在从上面，我们来了一个一般规则如下</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi na"><img src="../Images/90b8cb330801a9dd470449b927e85340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*br8OnR_7KntwRPFUZJT6wg.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">同种</figcaption></figure><blockquote class="nb nc nd"><p id="d289" class="jw jx ne jy b jz ka kb kc kd ke kf kg nf ki kj kk ng km kn ko nh kq kr ks kt ij bi translated">根据规则，我们会不断分割数据，直到数据的同质性低于某个阈值，因此您可以一步一步地选择属性并分割数据，这样每次分割后同质性都会增加。当产生的叶子足够同质时，你就停止分裂。什么是足够同质的？你定义了同质性的数量，当达到这个数量时，树应该停止进一步分裂。让我们看看有哪些具体的方法被用来衡量同质性。</p></blockquote><p id="512f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们用一个假设的数据集例子来理解所有的同质性度量</p><p id="22ed" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们需要根据特定人群的年龄和性别来预测他们是否能打板球。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi ni"><img src="../Images/1f94bde1ba5ff9b532f85939960afbc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NZbYqZ-DsFt8KtGzb5OvZg.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">例子</figcaption></figure><p id="3c8a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在上面的例子中，我们需要决定是否根据年龄或性别来划分，以决定这个人是否能打板球？让我们看看同质性度量来做决定</p><h2 id="ac4e" class="mp kv iq bd kw mq mr dn la ms mt dp le kh mu mv li kl mw mx lm kp my mz lq iw bi translated">基尼指数</h2><p id="a820" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">基尼指数使用数据集中各种标签概率的平方和。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/5828c06bd8581b9f7e0e9feffd4f6278.png" data-original-src="https://miro.medium.com/v2/resize:fit:90/format:webp/1*yBr4VEo9Ct-Z-AWMwO-zfg.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">基尼</figcaption></figure><p id="f097" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">总基尼指数，</strong>如果按性别划分<strong class="jy ja">，</strong>将是</p><p id="fcaa" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">基尼指数(性别)=(男性节点的总观察分数)*男性节点的基尼指数+(女性节点的总观察分数)*女性节点的基尼指数。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nk"><img src="../Images/8026041d5911eb49fb966cd3f86c0e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHm-b9mUwKbQlquVIc2_ZQ.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">性别差异基尼指数</figcaption></figure><p id="f580" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">同样，如果我们按年龄划分，我们可以计算出基尼指数。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nl"><img src="../Images/bf2a47238aab81ee6ceac1017d18cb9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uiTWNUph0j-maIYvfJMP_g.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">年龄划分的基尼指数</figcaption></figure><p id="e0be" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">假设您有一个带有2个类别标签的数据集。如果数据集是完全同质的(所有数据点都属于标签1)，那么找到对应于标签2的数据点的概率将是0，找到对应于标签1的数据点的概率将是1。所以p1= 1，p2= 0。基尼系数等于1，在这种情况下是最高的。同质性越高，基尼指数越高。</p><p id="58cc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">所以如果你必须在年龄和性别这两个分裂中做出选择。按性别划分的基尼系数高于按年龄划分的基尼系数；所以你继续按性别划分。</p><h2 id="8534" class="mp kv iq bd kw mq mr dn la ms mt dp le kh mu mv li kl mw mx lm kp my mz lq iw bi translated">熵和信息增益</h2><p id="4dd2" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">另一个同质性度量是信息增益。这个想法是利用熵的概念。熵量化了数据中的无序程度，和基尼指数一样，它的值也从0到1不等。</p><p id="da15" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">熵由下式给出</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f62b1e2d4c662a114f24cfecf9a93fce.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*_nwXVssgBO6Z9dsWWGH7kA.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">熵</figcaption></figure><p id="77bc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">其中，p _i是找到带有标签I的点的概率，与基尼系数相同，k是不同标签的数量，ε[D]是数据集D的熵。</p><p id="c88a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">信息增益=ε[D]ε[DA]即原始数据集的熵减去分割后分区熵的加权和。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nn"><img src="../Images/a88f4b3886f9268fc17a40ce18a599a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*ZQmc0bTQveDLy9Xkt0WN0A.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">信息增益</figcaption></figure><p id="5a57" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们考虑一个例子。您有四个数据点，其中两个属于类别标签“1”，另外两个属于类别标签“2”。您拆分了这些点，使得左侧分区有两个数据点属于标签“1”，右侧分区有另外两个数据点属于标签“2”。现在让我们假设你在一个叫做‘A’的属性上分裂。</p><ol class=""><li id="15ef" class="no np iq jy b jz ka kd ke kh nq kl nr kp ns kt nt nu nv nw bi translated">原始/父数据集的熵为ε[D]=[(24)log2(24)+(24)log2(24)]= 1.0。</li><li id="2102" class="no np iq jy b jz nx kd ny kh nz kl oa kp ob kt nt nu nv nw bi translated">分割后分区的熵为ε[DA]= 0.5∫log2(2/2)-0.5∫log2(2/2)= 0。</li><li id="42c4" class="no np iq jy b jz nx kd ny kh nz kl oa kp ob kt nt nu nv nw bi translated">分离后的信息增益为Gain[D，A]=ε[D]ε[DA]= 1.0。</li></ol><p id="174b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，对属性‘A’上的原始数据集进行拆分后的信息增益为<strong class="jy ja"> 1.0，</strong>，信息增益的值越大，拆分后数据的同质性越好。</p><blockquote class="nb nc nd"><p id="08a7" class="jw jx ne jy b jz ka kb kc kd ke kf kg nf ki kj kk ng km kn ko nh kq kr ks kt ij bi translated">对连续输出变量进行分割？您可以使用与线性回归模型类似的方式计算数据集(拆分前后)的平方。因此，拆分数据，使拆分后获得的分区的R2大于原始或父数据集的。换句话说，拆分后模型的拟合应该尽可能‘好’。</p></blockquote><p id="8b09" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我希望这个博客能让你以一种非常简单的方式理解决策树。在本系列的下一篇博客中，我们将讨论与决策树的<strong class="jy ja"> </strong>相关的<strong class="jy ja">超参数</strong>，并理解"<strong class="jy ja">随机森林的随机性是什么:)"</strong></p><h1 id="bcaa" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">参考</h1><ol class=""><li id="2f21" class="no np iq jy b jz ls kd lt kh oc kl od kp oe kt nt nu nv nw bi translated"><a class="ae of" href="http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/tree.html" rel="noopener ugc nofollow" target="_blank">http://ogrisel . github . io/sci kit-learn . org/sk learn-tutorial/modules/tree . html</a></li><li id="f8fe" class="no np iq jy b jz nx kd ny kh nz kl oa kp ob kt nt nu nv nw bi translated"><a class="ae of" href="https://www.udemy.com/course/complete-data-science-and-machine-learning-using-python/" rel="noopener ugc nofollow" target="_blank">https://www . udemy . com/course/complete-data-science-and-machine-learning-using-python/</a></li><li id="1e9c" class="no np iq jy b jz nx kd ny kh nz kl oa kp ob kt nt nu nv nw bi translated">Upgrad的数据科学项目(【https://www.upgrad.com/】T2</li></ol><h1 id="bbca" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">链接到我的媒体简介</h1><p id="bc15" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated"><a class="ae of" href="https://commondatascientist.medium.com/medium-to-bond-de3c81e193b8" rel="noopener">https://commondatascientist . medium . com/medium-to-bond-de 3c 81 e 193 b 8</a></p></div></div>    
</body>
</html>