<html>
<head>
<title>GELU, the ReLU Successor? Gaussian Error Linear Unit Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">葛鲁，热鲁接班人？高斯误差线性单位解释</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/is-gelu-the-relu-successor-deep-learning-activations-7506cf96724f?source=collection_archive---------0-----------------------#2022-08-30">https://pub.towardsai.net/is-gelu-the-relu-successor-deep-learning-activations-7506cf96724f?source=collection_archive---------0-----------------------#2022-08-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/9f7afefd92f0c22a333743727b2f59fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oJYECIqfabCiZt1j"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://unsplash.com/@willbassi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">威利安B. </a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="4a03" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本教程中，我们旨在全面解释<strong class="ki iu"> <em class="le">高斯误差线性单元</em> </strong>、<strong class="ki iu"> <em class="le"> GELU </em> </strong>激活如何工作。</p><p id="7251" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们能结合正则化和激活函数吗？2016年，作者丹·亨德里克斯和凯文·金佩尔发表了一篇论文。从那以后，这份报告已经更新了4次。作者引入了一个新的激活函数<strong class="ki iu">，</strong>，<strong class="ki iu">，<em class="le">高斯误差线性单元，GELU。</em>T19】</strong></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="1761" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">揭秘格鲁派</h1><p id="94db" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">GELU激活背后的动机是用非线性(即激活函数)来桥接随机正则化因子，例如丢弃。</p><p id="5dee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">退出正则化</strong>将一个神经元的输入与0随机相乘，随机地使它们不活动。另一方面，<strong class="ki iu"> ReLU </strong>激活确定性地将输入乘以0或1，这取决于输入的值。</p><p id="7281" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> GELU </strong>通过将输入乘以一个从0到1的值来合并两种功能。然而，这个0-1掩码的值虽然是随机确定的，但也取决于输入的值。</p><p id="445f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在数学上，GELU激活函数被公式化为:</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/2eb97c1f1cc921154ac92c395ed69001.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*2b_mG6oZo8InksCUjDsx9A.png"/></div></figure><p id="a166" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">φ(x)是标准正态分布的<a class="ae kf" href="https://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_functions" rel="noopener ugc nofollow" target="_blank">累积分布函数(CDF)。</a>该函数的选择源于这样一个事实，即神经元输入往往遵循正态分布，尤其是在使用批量标准化时。因此，本质上<strong class="ki iu"> GELU有更高的概率丢弃一个神经元(乘以0)，而X减少</strong>，因为P(X ≤ x)变得更小。请花点时间想一想，让它沉下去。因此，GELU应用的变换是随机的，但它依赖于通过φ(x)输入的值。</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/490f90f48dd511c911c880bb37259165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/0*lpKy2FLJ8NV0QkGY"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">图1:高斯误差线性单位(μ=0，σ=1)、校正线性单位和指数线性单位(ELU) (α=1)。来源[ <a class="ae kf" href="https://arxiv.org/pdf/1606.08415v3.pdf" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]</figcaption></figure><p id="abb1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于CDF P(X≤x)几乎等于0，因此观察GELU(x)如何从零开始。然而，在值-2附近，P(X≤x)开始增加。因此我们看到GELU(x)偏离零。对于正值，由于P(X≤x)向值1靠拢，GELU(x)开始逼近ReLU(x)。下图中，红线代表标准正态分布N(0，1)即P(X≤x)的CDF。</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/53fac1d335f447d8dbe29cfc9a3502ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8EMLwiZTccRCZKTM"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk translated">图2:不同高斯分布的累积分布函数。红线代表标准法线N(0，1)的CDF。<a class="ae kf" href="https://en.wikipedia.org/wiki/Normal_distribution#/media/File:Normal_Distribution_CDF.svg" rel="noopener ugc nofollow" target="_blank">来源维基百科</a>。</figcaption></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h2 id="8774" class="mw ln it bd lo mx my dn ls mz na dp lw kr nb nc ma kv nd ne me kz nf ng mi nh bi translated"><strong class="ak">葛鲁约计</strong></h2><p id="fab8" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">葛陆也可以通过公式来近似</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/080a2971de5e261bd09145283730af50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*0s38Dez_LJ6yF1JbAXHyIw.png"/></div></figure><p id="1d4c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果更大的前馈速度值得精确的代价。</p><h2 id="c846" class="mw ln it bd lo mx my dn ls mz na dp lw kr nb nc ma kv nd ne me kz nf ng mi nh bi translated"><strong class="ak">葛鲁变奏曲</strong></h2><p id="026e" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">还可以通过使用不同的CDF来修改GELU。例如，如果使用逻辑分布CDF (x ),那么我们将得到Sigmoid线性单位(路斯)x(x)。此外，我们可以选择一个CDF N(μ，σ),其中μ和σ是可学习的超参数。</p><h2 id="d9eb" class="mw ln it bd lo mx my dn ls mz na dp lw kr nb nc ma kv nd ne me kz nf ng mi nh bi translated"><strong class="ak">GELU激活功能的优点</strong></h2><p id="d85d" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">在[1]中，作者在3个不同的基准数据集上实验了针对ReLU和ELU激活函数的GELU的使用，这些数据集涵盖了计算机视觉(CIFAR 10/100分类)、自然语言处理(Twitter语音标记部分)和音频音素识别(<a class="ae kf" href="https://catalog.ldc.upenn.edu/LDC93s1" rel="noopener ugc nofollow" target="_blank"> TIMIT </a>帧分类)的任务。</p><p id="0b61" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在整个实验过程中，他们观察到与ReLU和ELU相比，使用GELU在准确性上有持续的提高。分析上:</p><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/1b4914ca7c4e4f431403299af3ea3843.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tPK9aYCImrSeoJhh"/></div></div></figure><p id="9614" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上表显示了4个数据集中的测试错误率。GELU始终实现最低的测试错误率，成为ReLU和ELU的神经网络激活函数的一种有前途的替代方法。</p><h2 id="fe16" class="mw ln it bd lo mx my dn ls mz na dp lw kr nb nc ma kv nd ne me kz nf ng mi nh bi translated">GELU激活函数的Python代码</h2><p id="d21c" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">如果你对用Python编写GELU或者用PyTorch或Tensorflow来使用它感兴趣，可以看看这篇简短的编码教程。</p><div class="nk nl gp gr nm nn"><a rel="noopener  ugc nofollow" target="_blank" href="/gelu-gaussian-error-linear-unit-code-python-tf-torch-neural-network-bert-de539517edef"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">GELU:高斯误差线性单位码(Python，TF，Torch)</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">代码教程为GELU，高斯误差线性单位激活函数。包括bare python，Tensorflow和Pytorch…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">pub.towardsai.net</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob jz nn"/></div></div></a></div><h2 id="4e28" class="mw ln it bd lo mx my dn ls mz na dp lw kr nb nc ma kv nd ne me kz nf ng mi nh bi translated"><strong class="ak">关于GELU激活的有趣事实</strong></h2><p id="f0e2" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">著名的论文<a class="ae kf" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">“一幅图像相当于16x16个字:大规模图像识别的变形金刚”</a>使视觉变形金刚(vit)流行起来，它利用了编码器变形金刚模块(3.1节)的全连接神经网络(MLP)中的GELU激活。这表明，高质量的研究人员认为GELU是一个很好的选择。</p><p id="fb28" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其他大型变形金刚模型，如伯特和GPT，也使用GELU作为激活函数。由于这些模型是巨大的，正则化变得非常重要，因此GELU成为如此巨大的变压器模型如此受欢迎的激活。</p><h2 id="2099" class="mw ln it bd lo mx my dn ls mz na dp lw kr nb nc ma kv nd ne me kz nf ng mi nh bi translated"><strong class="ak">参考文献</strong></h2><p id="f306" class="pw-post-body-paragraph kg kh it ki b kj mk kl km kn ml kp kq kr mm kt ku kv mn kx ky kz mo lb lc ld im bi translated">[1] <a class="ae kf" href="https://arxiv.org/abs/1606.08415" rel="noopener ugc nofollow" target="_blank">高斯误差线性单位(GELUs) </a></p><p id="d570" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]https://en.wikipedia.org/wiki/Normal_distribution<a class="ae kf" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank"/></p><p id="0071" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3] <a class="ae kf" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">一幅图像相当于16x16个字:大规模图像识别的变形金刚</a></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="f26b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">如果你学到了有用的东西，请关注我，获取更多深度学习内容和技术教程。使劲鼓掌也让我感觉很棒:)</em></p><p id="8a5e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你喜欢我的博客，你可以通过我的推荐链接加入Medium来支持我。我会得到一半的奖励，不需要你额外付费:)</p><div class="nk nl gp gr nm nn"><a href="https://medium.com/@poulinakis.kon/membership" rel="noopener follow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">通过我的推荐链接加入Medium-Konstantinos Poulinakis</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">阅读深度学习，数据科学，技术和媒体上的想法。您的会员费直接支持…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">medium.com</p></div></div><div class="nw l"><div class="oc l ny nz oa nw ob jz nn"/></div></div></a></div><p id="0d1e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">感谢阅读，随意伸手！</em></p><p id="95a5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">我的链接:</strong> <a class="ae kf" href="https://medium.com/@poulinakis.kon" rel="noopener">中</a>|<a class="ae kf" href="https://www.linkedin.com/in/konstantinos-poulinakis-4554821a3/" rel="noopener ugc nofollow" target="_blank"><em class="le"/>LinkedIn</a>|<a class="ae kf" href="https://github.com/Poulinakis-Konstantinos" rel="noopener ugc nofollow" target="_blank">GitHub</a></p></div></div>    
</body>
</html>