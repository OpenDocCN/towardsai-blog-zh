<html>
<head>
<title>Machine Learning: How the Gradient Descent Algorithm Works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:梯度下降算法如何工作</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/machine-learning-how-the-gradient-descent-algorithm-works-61682d8570b6?source=collection_archive---------0-----------------------#2018-07-31">https://pub.towardsai.net/machine-learning-how-the-gradient-descent-algorithm-works-61682d8570b6?source=collection_archive---------0-----------------------#2018-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="c7cc" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">梯度下降是如何工作的？| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向AI </a></h2><div class=""/><p id="67f3" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">大多数机器学习算法通过最小化目标函数来执行预测建模，从而学习为了获得预测标签而必须应用于测试数据的权重。最简单的目标(损失)函数是误差平方和(SSE)函数，我们将其表示为J( <strong class="kb jd"> w </strong>):</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/c9a378ed5820e702ec07baffe9486e8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*9iyACkl0A-ad64u8SuqH8w.png"/></div></figure><p id="4ede" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">这里，x表示特征，y表示标签，D表示包含特征和标签的训练数据集，<strong class="kb jd"> w </strong>是通过最小化目标函数从模型中学习的权重。通常使用梯度下降(GD)算法来最小化目标函数。在GD方法中，权重根据以下程序更新:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/b73e3ed1bfcd0c9ebfe71008e925870b.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*YThJMuWpfQmCD2Np_3eSdQ.png"/></div></figure><p id="8b40" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">即在与梯度相反的方向上。这里，eta是一个小的正常数，称为学习率。</p><p id="bd7d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">但是GD算法为什么会起作用呢？</p><h1 id="eefa" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">为什么GD算法有效</strong></h1><p id="4837" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">为了说明，我们考虑一个简单的例子，即2D高斯函数。我们使用Mathematica软件进行计算。</p><pre class="ky kz la lb gt mj mk ml mm aw mn bi"><span id="74b6" class="mo lh it mk b gy mp mq l mr ms"><strong class="mk jd">Plot3D[Exp[-(x² + y²)], {x, -2, 2}, {y, -2, 2}, PlotRange -&gt; All]</strong></span></pre><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/010d03c204104cec07fbed0ac50e29f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*I-8kxHeNQcf4UbGbPrJXRg.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated"><strong class="bd li">高斯函数的3D绘图。该函数在原点，即(0，0)处具有最大值。</strong></figcaption></figure><p id="6b3d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们看到该函数在(0，0)处有一个最大值。现在让我们生成函数的等值线图，并在其上叠加梯度向量方向的单位向量:</p><pre class="ky kz la lb gt mj mk ml mm aw mn bi"><span id="46cf" class="mo lh it mk b gy mp mq l mr ms"><strong class="mk jd">p1 = ContourPlot[Exp[-(x² + y²)], {x, -2, 2}, {y, -2, 2}];</strong></span><span id="4e7d" class="mo lh it mk b gy my mq l mr ms"><strong class="mk jd">p2 = VectorPlot[{-x/Sqrt[x² + y²] , -y/Sqrt[x² + y²]}, {x, -2,2}, {y, -2, 2}];</strong></span><span id="a90c" class="mo lh it mk b gy my mq l mr ms"><strong class="mk jd">Show[p1, p2]</strong></span></pre><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/09cc4abd2d59f67176ef60569721e499.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*cBup98n3vkK4nQ7wnvFL9w.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated"><strong class="bd li">高斯函数的等高线图和梯度向量的矢量场。</strong></figcaption></figure><p id="fa60" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们观察到梯度的单位向量(箭头)的向量场指向原点(0，0)，在这里函数达到其最大值。所以我们看到梯度向量总是指向函数最大值的方向。</p><p id="c6ef" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">因此，以下规则适用:</p><ol class=""><li id="a0de" class="na nb it kb b kc kd kg kh kk nc ko nd ks ne kw nf ng nh ni bi translated">为了最大化几个变量的函数，我们在梯度向量的方向上采取步骤。</li><li id="3f35" class="na nb it kb b kc nj kg nk kk nl ko nm ks nn kw nf ng nh ni bi translated">为了最小化几个变量的函数，我们采取与梯度向量相反的步骤。</li></ol><h1 id="7b28" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">GD算法的例子</strong></h1><p id="1a0b" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">假设我们想最小化目标函数:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b1f32421ebab3b65bdc6718314c41f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*1K0TsRTk7bieDHkZFYT8Nw.png"/></div></figure><p id="39f5" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">显然，这个函数在该点(w_x =0，w_y = 0)有一个全局最小值0。应用带有一些初始猜测(w_x = 2.5，w_y = -1.3)的GD算法，我们可以用几行代码表明该算法收敛到正确的最小值，即收敛到点(0，0)，如下所示:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi np"><img src="../Images/44fba07dd2cb7171f50ab99e6ec727de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*MoOnPb6aq8ruI4wpZKiIqw.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated"><strong class="bd li">出于说明目的的简单目标函数的最小化。</strong></figcaption></figure><p id="be80" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">总之，我们用一个非常简单的例子展示了GD算法是如何工作的。如果你想看看GD算法在一个真实的机器学习分类算法中是如何使用的，请看下面的资源库:<a class="ae nq" href="https://github.com/bot13956/LogisticRegression_gradient_descent" rel="noopener ugc nofollow" target="_blank">https://github . com/bot 13956/LogisticRegression _ gradient _ descent</a></p><p id="fcc7" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">有关GD算法的更多信息，请参见以下书籍:Sebastian Raschka的《Python机器学习》(第2章和第3章)。</p><p id="5006" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">感谢阅读。</p></div></div>    
</body>
</html>