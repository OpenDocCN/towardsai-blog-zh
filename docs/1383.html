<html>
<head>
<title>What is Deep Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是深度学习？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/what-is-deep-learning-34767bb10366?source=collection_archive---------0-----------------------#2021-01-15">https://pub.towardsai.net/what-is-deep-learning-34767bb10366?source=collection_archive---------0-----------------------#2021-01-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/2ccdd1687c2094bab3c1a2de7d0b98a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ejR3Ro0KEwS8U74G6536sQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://pixabay.com/illustrations/web-network-neural-network-1487046/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><h2 id="d5fc" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/editorial" rel="noopener ugc nofollow" target="_blank">社论</a></h2><div class=""/><div class=""><h2 id="e2d5" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">深入探讨“什么是深度学习”,同时提供关于深度学习、其工作原理以及一些示例实现的激动人心的简单介绍</h2></div><p id="8315" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">作者:</strong>桑妮娅·帕维斯，<a class="ae jg" href="https://mktg.best/vguzs" rel="noopener ugc nofollow" target="_blank">罗伯特·伊里翁多</a></p><p id="4fc5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">本教程的代码可在</strong><a class="ae jg" href="https://github.com/towardsai/tutorials/tree/master/deep-learning" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Github</strong></a><strong class="lj jt">上获得，其完整实现也可在</strong><a class="ae jg" href="https://colab.research.google.com/drive/1sV7ZaSegIvR6nL8IYFapSOyd2DBJ-qiY?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Google Colab</strong></a><strong class="lj jt">上获得。</strong></p><div class="is it gp gr iu md"><a href="https://towardsai.net/members" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">加入我们吧↓</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">“走向人工智能”是一个讨论人工智能、数据科学、数据可视化、深度学习的社区</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><h1 id="c33b" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">介绍</h1><p id="a077" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">在过去十年中，深度学习在现实生活应用和研究中的使用变得至关重要，例如，随着图像识别、迁移学习、计算机视觉、推荐系统、语言理解、医疗保健、机器智能等领域的进步[ <a class="ae jg" href="https://www2.cs.duke.edu/courses/spring20/compsci527/papers/Pouyanfar.pdf" rel="noopener ugc nofollow" target="_blank"> 2 </a> ] [ <a class="ae jg" href="https://www.youtube.com/watch?v=TFlV57P8JKo" rel="noopener ugc nofollow" target="_blank"> 9 </a> ]。</p><p id="1fb5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如，如果你曾经使用脸书发布照片，你可能已经见证了他们的技术如何识别照片中的人。这些可以是你的朋友，也可以是朋友的朋友，在大多数情况下，即使照片不清晰模糊，他们的图像识别软件在识别其中的人并自动标记他们上的准确率也很高[ <a class="ae jg" href="https://engineering.fb.com/2017/02/02/ml-applications/building-scalable-systems-to-understand-content/" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]。</p><p id="f810" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，深度学习的应用不仅仅局限于图像识别。本教程将提供一条通往<strong class="lj jt">深度学习</strong>的直截了当的基本路径，同时解释它如何工作、它的应用和它的实现。</p><p id="31c3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们开始吧！</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/83351d3a26a3330dce63898da4adf17e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jw65b47kTwPdC-9qhe5qfQ.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:人工智能(AI) vs机器学习(ML) vs深度学习(DL)。</figcaption></figure><h1 id="edda" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">什么是深度学习？</h1><p id="9968" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated"><strong class="lj jt">深度学习</strong> (DL)是<a class="ae jg" href="https://mld.ai/mldcmu" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">机器学习</strong> </a> (ML)方法论的一个独特子集，使用<a class="ae jg" href="https://towardsai.net/p/machine-learning/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">人工神经网络</strong> </a> (ANNs)不知不觉中受人类大脑中发现的神经元结构的启发。</p><p id="f3f3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在深度学习中，单词<strong class="lj jt"> deep </strong>意义重大，因为它指的是人工神经网络中的大量层。如今，深度学习是一种成功的方法，主要是由于训练数据的大量可用性和相对低成本的GPU用于非常高效的数值计算[ <a class="ae jg" href="https://d2l.ai/chapter_introduction/index.html" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]。</p><p id="b8ea" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从根本上说，深度学习旨在学习从经验中表现世界。把它想象成黑盒中的统计数据，然而，它在学习模式方面非常有效。</p><p id="f4d9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">总而言之:</p><blockquote class="nu"><p id="c7a7" class="nv nw jj bd nx ny nz oa ob oc od mc dk translated">深度学习可以定义为输入和输出之间的一系列层，在类似于人脑的一系列阶段[ <a class="ae jg" href="https://www.cs.brandeis.edu/~cs136a/CS136a_Slides/DeepLearning_Corne.pdf" rel="noopener ugc nofollow" target="_blank"> 14 </a> ]中执行特征识别和处理。它的目标是通过构建模拟大脑中生物神经元功能的算法来模仿人脑。</p></blockquote><h1 id="2e50" class="ms mt jj bd mu mv mw mx my mz na nb nc ky oe kz ne lb of lc ng le og lf ni nj bi translated">机器学习与深度学习</h1><p id="1f96" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">图2解释了机器学习和深度学习的工作流程之间的清晰和概念性的想法:</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6f856ffe772bb434f1d6503697d5e1e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WwcRvBluU8YXligP8Rhzvg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:机器学习与深度学习工作流程。</figcaption></figure><h1 id="4bc7" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">深度学习简史</h1><p id="457a" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">深度学习的早期阶段可以追溯到1943年，当时沃尔特·皮茨和沃伦·T21、麦卡洛克创建了一个基于人脑神经回路的计算机模型。研究人员将算法和其他逻辑结合起来模拟人脑，开发人工神经网络的努力来自<strong class="lj jt">阿列克谢·格里戈里耶维奇·伊瓦赫年科</strong>。</p><p id="4db4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在1979年<strong class="lj jt">，</strong>，<strong class="lj jt">福岛</strong>设计了具有多个池和卷积层的神经网络。他们的设计让计算机能够轻松地学习和识别事物。在<strong class="lj jt"> 1989年</strong>，<strong class="lj jt"> Yann LeCun </strong>在贝尔实验室提供了第一个反向传播的实际演示。他将卷积神经网络与反向传播结合起来，读取“<strong class="lj jt">手写</strong>”数字[ <a class="ae jg" href="https://nyuscholars.nyu.edu/en/publications/handwritten-digit-recognition-applications-of-neural-net-chips-an-2" rel="noopener ugc nofollow" target="_blank"> 15 </a> ]。</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/6e76aec376aad0b67ce7306b3cb9cd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Iih4pAsM4fzu5Bdu.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3:深度学习进化相关简史[ <a class="ae jg" href="https://dreaven.github.io/papers/deep_learning_lecture_at_columbia.pdf" rel="noopener ugc nofollow" target="_blank"> 10 </a> ]。</figcaption></figure><h1 id="a6c3" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">深度学习的有用术语</h1><p id="fce9" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">查看深度学习中使用的以下术语:</p><ul class=""><li id="1f80" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated"><strong class="lj jt">模型</strong>:模型是通过实现算法从数据中学习到的<strong class="lj jt">特定表示</strong>。模型也被称为<strong class="lj jt">假设</strong>。</li><li id="fdc5" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">特征:</strong>特征是我们数据的一个具体的、可测量的属性。特征向量可以方便地描述一组数字特征。深度学习模型将特征向量作为输入。例如，为了预测一种水果，可能有颜色、气味、味道等特征。</li><li id="28c9" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">目标(标签):</strong>目标变量或标签是我们的模型要预测的值。</li><li id="e5d8" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">训练:</strong>方法是给定一组输入(特征)，它是必需的输出(标签)，所以在训练之后，它有一个模型(<strong class="lj jt">假设</strong>)，然后将新数据映射到被训练的类别之一。</li><li id="4a0b" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">预测:</strong>一旦一个模型准备好了，就可以给它输入一组输入来提供一个预测的输出(标签)。</li><li id="ceee" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">历元:</strong>历元是一种超参数，它是一次迭代，由一次前向传递和一次后向传递组成。由于一个历元不能应用于一个巨大的数据集，它被划分成多个批次。迭代是指完成一个历元所需的批次数量。例如，如果数据集中有<em class="ow"> 5000个</em>样本，并且需要找到一个时期。假设有500个批次，那么将有<strong class="lj jt"> 10次迭代</strong>来完成1个历元。</li><li id="826e" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">神经元</strong>:在深度学习中，神经元是模拟<strong class="lj jt">生物神经元功能</strong>的数学函数。通常，一个神经元计算其输入的加权平均值，这个和通过一个非线性函数传递，通常称为激活函数，如<strong class="lj jt"> sigmoid </strong>。</li><li id="6d80" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">轴突:</strong>它们是我们神经系统的抽象表示，神经系统包含一组神经元，这些神经元通过所谓的<strong class="lj jt">轴突相互通信。</strong></li><li id="f69e" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">层:</strong> A层是深度学习中最高级的构建块。层是一个容器，通常接受加权输入，用一组非线性函数对其进行转换，然后将这些值作为输出传输到下一层。</li><li id="a130" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">密集层:</strong>密集层表示矩阵向量乘法。<strong class="lj jt"/>密集层是规则的、深度连接的<strong class="lj jt">神经网络</strong>层。这是最常见和最常用的层。</li></ul><h1 id="9f7f" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">神经元</h1><p id="3794" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">生物神经元激发了深度学习神经元的形成。</p><h1 id="e28e" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">生物神经元</h1><p id="0ed6" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">生物神经元由以下组成:</p><ul class=""><li id="c8df" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated"><strong class="lj jt">一个细胞体或胞体。</strong></li><li id="5577" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">一个或多个树突</strong>:其主要职责是接收来自其他神经元的信号。</li><li id="b3d0" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated"><strong class="lj jt">轴突:</strong>它将同一神经元产生的信号传递给其他相连的神经元。</li></ul><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bd773c49dd3bf0056d63a46d4fd4b270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xfK0dtTz4AD5MF-1.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:一个生物神经元。| BruceBlaus，<a class="ae jg" href="https://creativecommons.org/licenses/by/3.0" rel="noopener ugc nofollow" target="_blank"> CC BY 3.0 </a>，via <a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png" rel="noopener ugc nofollow" target="_blank"> Wikimedia Commons </a></figcaption></figure><p id="9fce" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从一个阶段到另一个阶段的过渡是由树突拾取的信号所代表的外部刺激引起的。每个信号都有兴奋或抑制作用[ <a class="ae jg" href="https://www.amazon.com/Deep-Learning-TensorFlow-Explore-networks/dp/1786469782" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]。神经元处于积累所有接收信号的空闲状态。</p><h1 id="16c0" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">人工神经元</h1><p id="a227" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">类似于生物过程，人工神经元包括:</p><ul class=""><li id="d884" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated">为了从一个或多个输出连接中收集数字信号，一个或多个输入链接携带其他<strong class="lj jt">神经元的信号</strong>。</li><li id="2b67" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">每个神经元给人工神经元用来考虑发送的每个信号的链路分配一个<strong class="lj jt">权重</strong>。</li><li id="da78" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">一个或多个输出链路为其他神经元传送信号。</li><li id="01cf" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">一个<strong class="lj jt">激活函数</strong>确定输出信号的数值，该数值基于从与其他神经元的输入连接接收的信号。</li></ul><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/b0186f4ac6d51dd32bd4f9f409702099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G_FbPWxGLCqxf8Y9"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图5:感知器的图形表示，神经网络模型|杰拉多·弗朗西斯科·佩雷斯·拉耶德拉，<a class="ae jg" href="https://creativecommons.org/licenses/by/3.0" rel="noopener ugc nofollow" target="_blank"> CC BY 3.0 </a>，通过UPCommons，开放访问[ <a class="ae jg" href="https://upcommons.upc.edu/bitstream/handle/2117/117737/131440.pdf?sequence=1&amp;isAllowed=y" rel="noopener ugc nofollow" target="_blank"> 11 </a></figcaption></figure><p id="66c5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以下是人工神经元的方程式:</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/4577e3108149dbab9744c0af37e604f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*yRQBulan6c60Sk2C.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图6:人工神经元的方程式。</figcaption></figure><p id="a7dd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，总结神经元:</p><blockquote class="nu"><p id="9c1f" class="nv nw jj bd nx ny nz oa ob oc od mc dk translated"><em class="oz">神经元是神经网络的基本单位。一个神经元接受输入，对它们进行一些数学运算，并产生一个输出[ </em> <a class="ae jg" href="https://victorzhou.com/blog/intro-to-neural-networks/" rel="noopener ugc nofollow" target="_blank"> <em class="oz"> 5 </em> </a> <em class="oz"> ]。</em></p></blockquote><p id="654f" class="pw-post-body-paragraph lh li jj lj b lk pa kt lm ln pb kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">图7显示了一个双输入神经元的示意图:</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/f38e8508d199f7fd94ae3b66d3fd64c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rkIchOchX3oQAp9x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7:由<a class="ae jg" href="https://victorzhou.com/" rel="noopener ugc nofollow" target="_blank">Victor Zhou</a>[<a class="ae jg" href="https://victorzhou.com/blog/intro-to-neural-networks/" rel="noopener ugc nofollow" target="_blank">5</a>[<a class="ae jg" href="https://github.com/vzhou842/victorzhou.com/blob/master/NOTICE" rel="noopener ugc nofollow" target="_blank">12</a>]创作的<a class="ae jg" href="https://github.com/vzhou842/victorzhou.com/blob/master/NOTICE" rel="noopener ugc nofollow" target="_blank"> MIT许可</a>下的双输入神经元、知识共享图像。</figcaption></figure><p id="2811" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图6神经元中发生了以下计算:</p><p id="6af0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">每个输入都乘以一个权重:</strong></p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/29f9bfdff3fee6ad3e565d84ed2dfed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/0*C4iaUspjA_JKPcT-.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:输入乘以权重。</figcaption></figure><p id="cdba" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">所有加权输入与<em class="ow"> a </em>偏差<em class="ow"> b </em> : </strong>相加</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/46a4cb7defebe415f1b03098bcb0dda7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xm3hdFY2gNXtE-QX.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:加权输入与偏差b相加。</figcaption></figure><p id="1390" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">最后，总和通过一个激活函数传递:</strong></p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/b62e1a6ddf72eed2749f2a3b417d56f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-_ZLAYgXKiiKkrkT.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:激活函数方程。</figcaption></figure><p id="96d9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">激活函数</strong>用于将一个无界输入转换成一个友好的、可预测形式的输出。</p><h1 id="6a98" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">在神经网络上组合神经元</h1><p id="e3ad" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">神经网络只不过是一组神经元的组合。图11显示了一个简单的人工神经网络的样子:</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/60d3f5f48d7ce85ecdf2ad64cd7474a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/0*G50yFrjnvXy2LUJY.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:前馈神经网络[ <a class="ae jg" href="https://towardsai.net/p/machine-learning/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e" rel="noopener ugc nofollow" target="_blank"> 8 </a> ]的表示。</figcaption></figure><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/8519b06048d39c85f5dc6546365992c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NXSkyfnvOHgWUE8s.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12:感知机:大脑中信息存储和组织的概率模型[16] |来源:康奈尔航空实验室的弗兰克·罗森布拉特的Mark I感知机。纽约水牛城，1960年[17]</figcaption></figure><h1 id="3384" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">感知器</h1><p id="bd49" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">感知器是一个基本的单层神经网络。如此直白，其核心就是一个简单化的激活函数，一个简单的二元函数，只有两种可能和可计算的结果。图13展示了一个人工感知器:</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/fd6d872ad4c8989dd63102226713e8a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/0*7sosDbRQY_i4cScI.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图13:感知器(p) [ <a class="ae jg" href="https://towardsai.net/p/machine-learning/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e" rel="noopener ugc nofollow" target="_blank"> 8 </a>的表示。</figcaption></figure><p id="e3b1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感知器是由科学家<strong class="lj jt">弗兰克·罗森布拉特</strong>在20世纪50年代和60年代开发的。感知器接受许多二进制输入(u1，u2，...)并产生单个二进制输出(比如v1)。感知器可以有多个输入，但只输出一个二进制标签。</p><p id="0011" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感知器由以下组件组成:</p><ul class=""><li id="c68c" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated">分对数</li><li id="15ba" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">阶跃激活函数</li></ul><h1 id="2c36" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">分对数</h1><p id="c28a" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated"><strong class="lj jt"> logit </strong>的方程与直线<strong class="lj jt">(y = MX+c)</strong>的方程共振，如图14所示:</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/472ac7f6074960866bc1f45c8d60cce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*ceZ2DizLv5Qa86IqFx3UeQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图14:直线方程</figcaption></figure><p id="dc99" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">类似地，logit函数如图15所示:</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/4a01ab54ee9d16a7b2b03e6f52bc6f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/0*yBKB0jU52wUchy2b.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图logit函数的等式</figcaption></figure><p id="78bc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中<em class="ow"> w </em>是应用于每个输入的权重，<em class="ow"> b </em>是偏置项。</p><h1 id="a22e" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">阶跃激活函数</h1><p id="12d9" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">阶跃激活函数表明，给定logit的值，是否应该将神经元从这种感知中驱逐出去。图16显示了逐步激活功能:</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi po"><img src="../Images/8d91e7db930b962f9667dcc4f9c94b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/0*xUyzIYzZz-9Lz1oZ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图16:步进激活功能，Omegatron，<a class="ae jg" href="https://creativecommons.org/licenses/by-sa/3.0" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 3.0 </a>，通过<a class="ae jg" href="https://commons.wikimedia.org/" rel="noopener ugc nofollow" target="_blank">维基共享</a></figcaption></figure><p id="8fa7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，从上图来看，只有当logit函数值大于或等于<strong class="lj jt"> 0 </strong>时，神经元才会被激发。</p><p id="3bb6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">决策边界的两种情况:</p><ul class=""><li id="a8ee" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated">在单输入感知器的情况下，判定边界是一条线性线。</li><li id="5c4e" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">在多输入感知器的情况下，决策边界扩展到一个超平面，该超平面比它所在的表面的维度小一个维度。</li></ul><p id="54a4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">总而言之:</p><blockquote class="pp pq pr"><p id="f359" class="lh li ow lj b lk ll kt lm ln lo kw lp ps lr ls lt pt lv lw lx pu lz ma mb mc im bi translated"><strong class="lj jt"> <em class="jj">感知器= logit +阶跃函数</em> </strong></p></blockquote><h1 id="8aed" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">层</h1><p id="3ce2" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">深度学习中的<strong class="lj jt">层</strong>是一个常用术语，适用于神经网络中特定深度的几个节点一起执行。每一层都试图通过最小化误差/成本函数来学习数据的不同方面。</p><p id="4c24" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如，在图像识别中，第一层可以学习边缘检测，第二层可以检测眼睛，第三层可以检测鼻子，等等。</p><p id="9041" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">不同的公共层:</p><ul class=""><li id="cb95" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated">输入层</li><li id="9474" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">隐蔽层</li><li id="c9af" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">输出层</li></ul><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/af39ed352ee6977b6313871f8744fe1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8ciqlZg39Ys5me9l.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图17:神经网络中的不同层[ <a class="ae jg" href="https://towardsai.net/p/machine-learning/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e" rel="noopener ugc nofollow" target="_blank"> 8 </a> ]。</figcaption></figure><h1 id="bab3" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">输入层(输入像元)</h1><p id="26ad" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">该层包含原始数据。神经网络的<strong class="lj jt">输入层</strong>包括人工<strong class="lj jt">输入</strong>神经元，并通过后续的<strong class="lj jt">人工神经元层</strong>开始将初始数据输入系统进行进一步处理。<strong class="lj jt">输入层</strong>是神经网络工作流程的起点。</p><h1 id="4906" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">隐藏层(隐藏单元格)</h1><p id="d5f6" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated"><strong class="lj jt">在算法的输入和输出之间存在一个</strong>隐藏层，其中函数对输入使用权重，并引导它们通过一个激活函数作为输出。因此，<strong class="lj jt">隐藏层</strong>对进入网络的输入进行非线性改变。</p><h1 id="9816" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">输出图层(输出像元)</h1><p id="4901" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">输出层是最简单的，通常由分类问题的单个输出组成。它是单个节点，但仍被视为神经网络中的<em class="ow">层</em>，因为它可以包含多个节点。输出层负责产生最终结果。神经网络中必须始终有一个输出层。</p><h1 id="749a" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">重量</h1><p id="88ae" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">权重是连接的力量。如果它增加了投入，那么它对产出的影响有多大。权重<strong class="lj jt"> </strong>控制两个神经元之间的信号(或连接强度)。简而言之，权重决定了输入对输出的影响程度。</p><p id="078f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">示例:</p><p id="c572" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们承担三个任务:1)玩，2)工作，3)睡觉。</p><p id="dfc1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们更加重视“<strong class="lj jt"> play”，</strong>那么“play”将比其他选项具有更高的权重。</p><p id="cdf4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，在等式中:</p><blockquote class="pp pq pr"><p id="c492" class="lh li ow lj b lk ll kt lm ln lo kw lp ps lr ls lt pt lv lw lx pu lz ma mb mc im bi translated"><em class="jj"> Y = F(x1，x2，x3)=w1*x1 + w2*x2 + w3*x3 </em></p></blockquote><p id="f268" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">计算<strong class="lj jt"> Y，</strong> x1将比x2和x3更重要。因此，play的功能是play(x1)。</p><h1 id="f4b0" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">学习率</h1><p id="5a2b" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">学习率是定义更新步骤在多大程度上控制权重的当前值的参数。<strong class="lj jt">学习率</strong>指的是优化算法中的调整参数，它定义了每次迭代的步长，同时向最小损失函数移动。【<a class="ae jg" href="https://en.wikipedia.org/wiki/Learning_rate" rel="noopener ugc nofollow" target="_blank"> 6 </a>】。</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/184c677ec0c31c0594f6f8d9b34fb44d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*dLTHvNbctD8Utqgong4jyw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图18:学习率</figcaption></figure><p id="d8f4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">总结一下:</p><blockquote class="nu"><p id="c0ff" class="nv nw jj bd nx ny nz oa ob oc od mc dk translated">学习率控制神经网络模型学习问题的快慢。</p></blockquote><h1 id="a927" class="ms mt jj bd mu mv mw mx my mz na nb nc ky oe kz ne lb of lc ng le og lf ni nj bi translated">人工神经网络(ann)是如何学习的？</h1><p id="2242" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">人工神经网络的学习过程被设置为优化其权重的迭代过程，因此是受监督类型[ <a class="ae jg" href="https://d2l.ai/chapter_introduction/index.html" rel="noopener ugc nofollow" target="_blank"> 3 </a> ][ <a class="ae jg" href="https://www.amazon.com/Deep-Learning-TensorFlow-Explore-networks/dp/1786469782" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]。</p><h1 id="a52d" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">重量衰减</h1><p id="bbb9" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">权重衰减是用于避免过度拟合的正则化技术。它是通过删除元素来降低模型复杂性的一种概括。神经网络也会过度拟合。可以通过将所有权重/偏差初始化为最小随机值并在学习过程中增加它们来避免。</p><p id="0db0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些是体重下降的不同步骤:</p><ul class=""><li id="6c32" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated">人们可以在验证集上检查性能并尽早停止。</li><li id="6d61" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">或者，可以改变更新规则以阻止大的权重:</li></ul><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/b574b47f95a53fafbf7621f7177fce70.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/0*tLInLwMke-nWjaYw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图19:重量衰减。</figcaption></figure><ul class=""><li id="6695" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated">使用x验证设置lambda的值。</li></ul><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi px"><img src="../Images/e4a33891e1e9d1b264c0573d65cf97a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*V0OqsuXwNo2WaGtn.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图20:不同重量衰减超参数[ <a class="ae jg" href="https://cedar.buffalo.edu/~srihari/CSE676/5.3%20MLBasics-Hyperparam.pdf" rel="noopener ugc nofollow" target="_blank"> 13 </a> ]。</figcaption></figure><h1 id="87fe" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">神经元的实现</h1><p id="cbde" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">下面的Python代码实现旨在创建一个人工神经元。</p><p id="58c9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">导入数量:</p><pre class="nq nr ns nt gt py pz qa qb aw qc bi"><span id="575f" class="qd mt jj pz b gy qe qf l qg qh">import numpy as np</span></pre><p id="1e42" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">创建一个Sigmoid函数:</p><pre class="nq nr ns nt gt py pz qa qb aw qc bi"><span id="5c56" class="qd mt jj pz b gy qe qf l qg qh">def sigmoid(x):<br/>   return 1/ (1 + np.exp(-x))</span></pre><p id="3b49" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">创建神经元:</p><pre class="nq nr ns nt gt py pz qa qb aw qc bi"><span id="0bc8" class="qd mt jj pz b gy qe qf l qg qh">class Neuron:<br/>   <br/>   def __init__(self, weights, bias):<br/>        self.weights = weights<br/>        self.bias = bias</span><span id="948a" class="qd mt jj pz b gy qi qf l qg qh">   def feedforwards(self, inputs):<br/>        total = np.dot(self.weights, inputs) + self.bias<br/>        return sigmoid(total)</span></pre><p id="75f9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">神经元的输入和执行:</p><pre class="nq nr ns nt gt py pz qa qb aw qc bi"><span id="189d" class="qd mt jj pz b gy qe qf l qg qh">weights = np.array([0, 1])<br/>bias = 4<br/>neuron = Neuron(weights, bias)<br/>x = np.array([2, 3])<br/>forward = neuron.feedforwards(x)<br/>print(forward)</span></pre><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/0d7d55a782c4c887fcfedcb3a98b715f.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/0*wNiIBls9yNVo6bAh.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图21:神经元的输出。</figcaption></figure><h1 id="fe96" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">反向传播</h1><p id="9490" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">在深度学习中，反向传播是一种用于训练多层感知机的监督学习算法。</p><h1 id="e715" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">为什么需要反向传播？</h1><p id="fea8" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">在创建神经网络时，在开始时，用一些随机值或事实的任何变量来初始化权重。显然，数据科学家不是超人。选定的权重值不必是正确的或最适合模型的。</p><p id="afd7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数据科学家最初选择一些值，但我们的模型输出与我们的实际输出相差甚远，即误差值很大。</p><p id="7374" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">现在，他们将如何减少误差？</strong></p><p id="1320" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基本上，数据科学家需要做的是解释模型，以改变参数(权重)，使误差最小。</p><p id="b0a9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">训练模型的步骤:</p><ul class=""><li id="f48e" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated">计算误差</li><li id="7433" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">最小误差</li><li id="3ff9" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">更新参数—如果误差很大，更新参数(权重和偏差)。之后，再次检查错误。重复该过程，直到误差最小。</li><li id="eae1" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">该模型已准备好进行预测。</li></ul><p id="427a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，他们需要训练模型，反向传播是一种训练模型的方法。反向传播算法使用delta规则或梯度下降技术在权重空间中寻找误差函数的最小值[ <a class="ae jg" href="https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md" rel="noopener ugc nofollow" target="_blank"> 7 </a> ]。</p><p id="6b61" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">反向传播的步骤:</p><ul class=""><li id="d09f" class="oi oj jj lj b lk ll ln lo lq ok lu ol ly om mc on oo op oq bi translated">首先，将某个随机值初始化为“W ”,并向前传播。</li><li id="ce5d" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">然后，注意到有一些错误。该方法将其向后传播以降低错误率并增加“w”的值</li><li id="b8f2" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">之后，也注意到误差增加了。人们开始知道“W”的值不能增加。</li><li id="4be0" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">因此，它再次向后传播，并且“W”的值减小。</li><li id="3c52" class="oi oj jj lj b lk or ln os lq ot lu ou ly ov mc on oo op oq bi translated">现在，注意到误差已经减小。</li></ul><p id="fd81" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基本上，它试图达到“全球损失最小化”</p><figure class="nq nr ns nt gt iv gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/d9184d18822344aa092434a9af8ed5ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*oZhKk4LXn9pL0rQoK4Xsdw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图22:全局损失最小。</figcaption></figure><h1 id="ad3b" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">结论</h1><p id="812a" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">深度学习是机器学习的一部分，专注于人工神经网络。这是一种人工智能技术，指导计算机像人类一样执行精确的任务。</p><p id="14ce" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">深度学习模型由于其高准确性而获得了越来越多的关注，有时其水平高于人类的表现。在深度学习中，计算机可以对图像、文本、声音等进行分类。</p><p id="888c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些模型使用大型神经架构和大量标记数据进行训练。如今，深度学习被用于所有行业的各种任务。神经网络是一种建模输入/输出函数的灵活方式。它对噪声数据是鲁棒的，并且可以使用权重衰减或早期停止来避免过拟合。</p></div><div class="ab cl ql qm hx qn" role="separator"><span class="qo bw bk qp qq qr"/><span class="qo bw bk qp qq qr"/><span class="qo bw bk qp qq"/></div><div class="im in io ip iq"><p id="f161" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">免责声明:</strong>本文表达的观点仅代表作者个人观点，不代表卡内基梅隆大学或其他(直接或间接)与作者相关的公司的观点。这些文章并不打算成为最终产品，而是当前思想的反映，同时也是讨论和改进的催化剂。</p><p id="fc0e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">除非另有说明，所有图片均来自作者。</strong></p><p id="b4a3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过<a class="ae jg" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向艾</a>发布</p><h1 id="7124" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">资源</h1><h2 id="a8bc" class="qd mt jj bd mu qs qt dn my qu qv dp nc lq qw qx ne lu qy qz ng ly ra rb ni jp bi translated">教程的伴侣</h2><p id="7f1a" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated"><a class="ae jg" href="https://github.com/towardsai/tutorials/tree/master/deep-learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> Github库</strong> </a>。</p><p id="cfe4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://colab.research.google.com/drive/1sV7ZaSegIvR6nL8IYFapSOyd2DBJ-qiY?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> Google Colab实现</strong> </a> <strong class="lj jt">。</strong></p><h2 id="aa0b" class="qd mt jj bd mu qs qt dn my qu qv dp nc lq qw qx ne lu qy qz ng ly ra rb ni jp bi translated">书</h2><p id="f9e1" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">如果你真的对深度学习感兴趣，下面这本书可能是最好的伴侣，而且是免费的</p><p id="5f7a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://d2l.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">潜入深度学习</strong> </a>、阿斯顿·张、扎卡里·c·利普顿、李牧和亚历山大·j·斯莫拉</p><h1 id="03a6" class="ms mt jj bd mu mv mw mx my mz na nb nc ky nd kz ne lb nf lc ng le nh lf ni nj bi translated">参考</h1><p id="6a42" class="pw-post-body-paragraph lh li jj lj b lk nk kt lm ln nl kw lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">[1]坎德拉，J. (2017)。构建可扩展的系统来理解内容——脸书工程。检索于2021年1月15日，来自<a class="ae jg" href="https://engineering.fb.com/2017/02/02/ml-applications/building-scalable-systems-to-understand-content/" rel="noopener ugc nofollow" target="_blank">https://engineering . FB . com/2017/02/02/ml-applications/building-scalable-systems-to-understand-content/</a></p><p id="2c48" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]《深度学习的调查:算法、技术和应用》，Poyanfar等人，(2021)。检索于2021年1月15日，来自<a class="ae jg" href="https://www2.cs.duke.edu/courses/spring20/compsci527/papers/Pouyanfar.pdf" rel="noopener ugc nofollow" target="_blank">https://www2 . cs . duke . edu/courses/spring 20/compsci 527/papers/pouyanfar . pdf</a></p><p id="e72b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3]深入研究深度学习—深入研究深度学习0.16.0文档，Aston Zhang和Zachary C. Lipton以及李牧和Alexander J. Smola。(2021).检索于2021年1月15日，来自https://d2l.ai/<a class="ae jg" href="https://d2l.ai/" rel="noopener ugc nofollow" target="_blank"/></p><p id="45e5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4] Zaccone，Giancarlo等<em class="ow">tensor flow深度学习:借助TensorFlow </em>的力量，让你的机器学习知识更上一层楼。帕克特出版社，2017年。</p><p id="db51" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]维克多·周。"初学者的机器学习:神经网络导论."<em class="ow">周胜利</em>，周胜利，<a class="ae jg" href="https://victorzhou.com/blog/intro-to-neural-networks/." rel="noopener ugc nofollow" target="_blank"/></p><p id="c5e2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6]“学习率。”<em class="ow">维基百科</em>，维基媒体基金会，2021年1月4日，<a class="ae jg" href="https://en.wikipedia.org/wiki/Learning_rate" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Learning_rate</a>。</p><p id="d775" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[7]阿列克谢·格里戈里耶夫(2021)。检索于2021年1月11日，来自<a class="ae jg" href="https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md" rel="noopener ugc nofollow" target="_blank">https://github . com/alexeygrigorev/data-science-interversations/blob/master/theory . MD</a></p><p id="1a54" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[8]神经网络的主要类型及其应用—教程。(2021).检索于2021年1月15日，来自<a class="ae jg" href="https://towardsai.net/p/machine-learning/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e" rel="noopener ugc nofollow" target="_blank">https://toward sai . net/p/machine-learning/main-types-of-neural-networks-and-its-applications-tutorial-734480 D7 ec8e</a></p><p id="d7fc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[9]深度学习第一部分，Ruslan Salakhutdinov，(2021)。于2021年1月15日从<a class="ae jg" href="https://www.youtube.com/watch?v=TFlV57P8JKo" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=TFlV57P8JKo</a>检索</p><p id="c095" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[10]《深度学习导论》，权耕，哥伦比亚大学，(2021)。检索于2021年1月15日，来自<a class="ae jg" href="https://dreaven.github.io/papers/deep_learning_lecture_at_columbia.pdf" rel="noopener ugc nofollow" target="_blank">https://dreaven . github . io/papers/deep _ learning _ lecture _ at _ Columbia . pdf</a></p><p id="c849" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[11]使用深度学习对点云进行实例分割，杰拉多·弗朗西斯科·佩雷斯·拉耶德拉，UPC，(2021)。检索于2021年1月15日，来自<a class="ae jg" href="https://upcommons.upc.edu/bitstream/handle/2117/117737/131440.pdf?sequence=1&amp;isAllowed=y" rel="noopener ugc nofollow" target="_blank">https://up commons . UPC . edu/bitstream/handle/2117/117737/131440 . pdf？sequence=1 &amp; isAllowed=y </a></p><p id="ba7d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[12] Victor Zhou，MIT License，Github，【https://Github . com/vzhou 842/Victor Zhou . com/blob/master/NOTICE</p><p id="07c8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[13]超参数和验证集，Sargur N. Srihari，(2021年)。检索于2021年1月15日，来自<a class="ae jg" href="https://cedar.buffalo.edu/~srihari/CSE676/5.3%20MLBasics-Hyperparam.pdf" rel="noopener ugc nofollow" target="_blank">https://cedar . buffalo . edu/~ Sri Hari/CSE 676/5.3% 20m lbasics-hyperparam . pdf</a></p><p id="f112" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[14]《深度学习导论》，大卫·沃尔夫·康恩，开放式课件，(2021)。检索于2021年1月15日，来自<a class="ae jg" href="https://www.cs.brandeis.edu/~cs136a/CS136a_Slides/DeepLearning_Corne.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . brandeis . edu/~ cs36a/cs36a _ Slides/deep learning _ corne . pdf</a></p><p id="f0a3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[15] Y. LeCun，L. D. Jackel，B. Boser，J. S. Denker，H. P. Graf，I. Guyon，D. Henderson，R. E. Howard和W. Hubbard:手写数字识别:神经网络芯片和自动学习的应用，载于Fogelman，f .和Herault，j .和Burnod，y .(编辑)，<em class="ow">《神经计算、算法、结构和应用》，Springer，Les Arcs，法国，1989年。</em></p><p id="7f6c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[16]康奈尔航空实验室的弗兰克·罗森布拉特的Mark I感知机。纽约布法罗，1960 | Instagram，卡耐基梅隆大学机器学习系|<a class="ae jg" href="https://www.instagram.com/p/Bn_s3bjBA7n/" rel="noopener ugc nofollow" target="_blank">https://www.instagram.com/p/Bn_s3bjBA7n/</a></p><p id="6cfb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[17]反向传播|维基百科|<a class="ae jg" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Backpropagation</a></p></div></div>    
</body>
</html>