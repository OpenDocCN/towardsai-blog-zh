<html>
<head>
<title>Image Captioning with CLIP and GPT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带剪辑和GPT的图像字幕</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/image-captioning-with-clip-and-gpt-d0cb3f3fddda?source=collection_archive---------0-----------------------#2021-12-24">https://pub.towardsai.net/image-captioning-with-clip-and-gpt-d0cb3f3fddda?source=collection_archive---------0-----------------------#2021-12-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="3372" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="38d8" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用剪辑和GPT模型轻松生成图像的文本描述！</h2></div><blockquote class="kr ks kt"><p id="2253" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/clipcap/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/clipcap/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><figure class="ls lt lu lv gt lw"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="b765" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">我们已经看到人工智能使用GANs从其他图像生成图像。然后，有模型能够使用文本生成有问题的图像。2021年初，<a class="ae lr" rel="noopener ugc nofollow" target="_blank" href="/openais-dall-e-text-to-image-generation-explained-1f6fb4bb5a0a?source=your_stories_page----------------------------------------"> DALL-E </a>发表，击败了所有之前使用CLIP从文本输入中生成图像的尝试，CLIP是一种以文本为导向链接图像的模型。一个非常相似的任务叫做图像字幕，听起来可能很简单，但实际上也很复杂。它是机器生成图像的自然描述的能力。事实上，这几乎与机器理解图像和它生成的文本一样困难，就像文本到图像的合成一样。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/f39b4041da9bfac6e4b93428678a0b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*P3G4OsyxAddNig13uqRJZg.jpeg"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">生成的标题:一串香蕉放在桌子上</figcaption></figure><p id="b0e6" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">简单地标记你在图像中看到的对象是很容易的。这可以使用经典的分类器模型来完成。但是要理解在一张二维图片中发生了什么却是另一个挑战。人类可以很容易地做到这一点，因为我们可以根据过去的经验进行插值，我们甚至可以将自己置于照片中人的位置，并快速了解正在发生的事情。对于一台只能看到像素的机器来说，这是一个完全不同的挑战。然而，研究人员发表了一个惊人的新模型，它在这方面做得非常好。</p><p id="94ef" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">正如研究人员明确指出的，“图像字幕是视觉语言理解中的一项基本任务”，我完全同意。结果很棒，但更酷的是它是如何工作的，所以让我们深入研究一下这个模型及其内部工作原理…</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mj"><img src="../Images/93e632770e9f3cdadb73781e531f90a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3MxWLAY4HHRSHr5s_K2RA.jpeg"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">生成的标题:学生们正在观赏樱花</figcaption></figure><p id="39ce" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">在这种情况下，研究人员使用CLIP来完成这项任务。如果你不熟悉CLIP是如何工作的，或者为什么它如此神奇，我强烈邀请你阅读我写的许多文章中的一篇。简而言之，CLIP通过将两种类型的数据编码到一个相似的表示中来将图像和文本链接起来，以便进行比较。这就像用一段简短的摘要来比较电影和书籍一样。仅给出这样一个概要，你就能说出它是关于什么的，并对两者进行比较，但你不知道它是关于一部电影还是一本书。在这种情况下，电影是图像，书籍是文本描述。<br/>然后，CLIP创建自己的摘要，使用bits差异的距离计算对两个片段进行简单的比较。您已经可以看到CLIP似乎非常适合这个任务，但是它需要更多的工作来满足我们的需求。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi mo"><img src="../Images/8d3ae5b3cbff7c3c101afb7cce8ffe34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zxEuIWPRWajIalAy.png"/></div></a></figure><p id="73ce" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">在这里，CLIP将简单地用作比较文本输入和图像输入的工具，所以我们仍然需要生成这样一个可能描述图像的文本。<br/>他们不是用CLIP的编码将文本与图像进行比较，而是简单地使用CLIP的网络对图像进行编码，并使用这种生成的编码信息作为一种方式来指导未来使用另一种模型的文本生成过程。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mp"><img src="../Images/fede13489f12b3da683b0e3595213ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8RLzDpMfi6sLScqx2SguaA.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">ClipCap模型。图片来自报纸。</figcaption></figure><p id="1eed" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">像GPT-3这样的任何语言模型都可以执行这样的任务，这可以改善结果，但研究人员选择了它的前身GPT-2，这是强大的OpenAI模型的一个更小更直观的版本。他们基本上是使用剪辑的编码来调节GPT 2的文本生成。所以CLIP的模型已经训练好了，他们使用了GPT-2的预训练版本，他们将使用CLIP的编码作为指导来进一步训练，以确定文本生成的方向。这并不简单，因为他们仍然需要将剪辑的编码转换为GPT-2可以理解的表示形式，但这也并不复杂。它将简单地学习将剪辑的编码转换成与典型的单词嵌入具有相同维度的多个向量。</p><p id="a33d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">学习如何将CLIP的输出与GPT-2的输入相匹配的这一步骤将在培训期间教授，因为GPT-2和CLIP都已经是完成各自任务的训练有素的强大模型。所以你可以把这看作是第三种学习模式，称为映射网络，唯一的责任是把一种语言翻译成另一种语言，这仍然是一项具有挑战性的任务。如果你对这种映射网络的实际架构感到好奇，他们尝试了简单的多层感知器或MLP和变压器架构，证实了后者更强大，可以学习一组细致的嵌入，当使用强大的预训练语言模型时，这些嵌入将更适合这项任务。如果你不熟悉变形金刚，你应该花5分钟时间阅读我写的关于变形金刚的文章，因为在不久的将来，你只会更频繁地遇到这种类型的网络。</p><p id="9587" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">这个模型非常简单，而且功能极其强大。想象一下，剪辑与GPT-3以这样的方式合并。我们可以使用这样的模型来自动描述电影，或者为盲人和视障人士创建更好的应用程序。这对于现实世界的应用程序来说非常令人兴奋！当然，这只是这个新模型的一个简单概述，您可以在下面的描述中找到关于实现的更多细节。我希望你喜欢这篇文章，如果是的话，请花一点时间和一个可能会感兴趣的朋友分享一下。</p><p id="f45a" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">感谢您的阅读，请继续关注我的下一篇文章，这是今年的最后一篇，也是相当令人兴奋的一篇！</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/d2c1878ab5e79ab4ebdb9c0d7054eee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*1XBD2R-dB0UvSV-W181KAQ.jpeg"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk translated">生成的描述:几个人站在一头大象旁边</figcaption></figure><figure class="ls lt lu lv gt lw gh gi paragraph-image"><a href="https://www.louisbouchard.ai/learnai/"><div class="gh gi mq"><img src="../Images/31b7f4eca120f08639da2c82f78c562f.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*7rsII-WO6_oYP6FH.png"/></div></a></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="63d0" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld lz lf lg lh ma lj lk ll mb ln lo lp lq im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)，并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">简讯</strong> </a>！</p><h2 id="72fd" class="my mz it bd na nb nc dn nd ne nf dp ng lz nh ni nj ma nk nl nm mb nn no np iz bi translated">支持我:</h2><ul class=""><li id="2a36" class="nq nr it kx b ky ns lb nt lz nu ma nv mb nw lq nx ny nz oa bi translated">支持我的最好方式是成为这个网站<strong class="kx jd"> </strong>的会员，或者如果你喜欢视频格式，在<strong class="kx jd"> YouTube </strong>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="597e" class="nq nr it kx b ky ob lb oc lz od ma oe mb of lq nx ny nz oa bi translated">在<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a>跟我来</li><li id="7b15" class="nq nr it kx b ky ob lb oc lz od ma oe mb of lq nx ny nz oa bi translated">想进入AI或者提升自己的技能，<a class="ae lr" href="https://www.louisbouchard.ai/learnai/" rel="noopener ugc nofollow" target="_blank">看这个</a>！</li></ul><h2 id="6799" class="my mz it bd na nb nc dn nd ne nf dp ng lz nh ni nj ma nk nl nm mb nn no np iz bi translated">参考</h2><ul class=""><li id="95c5" class="nq nr it kx b ky ns lb nt lz nu ma nv mb nw lq nx ny nz oa bi translated">莫凯迪，r .，赫兹，a .和伯尔曼诺，A.H .，2021。ClipCap:图像字幕的剪辑前缀。<a class="ae lr" href="https://arxiv.org/abs/2111.09734" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2111.09734</a></li><li id="4213" class="nq nr it kx b ky ob lb oc lz od ma oe mb of lq nx ny nz oa bi translated">代号:【https://github.com/rmokady/CLIP_prefix_caption T2】</li><li id="3d74" class="nq nr it kx b ky ob lb oc lz od ma oe mb of lq nx ny nz oa bi translated">Colab演示:<a class="ae lr" href="https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1 tuoac 5 f 4 sc 7 qid 56 z 0ap-str 3 rw dk 0 zv？usp =分享</a></li></ul></div></div>    
</body>
</html>