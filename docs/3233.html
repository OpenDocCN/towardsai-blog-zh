<html>
<head>
<title>Gradient Descent Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降优化</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/gradient-descent-optimization-15ee38e75e12?source=collection_archive---------4-----------------------#2022-10-18">https://pub.towardsai.net/gradient-descent-optimization-15ee38e75e12?source=collection_archive---------4-----------------------#2022-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1c3c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">梯度下降优化算法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6c74825315a504b65d3211f18518b7c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Rjw3gAM-ap_FCL_j.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:图片来自<a class="ae kv" href="https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/ways-to-represent-multivariable-functions/a/multidimensional-graphs" rel="noopener ugc nofollow" target="_blank">可汗学院</a></figcaption></figure><p id="650e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大多数深度学习算法通过优化某种目标(或损失)函数来进行训练。我们通常试图最大化或最小化这些功能。基于梯度的优化是用于训练神经网络的最广泛使用的技术。</p><p id="3397" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将看到众所周知的梯度下降算法。我们将讨论它是什么，为什么它工作，最后，为张量流实现一个梯度下降优化器。</p><p id="338c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章是写给那些刚刚听说梯度下降的人，或者那些认为梯度下降是一个黑盒，对发生的事情有一个肤浅的理解，但想更深入地了解事情为什么是这样的人。</p><p id="df7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注</strong>:本文灵感来源于《<a class="ae kv" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">》深度学习教材</strong> </a> <strong class="ky ir"> : </strong> <a class="ae kv" href="https://www.deeplearningbook.org/contents/numerical.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">第四章:数值计算</strong> </a>》和《<a class="ae kv" href="https://cs231n.github.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">斯坦福CS class CS231n:用于视觉识别的卷积神经网络注</strong> </a>。”</p><h1 id="e97d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">内容</h1><p id="6982" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这是一篇相当长的文章，所有这些部分可能不会立刻引起读者的兴趣或关注。请随意使用以下列表导航并跳到与您相关的部分:</p><ol class=""><li id="9e01" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated"><a class="ae kv" href="#4219" rel="noopener ugc nofollow"> <strong class="ky ir">损失函数</strong> </a></li><li id="74b0" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><a class="ae kv" href="#3832" rel="noopener ugc nofollow"> <strong class="ky ir">优化</strong> </a> <strong class="ky ir"> <br/> </strong> 1。<strong class="ky ir"> </strong> <a class="ae kv" href="#4837" rel="noopener ugc nofollow"> <strong class="ky ir">随机搜索</strong> </a> <strong class="ky ir"> <br/> </strong> 2。<strong class="ky ir"> </strong> <a class="ae kv" href="#4ac1" rel="noopener ugc nofollow"> <strong class="ky ir">随机局部搜索</strong> </a> <strong class="ky ir"> <br/> </strong> 3。<strong class="ky ir"> </strong> <a class="ae kv" href="#08d7" rel="noopener ugc nofollow"> <strong class="ky ir">基于梯度的优化</strong> </a></li><li id="0c50" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><a class="ae kv" href="#ec42" rel="noopener ugc nofollow"> <strong class="ky ir">渐变下降</strong> </a> <strong class="ky ir"> <br/> </strong> 1。<strong class="ky ir"> </strong> <a class="ae kv" href="#e131" rel="noopener ugc nofollow"> <strong class="ky ir">最小值</strong> </a> <strong class="ky ir"> <br/> </strong> 2。<strong class="ky ir"> </strong> <a class="ae kv" href="#ba09" rel="noopener ugc nofollow"> <strong class="ky ir">为什么不能直接把导数等同于0，求解x？</strong> </a> <strong class="ky ir"> <br/> </strong> 3。<strong class="ky ir"> </strong> <a class="ae kv" href="#d322" rel="noopener ugc nofollow"> <strong class="ky ir">参数更新公式</strong></a><strong class="ky ir"><br/></strong>a .<strong class="ky ir"/><a class="ae kv" href="#e7ba" rel="noopener ugc nofollow"><strong class="ky ir">为什么负梯度是最陡下降的方向？</strong></a><strong class="ky ir"><br/></strong>b .<strong class="ky ir"/><a class="ae kv" href="#7928" rel="noopener ugc nofollow"><strong class="ky ir">为什么我们说‘往渐变的反方向走’</strong>T48】</a></li><li id="6d6c" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><a class="ae kv" href="#1419" rel="noopener ugc nofollow"> <strong class="ky ir">雅各派和黑森派</strong> </a> <strong class="ky ir"> <br/> </strong> 1。<strong class="ky ir"> </strong> <a class="ae kv" href="#ce6c" rel="noopener ugc nofollow"> <strong class="ky ir">二阶导数测试和二阶优化</strong> </a> <strong class="ky ir"> <br/> </strong> 2。<strong class="ky ir"> </strong> <a class="ae kv" href="#4d8c" rel="noopener ugc nofollow"> <strong class="ky ir">为什么不对神经网络使用二阶优化算法？</strong> </a></li><li id="3177" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><a class="ae kv" href="#ef32" rel="noopener ugc nofollow"> <strong class="ky ir">计算渐变</strong> </a> <strong class="ky ir"> <br/> </strong> 1。<strong class="ky ir"> </strong> <a class="ae kv" href="#0dc6" rel="noopener ugc nofollow"> <strong class="ky ir">利用有限差分</strong> </a> <strong class="ky ir"> <br/> </strong> 2。<strong class="ky ir"> </strong> <a class="ae kv" href="#a6e1" rel="noopener ugc nofollow"> <strong class="ky ir">利用微积分</strong> </a></li><li id="5ae5" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><a class="ae kv" href="#51a7" rel="noopener ugc nofollow"> <strong class="ky ir">实现</strong> </a></li><li id="9e97" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><a class="ae kv" href="#7fcb" rel="noopener ugc nofollow"> <strong class="ky ir">结论</strong> </a></li><li id="1c27" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><a class="ae kv" href="#0913" rel="noopener ugc nofollow">T102】参考文献T104】</a></li></ol><h1 id="4219" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">损失函数</h1><p id="e8cc" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">损失函数也称为目标函数、误差函数、成本函数或标准，可以是任何(可微分的)函数，有助于根据基本事实评估算法的输出。它可以是从连续值的<a class="ae kv" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差</a>到概率分布的<a class="ae kv" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">交叉熵</a>损失的任何值。虽然损失函数非常有趣，但今天，我们只关心如何最小化它们。</p><p id="317c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了理解我们到底想要什么，让我们想象一个损失函数。实际上，许多参数会改变损失，因此很难想象。为了简单起见，我们只考虑2。</p><p id="86aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们定义一个损耗:<code class="fe nd ne nf ng b">y = L(x, z)</code>其中<code class="fe nd ne nf ng b">x, y, z</code>分别是沿X、Y、Z轴的笛卡尔坐标。<br/>让我们取随机值<code class="fe nd ne nf ng b">wx</code>和<code class="fe nd ne nf ng b">wz</code>，我们得到空间中的点:<code class="fe nd ne nf ng b">(wx, yw, wz)</code>。<br/>现在，我们可以随机改变一些<code class="fe nd ne nf ng b">m</code>作为<code class="fe nd ne nf ng b">ywₘ = L(wx + m, z)</code>，这将给出沿xy平面的损耗值<code class="fe nd ne nf ng b">yw</code>的线图，或者我们可以改变一些<code class="fe nd ne nf ng b">m</code>和<code class="fe nd ne nf ng b">n</code>作为<code class="fe nd ne nf ng b">ywₘₙ = L(wx + m, z + n)</code>，这将给出三维图。下面是多阶层SVM损失的情况:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/ac2f3ab67729dccbbca65bdb5babf781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Na8icVsfTWL5lKnElDnSUQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">多类SVM的损失函数景观。<strong class="bd lu">左:</strong>仅改变<code class="fe nd ne nf ng b">m</code>的一维损失。<strong class="bd lu">右:</strong>通过改变<code class="fe nd ne nf ng b">m</code>和<code class="fe nd ne nf ng b">n</code>得到二维损失切片。<strong class="bd lu">蓝色</strong> =低损耗，<strong class="bd lu">红色</strong> =高损耗。来源:图片由<a class="ae kv" href="https://cs231n.github.io/optimization-1/#vis" rel="noopener ugc nofollow" target="_blank">斯坦福CS CS231n备注</a></figcaption></figure><p id="e6fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以我们最终想要的是找到一些<code class="fe nd ne nf ng b">wx*</code>和<code class="fe nd ne nf ng b">wz*</code>使得<code class="fe nd ne nf ng b">yw</code>(即损失)最小(最暗的蓝点)。从任意值开始，逐步降低到这些“最佳”值，这被称为<strong class="ky ir">优化</strong>。</p><h1 id="3832" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">最佳化</h1><p id="4713" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在上一节中，我们看到了在这种情况下优化到底是什么。现在，让我们看看我们可以如何着手，并了解什么是基于梯度的优化，以及为什么它被用来训练神经网络。</p><p id="e148" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们想一些简单的方法来做到这一点:</p><h2 id="4837" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">随机搜索</h2><p id="78e8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们随机选取一些值，并记录能产生最佳损失的值:</p><pre class="kg kh ki kj gt nu ng nv nw aw nx bi"><span id="8136" class="ni lt iq ng b gy ny nz l oa ob">x*, z* = 0, 0<br/>bestloss = inf<br/>for a few iterations<br/>    x, z = random(), random()<br/>    y = L(x, z)<br/>    if y &lt; bestloss then<br/>        bestloss, x*, z* = y, x, z</span></pre><p id="fccb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">坐标空间中有无穷多个点，以最优值结束的几率非常低。所以这是一个非常糟糕的方法。让我们看看我们是否能做得更好。</p><h2 id="4ac1" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">随机局部搜索</h2><p id="90e4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们可以做一些类似于可视化损失函数时所做的事情。我们可以从随机值开始，生成一些小随机<code class="fe nd ne nf ng b">Δx, Δz</code>，分别加到<code class="fe nd ne nf ng b">x</code>和<code class="fe nd ne nf ng b">z</code>上。如果新值改善了损失，我们保留它们:</p><pre class="kg kh ki kj gt nu ng nv nw aw nx bi"><span id="fcb8" class="ni lt iq ng b gy ny nz l oa ob">x*, z* = random(), random()<br/>bestloss = inf<br/>for a few iterations<br/>    <!-- -->Δ<!-- -->x, <!-- -->Δ<!-- -->z = random(), random()<br/>    x, z = x* + <!-- -->Δ<!-- -->x, z* + Δz<br/>    y = L(x, z)<br/>    if y &lt; bestloss then<br/>        bestloss, x*, z* = y, x, z</span></pre><p id="f622" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://cs231n.github.io/optimization-1/#optimization" rel="noopener ugc nofollow" target="_blank">类比:</a>把这想象成一个被蒙住眼睛的徒步旅行者试图爬下一座小山，在前进之前探测每一步。</p><p id="d207" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然看起来不错，但这种方法仍然非常随意，缺乏结构。此外，更大的问题是它非常昂贵。在最坏的情况下，人们可能不得不探测函数上的每一点，这甚至是不可能的。</p><h2 id="08d7" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">基于梯度的优化</h2><p id="986d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们回到高中，回忆一下初等微积分。对于函数<code class="fe nd ne nf ng b">y = <em class="oc">f</em>(x)</code>，导数<code class="fe nd ne nf ng b"><em class="oc">f'</em>(x)</code>或<code class="fe nd ne nf ng b"><em class="oc">d</em>y/<em class="oc">d</em>x</code>定义为<code class="fe nd ne nf ng b">y</code>相对于<code class="fe nd ne nf ng b">x</code>微小变化的变化率。也是<code class="fe nd ne nf ng b"><em class="oc">f</em>(x)</code>在<code class="fe nd ne nf ng b">x</code>点的斜率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/344b2cec0e26dfe7fdbe755f9563f2f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MSrWBR5cbnBYvy5aAChWYA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:图片由作者提供</figcaption></figure><p id="e998" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个表达式的解释和说明超出了今天讨论的范围，但是如果你仔细观察，你会发现它是不言自明的。也可以看作是标准的<a class="ae kv" href="https://www.google.com/search?q=slope+formula&amp;rlz=1C5CHFA_enIN959IN959&amp;oq=slope+formula&amp;aqs=chrome..69i57j0i20i263i512j0i512l8.2000j0j7&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank">斜率公式</a>。</p><p id="f43c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以进一步求解<code class="fe nd ne nf ng b"><em class="oc">f</em>(x + h)</code>，并将其近似为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/29eb5ebe0390df89a6e8bb6f25978a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ahXeapedxUfgcBGKg2sxw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:图片由作者提供</figcaption></figure><p id="3f2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">许多教科书和论文都假定了上述表达式。这就是它的来源。</p><p id="026f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面的表达式，我们可以求解并得出结论:</p><pre class="kg kh ki kj gt nu ng nv nw aw nx bi"><span id="7d97" class="ni lt iq ng b gy ny nz l oa ob">for a small enough <!-- -->ϵ<!-- -->, <strong class="ng ir"><em class="oc">f</em>(x — ϵ sign(<em class="oc">f</em>’(x)))</strong> is smaller than <em class="oc">f</em>(x)</span></pre><p id="4526" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以看起来像是通过使用上面的导数，我们可以通过更新<code class="fe nd ne nf ng b">x</code>来取一个‘足够小的’<code class="fe nd ne nf ng b">ϵ</code>并逐步减少<code class="fe nd ne nf ng b"><em class="oc">f</em>(x)</code>。这个技术叫做<strong class="ky ir">梯度下降</strong>。</p><p id="ae03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这个例子是一个单变量函数。通常，我们会处理多元函数(有时在神经网络的情况下会有数百万个变量)。每个变量代表在给定的'<strong class="ky ir">方向</strong>'上函数的大小所以对于多元函数，我们取<a class="ae kv" href="https://en.wikipedia.org/wiki/Partial_derivative" rel="noopener ugc nofollow" target="_blank">偏导数</a>，即每个方向(或变量)的单独导数w.r.t。</p><blockquote class="of og oh"><p id="d11b" class="kw kx oc ky b kz la jr lb lc ld ju le oi lg lh li oj lk ll lm ok lo lp lq lr ij bi translated">梯度，∇ f就是一个函数所有偏导数的向量</p></blockquote><p id="fba1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在下一节更详细地讨论梯度下降。</p><h1 id="ec42" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">梯度下降</h1><p id="d6ba" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">到目前为止，我们已经看到了如何最小化一个函数。现在让我们考虑在哪里停止，也就是说，我们如何知道我们是否已经达到了函数的“最小”值？</p><h2 id="e131" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">最小值</h2><p id="8945" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><code class="fe nd ne nf ng b"><em class="oc">f</em>’(x) = 0</code>定义斜率为0的点。在二维空间中，这些点是函数的斜率平行于x轴的点，因此为0。类似的想法在更高的维度中也有。这些点被称为<strong class="ky ir">临界点或静止点</strong>。因为导数是0，所以没有关于向哪个方向移动的信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/52cfb135a9f0ff1e2db1bbee5af1386b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uB_QAve19uErH4azRX3O1A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">临界点的三种类型。来源:图片来自<a class="ae kv" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"> deeplearningbook </a></figcaption></figure><ol class=""><li id="9240" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">给出<code class="fe nd ne nf ng b"><em class="oc">f</em>(x)</code>的绝对最小值的点是<strong class="ky ir">全局最小值</strong>。而<strong class="ky ir">局部最小值</strong>是<code class="fe nd ne nf ng b"><em class="oc">f</em>(x)</code>低于所有邻近点的点。<br/> <strong class="ky ir">由于这里的导数为0，我们不能向</strong> <code class="fe nd ne nf ng b"><strong class="ky ir"><em class="oc">f</em>(x)</strong></code> <strong class="ky ir">的更低值移动。</strong></li><li id="9c58" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">反之亦然，对于<strong class="ky ir">全局和局部最大值</strong>。</li><li id="a845" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">最后，<strong class="ky ir">鞍点</strong>既不是最小值也不是最大值。</li></ol><p id="bf54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">值得注意的是，我们之前看到的多类SVM损失函数属于一类叫做<a class="ae kv" href="https://en.wikipedia.org/wiki/Convex_function" rel="noopener ugc nofollow" target="_blank">凸函数</a>的函数。有一个独立的数学优化子领域致力于凸优化。</p><p id="e569" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相反，神经网络是高度非线性的，并且损失是非凸的。这意味着我们从凸函数中得到的概念可能不容易应用于神经网络。</p><p id="0f8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，一个严格凸函数只有一个最小值。然而，在深度学习的背景下，我们正在处理非常复杂的损失函数，这些函数可能具有多个局部和全局最小值，并且最有可能是崎岖不平的地形——使得梯度下降变得不平凡。</p><p id="5998" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，很难，或者在某些情况下，甚至不可能达到全球最低水平。因此，我们满足于一个非常低的损失值，甚至可能不是最低值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/d63dc77fcf58170c470542ba0665c30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-HqauR6crHGPCygc6iyELw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">多个最小值的比较。来源:图片来自<a class="ae kv" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"> deeplearningbook </a></figcaption></figure><p id="7b1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，当<code class="fe nd ne nf ng b"><em class="oc">f'</em>(x) = 0</code>时，如果我们处于函数的极限，可能会有一个非常有效的问题:</p><h2 id="ba09" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">为什么不能直接把导数等于0，求解x呢？</h2><p id="4b68" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这不就是我们在高中被教导的吗？那么这有什么问题呢？迭代梯度下降真的有必要吗？</p><p id="4e36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哦，不！我们确实可以没有它而生活。</p><p id="738d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但让我们考虑一下这个问题——在经典线性回归的情况下，将成本函数的导数等于零并求解参数涉及对矩阵求逆(<a class="ae kv" href="http://faculty.cas.usf.edu/mbrannick/regression/regma.htm" rel="noopener ugc nofollow" target="_blank">这里是为什么</a>)。即使最有效的技术也有复杂度<code class="fe nd ne nf ng b">O(n³)</code>，其中<code class="fe nd ne nf ng b">n</code>是参数的数量。</p><p id="a4f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在有了神经网络，我们正在讨论数百万个参数。此外，它是一个比线性回归复杂得多的函数。所以解决起来会更加困难。此外，这些损失在任何地方都可能是可微的，也可能不是可微的。</p><p id="9ace" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像梯度下降这样的迭代方法为这些问题提供了一个更加实用和合理有效的解决方案。</p><p id="9e81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，如果更容易或者更快的话，我们总是可以把导数等于0。</p><h2 id="d322" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">参数更新公式</h2><p id="da67" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">到目前为止，我们已经讨论了我们到底想要什么，然后我们讨论了各种方法，并专注于一个，然后我们看到了停止条件看起来像什么。现在让我们看看如何在每一步实际求解<code class="fe nd ne nf ng b">x</code>。</p><p id="b7c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们先退一步，回忆一下我们对<code class="fe nd ne nf ng b"><em class="oc">f</em>(x — ϵ sign(<em class="oc">f</em>’(x)))</code>的推断。如果<code class="fe nd ne nf ng b">ϵ</code>足够小，这个值将小于<code class="fe nd ne nf ng b"><em class="oc">f</em>(x)</code>。让我们花点时间想想<code class="fe nd ne nf ng b">x — ϵ sign(<em class="oc">f</em>’(x))</code>项——如果<code class="fe nd ne nf ng b"><em class="oc">f</em>’(x)</code>为正，那么我们新的<code class="fe nd ne nf ng b">x</code>将是<code class="fe nd ne nf ng b">x — ϵ</code>，如果<code class="fe nd ne nf ng b"><em class="oc">f</em>’(x)</code>为负，那么<code class="fe nd ne nf ng b">x</code>将是<code class="fe nd ne nf ng b">x + ϵ</code>。所以我们基本上只是用导数来表示它的符号，然后把它反过来。</p><blockquote class="of og oh"><p id="7928" class="kw kx oc ky b kz la jr lb lc ld ju le oi lg lh li oj lk ll lm ok lo lp lq lr ij bi translated">在大多数关于梯度下降的教科书/论文/文章中，当他们说‘向梯度的相反方向走’时，这就是他们的意思。</p></blockquote><p id="7cd5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这只是一个变量。实际上，我们正在处理数以百万计的问题。那么这适用于多变量的情况吗？如果是，如何实现？让我们找出答案。</p><p id="e26a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注意</strong>从现在开始，我们将x表示为多个参数(变量)的集合。所以，</p><pre class="kg kh ki kj gt nu ng nv nw aw nx bi"><span id="5694" class="ni lt iq ng b gy ny nz l oa ob">x = (x₁, x₂, x₃, …, xₙ) and<br/>f(x) = f(x₁, x₂, x₃, …, xₙ)</span></pre><p id="e7ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们在处理向量，我们将需要<a class="ae kv" href="https://mathworld.wolfram.com/DirectionalDerivative.html" rel="noopener ugc nofollow" target="_blank">方向导数</a>。</p><p id="15b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">梯度向量<code class="fe nd ne nf ng b">∇ₓ<em class="oc">f</em>(x)</code>的分量代表函数<code class="fe nd ne nf ng b"><em class="oc">f</em>(x)</code>相对于其每个变量<code class="fe nd ne nf ng b">xᵢ</code>的变化率。然而，我们希望知道<code class="fe nd ne nf ng b"><em class="oc">f</em>(x)</code>随着其变量从一个给定点沿任何方向变化，它是如何变化的。</p><p id="9fc9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以函数<code class="fe nd ne nf ng b"><em class="oc">f</em>(x)</code>在方向<code class="fe nd ne nf ng b">υ</code>(单位向量)上的方向导数，在点<code class="fe nd ne nf ng b">x</code>处，由<code class="fe nd ne nf ng b"><em class="oc">f</em>(x + αυ)</code> w.r.t <code class="fe nd ne nf ng b">α</code>的偏导数给出。，即<code class="fe nd ne nf ng b">α = 0</code>时的<code class="fe nd ne nf ng b">∂f(x + αυ)/∂α</code>。基本上，<code class="fe nd ne nf ng b">α</code>变化不大的<code class="fe nd ne nf ng b">f(x + αυ)</code>的变化率，因为<code class="fe nd ne nf ng b">υ</code>只是一个方向向量，不能改变。让我们来解决这个问题:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/34f85728960a1a0a205bcccf0b8ecf93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5UGfeAcIvTYdHaWBws0pQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:图片由作者提供</figcaption></figure><p id="bed5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面我们可以说，对于任何给定的方向<code class="fe nd ne nf ng b">υ</code>，方向导数与函数<code class="fe nd ne nf ng b"><em class="oc">f</em>(x)</code>在<code class="fe nd ne nf ng b">x</code>点的梯度与方向本身之间的夹角余弦成正比。</p><p id="2dde" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在在梯度下降的背景下，我们可以在给定点<code class="fe nd ne nf ng b">x</code>取一个任意可能方向的单位向量，并朝那个方向移动。然而，最好是朝着<code class="fe nd ne nf ng b"><em class="oc">f</em></code>减少最快的方向移动。为此，方向导数必须是最大的，但方向相反，即<code class="fe nd ne nf ng b">x</code>变化小的函数的变化率在<code class="fe nd ne nf ng b">-υ</code>方向上最大。当<code class="fe nd ne nf ng b">cosθ</code>值为<code class="fe nd ne nf ng b">-1</code>时会发生这种情况。所以我们必须为<code class="fe nd ne nf ng b">θ = π</code>选择一个<code class="fe nd ne nf ng b">υ</code>。这就是矢量<code class="fe nd ne nf ng b">-∇ₓ<em class="oc">f</em>(x)</code>。</p><p id="9231" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">利用这一点和前面的想法，我们可以得出以下更新:</p><pre class="kg kh ki kj gt nu ng nv nw aw nx bi"><span id="4334" class="ni lt iq ng b gy ny nz l oa ob">x<em class="oc">'</em> = x - ϵ <!-- -->∇ₓ<em class="oc">f</em>(x)     ... where <!-- -->ϵ is the learning rate</span></pre><p id="e197" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常，我们为<code class="fe nd ne nf ng b">ϵ</code>选择一个很小的值。有时我们可以这样求解<code class="fe nd ne nf ng b">ϵ</code>使得<code class="fe nd ne nf ng b">υᵀ∇ₓ<em class="oc">f</em>(x') = 0</code>。<a class="ae kv" href="https://math.stackexchange.com/a/2846728/872052" rel="noopener ugc nofollow" target="_blank">把这想象成走下一座小山，直到你到达一个点，在那里你开始再次向上走</a>(因为<code class="fe nd ne nf ng b">υᵀ∇ₓ<em class="oc">f</em>(x') = 0</code>、<code class="fe nd ne nf ng b">cosθ = 0</code>，因此<code class="fe nd ne nf ng b">υ</code>和<code class="fe nd ne nf ng b">∇ₓ<em class="oc">f</em>(x')</code>是正交的)。另一个策略是评估<code class="fe nd ne nf ng b">ϵ</code>的几个任意值的<code class="fe nd ne nf ng b">x’</code>，并选择一个给出<code class="fe nd ne nf ng b"><em class="oc">f</em>(x')</code>最小值的值。这个策略叫做<a class="ae kv" href="https://en.wikipedia.org/wiki/Line_search" rel="noopener ugc nofollow" target="_blank">线搜索</a>。</p><p id="4b83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这被称为最陡下降法或<strong class="ky ir">梯度下降法</strong>。</p><h1 id="1419" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">雅各宾派和黑森派</h1><p id="120b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">到目前为止，我们已经将梯度<code class="fe nd ne nf ng b">∇ₓ<em class="oc">f</em>(x)</code>用于多元函数，这些函数将向量作为输入(<code class="fe nd ne nf ng b">x = (x₁, x₂, …, xₙ)</code>)，输出标量(<code class="fe nd ne nf ng b">y = <em class="oc">f</em>(x)</code>)。然而，在处理输入和输出向量的函数时，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/edcc064b17bf8bff73836ba14e7fd172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*up218KkpIkmHbubsdMn4aw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:图片由作者提供</figcaption></figure><p id="1f58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用雅可比矩阵。它是所有偏导数的矩阵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/74186325caf3e404c4590eb16e52e63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7nYn7rt61tfCWqiaEXQfNw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:图片由作者提供</figcaption></figure><p id="9458" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这在应用<a class="ae kv" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction" rel="noopener ugc nofollow" target="_blank">链式法则</a>时特别有用。</p><p id="3afb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们对一个导数求导，我们得到一个<strong class="ky ir">二阶导数</strong>或者一个<strong class="ky ir">二阶导数</strong> ( <code class="fe nd ne nf ng b"><em class="oc">f</em>”(x)</code>)。这个量将告诉我们，当我们改变输入时，一阶导数如何变化。我们可以将二阶<br/>导数视为测量曲率。</p><p id="7a4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">二阶导数的使用超出了本文的范围，所以我们将简要地看看它是什么:</p><h2 id="ce6c" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">二阶导数检验和二阶优化</h2><p id="f9d1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果函数是一条线，仅仅梯度就足以告诉我们它的曲率(即，如果<code class="fe nd ne nf ng b">slope &lt; 0</code>那么函数下降；如果<code class="fe nd ne nf ng b">slope &gt; 0</code>那么它上升；而<code class="fe nd ne nf ng b">slope = 0</code>表示一条平线，如前所述)。否则，我们可以使用二阶导数。比如对于一个二次函数，导数是一条线，那么我们可以取导数的导数，用上面的类比。</p><p id="291c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这对于评估临界点<code class="fe nd ne nf ng b"><em class="oc">f</em>’(x) = 0</code>特别有用。这里，如果二阶导数<code class="fe nd ne nf ng b"><em class="oc">f</em>”(x) &gt; 0</code>，那么当我们向右移动时，<code class="fe nd ne nf ng b"><em class="oc">f</em>’(x)</code>随着我们向左移动而增大和减小。这意味着我们处于局部最小值。同样，当<code class="fe nd ne nf ng b"><em class="oc">f</em>”(x) &lt; 0</code>时，我们处于局部最大值。然而很遗憾的是，在<code class="fe nd ne nf ng b"><em class="oc">f</em>”(x) = 0</code>的时候，是鞍点还是平面没有定论。但是在多维度上，实际上在某些情况下是可以找到鞍点的正证据的。这被称为<strong class="ky ir">二阶导数测试</strong>。</p><p id="5f17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在多维或多元函数的情况下，我们有<a class="ae kv" href="https://en.wikipedia.org/wiki/Hessian_matrix" rel="noopener ugc nofollow" target="_blank">海森矩阵</a>。这些就像雅可比，但是有二阶导数。同样，黑森是梯度的雅可比。</p><p id="5fd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果你注意到，有了关于函数曲率的信息，我们实际上可以为参数更新确定一个正确的步长，它不会超过函数值。我们可以通过在优化中包含Hessian矩阵来做到这一点。一种最简单的方法叫做<a class="ae kv" href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" rel="noopener ugc nofollow" target="_blank">牛顿法</a>:</p><pre class="kg kh ki kj gt nu ng nv nw aw nx bi"><span id="0680" class="ni lt iq ng b gy ny nz l oa ob">x' = x - H⁻¹∇f</span></pre><p id="08bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这属于使用海森矩阵的整个算法家族——<strong class="ky ir">二阶优化</strong>算法。另一方面，使用梯度的算法被称为<strong class="ky ir">一阶优化</strong>算法。</p><h2 id="4d8c" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">为什么不对神经网络使用<strong class="ak">二阶优化</strong>算法？</h2><p id="cebe" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">它们在速度和内存方面非常昂贵。为了构造hessian矩阵，我们需要所有变量和梯度组合的导数，也就是O(n)。</p><p id="a457" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，实现二阶导数非常困难。<a class="ae kv" href="https://stats.stackexchange.com/questions/394083/why-second-order-sgd-convergence-methods-are-unpopular-for-deep-learning" rel="noopener ugc nofollow" target="_blank">这里列出了二阶优化算法的其他问题</a>，这使得它们不适用于神经网络。</p><h1 id="ef32" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">计算梯度</h1><p id="deb3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们可以用两种方法做到这一点:有限差分公式和使用微积分。</p><h2 id="0dc6" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">使用有限差分</h2><p id="7370" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">对于所有的偏导数，我们可以直接应用前面看到的公式。但是，建议使用修改后的版本:</p><pre class="kg kh ki kj gt nu ng nv nw aw nx bi"><span id="bb26" class="ni lt iq ng b gy ny nz l oa ob"><em class="oc">f'</em>(x) = (<em class="oc">f</em>(x + h) − <em class="oc">f</em>(x − h)) / 2h</span></pre><p id="6275" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这部电影探索了问题的两面。要计算梯度:</p><pre class="kg kh ki kj gt nu ng nv nw aw nx bi"><span id="29bb" class="ni lt iq ng b gy ny nz l oa ob">enumerate all parameters with i<br/>    backup = x[i]<br/>    <br/>    x[i] = backup - h<br/>    fxminush = f(x)<br/>    <br/>    x[i] = backup + h<br/>    fxplush = f(x)<br/>    <br/>    x[i] = backup<br/>    <br/>    gradient[i] = (fxplush - fxminush) / 2h</span></pre><p id="38db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法的明显缺点是它是近似的。此外，上述循环将在每次更新时运行。所以基本上，如果我们有一百万个参数和一千个步骤，这将运行<code class="fe nd ne nf ng b">1000 * 1000000</code>次迭代。这非常昂贵，仅用于<a class="ae kv" href="https://cs231n.github.io/neural-networks-3/#gradcheck" rel="noopener ugc nofollow" target="_blank">梯度检查</a>健全性测试。</p><h2 id="a6e1" class="ni lt iq bd lu nj nk dn ly nl nm dp mc lf nn no me lj np nq mg ln nr ns mi nt bi translated">使用微积分</h2><p id="97f1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们可以用函数导数的直接公式来代替近似公式。之后，我们可以使用这个导数获得梯度，它可以直接用于更新步骤。</p><blockquote class="of og oh"><p id="3e1e" class="kw kx oc ky b kz la jr lb lc ld ju le oi lg lh li oj lk ll lm ok lo lp lq lr ij bi translated">然而，这并不是现代机器学习库的确切工作方式——相反，它们使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="noopener ugc nofollow" target="_blank">自动微分</a>。其思想是，我们所做的所有运算最终都是初等算术(加、减、乘、除)运算和初等函数(sin、cos、log等)的序列或组合。).因此人们可以记录执行的顺序并应用<a class="ae kv" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction" rel="noopener ugc nofollow" target="_blank">链规则</a>。</p></blockquote><h1 id="51a7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">履行</h1><p id="ec04" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">最后，让我们尝试为TensorFlow实现一个优化器。关于在TensorFlow中实现自定义优化器的更多信息，<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#creating_a_custom_optimizer_2" rel="noopener ugc nofollow" target="_blank">参见此</a>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:作者代码</figcaption></figure><p id="8d0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了测试这一点，我首先针对<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD" rel="noopener ugc nofollow" target="_blank">内置的SGD优化器</a>训练了一个用于<a class="ae kv" href="https://www.tensorflow.org/tutorials/keras/classification" rel="noopener ugc nofollow" target="_blank">简单MNIST分类</a>的模型，然后是这个模型，并比较了结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/70721e1ff8ef88f9f7e5062b117bf0bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xMnKswpgFGpXWQpEnkH5Rw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">训练MNIST分类:<strong class="bd lu">左:</strong>内置SGD，<strong class="bd lu">右:</strong>我们的优化器。来源:图片由作者提供</figcaption></figure><p id="7383" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到它运行良好。</p><h1 id="7fcb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="66a2" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这篇相当详尽的文章中，</p><ol class=""><li id="e8d3" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">我们讨论了优化的必要性。然后，我们看到了一些优化策略，并介绍了基于梯度的优化算法。</li><li id="263d" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">我们讲了梯度下降算法，回答了几个为什么，什么是w.r.t梯度下降。我们还推导了参数更新的表达式。</li><li id="0430" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">然后，我们简要地谈到了雅可比、黑森和二阶优化算法的思想。</li><li id="f521" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">我们看到了如何计算任意给定函数的梯度，并比较了两种方法。</li><li id="a943" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">最后，我们为张量流实现了一个简单的梯度下降优化器。</li></ol><h1 id="0913" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><div class="ot ou gp gr ov ow"><a href="https://www.deeplearningbook.org/" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">深度学习</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">深度学习教材是一个资源，旨在帮助学生和从业人员进入机器领域…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">www.deeplearningbook.org</p></div></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://cs231n.github.io/optimization-1/" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">用于视觉识别的CS231n卷积神经网络</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">目录:在上一节中，我们介绍了图像分类环境中的两个关键组件…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">cs231n.github.io</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk kp ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://stats.stackexchange.com/questions/212619/why-is-gradient-descent-required" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">为什么需要梯度下降？</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">感谢您为交叉验证提供答案！请务必回答问题。提供详细信息并分享…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">stats.stackexchange.com</p></div></div><div class="pf l"><div class="pl l ph pi pj pf pk kp ow"/></div></div></a></div><p id="367f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为什么梯度是最陡上升的方向:<a class="ae kv" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent" rel="noopener ugc nofollow" target="_blank">https://www . khanacademy . org/math/multivariable-calculus/multivariable-derivatives/gradient-and-direction-derivatives/v/why-the-gradient-is-direction-ascend的方向</a></p><div class="ot ou gp gr ov ow"><a href="https://stats.stackexchange.com/questions/394083/why-second-order-sgd-convergence-methods-are-unpopular-for-deep-learning" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">为什么二阶SGD收敛方法在深度学习中不受欢迎？</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">似乎，特别是对于深度学习，有一些非常简单的方法可以优化SGD…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">stats.stackexchange.com</p></div></div><div class="pf l"><div class="pm l ph pi pj pf pk kp ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://www.tensorflow.org/tutorials/keras/classification" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">基本分类:对服装图像进行分类| TensorFlow核心</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">本指南训练一个神经网络模型来分类服装图像，如运动鞋和衬衫。没关系，如果你…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">www.tensorflow.org</p></div></div><div class="pf l"><div class="pn l ph pi pj pf pk kp ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#creating_a_custom_optimizer_2" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ir gy z fp pb fr fs pc fu fw ip bi translated">TF . keras . optimizer . optimizer | tensor flow v 2 . 10 . 0</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">Keras优化器的基类。</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">www.tensorflow.org</p></div></div></div></a></div></div></div>    
</body>
</html>