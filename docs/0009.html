<html>
<head>
<title>Machine Learning: Dimensionality Reduction via Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:通过主成分分析降维</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/machine-learning-dimensionality-reduction-via-principal-component-analysis-1bdc77462831?source=collection_archive---------1-----------------------#2018-10-25">https://pub.towardsai.net/machine-learning-dimensionality-reduction-via-principal-component-analysis-1bdc77462831?source=collection_archive---------1-----------------------#2018-10-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="474f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">面向AI的降维|<a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank"/></h2><div class=""/><p id="343d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在机器学习中，包含特征(预测器)和离散类别标签的数据集(用于逻辑回归等分类问题)；或特征和连续结果(对于线性回归问题),用于构建预测模型，该模型可以对看不见的数据进行预测。模型的预测能力在很大程度上取决于训练数据集的质量和大小。</p><p id="3db5" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">一般来说，数据集越大越好，但是，在数据集的大小和训练所需的计算时间之间总会有一个折衷。事实证明，在一些非常大的数据集中，特征中可能存在大量冗余，或者数据集中可能存在大量不重要的特征，因此降维技术可以用于仅选择训练所需的有限数量的相关特征。</p><p id="726a" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">主成分分析(PCA)是一种用于特征提取的统计方法。PCA用于高维和相关数据。PCA的基本思想是将原始的特征空间转换到主成分空间，如下所示:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/c99f379330c8ab818b6b67991c9f25c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*xHTGEOLxzAKpt7ARdfa5qA.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><strong class="bd lj">图1: PCA算法从旧的特征空间转换到新的特征空间，以去除特征相关性。图片改编自:《塞巴斯蒂安·拉什卡的Python机器学习》</strong></figcaption></figure><p id="d45f" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">PCA变换实现了以下目标:</p><p id="70f4" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd"> a) </strong>通过只关注占数据集中方差大部分的成分，减少最终模型中使用的特征数量。</p><p id="bc5d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd"> b) </strong>去除特征之间的相关性。</p><h2 id="b53f" class="lk ll it bd lj lm ln dn lo lp lq dp lr kk ls lt lu ko lv lw lx ks ly lz ma iz bi translated">PCA是如何工作的？</h2><p id="2c47" class="pw-post-body-paragraph jz ka it kb b kc mb ke kf kg mc ki kj kk md km kn ko me kq kr ks mf ku kv kw im bi translated">为了说明PCA是如何工作的，我们通过检查虹膜数据集来展示一个例子。</p><p id="d791" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">代码可以在<a class="ae mg" href="https://github.com/bot13956/principal_component_analysis_iris_dataset/blob/master/PCA_irisdataset.R" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><p id="cca1" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">让我们看看协方差矩阵:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/070105bca03e442ce1d6250911023ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*t34aaaogKX22guHgoVUOfg.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><strong class="bd lj">该表显示了虹膜数据集中原始特征之间的强相关性。</strong></figcaption></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mi"><img src="../Images/3bea9fff434e5b3be5c99c164bd7886a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*hHyGQv-wX6_kTS9I5FqYrw.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><strong class="bd lj">该图是一个矩阵图，显示了原始特征之间的散点图、密度图和相关系数。注意原始特征之间的强相关性。</strong></figcaption></figure><p id="7986" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">现在让我们检查变换后的协方差矩阵:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/c73d1f46fe74b264b2d0096e7aea18c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*IbiSITSRYYm2gpZt7OfI6Q.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><strong class="bd lj">该表显示了变换特征之间的零相关性。</strong></figcaption></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/b0e1fd371ad87f2ae807e8da56e1e3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*1O6cB50KqgdKfhl_sCGBSw.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><strong class="bd lj">该图是一个矩阵图，显示了散点图、密度图和主成分之间的相关系数。我们看到特征之间的相关性已经被去除。</strong></figcaption></figure><p id="7e37" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">以下是PCA计算中有用指标的总结:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/6ea532fe758e9b27d8b5e01091572cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*vU4D4rtreCzxxqiRjvPILw.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><strong class="bd lj">组件重要性汇总。</strong></figcaption></figure><p id="03a0" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">基于这一总结，我们看到99.5%的方差是由前三个主成分贡献的。这意味着在最终模型中，第四主成分PC4可以被丢弃，因为它对方差的贡献可以忽略不计。</p><p id="8a75" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">总之，为了说明的目的，我们已经展示了如何使用iris数据集在R中实现PCA算法。你可以在<a class="ae mg" href="https://github.com/bot13956/principal_component_analysis_iris_dataset/blob/master/PCA_irisdataset.R" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上下载代码。</p><p id="8825" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">感谢阅读。</p></div></div>    
</body>
</html>