<html>
<head>
<title>The AI Monthly Top 3 — December 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能月度前三名—2021年12月</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-ai-monthly-top-3-december-2021-4791dee63e31?source=collection_archive---------3-----------------------#2021-12-28">https://pub.towardsai.net/the-ai-monthly-top-3-december-2021-4791dee63e31?source=collection_archive---------3-----------------------#2021-12-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="43d0" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/editorial" rel="noopener ugc nofollow" target="_blank">社论</a></h2><div class=""/><div class=""><h2 id="0997" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">12月最有趣的人工智能突破，包括视频演示、短文、代码和论文参考。</h2></div><blockquote class="kr ks kt"><p id="1f6f" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/the-ai-monthly-top-3-december-2021/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/the-ai-monthly-top-3-december-2021/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/9d19b92bdbf22784133013d995508f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OexLSiT0dH96-Js0RBUXmA.png"/></div></div></figure><p id="7ac8" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">如果你错过了其中的任何一篇，这里有3篇本月最有趣的研究论文。它是按发布日期排列的人工智能和数据科学的<strong class="kx jd">最新突破的精选列表，带有<strong class="kx jd">清晰的视频解释</strong>、<strong class="kx jd">指向更深入文章的链接</strong>和<strong class="kx jd">代码</strong>(如果适用)。享受阅读，如果我错过了任何重要的论文，请在评论中告诉我，或者直接在<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我！</strong></p><p id="8bf3" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">如果你也想阅读更多的研究论文，我推荐你阅读我的文章<a class="ae lr" rel="noopener ugc nofollow" target="_blank" href="/how-to-read-more-research-papers-7737e3770d7f"><strong class="kx jd"/></a>，在那里我分享了寻找和阅读更多研究论文的最佳技巧。</p><blockquote class="mh"><p id="3d38" class="mi mj it bd mk ml mm mn mo mp mq lq dk translated"><em class="mr">关注我上</em> <a class="ae lr" href="https://whats-ai.medium.com/membership" rel="noopener"> <em class="mr">中</em> </a> <em class="mr">看这个AI top 3月！</em></p></blockquote></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="91b2" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">论文#1:</h1><h2 id="7dad" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated"><a class="ae lr" href="https://arxiv.org/abs/2111.03186" rel="noopener ugc nofollow" target="_blank"> EditGAN:高精度语义图像编辑[1] </a></h2><p id="cab2" class="pw-post-body-paragraph ku kv it kx b ky oc kd la lb od kg ld me oe lg lh mf of lk ll mg og lo lp lq im bi translated">控制快速草稿中的任何功能，它将只编辑你想要的，保持图像的其余部分不变！NVIDIA，MIT和UofT基于GANs的草图模型的SOTA图像编辑。</p><h2 id="a29b" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated">观看视频</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="8cea" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated">简短阅读版本</h2><div class="oj ok gp gr ol om"><a rel="noopener  ugc nofollow" target="_blank" href="/image-editing-from-sketches-editgan-4cacca609e2d"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">从草图编辑图像:EditGAN</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">控制快速草稿中的任何功能，它将只编辑你想要的，保持图像的其余部分不变！SOTA…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">pub.towardsai.net</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa mc om"/></div></div></a></div><p id="5c01" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated"><a class="ae lr" href="https://nv-tlabs.github.io/editGAN/" rel="noopener ugc nofollow" target="_blank">代码</a></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi pb"><img src="../Images/82ab72982d5400d663804ace3006eff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dTAMU2gDvC7zlkpd.png"/></div></a></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="9ac6" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">论文#2:</h1><h2 id="aec9" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated"><a class="ae lr" href="https://arxiv.org/pdf/2112.05504.pdf" rel="noopener ugc nofollow" target="_blank">城市NeRF:城市规模的建筑NeRF[2]</a></h2><p id="0303" class="pw-post-body-paragraph ku kv it kx b ky oc kd la lb od kg ld me oe lg lh mf of lk ll mg og lo lp lq im bi translated">这个模型被称为CityNeRF，它是从NeRF发展而来的，我之前在我的频道中介绍过。NeRF是首批使用辐射场和机器学习从图像中构建3D模型的模型之一。但是NeRF并不是那么有效，而且只适用于单一规模。在这里，CityNeRF同时应用于卫星和地面图像，为任何视点生成各种3D模型比例。简而言之，他们将NeRF带到了城市规模。但是怎么做呢？</p><h2 id="2349" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated">观看视频</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="1e3b" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated">简短阅读版本</h2><div class="oj ok gp gr ol om"><a rel="noopener  ugc nofollow" target="_blank" href="/technology-fcb0fbfa9c00"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">CityNeRF:城市比例的3D渲染！</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">以任何比例生成具有高质量细节的城市级3D场景！</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">pub.towardsai.net</p></div></div><div class="ov l"><div class="pc l ox oy oz ov pa mc om"/></div></div></a></div><p id="fef7" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated"><a class="ae lr" href="https://city-super.github.io/citynerf/" rel="noopener ugc nofollow" target="_blank">代码</a></p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="30fa" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">论文#3:</h1><h2 id="dff7" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated"><a class="ae lr" href="https://arxiv.org/abs/2111.09734" rel="noopener ugc nofollow" target="_blank"> ClipCap:图像字幕的剪辑前缀[3] </a></h2><p id="a609" class="pw-post-body-paragraph ku kv it kx b ky oc kd la lb od kg ld me oe lg lh mf of lk ll mg og lo lp lq im bi translated">我们已经看到人工智能使用GANs从其他图像生成图像。然后，有模型能够使用文本生成有问题的图像。2021年初，<a class="ae lr" rel="noopener ugc nofollow" target="_blank" href="/openais-dall-e-text-to-image-generation-explained-1f6fb4bb5a0a?source=your_stories_page----------------------------------------"> DALL-E </a>发布，击败了之前所有使用CLIP从文本输入中生成图像的尝试，CLIP是一种以文本为导向链接图像的模型。一个非常相似的任务叫做图像字幕，听起来可能很简单，但实际上也很复杂。它是机器生成图像的自然描述的能力。</p><p id="79c7" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">简单地标记你在图像中看到的物体是很容易的，但要理解在一张二维图像中发生的事情却是另一个挑战，这个新模型做得非常好！</p><h2 id="678c" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated">观看视频</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="0b2c" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated">简短阅读版本</h2><div class="oj ok gp gr ol om"><a rel="noopener  ugc nofollow" target="_blank" href="/image-captioning-with-clip-and-gpt-d0cb3f3fddda"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">带剪辑和GPT的图像字幕</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">使用剪辑和GPT模型轻松生成图像的文本描述！</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">pub.towardsai.net</p></div></div><div class="ov l"><div class="pd l ox oy oz ov pa mc om"/></div></div></a></div><p id="ec27" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated"><a class="ae lr" href="https://github.com/rmokady/CLIP_prefix_caption" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="4d5f" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated"><a class="ae lr" href="https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab演示</a></p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="4c91" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">如果你喜欢我的工作，并想了解人工智能的最新动态，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)，并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">简讯</strong> </a>！</p><h2 id="b787" class="nr na it bd nb ns nt dn nf nu nv dp nj me nw nx nl mf ny nz nn mg oa ob np iz bi translated">支持我:</h2><ul class=""><li id="ce8c" class="pe pf it kx b ky oc lb od me pg mf ph mg pi lq pj pk pl pm bi translated">支持我的最好方式是在<a class="ae lr" href="https://whats-ai.medium.com/membership" rel="noopener"> <strong class="kx jd">媒体</strong> </a> <strong class="kx jd"> </strong>上关注我，或者如果你喜欢视频格式，在<a class="ae lr" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"><strong class="kx jd">YouTube</strong></a><strong class="kx jd"/>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="4bad" class="pe pf it kx b ky pn lb po me pp mf pq mg pr lq pj pk pl pm bi translated">支持我在<a class="ae lr" href="https://www.patreon.com/whatsai" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> Patreon </strong> </a> <strong class="kx jd">上的工作。</strong></li><li id="a47f" class="pe pf it kx b ky pn lb po me pp mf pq mg pr lq pj pk pl pm bi translated">加入我们的<a class="ae lr" href="https://discord.gg/learnaitogether" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> Discord社区:</strong> <strong class="kx jd">一起学习AI</strong></a>和<em class="kw">分享你的项目、论文、最佳课程、寻找Kaggle队友等等！</em></li><li id="1d3f" class="pe pf it kx b ky pn lb po me pp mf pq mg pr lq pj pk pl pm bi translated">这里是我作为一名研究科学家每天用来寻找和阅读人工智能研究论文的最有用的工具… <a class="ae lr" href="https://www.louisbouchard.ai/research-papers/" rel="noopener ugc nofollow" target="_blank">在这里阅读更多。</a></li></ul><h1 id="6dbf" class="mz na it bd nb nc ps ne nf ng pt ni nj ki pu kj nl kl pv km nn ko pw kp np nq bi translated">参考</h1><p id="06ec" class="pw-post-body-paragraph ku kv it kx b ky oc kd la lb od kg ld me oe lg lh mf of lk ll mg og lo lp lq im bi translated">[1] Ling，h .，Kreis，k .，Li，d .，Kim，S.W .，Torralba，a .和Fidler，s .，2021年5月。EditGAN:高精度语义图像编辑。在第三十五届神经信息处理系统会议上。</p><p id="0692" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">[2]，杨，徐，李，潘，徐，赵，饶，李，戴，林，2021 .城市NeRF:在城市尺度上建造NeRF。</p><p id="4830" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">[3]莫凯迪、赫兹和伯尔曼诺，2021年。ClipCap:图像字幕的剪辑前缀。<a class="ae lr" href="https://arxiv.org/abs/2111.09734" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2111.09734</a></p></div></div>    
</body>
</html>