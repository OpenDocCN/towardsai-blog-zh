<html>
<head>
<title>OpenAI’s Pedagogical Method for Interpretable Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI的可解释机器学习教学方法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/openais-pedagogical-method-for-interpretable-machine-learning-dcc833ae1974?source=collection_archive---------1-----------------------#2022-05-30">https://pub.towardsai.net/openais-pedagogical-method-for-interpretable-machine-learning-dcc833ae1974?source=collection_archive---------1-----------------------#2022-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="de5e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一篇试图为理解神经网络如何决策奠定教学基础的论文。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b0763f72fa8b06b1f42e8267c8f0a10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*En-5z662-cN3X4qw.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://www.linkedin.com/learning/learning-xai-explainable-artificial-intelligence/explainable-ai-expanding-the-frontiers-of-artificial-intelligence?autoplay=true" rel="noopener ugc nofollow" target="_blank">https://www . LinkedIn . com/learning/learning-xai-explaible-artificial-intelligence/explaible-ai-expanding-the-frontiers-of-artificial-intelligence？autoplay=true </a></figcaption></figure><blockquote class="kz la lb"><p id="8ef5" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过125，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="lz ma gp gr mb mc"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">序列</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">与机器学习、人工智能和数据发展保持同步的最佳资源…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">thesequence.substack.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq ks mc"/></div></div></a></div><p id="51bb" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">解释和诠释知识是现代深度学习系统中最困难的问题之一。在有监督的深度学习系统中，训练模型的过程和模型中建立的知识几乎是无法解释的。然而，对知识的解释是人类学习方式中的一个关键因素。让我们来看一个经典的师生场景，在这个场景中，老师试图用一系列的例子向学生传达一个特定的概念。根据学生的反馈，老师将调整他的解释，并试图选择最合适的例子来提高学生的知识。这种教学过程对人类非常有效，但对神经网络来说却非常失败。</p><p id="9a53" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">深度学习系统中一些最有趣的场景需要人类和神经网络之间的无缝协作。然而，在大多数情况下，由于双方使用不同的协议，建立协作非常困难。这方面最有趣的论文之一是几年前OpenAI发表的。在标题<a class="ae ky" href="https://arxiv.org/abs/1711.00694" rel="noopener ugc nofollow" target="_blank">可解释和教育示例</a>下，OpenAI提出了一种方法，试图通过提出一种更具教育意义的方式来解决这一挑战，以技术深度学习系统。</p><p id="fcdc" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">在标题<a class="ae ky" href="https://arxiv.org/abs/1711.00694" rel="noopener ugc nofollow" target="_blank">“可解释的和教学的例子”</a>下，OpenAI的研究人员阐述了一个有趣的论文，关于是什么使得理解深度学习系统的知识如此困难。在他们看来，挑战的一部分是大多数深度学习架构依赖于教师和学生的神经网络共同训练，这阻止了两者之间的任何反馈循环。OpenAI团队没有采用这种模型，而是提出了一种结构，在这种结构中，教师和学生网络可以被迭代训练，从而产生更多可解释的教学策略。</p><h1 id="9429" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">可解释的机器学习</h1><p id="1260" class="pw-post-body-paragraph lc ld it lf b lg nm ju li lj nn jx ll mr no lo lp ms np ls lt mt nq lw lx ly im bi translated">OpenAI可解释教学策略可以被视为两个神经网络(学生和教师)之间的博弈动态。游戏的目标是让学生根据概念的例子来猜测特定的概念，而教师的目标是学会为学生选择最能说明问题的例子。使用图像识别场景作为类比，学生应该尝试猜测特定图像中的概念，而教师应该尝试选择最合适的图像来提高学生的知识。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b857f55cfb5f218e3512076513af4dd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/0*k-RG7rcso6N7BYQl.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:https://arxiv.org/abs/1711.00694<a class="ae ky" href="https://arxiv.org/abs/1711.00694" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="bb74" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">可解释教学的两阶段技术是这样工作的:“学生”神经网络被给予随机选择的概念输入示例，并使用传统的监督学习方法从这些示例中训练以猜测正确的概念标签。在第二步中,“教师”网络——它有一个要教授的预期概念，并可以访问将概念与例子联系起来的标签——在学生身上测试不同的例子，并查看学生给它们分配了哪些概念标签，最终集中在它需要给出的最小例子集上，让学生猜测预期的概念。</p><p id="bf2f" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">OpenAI方法的关键是教师和学生网络被迭代训练，而不是联合训练。在传统模型中，两个神经网络将一起训练，选择人类难以解释的例子。OpenAI技术的目标是产生更多可解释的教学策略，但我们如何真正量化可解释的教学策略？为了评估模型的性能，OpenAI团队以两个基本指标为中心:</p><p id="bc2a" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">1.评估在每项任务中，所选择的策略与人类设计的过于直观的策略有多相似。</p><p id="585e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">2.评估人类教学中所选策略的有效性。</p><p id="d703" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">OpenAI的研究人员在各种各样的场景中应用了可解释的策略，产生了显著的结果，大大改善了传统技术。更具体地说，可解释教学引导学生模型学习可解释的学习策略，然后约束教师学习可解释的教学策略。</p></div></div>    
</body>
</html>