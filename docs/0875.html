<html>
<head>
<title>Getting Meaning from Text: Self-attention Step-by-step Video</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从文本中获取意义:自我关注分步视频</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/getting-meaning-from-text-self-attention-step-by-step-video-7d8f49694f89?source=collection_archive---------3-----------------------#2020-09-01">https://pub.towardsai.net/getting-meaning-from-text-self-attention-step-by-step-video-7d8f49694f89?source=collection_archive---------3-----------------------#2020-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/dc45812de4a4215d6597fee1106f4369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XvVEQwBftk6ZiqgsXXQig.png"/></div></div></figure><h2 id="198a" class="iz ja jb bd b dl jc jd je jf jg jh dk ji translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><p id="7cd6" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">2019年10月，<a class="ae lf" href="https://blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">谷歌宣布</a>将使用其研究人员开发的BERT模型处理搜索查询。这个模型可以抓住语言中困难的细微差别:在搜索<em class="lg"> 2019巴西前往美国的旅行者需要签证</em>，了解到该旅行者是巴西人，目的地是美国。谷歌表示，从现在开始，这一搜索将返回美国驻巴西大使馆的页面，不再显示有关美国公民前往巴西的页面。</p><p id="c7a5" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">值得注意的是，许多像BERT 这样的变形金刚模型的核心的大部分注意力机制仅仅依赖于一些基本的向量运算。</p><p id="47e7" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj jl">我们来看看效果如何。</strong></p><figure class="lh li lj lk gt is"><div class="bz fp l di"><div class="ll lm l"/></div></figure><h1 id="9af2" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">语境怎么了？</h1><p id="b0c2" class="pw-post-body-paragraph kh ki jb kj b kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la mp lc ld le ij bi translated">如果你看到单词<em class="lg"> bank </em>，你可能会想到一家金融机构、你的顾问工作的办公室、在移动中给手机充电的便携式电池，甚至是湖边或河边。<br/>如果给你更多的上下文，比如在<em class="lg">河边散步</em>，你会意识到<em class="lg">河岸</em>和<em class="lg">河</em>很配，所以它一定是指水边的土地。你也可以意识到你可以从这个河岸走过，所以它看起来一定像一条沿河的人行道。整个句子<strong class="kj jl">加起来</strong>就形成了一幅<em class="lg">银行</em>的脑海画面。</p><p id="13f0" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">自我关注试图做同样的事情。</p><h1 id="4213" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">单词嵌入</h1><p id="a81b" class="pw-post-body-paragraph kh ki jb kj b kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la mp lc ld le ij bi translated">像<em class="lg"> bank </em>这样的单词，当它表示一段基本的文本时，被称为<strong class="kj jl"> token </strong>，通常被编码为一个实值的连续值的向量:嵌入向量。</p><p id="79a0" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><a class="ae lf" href="http://jalammar.github.io/illustrated-word2vec/" rel="noopener ugc nofollow" target="_blank">确定标记的嵌入向量内的值</a>是文本处理中繁重工作的一大部分。令人欣慰的是，有了数百个维度来组织已知标记的词汇，嵌入可以被预先训练成<a class="ae lf" href="https://www.youtube.com/watch?v=gQddtTdmG_8" rel="noopener ugc nofollow" target="_blank">以反映其标记在自然语言中如何相关的方式进行数字关联</a>。</p><h1 id="6749" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">如何将嵌入情境化？</h1><p id="c73f" class="pw-post-body-paragraph kh ki jb kj b kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la mp lc ld le ij bi translated">自然语言处理(NLP)技术水平的关键是转换嵌入，以从任何给定句子中的标记创建正确的数字图像。</p><p id="5579" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这就是<strong class="kj jl">比例点积自我关注</strong>机制优雅地(主要)通过线性代数的一些运算所做的。</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mq"><img src="../Images/42b81e9104f1f34eec9b2f4454b71369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iHhebIuMqG6bYnUhyacl5g.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">自我关注机制。图片由作者提供。</figcaption></figure><ul class=""><li id="f414" class="mv mw jb kj b kk kl ko kp ks mx kw my la mz le na nb nc nd bi translated"><strong class="kj jl">令牌关系<br/> </strong>一个句子中的词语有时相互关联，像<em class="lg">河</em>和<em class="lg">岸、</em>有时不关联。为了确定两个令牌的相关程度，attention只需计算它们嵌入之间的<strong class="kj jl">标量积</strong>。<br/>我们可以想象,<em class="lg"> bank </em>和<em class="lg"> river </em>的嵌入更加相似，因为它们都应该对自然的方面进行编码，因此它们的标量积应该比令牌完全不相关的情况高。</li><li id="52bf" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><strong class="kj jl">关键字、查询和值</strong> <br/>不幸的是，当两个记号相同时，直接在嵌入上计算标量积只会给出较高的值，否则给出较小的值。但是语法分析告诉我们，完全不同的词之间可能会发生重要的关系:主语和动词、介词和补语等等。<br/>为了具有更大的灵活性，嵌入通过不同的<strong class="kj jl">线性投影</strong>，使得一次嵌入创建一个<strong class="kj jl">键</strong>、一个<strong class="kj jl">查询、</strong>和一个<strong class="kj jl">值</strong>向量。投影允许我们选择关注嵌入的哪些组件，并确定它们的方向，以便键和查询之间的标量积代表重要的关系。</li><li id="39ac" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><strong class="kj jl">激活</strong> <br/>查询和键之间的标量积给出了查询的令牌和所有其他令牌之间的关系级别，通常<strong class="kj jl">缩小</strong>以获得数值稳定性，然后通过<strong class="kj jl"> softmax </strong> <a class="ae lf" href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/activations" rel="noopener ugc nofollow" target="_blank">激活函数</a>。softmax使大型关系变得更加重要。由于这种操作是非线性的，这也意味着可以多次重新应用自我注意来实现更多更复杂的转换，使过程深度学习。</li><li id="505c" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><strong class="kj jl">线性组合</strong> <br/>新的<strong class="kj jl">上下文化嵌入</strong>通过组合对应于每个输入令牌的<strong class="kj jl">值</strong>来创建，其比例由softmax函数的结果给出:如果<em class="lg"> river </em>令牌的查询与<em class="lg"> bank </em>令牌的关键字有很强的关系，那么<em class="lg"> bank </em>的值很大一部分被添加到<em class="lg"> river </em>的上下文化嵌入中。</li></ul><h1 id="4a0f" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">多头注意力和BERT</h1><p id="3997" class="pw-post-body-paragraph kh ki jb kj b kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la mp lc ld le ij bi translated">可以使用<strong class="kj jl">多个</strong> <strong class="kj jl">不同的<strong class="kj jl">组的</strong>、<strong class="kj jl">查询</strong>和<strong class="kj jl">值</strong>投影来投影单个输入嵌入序列，即所谓的<strong class="kj jl">多头关注</strong>。每个投影集可以专注于计算记号之间的不同类型的关系，并创建特定的上下文化嵌入。</strong></p><p id="f3c1" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">来自不同注意力中心的语境化嵌入被简单地连接在一起。</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/7a15442a5ceed4164f1b5de68b2cddd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_E2Ycw7oHpIJkp_OeOOTNg.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">多头关注。图片由作者提供。</figcaption></figure><p id="2141" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">自然语言处理的深度学习模型通常应用许多层的多头注意力，并混合额外的操作以获得稳健的结果。</p><figure class="lh li lj lk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/e4543d9d9f4a82ecdbab6832a857d2f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X80E2bByAXJ2kEWuqwJSzw.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk translated">BERT处理一个句子以输出更能代表真实意思的语境化嵌入。图片由作者提供。</figcaption></figure><p id="a988" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">例如，<a class="ae lf" href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/bert-encoder" rel="noopener ugc nofollow" target="_blank"> BERT编码器</a>使用令牌的工件嵌入，但总是从将它们添加到<strong class="kj jl">位置</strong> <strong class="kj jl">嵌入</strong>开始。这一步给出了关于输入句子中标记顺序的信息，否则自我注意不会考虑这些信息。</p><p id="9be7" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">额外的线性投影、归一化和前馈层使整个模型更加灵活和稳定。</p><p id="80fc" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">结果是一个模型，它可以消除自然文本中的歧义，并将其减少到精确的值，您可以使用这些值来自动搜索、分类甚至注释文本内容。</p><h1 id="81af" class="ln lo jb bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">阅读更多</h1><p id="3948" class="pw-post-body-paragraph kh ki jb kj b kk ml km kn ko mm kq kr ks mn ku kv kw mo ky kz la mp lc ld le ij bi translated">文学</p><ul class=""><li id="54b7" class="mv mw jb kj b kk kl ko kp ks mx kw my la mz le na nb nc nd bi translated"><a class="ae lf" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">你需要的只是关注</a></li><li id="a3ea" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><a class="ae lf" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a></li><li id="237d" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><a class="ae lf" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank">谷歌的神经机器翻译系统:弥合人类和机器翻译之间的鸿沟</a></li></ul><p id="32e3" class="pw-post-body-paragraph kh ki jb kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在线资源</p><ul class=""><li id="9515" class="mv mw jb kj b kk kl ko kp ks mx kw my la mz le na nb nc nd bi translated"><a class="ae lf" href="http://jalammar.github.io/illustrated-word2vec/" rel="noopener ugc nofollow" target="_blank">图文并茂的Word2vec </a></li><li id="7096" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><a class="ae lf" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图示变压器</a></li><li id="28e3" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><a class="ae lf" href="https://www.youtube.com/watch?v=gQddtTdmG_8" rel="noopener ugc nofollow" target="_blank">矢量化单词(单词嵌入)</a></li><li id="d37e" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><a class="ae lf" href="https://peltarion.com/webinars/nlp-and-bert" rel="noopener ugc nofollow" target="_blank">网络研讨会:NLP和BERT将如何改变语言游戏</a></li><li id="a9e9" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><a class="ae lf" href="https://peltarion.com/blog/data-science/a-year-in-review-nlp-in-2019" rel="noopener ugc nofollow" target="_blank">一年回顾:2019年NLP</a></li><li id="d0f4" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><a class="ae lf" href="https://blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">比以往任何时候都更好地理解搜索</a></li><li id="b2b1" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><a class="ae lf" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">带注释的变压器</a></li><li id="a597" class="mv mw jb kj b kk ne ko nf ks ng kw nh la ni le na nb nc nd bi translated"><a class="ae lf" href="https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L558-L751" rel="noopener ugc nofollow" target="_blank">为伯特实现的谷歌官方关注线</a></li></ul></div></div>    
</body>
</html>