<html>
<head>
<title>Semi-supervised Learning Guide; 3 Models Rise on Top</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">半监督学习指南；3款车型拔得头筹</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/semi-supervised-learning-guide-3-models-rise-on-top-4b03f86cdd52?source=collection_archive---------2-----------------------#2022-07-15">https://pub.towardsai.net/semi-supervised-learning-guide-3-models-rise-on-top-4b03f86cdd52?source=collection_archive---------2-----------------------#2022-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="54c4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我揭示了半监督学习的挑战、最佳实践、9种技术、16种基本模型，以及3种特定模型为何是当今的必学之物</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4dcd0eef3015a082867083df138cdbcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j7YACULve0why_WhlzWehw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的安德鲁·麦克默特里</figcaption></figure><h1 id="c2e3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">半监督学习的介绍性定义</h1><p id="5c90" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">半监督学习是一种机器学习，其中一些训练数据被标记，一些没有被标记。这允许算法从两种数据中学习。因此，半监督学习使用标记和未标记的数据来训练模型。</p><p id="4b7a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">更多技术定义</p><p id="8509" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">半监督学习是一种学习类型，其中学习者可以访问标记和未标记的数据。当没有足够的标记数据来训练模型时，这种访问级别可能是有益的。然而，在未标记的数据中仍然有足够的信息允许模型学习。</p><h1 id="2296" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是半监督模型？</h1><p id="12db" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">半监督学习模型是一种结合了监督和非监督学习技术的机器学习算法。这些模型通常使用少量的已标记数据来训练模型，然后利用该模型来标记额外的未标记数据。当没有足够的已标记数据来训练监督模型，但未标记数据中仍有足够的信息可供学习时，这可能是一种实用的方法。半监督学习的目标可以是从未标记的数据中学习，以提高模型在标记数据上的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/9446ed10156566820723af3d70f9ddc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k-jj1QTD93AXgqRVoTEaJg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">作者Pixabay</figcaption></figure><h1 id="940d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3半监督学习面临的挑战</h1><p id="9f4a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">半监督学习是机器学习的一个子领域，旨在从标记和未标记的数据中学习。挑战在于，获得标记数据通常比获得未标记数据更容易。解决这个问题的一种方法是使用生成模型，它可以生成像训练数据这样的新数据。另一种方法是使用聚类算法对相似的数据点进行分组。</p><p id="0cf3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">简单3 </strong></p><p id="8229" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">1.某些模型的训练数据量可能不足。</p><p id="c53f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.训练数据可能缺乏多样性。</p><p id="916a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.训练数据可能有噪声或包含不可靠的标签</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/1cec707b47f30b4f68df8cffdd506a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yY6bupTFubDHQGeKBjUMpA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由来自Pexels的<a class="ae ky" href="https://www.pexels.com/@pixabay/" rel="noopener ugc nofollow" target="_blank">皮克斯贝</a></figcaption></figure><p id="e590" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">越技3 </strong></p><p id="5381" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">1.标签的可用性:在许多现实世界的应用程序中，只有有限数量的标签可用。这为训练监督模型带来了挑战，监督模型通常需要大量的标记数据。</p><p id="165f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.获得好的特征的困难:在许多问题中，挑战是找到从数据中提取的正确特征，并设计对手头的任务有用的特征，并且可以从有限数量的标记数据中学习。</p><p id="8fe3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.高维度的诅咒:在高维度的数据集中，大部分点距离较远，很难找到它们之间的相似性。这使得很难从有限的标记数据中学习，因为没有附近的数据可以进行归纳。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/3b77c9d2c827bfa723903a8830d45442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOEoaKhybehecL8OuQsztA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的<a class="ae ky" href="https://www.pexels.com/@megapixelstock/" rel="noopener ugc nofollow" target="_blank">百万像素股票</a></figcaption></figure><h1 id="fcb6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">半监督学习大放异彩</h1><p id="2dc0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">半监督学习在有有限数量的标记数据和大量的未标记数据时是有效的。当存在大量标签噪声时，它还可以用于提高监督学习模型的性能。例如，它可以用于以下情况:文本分类、图像分类和语音识别。</p><p id="1d82" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3个例子:</p><p id="a92e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">1)一个组织有一个小的已标记客户数据数据集，但有一个大得多的网站日志数据集(未标记)。半监督学习可以用来预测哪些客户可能会购买。</p><p id="3842" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2)医疗保健组织有一个最小规模的已标记患者数据集，但有一个更大的未标记数据集(如健康记录)。半监督学习可以预测哪些患者有患某些疾病的风险。</p><p id="7177" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3)一个城市有一个小的已标记交通数据数据集，但它的摄像机镜头是一个更大的未标记数据数据集。我们可以部署半监督学习来预测交通模式和识别拥堵热点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/8ab039bf0bd103d4283568c20509bf02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eemLRWhyWdoRCZX_xDrmRQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的<a class="ae ky" href="https://www.pexels.com/@flex-point-security-37073483/" rel="noopener ugc nofollow" target="_blank"> Flex Point Security </a></figcaption></figure><h1 id="bc55" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">何时使用半监督学习需要考虑的3种方法</h1><p id="ce3c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.一种方法是使用生成模型，如深度玻尔兹曼机器，为手头的任务预先训练神经网络的参数。第一种方法是学习数据分布，然后使用学习的模型来初始化神经网络的参数。</p><p id="60f6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.另一种方法是使用自学习算法，如赢家通吃算法[1]，从未标记的数据中学习可用于初始化神经网络的特征。</p><p id="8218" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.最后，可以使用生成式对抗网络来生成类似于训练数据的样本。生成的样本可以以半监督的方式训练神经网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/6d3bf9f48d396f76977f6978ba186304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gr5crxA2xkKCOg1MSDqX5A.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自佩克斯的阿纳斯塔西娅·舒拉耶娃</figcaption></figure><h1 id="f5af" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">9种半监督学习技术</h1><p id="ff0c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.主动学习</p><p id="0c6a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">主动学习是一种半监督学习技术，通常在标记数据成本高或耗时时使用。主动学习算法创建了一个模型，可用于对新数据进行预测。然后，该模型用于从未标记的数据中选择应该标记的实例。目标是使用尽可能少的标签，同时实现预测任务的高准确性。</p><p id="0cd8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.标签传播</p><p id="fdd3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">标签传播是一种半监督学习技术，它将标签从有标签的数据点传播到无标签的数据点。这个想法是，如果两个数据点相似，它们很可能有相同的标签。标签传播从一组种子标签开始，然后将这些标签传播到数据集中的其他点。该过程继续进行，直到所有的点都被[2]标记或者没有进一步的标记可以传播。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/fd986596a12c5686f2a9abb1a2827c2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5dZf-4PShfw2xKAWJWzteA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的Anna Shvets</figcaption></figure><p id="4429" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.自我训练</p><p id="b6ed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">自训练是一种半监督学习技术，可以在有小数据和大数据时使用，小数据被标记，大数据未被标记。这个想法是在标记的数据上训练一个模型，然后使用这个模型来标记未标记的数据。然后，在标记和未标记的数据上重新训练该模型。重复这个过程，直到收敛。</p><p id="cc4a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.合作培训</p><p id="ef19" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">联合训练是一种半监督学习技术，它依赖于两种数据视图，每种视图包含不同的信息。例如，您可以让一个视图包含文本数据，另一个视图包含图像数据。在每个视图上分别训练一个模型[3]，然后将这些模型组合起来标记未标记的数据。</p><p id="7291" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.三元训练</p><p id="c79f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Tri-training是一种半监督学习技术，它使用三个数据视图，每个视图包含不同的信息。例如，您可能有一个包含文本数据的透视图，另一个包含图像数据的视图，以及另一个包含音频数据的视图。在每个视图上分别训练一个模型，然后将这些模型组合起来标记未标记的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/f21c5293d36d6cb10c26ef5757a93a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJe6p57VhOpfSwAlJnRejQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由来自Pexels的<a class="ae ky" href="https://www.pexels.com/@pixabay/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><p id="9a07" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.生成对抗网络</p><p id="2c6c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">GANs是用于无监督[4][7]和半监督学习的神经网络。gan有两部分:一个发生器和一个鉴别器[10]。生成器产生合成的例子，而鉴别器试图将它们分类为真或假。训练过程使生成器和鉴别器在对抗游戏中相互竞争。随着训练的进行，生成器会产生越来越真实的例子，直到鉴别器无法将它们与真实的例子区分开来。</p><p id="b8f6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">7.自动编码器</p><p id="63d4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">自动编码器是一种用于无监督和半监督学习的神经网络[5]。自动编码器获取输入数据，并尝试在输出层准确地重建数据。为此，自动编码器学习在称为潜在层的中间层将输入数据压缩成更小的表示形式。潜在层由比输入层更少的单元组成，因此它可以有效地对输入数据进行降维。一旦自动编码器学会了压缩输入数据，它就可以使用该知识来重建在训练期间看不到的新输入。</p><p id="c377" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">8.受限玻尔兹曼机器</p><p id="26af" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">RBM是一种用于无监督和半监督学习的神经网络[7]。RBM获取输入数据，并尝试使用称为“神经元”的隐藏单元在输出层重建数据。每个神经元都与所有的输入输出单元相连，但神经元只允许与相邻层的其他神经元相连(因此得名“受限”)。在训练期间，RBM通过调整神经元之间的权重来学习近似输入数据分布，使得相似的输入在每个隐藏层神经元处具有相似的激活。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/03b66e07b533ea5a92619b7326d4ff88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lzFsCWApnlGBxMekW4LbdQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的足球妻子</figcaption></figure><p id="61b1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">9支持向量机</p><p id="d3be" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">支持向量机有监督的机器学习[6][8]算法用于分类任务。SVM在特征空间中找到一个超曲面，该超曲面最大化类别之间的间隔(即，具有不同类别标签的点集)。换句话说，支持向量机试图找到一个接近所有训练数据点的决策边界，而不跨越来自不同类别的任何点。</p><h1 id="270f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">15个基本的半监督模型</h1><p id="5066" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.深度信念网络[15]:可用于半监督学习的概率图形模型。它们由一堆受限的玻尔兹曼机组成，是概率模型。该算法旨在学习数据的潜在表示，并可用于分类或特征学习。</p><p id="a8b6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.神经网络自回归模型:它们使用标记和未标记的数据来学习数据的底层结构[16]。该模型由一个根据标记数据训练的自动编码器和一个根据未标记数据训练的第二网络组成。第二个网络用于预测未标记数据的标签。</p><p id="e8e1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.多模态深度学习:这种神经网络架构使用多种输入模态来学习数据表示。它的动机是观察到[11]许多现实世界的问题涉及各种来源的数据，一个单一的模式可能不足以学习一个好的表现。例如，在图像分类中，如果背景混乱或对象被遮挡，仅使用视觉信息可能是不够的。在这种情况下，添加音频或文本数据可以帮助网络学习更好的表示。</p><p id="1a74" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.生成随机网络:这个网络包括一个编码器和一个解码器[17]。编码器获取输入数据并将其映射到潜在空间，而解码器获取潜在空间向量并重建原始输入数据[18]。这种类型的网络是通过让编码器和解码器相互竞争来训练的[19]:编码器试图将数据映射到易于解码的潜在空间向量。相反，解码器试图从这些向量中重建数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/2e6ca76bbc2fa7226b18f7754f7f8a34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DKSCT52KMdvtzEbZkas8EA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由pexels的<a class="ae ky" href="https://www.pexels.com/@mart-production/" rel="noopener ugc nofollow" target="_blank"> MART生产</a></figcaption></figure><p id="365a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.变分自动编码器:变分自动编码器(VAE)是一个生成模型，可以从标记和未标记的数据中学习[20]。VAE的目标是学习可用于生成新数据点的数据的潜在表示。VAEs建立在编码器-解码器网络之上[21]。编码器网络接收输入数据点并输出潜在向量。解码器网络接收潜在向量并输出输入数据点的重构。VAE的训练过程包括优化两个损失函数:重建损失和正则化损失。重建损失就是重建和原始输入之间的误差。正则化损失促使潜在向量接近标准正态分布。</p><p id="39db" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.对抗性自动编码器:用于无监督或半监督学习的自动编码器[22]。对抗性自动编码器建立在两个组件上:将输入数据压缩到潜在空间的编码器网络和从潜在空间重构输入数据的解码器网络[23]。此外，对抗性自动编码器还有第三个组成部分:一个鉴别器网络，它被训练来区分真实数据和生成数据的潜在表示。对抗性自动编码器旨在学习一种潜在的表示，这种表示既能提供信息，又无法与真实数据区分开来。</p><p id="5452" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">7.CycleGAN:这是一种半监督学习模型，由朱俊彦等人在2017年发表的题为“使用循环一致的对抗网络进行不成对的图像到图像的翻译”的论文中提出[9]。该模型被设计成在没有成对训练数据的情况下学习图像到图像的翻译。CycleGAN背后的基本思想是使用两个生成器G和F在两个域X和Y之间进行翻译[24]。g将图像从域X映射到域Y，而F将图像从域Y映射到域X。对抗性损失确保翻译的图像看起来逼真[25]，而循环一致性损失确保翻译的一致性。例如，如果一个梨的图像被翻译成一个香蕉的图像，那么将香蕉翻译回一个梨应该会给出原始的梨图像。CycleGAN可用于多种应用，如图像风格转换、物体变形和照片增强[26]。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/608026099c552aa9c2e259b6ce3983dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z5iBgOo3d-Pp84-b2zZm-w.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的Gustavo Fring</figcaption></figure><p id="2200" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">8.Pix2Pix:图像到图像翻译的模型[27]。该模型是在成对的图像上训练的，例如一个人的图像和具有不同物理外观的那个人的图像。给定第一图像，模型学习生成第二图像。</p><p id="d7e3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">9.上下文条件GAN:上下文条件GAN (CCGAN)是一种半监督学习模型，它将GAN与上下文条件分类器相结合，以产生更真实的图像[28]。CCGAN应用GAN的生成网络来生成图像，然后使用上下文条件分类器来标记生成的图像。这种方法可以提高生成图像的质量，使它们更加逼真。</p><p id="14f1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">10.使用GANs进行插补:使用生成式对抗网络(GAN)对数据集中缺失的数据点进行插补[29]。然后，在完整的数据集上训练GAN，并使用生成的数据点来替换丢失的数据点。这允许模型从完整的数据集学习，同时仍然使用不完整的数据集进行训练。</p><p id="7519" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">11.集成方法:集成方法是一种结合多个模型预测的机器学习技术[30]。它们用于通过组合多个模型的估计值来提高预测模型的准确性。集成方法通常用于半监督学习，其目标是组合在不同数据子集上训练的多个模型的预测。例如，常见的方法是在已标记的数据集上训练一个模型，在未标记的数据集上训练另一个模型。然后可以将两个模型的预测结合起来，形成更准确的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/01d84bdeb9f4f935097f9241225ee50b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*beIpr3NS1TmLOHO_Z4eanw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由Pexels的<a class="ae ky" href="https://www.pexels.com/@kampus/" rel="noopener ugc nofollow" target="_blank"> Kampus生产</a></figcaption></figure><p id="2806" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">12.共同训练:一种半监督学习方法，由两个在不同数据视图上训练的分类器组成[31]。视图可以由不同的特征表示或不同的特征子集生成。这两个分类器对未标记的数据进行预测，并且由这两个分类器预测的标记被添加到训练集[32]。重复这个过程，直到分类器收敛。</p><p id="bb2e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">13.分裂标记:分裂标记是一种半监督学习技术，其中训练数据被随机分成两组，A和B [13]。集合A被标记并用于训练模型，而集合B未被标记。然后将该模型应用于集合B以预测标签。类似地，首先在少量已标记数据上训练模型，然后使用模型来标记更大量的未标记数据。然后，在标记和未标记的数据上重新训练该模型。这个过程可以重复多次。</p><p id="c02b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">14.直推式SVM:直推式SVM是一种半监督学习模型，它使用线性SVM来预测数据点的标签。该模型在标记数据集[12]上训练，然后用于预测未标记数据点的标记。直推式SVM既可以用于分类，也可以用于回归。另一种(更技术性的)解释是，该算法首先将数据转换到高维空间中。接下来，使用支持向量机对数据点进行分类。最后，算法返回到原始空间，并将点映射回它们的原始标签。</p><p id="98ce" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">15.流形正则化:这种技术增加了损失函数的惩罚，并鼓励模型学习接近低维流形的表示。这有助于防止过度拟合并提高泛化能力。此外，该技术在基于流形学习的模型(例如支持向量机和k-最近邻)的训练期间向损失函数添加了惩罚。惩罚鼓励模型找到接近训练数据的数据的低维表示；这种方法可以提高模型的泛化性能，使其更能抵抗过拟合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/c99791789d35ed6c1bb384110fe9ff1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lz8nTmEc0MFikKQR09DQqQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的米哈伊尔·尼洛夫</figcaption></figure><h1 id="0e8f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3个模型拔得头筹:现在要学习的关键模型</h1><p id="e6bf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">它们是基于能量的生成模型[33]和图形模型。</p><p id="cb1c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">生成模型是一种无监督学习算法，旨在学习输入数据的联合概率分布[37]。隐式或显式密度估计都可以做到这一点。隐式密度估计包括从一组给定的数据点直接学习基本分布的参数。相反，显式密度估计首先估计每个数据点的密度，然后将它们组合起来形成总体分布。</p><p id="a841" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">基于能量的模型是一种无监督学习算法，定义了输入数据的能量函数[34]。能量函数表示输入数据的配置的稀有性，训练目标是找到具有最低能量的配置。基于能量的模型可用于密度估计、聚类和生成敌对网络(GANs) [35]。</p><p id="4e7e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">图形模型是一种概率模型，将随机变量之间的关系表示为图形[36]。最流行的图形模型是贝叶斯网络，一个有向无环图(DAG ),其中每个节点代表一个随机变量[38],每个边代表两个变量之间的依赖关系。图形模型可用于监督、非监督和半监督学习。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/47e884f600346c69ef78a39a6d0c744a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8AeICNTGJ62NZx1lir9jtA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的<a class="ae ky" href="https://www.pexels.com/@mikhail-nilov/" rel="noopener ugc nofollow" target="_blank">米哈伊尔·尼洛夫</a></figcaption></figure><h1 id="4c8d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">六大最佳实践</h1><p id="c82d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.选择正确的模型:并非所有的机器学习模型都是平等的，有些模型比其他模型更适合半监督学习。一定要选择一个能够有效地从有标签和无标签数据中学习的模型。</p><p id="f9b3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.预处理数据:原始数据通常不是训练机器学习模型的理想格式。为了最大限度地利用半监督学习算法，有必要对数据进行预处理，使其符合算法可以处理的格式。半监督学习算法通常需要预处理才能正确工作。这可能涉及数据标准化、去除异常值或维度缩减。</p><p id="596c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.调整您的超参数:机器学习模型有许多不同的超参数，可以通过调整来提高性能。在调整这些参数时，使用标记数据和未标记数据，以便模型可以从这两种类型的数据中学习。</p><p id="e093" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.使用交叉验证:交叉验证是一种通过在多个数据子集上训练机器学习模型并在每个子集上测试它来评估机器学习模型的技术。这在半监督学习中尤其重要，因为[39]允许模型从标记和未标记的数据中学习。</p><p id="a5f4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.集成方法:集成方法是用于组合多个机器学习模型的预测的技术。这在半监督学习中是有益的，因为它允许模型相互学习并提高它们的性能。</p><p id="c6a6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.避免负迁移[38]:当使用数据的多个视图时，避免负迁移至关重要，负迁移是指从一个角度学到的知识被负迁移到另一个视图。例如，当两个视图不兼容或标签不正确时，就会发生负迁移。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="7228" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您有任何编辑/修改建议或关于进一步扩展此主题的建议，请考虑与我分享您的想法。</p><h1 id="8f6d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">另外，请考虑订阅我的每周简讯:</h1><div class="nl nm gp gr nn no"><a href="https://pventures.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">周日报告#1</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">设计思维与AI的共生关系设计思维能向AI揭示什么，AI又能如何拥抱…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">pventures.substack.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc ks no"/></div></div></a></div><h1 id="68ad" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">我写了以下与这篇文章相关的内容；他们可能与你有相似的兴趣:</strong></h1><p id="63c2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我在这个链接写了关于<strong class="lt iu">监督学习</strong>的文章:</p><div class="nl nm gp gr nn no"><a rel="noopener  ugc nofollow" target="_blank" href="/supervised-learning-31-of-the-most-important-models-5-are-a-must-learn-9c62444905fa"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">监督学习:31个最重要的模型:5是必须学习的</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">我概述了31个关键的监督学习模型，并揭示了必须学习的前5个模型。</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">pub.towardsai.net</p></div></div><div class="nx l"><div class="od l nz oa ob nx oc ks no"/></div></div></a></div><p id="4165" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我之前提到过<strong class="lt iu">无监督学习</strong>:</p><div class="nl nm gp gr nn no"><a rel="noopener  ugc nofollow" target="_blank" href="/unsupervised-learning-14-of-the-most-important-algorithms-b3e9e07350c9"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">无监督学习:14种最重要的算法</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">14种算法及其使用案例的细分。</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">pub.towardsai.net</p></div></div><div class="nx l"><div class="oe l nz oa ob nx oc ks no"/></div></div></a></div><p id="8d28" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你对我的<strong class="lt iu"> NLP指南</strong>感兴趣，可以在这里找到:</p><div class="nl nm gp gr nn no"><a rel="noopener  ugc nofollow" target="_blank" href="/16-open-source-nlp-models-for-sentiment-analysis-one-rises-on-top-b5867e247116"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">16个用于情感分析的开源NLP模型；一个在顶端升起</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">介绍16款车型，深入了解风格。</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">pub.towardsai.net</p></div></div><div class="nx l"><div class="of l nz oa ob nx oc ks no"/></div></div></a></div></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="0751" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">就是这样，伙计们；我轻轻地划了一下这个区域的表面。我不喜欢撰写占用不必要空间的冗长介绍或结论。如果你对进一步拓展这个话题有任何问题或建议，请与我分享你的想法。</p><p id="0a41" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og">参考文献:</em></p><p id="b4a0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 1。</em><a class="ae ky" href="https://ml.informatik.uni-freiburg.de/former/_media/documents/teaching/ss11/ml/06_wtan.pd" rel="noopener ugc nofollow" target="_blank"><em class="og">https://ml . informatik . uni-freiburg . de/former/_ media/documents/teaching/ss11/ml/06 _ wtan . PD</em></a></p><p id="c474" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 2。分析学进展:整合动态数据挖掘与模拟。</em><a class="ae ky" href="https://leeds-faculty.colorado.edu/glover/article%20-%20IBM%20Dynamic%20Data%20Mining%20Sim%20Opt.pdf" rel="noopener ugc nofollow" target="_blank"><em class="og">https://Leeds-faculty . Colorado . edu/glover/article % 20-% 20 IBM % 20 dynamic % 20 data % 20 mining % 20 sim % 20 opt . pdf</em></a></p><p id="bdfa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 3。在多视图中选择视图的堆叠惩罚逻辑回归。</em><a class="ae ky" href="https://arxiv.org/abs/1811.02316" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/1811.02316</em></a></p><p id="f204" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 4。生成对抗网络深度学习。</em><a class="ae ky" href="https://atcold.github.io/pytorch-Deep-Learning/en/week09/09-3/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://at cold . github . io/py torch-Deep-Learning/en/week 09/09-3/</em></a></p><p id="745f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 5。自动编码器的类型-机器学习概念。</em><a class="ae ky" href="https://www.ml-concepts.com/2022/03/31/types-of-autoencoders/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://www . ml-concepts . com/2022/03/31/types-of-auto encoders/</em></a></p><p id="856e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 6。支持向量机在磁共振脑图像分类中的应用。</em><a class="ae ky" href="https://topics.hails.info/svm.html" rel="noopener ugc nofollow" target="_blank"><em class="og">https://topics.hails.info/svm.html</em></a></p><p id="5eb2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 7。墨菲，K. P. (2012年)。机器学习:概率观点(第1版。).麻省理工出版社。</em></p><p id="fa93" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 8。使用多代理系统的异常检测。</em><a class="ae ky" href="https://users.encs.concordia.ca/~abdelw/papers/Khosravifar_MSc_S2018.pdf" rel="noopener ugc nofollow" target="_blank"><em class="og">https://users . encs . Concordia . ca/~ abdelw/papers/Khosravifar _ MSc _ s 2018 . pdf</em></a></p><p id="2b74" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 9。</em><a class="ae ky" href="https://arxiv.org/abs/1703.10593" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/1703.10593</em></a></p><p id="9968" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 10。无任何标签的脑肿瘤语义分割。</em><a class="ae ky" href="https://www.lfb.rwth-aachen.de/bibtexupload/pdf/WEN19a.pdf" rel="noopener ugc nofollow" target="_blank"><em class="og">https://www.lfb.rwth-aachen.de/bibtexupload/pdf/WEN19a.pdf</em></a></p><p id="e2f1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 11。</em><a class="ae ky" href="https://arxiv.org/abs/2205.03873" rel="noopener ugc nofollow" target="_blank"><em class="og"/></a></p><p id="5697" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">12。如何为计算机视觉用例创建训练数据？<a class="ae ky" href="https://appen.com/blog/how-to-create-training-data-for-computer-vision-use-cases/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://appen . com/blog/how-to-create-training-data-for-computer-vision-use-cases/</em></a></p><p id="8370" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 13。脑磁图自适应解决了人脸的时空特征。</em><a class="ae ky" href="https://www.jneurosci.org/content/35/45/15088" rel="noopener ugc nofollow" target="_blank"><em class="og">https://www.jneurosci.org/content/35/45/15088</em></a></p><p id="3334" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 14。S4L:自我监督半监督学习。</em><a class="ae ky" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_S4L_Self-Supervised_Semi-Supervised_Learning_ICCV_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank"><em class="og">https://open access . the CVF . com/content _ ICCV _ 2019/papers/翟_ S4L _自我监督_半监督_学习_ICCV_2019_paper.pdf </em> </a></p><p id="3e39" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">15。<a class="ae ky" href="https://www.researchgate.net/publication/321394780_Improved_Classification_with_Semi-supervised_Deep_Belief_Network" rel="noopener ugc nofollow" target="_blank"><em class="og">https://www . research gate . net/publication/321394780 _ Improved _ class ification _ with _ Semi-supervised _ Deep _ Belief _ Network</em></a></p><p id="c0cf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">16。<a class="ae ky" href="https://ai.facebook.com/blog/ar-net-a-simple-autoregressive-neural-network-for-time-series/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://ai . Facebook . com/blog/ar-net-a-simple-auto regressive-neural-network-for-time-series/</em></a></p><p id="04ab" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">17。<a class="ae ky" href="https://arxiv.org/abs/1611.07119" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/1611.07119</em></a></p><p id="dcb5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 18。</em><a class="ae ky" href="https://arxiv.org/abs/1503.05571?context=cs" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/1503.05571?context=cs</em></a></p><p id="d4de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 19。</em><a class="ae ky" href="https://proceedings.neurips.cc/paper/2020/file/e586a4f55fb43a540c2e9dab45e00f53-Paper.pdf" rel="noopener ugc nofollow" target="_blank"><em class="og">https://proceedings . neur IPS . cc/paper/2020/file/e 586 a4 f 55 FB 43 a 540 c 2e 9 dab 45 e 00 f 53-paper . pdf</em></a></p><p id="be0a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">20。<a class="ae ky" href="https://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://bjlkeng . github . io/posts/semi-supervised-learning-with-variable-auto encoders/</em></a></p><p id="997c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 21。</em><a class="ae ky" href="https://arxiv.org/abs/2101.07240" rel="noopener ugc nofollow" target="_blank"><em class="og"/></a></p><p id="b9d8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">22。<a class="ae ky" href="https://adversarial-autoencoder-classif.readthedocs.io/en/latest/semi_supervised.html" rel="noopener ugc nofollow" target="_blank"><em class="og">https://adversarial-auto encoder-classif . readthe docs . io/en/latest/semi _ supervised . html</em></a></p><p id="e127" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 23。</em><a class="ae ky" href="https://arxiv.org/abs/1907.06078" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/1907.06078</em></a></p><p id="66b9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 24。</em><a class="ae ky" href="https://arxiv.org/abs/1908.11569" rel="noopener ugc nofollow" target="_blank">【https://arxiv.org/abs/1908.11569】T21</a></p><p id="f6fb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">25。<a class="ae ky" href="https://link.springer.com/article/10.1007/s11548-021-02490-2" rel="noopener ugc nofollow" target="_blank"><em class="og">https://link . springer . com/article/10.1007/s 11548-021-02490-2</em></a></p><p id="4aa5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og">二十六。</em><a class="ae ky" href="https://github.com/arnab39/Semi-supervised-segmentation-cycleGAN" rel="noopener ugc nofollow" target="_blank"><em class="og">https://github . com/arnab 39/Semi-supervised-segmentation-cycle gan</em></a></p><p id="ef61" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">27。<a class="ae ky" href="https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://machine learning mastery . com/semi-supervised-generative-adversarial-network/</em></a></p><p id="6cd8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 28。</em><a class="ae ky" href="https://arxiv.org/abs/1611.06430" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/1611.06430</em></a></p><p id="e1e0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 29。</em><a class="ae ky" href="https://ieeexplore.ieee.org/document/9815858" rel="noopener ugc nofollow" target="_blank"><em class="og">https://ieeexplore.ieee.org/document/9815858</em></a></p><p id="5fc9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">30。<a class="ae ky" href="https://www.sciencedirect.com/science/article/pii/S0950705121000010" rel="noopener ugc nofollow" target="_blank"><em class="og">https://www . science direct . com/science/article/pii/s 0950705121000010</em></a></p><p id="159c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 31。</em><a class="ae ky" href="https://arxiv.org/abs/2107.04795" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/2107.04795</em></a></p><p id="aa35" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 32。</em><a class="ae ky" href="https://arxiv.org/abs/1803.05984" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/1803.05984</em></a></p><p id="5a85" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 33。</em><a class="ae ky" href="https://openreview.net/pdf?id=UlZGDIQlbH1" rel="noopener ugc nofollow" target="_blank"><em class="og"/></a></p><p id="fecf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 34。</em><a class="ae ky" href="https://arxiv.org/abs/2010.13116" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/2010.13116</em></a></p><p id="0de2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">35。<a class="ae ky" href="https://www.bmvc2021-virtualconference.com/assets/papers/0451.pdf" rel="noopener ugc nofollow" target="_blank"><em class="og">https://www . bmvc 2021-virtual conference . com/assets/papers/0451 . pdf</em></a></p><p id="5415" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">36。<a class="ae ky" href="https://arxiv.org/abs/2102.13303" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/2102.13303</em></a></p><p id="6e55" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 37。</em><a class="ae ky" href="https://arxiv.org/abs/2007.00155" rel="noopener ugc nofollow" target="_blank"><em class="og">https://arxiv.org/abs/2007.00155</em></a></p><p id="820e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="og"> 38。马尔可夫链和隐马尔可夫模型——普渡大学。</em><a class="ae ky" href="https://www.stat.purdue.edu/~jianzhan/notes/HMM.pdf" rel="noopener ugc nofollow" target="_blank">【https://www.stat.purdue.edu/~jianzhan/notes/HMM.pdf】T21</a></p><p id="c3d5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">39。数据流形上的局部高阶正则化。<a class="ae ky" href="https://www.arxiv-vanity.com/papers/1602.03805/" rel="noopener ugc nofollow" target="_blank"><em class="og">https://www.arxiv-vanity.com/papers/1602.03805/</em></a></p></div></div>    
</body>
</html>