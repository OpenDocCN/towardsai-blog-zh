<html>
<head>
<title>Difference Between Normalization and Standardization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">规范化和标准化的区别</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/difference-between-normalization-and-standardization-745030eaf96f?source=collection_archive---------1-----------------------#2022-09-11">https://pub.towardsai.net/difference-between-normalization-and-standardization-745030eaf96f?source=collection_archive---------1-----------------------#2022-09-11</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="b7d7" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">了解规范化和标准化之间的区别，不同的方法，以及最重要的是，何时应该考虑使用规范化或标准化。</h2></div></div><div class="ab cl kj kk hy kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="in io ip iq ir"><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gi gj kq"><img src="../Images/4c42f24e9cedd276546c9a781325bacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FZMI4VskNj5PNdE7fqJQtw.jpeg"/></div></div></figure><p id="fcd4" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv">目录</strong></p><p id="c214" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><a class="ae ly" href="#343a" rel="noopener ugc nofollow">简介</a><br/>T5】什么是特征缩放，为什么重要？ <br/> <a class="ae ly" href="#e4bf" rel="noopener ugc nofollow">归一化</a> <br/> ∘ <a class="ae ly" href="#5da7" rel="noopener ugc nofollow">最小最大缩放器</a> <br/> ∘ <a class="ae ly" href="#b615" rel="noopener ugc nofollow">最大最小缩放器</a> <br/> ∘ <a class="ae ly" href="#0ae1" rel="noopener ugc nofollow">鲁棒缩放器</a> <br/> <a class="ae ly" href="#1d89" rel="noopener ugc nofollow">标准化</a> <br/> ∘ <a class="ae ly" href="#cebf" rel="noopener ugc nofollow">标准缩放器</a> <br/> <a class="ae ly" href="#3141" rel="noopener ugc nofollow">概要</a> <br/> <a class="ae ly" href="#059a" rel="noopener ugc nofollow">参考文献</a></p><h1 id="a0e5" class="lz ma iu bd mb mc md me mf mg mh mi mj ka mk kb ml kd mm ke mn kg mo kh mp mq bi translated">介绍</h1><p id="f1d7" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">特征缩放是机器学习流水线中的重要步骤之一。用于特征缩放的两种常用技术是<strong class="le iv">标准化</strong>和<strong class="le iv">标准化</strong>。但是<em class="mw">规范化和标准化的区别是什么？什么时候应该使用规范化和标准化？</em>这是刚开始数据科学之旅的人很常见的问题。让我们试着在文章中回答这些问题。</p><h1 id="14b2" class="lz ma iu bd mb mc md me mf mg mh mi mj ka mk kb ml kd mm ke mn kg mo kh mp mq bi translated">什么是特征缩放，为什么它很重要？</h1><p id="b70c" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">特征缩放是一种变换普通范围内数据的方法— [0，1]或[-1，1]或[-2，2]等。如果不对数据应用特征缩放，则机器学习模型对具有大值的特征给予更高的权重，从而导致有偏差的模型。</p><p id="0c12" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">例如，让我们考虑两个特征——总账单和小费。正如您所看到的，这两个特性在不同的范围内。模型只看数字。它不知道哪个是total_bill，哪个是tip。在构建模型时不进行要素缩放，模型将偏向于具有较大值的要素，从而导致模型有偏差。为了缓解这个问题，我们需要对数据使用特征缩放。</p><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div class="gi gj mx"><img src="../Images/c15a57523cf0d9b542441f1debde046f.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*8BYrkVSJpADtuy_FxI3oPA.png"/></div><figcaption class="my mz gk gi gj na nb bd b be z dk translated">来自Seaborn库的tips数据集的示例</figcaption></figure><p id="df10" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv"> <em class="mw">注</em> </strong> <em class="mw">:特征缩放并不是所有机器学习算法都必须的。基于距离的算法，如线性回归、逻辑回归、支持向量机、KNN、K-means等。，梯度下降优化算法最适合特征缩放。但是基于树的模型如random forest、XGBoost、LightGBM等。，不受缩放的影响。</em></p><pre class="kr ks kt ku gu nc nd ne bn nf ng bi"><span id="bce5" class="nh ma iu nd b be ni nj l nk nl">import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.preprocessing import MaxAbsScaler<br/>from sklearn.preprocessing import RobustScaler<br/>from sklearn.preprocessing import StandardScaler<br/>sns.set(font_scale=1.5)<br/><br/>df = sns.load_dataset('tips')<br/>df = df[['total_bill', 'tip']]<br/>df.head()<br/><br/>fig, axes = plt.subplots(1, 3, figsize=(18, 5))<br/>sns.histplot(data=df, x='total_bill', ax=axes[0])<br/>sns.histplot(data=df, x='tip', ax=axes[1])<br/>sns.scatterplot(data=df, x='total_bill', y='tip', ax=axes[2]);</span></pre><h1 id="e4bf" class="lz ma iu bd mb mc md me mf mg mh mi mj ka mk kb ml kd mm ke mn kg mo kh mp mq bi translated">正常化</h1><p id="9b23" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">归一化是一种特征缩放技术，用于将数据中的特征带到一个公共范围，比如[0，1]或[-1，0]或[-1，1]。在这一节中，我们将介绍下面讨论的3种流行的标准化方法。</p><h2 id="5da7" class="nm ma iu bd mb nn no dn mf np nq dp mj ll nr ns ml lp nt nu mn lt nv nw mp nx bi translated">最小最大缩放器</h2><p id="0f25" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">该方法单独缩放每个特征，使其在范围[0，1]内。每个特征值减去最小值，然后除以最大值和最小值之差。</p><p id="b480" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">它使用最小和最大值进行缩放，最小和最大值对异常值都很敏感。因此，MinMaxScaler方法对异常值也很敏感。注意，MinMaxScaler不会改变数据的分布。</p><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div class="gi gj ny"><img src="../Images/18213b4d6e4cc30fde573e4600b38dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*XLYHmUn4YRvz89BfzoPjmw.png"/></div></figure><p id="fc2e" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv"> Scikit-learn实现</strong></p><p id="dc9b" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">可以使用Sklearn的MinMaxScaler，如下图所示。</p><pre class="kr ks kt ku gu nc nd ne bn nf ng bi"><span id="9029" class="nh ma iu nd b be ni nj l nk nl">from sklearn.preprocessing import MinMaxScaler<br/><br/>scaler = MinMaxScaler()<br/>minmaxscaler_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)</span></pre><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gi gj nz"><img src="../Images/b6a77863a774de0d3cc48b07b6d370c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_glZQx2hGLUVIR3hLeEpRw.png"/></div></div></figure><p id="2294" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv">什么时候使用MinMaxScaler？</strong></p><ul class=""><li id="2c3e" class="oa ob iu le b lf lg li lj ll oc lp od lt oe lx of og oh oi bi translated">当特征的分布未知时(即，如果特征不是正态分布的)，最好使用最小最大缩放器。</li><li id="6e88" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated">如果您正在使用的底层机器学习算法不对数据的分布做出任何假设(例如kNN、神经网络等)，也可以考虑MinMaxScaler。).</li><li id="cc85" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated">仅当要素很少或没有异常值时，才考虑使用MinMaxScaler。</li></ul><p id="a9fa" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv"> <em class="mw">注意</em> </strong> <em class="mw">:默认情况下，MinMaxScaler在[0，1]范围内缩放数据。但是，您可以根据需要通过设置</em> <code class="fe oo op oq nd b">feature_range</code> <em class="mw">参数来修改此范围。</em></p><h2 id="b615" class="nm ma iu bd mb nn no dn mf np nq dp mj ll nr ns ml lp nt nu mn lt nv nw mp nx bi translated">MaxAbsScaler</h2><p id="ece0" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">MaxAbsScaler是另一种归一化技术，该方法单独缩放每个要素，使其在下面提到的不同情况下处于范围[0，1]或[-1，0]或[-1，1]内。</p><ul class=""><li id="b11b" class="oa ob iu le b lf lg li lj ll oc lp od lt oe lx of og oh oi bi translated">仅正值:[0，1]</li><li id="0d09" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated">仅负值:[-1，0]</li><li id="a4dc" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated">正值和负值:[-1，1]</li></ul><p id="7dea" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">在这种方法中，每个特征值除以最大绝对值。因为这种方法使用最大值，因此它对异常值也很敏感，如MinMaxScaler。</p><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div class="gi gj or"><img src="../Images/05d69c7f91df7ac9654bcc15cb6e2723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*KmvC4MZYvPo3CWnP8_IgSg.png"/></div></figure><p id="720e" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv"> Scikit-learn实现</strong></p><p id="9bda" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">可以使用Sklearn的MaxAbsScaler，如下图。</p><pre class="kr ks kt ku gu nc nd ne bn nf ng bi"><span id="e98d" class="nh ma iu nd b be ni nj l nk nl">from sklearn.preprocessing import MaxAbsScaler<br/><br/>scaler = MaxAbsScaler()<br/>maxabsscaler_df = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)</span></pre><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gi gj os"><img src="../Images/ef9ad89a82f536bad9312ca41421feb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MIyarULB0n6ILp1TODEgwg.png"/></div></div></figure><p id="59d3" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv">什么时候使用MaxAbsScaler？</strong></p><ul class=""><li id="3b21" class="oa ob iu le b lf lg li lj ll oc lp od lt oe lx of og oh oi bi translated">如果数据是稀疏的(即大多数值为零)，则必须考虑使用MaxAbsScaler。事实上，MaxAbsScaler是专门为稀疏数据设计的。</li></ul><p id="3b35" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv"><em class="mw"/></strong><em class="mw">—如果数据集很稀疏，为什么要使用MaxAbsScaler？请阅读本文作者Christian vers loot</em><a class="ae ly" href="https://github.com/christianversloot/machine-learning-articles/blob/main/feature-scaling-with-python-and-sparse-data.md" rel="noopener ugc nofollow" target="_blank"><em class="mw">此处</em> </a> <em class="mw">了解。</em></p><h2 id="0ae1" class="nm ma iu bd mb nn no dn mf np nq dp mj ll nr ns ml lp nt nu mn lt nv nw mp nx bi translated">鲁棒定标器</h2><p id="bfd1" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">MinMaxScaler和MaxAbsScaler对异常值很敏感。因此，替代方案是鲁棒定标器。RobustScaler不像MinMaxScaler或MaxAbsScaler那样使用最小值和最大值，而是使用IQR，因此它对异常值具有鲁棒性。</p><p id="7e1e" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">鲁棒定标器的计算公式如下所示。如您所见，中位数从数据点中移除，并根据IQR(四分位数间距)进行缩放。计算出的中值和IQR被存储起来，以便在测试集的转换过程中使用。每个特征的缩放是独立进行的。</p><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div class="gi gj ot"><img src="../Images/317f63771e85b9eeb81384f6699b6b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*QSnqRIOxHeYJ9yknktHv6A.png"/></div></figure><p id="2558" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv"> Scikit-learn实现</strong></p><p id="5c91" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">您可以使用Sklearn的RobustScaler，如下所示。</p><pre class="kr ks kt ku gu nc nd ne bn nf ng bi"><span id="a23e" class="nh ma iu nd b be ni nj l nk nl">from sklearn.preprocessing import RobustScaler<br/><br/>scaler = RobustScaler()<br/>robustscaler_df = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)</span></pre><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gi gj ou"><img src="../Images/a8215d5399dd5c4d48b236ee705d8ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u0RsMR2Vmb1F1sEEMc-d9g.png"/></div></div></figure><p id="e57e" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv">什么时候使用RobustScaler？</strong></p><ul class=""><li id="502e" class="oa ob iu le b lf lg li lj ll oc lp od lt oe lx of og oh oi bi translated">当数据包含异常值时，最好使用RobustScaler，因为它对异常值的敏感度低于MinMaxScaler和MaxAbsScaler。</li></ul><h1 id="1d89" class="lz ma iu bd mb mc md me mf mg mh mi mj ka mk kb ml kd mm ke mn kg mo kh mp mq bi translated">标准化</h1><p id="dabf" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">标准化是机器学习中最常用的特征缩放技术。这是因为一些算法假设数据呈正态或接近正态分布。如果特征呈正态分布，则模型表现不佳。StandardScaler和标准化指的是同一个东西。</p><h2 id="cebf" class="nm ma iu bd mb nn no dn mf np nq dp mj ll nr ns ml lp nt nu mn lt nv nw mp nx bi translated">标准缩放器</h2><p id="9298" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">此方法移除平均值，并用单位方差(或标准偏差)缩放数据。计算出的平均值和标准偏差被存储起来，以便在测试集的转换过程中使用。数据中每个要素的缩放是独立进行的。</p><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div class="gi gj ov"><img src="../Images/5dd6bf61b5fa9d50e03763387e377983.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*l10x4mNCuhVzEWs2j5QQ_g.png"/></div></figure><p id="2d41" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv"> Scikit-learn实现</strong></p><p id="06df" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">可以使用Sklearn的StandardScaler，如下图。</p><pre class="kr ks kt ku gu nc nd ne bn nf ng bi"><span id="087b" class="nh ma iu nd b be ni nj l nk nl">from sklearn.preprocessing import StandardScaler<br/><br/>scaler = StandardScaler()<br/>standardscaler_df = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)</span></pre><p id="c438" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">StandardScaler使用均值，均值对异常值很敏感。因此，异常值对标准定标器有影响。</p><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gi gj ow"><img src="../Images/0c94a50675232a72770022328ebea549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bcGxSJlGp4r_NXNAv_WkKQ.png"/></div></div></figure><p id="5d70" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated"><strong class="le iv">什么时候使用StandardScaler？</strong></p><ul class=""><li id="5ff7" class="oa ob iu le b lf lg li lj ll oc lp od lt oe lx of og oh oi bi translated">如果特性是正态分布的，那么，StandardScaler将是你的首选。</li><li id="26a2" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated">如果您使用的底层机器学习算法对数据的正态分布做出假设(例如，线性回归、逻辑回归等)，请考虑使用StandardScaler。)</li><li id="36f9" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated">如果数据中有异常值，那么您可以删除这些异常值，并使用minmax scaler/maxabscaler/standard scaler。</li></ul><h1 id="3141" class="lz ma iu bd mb mc md me mf mg mh mi mj ka mk kb ml kd mm ke mn kg mo kh mp mq bi translated">摘要</h1><p id="3dd1" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">规范化和标准化是两种流行的特征缩放技术。下表总结了这两种方法。</p><figure class="kr ks kt ku gu kv gi gj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gi gj ox"><img src="../Images/d19857d5333cca229a4f07ca05cd0d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G3IXvGuD-fnxrQNxTZII_w.png"/></div></div><figcaption class="my mz gk gi gj na nb bd b be z dk translated">作者图片</figcaption></figure><p id="75e7" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">但是，请注意，并不是所有算法都必须进行特征缩放。基于树的算法，例如决策树算法、随机森林算法、梯度提升树等。不需要特征缩放。</p><h1 id="c4c0" class="lz ma iu bd mb mc md me mf mg mh mi mj ka mk kb ml kd mm ke mn kg mo kh mp mq bi translated">进一步阅读</h1><ul class=""><li id="756c" class="oa ob iu le b lf mr li ms ll oy lp oz lt pa lx of og oh oi bi translated"><a class="ae ly" rel="noopener ugc nofollow" target="_blank" href="/introduction-to-intel-distribution-of-openvino-toolkit-b1ba5b0cf24f">英特尔发布OpenVINO Toolkit简介</a></li><li id="089b" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated"><a class="ae ly" href="https://pythonsimplified.com/introduction-to-intels-oneapi-ai-analytics-toolkit/" rel="noopener ugc nofollow" target="_blank">英特尔oneAPI人工智能分析工具包简介</a></li><li id="cc23" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated">什么是Scikit-learn管道？</li><li id="eac8" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated">【OneHotEncoder和get_dummies的区别</li><li id="6c0a" class="oa ob iu le b lf oj li ok ll ol lp om lt on lx of og oh oi bi translated"><a class="ae ly" href="https://pythonsimplified.com/how-to-create-interactive-plots-in-pandas/" rel="noopener ugc nofollow" target="_blank">如何在《熊猫》中创造互动情节</a></li></ul><p id="3e60" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">我希望你喜欢阅读这篇文章。如果你喜欢我的文章并想订阅Medium，你可以点击这里的<a class="ae ly" href="https://chetanambi.medium.com/membership" rel="noopener"><strong class="le iv"/></a>。你的会员费直接支持我和你看的其他作家。你也可以在媒体上看到所有的故事。</p><div class="pb pc gq gs pd pe"><a href="https://chetanambi.medium.com/membership" rel="noopener follow" target="_blank"><div class="pf ab fp"><div class="pg ab ph cl cj pi"><h2 class="bd iv gz z fq pj fs ft pk fv fx it bi translated">通过我的推荐链接加入媒体- Chetan Ambi</h2><div class="pl l"><h3 class="bd b gz z fq pj fs ft pk fv fx dk translated">阅读Chetan Ambi的每一个故事(以及媒体上成千上万的其他作家)。您的会员费直接支持…</h3></div><div class="pm l"><p class="bd b dl z fq pj fs ft pk fv fx dk translated">chetanambi.medium.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps la pe"/></div></div></a></div></div><div class="ab cl kj kk hy kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="in io ip iq ir"><h1 id="059a" class="lz ma iu bd mb mc pt me mf mg pu mi mj ka pv kb ml kd pw ke mn kg px kh mp mq bi translated">参考</h1><p id="62b3" class="pw-post-body-paragraph lc ld iu le b lf mr jv lh li ms jy lk ll mt ln lo lp mu lr ls lt mv lv lw lx in bi translated">[1].<a class="ae ly" href="https://scikit-learn.org/stable/modules/preprocessing.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/preprocessing.html</a></p><p id="b9ad" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">[2].<a class="ae ly" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . minmax scaler . html</a></p><p id="bee1" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">[3].<a class="ae ly" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . maxabscaler . html</a></p><p id="131e" class="pw-post-body-paragraph lc ld iu le b lf lg jv lh li lj jy lk ll lm ln lo lp lq lr ls lt lu lv lw lx in bi translated">[4].<a class="ae ly" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . robust scaler . html</a></p></div></div>    
</body>
</html>