<html>
<head>
<title>From MAML to MAML++</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从MAML到MAML++</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/from-maml-to-maml-20de07203d59?source=collection_archive---------0-----------------------#2020-04-30">https://pub.towardsai.net/from-maml-to-maml-20de07203d59?source=collection_archive---------0-----------------------#2020-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eb42" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学会学习</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f17da523024263c6a89e0a0763a0d7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tvS5hgXiAvveadlO"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="3a21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想象一下，你只看到狗几次，人类就能够识别一个新的概念或想法，并在以后识别狗。元学习受到这一思想的启发，而模型不可知的元学习，又名MAML (Finn et al. 2017)，是元学习研究的突破之一。</p><p id="d413" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你不熟悉元学习或MAML (Finn et al. 2017)，你可以查看元学习故事中的<a class="ae ky" href="https://medium.com/towards-artificial-intelligence/a-gentle-introduction-to-meta-learning-8e36f3d93f61" rel="noopener">元学习介绍</a>和<a class="ae ky" href="https://medium.com/dataseries/unsupervised-learning-in-meta-learning-f71c549e2ae2" rel="noopener">无监督学习。简而言之，元学习旨在克服缺乏训练问题和处理预测时间中看不见的标签。MAML (Finn et al. 2017)的目标是学习一个模型，如果模型经过预先训练，它可以在新任务上取得快速进展。</a></p><p id="bf95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管MAML (Finn等人，2017年)取得了良好的效果，但它也存在一些缺点。训练时间长是弊端之一。与一般的神经网络模型不同，MAML (Finn等人，2017年)包括两个梯度计算。两年后，研究人员重新审视了这种方法，并在原有方法的基础上提出了一些改进。</p><p id="927b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们涵盖了MAML++ (Antonio等人，2018)，这是MAML的改进版本。首先，我们通过MAML的问题或局限性。之后，我们将讨论MAML++如何解决上述问题。</p><h1 id="d53d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">MAML++</h1><p id="19d5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">虽然MAML是一个简单而强大的框架，但它有一些局限性，而MAML++就是为了克服这些局限性而开发的。</p><h2 id="3441" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">训练不稳定性:</h2><p id="4952" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">通过设计，参数在训练中被多次更新。由于缺少跳跃连接层，每个梯度都需要通过所有网络。它引入了梯度爆炸和梯度递减问题。</p><p id="53c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提出了多步损耗优化(MSL)来计算每一步后的加权损耗和，而不是内环的最后一步。加权方法确保模型参与后面的步骤而不是前面的步骤。</p><h2 id="ad7e" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">二阶导数成本</h2><p id="9d61" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">MAML使用一阶导数而不是二阶导数来计算梯度。它节省了大量的计算成本，但牺牲了泛化性能。</p><p id="deb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了平衡计算成本和性能，提出了导数阶退火算法。简单来说，MAML++在前50个历元使用一阶渐变，切换到二阶渐变。</p><h2 id="aa38" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">缺少批量标准化统计累积</h2><p id="180d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">MAML使用当前批次统计数据来执行批次标准化。但是，使用累积统计数据将覆盖全局平均值和标准差，而不是局部。</p><p id="4271" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提出了逐步批量归一化运行统计(BNRS)。不是使用当前批次统计数据来执行批次归一化，而是为每个批次归一化图层引入运行批次均值和标准差。它是逐步更新的，而不是对所有步骤都使用静态值。</p><h2 id="3c6a" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">共享(跨步骤)批次标准化偏差</h2><p id="9203" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">所有批次标准化偏差不会在同一个内循环中更新。假设所有基本模型在整个内循环更新中是相同的，并且具有相同的特征分布。</p><p id="7a3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提出了分步批量归一化权重和偏差(BNWB)。简单地说，偏见是在内循环的每一步中习得的。</p><h2 id="4652" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">共享内循环(跨步骤和跨参数)学习率</h2><p id="ac41" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">与上述问题相同，学习率在同一个内循环中共享。为了找到更好的结果，它需要更多的计算成本。</p><p id="587b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提出了学习每层每步的学习速率和梯度方向(LSLR)。根据自我解释，它学习网络和基础网络中每一层的学习速率和方向。</p><h2 id="ea41" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">固定外环学习速率</h2><p id="1b5f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">从最新的研究来看，使用动态学习率可以提高泛化性能。</p><p id="6886" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提出了元优化器学习速率的余弦退火算法。与静态学习速率相比，退火方法不是使用静态外环学习速率，而是产生更高性能的已被证明的方法。余弦退火是一种根据步长改变学习方式的方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/fdbb23ce79fe12bfbde5623b5203167e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*OZm7mgGgVzv_h7v8.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">余弦退火(从<a class="ae ky" href="https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163" rel="noopener" target="_blank">介质</a></figcaption></figure><h1 id="e383" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">实验</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7fdb506ecffe2bacdf6004201632471c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*MIvR9XPJGlwDuWQe0MmAiw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">Omniglot数据集性能比较(安托尼乌等人，2018年)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/9c79df2a3236177935200d033b476514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*24whsOTxamo1jcD8_w7QqA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">迷你图像网络数据集性能比较(安托尼乌等人，2018年)</figcaption></figure><h1 id="713b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">延伸阅读</h1><ul class=""><li id="df0f" class="nh ni it lb b lc mn lf mo li nj lm nk lq nl lu nm nn no np bi translated"><a class="ae ky" href="https://medium.com/towards-artificial-intelligence/a-gentle-introduction-to-meta-learning-8e36f3d93f61" rel="noopener">元学习简介</a></li><li id="2de0" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><a class="ae ky" href="https://medium.com/dataseries/unsupervised-learning-in-meta-learning-f71c549e2ae2" rel="noopener">元学习中的无监督学习</a></li></ul><h1 id="3bf9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">关于我</h1><p id="a433" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。你可以通过<a class="ae ky" href="https://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="5920" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><ul class=""><li id="6d39" class="nh ni it lb b lc mn lf mo li nj lm nk lq nl lu nm nn no np bi translated">C.芬恩、p .阿贝耳和s .莱文。<a class="ae ky" href="https://arxiv.org/pdf/1703.03400.pdf" rel="noopener ugc nofollow" target="_blank">用于深度网络快速适应的模型不可知元学习</a>。2017</li><li id="5201" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">A.安托尼乌，爱德华和斯托基。<a class="ae ky" href="https://arxiv.org/pdf/1810.09502.pdf" rel="noopener ugc nofollow" target="_blank">如何训练你的MAML </a>。2018.</li></ul></div></div>    
</body>
</html>