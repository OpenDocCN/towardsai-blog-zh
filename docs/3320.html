<html>
<head>
<title>Evaluating Mode Collapse in GANs Using NDB Score</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用NDB分数评估GANs中的模式崩溃</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/evaluating-mode-collapse-in-gans-using-ndb-score-446f17791d16?source=collection_archive---------3-----------------------#2022-11-17">https://pub.towardsai.net/evaluating-mode-collapse-in-gans-using-ndb-score-446f17791d16?source=collection_archive---------3-----------------------#2022-11-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3ea8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是我从GAN生成的一些艺术作品。他们一点也不引人注目，但他们是多样化的。然而，情况并非总是如此。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/15363e47d411348126d2753c94ef3f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0r-8-4l6pqBDR0RMOkRIBA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">GAN生成的图像</figcaption></figure><p id="9f41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一组图片来自我训练的另一个甘。它们不仅可怕，而且一模一样。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/c76034e1dddf2ba6af6473b724804a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I0P_cQLTbhBHwbhTMnRWrQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">Gan生成的图像</figcaption></figure><p id="388b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">众所周知，甘人很难训练。它们很少(如果有的话)收敛，并且经常遭受模式崩溃。如上图所示，当GANs无法获得数据分布中的不同模式并无情地生成相似的图片时，就会发生模式崩溃。</p><p id="b660" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">仅仅通过绘制图像来发现模式崩溃是很方便的，但是随着数据集大小的增加，对其进行定量评估可能会更方便。我们将使用<strong class="jp ir">NDB分数</strong>来完成。</p><p id="3646" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章假设你熟悉GAN训练机制。如果你不知道它们是如何运作的，请参考这篇<a class="ae lb" href="https://jonathan-hui.medium.com/gan-whats-generative-adversarial-networks-and-its-application-f39ed278ef09" rel="noopener">文章</a>。</p><h1 id="00df" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">模式崩溃</strong></h1><p id="fc35" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">你看，模式崩溃在甘的训练策略中根深蒂固。真实世界的数据是多模态的，理想的GAN必须捕捉所有这些模态。例如，MNIST数据集中的每个数字都是一个独立的模式，您可能更喜欢生成所有数字的GAN。然而，我们通常不会鼓励他们这样做。</p><p id="05a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设生成器很好地构造了数字“2”来欺骗鉴别器。它不需要再拥挤了。然而，在训练迭代期间，鉴别器将接收这些生成的被标记为假的二进制数，并随着时间的推移，学会捕捉虚张声势。当这种情况发生时，生成器可以很容易地切换到另一个数字，比如“3”，并继续模式崩溃循环。直觉上，你可以认为这是在工作不足的情况下对额外工作的冷漠。</p><p id="f841" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们学习定性地跟踪这一现象。</p><h1 id="3b3c" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">设置GAN</h1><p id="e7c8" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">可以在以下链接中找到此实施的完整记录:</p><div class="mf mg gp gr mh mi"><a href="https://www.kaggle.com/code/shashank069/gan-art-and-ndb-score/notebook" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab fo"><div class="mk ab ml cl cj mm"><h2 class="bd ir gy z fp mn fr fs mo fu fw ip bi translated">甘艺术与乐谱</h2><div class="mp l"><h3 class="bd b gy z fp mn fr fs mo fu fw dk translated">使用Kaggle笔记本探索和运行机器学习代码|使用来自多个数据源的数据</h3></div><div class="mq l"><p class="bd b dl z fp mn fr fs mo fu fw dk translated">www.kaggle.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw kv mi"/></div></div></a></div><p id="1ca3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://github.com/shashank14k/Generative_Models/blob/main/GAN/notebooks/gan-art-and-ndb-score.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/shashank 14k/Generative _ Models/blob/main/GAN/notebooks/GAN-art-and-nd b-score . ipynb</a></p><p id="65df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于培训的数据可以在<a class="ae lb" href="https://www.kaggle.com/datasets/bryanb/abstract-art-gallery" rel="noopener ugc nofollow" target="_blank">这里找到</a> ( <a class="ae lb" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">执照</a>)。这是其中的一些图片。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mx"><img src="../Images/c82a921d2e1ecc1ae94d2719406f4262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lb7an99JIC7r7Z8FCXpJyA.png"/></div></div></figure><p id="8a03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将从进口开始。</p><pre class="km kn ko kp gt my mz na nb aw nc bi"><span id="eee0" class="nd ld iq mz b gy ne nf l ng nh">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import tensorflow  as tf<br/>from tensorflow.keras import layers<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Reshape, Conv2D, BatchNormalization, Conv2DTranspose<br/>from tensorflow.keras.layers import LeakyReLU, Dropout, ZeroPadding2D, Flatten, Activation<br/>from tensorflow.keras.optimizers import Adam<br/>from sklearn.cluster import KMeans</span></pre><p id="89af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将使用TensorFlow数据加载器从目录中加载图像，将它们的形状缩小到(64，64)，并对它们进行归一化。注意，这里的批处理大小是全局批处理的一半，因为另一半来自生成器图像。</p><pre class="km kn ko kp gt my mz na nb aw nc bi"><span id="8c8b" class="nd ld iq mz b gy ne nf l ng nh">BATCH = 64<br/>IMG_SIZE = (64,64)<br/>LATENT_DIM = 100<br/>EPOCHS = 600<br/>PATH = "../input/abstract-art-gallery/Abstract_gallery/Abstract_gallery"<em class="ni"><br/>#Importing data</em><br/>batch_s = int(BATCH/2)<br/><em class="ni">#Import as tf.Dataset</em><br/>data = tf.keras.preprocessing.image_dataset_from_directory(PATH, label_mode = None, image_size = IMG_SIZE, batch_size = batch_s).map(lambda x: x /255.0)</span></pre><p id="fc92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们来构建生成器和鉴别器。请注意，鉴别器不包括任何池层。根据这篇2015年<a class="ae lb" href="http://https//arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank">的论文</a>，交错卷积比池化层表现更好。</p><pre class="km kn ko kp gt my mz na nb aw nc bi"><span id="df6a" class="nd ld iq mz b gy ne nf l ng nh">generator=Sequential()<br/>generator.add(Dense(4*4*512,input_shape=[LATENT_DIM]))<br/>generator.add(Reshape([4,4,512]))<br/>generator.add(Conv2DTranspose(256, kernel_size=4, strides=2, padding="same"))<br/>generator.add(LeakyReLU(alpha=0.2))<br/>generator.add(BatchNormalization())<br/>generator.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding="same"))<br/>generator.add(LeakyReLU(alpha=0.2))<br/>generator.add(BatchNormalization())<br/>generator.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding="same"))<br/>generator.add(LeakyReLU(alpha=0.2))<br/>generator.add(BatchNormalization())<br/>generator.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding="same",<br/>                                 activation='sigmoid'))</span><span id="dca9" class="nd ld iq mz b gy nj nf l ng nh">discriminator=Sequential()<br/>discriminator.add(Conv2D(32, kernel_size=4, strides=2, padding="same",input_shape=[64,64, 3]))<br/>discriminator.add(Conv2D(64, kernel_size=4, strides=2, padding="same"))<br/>discriminator.add(LeakyReLU(0.2))<br/>discriminator.add(BatchNormalization())<br/>discriminator.add(Conv2D(128, kernel_size=4, strides=2, padding="same"))<br/>discriminator.add(LeakyReLU(0.2))<br/>discriminator.add(BatchNormalization())<br/>discriminator.add(Conv2D(256, kernel_size=4, strides=2, padding="same"))<br/>discriminator.add(LeakyReLU(0.2))<br/>discriminator.add(Flatten())<br/>discriminator.add(Dropout(0.5))<br/>discriminator.add(Dense(1,activation='sigmoid'))</span></pre><p id="0c07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">定义培训流程</p><pre class="km kn ko kp gt my mz na nb aw nc bi"><span id="b442" class="nd ld iq mz b gy ne nf l ng nh">class <strong class="mz ir">GAN</strong>(tf.keras.Model):<br/>    def __init__(self, discriminator, generator, latent_dim):<br/>        super(GAN, self).__init__()<br/>        self.discriminator = discriminator<br/>        self.generator = generator<br/>        self.latent_dim = latent_dim<br/><br/>    def compile(self, d_optimizer, g_optimizer, loss_fn):<br/>        super(GAN, self).compile()<br/>        self.d_optimizer = d_optimizer<br/>        self.g_optimizer = g_optimizer<br/>        self.loss_fn = loss_fn<br/>        self.dloss = tf.keras.metrics.Mean(name="discriminator_loss")<br/>        self.gloss = tf.keras.metrics.Mean(name="generator_loss")<br/><br/>    @property<br/>    def metrics(self):<br/>        return [self.dloss, self.gloss]<br/><br/>    def train_step(self, real_images):<br/>        batch_size = tf.shape(real_images)[0]<br/>        noise = tf.random.normal(shape=(batch_size, self.latent_dim))<br/>        generated_images = self.generator(noise)<br/>        combined_images = tf.concat([generated_images, real_images], axis=0)<br/>        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)<br/>        labels += 0.05 * tf.random.uniform(tf.shape(labels))<br/>        with tf.GradientTape() as tape:<br/>            predictions = self.discriminator(combined_images)<br/>            dloss = self.loss_fn(labels, predictions)</span><span id="f513" class="nd ld iq mz b gy nj nf l ng nh">grads = tape.gradient(dloss, self.discriminator.trainable_weights)<br/>        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))<br/><br/>        noise = tf.random.normal(shape=(2*batch_size, self.latent_dim))<br/>        labels = tf.zeros((2*batch_size, 1))<br/>        with tf.GradientTape() as tape:<br/>            predictions = self.discriminator(self.generator(noise))<br/>            gloss = self.loss_fn(labels, predictions)<br/>        grads = tape.gradient(gloss, self.generator.trainable_weights)<br/>        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))<br/>        self.dloss.update_state(dloss)<br/>        self.gloss.update_state(gloss)<br/>        return {"d_loss": self.dloss.result(), "g_loss": self.gloss.result()}</span></pre><p id="6437" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们转到评估。我们将使用k-means聚类，这可能在定性上没有意义，因为我们所有的图像都是随机的绘画(单个类)。尽管如此，我希望k-means算法能够识别它们之间微妙的相似之处，并创建适当的聚类。</p><p id="79da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们有形状的RGB图像(64，64)。为了降低维度，我们将沿着最后一个轴对数组进行平均，以将其转换为灰度。请注意，转换为灰度的实际公式是不同的。更多信息请参考此<a class="ae lb" href="https://www.kaggleusercontent.com/kf/109724707/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..HZt6f9T0li9A_Q8jD_FGqg.QMgQ2PtJ5EviJHKNG6-uz1-P8u39TO2Y5l1jyB2fOKaMQY0dx-jbbtos7PWCGBeDPO5baMlc08Ca9gpzI_a2ihJOyYiJUTPOAonG09Tg4fDQz3TnciDd_rGmJWUuRWMock1bhV4L08p3YQHPnqX8FTof0rGWxWABbtp5v9MwYhQxvmiBibVCx0fNlCy5G-mBSzC5_3W2_fRC2TXQMC7L4TFlidmGRbyrgLc8I4VxDfT_RwYMbhDYgKD2x2p220OWB8-YKWLnMfUpljNH9jxkjgB0Xs3MT-c5JxYUzGkpRsINVcXLhZv2muSUA-Px7F6f3vo-BWyYzIryJOrPihlI7s__1jaaV005BknMdj3rIfUpPBIzCdRcfdrpgStCRu3NPOcmkwnpeXYTA1ESpI8ic4oST_D6CDH_RO3KyuV_s6RjWMI9-IWbYB25RoHTxStvWcDMErDfBmTHTw7D22JbW5BZcq4tkJx4_z8epnMrZ6xOlwdkzDTNLyyIEdnIp3AZ_76AogZa3eambE3zd3L93mnyzwiHnJ55zpgOztlW0clIzk88-Wya7t540PC0rJyxOxSv1YkdPo4cXYstc2McKbUU2wbVovoDUzePvLhZKr0OcmFCNzUND5RqQAPinNJP9MdTur3a7o_APVn1dS5nwF7fjz0vDQ913QznwyhwG0w.q3cRIUL4Ytk8oTJbFyju7w/!https://e2eml.school/convert_rgb_to_grayscale.html" rel="noopener ugc nofollow" target="_blank">链接</a>。我们可以使用自动编码器/PCA来进一步缩小维度，但我现在不会这么做。最后，为了聚类，我们还将使图像变平。</p><pre class="km kn ko kp gt my mz na nb aw nc bi"><span id="4132" class="nd ld iq mz b gy ne nf l ng nh">images = np.asarray(images)<br/>images = np.mean(images,axis=3)<br/>images = images.reshape((images.shape[0],-1))</span></pre><p id="26ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了限制计算量，我只使用了前500张图片来创建聚类。肘部得分还没有完全稳定下来。这可能是因为同一类图像之间的细微差异。进一步降低图像维数可能有助于创建更好的聚类。为了便于说明，我们将使用第7簇中的扭结。</p><pre class="km kn ko kp gt my mz na nb aw nc bi"><span id="3057" class="nd ld iq mz b gy ne nf l ng nh">elbow_scores=[]<br/>for c <strong class="mz ir">in</strong> range(4,10):<br/>    kmeans = KMeans(c)<br/>    kmeans.fit(images[:500])<br/>    elbow_scores.append(kmeans.inertia_)<br/><br/>plt.plot(range(4,10),elbow_scores)<br/>plt.xlabel('Number of Clusters')<br/>plt.title('Elbow Score')<br/>plt.show()</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nk"><img src="../Images/dba380067cdef160917e128d277252d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1sohQU0XoS9T1IUIVIgwPw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</figcaption></figure><p id="dc49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们将从生成器中生成500幅图像，并查看它们属于哪一类。</p><pre class="km kn ko kp gt my mz na nb aw nc bi"><span id="4f1e" class="nd ld iq mz b gy ne nf l ng nh">kmeans=KMeans(7)<br/>train_classes=kmeans.fit_predict(images[:500])<br/>arr = tf.random.normal(shape=(500,LATENT_DIM))<br/>generated_portraits = generator(arr)<br/>generated_portraits = np.array(generated_portraits).mean(axis=3).reshape((generated_portraits.shape[0],-1))<br/>generated_classes = kmeans.predict(generated_portraits)</span></pre><p id="e6e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经生成了除第四组以外的所有图像。GAN似乎已经很好地学习了分布，并且可以通过更多的训练迭代/超参数调整来改进。接下来，我们扩展这个评估来创建一个更具体的统计测试(NDB分数)。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nl"><img src="../Images/2e7ff628108fe18147ae051ed71397cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ADA42mNdzTbMaHgNhcFpA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</figcaption></figure><h1 id="7d24" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">NDB分数</h1><p id="7b6f" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">理想的GAN必须非常接近真实的数据分布。这是用NDB分数量化的。计算方法如下:</p><ol class=""><li id="69e8" class="nm nn iq jp b jq jr ju jv jy no kc np kg nq kk nr ns nt nu bi translated">将训练数据(t样本)聚类到“n”个箱中(就像我们将绘画聚类到7个箱中一样)</li><li id="3a89" class="nm nn iq jp b jq nv ju nw jy nx kc ny kg nz kk nr ns nt nu bi translated">生成(g个样本)图像</li><li id="3477" class="nm nn iq jp b jq nv ju nw jy nx kc ny kg nz kk nr ns nt nu bi translated">预测每个生成图像的聚类(bin)</li><li id="56dd" class="nm nn iq jp b jq nv ju nw jy nx kc ny kg nz kk nr ns nt nu bi translated">对于每个箱子，进行以下测试:</li></ol><p id="f1ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">a.计算箱中训练样本和生成样本的比例</p><p id="70c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">b.将它们的差值除以标准误差SE，计算如下。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/39c875eb31e77e2940231cbaff048438.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/0*zn1wUXUhpLwvW94X"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</figcaption></figure><p id="ec6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的“P”和“q”是指训练和生成的数据，“P”是混合样本的比例。</p><p id="4190" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">c.如果对应于z得分的p值小于阈值，则认为该条柱在统计上不同</p><p id="56b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">5.将统计上不同的箱数除以箱总数。这会产生一个数字b/w 0和1，量化真实分布和学习分布之间的差异。</p><p id="7ebe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">6.如果上述量大于设定的阈值，则认为GAN遇到了模式崩溃。</p><pre class="km kn ko kp gt my mz na nb aw nc bi"><span id="94b5" class="nd ld iq mz b gy ne nf l ng nh">def ndb_score(training_data_classes,generated_data_classes,num_classes,z_threshold):<br/>    ndb = []<br/>    NT = len(training_data_classes)<br/>    NG = len(generated_data_classes)<br/>    for i <strong class="mz ir">in</strong> range(num_classes):<br/>        nt = np.sum(training_data_classes==i)<br/>        pt = nt/len(training_data_classes) <em class="ni">#training data proportion for bin</em><br/>        ng = np.sum(generated_data_classes==i)<br/>        pg = ng/len(generated_data_classes) <em class="ni">#generated data proportion for bin</em><br/>        P = (nt+ng)/(NT+NG)<br/>        SE = (P*(1-P)*((1/NT)+(1/NG)))**0.5<br/>        if abs((pt-pg)/SE) &gt; z_threshold:<br/>            ndb.append(i)<br/>    print(f"Statisticall different classes:<strong class="mz ir">{</strong>ndb<strong class="mz ir">}</strong>")<br/>    print(f"ndb score: <strong class="mz ir">{</strong>len(ndb)/num_classes<strong class="mz ir">}</strong>")</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nl"><img src="../Images/1fa2a9dd86ea3691e68b4062002d64a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJi1pma4iXakCDvzMSaJ-w.png"/></div></div></figure><p id="77b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的GAN的NDB分数为0.25，只有两个集群(4，5)在统计上有所不同。所以，我们已经成功地避开了模式崩溃的迂回壕沟。该函数可以成为GAN类的一部分，并作为验证方案在每个时期结束时运行。你可以在这里找到代码。</p><h2 id="30a6" class="nd ld iq bd le ob oc dn li od oe dp lm jy of og lq kc oh oi lu kg oj ok ly ol bi translated">结论</h2><p id="ac2a" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">谢谢你一直读到最后。围绕如何避免模式崩溃有很多想法。除了调整超参数和尝试不同的损失函数，可以采用不同的训练策略。这篇<a class="ae lb" href="https://arxiv.org/pdf/1606.03498.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>详细介绍了一些机制。我会试着在其他的文章中介绍它们。</p></div><div class="ab cl om on hu oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ij ik il im in"><p id="015b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参考资料:</p><ol class=""><li id="cc24" class="nm nn iq jp b jq jr ju jv jy no kc np kg nq kk nr ns nt nu bi translated"><a class="ae lb" href="https://wandb.ai/authors/DCGAN-ndb-test/reports/Measuring-Mode-Collapse-in-GANs--VmlldzoxNzg5MDk#:~:text=The%20NDB%20score%20is%20one,in%20the%20score%20over%20time" rel="noopener ugc nofollow" target="_blank">https://wandb . ai/authors/DCGAN-nd b-test/reports/Measuring-Mode-Collapse-in-GANs-vmlldzoxnzg 5 mdk #:~:text = The % 20 ndb % 20 score % 20 is % 20 one，in % 20 The % 20 score % 20 over % 20 time</a>。</li><li id="16e9" class="nm nn iq jp b jq nv ju nw jy nx kc ny kg nz kk nr ns nt nu bi translated"><a class="ae lb" href="https://arxiv.org/abs/1805.12462" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1805.12462</a></li><li id="3411" class="nm nn iq jp b jq nv ju nw jy nx kc ny kg nz kk nr ns nt nu bi translated">数据集:<a class="ae lb" href="https://www.kaggle.com/datasets/bryanb/abstract-art-gallery" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/bryanb/abstract-art-gallery</a></li></ol></div></div>    
</body>
</html>