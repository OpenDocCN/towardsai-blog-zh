<html>
<head>
<title>KNN (K-Nearest Neighbors) is Dead!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">KNN(K-最近邻)死了！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/knn-k-nearest-neighbors-is-dead-fc16507eb3e?source=collection_archive---------1-----------------------#2020-12-02">https://pub.towardsai.net/knn-k-nearest-neighbors-is-dead-fc16507eb3e?source=collection_archive---------1-----------------------#2020-12-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="293f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/opinion" rel="noopener ugc nofollow" target="_blank">观点</a></h2><div class=""/><div class=""><h2 id="5fcf" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">人工神经网络万岁，他们在sklearn的KNN上实现了380倍的惊人加速，同时提供了99.3%的相似结果。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f874c919a62a346f1ef721c04682d4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Co53wWu5zBkAtNMqeTWy7g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">KNN就像死了一样！<a class="ae lh" href="https://github.com/stephenleo/adventures-with-ann/blob/main/knn_is_dead.ipynb" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="1e02" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们正在经历灭绝级别的事件。不，不是COVID19。我说的是几乎每个数据科学课程中都教授的流行的KNN算法的消亡！请继续阅读，了解在每一位数据科学家的工具包中，是什么取代了这一主食。</p><h1 id="1954" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">KNN背景</h1><p id="656a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在机器学习社区中，找到“K”个与任何给定项目相似的项目被广泛称为“相似性”搜索或“最近邻”(NN)搜索。最广为人知的神经网络搜索算法是K-最近邻(KNN)算法。在KNN，给定一组对象，如手机的电子商务目录，我们可以从整个目录中为任何新的搜索查询找到少量(K个)最近邻。例如，在下面的示例中，如果您设置K = 3，那么每个“iPhone”的3个最近邻居就是另一个“iPhone”同样，每个“Samsung”最近的3个邻居都是Samsung。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/eee0af0c8ce0b11f5bc1039e99affc00.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*vP78oIQ6M5hY2r_85_6bMA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">6款手机目录</figcaption></figure><h2 id="a256" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">与KNN的问题</h2><p id="ec47" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">虽然KNN很擅长寻找相似的物品，但它使用详尽的成对距离计算来寻找邻居。如果您的数据包含1000个商品，那么为了找到一个新产品的K=3个最近邻，该算法需要执行该新产品到数据库中所有其他产品的1000次距离计算。嗯，还不算太糟。但是想象一下现实世界中的客户对客户(C2C)市场，数据库中有数百万件产品，每天可能有数千件新产品上传。将每一款新产品与数百万款产品进行比较是浪费时间的，也就是根本不可扩展的<strong class="lk jd"><em class="nn"/></strong>。</p><h2 id="2a28" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">解决方案</h2><p id="09b0" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">将最近邻扩展到大数据量的解决方案是完全避开强力距离计算，而是使用一种更复杂的算法，称为近似最近邻(ANN)。</p><h1 id="64d1" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">近似最近邻(ANN)</h1><p id="0a07" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">严格地说，人工神经网络是一类算法，其中在人工神经网络搜索过程中允许出现少量错误。但在现实世界的C2C市场中,“真实”邻居的数量高于被搜索的“K”个最近邻居，人工神经网络可以在很短的时间内达到与强力KNN相当的显著准确性。有几种人工神经网络算法，例如</p><ol class=""><li id="8086" class="no np it lk b ll lm lo lp lr nq lv nr lz ns md nt nu nv nw bi translated">Spotify的[ <a class="ae lh" href="https://github.com/spotify/annoy" rel="noopener ugc nofollow" target="_blank">惹恼了</a></li><li id="9fb6" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">谷歌的[ <a class="ae lh" href="https://github.com/google-research/google-research/tree/master/scann" rel="noopener ugc nofollow" target="_blank"> ScaNN </a></li><li id="c0dd" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">脸书的[<a class="ae lh" href="https://github.com/facebookresearch/faiss" rel="noopener ugc nofollow" target="_blank">失败</a></li><li id="68e3" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">我个人最喜欢的是:层次导航小世界图[ <a class="ae lh" href="https://github.com/nmslib/hnswlib" rel="noopener ugc nofollow" target="_blank"> HNSW </a></li></ol><p id="4714" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章的其余部分将Python的<code class="fe oc od oe of b">sklearn</code>中实现的KNN算法与Python的<code class="fe oc od oe of b">hnswlib</code>包中实现的称为分层可导航小世界(HNSW)图的优秀人工神经网络算法进行基准测试。我将使用一个包含527000件“手机&amp;配件”产品的大型[ <a class="ae lh" href="http://deepyeti.ucsd.edu/jianmo/amazon/" rel="noopener ugc nofollow" target="_blank">亚马逊产品数据集</a> ]来证明HNSW在速度上要优越得多(精确地说快380倍)，同时提供99.3%类似于sklearn的KNN的结果。</p><h2 id="8848" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">分层可导航小世界</h2><p id="0820" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在HNSW<a class="ae lh" href="https://arxiv.org/abs/1603.09320" rel="noopener ugc nofollow" target="_blank">【paper @ arxiv】</a>中，作者描述了一种使用多层图的ANN算法。在元素插入期间，通过随机选择具有指数衰减概率分布的每个元素的最大层，逐步构建HNSW图。这确保了层=0具有许多元素来实现精细搜索，而层=2具有e^-2较低数量的元素来促进粗略搜索。最近邻搜索从最顶层开始进行粗略搜索，并向下进行，直到最底层，使用贪婪图路由遍历图并找到所需数量的邻居。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/377dc13303c526b66ee8476086621093.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*muRRpELptzPmSOdQreR3zg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">HNSW图形结构。最近邻搜索从最顶层(粗略搜索)开始，到最底层(精细搜索)结束。<a class="ae lh" href="https://arxiv.org/abs/1603.09320" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="c519" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">HNSW Python包</h2><p id="fcfd" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">整个HNSW算法都是用C++写的，带有Python绑定，可以通过输入<code class="fe oc od oe of b">pip install hnswlib</code>安装在你的机器上。一旦你安装了这个包并导入了它，创建HNSW图需要几个步骤，我已经把它们打包成了下面的一个方便函数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="7a81" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦创建了HNSW索引，查询“K”最近邻就像调用下面的一行代码一样简单。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="60d3" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">KNN与安基准测试实验</h1><h2 id="ba45" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">计划</h2><p id="156b" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们将首先下载一个包含500K+行的大型数据集。然后，我们将通过使用预先训练的<code class="fe oc od oe of b">fasttext</code>句子向量，将文本列转换为<code class="fe oc od oe of b">300d </code>嵌入向量。然后，我将在不同长度的输入数据<code class="fe oc od oe of b">[1000, 10000, 100000, len(data)]</code>上训练KNN和HNSW ANN模型，以测量数据大小对速度的影响。最后，我将从两个模型中查询K = 10和100个最近邻，以测量K对速度的影响。首先，让我们导入必要的包和模型。这将需要一些时间，因为<code class="fe oc od oe of b">fasttext </code>模型需要从互联网上下载。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="14e2" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">数据</h2><p id="cf3a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们将使用[ <a class="ae lh" href="http://deepyeti.ucsd.edu/jianmo/amazon/" rel="noopener ugc nofollow" target="_blank">亚马逊产品数据集</a> ]，其中包含“手机&amp;配件”类别中的527000件产品。从链接下载数据集，并运行以下代码将其转换为数据框。我们只需要产品<code class="fe oc od oe of b">title</code>列，因为我们将使用它来搜索类似的产品。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="234d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果一切运行正常，您应该会看到如下输出</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/335f64bf306e06ccf0d3fcffce18ce80.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*Jq0B1lIYGzSfL-DD4mvRxw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">亚马逊产品数据集</figcaption></figure><h2 id="b119" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">把...嵌入</h2><p id="294d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">要在文本数据上运行任何相似性搜索，我们必须首先将它转换成一个数字向量。一种快速便捷的方法是使用预先训练好的网络嵌入层，比如脸书的[ <a class="ae lh" href="https://github.com/facebookresearch/fastText" rel="noopener ugc nofollow" target="_blank"> FastText </a> ]提供的嵌入层。因为我们希望所有的行都有相同的长度向量，不管<code class="fe oc od oe of b">title</code>中的字数是多少，我们将对df中的<code class="fe oc od oe of b">title</code>列应用<code class="fe oc od oe of b">get_sentence_vector</code>方法。一旦嵌入完成，我们提取<code class="fe oc od oe of b">emb</code>列作为输入到我们的神经网络算法的列表列表。理想情况下，您应该在此步骤之前运行一些文本清理预处理。此外，使用微调嵌入模型通常是一个好主意。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="8ef6" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">标杆管理</h2><p id="5be8" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">现在我们有了算法的输入，让我们运行基准测试。我们将在搜索空间中的产品数量和正在搜索的<code class="fe oc od oe of b">K</code>最近邻居的循环中运行测试。</p><p id="12f1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在每次迭代中，除了记录每个算法花费的时间之外，我们还检查<code class="fe oc od oe of b">pct_overlap</code>作为也被人工神经网络选为最近邻的KNN最近邻的数量的比率。</p><p id="bcf2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">小心！整个测试在一台24x7运行的8核30 GB RAM机器上运行了大约6天，因此这可能需要一些时间。理想情况下，您可以通过多重处理来加快速度，因为每次运行都是相互独立的。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="483e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">运行结束时的输出如下所示。正如你已经从表中看到的，西南大学完全击败了KNN！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/6af04303581a565258cf9e662a9cbe5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*pir9aIEp9SmPGqCtuX7Oug.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">表格形式的结果</figcaption></figure><h2 id="0237" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">结果</h2><p id="d9ba" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">让我们以图表的形式来看一下基准测试结果，以便真正了解差异的大小。我将使用标准的<code class="fe oc od oe of b">matplotlib</code>代码来绘制这些图表。两个轴都在<code class="fe oc od oe of b">log</code>刻度内。差别是惊人的！HNSW ANN在查询K=10和100个最近邻居所需的时间方面胜过KNN。当搜索空间包含约500K个产品时，在ANN上搜索100个最近邻要快380倍。同时，KNN和安都找到99.3%相同的最近邻。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f874c919a62a346f1ef721c04682d4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Co53wWu5zBkAtNMqeTWy7g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">当搜索空间包含约500K个元素时，HNSW ANN比Sklearn的KNN快380倍，并且为搜索空间中的每个元素找到K=100个最近邻。<a class="ae lh" href="https://github.com/stephenleo/adventures-with-ann/blob/main/knn_is_dead.ipynb" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/d210d49fefc8bba2a9814890ad8ae8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*ZJ20chPIA1UU85_g-hfUnQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">当搜索空间包含约500K个元素时，HNSW ANN产生99.3%的与Sklearn的KNN相同的最近邻，并且在搜索空间中为每个元素找到K=100个最近邻。<a class="ae lh" href="https://github.com/stephenleo/adventures-with-ann/blob/main/knn_is_dead.ipynb" rel="noopener ugc nofollow" target="_blank">信号源</a></figcaption></figure><p id="820b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有了这些结果，我想可以有把握地说，“KNN死了！”再也没有合理的理由使用<code class="fe oc od oe of b">sklearn's</code> KNN了。我希望这篇文章对你有用。这篇文章的所有代码都可以在我的<a class="ae lh" href="https://github.com/stephenleo/adventures-with-ann/blob/main/knn_is_dead.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到。感谢您的阅读！</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><p id="b492" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下一步是什么？如果你想知道哪种人工神经网络算法在你自己的数据集上是最好的，请查看我在这个系列中的下一篇文章:</p><div class="ot ou gp gr ov ow"><a href="https://medium.com/towards-artificial-intelligence/how-to-choose-the-best-nearest-neighbors-algorithm-8d75d42b16ab" rel="noopener follow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd jd gy z fp pb fr fs pc fu fw jc bi translated">如何选择最佳最近邻算法</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">一种数据驱动的方法，用于在您的自定义数据集上选择最快、最准确的人工神经网络算法</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">medium.com</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk lb ow"/></div></div></a></div><p id="a062" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想在大数据上部署人工神经网络，你可以在Docker容器中使用Elasticsearch</p><div class="ot ou gp gr ov ow"><a href="https://medium.com/towards-artificial-intelligence/approximate-nearest-neighbors-on-elastic-search-with-docker-15342153f22a" rel="noopener follow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd jd gy z fp pb fr fs pc fu fw jc bi translated">基于Docker弹性搜索的近似最近邻</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">将人工神经网络扩展到上亿</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">medium.com</p></div></div><div class="pf l"><div class="pl l ph pi pj pf pk lb ow"/></div></div></a></div><p id="3074" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">编辑1:一个代码错误已被修复，结果有相应的变化，但结论没有任何变化。图已更新到更高的质量和log-Y轴。<br/>编辑2:更新了github gists的代码片段</p></div></div>    
</body>
</html>