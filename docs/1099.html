<html>
<head>
<title>From Impossible to Irreplaceable: BERT in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从不可能到不可替代:自然语言处理中的伯特</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/from-impossible-to-irreplaceable-bert-in-nlp-d0dd39dfef9f?source=collection_archive---------3-----------------------#2020-10-29">https://pub.towardsai.net/from-impossible-to-irreplaceable-bert-in-nlp-d0dd39dfef9f?source=collection_archive---------3-----------------------#2020-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a0fd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><p id="1c61" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">伯特是来自变压器的T2、T4、编码器的代表</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi kx"><img src="../Images/80891eb2c43d9f83890d614fb9c59f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_H-ihLvBB2D6EcGIEpMoA.jpeg"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图片来自著名的电视连续剧《新民主主义者联盟》</figcaption></figure><h1 id="9216" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">介绍</h1><p id="5e06" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated"><strong class="kb jd">NLP中的难点:</strong>在NLP领域中，为了训练一个ML模型，我们需要一个与所研究问题的上下文相关的适当数据集。但是通常很难获得这种特定领域的数据，甚至发现执行标注是一项相当繁重的任务。</p><p id="1718" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">解决方案:</strong>为了解决这种情况，研究人员创建了一个通用模型，该模型在互联网上找到的大量未标注的原始文本上进行训练，其中包括广阔领域的上下文。</p><p id="6e42" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">BERT就是这样一种解决方案，它可以针对任何NLP相关的上下文预测进行微调。</p><h1 id="c88f" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">伯特开始发挥作用了。</h1><p id="b738" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">有多种算法试图找到NLP问题的解决方案。那么，伯特适合在哪里，它比其他人好在哪里？</p><p id="a125" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">让我们更进一步去发现</p><blockquote class="mq"><p id="8a04" class="mr ms it bd mt mu mv mw mx my mz kw dk translated">BERT是一个预先训练好的双向模型，它是根据来自网络的大量数据——原始论文——进行训练的</p></blockquote><p id="5b27" class="pw-post-body-paragraph jz ka it kb b kc na ke kf kg nb ki kj kk nc km kn ko nd kq kr ks ne ku kv kw im bi translated">传统上，NLP领域是从像word2vec这样的上下文无关模型发展而来的，word 2 vec将每个单词转换为相应的数字表示，因为机器学习模型只能接受一组数字作为输入。使用这种方法，一个单词独立于句子中的其他单词，完全忽略了潜在的含义，导致准确性降低。</p><p id="da8e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">这就是伯特出手相救的地方。BERT是一个基于上下文的双向模型，它从左到右以及从右到左理解单词的当前含义。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/0b0f051598273d5ea1f7a064e3fc110d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*LuMNB3WLNzw2xM-Qtk2hHw.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图片来自原纸— <a class="ae ng" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><p id="3294" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在这里，来自每个单词的信息传递到其他每个单词，使它成为双向的，并利用单词前后的上下文。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="17cd" class="ln lo it bd lp lq no ls lt lu np lw lx ly nq ma mb mc nr me mf mg ns mi mj mk bi translated">伯特是什么做的？</h1><h2 id="aa4e" class="nt lo it bd lp nu nv dn lt nw nx dp lx kk ny nz mb ko oa ob mf ks oc od mj iz bi translated">构建模块—变压器</h2><p id="c0bd" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">BERT属于基于变压器的模型。变压器<strong class="kb jd">模型</strong>使用<strong class="kb jd"> <em class="oe">自关注机制</em> </strong>进行学习，该自关注机制具有恒定的步数(此时聚光灯下要学习的单词或字符的数量)，并且它直接对序列中所有单词的关系进行建模，而不考虑其位置。与<em class="oe">传统的RNN、</em>模型顺序学习上下文相比，这种从所有单词并行学习的类型是一个巨大的优势。</p><p id="046f" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">变形金刚也利用<strong class="kb jd">注意机制</strong>。在这里，转换器将每个单词与其他每个单词进行比较，并生成一个注意力分数。这个<strong class="kb jd"> <em class="oe">注意力得分</em> </strong>表示一个单词对于预测下一个单词/字符有多重要。然后，这个注意力得分被用作所有单词的加权平均值，并被输入到完全连接的网络中。</p><p id="15de" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">本质上，转换器具有读取输入并生成上述注意力分数的编码器和解码器，解码器使用来自编码器的结果来逐字预测。</p><p id="8462" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">变压器工作:</strong></p><ul class=""><li id="10d7" class="of og it kb b kc kd kg kh kk oh ko oi ks oj kw ok ol om on bi translated">转换器为每个单词创建初始嵌入/表示(将字符串转换为数字)。</li><li id="cf24" class="of og it kb b kc oo kg op kk oq ko or ks os kw ok ol om on bi translated">使用自我关注，它从所有单词中获得信息，并为每个单词创建新的嵌入，它具有所有的上下文。</li><li id="2f09" class="of og it kb b kc oo kg op kk oq ko or ks os kw ok ol om on bi translated">上述步骤并行执行多次，并创建一个新的表示。</li><li id="87a5" class="of og it kb b kc oo kg op kk oq ko or ks os kw ok ol om on bi translated">解码器一次创建一个单词。</li><li id="56af" class="of og it kb b kc oo kg op kk oq ko or ks os kw ok ol om on bi translated">解码器还试图基于来自编码器输出的信息以及先前预测的字来创建新的嵌入。</li></ul><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ot"><img src="../Images/08f20686b650f3f08e141cd20a28cab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61qJlCwBYJgOmilalTADmQ.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">《变形金刚》中的编码器和解码器(图片由作者提供)</figcaption></figure><p id="fa00" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">因为BERT的目标是只生成语言模型，所以它只利用了Transformers中的编码器概念。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="f243" class="ln lo it bd lp lq no ls lt lu np lw lx ly nq ma mb mc nr me mf mg ns mi mj mk bi translated">伯特是如何工作的？</h1><p id="8d5e" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">许多最初的语言模型都是单向的。单向模型的一个主要缺点是，如果一个句子太长而记不住，它往往会忘记上下文。双向概念似乎是解决上述问题的一个可行方案。但是，既然单向失败了，双向是很自然的，为什么以前没有实现呢？</p><h2 id="0db2" class="nt lo it bd lp nu nv dn lt nw nx dp lx kk ny nz mb ko oa ob mf ks oc od mj iz bi translated">双向学习:</h2><p id="2e35" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">在一个标准模型中，下一个单词的预测是通过用当前单词进行调节来进行的。所以既可以从左到右训练，也可以从右到左训练。</p><p id="1af1" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在双向的情况下，一个字必须以前一个字和下一个字为条件。这可能会导致“看到自己”的现象，在预测之前，模型实际上可能会看到一个单词(要预测的)。这真的没有帮助，因为有严重的<a class="ae ng" href="https://en.wikipedia.org/wiki/Bias–variance_tradeoff" rel="noopener ugc nofollow" target="_blank">偏差</a>。</p><p id="81f1" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd"> <em class="oe">例如:</em> </strong> <em class="oe">我喜欢凉爽、秋高气爽的天气</em></p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ou"><img src="../Images/8ae1b1e2041165c7f8e0ffc8b9d3b674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vOcSYK84UViAfABtOPZ0fg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">顺序词的双向条件作用示例(图片由作者提供)</figcaption></figure><p id="80ba" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">例如，在我们的单词预测引擎中，我们假设<em class="oe"> fall </em>是正在研究的单词。但是如果是条件双向的，单词<em class="oe"> fall </em>肯定已经可以用于前一个单词<em class="oe"> crisp </em>。这使得模型直接预测单词，因此什么也没学到。</p><h2 id="5ef8" class="nt lo it bd lp nu nv dn lt nw nx dp lx kk ny nz mb ko oa ob mf ks oc od mj iz bi translated">伯特的策略是:</h2><p id="34e1" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">为了防止上述情况，BERT通过屏蔽(隐藏)一些随机单词(记号)来利用双向技术，并且该模型试图预测那些被屏蔽的单词。这种方法叫做<strong class="kb jd">掩蔽学习模式，</strong>也就是俗称的完形填空任务。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="12e8" class="ln lo it bd lp lq no ls lt lu np lw lx ly nq ma mb mc nr me mf mg ns mi mj mk bi translated">建筑深潜</h1><h2 id="081b" class="nt lo it bd lp nu nv dn lt nw nx dp lx kk ny nz mb ko oa ob mf ks oc od mj iz bi translated">预培训架构类型概述</h2><p id="30a5" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">增加BERT受欢迎程度的重要一点是，它是一个预先训练好的模型，可以直接用于解决上下文相关或非上下文相关的问题。基本上，有两种类型的预训练架构:</p><ol class=""><li id="351a" class="of og it kb b kc kd kg kh kk oh ko oi ks oj kw ov ol om on bi translated"><em class="oe">基于特征:</em>这里，预先训练的表示(嵌入)只是作为附加特征使用。因此，基础架构将是特定于任务的，并且计算要求很高。</li><li id="e71e" class="of og it kb b kc oo kg op kk oq ko or ks os kw ov ol om on bi translated"><em class="oe">微调:</em>这里不介绍特定于任务的架构。直接使用预先训练的表示，但是必须基于任务替角进行微调(用标签训练)。</li></ol><p id="32bc" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">BERT属于<em class="oe">微调</em>方法，它使用更少的架构，导致更少的资源需求</p><h2 id="571c" class="nt lo it bd lp nu nv dn lt nw nx dp lx kk ny nz mb ko oa ob mf ks oc od mj iz bi translated">伯特框架</h2><p id="9bd0" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">对于任何任务，有两个步骤来实现BERT。</p><ul class=""><li id="412b" class="of og it kb b kc kd kg kh kk oh ko oi ks oj kw ok ol om on bi translated">预训练阶段:这是用大量<strong class="kb jd">未标记的</strong>数据训练模型(注意力模型)的阶段</li><li id="e154" class="of og it kb b kc oo kg op kk oq ko or ks os kw ok ol om on bi translated">微调阶段:这里用预先训练的参数初始化BERT模型，但是通过使用来自所研究的特定问题的标记为的<strong class="kb jd">数据对它们进行微调。</strong></li></ul><h2 id="882e" class="nt lo it bd lp nu nv dn lt nw nx dp lx kk ny nz mb ko oa ob mf ks oc od mj iz bi translated">培训前阶段:</h2><p id="49f7" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">对于任何要执行的模型，都需要有一个预测目标。通常的模型倾向于以预测下一个单词为目标，但是在双向方法中，目标是用两种策略设定的。两种策略都在无监督方法上训练数据。</p><p id="2981" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">掩蔽学习模型:</strong></p><p id="49ff" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在这种技术中，随机输入标记被屏蔽(标记被替换为<strong class="kb jd">【MASK】</strong>)，模型被训练来预测这些具有周围单词的屏蔽标记。</p><p id="f718" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在模型被训练之后，来自转换器的屏蔽令牌的最终隐藏向量与单词嵌入相乘，以获得logitss(模型的非归一化预测)，该logit随后被发送到softmax层，以获得归一化的概率分布。</p><p id="af6c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">由于在模型的实时实现中(<em class="oe">微调阶段</em> ) <strong class="kb jd">【掩码】</strong>)参数基本上不会出现在输入中(由于所研究的问题可能不同)，BERT通过考虑以下因素来处理训练:</p><p id="b843" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">训练生成器屏蔽随机选择的标记位置的15%,其中:</p><ul class=""><li id="59f9" class="of og it kb b kc kd kg kh kk oh ko oi ks oj kw ok ol om on bi translated"><strong class="kb jd"> 80% </strong>的时候:用【面具】代替了这个词</li><li id="0f80" class="of og it kb b kc oo kg op kk oq ko or ks os kw ok ol om on bi translated"><strong class="kb jd"> 10% </strong>的时间:用一个随机单词替换这个单词</li><li id="c3bc" class="of og it kb b kc oo kg op kk oq ko or ks os kw ok ol om on bi translated"><strong class="kb jd"> 10% </strong>的时间:保持字不变。这是为了使最终的表现偏向实际的语言观察</li></ul><h2 id="93fc" class="nt lo it bd lp nu nv dn lt nw nx dp lx kk ny nz mb ko oa ob mf ks oc od mj iz bi translated">下一句预测</h2><p id="5c5e" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">这种技术有助于理解句子关系，因为许多语言模型不能直接捕捉这种技术。句子关系无非是给定的句子是否是前一句的后继。</p><p id="c52d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">下一个句子预测任务只不过是二进制分类问题，其中50%的时间，对于两个句子A和B: B是实际的后继者或A(标记为IsNext)，另50%的时间B是随机选择的(标记为NotNext)。</p><h2 id="a4b2" class="nt lo it bd lp nu nv dn lt nw nx dp lx kk ny nz mb ko oa ob mf ks oc od mj iz bi translated">伯特嵌入</h2><p id="d21e" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">嵌入是一种用于将给定的原始文本转换成有意义的格式(通常是数字形式的文字表示)的技术，模型能够理解这种格式，同时能够基于这种格式提供准确的预测。</p><p id="40bc" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">BERT有3级嵌入，如下图所示</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ow"><img src="../Images/43efc31a0c6592b9e3e01ebb00799e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tJf3ZjAx6rrd04SCSnwP_w.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">BERT输入嵌入(来自原始文件的图像— <a class="ae ng" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><p id="1fce" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">输入:</strong><em class="oe"/>指定分类标签，提到给定的句子是否遵循前一句的上下文。SEP 标记插在每个句子的末尾</p><p id="3156" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd"> Token嵌入:</strong>NLP中的任何一个句子都被分割成单词称为Token。然后，每个标记被转换成向量表示。这是将给定字符串转换成数字的部分，以便模型可以更好地理解它。创建的矢量表示具有固定的维数。</p><p id="595a" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">片段嵌入:</strong>这个嵌入描述了记号属于哪个句子。例如，如果有两个由<em class="oe">【SEP】</em>标记分隔的句子，那么一个学习嵌入<em class="oe"> Ea </em>被连接到属于A的句子的标记，并且<em class="oe"> Eb </em>被连接到属于b的句子的标记。因此有两个可能的片段嵌入</p><p id="0975" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">位置嵌入:</strong>该嵌入是序列中每个位置的学习向量表示。由于变压器不像RNN那样具有固有的时序性，因此应该可以获得一些时序方面的信息。因此，句子中的每个标记都有自己经过训练的嵌入，因此模型了解它的位置。</p><h2 id="61fd" class="nt lo it bd lp nu nv dn lt nw nx dp lx kk ny nz mb ko oa ob mf ks oc od mj iz bi translated">微调阶段:</h2><p id="feac" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">微调BERT无非是使用预先训练好的架构来解决给定的下游问题。对于每个任务，特定于任务的输入和输出被插入，因此它学习带有适当标签的结果。</p><p id="9f59" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在预训练中类似于句子A &amp; B的输入可以是1)问题回答中的问题段落对，2)释义中的句子对，3)文本分类中的退化文本对。在输出端，得到的表征表示被发送到输出层用于表征级预测任务，并且<em class="oe"> CLS </em>表示被馈送到输出层用于分类任务，如情感分析。</p><h1 id="51b6" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">外卖食品</h1><p id="4caa" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">当双向学习的想法第一次被提出时，它被高度认为是实际上不可能的。然而，在可行的策略下，BERT模型整体上是自然语言处理领域的突破性创新。</p><p id="c370" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">尽管这个模型已经由GoogleAI发布了2年，但它仍然是许多日常应用的支柱，包括<em class="oe">谷歌搜索、谷歌翻译、Gmail文本建议。</em></p><h1 id="5586" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">参考</h1><p id="6562" class="pw-post-body-paragraph jz ka it kb b kc ml ke kf kg mm ki kj kk mn km kn ko mo kq kr ks mp ku kv kw im bi translated">📙<a class="ae ng" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">伯特:用于语言理解的深度双向转换器的预训练</a></p><p id="7558" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">📙<a class="ae ng" href="http://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank"> Transformer:一种用于语言理解的新型神经网络架构</a></p></div></div>    
</body>
</html>