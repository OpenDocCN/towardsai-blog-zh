<html>
<head>
<title>I Fine-Tuned GPT-2 on 110K Scientific Papers. Here’s The Result</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我在11万篇科学论文上微调了GPT-2。这是结果</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/i-fine-tuned-gpt-2-on-110k-scientific-papers-heres-the-result-9933fe7c3c26?source=collection_archive---------0-----------------------#2022-11-10">https://pub.towardsai.net/i-fine-tuned-gpt-2-on-110k-scientific-papers-heres-the-result-9933fe7c3c26?source=collection_archive---------0-----------------------#2022-11-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2f3e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">AI写内容很常见，但是一个AI有可能写技术论文吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2e9e06ea19faad208bbf661c98efeb5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPxBzR1WO12lv9iM_zZghg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">用人工智能生成科学文本。图片由作者提供。</figcaption></figure><p id="59e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">人工智能</strong>如今被广泛使用，能够<strong class="la iu">在多项任务中实现超人的表现</strong>。<strong class="la iu">文本生成</strong>是人工智能的新兴应用之一，被用于<strong class="la iu">几种场景</strong>。自由形式的文本生成、Q &amp; A和抽象摘要只是其中的一部分。</p><p id="cb24" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了调查一个<strong class="la iu">人工智能是否可以写技术论文，</strong>我在大约<strong class="la iu"> 100K的机器学习论文</strong>上训练了一个随意语言模型。</p><p id="6fd1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">结果质量如何？提议的方法有什么局限性？有可能让GPT-2写一篇完整的论文吗？这些是我将尝试回答的问题。</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="2cb9" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">介绍</h2><p id="8252" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated"><strong class="la iu">生成式预训练变压器</strong> (GPT) 2是由<strong class="la iu"> OpenAI </strong>于2019年开发的人工智能，允许多种用途:<strong class="la iu">文本摘要</strong>、<strong class="la iu">翻译</strong>、<strong class="la iu">问答</strong>、<strong class="la iu">文本生成</strong>。GPT-2是在大型英语数据语料库上预先训练的<strong class="la iu">，此外，可以针对特定任务</strong>进行<strong class="la iu">微调。</strong></p><p id="0f05" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将使用<strong class="la iu">hugging face</strong><strong class="la iu">distilted-gp T2</strong>(distil GPT 2)模型。<strong class="la iu"> DistilGPT2 </strong>拥有<strong class="la iu">8200万个参数</strong>，是由<strong class="la iu">知识蒸馏</strong>、<strong class="la iu">、</strong>开发的<strong class="la iu">，而且<strong class="la iu">、</strong>比GPT-2的<strong class="la iu">轻</strong>、<strong class="la iu">快</strong>。</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="9730" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">1.导入工具</h2><p id="deef" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">我从导入所有需要的<strong class="la iu">工具</strong>和<strong class="la iu">库</strong>开始。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8197" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">2.导入基线模型和标记器</h2><p id="e8a9" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">然后，我使用<code class="fe nc nd ne nf b">TFAutoModelForCasualLM</code>和<code class="fe nc nd ne nf b">AutoTokenizer</code>来<strong class="la iu">自动</strong>加载基于特定<strong class="la iu">检查点</strong>的<strong class="la iu">正确模型</strong>。检查点包含预训练模型的<strong class="la iu">重量。</strong></p><p id="79bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本例中，我导入了<strong class="la iu"> DistilGPT-2检查点。我还将<strong class="la iu">序列结束标记</strong>设置为填充标记。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8698" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">3.导入数据</h2><p id="dfce" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">用于<strong class="la iu">微调</strong>操作的数据集在<strong class="la iu"> Huggingface Hub </strong>上可用，并且它是托管在<strong class="la iu"> Kaggle </strong>上的更大数据集的一个<strong class="la iu">子集。</strong></p><p id="22b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由<strong class="la iu">康奈尔大学</strong>发布的原始数据集包含<strong class="la iu"> 1.7M的<strong class="la iu">标题</strong>和<strong class="la iu">摘要</strong>以及属于<strong class="la iu"> STEM类别的</strong>科学论文。</strong>托管在Huggingface Hub上的子集包含关于属于<strong class="la iu">机器学习类别</strong>的大约<strong class="la iu"> 100K篇论文</strong>的信息。</p><p id="2dad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我决定<strong class="la iu">只在<strong class="la iu">摘要</strong>上微调</strong> DistilGPT-2。我从从Huggingface Hub加载数据集开始。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="b263" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据集由<strong class="la iu"> 117592行</strong>组成，有<strong class="la iu"> 4列</strong>(其中两列无用)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="d31c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">经过这一步，我决定用直方图可视化出摘要的<strong class="la iu">长度</strong> <strong class="la iu">分布</strong> <strong class="la iu">(以字为单位)。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/8d4041e2b959d45702d623bd96faeb80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CORK_wv3c-QxVPNiWEbWEg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">摘要长度分布。图片由作者提供。</figcaption></figure><p id="510a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大多数摘要的长度在100到250字之间，只有少数超过300字的摘要是T42的。具体来说:<strong class="la iu">众数</strong> =150，<strong class="la iu">均值</strong> =167，<strong class="la iu">中位数</strong> =164。</p><p id="cc2d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了给出关于数据集的信息之外，直方图还允许我<strong class="la iu">确定输入到模型的输入</strong>的最大长度。</p><p id="85d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我决定将<strong class="la iu">最大输入长度</strong>设置为<strong class="la iu"> 300个令牌</strong>:超过这个<strong class="la iu">的摘要将被截断</strong>。这是因为所有的输入都必须用<strong class="la iu">填充到相同的长度</strong>，长的文本序列<strong class="la iu">会大大增加</strong> <strong class="la iu">的训练时间</strong>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="085a" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">4.分为训练集和验证集</h2><p id="a631" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">接下来，我<strong class="la iu">将数据集</strong>分割成<strong class="la iu">训练</strong>和<strong class="la iu">验证</strong> <strong class="la iu">集合</strong>和<code class="fe nc nd ne nf b">train_test_split()</code>。也可以用参数<code class="fe nc nd ne nf b">test_size</code>指定分区的大小。</p><p id="f609" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nc nd ne nf b">train_test_split()</code>返回一个<code class="fe nc nd ne nf b">Datasets</code>的字典，以前是一个<code class="fe nc nd ne nf b">DatasetDict</code>。虽然可以用一个<code class="fe nc nd ne nf b">DatasetDict</code>工作，但我更喜欢使用两个独立的<code class="fe nc nd ne nf b">Datasets</code> : <code class="fe nc nd ne nf b">train</code>和<code class="fe nc nd ne nf b">val</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8cbf" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">5.用高频标记器标记数据</h2><p id="c9b0" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">为了对数据进行标记化，我定义了一个<strong class="la iu">通用标记化函数</strong>，然后我<strong class="la iu">通过使用<code class="fe nc nd ne nf b">map()</code>将这个函数应用于所有的样本</strong>。在标记化函数内部，我使用了一开始导入的<strong class="la iu">标记化器</strong>。</p><p id="b884" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">记号赋予器有一些<strong class="la iu">重要参数需要设置</strong>:</p><ol class=""><li id="26d3" class="nh ni it la b lb lc le lf lh nj ll nk lp nl lt nm nn no np bi translated"><strong class="la iu">列对<em class="lu">列进行标记化。</em></strong>在这种情况下“抽象”。</li><li id="7893" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt nm nn no np bi translated"><strong class="la iu"> <em class="lu">填充</em> </strong>。在这种情况下，<strong class="la iu">的= " max _ lenght "填充一个序列</strong>到由max_length参数指定的<strong class="la iu">最大长度</strong>。</li><li id="fe88" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt nm nn no np bi translated"><strong class="la iu"> <em class="lu">截断</em> </strong>。如果为真，<strong class="la iu">截断长度超过最大长度</strong>的序列<strong class="la iu">，最大长度由max_length参数指定。</strong></li><li id="7e65" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt nm nn no np bi translated"><strong class="la iu"> <em class="lu">最大_长度</em> </strong>。指定序列的<strong class="la iu">最大长度</strong>。</li></ol><p id="f77c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，默认情况下，<code class="fe nc nd ne nf b">map()</code>方法发送1000份样本。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8f7f" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">6.向训练集和验证集添加标签</h2><p id="232b" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">在随意语言建模中，标签是<strong class="la iu">输入标记</strong> (input_ids) <strong class="la iu">右移</strong>。这个操作<strong class="la iu">是由Huggingface transformer自动完成的，</strong>因此<strong class="la iu"> </strong> I <strong class="la iu"> </strong>用令牌 (input_ids)的<strong class="la iu">副本在数据集中创建了一个<em class="lu">标签</em>列。</strong></p><p id="f63f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在此操作之后，训练和验证集有<strong class="la iu">三列</strong>:来自<strong class="la iu">标记化</strong>过程的<code class="fe nc nd ne nf b">input_ids</code>和<code class="fe nc nd ne nf b">attention_mask</code>，以及来自<code class="fe nc nd ne nf b">create_labels()</code>过程的<code class="fe nc nd ne nf b">labels</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="2e1f" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">7.将训练集和验证集转换为TF数据集</h2><p id="a046" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">接下来，我<strong class="la iu">将数据集</strong>转换为<code class="fe nc nd ne nf b">tf.data.Dataset</code>，这样<strong class="la iu"> Keras就可以自然理解</strong>；为此，我使用了<code class="fe nc nd ne nf b">Model.prepare_tf_dataset()</code>。</p><p id="8a2b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于<code class="fe nc nd ne nf b">Dataset.to_tf_dataset()</code>方法，<code class="fe nc nd ne nf b">Model.prepare_tf_dataset()</code>可以<strong class="la iu">自动</strong>确定<strong class="la iu">使用哪些列名作为输入</strong>，<strong class="la iu"> </strong>提供了一个<strong class="la iu">默认数据整理器。</strong></p><p id="2005" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，我只对火车数据进行了<strong class="la iu">混洗。经过一番实验，我发现<strong class="la iu">最优批量= 16 </strong>。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="82cd" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">8.编译、拟合和评估模型</h2><p id="536e" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">在拟合模型之前，我设置了一个<strong class="la iu">学习率调度器</strong>和<strong class="la iu">优化器</strong>。我使用了Keras<strong class="la iu">的<code class="fe nc nd ne nf b">ExponentialDecay</code>调度器和Huggingface </strong>的<code class="fe nc nd ne nf b">AdamWeightDecay</code>优化器。</p><p id="f4fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">学习率衰减</strong>是一种技术<strong class="la iu">随着时间的推移降低学习率</strong>。随着<strong class="la iu">指数衰减</strong>，学习率以<strong class="la iu">指数方式降低</strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="627b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来我<strong class="la iu">编译了模型</strong>。变压器模型通常<strong class="la iu">在内部计算损耗，</strong>并且不需要指定损耗参数。对于<strong class="la iu">语言建模，</strong>选择的损失是<strong class="la iu">交叉熵</strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="6da1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此时，我设置了一个对Huggingface Hub 的<strong class="la iu">回调，以<strong class="la iu">保存微调后的模型</strong>。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="ae79" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我还设置了一个对Tensorboard 的<strong class="la iu">回调。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="914b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我<strong class="la iu">通过调用<code class="fe nc nd ne nf b">fit()</code>方法来拟合模型</strong>。我指定了<strong class="la iu">序列</strong>和<strong class="la iu">验证</strong>集合和<strong class="la iu">周期数</strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="3785" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在训练步骤之后，<strong class="la iu"> </strong> I <strong class="la iu">对模型</strong>进行评估，并在验证集上得到其<strong class="la iu">交叉熵损失</strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="c90a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">损失=2.2371 </strong>。一般来说，语言模型的<strong class="la iu">质量是用“<strong class="la iu">困惑度</strong>来衡量的。为了将交叉熵转换为困惑，我简单地将<strong class="la iu">提高到交叉熵</strong>损失的幂。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="b736" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种情况下，<strong class="la iu">困惑=9.37 </strong>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="4d0d" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">9.使用管道生成文本</h2><p id="1572" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">在这一点上，我利用Huggingface提供的<code class="fe nc nd ne nf b">pipeline</code>功能来<strong class="la iu">查看运行中的模型</strong>。</p><p id="7c8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我建立了一个<strong class="la iu">文本生成管道，</strong>指定了<strong class="la iu">微调过的</strong> <strong class="la iu">模型、</strong><strong class="la iu">分词器、</strong>和<strong class="la iu">框架</strong>来使用<strong class="la iu">。</strong> <code class="fe nc nd ne nf b">max_new_tokens</code>除了提供初始提示外，允许指定生成的<strong class="la iu">最大令牌数</strong>(字数)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="4ccd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">两行代码足以让<strong class="la iu">用管道</strong>生成文本:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="6ad7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nc nd ne nf b">pipeline</code>不是<strong class="la iu">使用模型的唯一方式:可以用<strong class="la iu">手动标记提示</strong>，<strong class="la iu">生成新的标记</strong>，<strong class="la iu">将标记</strong>解码为自然语言。这里有一个例子:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="a5f4" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">12.结果分析</h2><p id="7506" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">在对模型进行微调后，我想了解<strong class="la iu">模型学到了什么</strong>以及<strong class="la iu">生成的文本如何受到<strong class="la iu">论文摘要用于训练</strong>这一事实的影响</strong>。</p><p id="796e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我使用<em class="lu">“推荐系统的角色”</em>作为提示生成了一个样本文本。这是模型生成的<strong class="la iu">输出:</strong></p><pre class="kj kk kl km gt nv nf nw nx aw ny bi"><span id="0e21" class="mc md it nf b gy nz oa l ob oc">'the role of recommender systems in the real-world is still largely to be demonstrated by the lack of data and the need for data. Hence, for many recommendation systems such as Amazon or Spotify, it is necessary to provide a user knowledge of the content that has been clicked during the recommendation and provide a user knowledge of the user preferences. The previous works attempt to exploit data related to items they have clicked during an appropriate time frame. But little attention has been paid to the problem of item classification where a suitable time-frame is available for user prediction. In this paper, we propose a multi-task learning approach to address the problem of item classification. For each task, we apply the contextual cues introduced by the user, and then learn to predict the user's purchased items' interests.   Since the contexts of user preferences, we consider the feature that the user's preference (the time-frame) is present at the time of recommendation. In particular, we propose an alternative method for attribute-aware learning that utilizes the contextual cues in the sequence and the user's preferences to learn a classifier that classifies the user according to the contextual cues. This is done by maximizing the mutual information between the user's rating and the content-aware prediction task. The experimental results show that our model achieves better accuracy than the existing state-of-the-art methods, achieving up to 33.6% more accuracy on real-world recommendation tasks compared to the state-of-the-art methods. Our source code is available at <a class="ae od" href="http://github.com/J-medylerFashion/jmedian.github.'" rel="noopener ugc nofollow" target="_blank">http://github.com/J-medylerFashion/jmedian.github.'</a></span></pre><p id="238e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个结果听起来似乎是从一个现有的摘要中复制粘贴的，但是在用一些<strong class="la iu">反抄袭</strong>解决方案检查后，我意识到它是<strong class="la iu"> 100%独特的</strong>。</p><p id="a6f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在学习过程中，模型<strong class="la iu">捕捉到摘要</strong>的共同特征，并学习<strong class="la iu">如何复制它们</strong>，同时仍然<strong class="la iu">生成新的文本</strong>。有趣的是，该模型使用了<strong class="la iu">科学语言</strong>和<strong class="la iu">常用表达方式</strong> : <em class="lu">前人著作… </em>，<em class="lu">本文… </em>，<em class="lu">我们提出… </em>实验结果。</p><p id="40d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该模型还了解到，有时一个<strong class="la iu">存储库被添加到抽象中:</strong>在这个例子中，生成的文本包含一个到GitHub存储库的<strong class="la iu"> URL。URL和存储库不存在，因此<strong class="la iu">已经由模型</strong>生成(并且没有被复制)。</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="1021" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作为第二个实验，我通过使用“<em class="lu">集群</em>”作为提示生成了一个样本文本。在这种情况下，提示只由<strong class="la iu">一个单词</strong>组成，因此文本生成不是由<strong class="la iu">附加上下文</strong>驱动的。这是<strong class="la iu">输出:</strong></p><pre class="kj kk kl km gt nv nf nw nx aw ny bi"><span id="8b04" class="mc md it nf b gy nz oa l ob oc">'clustering can be used to extract clusters from data points. However, in many real-world scenarios, data points often appear in non-Euclidean relaxations, which allows different clusters to be discovered simultaneously without the need for specific optimization. In this paper, we consider a class of applications where clustering methods are applied. A common example is that in image selection problems, we show to the optimizer that the expected improvement will be obtained by minimizing the average performance of the clustering method. Our main contribution is a characterization of optimization problems with respect to clustering methods. Specifically, we present a generalization of the optimal transport method for which our special bounds in terms of the maximum number of clusters are derived. We apply this algorithm to classification of human brain tumors, showing that it is computationally efficient. We show that based on our analysis of a particular type of tumor, our analysis leads to faster convergence to the optimal solution.'</span></pre><p id="ea9c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样在这种情况下，文本被报道为<strong class="la iu">独特的</strong>并且有<strong class="la iu">常见表达</strong> : <em class="lu">在本文中……</em>，<em class="lu">我们的主要贡献……</em>，<em class="lu">我们呈现……</em>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="19fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第三个实验，我用的是<em class="lu">“文本生成的变形金刚”</em>。结果如下:</p><pre class="kj kk kl km gt nv nf nw nx aw ny bi"><span id="9357" class="mc md it nf b gy nz oa l ob oc">'transformers for text generation are becoming increasingly popular in computer vision, and so are the prevalent datasets that contain high-dimensional representations without manual feature engineering. We propose two algorithms for image generation from convolutional networks. We give the first procedure to remove important parts of this architecture and propose a novel architecture dubbed Multi-scale Text Generation Network (MTVGNet). Our MTVGNet-like architecture produces a compact set of representations without changing the model architecture or the pre-trained convolutional network. The resulting representation is compact and can be used as both training examples in training and inference in inference. Extensive numerical experiments on image synthesis and computer vision demonstrate that MTVGNet-like improves generative model performance by up to 32% over state-of-the-art methods for unconditional image generation, while incurring only 20% higher frame quality.'</span></pre><p id="8d3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">即使在第三个例子中，也有多个<strong class="la iu">元素为科学摘要所共有，</strong>且整体质量<strong class="la iu">略好于前一个</strong>。</p><p id="8fd3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">引用的<em class="lu">多尺度文本生成网络(MTVGNet) </em>似乎是模型的<strong class="la iu">“发明”，因为我在文献中找不到参考。</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="723e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我想用一些<strong class="la iu">单词云</strong>来结束这部分。第一个代表数据集中所有摘要中<strong class="la iu">最频繁出现的单词。其他的描绘了从<strong class="la iu">不同提示</strong>产生的</strong>10个文本样本中<strong class="la iu">最常见的单词。</strong></p><p id="721b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可以立即注意到数据集<strong class="la iu">摘要</strong>和<strong class="la iu">生成样本</strong>之间的<strong class="la iu">词语相似度</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/e682a35655d1ae02d7331b244fb5b141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LFthhAY0muMc5MjOS0-__w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">所有摘要中出现频率最高的词。图片由作者提供。</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/55656f2efbd9446f2472b289b273fcd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TjpZJdCaDBBrWXatnAla3w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">10个人工智能生成的文本样本中最常用的单词。提示:推荐系统的作用。图片由作者提供。</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/16c5814eb09a7e74da80ef59ae9b1008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnmMgSZdFdhDaEazOSs2QQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">10个人工智能生成的文本样本中最常用的单词。提示:聚类。图片由作者提供。</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/fa06b8afa72916918ff07c9eafb221c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TjVU0-tR6EnX0_wvwJBhDA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">10个人工智能生成的文本样本中最常用的单词。提示:用于文本生成的转换器。图片由作者提供。</figcaption></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="1f10" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">结论</h2><p id="474d" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">在本文中，我对科学论文摘要的一个转换器进行了微调。 <em class="lu">结果的质量如何？</em> <em class="lu">这种方式有什么局限性？有可能让GPT-2写一篇完整的论文吗？</em></p><p id="95ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模特已经学会了<strong class="la iu">摘要一般是怎么写的</strong>并试图<strong class="la iu">复制同样的<em class="lu">风格</em> </strong>。<strong class="la iu">成绩还不错，</strong>考虑到<strong class="la iu">可用数据</strong>和<strong class="la iu">只有一个训练历元</strong>。</p><p id="7221" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该模型似乎能够<strong class="la iu">生成关于不同机器学习主题的技术文本</strong>，但结果<strong class="la iu">并不总是完全有意义，</strong>有时<strong class="la iu">会有错误</strong>。</p><p id="a530" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，这种方法有一些限制，其中之一是生成文本的T2长度。虽然有可能通过<strong class="la iu">生成多个文本块</strong>来克服这个问题，但是在某些时候，将<strong class="la iu">所生成的不同部分进行逻辑连接</strong>会很困难。</p><p id="05a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总之，即使模型<strong class="la iu">不能写出一篇完整的技术文章</strong>，我仍然惊讶并确信<strong class="la iu">一些可实现的结果</strong>仍然可以成为<strong class="la iu">鼓舞</strong>或<strong class="la iu">暗示</strong>。</p><p id="7e95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">感谢阅读！</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="3609" class="mc md it bd me mf mg dn mh mi mj dp mk lh ml mm mn ll mo mp mq lp mr ms mt mu bi translated">额外资源</h2><ul class=""><li id="1dc0" class="nh ni it la b lb mv le mw lh oi ll oj lp ok lt ol nn no np bi translated">E.比安奇，<em class="lu"> </em> <a class="ae od" href="http://colab.research.google.com/drive/1APs0b3PaLYj77IVRY3qX_5VwOzQDBg_r?usp=sharing" rel="noopener ugc nofollow" target="_blank">用Tensorflow微调GPT-2进行文本生成</a> (2022)，谷歌合作实验室</li><li id="cbc9" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt ol nn no np bi translated">抱脸<em class="lu">、</em>、<a class="ae od" href="http://huggingface.co/docs" rel="noopener ugc nofollow" target="_blank">文档</a> (2022)</li><li id="67ab" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt ol nn no np bi translated">抱脸，<a class="ae od" href="http://huggingface.co/distilgpt2" rel="noopener ugc nofollow" target="_blank">蒸馏2 </a> (2022)</li><li id="ed0b" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt ol nn no np bi translated">康奈尔大学，Kaggle上的arXiv数据集</li><li id="3408" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt ol nn no np bi translated">CShorten，<a class="ae od" href="http://huggingface.co/datasets/CShorten/ML-ArXiv-Papers" rel="noopener ugc nofollow" target="_blank">ML-ArXiv-论文数据集</a> (2022)</li><li id="b35b" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt ol nn no np bi translated">维基百科贡献者，<a class="ae od" href="http://en.wikipedia.org/wiki/Perplexity" rel="noopener ugc nofollow" target="_blank">困惑</a> (2022)</li><li id="0695" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt ol nn no np bi translated"><a class="ae od" href="http://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">维基百科撰稿人，交叉熵</a> (2022)</li><li id="6eaf" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt ol nn no np bi translated">维基百科撰稿人，<a class="ae od" href="http://en.wikipedia.org/wiki/GPT-2" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a> (2022)</li><li id="c44a" class="nh ni it la b lb nq le nr lh ns ll nt lp nu lt ol nn no np bi translated">Keras团队，<a class="ae od" href="http://keras.io/api/optimizers/learning_rate_schedules/exponential_decay" rel="noopener ugc nofollow" target="_blank"> Keras文件:指数衰减</a> (2022)</li></ul></div></div>    
</body>
</html>