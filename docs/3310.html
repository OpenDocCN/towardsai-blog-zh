<html>
<head>
<title>Improve the Performance Easily in TensorFlow Using Graph Mode</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用图形模式轻松提高TensorFlow的性能</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/improve-the-performance-easily-in-tensorflow-using-graph-mode-288ee35dddae?source=collection_archive---------2-----------------------#2022-11-15">https://pub.towardsai.net/improve-the-performance-easily-in-tensorflow-using-graph-mode-288ee35dddae?source=collection_archive---------2-----------------------#2022-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dc97" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们将看到使用自动图形模式代码修饰来获得显著的性能提升是多么容易。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d3377f31843420db99d49978db4140ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2d72L83uHnZvYPd8"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">马修·施瓦茨在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="854f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本来TensorFlow只允许你在图形模式下编码，但是自从引入了在渴望模式下编码的能力后，大部分生产的笔记本都是渴望模式。</p><p id="f744" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以至于很难找到用图模式编写的代码，大多数刚开始或者已经创建模型一段时间的TensorFlow程序员都没有用图模式编程过。</p><p id="2c4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事实是，图形模式下的代码阅读和维护起来要复杂得多，尽管它的效率也高得多。幸运的是，TensorFlow为我们提供了一种从渴望模式到图形模式的简单方法:签名！</p><p id="2b17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">获得一个想法的最好方法是在Eager和Graph模式下查看一个基本函数的代码。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="4227" class="lx ly iq lt b be lz ma l mb mc">#Eager mode. <br/>def func_eager(x):<br/>    if (x &gt;0):<br/>        x = x + 1<br/>    return x     <br/><br/>#Same function in graph mode. <br/>def func_graph(x):<br/>    def if_true():<br/>        return x + 1<br/>    def if_false():<br/>        return x<br/>    x=tf.cond(tf.greater(x, 0), if_true, if_false)</span></pre><p id="dd29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个函数真的很简单:如果<strong class="ky ir"> <em class="md"> x </em> </strong>大于0，它就给<strong class="ky ir">T5】x</strong>加1。</p><p id="a1c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">急切功能毋庸置疑。对于懂一点编程的人来说，阅读起来很简单。但是图形函数中的代码变得非常复杂，很难阅读和理解，更不用说写了。尽管如此，它对机器来说是更好的代码，并且更容易并行执行，这将导致更好的性能。</p><p id="9a9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图形模式代码获得的这些性能改进使它成为许多笔记本电脑非常感兴趣的技术。我们必须利用这一点。</p><p id="bde1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一节中，我们将研究一些如何使用亲笔签名的例子。基于一个笔记本，可以在Kaggle上找到。在其中，您可以找到Graph和Eager模式下的函数，从而可以比较这两种方法的性能。</p><div class="me mf gp gr mg mh"><a href="https://www.kaggle.com/code/peremartramanonellas/improve-tensorflow-performance-with-graph-mode" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd ir gy z fp mm fr fs mn fu fw ip bi translated">使用图形模式提高张量流性能。</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">使用Kaggle笔记本探索和运行机器学习代码|使用来自无附加数据源的数据</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">www.kaggle.com</p></div></div><div class="mq l"><div class="mr l ms mt mu mq mv kp mh"/></div></div></a></div><h1 id="0175" class="mw ly iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated">测试签名。</h1><p id="38dc" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">我们将使用与上面相同的函数。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="0b02" class="lx ly iq lt b be lz ma l mb mc">@tf.function<br/>def func(x):<br/>    if x &gt; 0:<br/>        x = x + 1<br/>    return x</span></pre><p id="08a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">唯一的修改是用<strong class="ky ir"> <em class="md"> @tf.function </em> </strong>来修饰函数。我们不需要其他任何东西，但是我们很快就会明白为什么不总是这样，为什么不是所有的代码都可以100%移植到图形模式。</p><p id="44ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了知道如何转换代码，查看生成的代码也很有趣。我们只用一行代码就可以做到。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="3e78" class="lx ly iq lt b be lz ma l mb mc">print (tf.autograph.to_code(func.python_function))</span></pre><p id="1a08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">显示的代码是:</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="0036" class="lx ly iq lt b be lz ma l mb mc">def tf__func(X):<br/>    with ag__.FunctionScope('func', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:<br/>        do_return = False<br/>        retval_ = ag__.UndefinedReturnValue()<br/><br/>        def get_state():<br/>            return (x,)<br/><br/>        def set_state(vars_):<br/>            nonlocal x<br/>            (x,) = vars_<br/><br/>        def if_body():<br/>            nonlocal x<br/>            x = (ag__.ld(x) + 1)<br/><br/>        def else_body():<br/>            nonlocal x<br/>            pass<br/>        x = ag__.Undefined('x')<br/>        ag__.if_stmt((ag__.ld(x) &amp;gt; 0), if_body, else_body, get_state, set_state, ('x',), 1)<br/>        try:<br/>            do_return = True<br/>            retval_ = ag__.ld(x)<br/>        except:<br/>            do_return = False<br/>            raise<br/>        return fscope.ret(retval_, do_return)</span></pre><p id="af91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它比我写的具有相同功能的要复杂得多，尽管如果我们仔细观察，它具有相同的结构。</p><p id="82e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有要执行的代码都包含在包含智能的行之前定义的函数中。流程由<strong class="ky ir"> <em class="md"> if_stmt </em> </strong>函数控制，该函数接收一个条件，并具有在函数为真或假时执行的函数。在我们的例子中，条件是<strong class="ky ir"> <em class="md"> x </em> </strong>大于0。满足条件就调用<strong class="ky ir"> <em class="md"> if_body </em> </strong>，不满足条件就调用<strong class="ky ir"> <em class="md"> else_body </em> </strong>。</p><p id="77f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看一些情况，如果我们想在图形模式下使用代码，我们必须修改代码。</p><p id="a8f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，<strong class="ky ir">我们不能在函数中声明张量变量，如果我们需要它们的话</strong>。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="ba18" class="lx ly iq lt b be lz ma l mb mc">#This function will fail as it does not support declaring tf variables in the body. <br/>#if you want to execute and test it, remove the comments. &lt;/em&gt;<br/>#@tf.function<br/>#def f(x):<br/>#    v = tf.Variable(1.0)<br/>#    return v.assign_add(x)<br/><br/>#For it to work, we just have to remove the variable and declare it outside the function<br/>v = tf.Variable(1.0)<br/>@tf.function<br/>def f(x):<br/>    return v.assign_add(x)</span></pre><p id="9cdd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解决方法很简单，就是在函数体外部声明变量。</p><p id="526b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个要考虑的例子是<strong class="ky ir"> <em class="md"> print() </em> </strong>函数是如何工作的。在图形模式下，这个函数将只执行一次，不管它是否在应该执行多次的循环中。解决方法很简单，用<strong class="ky ir"> <em class="md"> tf.print() </em> </strong>函数替换即可。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="1735" class="lx ly iq lt b be lz ma l mb mc">@tf.function<br/>def print_test(): <br/>    tf.print("with tf.print")<br/>    print("with print")<br/><br/>for i &lt;strong&gt;in&lt;/strong&gt; range(5):<br/>    print_test()</span></pre><p id="daf4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该函数的结果将是:</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="c2e2" class="lx ly iq lt b be lz ma l ns mc">with print<br/>with tf.print<br/>with tf.print<br/>with tf.print<br/>with tf.print<br/>with tf.print</span></pre><p id="c6ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也就是说，该块将被执行五次，但只对<strong class="ky ir"> <em class="md"> print() </em> </strong>进行一次调用。<strong class="ky ir"> <em class="md">断言</em> </strong>也会发生类似的情况，必须用相应的<strong class="ky ir"><em class="md">TF . debugging . ASSERT</em></strong>替换。</p><p id="283c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这三种情况只是一个示例，尽管它们是我们在试图将代码从Eager模式传递到Graph模式时最先遇到的。</p><h1 id="b26e" class="mw ly iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated">使用几个数据集比较Eager和Graph模式。</h1><p id="ab06" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">在Kaggle 上可以找到<a class="ae kv" href="https://www.kaggle.com/code/peremartramanonellas/improve-tensorflow-performance-with-graph-mode" rel="noopener ugc nofollow" target="_blank">的笔记本里，你会找到所有的代码。在本文中，我将只展示与图形模式中的测试和代码直接相关的部分。</a></p><p id="48a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该笔记本已经准备好处理几个非常流行的数据集:<strong class="ky ir"> <em class="md">猫vs狗</em> </strong>和<strong class="ky ir"> <em class="md">人vs马</em> </strong>。在Kaggle中，由于平台的内存限制，我使用了人与马的数据集。你可以试试<strong class="ky ir"> <em class="md">猫vs狗</em> </strong>数据集，如果你下载了你的笔记本并在内存更大的机器上运行的话。</p><p id="42b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个数据集是TensorFlow数据集数据库的一部分。因此，在任何环境中使用笔记本都更加容易，而不必担心从数据集中获取数据。</p><p id="56d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为更容易看到所获得的改进，所以我使用了一个定制模型。</p><p id="d85b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以加速的一个领域是数据处理。在这种情况下，它们是简单的图像，几乎没有处理，但即使如此，也可以看到显著的百分比改善。在像自然语言处理这样的数据处理非常繁重的领域，改进将更加显著。这取决于数据集的大小和您想要执行的处理。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="74d1" class="lx ly iq lt b be lz ma l mb mc">#Treat the image in eager mode. <br/>def map_fn_eager(img, label):<br/>    # resize the image<br/>    img = tf.image.resize(img, size=[IMAGE_SIZE, IMAGE_SIZE])<br/>    # normalize the image<br/>    img /= 255.0<br/>    return img, label<br/><br/>#Treat the image in graph mode. <br/>@tf.function<br/>def map_fn_graph(img, label):<br/>    # resize the image<br/>    img = tf.image.resize(img, size=[IMAGE_SIZE, IMAGE_SIZE])<br/>    # normalize the image<br/>    img /= 255.0<br/>    return img, label<br/><br/># Prepare train dataset by using preprocessing with map_fn_eager or graph, shuffling and batching<br/>def prepare_dataset(train_examples, validation_examples, test_examples, num_examples, map_fn, batch_size):<br/>    train_ds = train_examples.map(map_fn).shuffle(buffer_size = num_examples).batch(batch_size)<br/>    valid_ds = validation_examples.map(map_fn).batch(batch_size)<br/>    test_ds = test_examples.map(map_fn).batch(batch_size)<br/>    <br/>    return train_ds, valid_ds, test_ds</span></pre><p id="cf2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以看到，<strong class="ky ir"> <em class="md"> map_fn_eager </em> </strong>和<strong class="ky ir"> <em class="md"> map_fn_graph </em> </strong>这两个函数的代码是一样的，只是用@tf.function的修饰有所不同。</p><p id="a241" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还有第三个函数<strong class="ky ir"><em class="md">prepare _ dataset</em></strong>，这个函数将被调用来准备数据集。它将接收要执行的函数作为参数。当我们调用这个函数时，我们将指出我们是否想要执行准备在图形模式或渴望模式下工作的函数。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="1430" class="lx ly iq lt b be lz ma l mb mc">start_time = time.time()<br/>train_ds_eager, valid_ds_eager, test_ds_eager = prepare_dataset(train_examples, <br/>                                                                validation_examples, <br/>                                                                test_examples, <br/>                                                                num_examples, <br/>                                                                map_fn_eager, BATCH_SIZE)<br/>end_time = time.time()<br/>print ("Eager Time spend:",  end_time - start_time)<br/><br/>start_time = time.time()<br/>train_ds_graph, valid_ds_graph, test_ds_graph = prepare_dataset(train_examples, <br/>                                                                validation_examples, <br/>                                                                test_examples, <br/>                                                                num_examples, <br/>                                                                map_fn_graph, BATCH_SIZE)<br/>end_time = time.time()<br/>print ("GraphTime spend:",  end_time - start_time)</span></pre><pre class="nt ls lt lu bn lv lw bi"><span id="568b" class="lx ly iq lt b be lz ma l ns mc">Eager Time sped: 0.061063528060913086<br/>Graph Time spend: 0.0561823844909668</span></pre><p id="e9d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，这是一个非常快速的过程，这很正常，因为数据集很小而且很简单。即便如此，我们只需要一行代码就可以获得15%的性能提升。一点也不差。<strong class="ky ir">最重要的改进将出现在模式</strong> l的执行中。</p><p id="ee80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Kaggle中使用的笔记本可以与三种型号配合使用。其中两个是从TensorFlow的HUB获得的:A resnet_50和resnet_v2_152。但我们将看到第三个模型的结果，这个模型简单得多，是用TensorFlow的顺序API构建的。如果您可以在更强大的机器上运行它，请不要犹豫尝试其他两个模型。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="8e64" class="lx ly iq lt b be lz ma l mb mc">#MODULE_HANDLE = 'https://tfhub.dev/tensorflow/resnet_50/feature_vector/1'<br/>#MODULE_HANDLE = 'https://tfhub.dev/google/imagenet/resnet_v2_152/classification/5'<br/>#model = tf.keras.Sequential([<br/>#    hub.KerasLayer(MODULE_HANDLE, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),<br/>#    tf.keras.layers.Dense(num_classes, activation='softmax')<br/>#])<br/><br/>model = tf.keras.models.Sequential([<br/>            tf.keras.layers.Conv2D(16, (4,4), activation="relu", input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),<br/>            tf.keras.layers.MaxPooling2D(2,2),<br/>            tf.keras.layers.Dropout(0.2),   <br/>            tf.keras.layers.Conv2D(32, (4,4), activation="relu"),<br/>            tf.keras.layers.MaxPooling2D(2,2),  <br/>            tf.keras.layers.Dropout(0.2),  <br/>            tf.keras.layers.Conv2D(64, (4,4), activation="relu"),<br/>            tf.keras.layers.MaxPooling2D(2,2),  <br/>            tf.keras.layers.Dropout(0.5), <br/>            tf.keras.layers.Flatten(), <br/>            tf.keras.layers.Dense(512, activation="relu"), <br/>            tf.keras.layers.Dense(2, activation="softmax")])<br/>model.summary()</span></pre><pre class="nt ls lt lu bn lv lw bi"><span id="deda" class="lx ly iq lt b be lz ma l ns mc">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>conv2d (Conv2D)              (None, 221, 221, 16)      784       <br/>_________________________________________________________________<br/>max_pooling2d (MaxPooling2D) (None, 110, 110, 16)      0         <br/>_________________________________________________________________<br/>dropout (Dropout)            (None, 110, 110, 16)      0         <br/>_________________________________________________________________<br/>conv2d_1 (Conv2D)            (None, 107, 107, 32)      8224      <br/>_________________________________________________________________<br/>max_pooling2d_1 (MaxPooling2 (None, 53, 53, 32)        0         <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 53, 53, 32)        0         <br/>_________________________________________________________________<br/>conv2d_2 (Conv2D)            (None, 50, 50, 64)        32832     <br/>_________________________________________________________________<br/>max_pooling2d_2 (MaxPooling2 (None, 25, 25, 64)        0         <br/>_________________________________________________________________<br/>dropout_2 (Dropout)          (None, 25, 25, 64)        0         <br/>_________________________________________________________________<br/>flatten (Flatten)            (None, 40000)             0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 512)               20480512  <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 2)                 1026      <br/>=================================================================<br/>Total params: 20,523,378<br/>Trainable params: 20,523,378<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="9532" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，该型号将是定制型号。这意味着我将编写将在每一步中执行的函数，以及控制纪元训练的函数。第二个函数将在渴望和图形模式下执行。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="d7cd" class="lx ly iq lt b be lz ma l mb mc"># Custom training step. This function is executed in each step each epoch. <br/>def train_one_step(model, optimizer, x, y, train_loss, train_accuracy):<br/>    with tf.GradientTape() as tape:<br/>        &lt;em&gt;# Run the model on input x to get predictions&lt;/em&gt;<br/>        predictions = model(x)<br/>        &lt;em&gt;# Compute the training loss using `train_loss`, passing in the true y and the predicted y&lt;/em&gt;<br/>        loss = train_loss(y, predictions)<br/><br/>    # Using the tape and loss, compute the gradients on model variables using tape.gradient<br/>    grads = tape.gradient(loss, model.trainable_variables)<br/>    <br/>    # Zip the gradients and model variables, and then apply the result on the optimizer<br/>    optimizer.apply_gradients(zip(grads, model.trainable_variables))<br/><br/>    # Call the train accuracy object on ground truth and predictions<br/>    train_accuracy(y, predictions)<br/>    return loss</span></pre><p id="330d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述功能将在每个步骤中执行。如你所见，它没有秘密。它进行预测，恢复损失，并计算传递给优化器的梯度，优化器决定如何修改权重。</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="ad37" class="lx ly iq lt b be lz ma l mb mc">def train_eager(model, optimizer, epochs, train_ds, train_loss, train_accuracy, valid_ds, val_loss, val_accuracy):<br/>    step = 0<br/>    loss = 0.0<br/>    for epoch &lt;strong&gt;in&lt;/strong&gt; range(epochs):<br/>        for x, y &lt;strong&gt;in&lt;/strong&gt; train_ds:<br/>            # training step number increments at each iteration<br/>            step += 1<br/><br/>            # Run one training step by passing appropriate model parameters<br/>            # required by the function and finally get the loss to report the results<br/>            loss = train_one_step(model, optimizer, x, y, train_loss, train_accuracy)<br/><br/>            # Use tf.print to report your results.&lt;/em&gt;<br/>            # Print the training step number, loss and accuracy<br/>            print('Step', step, <br/>                   ': train loss', loss, <br/>                   '; train accuracy', train_accuracy.result())<br/><br/>        for x, y &lt;strong&gt;in&lt;/strong&gt; valid_ds:<br/>            # Call the model on the batches of inputs x and get the predictions<br/>            y_pred = model(x)<br/>            loss = val_loss(y, y_pred)<br/>            val_accuracy(y, y_pred)<br/>        # Print the validation loss and accuracy<br/>        <br/>        tf.print('val loss', loss, '; val accuracy', val_accuracy.result())<br/><br/>@tf.function<br/>def train_graph(model, optimizer, epochs, train_ds, train_loss, train_accuracy, valid_ds, val_loss, val_accuracy):<br/>    step = 0<br/>    loss = 0.0<br/>    for epoch in range(epochs):<br/>        for x, y in train_ds:<br/>            # training step number increments at each iteration<br/>            step += 1<br/><br/>            # Run one training step by passing appropriate model parameters<br/>            # required by the function and finally get the loss to report the results<br/>            loss = train_one_step(model, optimizer, x, y, train_loss, train_accuracy)<br/><br/>            # Use tf.print to report your results.<br/>            # Print the training step number, loss and accuracy<br/>            tf.print('Step', step, <br/>                   ': train loss', loss, <br/>                   '; train accuracy', train_accuracy.result())<br/><br/>        for x, y in valid_ds:<br/>            # Call the model on the batches of inputs x and get the predictions<br/>            y_pred = model(x)<br/>            loss = val_loss(y, y_pred)<br/>            val_accuracy(y, y_pred)<br/><br/>        # Print the validation loss and accuracy<br/>        tf.print('val loss', loss, '; val accuracy', val_accuracy.result())</span></pre><p id="a524" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以看到，两个函数的代码几乎是一样的。<strong class="ky ir">唯一需要的修改是用<em class="md"> tf.print() </em> </strong>函数替换<em class="md"> print() </em>函数。这些功能在每个步骤调用<strong class="ky ir"> <em class="md"> train_eager </em> </strong>，并在指定的时期内执行。</p><p id="2600" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看在渴望模式下执行该函数的结果:</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="7a7b" class="lx ly iq lt b be lz ma l mb mc">#Solving the model in eager mode, and printing the time elapsed.<br/>st = time.time()<br/>train_eager(model, optimizer, 6, train_ds_eager, <br/>            train_loss, train_accuracy, valid_ds_eager, <br/>            val_loss, val_accuracy)<br/>et = time.time()<br/>print('Eager mode spent time: ' et - st)</span></pre><pre class="nt ls lt lu bn lv lw bi"><span id="7851" class="lx ly iq lt b be lz ma l ns mc">......<br/>Step 137 : train loss tf.Tensor(0.00049685716, shape=(), dtype=float32) ; train accuracy tf.Tensor(0.9509188, shape=(), dtype=float32)<br/>Step 138 : train loss tf.Tensor(0.0001548939, shape=(), dtype=float32) ; train accuracy tf.Tensor(0.9510895, shape=(), dtype=float32)<br/>val loss 4.05896135e-05 ; val accuracy 0.98780489<br/>Eager mode spent time: 21.57599711418152</span></pre><p id="7837" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图表模式:</p><pre class="kg kh ki kj gt ls lt lu bn lv lw bi"><span id="6490" class="lx ly iq lt b be lz ma l mb mc">#Solving the model in graph mode, and printing the time elapsed.<br/>st = time.time()<br/>train_graph(model, optimizer, 6, train_ds_graph, <br/>            train_loss, train_accuracy, valid_ds_graph, <br/>            val_loss, val_accuracy)<br/>et = time.time()<br/>print('Graph mode spent time: ' et - st)</span></pre><pre class="nt ls lt lu bn lv lw bi"><span id="cc1d" class="lx ly iq lt b be lz ma l ns mc">.............<br/>Step 137 : train loss 0.000154053108 ; train accuracy 0.975502133<br/>Step 138 : train loss 3.81469249e-07 ; train accuracy 0.975544751<br/>val loss 2.89767604e-06 ; val accuracy 0.993902445<br/>11.941826820373535</span></pre><p id="3529" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如您所观察到的，模型执行中的性能改进是相当可观的。从21秒到12秒，提高了大约50%。</p><p id="c355" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你知道这些数字有些不准确。我们使用玩具数据集。</p><p id="85a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想象一下您在大型数据集中可以节省的容量。除了这种提高不仅体现在训练时间上，还体现在推理时间上。</p><h1 id="4269" class="mw ly iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated">结论。</h1><p id="6f04" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">老实说，我认为让我们的模型支持图形模式势在必行。不一定要土生土长，但是要支持亲笔签名。</p><p id="d7a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在性能上获得的改进可能非常可观，我们的模型在图形模式下的训练速度比在Eager模式下快50%。如果我们想一想修改代码来实现这一改进是多么容易，很明显，我们必须尝试修改我们所有的笔记本。</p><p id="9299" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们发现不仅性能有所提高，而且获得的精度值也有所不同。我无法解释为什么。但这是我们必须控制的事情，如果获得的指标不相同，我们必须考虑在每次将函数从渴望模式传递到图形模式时重新评估模型，反之亦然。</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="a057" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是关于TensorFlow和Keras中的高级主题的系列文章的一部分，<strong class="ky ir">如果你喜欢它，可以考虑在</strong> <a class="ae kv" href="https://medium.com/@peremartra" rel="noopener"> <strong class="ky ir"> Medium </strong> </a>上关注我以获得关于新文章的更新。当然，也欢迎你<strong class="ky ir">在</strong> <a class="ae kv" href="https://www.linkedin.com/in/pere-martra-0310631" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> LinkedIn </strong> </a>上与我连线。</p><div class="me mf gp gr mg"><div role="button" tabindex="0" class="ab bv gv cb fp ob oc bn od kp ex"><div class="oe l"><div class="ab q"><div class="l di"><img alt="Pere Martra" class="l de bw of og fe" src="../Images/92b0a2efbdea5f262a3b203de380857b.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*15GCvLcX8MrvabnQEAHteA.jpeg"/><div class="fb bw l of og fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://medium.com/@peremartra?source=post_page-----288ee35dddae--------------------------------" rel="noopener follow" target="_top">佩雷·马特拉</a></p></div></div><div class="oj ok gw l"><h2 class="bd ir tr ts fp tt fr fs mn fu fw ip bi translated">张量流超越基础</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi tu au tv tw tx qf ty an eh ei tz ua ub el em eo de bk ep" href="https://medium.com/@peremartra/list/tensorflow-beyond-the-basics-24c4c6a844d8?source=post_page-----288ee35dddae--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="uc l fo"><span class="bd b dl z dk">3 stories</span></div></div></div><div class="ow dh ox fp ab oy fo di"><div class="di oo bv op oq"><div class="dh l"><img alt="" class="dh" src="../Images/a6cc8494220dde3f66b935ddf24ef15d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*2d72L83uHnZvYPd8"/></div></div><div class="di oo bv or os ot"><div class="dh l"><img alt="" class="dh" src="../Images/a0ef9ec52ab60f8a8c436e6dcafd270b.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*n3IB5EW--404lf3ZZlo2sg.jpeg"/></div></div><div class="di bv ou ov ot"><div class="dh l"><img alt="" class="dh" src="../Images/641c560d79ca678019b7ddabb33854e2.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*fsP7FbIqO2MEl4SsaP374g.jpeg"/></div></div></div></div></div></div></div>    
</body>
</html>