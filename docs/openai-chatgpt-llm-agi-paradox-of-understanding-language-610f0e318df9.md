# OpenAI，ChatGPT，LLM，AGI:理解语言的悖论

> 原文：<https://pub.towardsai.net/openai-chatgpt-llm-agi-paradox-of-understanding-language-610f0e318df9?source=collection_archive---------0----------------------->

## 探索语言、理解和智力的意义。

每个人都在大肆宣传大型语言模型的实用性。有太多令人愤慨的说法，比如“ChatGPT 会扼杀这些工作”或者“ChatGPT 会改变我们互动的方式”等等。另一方面，对这些模型生成的文本也有很多批评，主要来自于这些只是下一个单词或句子的概率表示的观点。这些模型已经产生了听起来不错的输出，但完全是 jibberish 或者是概率权重矩阵的随机游走。像 GPT-3、PaLM 和 BERT 这样的 LLM 是为了探索和重新创建语言的一般结构而创建的，同时维护作为上下文的一般知识。我必须说这些模型已经很好的实现了这个目标。

![](img/2cd52129f9229ded347cada61161eac4.png)

Photo by [愚木混株 cdd20](https://unsplash.com/@cdd20?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

## 这是否意味着模型已经理解了输入？

简而言之，没有。然而，它确实意味着该模型已经在先前观察到的周围其他句子的分布的上下文中识别出输入句子的一般上下文，并且创建了符合历史上下文的足够一般的输出。这可能就是 AGI(人工通用智能)中的 G(通用)。尽管如此，我还是不愿意称之为智慧。要称一个实体/过程/流为智能，我们大概需要进行一次回答“什么是智能”的哲学之旅。

根据牛津大学的说法，人工智能是“能够执行通常需要人类智能的任务的计算机系统的理论和发展，如视觉感知、语音识别、决策和语言之间的翻译。”人类智能被定义为“以逻辑方式学习、理解和思考事物的能力；做好这件事的能力”。然而，这个定义并不具有可操作性。为了使它具有可操作性，让我们在“理解”这个不太复杂的结构上构建它。我们可以说，理解的过程就是学习。既然我们定义了“理解”和“学习”,我们就很好地涵盖了人类智力定义的复杂性。

## 理解的逻辑结构

我的假设是，理解一定不仅仅是文字的意义。跟我走这条逻辑-

一个人有意图去描述或表达他们的想法。词语表达了描述思想广度意图。然而，基于所选择的词来表现所有细节的时间和能力是有限的。从数学上来说，人们可以说词语代表了表达所有需要表达的意图的较低维度的表达。在一次好的谈话中，这种较低维度的表达足以表达思想。然而，根据定义，低维表示是以损失信息为代价的。因此，口头、书面或表达的语言在双向对话中固有信息损失。

此外，这些表达出来的想法由一个人在他们经历过的过去和当前的情境中听这些词或读这些词(较低维度的表示)来解释。我们把解释者的情况叫做接收者的**情况**。这意味着低维表示的解释就像贝叶斯后验，其中条件是接收者的情况。

为了使理解完整或与人类交流相当，思想的低维表达需要保留足够的信息，并且发射器和接收器的情况需要足够相似以避免过度失真。换句话说，它需要足够相似，不会增加信息损失。这意味着真正的理解需要接受者的输入，使用正确的语境提供答案。

## 智力

回到智力的概念是一个更高层次的任务。这是一种广义的理解能力。从应用的角度来说，它是一个通用语言模型在给定情况下输出的适当性的函数。我的描述是，没有足够的理由来测试一个模型的智能，直到我们能够解决整合接收者的背景的问题，这更接近于理解的结构。

![](img/ec397230ae53a22ebe61b7597013ad36.png)

由[格雷格·拉科齐](https://unsplash.com/@grakozy?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

## 理解是普遍的吗？

LLM 能够以保持语言的广义上下文和结构的方式编码和解码语言。正如我们前面讨论的，上下文是从周围数据的上下文的文本表示中借用的，其中多头注意力用于近似提示的含义。我们还讨论了“现实生活环境”(真实)的表示在文本上下文中仅被部分捕获，这限制了纯文本 LLM 解析对提示的良好响应的能力。

再往前一步，我们知道每个人都有一套独特的环境表征。生活在城市的人对同一个问题的回答与生活在农村的人不同。然而，从他们自己的语境角度来看，他们两人的回答都是恰当的。另一个例子是那些富有同情心的人和那些缺乏同情心的人对同样的情况有不同的反应。然而，从他们自己的角度来看，他们都是正确的。这就是我们开始讨论主观和客观 AGI 的界限的地方。在没有非文本语境的情况下，AGI 在客观上可能比主观上更合适。

我可以说，理解的“过程”是普遍的。然而，理解的“结果”是主观的。主观推断和客观推断之间有细微的界限。对太多个性化的环境(非文本)信息进行训练可能会增加主观准确性，但会失去客观的概括能力。这增加了在客观背景下保持正确的责任。否则，它很可能是一个回音室，模型在这里产生用户想要接收的输出。

![](img/2f839d1cc3d15841ddf14be6ddc02f2f.png)

Pawel Czerwinski 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

## 理解语言的悖论

所以，我们到了..“语言”提供了表达理解输出的手段。“理解”是产生输出的思维(编码)表示的过程(编码阅读/听力+添加环境背景)。“语言”用于将理解的输出解码成另一种符合语言结构的表达。这些话是用来表达思想/理解的。

虽然 LLM 可能能够完成对理解输出进行编码和解码的任务。它还能够将从相似文本的历史使用中携带的有限上下文注入解码过程。它很可能只是产生一个听起来不错的通用输出，但很可能是不可操作的。为了完成智能的定义，LLM 需要有能力探索训练空间的边界。在那之前，尽管 LLM 可以重新提出“理解”的定义，但它无法定义它。

![](img/de1869f56fd3fdcbec4b41c7a5da037b.png)

来支持我🔔 ***拍手*** | ***跟随|*** [***订阅***](https://ithinkbot.com/subscribe) **🔔**

使用我的链接成为会员:[https://ithinkbot.com/membership](https://ithinkbot.com/membership)

检查我的其他作品—

[](https://ithinkbot.com/2022-year-of-chatgpt-what-is-the-future-979b034efdf9) [## 2022:chat GPT 年，未来如何？

### 看着水晶球

ithinkbot.com](https://ithinkbot.com/2022-year-of-chatgpt-what-is-the-future-979b034efdf9) [](/openai-debuts-chatgpt-50dd611278a4) [## OpenAI 首次亮相 ChatGPT

### OpenAI 周三发布了一款名为 ChatGPT 的新模型。这个模型被训练成在一个…

pub.towardsai.net](/openai-debuts-chatgpt-50dd611278a4) [](/what-is-gpt-4-and-when-9f5073f25a6d) [## 什么是 GPT-4(什么时候？)

### GPT-4 是一个自然语言处理模型，由 openAI 作为 GPT-3 的继承者开发

pub.towardsai.net](/what-is-gpt-4-and-when-9f5073f25a6d)