<html>
<head>
<title>Unsupervised Data Augmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督数据扩充</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/unsupervised-data-augmentation-6760456db143?source=collection_archive---------2-----------------------#2019-08-05">https://pub.towardsai.net/unsupervised-data-augmentation-6760456db143?source=collection_archive---------2-----------------------#2019-08-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b317cc088d7059b9cc9da65842de6330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1NvpsIcE3_oTbHQ_"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae jg" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><h2 id="fc2e" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">数据增强| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">看向人工智能</a></h2><div class=""/><p id="16f6" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们拥有的数据越多，我们能够实现的性能就越好。然而，注释大量的训练数据是非常奢侈的。因此，适当的数据扩充有助于提高模型性能。<a class="ae jg" href="https://arxiv.org/pdf/1904.12848.pdf" rel="noopener ugc nofollow" target="_blank">无监督数据增强</a>(谢等，2019)的作者提出无监督数据增强(UDA)帮助我们通过利用几种数据增强方法来建立更好的模型。</p><blockquote class="ln lo lp"><p id="45a1" class="kp kq lq kr b ks kt ku kv kw kx ky kz lr lb lc ld ls lf lg lh lt lj lk ll lm im bi translated">在自然语言处理领域，由于语言的高度复杂性，对文本进行扩充是非常困难的。不是每一个词我们都可以用其他词来代替，比如a，an，the。另外，不是每个单词都有同义词。即使改变一个单词，上下文也会完全不同。另一方面，在计算机视觉领域生成增强图像相对容易。即使引入噪声或裁剪掉图像的一部分，模型仍然可以对图像进行分类。</p></blockquote><p id="bb72" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">谢等人对图像分类(<code class="fe lu lv lw lx b">AutoAugment</code>)和文本分类(<code class="fe lu lv lw lx b">Back translation</code>、<code class="fe lu lv lw lx b">TF-IDF based word replacing</code>)进行了多次数据增强实验。在生成足够大的模型训练数据集后，作者注意到模型很容易过度拟合。因此，他们引入<code class="fe lu lv lw lx b">Training Signal Annealing (TSA)</code>来克服它。</p><h1 id="7941" class="ly lz jj bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">增强策略</h1><p id="1741" class="pw-post-body-paragraph kp kq jj kr b ks mw ku kv kw mx ky kz la my lc ld le mz lg lh li na lk ll lm im bi translated">本节将介绍计算机视觉(CV)和自然语言处理(NLP)领域中的三种数据增强。</p><h2 id="4315" class="nb lz jj bd ma nc nd dn me ne nf dp mi la ng nh mm le ni nj mq li nk nl mu jp bi translated">图像分类的自动增强</h2><p id="6ba3" class="pw-post-body-paragraph kp kq jj kr b ks mw ku kv kw mx ky kz la my lc ld le mz lg lh li na lk ll lm im bi translated">自动增强是谷歌在2018年发现的。这是一种自动增强图像的方法。与传统的图像增强库不同，AutoAugment旨在找到自动操作数据的最佳策略。</p><p id="163d" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以访问<a class="ae jg" href="https://github.com/tensorflow/models/tree/master/research/autoaugment" rel="noopener ugc nofollow" target="_blank">这里</a>的模型和实现。</p><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ef204e7ea4dd0c3805d2f9a78aae8568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*nwnEJoSn5-sxNpG6DOlFVA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">自动增强生成的结果(Cubuk等人，2018年)</figcaption></figure><h2 id="bfb6" class="nb lz jj bd ma nc nd dn me ne nf dp mi la ng nh mm le ni nj mq li nk nl mu jp bi translated">文本分类的反向翻译</h2><p id="05d0" class="pw-post-body-paragraph kp kq jj kr b ks mw ku kv kw mx ky kz la my lc ld le mz lg lh li na lk ll lm im bi translated">反向翻译是一种利用翻译系统生成数据的方法。假设我们有一个将英语翻译成粤语的模型，反之亦然。可以通过将原始数据从英语翻译成粤语，然后再翻译回英语来检索扩充数据。</p><blockquote class="ln lo lp"><p id="775c" class="kp kq lq kr b ks kt ku kv kw kx ky kz lr lb lc ld ls lf lg lh lt lj lk ll lm im bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1511.06709.pdf" rel="noopener ugc nofollow" target="_blank"> Sennrich et al. (2015) </a>使用反向翻译方法生成更多的训练数据来提高翻译模型的性能。</p></blockquote><figure class="nn no np nq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/289f49ed324e32e747a77d6a2b573232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pkj0hnD43MJuMUUAgHpwKA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">的例子(谢等，2019)</figcaption></figure><h2 id="8ff5" class="nb lz jj bd ma nc nd dn me ne nf dp mi la ng nh mm le ni nj mq li nk nl mu jp bi translated">基于TF-IDF的文本分类单词替换</h2><p id="7d92" class="pw-post-body-paragraph kp kq jj kr b ks mw ku kv kw mx ky kz la my lc ld le mz lg lh li na lk ll lm im bi translated">虽然反向翻译有助于生成大量数据，但不能保证关键字在翻译后会被保留。一些关键字比其他关键字携带更多的信息，翻译后可能会被遗漏。</p><p id="a0fe" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，谢等人使用<a class="ae jg" href="https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016" rel="noopener" target="_blank"> TF-IDF </a>来解决这一限制。TF-IDF的概念是高频可能不能提供太多的信息增益。换句话说，稀有词对模型贡献了更多的权重。如果在同一文档(即培训记录)中出现的次数增加，单词的重要性将增加。另一方面，如果它出现在语料库(即其他训练记录)中，它将被减少。</p><p id="b7a6" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">IDF分数由DBPedia语料库计算。将为每个令牌计算TF-IDF分数，并根据TF-IDF分数替换它。TF-IDF分数低的会有很大概率被换下。</p><p id="c164" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你对使用基于TF-IDF的单词替换进行数据扩充感兴趣，你可以访问<a class="ae jg" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank"> nlpaug </a>了解python实现。</p><h1 id="3f41" class="ly lz jj bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">训练信号退火(TSA)</h1><p id="f41a" class="pw-post-body-paragraph kp kq jj kr b ks mw ku kv kw mx ky kz la my lc ld le mz lg lh li na lk ll lm im bi translated">在使用上述技巧生成大量数据后，谢等人注意到该模型容易过拟合。因此，他们引入了TSA。在模型训练期间，具有高置信度的样本将从损失函数中移除，以防止过度训练。</p><p id="b20c" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下图显示了ηt的取值范围，而K是类别数。如果概率高于ηt，它将从损失函数中移除。</p><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/74dbbc5670464fe54b99526492dfb429.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*ITyrjAHpn21ua7bDkSBaNQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">去除高概率样本的阈值(谢等，2019)</figcaption></figure><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ade856ad0ff9e0b520811599643b59fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*wts7wcoL1hsED_5eaNGyuA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">TSA的目标函数(谢等，2019)</figcaption></figure><p id="74c5" class="pw-post-body-paragraph kp kq jj kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">3ηt的计算考虑了不同的场景。</p><ul class=""><li id="1770" class="nu nv jj kr b ks kt kw kx la nw le nx li ny lm nz oa ob oc bi translated">线性时间表:持续增长</li><li id="1064" class="nu nv jj kr b ks od kw oe la of le og li oh lm nz oa ob oc bi translated">Log-schedule:训练初期成长较快。</li><li id="fb8b" class="nu nv jj kr b ks od kw oe la of le og li oh lm nz oa ob oc bi translated">Exp-schedule:在训练结束时成长更快。</li></ul><figure class="nn no np nq gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/568f85d29164211b25e193a4a2392792.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*wJrF9YPqo5VmxKc1yk5Tmw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">三个时间表中的训练过程(谢等，2019)</figcaption></figure><h1 id="c77b" class="ly lz jj bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">建议</h1><ul class=""><li id="b0aa" class="nu nv jj kr b ks mw kw mx la oj le ok li ol lm nz oa ob oc bi translated">上述方法旨在解决作者在他们的问题中所面临的问题。如果你了解你的数据，你应该量身定做增强方法。请记住，数据科学的黄金法则是垃圾进垃圾出。</li></ul><h1 id="94ec" class="ly lz jj bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph kp kq jj kr b ks mw ku kv kw mx ky kz la my lc ld le mz lg lh li na lk ll lm im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。在<a class="ae jg" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae jg" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上随时联系<a class="ae jg" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>。</p><h1 id="be7f" class="ly lz jj bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">延伸阅读</h1><ul class=""><li id="9cb7" class="nu nv jj kr b ks mw kw mx la oj le ok li ol lm nz oa ob oc bi translated"><a class="ae jg" href="https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28" rel="noopener" target="_blank">NLP中的数据扩充</a></li><li id="e4a8" class="nu nv jj kr b ks od kw oe la of le og li oh lm nz oa ob oc bi translated"><a class="ae jg" href="https://towardsdatascience.com/data-augmentation-library-for-text-9661736b13ff" rel="noopener" target="_blank">文本的数据扩充</a></li><li id="4780" class="nu nv jj kr b ks od kw oe la of le og li oh lm nz oa ob oc bi translated"><a class="ae jg" href="https://towardsdatascience.com/data-augmentation-for-audio-76912b01fdf6" rel="noopener" target="_blank">音频数据增强</a></li><li id="d042" class="nu nv jj kr b ks od kw oe la of le og li oh lm nz oa ob oc bi translated"><a class="ae jg" href="https://towardsdatascience.com/data-augmentation-for-speech-recognition-e7c607482e78" rel="noopener" target="_blank">声谱图数据增强</a></li><li id="89bd" class="nu nv jj kr b ks od kw oe la of le og li oh lm nz oa ob oc bi translated"><a class="ae jg" href="https://hackernoon.com/does-your-nlp-model-able-to-prevent-adversarial-attack-45b5ab75129c" rel="noopener ugc nofollow" target="_blank">您的NLP模型能够防止恶意攻击吗？</a></li><li id="b604" class="nu nv jj kr b ks od kw oe la of le og li oh lm nz oa ob oc bi translated"><a class="ae jg" href="https://github.com/DeepVoltaire/AutoAugment" rel="noopener ugc nofollow" target="_blank">非官方自动增强实现</a></li></ul><h1 id="1470" class="ly lz jj bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">参考</h1><ul class=""><li id="82fd" class="nu nv jj kr b ks mw kw mx la oj le ok li ol lm nz oa ob oc bi translated">R.森里奇，b .哈多和一棵桦树。<a class="ae jg" href="https://arxiv.org/pdf/1511.06709.pdf" rel="noopener ugc nofollow" target="_blank">用单语数据改进神经机器翻译模型</a>。2015</li><li id="511a" class="nu nv jj kr b ks od kw oe la of le og li oh lm nz oa ob oc bi translated">E.D. Cubuk、B. Zoph、D. Mane、V. Vasudevan和Q. V. Le。自动增强:从数据中学习增强策略。2018</li><li id="103a" class="nu nv jj kr b ks od kw oe la of le og li oh lm nz oa ob oc bi translated">谢，戴，贺维，梁明堂，乐庆伟。<a class="ae jg" href="https://arxiv.org/pdf/1904.12848.pdf" rel="noopener ugc nofollow" target="_blank">无监督数据增强</a>。2019</li></ul></div></div>    
</body>
</html>