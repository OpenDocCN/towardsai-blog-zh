<html>
<head>
<title>SiEBERT, RoBERTa, and BERT: Which One to Implement in 2022?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">西伯特、罗伯塔和伯特:2022年实施哪一个？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/siebert-roberta-and-bert-which-one-to-implement-in-2022-2964cb3a40b6?source=collection_archive---------0-----------------------#2022-07-03">https://pub.towardsai.net/siebert-roberta-and-bert-which-one-to-implement-in-2022-2964cb3a40b6?source=collection_archive---------0-----------------------#2022-07-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2a48" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这些实施管道的介绍和最新更新。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5f8846129024a27a53ede3da332bb208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-6HhemojYulOWvXm8Ku3gg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Unsplash的安迪·凯利</figcaption></figure><h1 id="f820" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">伯特简介:</h1><p id="f3c5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">BERT是谷歌为自然语言处理(NLP)设计的深度学习模型。BERT旨在使机器能够以类似于人类的方式理解文本上下文。为此，BERT使用了一种叫做双向编码的技术。这意味着它从左到右和从右到左查看文本，以提高几种自然语言理解任务的技术水平。例如，这种分析文本的方法允许BERT捕捉单词前后的上下文。因此，BERT在几个NLP任务中非常有效，包括问题回答、文本分类和语言建模。</p><p id="7970" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">BERT基于Transformer模型，这是一种用于解析和处理序列数据的神经网络。Transformer模型最初是在论文“注意力是你所需要的”(Vaswani等人，2017年)中介绍的，此后在各种NLP任务中取得了成功。BERT已经被用于构建一些有用的应用程序。例如，拥抱人脸库提供了一组预训练的模型，可用于文本分类、问题回答和其他任务。此外，该模型可以在特定任务上进行微调，如情感分析，以实现更好的性能。</p><p id="0d75" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该模型旨在成为自然语言理解的通用模型(NLU)，可用于文本分类等广泛的任务。BERT的关键优势在于它可以应用于任何语言。此外，该模型是在大型文本语料库(包括整个维基百科)上预先训练的，因此很容易使用少量训练数据针对特定任务对模型进行微调。</p><p id="715c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用BERT的主要优势之一是，它可以帮助您在处理机器学习模型时避免潜在的陷阱。例如，如果您正在训练一个模型来对文本进行分类，如果训练数据不平衡，您可能会无意中引入偏差。使用BERT，您可以在平衡的数据集上训练您的模型，这有助于减少偏差。此外，通过提供上下文中单词含义的信息，BERT可以帮助您提高模型的性能。如果您使用的是低资源语言，这种方法会很有帮助。此外，BERT可用于预训练您的模型。如果你想使用一个最先进的模型，但是没有资源从头开始训练它，那么预训练可能是有益的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/5bfd22dc694ab0f4a5ffd1108ca6236f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uGKJ8Qo6rF0YpFsxLIWD7A.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@tetrakiss?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿森托古列夫</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">的Unsplash </a></figcaption></figure><h1 id="02c3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">以下是BERT的7大优势:</h1><p id="bca8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.最先进的自然语言处理:BERT为许多自然语言处理任务的准确性设立了新的标准，包括问题回答、文本分类和序列标记。</p><p id="e610" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.更少的假设:大多数以前的NLP模型对输入文本的结构做了强有力的假设，例如它是一个结构良好的句子。BERT做了更少的假设，使得它更加灵活和准确。</p><p id="0b99" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.双向:BERT是一个双向模型，这意味着它从两个方向(左和右)考虑单词的上下文。这一点很重要，因为许多单词根据上下文可能有不同的意思。</p><p id="ae54" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.预训练:BERT是在大型文本语料库上进行预训练的，可以快速适应任何新任务。</p><p id="1043" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.迁移学习:BERT可以用于各种任务，只需很少或不需要特定任务的培训。这使得训练和部署非常有效。</p><p id="d463" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.可伸缩性:BERT是高度可伸缩的，可以在大型数据集上进行训练。</p><p id="7f25" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">7.开源:BERT是开源的，任何人都可以使用和改进。</p><p id="3fad" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">BERT是自然语言处理中的一个重要突破，因为它首次代表了一个模型可以从无标签文本中预训练深度双向表示。这是一项重大成就，因为它允许模型用于各种任务，而不需要特定任务的训练数据。随着NLU的使用越来越广泛，BERT可能会成为开发人员越来越流行的工具。此外，该模型的灵活性和易用性使其成为那些希望创建复杂的NLP应用程序的人的宝贵资产。</p><h1 id="798f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">罗伯塔简介:</h1><p id="05f9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">RoBERTa是脸书为自然语言处理(NLP)模型开发的训练和评估工具。它被设计成流行的BERT NLP模型的一个更加健壮和灵活的版本。RoBERTa在几项自然语言处理任务上的表现超过了BERT，包括句子分类、问题回答和自然语言推理。</p><h1 id="5222" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">RoBERTa可用于NLP任务的10个示例:</h1><p id="7d15" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.情感分析</p><p id="f480" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.主题分类</p><p id="2f85" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.文本摘要</p><p id="0f66" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.问题回答</p><p id="8708" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.对话生成</p><p id="13de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.文本到语音转换</p><p id="7641" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">7.图像字幕</p><p id="8582" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">8.语音识别</p><p id="d18a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">9.机器翻译</p><p id="5652" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">10.自然语言理解</p><h1 id="0de2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">与BERT相比，RoBERTa及其灵活性和稳健性的10种解释:</h1><p id="0c10" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.它使用了比BERT更广泛的训练数据集，包含超过1.6亿个句子。</p><p id="b5b6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.它使用比BERT更深的变压器模型，有24个变压器层，而不是12个。</p><p id="7dde" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.使用屏蔽语言建模目标而不是像BERT那样的下一句预测目标来训练它。这使得RoBERTa能够更好地模拟句子中单词的上下文。</p><p id="fba4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.它使用的词汇量比BERT大得多，超过100万个单词。</p><p id="5ce2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.它使用子词标记化，这允许它更好地处理词汇表外的词。</p><p id="4a95" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.它使用动态屏蔽进行训练，动态屏蔽随机屏蔽一定比例的输入标记，而不是总是屏蔽相同的标记。</p><p id="e1be" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">7.它在比BERT更长的序列上被训练，多达512个记号。</p><p id="8071" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">8.它使用一种称为似然比训练的技术，旨在鼓励模型预测更加自信。</p><p id="c943" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">9.它包括一个句子级目标，鼓励模型保持句子之间的衔接。</p><p id="a85a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">10.它建立在流行的PyTorch深度学习框架之上，更易于使用和扩展。</p><h1 id="69ec" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关于罗伯塔内部运作的9个解释:</h1><p id="b7f6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.这是一个更新的模型，因此受益于Transformer架构的进步(例如，它使用更大的训练语料库和更多的层)。</p><p id="b21c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.它更能抵抗训练数据的过拟合。</p><p id="b0cc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.它在几个自然语言理解任务上取得了较好的性能。</p><p id="f93e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.它使用字节对编码(BPE ),而不是像BERT那样的单词块编码，这有助于它更好地处理词汇表以外的单词。</p><p id="cd3a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.它有一个更有效的训练动态掩蔽方案。</p><p id="e782" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.它集成了来自变压器(BERT)和OpenAI GPT(单向变压器模型)的双向编码器表示的思想。</p><p id="b737" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">7.它被设计为对敌对输入(例如，被操纵来欺骗机器学习模型的文本数据)更加鲁棒。</p><p id="621a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">8.它可以处理比BERT更长的序列，这使它更适合于回答需要更多上下文的问题等任务。</p><p id="6424" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">9.它使用一种叫做知识蒸馏的技术来提高下游任务的性能。</p><h1 id="6575" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">西伯特简介:</h1><p id="2f42" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.它能够对各种类型的英语文本进行可靠的二元情感分析。对于每一种情况，它预测积极(1)或消极(0)情绪[3]。</p><p id="bf56" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.该模型在许多任务上取得了最先进的结果，包括情感分析、问题回答和文本分类。</p><p id="4ca3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.该模型是开源的，可以在GitHub上获得。</p><h1 id="fe69" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">SiEBERT的7大优势:</h1><p id="a7d7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.它是一个预先训练的语言模型，可以针对各种NLP任务进行微调，如文本分类、序列标记和问题回答。</p><p id="7925" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.在各种NLP任务中，它已经被证明优于其他预先训练的语言模型，例如BERT。</p><p id="e864" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.它比其他预先训练的语言模型需要更少的数据，这使它成为低资源语言的理想选择。</p><p id="e198" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.它对不在词汇表中的单词是健壮的，并且可以处理输入文本中的拼写错误和错误。</p><p id="ad4b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.它在大型数据语料库上训练，比其他在较小数据集上训练的模型更准确。</p><p id="137c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.它有英语和德语两种版本，是多语言应用的好选择。</p><p id="417b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">7.它是开源的，可以免费使用。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="0e83" class="na la it bd lb nb nc dn lf nd ne dp lj ma nf ng ll me nh ni ln mi nj nk lp nl bi translated">这是对伯特、罗伯塔和西伯特的高层次介绍。请pm我，如果你有任何主题，你想让我涵盖这些实施。</h2><h1 id="80f5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">另外，请考虑订阅我的每周简讯:</h1><div class="nm nn gp gr no np"><a href="https://pventures.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">周日报告#1</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">设计思维与AI的共生关系设计思维能向AI揭示什么，AI又能如何拥抱…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">pventures.substack.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od ks np"/></div></div></a></div><p id="c256" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">参考资料:</p><p id="b181" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">1.facebook/opt-30b拥抱脸。https://huggingface.co/facebook/opt-30b<a class="ae ky" href="https://huggingface.co/facebook/opt-30b" rel="noopener ugc nofollow" target="_blank"/></p><p id="9cf6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.GRUBERT:一种基于GRU的方法来融合Twitter的BERT隐藏层。【https://aclanthology.org/2020.aacl-srw.19.pdf T4】</p><p id="215b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.<a class="ae ky" href="https://huggingface.co/siebert/sentiment-roberta-large-english" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/siebert/sensation-Roberta-large-English</a></p><p id="1f93" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.<a class="ae ky" href="https://huggingface.co/xlm-roberta-base" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/xlm-roberta-base</a></p><p id="5dae" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.塞伯特研究:<a class="ae ky" href="https://deliverypdf.ssrn.com/delivery.php?ID=297024085004102027025078086080081099024027003059021038093122011009025122091030102022021026029022118061047123124111121104064093044038034079014003025084066093096104076035053051120118024001007126089099083086107123025087027022014096015111089068067019082106&amp;EXT=pdf&amp;INDEX=TRUE" rel="noopener ugc nofollow" target="_blank">https://deliverypdf.ssrn.com/delivery.php?ID = 29702408500410202702507808608008109902402700305902103809312201100902512209103020220202102602902118061047123124112040640640112112121212121212120202020404121212040406404012121212</a></p></div></div>    
</body>
</html>