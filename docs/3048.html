<html>
<head>
<title>10 Commonly Used Loss Functions Explained with Python Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python代码解释的10个常用损失函数</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/10-commonly-used-loss-functions-explained-with-python-code-59967e1f3c8d?source=collection_archive---------2-----------------------#2022-08-17">https://pub.towardsai.net/10-commonly-used-loss-functions-explained-with-python-code-59967e1f3c8d?source=collection_archive---------2-----------------------#2022-08-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cdda" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解机器学习中的损失函数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/74dc6dd7aaa39be75a7195f7ced0ac02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bO3QUuxOYUJ0HwXG"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">麦克斯韦·尼尔森在T2的照片</figcaption></figure><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="99eb" class="le lf it la b gy lg lh l li lj"><strong class="la iu">Table of contents:<br/></strong>What is a loss function?<br/>Loss Functions vs Metrics<br/>Why are loss functions applied while building the model?</span><span id="4c2d" class="le lf it la b gy lk lh l li lj"><strong class="la iu">Regression:<br/></strong>Mean Squared Error (MSE)<br/>Mean Absolute Error (MAE)<br/>Root Mean Squared Error (RMSE)<br/>Mean Bias Error (MBE)<br/>Huber Loss</span><span id="6a6a" class="le lf it la b gy lk lh l li lj"><strong class="la iu">Binary Classification:<br/></strong>Binary Cross Entropy (BCE)<br/>Hing Loss and Squared Hing Loss (HL and SHL)<br/>Likelihood Loss (LHL)</span><span id="78e7" class="le lf it la b gy lk lh l li lj"><strong class="la iu">Multinomial Classification:<br/></strong>Categorical Cross Entropy (CCE)<br/>Kullback-Leibler Divergence (KLD)</span></pre><h2 id="a0cd" class="le lf it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">什么是损失函数？</h2><p id="d363" class="pw-post-body-paragraph mf mg it mh b mi mj ju mk ml mm jx mn ls mo mp mq lw mr ms mt ma mu mv mw mx im bi translated">损失函数是一种衡量模型与数据拟合程度的算法。损失函数测量实际测量值和预测值之间的距离。这样，损失函数的值越高，预测就越错误。相反，具有较低值的损失函数意味着预测更接近真实值。为每个单独的观察值(数据点)计算损失函数。平均所有损失函数的值的函数被称为<strong class="mh iu"> <em class="my">成本函数</em> </strong>。</p><p id="5f6f" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">损失函数:</strong>计算每个样本的误差。<br/> <strong class="mh iu">代价函数:</strong>所有损失函数的平均值。</p><h2 id="711e" class="le lf it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">损失函数与度量</h2><p id="ad45" class="pw-post-body-paragraph mf mg it mh b mi mj ju mk ml mm jx mn ls mo mp mq lw mr ms mt ma mu mv mw mx im bi translated">一些损失函数也被用作评估指标。然而，损失函数和度量具有不同的目的。虽然度量用于评估最终模型的性能并比较不同模型的性能，但是损失函数在模型构建阶段用作正在创建的模型的优化器。损失函数指导模型如何最小化误差。</p><p id="a591" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">度量:</strong>模型与数据的吻合程度<br/> <strong class="mh iu">损失函数:</strong>模型与数据的吻合程度</p><h2 id="0504" class="le lf it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">为什么在构建模型时要应用损失函数？</h2><p id="af8d" class="pw-post-body-paragraph mf mg it mh b mi mj ju mk ml mm jx mn ls mo mp mq lw mr ms mt ma mu mv mw mx im bi translated">由于损失函数测量预测值和实际值之间的距离，因此在构建模型时会使用损失函数，以便以一种改进的方式(通常是梯度下降，但我将在另一篇文章中讨论)来指导模型。这意味着，在构建模型的过程中，如果要素的权重发生变化，我们可以做出更好或更差的预测。损失函数用于告知模型应该更改的要素权重以及更改的方向。</p><blockquote class="ne nf ng"><p id="3a74" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi translated">我们可以在机器学习中使用多种损失函数，这取决于我们试图解决的问题类型、数据质量和分布以及我们使用的算法。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/813b268fc326f20c6a8c902bdf83d7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RFCtBB4as5O8LHR1J5gKzA.png"/></div></div></figure><h2 id="f400" class="le lf it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated"><strong class="ak">回归问题:</strong></h2><p id="7188" class="pw-post-body-paragraph mf mg it mh b mi mj ju mk ml mm jx mn ls mo mp mq lw mr ms mt ma mu mv mw mx im bi translated"><strong class="mh iu">均方误差(MSE) <br/> </strong>均方误差求所有预测值与真实值的平方差，并将其全部平均。它用于回归问题。</p><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="cc52" class="le lf it la b gy lg lh l li lj"><strong class="la iu">def <em class="my">MSE</em></strong> (y, y_predicted):<br/>   sq_error = (y_predicted - y) ** 2<br/>   sum_sq_error = np.sum(sq_error)<br/>   mse = sum_sq_error/y.size<br/>   <strong class="la iu">return</strong> mse</span></pre><p id="ef60" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">平均绝对误差(MAE) <br/> </strong>该损失函数计算为预测值和真实值之间的绝对差值的平均值。当数据有异常值时，这是比均方误差更好的度量。</p><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="f020" class="le lf it la b gy lg lh l li lj"><strong class="la iu">def <em class="my">MAE</em></strong> (y, y_predicted):<br/>   error = y_predicted - y<br/>   absolute_error = np.absolute(error)<br/>   total_absolute_error = np.sum(absolute_error)<br/>   mae = total_absolute_error/y.size<br/>   <strong class="la iu">return</strong> mae</span></pre><p id="7a29" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">均方根误差【RMSE】<br/></strong>是均方误差的平方根。如果我们不想惩罚更大的错误，这是一个理想的转换。</p><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="da05" class="le lf it la b gy lg lh l li lj"><strong class="la iu">def<em class="my"> RMSE</em></strong> (y, y_predicted):<br/>   sq_error = (y_predicted - y) ** 2<br/>   total_sq_error = np.sum(sq_error)<br/>   mse = total_sq_error/y.size<br/>   rmse = math.sqrt(mse)<br/>   <strong class="la iu">return</strong> rmse</span></pre><p id="4c09" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">平均偏差误差(MBE) <br/> </strong>类似于平均绝对误差，但不使用绝对函数。缺点是负误差和正误差可以相互抵消，所以当研究者知道误差只有一个方向时更适用。</p><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="2d28" class="le lf it la b gy lg lh l li lj"><strong class="la iu">def <em class="my">MBE</em></strong> (y, y_predicted):<br/>   error = y_predicted -  y<br/>   total_error = np.sum(error)<br/>   mbe = total_error/y.size<br/>  <strong class="la iu"> return</strong> mbe</span></pre><p id="a0fe" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">休伯损失<br/> </strong>休伯损失函数结合了平均绝对误差(MAE)和均方误差(MSE)的优势。这是因为哈伯损失是一个有两个分支的函数。一个分支应用于符合期望值的MAE，另一个分支应用于异常值。哈伯损耗的一般函数是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/3c15c18c6bde8b90bd981ce557a484c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNtaWpY4Nxgj6ppOxYQ8pA.png"/></div></div></figure><p id="b599" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><em class="my">其中:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/d83d64051855e00942ee0fbb86e88567.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*HjfuAlhcT0LyaNB3sjzoDQ.png"/></div></figure><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="04c7" class="le lf it la b gy lg lh l li lj">- We already know how to find MAE from previous code examples.</span><span id="a246" class="le lf it la b gy lk lh l li lj"><strong class="la iu">def</strong> <strong class="la iu"><em class="my">hubber_loss</em></strong> (y, y_predicted, delta)<br/>   delta = 1.35 * MAE<br/>   y_size = y.size<br/>   total_error = 0<br/>   <strong class="la iu">for</strong> i <strong class="la iu">in </strong>range (y_size):<br/>      erro = np.absolute(y_predicted[i] - y[i])<br/>      <strong class="la iu">if </strong>error &lt; delta:<br/>         hubber_error = (error * error) / 2<br/>      <strong class="la iu">else:</strong><br/>         hubber_error = (delta * error) / (0.5 * (delta * delta))<br/>      total_error += hubber_error<br/>   total_hubber_error = total_error/y.size<br/>   <strong class="la iu">return</strong> total_hubber_error</span></pre><p id="b0b6" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">二元分类:</strong></p><p id="0f6f" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">似然损失(LHL) <br/> </strong>该损失函数主要用于二分类问题。输出是地面真实标签的概率的乘积，并且相关联的成本函数是所有观察的平均值。下面举个二进制分类的例子，类别是[0]或者[1]。如果我们有一个输出概率等于或高于0.5，那么预测的类是[1]，否则是[0]。示例输出概率为:</p><blockquote class="ne nf ng"><p id="a42d" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi">[0.3 , 0.7 , 0.8 , 0.5 , 0.6 , 0.4]</p></blockquote><p id="12f9" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">相应的预测类别有:</p><blockquote class="ne nf ng"><p id="670b" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi">[0 , 1 , 1 , 1 , 1 , 0]</p></blockquote><p id="6acb" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">真正的类别包括:</p><blockquote class="ne nf ng"><p id="09ee" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi">[0 , 1 , 1 , 0 , 1 , 0]</p></blockquote><p id="5a9c" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">我们现在将使用真实类别和输出概率来计算可能性损失。如果真实类是<strong class="mh iu"><em class="my">【1】</em></strong>，我们用<strong class="mh iu"> <em class="my">输出概率。如果</em> </strong>真类是<strong class="mh iu"><em class="my"/></strong>，我们用<strong class="mh iu"><em class="my">1-输出概率</em> </strong>:</p><blockquote class="ne nf ng"><p id="d385" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi translated"><strong class="mh iu">总成本</strong>=((1–0.3)+0.7+0.8+(1–0.5)+0.6+(1–0.4))/6 =<strong class="mh iu">0.65</strong></p></blockquote><p id="3716" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">您还可以使用以下代码来计算可能性损失:</p><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="69d7" class="le lf it la b gy lg lh l li lj"><strong class="la iu">def <em class="my">LHL</em> </strong>(y, y_predicted):<br/>   likelihood_loss = (y * y_predicted) + ((1-y) * (y_predicted))<br/>   total_likelihood_loss = np.sum(likelihood_loss)<br/>   lhl = - total_likelihood_loss / y.size<br/>   <strong class="la iu">return</strong> lhl</span></pre><p id="1954" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">二元交叉熵</strong></p><p id="ce72" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">这个函数是对数似然损失的一个修正。增加对数有能力惩罚非常自信和非常错误的预测。二元交叉熵损失函数的一般公式为:</p><blockquote class="nn"><p id="aa71" class="no np it bd nq nr ns nt nu nv nw mx dk translated">— (y . log (p) + (1 — y)。对数(1 — p))</p></blockquote><p id="ddd9" class="pw-post-body-paragraph mf mg it mh b mi nx ju mk ml ny jx mn ls nz mp mq lw oa ms mt ma ob mv mw mx im bi translated">让我们用上一个示例中的值来尝试一下:</p><p id="fa42" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu"> <em class="my">输出概率</em></strong><em class="my">=【0.3，0.7，0.8，0.5，0.6，0.4】<br/></em><strong class="mh iu"><em class="my">真实类</em></strong><em class="my">=【0，1，1，0，1，0】</em></p><p id="d97a" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">— (0 .log(0.3)+(1–0)。log(1–0.3))= 0.155<br/>—(1。log(0.7)+(1–1)。log (0.3)) = 0.155 <br/> — (1。log(0.8)+(1–1)。log (0.2)) = 0.097 <br/> — (0。log(0.5)+(1–0)。log(1–0.5))= 0.301<br/>—(1。log(0.6)+(1–1)。log (0.4)) = 0.222 <br/> — (0。log(0.4)+(1–0)。log(1–0.4))= 0.222</p><p id="d0ef" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu"><em class="my"/></strong>=(0.155+0.155+0.097+0.301+0.222+0.222)/6 =<strong class="mh iu"><em class="my">0.192</em></strong></p><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="8a9e" class="le lf it la b gy lg lh l li lj"><strong class="la iu">def <em class="my">BCE</em></strong> (y, y_predicted):<br/>   ce_loss = y*(np.log(y_predicted))+(1-y)*(np.log(1-y_predicted))<br/>   total_ce = np.sum(ce_loss)<br/>   bce = - total_ce/y.size<br/>   <strong class="la iu">return</strong> bce</span></pre><p id="d3be" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu">铰链损失和平方兴损失(HL和SHL) <br/> </strong>铰链损失主要用于评估SVM模型。随着铰链的丢失，错误的预测和不太自信的正确预测会受到惩罚。一般损失函数是:</p><blockquote class="nn"><p id="f0f2" class="no np it bd nq nr ns nt nu nv nw mx dk translated">l(y) = max (0，1 — t . y)</p></blockquote><p id="e3ec" class="pw-post-body-paragraph mf mg it mh b mi nx ju mk ml ny jx mn ls nz mp mq lw oa ms mt ma ob mv mw mx im bi translated">其中<strong class="mh iu"> <em class="my"> t </em> </strong>为真实结局<strong class="mh iu"><em class="my">【1】</em></strong>或<strong class="mh iu"> <em class="my"> [-1]。</em> </strong></p><p id="4a23" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">要应用铰链损耗，类别应该是[1]或[-1](不是[0])。为了在铰链损失函数中不被惩罚，观测值不仅需要被正确分类，而且到超平面的距离应该大于余量(有把握的正确预测)。如果我们想进一步惩罚更高的误差，我们可以用MSE中使用的类似方法计算铰链损耗值的平方。</p><p id="2ce4" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">记住，在SVM，超平面的边缘越高，某个预测就越有把握。你可以在本文中重述SVM的概念:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/07bc857253858c3fd075df2bd6c12225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OzyHpuReiPaR4Lp43R84-w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/support-vector-machine-svm-for-binary-and-multiclass-classification-hands-on-with-scikit-learn-29cdbe5cb90e">https://pub . toward sai . net/support-vector-machine-SVM-for-binary-and-multi class-classification-hand-on-with-sci kit-learn-29 cdbe 5 CB 90 e</a></figcaption></figure><p id="c238" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">现在让我们看一个可视化的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/dacd5c91009fe82a72e00226c5cdb4e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*Qju1O4Tk2ECb__nCHqp3Qw.jpeg"/></div></figure><p id="212c" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">→如果预测的结果为1.5，真实类别为[1]，则损失将为0(零)，因为模型的可信度很高。</p><blockquote class="ne nf ng"><p id="1678" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi translated">损失=最大值(0，1–1 * 1.5)=最大值(0，-0.5) = 0</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/43316ed2410287b2c43789878708bf2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*Nk_n2pTcrCX1nucikd1T7w.jpeg"/></div></figure><p id="0d48" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">→如果一个观测的结果为0(零)，说明该观测在边界内(超平面)，真类为[-1]。成本为1，模型没有错也没有对，置信度非常低。</p><blockquote class="ne nf ng"><p id="116d" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi translated">loss = max (0，1-(-1)* 0)= max(0，1) = 1</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/10f7018a826fe70b3f942b32d09afb46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*oQaedeK40EoZ16V4DaQeOA.jpeg"/></div></div></figure><p id="9b7d" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">→如果一个观察的结果为2，但被错误分类(乘以[-1])，我们的距离为-2。成本是3(非常高)，因为我们的模型对一个错误的决策非常有信心。</p><blockquote class="ne nf ng"><p id="fac2" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi translated">loss = max (0，1 — (-1)。2) = max (0，1+2) = max (0，3) = 3</p></blockquote><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="2cff" class="le lf it la b gy lg lh l li lj"><strong class="la iu">#Hinge Loss<br/>def <em class="my">Hinge</em></strong> (y, y_predicted):<br/>   hinge_loss = np.sum(max(0 , 1 - (y_predicted * y)))<br/>   <strong class="la iu">return</strong> hinge_loss</span><span id="bad6" class="le lf it la b gy lk lh l li lj"><strong class="la iu">#Squared Hinge Loss</strong><br/><strong class="la iu">def <em class="my">SqHinge</em></strong> (y, y_predicted):<br/>   sq_hinge_loss = max (0 , 1 - (y_predicted * y)) ** 2<br/>   total_sq_hinge_loss = np.sum(sq_hinge_loss)<br/>   <strong class="la iu">return</strong> total_sq_hinge_loss</span></pre><h2 id="37c7" class="le lf it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated"><strong class="ak">多项分类:</strong></h2><h2 id="06cc" class="le lf it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated"><strong class="ak">范畴交叉熵(CCE) </strong></h2><p id="8111" class="pw-post-body-paragraph mf mg it mh b mi mj ju mk ml mm jx mn ls mo mp mq lw mr ms mt ma mu mv mw mx im bi translated">在多项式分类中，我们使用与二元交叉熵相似的公式，但是多了一个步骤。w首先需要计算每一对[y，y_predicted]的损耗，一般公式为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/79d8cde886ba1dd2fdbba499bd672bac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*YUegp78Y0dNVYjiY4a3x1w.png"/></div></figure><p id="297f" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">如果我们有一个三类问题，其中单个[y，y_predicted]对的输出是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/488853ddc43f162f80e577d2c3e31a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*Eha3snMwncadCZiAWQ5pWg.png"/></div></figure><p id="04f5" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">真实类是3，我们的模型是0.7自信真实类是3。要计算这一对的损耗:</p><blockquote class="ne nf ng"><p id="fda2" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi translated"><strong class="mh iu">损失</strong> = 0。log (0.1) + 0。log (0.2) + 1。log (0.7) = <strong class="mh iu"> -0.155 </strong></p></blockquote><p id="c21a" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">要获得成本函数的值，我们需要计算所有单个对的损失，将它们相加，最后乘以[-1/样本数]。成本函数由以下公式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/62d2f978cf11bc4f5082fdd39a24a494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DAynQBnP9ENB1d1iLLtq_Q.png"/></div></div></figure><p id="fae5" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">使用上面的例子，如果我们有第二对:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/facffc2bd25772b27a8ced89d8abe943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*hjHr4ezdLVx_d1sIGtW-ug.png"/></div></figure><blockquote class="ne nf ng"><p id="6fb3" class="mf mg my mh b mi mz ju mk ml na jx mn nh nb mp mq ni nc ms mt nj nd mv mw mx im bi translated"><strong class="mh iu">损耗</strong> = 0。log (0.4) + 1。log (0.4) + 0。log (0.2) = <strong class="mh iu"> -0.40 </strong></p></blockquote><p id="271c" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">成本函数的计算方法如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/9a25bbfca52770d3235db7e6b477a38a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S6wtQKQikMs3sq8Uefphgg.png"/></div></div></figure><p id="442e" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">使用代码示例会更容易理解:</p><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="9708" class="le lf it la b gy lg lh l li lj"><strong class="la iu">def <em class="my">CCE</em> </strong>(y, y_predicted):<br/>   cce_class = y * (np.log(y_predicted))<br/>   sum_totalpair_cce = np.sum(cce_class)<br/>   cce = - sum_totalpair_cce / y.size<br/>   <strong class="la iu">return</strong> cce</span></pre><p id="eeb9" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated"><strong class="mh iu"> Kullback-Leibler散度(KLD) <br/> </strong>类似于范畴交叉熵，但也考虑到了观察值出现的概率。如果我们的班级不平衡，这一点特别有用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/61eeca7fc162fcf9054cc4cf971bb1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*q3Qxorr4hp52TA62vjEsKQ.png"/></div></figure><p id="1f52" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">代码是:</p><pre class="kj kk kl km gt kz la lb lc aw ld bi"><span id="f057" class="le lf it la b gy lg lh l li lj"><strong class="la iu">def<em class="my"> KL</em></strong> (y, y_predicted):<br/>   kl = y * (np.log(y / y_predicted))<br/>   total_kl = np.sum(kl)<br/>   <strong class="la iu">return</strong> total_kl</span></pre><p id="895f" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">感谢您的阅读！如果您有建议要添加到这个列表中，请告诉我，不要忘记订阅以接收关于我未来出版物的通知。</p><p id="449d" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">如果:你喜欢这篇文章，别忘了关注我，这样你就能收到所有关于新出版物的更新。</p><p id="b4be" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">否则如果:你想了解更多，你可以通过<a class="ae ky" href="https://cdanielaam.medium.com/membership" rel="noopener">我的推荐链接</a>订阅媒体会员。这不会花你更多的钱，但是你要付我一杯咖啡的钱。</p><p id="95a3" class="pw-post-body-paragraph mf mg it mh b mi mz ju mk ml na jx mn ls nb mp mq lw nc ms mt ma nd mv mw mx im bi translated">Else:谢谢！</p></div></div>    
</body>
</html>