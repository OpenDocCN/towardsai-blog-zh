<html>
<head>
<title>Naive Bayes Classifiers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类器</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/naive-bayes-classifiers-79ac2c805f3f?source=collection_archive---------1-----------------------#2020-02-24">https://pub.towardsai.net/naive-bayes-classifiers-79ac2c805f3f?source=collection_archive---------1-----------------------#2020-02-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1c8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文讨论朴素贝叶斯分类器背后的理论及其实现。</p><p id="c2eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">朴素贝叶斯分类器是基于<strong class="jp ir">贝叶斯定理</strong>的分类算法集合。它不是一个单一的算法，而是一个算法家族，所有算法都有一个共同的原则，即每一对被分类的特征都是相互独立的。</p><p id="37e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，让我们考虑一个数据集。</p><p id="f6f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑一个虚构的数据集，它描述了打高尔夫球的天气条件。给定天气条件，每个元组将条件分类为适合(“是”)或不适合(“否”)打高尔夫。</p><p id="dede" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我们数据集的表格表示。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/950f4abb7f749dc5e821526ac08f2a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ig2VmPFbklD5MW_UdtOEEQ.png"/></div></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kx"><img src="../Images/7d83805071aa4636c105276606c71a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6AvE6q8m1ZyoiCmgnIt3HQ.png"/></div></div></figure><p id="1235" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据集分为两部分，即<strong class="jp ir">特征矩阵</strong>和<strong class="jp ir">响应向量</strong>。</p><ul class=""><li id="a017" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">特征矩阵包含数据集的所有向量(行)，其中每个向量由<strong class="jp ir">相关特征</strong>的值组成。在上面的数据集中，特征是“前景”、“温度”、“湿度”和“有风”。</li><li id="4a59" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">响应向量包含特征矩阵每一行的<strong class="jp ir">类变量</strong>(预测或输出)的值。在上面的数据集中，类变量名为“Play golf”</li></ul><p id="8faf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">假设:</strong></p><p id="5818" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">朴素贝叶斯的基本假设是，每个特征构成一个:</p><ul class=""><li id="fa6c" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">自主的</li><li id="8084" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">平等的</li></ul><p id="cdcd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对结果的贡献。</p><p id="65a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于我们的数据集，这个概念可以理解为:</p><ul class=""><li id="b728" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">我们假设没有一对特征是相互依赖的。例如，温度“热”与湿度无关，或者天气“多雨”对风没有影响。因此，这些特征被认为是独立于<strong class="jp ir">和</strong>的。</li><li id="57d0" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">其次，每个特征被赋予相同的权重(或重要性)。比如只知道唯一的温度和湿度，是无法准确预测结果的。没有一个属性是不相关的，并被假定为对结果的贡献<strong class="jp ir">相等</strong>。</li></ul><p id="eac7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注:</strong>朴素贝叶斯做出的假设在现实情况下一般不正确。独立性假设从来都是不正确的，但在实践中往往很有效。</p><p id="a763" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，在讨论朴素贝叶斯公式之前，了解贝叶斯定理是很重要的。</p><p id="27f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">贝叶斯定理根据已经发生的另一个事件的概率，求出一个事件发生的概率。贝叶斯定理在数学上表述为以下等式:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/12ff80518bb24783d7af5e392f418fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/1*M6jDvGQQkpEYG3LWBEvA9Q.gif"/></div></figure><p id="7cc5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中A和B是事件，P(B)是。0.</p><ul class=""><li id="caf0" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">假设事件B为真，我们试图找出事件A的概率。事件B也被称为<strong class="jp ir">证据</strong>。</li><li id="ec4d" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">P(A)是A的优先级(先验概率，即事件在证据出现之前的概率)。证据是一个未知实例的属性值(这里是事件B)。</li><li id="c0b5" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">P(A|B)是B的后验概率，即看到证据后事件的概率。</li></ul><p id="bd6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，关于我们的数据集，我们可以按以下方式应用贝叶斯定理:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/081c122a8865892a3ee1dc32a1223b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*1cM8AQqH-tt05vXzHXb6NQ.png"/></div></figure><p id="8b6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中y是类变量，X是从属特征向量(大小为<em class="lo"> n </em>),其中:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/cbdee02ec7b6a3f78aa74121b1fafdc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*fH08PSNnzb_-0vH01k9waA.png"/></div></figure><p id="6a87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了澄清，特征向量和相应的类变量的例子可以是:(参考数据集的第一行)</p><pre class="km kn ko kp gt lq lr ls lt aw lu bi"><span id="d4e8" class="lv lw iq lr b gy lx ly l lz ma">X = (Rainy, Hot, High, False)<br/>y = No</span></pre><p id="ebb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，基本上，P(X|y)在这里意味着“不打高尔夫球”的概率，假设天气条件是“有雨”、“温度高”、“湿度大”和“无风”。</p><p id="8350" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">天真的假设</strong></p><p id="3bd8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，是时候给贝叶斯定理一个天真的假设了，它是特征中的<strong class="jp ir">独立性</strong>。所以现在，我们将<strong class="jp ir">证据</strong>分割成独立的部分。</p><p id="cc4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，如果任意两个事件A和B是独立的，那么，</p><pre class="km kn ko kp gt lq lr ls lt aw lu bi"><span id="2782" class="lv lw iq lr b gy lx ly l lz ma">P(A,B) = P(A)P(B)</span></pre><p id="cf5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们得出结果:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/4229ed7e0d4574de51ce8043aaf6d4eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*qoaWHjV3lFCaSAGsC_PVEQ.png"/></div></figure><p id="4133" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这可以表示为:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/46b69363ae80a89eb93a47384771d467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*w-dS4AZ2jPCuIHv21xwvcw.png"/></div></figure><p id="1ee6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，对于给定的输入，分母保持不变，我们可以去掉这一项:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi md"><img src="../Images/ad5c2b8f3d69e9f3fba1fd8a9e8a807c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*_4JMpV3oi1NoTaB5TA-g2A.png"/></div></figure><p id="4fa9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们需要创建一个分类器模型。为此，我们为类变量<em class="lo"> y </em>的所有可能值找到一组给定输入的概率，并选取具有最大概率的输出。这可以用数学方法表示为:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi me"><img src="../Images/203ab9b83efafb41f7eba21ef5562a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*ri9UyJJuv3EeVWzX0wwN9w.png"/></div></figure><p id="d34e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，最后，我们剩下的任务是计算P(y)和P(xi | y)。</p><p id="2fc2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，P(y)也叫<strong class="jp ir">类概率，</strong>，P(xi | y)叫<strong class="jp ir">条件概率</strong>。</p><p id="b8f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不同的朴素贝叶斯分类器的区别主要在于它们对P(xi | y)的分布所做的假设。</p><p id="e308" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们尝试将上述公式手动应用于我们的天气数据集。为此，我们需要在数据集上做一些预计算。</p><p id="c427" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们需要为X中的每个xi和y中的每个yj找到P(xi | yj)。所有这些计算都在下表中进行了演示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mf"><img src="../Images/a8bd92a170b87ea0345bf05c07df3f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0iofXapEfZ47BW-JJF0zgA.png"/></div></div></figure><p id="1fe2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在上图中，我们已经在表1-4中手动计算了X中每个xi的P(xi | yj)和y中的yj。例如，打高尔夫球的概率，假设气温凉爽，即P(temp。=酷|打高尔夫=是)= 3/9。</p><p id="7a72" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，我们需要找到类别概率(P(y))，这已在表5中计算出来。比如P(打高尔夫=是)= 9/14。</p><p id="4902" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们完成了预先计算，分类器准备好了！</p><p id="a8d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们在一组新特性上测试一下(今天就称之为新特性):</p><pre class="km kn ko kp gt lq lr ls lt aw lu bi"><span id="afc5" class="lv lw iq lr b gy lx ly l lz ma">today = (Sunny, Hot, Normal, False)</span></pre><p id="91ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，打高尔夫球的概率由下式给出:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mg"><img src="../Images/f3cc41bbcf3db6c9b706a84a6de75d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PvLhaZeH3PpGu7TZjLrsgg.png"/></div></div></figure><p id="cba3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不打高尔夫的概率由下式给出:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mh"><img src="../Images/424cba55964c0979d15fceebc6292b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XDKSpoNyq206GYE8lzmcIg.png"/></div></div></figure><p id="f035" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为P(今天)在两种概率中是常见的，所以我们可以忽略P(今天),并找到比例概率:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/978135b6645ea39f7db95bb6ccd43864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*FkQfkdl34QUc1faPX1ju0A.png"/></div></figure><p id="f037" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">和</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/46d343b6f1e5f66f801394cb654556a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*SevvbN_es6-lFldBuKS4qw.png"/></div></figure><p id="0304" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，既然</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/ec8397bf54160836d7ea6f78ae125445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*qnMasEMkTuydYMwkmMHH1Q.png"/></div></figure><p id="2a05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使总和等于1(归一化)，可以将这些数字转换成概率:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/042f56d60d6cf8670ea3c660e2fd2fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*8j550ckER7jNsgaiCCrlLA.png"/></div></figure><p id="00df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">和</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/fb841d248137478a404b5e699745c0e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*9DODwNN1gEhiljoDZ_1EiQ.png"/></div></figure><p id="fd27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/15f58021006156c330e67c6b616e93b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*_kS1-ZCFgJ57DCs4y6WfGQ.png"/></div></figure><p id="d7ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，高尔夫将会被使用的预测是肯定的。</p><p id="2f23" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们上面讨论的方法适用于离散数据。在连续数据的情况下，我们需要对每个特性的值的分布做一些假设。不同的朴素贝叶斯分类器的区别主要在于它们对P(xi | y)的分布所做的假设。</p><p id="8a08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们在这里讨论一个这样的分类器。</p><p id="81f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">高斯朴素贝叶斯分类器</strong></p><p id="7789" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在高斯朴素贝叶斯中，假设与每个特征相关联的连续值按照<strong class="jp ir">高斯分布</strong>分布。高斯分布也被称为<a class="ae mo" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">正态分布</a>。绘制时，它给出一条钟形曲线，该曲线关于特征值的平均值对称，如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/a30966fbc3b7f9be3262f83c6b5c2802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/0*LCs2r-0nFQVyR2vU.png"/></div></figure><p id="26f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设特征的似然性是高斯的；因此，条件概率由下式给出:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/04a105a9af1c5826b86bd05e12c9f3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*fgMGu7g8A3rPVa4LAmYiUw.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mr"><img src="../Images/9e37fc09601a737827cdf5eb0281ab3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ndMSTQ6rfNUDnDBWUeOVA.png"/></div></div></figure><p id="c07e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其他流行的朴素贝叶斯分类器有:</p><ul class=""><li id="2115" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated"><strong class="jp ir">多项式朴素贝叶斯</strong>:特征向量代表由<strong class="jp ir">多项式分布</strong>产生的特定事件的频率。这是通常用于文档分类的事件模型。</li><li id="8248" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated"><strong class="jp ir">伯努利朴素贝叶斯</strong>:在多元伯努利事件模型中，特征是描述输入的独立布尔(二元变量)。像多项式模型一样，该模型对于文档分类任务是流行的，其中使用二元术语出现(即，单词是否在文档中出现)特征，而不是术语频率(即，单词在文档中的频率)。</li></ul><p id="6c30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们到达本文的结尾时，这里有一些重要的问题需要思考:</p><ul class=""><li id="3c96" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated">尽管朴素贝叶斯分类器的假设过于简化，但它在许多现实情况下工作得相当好，著名的有文档分类和垃圾邮件过滤。它们需要少量的训练数据来估计必要的参数。</li><li id="677f" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated">与更复杂的方法相比，朴素贝叶斯学习器和分类器可以非常快。类别条件特征分布的解耦意味着每个分布可以被独立地估计为一维分布，反过来，它有助于缓解源于维数灾难的问题。</li></ul><p id="1679" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参考资料:</p><ul class=""><li id="6c0c" class="ky kz iq jp b jq jr ju jv jy la kc lb kg lc kk ld le lf lg bi translated"><a class="ae mo" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Naive_Bayes_classifier</a></li><li id="af45" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated"><a class="ae mo" href="http://gerardnico.com/wiki/data_mining/naive_bayes" rel="noopener ugc nofollow" target="_blank">http://gerardnico.com/wiki/data_mining/naive_bayes</a></li><li id="2708" class="ky kz iq jp b jq lh ju li jy lj kc lk kg ll kk ld le lf lg bi translated"><a class="ae mo" href="http://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank">http://scikit-learn.org/stable/modules/naive_bayes.html</a></li></ul></div></div>    
</body>
</html>