<html>
<head>
<title>HydraSum: Disentangling Stylistic Features in Text Summarization… (Paper Review/Described)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">HydraSum:解开文本摘要中的文体特征…(论文综述/描述)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/hydrasum-disentangling-stylistic-features-in-text-summarization-paper-review-described-a885dd501f06?source=collection_archive---------4-----------------------#2022-05-26">https://pub.towardsai.net/hydrasum-disentangling-stylistic-features-in-text-summarization-paper-review-described-a885dd501f06?source=collection_archive---------4-----------------------#2022-05-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="c13d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">有没有可能用transformer架构训练一个模型来学习生成不同风格的摘要？</em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/43bc3f544505a3c4f65f95c2e8dec80f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QvOyrlPxBCA3cuHPEtii1g.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">图一。多解码器架构方案。(图片来自[1])</figcaption></figure><p id="e998" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然深度学习(特别是transformer architecture)确实不断推高SOTA分数，但它们有一个明显的缺点。不，我不是在说它们的内存使用情况！我们知道如何训练他们，但我们无法控制他们将<strong class="js iu">学到什么。例如，控制输出设置<em class="ko">长度</em>或<em class="ko">样式</em>是文本摘要模型中缺少的特性。让我们看看我们能做些什么…</strong></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="7047" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">本文想解决什么问题？</h1><p id="d326" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">本文主要关注文本摘要任务，并试图给模型的最终用户一种控制预测的感觉。他们特别提出了一种控制抽象性和特殊性的方法。它使用多解码器架构，其中每个解码器将捕获数据集的风格特征。该模型使用解码器输出的加权平均值(称为gate)来生成具有特定风格的摘要。</p><h1 id="1d3e" class="lm ln it bd lo lp mp lr ls lt mq lv lw lx mr lz ma mb ms md me mf mt mh mi mj bi translated">贡献</h1><p id="033e" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">本文通过一个<em class="ko"> k </em>解码器架构探索了上述想法，其中前<em class="ko"> m </em>层的参数在<em class="ko"> k </em>解码器之间共享，其中<em class="ko"> k=2 </em>和<em class="ko"> m=8。(图1) </em>还有一个门控机制(<em class="ko"> g </em>)，基本上是<em class="ko"> k </em>解码器输出的加权和。上述共享层的输出将被传递给前馈层+ softmax以分配权重。现在，我们来看看在<strong class="js iu">训练</strong>和<strong class="js iu">推理</strong>过程中它是如何工作的。</p><h2 id="acbe" class="mu ln it bd lo mv mw dn ls mx my dp lw kb mz na ma kf nb nc me kj nd ne mi nf bi translated">培养</h2><p id="959b" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">他们提出了两种方法来研究设置门机制权重的不同方式。(图2)值得注意的是，学习目标通常是最小化交叉熵损失。</p><ul class=""><li id="e6bc" class="ng nh it js b jt ju jx jy kb ni kf nj kj nk kn nl nm nn no bi translated"><strong class="js iu"><em class="ko"/></strong>:让模型在训练时决定最优权重，以达到每个解码器对最终摘要的最优贡献。我们将通过模型优化固定权重g和1-g。</li><li id="088d" class="ng nh it js b jt np jx nq kb nr kf ns kj nt kn nl nm nn no bi translated"><strong class="js iu"> <em class="ko">引导</em> </strong>:这种方法需要多加注意，因为作者需要精选大量样本(我找不到他们用了多少样本！)并手动分配权重。例如，如果我们希望解码器<strong class="js iu">从输入中学习复制/粘贴</strong>，我们可以选择摘要中有大量文章常用词的样本。然后，我们将为第一个解码器的输出分配更高的权重，以推动第一个解码器学习上述特征。</li></ul><h2 id="08eb" class="mu ln it bd lo mv mw dn ls mx my dp lw kb mz na ma kf nb nc me kj nd ne mi nf bi translated">推理</h2><p id="1856" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">作者尝试用不同的组合对输出进行采样。(图2)</p><ul class=""><li id="6b61" class="ng nh it js b jt ju jx jy kb ni kf nj kj nk kn nl nm nn no bi translated"><strong class="js iu"> <em class="ko">单独解码器</em> </strong>:表示只关注一个解码器。按照相同的复制/粘贴示例，用户可以将1分配给第一个解码器(因此，将0分配给其余的解码器)，以获得摘要来加强提取性。</li><li id="cb51" class="ng nh it js b jt np jx nq kb nr kf ns kj nt kn nl nm nn no bi translated"><strong class="js iu"> <em class="ko">混合</em> </strong>:让<em class="ko">型号</em>决定！显然，只有当我们使用非制导方法训练模型时，它才是可访问的。</li><li id="326a" class="ng nh it js b jt np jx nq kb nr kf ns kj nt kn nl nm nn no bi translated"><strong class="js iu"> <em class="ko">手动混合</em> </strong>:让<em class="ko">用户</em>决定！用户可以混合不同的风格，以获得更理想的输出。例如，如果第一个解码器学习了复制/粘贴风格，而第二个解码器学习了释义。可以更强调解释像[0.3，0.7]这样的旁路权重。</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nu"><img src="../Images/6abdae74ce3ffed330bff891f0685ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8uaG-G_U9H9CXb4ikcU7Q.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">图二。左图)引导式与非引导式训练方法；对)不同的推断方法。(图片来自[1])</figcaption></figure><h1 id="fec6" class="lm ln it bd lo lp mp lr ls lt mq lv lw lx mr lz ma mb ms md me mf mt mh mi mj bi translated">结果</h1><p id="00a5" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">作者做了一个很好的分析来说明不同的解码器会学习不同的风格。他们在抽象性、特异性、长度、可读性和质量(ROUGE)等指标上对这些方法进行了比较。如果你感兴趣的话，我推荐你去读这篇文章的“结果”部分，因为我不会涵盖所有的观点。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nv"><img src="../Images/6f3dfe59f30eca4dc280d6730ec5d50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqT2JIbwvYhAaKBsyu7IeQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">图3。结果表根据上述指标比较了单个和混合解码器。(图片来自[1])</figcaption></figure><p id="04ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文详细介绍了每一列及其含义。例如，可以看出，第一(D0)和第二(D1)解码器在捕捉XSum数据集中的抽象性方面没有不同。有人认为，数据集的性质会严重影响这种方法。此外，很明显，解码器的混合比单个解码器执行得更好，因为每个解码器学习最适合某些样本而不是整个数据集的风格。如图3所示，您可以看到使用单个解码器和它们的组合生成的摘要示例。第一个解码器(蓝色)专注于预测简短的extravice(从文章中复制短语)摘要，而第二个解码器可以编写较长的抽象(模型使用自己的单词)输出。这两个解码器的组合将生成一个具有两种特征的摘要！</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nw"><img src="../Images/5ac787ecb14d664573d522422df18578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HF3TjgvxWjxKJLk1GYh0Qw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">图3。展示单个和混合解码器的功能。(图片来自[1])</figcaption></figure><p id="c461" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对我来说，论文中最后一个有趣的发现是，他们表明独立训练解码器并在推理过程中混合它们以生成不同风格的文本是可能的。</p><p id="b34d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">附录中有附加实验，使用3解码器架构和不同数量的共享层，即6层和10层。以及与基线(BART)相比更倾向于HydraSum汇总的<strong class="js iu">人工评估</strong>流程。</p><h1 id="deef" class="lm ln it bd lo lp mp lr ls lt mq lv lw lx mr lz ma mb ms md me mf mt mh mi mj bi translated">最后的话，</h1><p id="a23d" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">作者使用了一个简单的想法来添加对生成的摘要的风格特征的控制。我不认为这种方法会成为文本摘要的新标准，仅仅因为我们在网络中增加的参数数量会增加内存的使用！但是，我相信这是一个很好的起点。</p><blockquote class="nx"><p id="9c05" class="ny nz it bd oa ob oc od oe of og kn dk translated">我每周给NLP的书呆子发一份时事通讯。如果您想了解自然语言处理的最新发展，可以考虑订阅。<br/> <a class="ae oh" href="https://nlpiation.github.io/" rel="noopener ugc nofollow" target="_blank">阅读更多，订阅</a> —加入酷孩子俱乐部，立即报名！</p></blockquote><h2 id="a0f2" class="mu ln it bd lo mv oi dn ls mx oj dp lw kb ok na ma kf ol nc me kj om ne mi nf bi translated">参考</h2><p id="0076" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">[1]t . Goyal，n . f . Rajani，刘，w .，&amp; kry ciński，W. (2021)。HydraSum:使用多解码器模型解开文本摘要中的风格特征。arXiv预印本arXiv:2110.04400。</p></div></div>    
</body>
</html>