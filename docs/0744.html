<html>
<head>
<title>Adversarially-Trained Deep Nets Transfer Better</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对抗训练的深网转移得更好</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/adversarially-trained-deep-nets-transfer-better-af54c82580f6?source=collection_archive---------2-----------------------#2020-07-31">https://pub.towardsai.net/adversarially-trained-deep-nets-transfer-better-af54c82580f6?source=collection_archive---------2-----------------------#2020-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7dcd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><h1 id="5666" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">有什么实际影响？</h1><p id="87b5" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们发现了一种新的方法，在将学习快速转移到<strong class="kz jd"><em class="lv"/></strong>的背景下，使用对抗训练的深度神经网络(DNNs)在图像分类任务上实现更高的精度<strong class="kz jd"><em class="lv"/></strong>——即使在<strong class="kz jd"> <em class="lv">有限的训练数据</em> </strong>可用时。你可以在<em class="lv"/><a class="ae lw" href="https://arxiv.org/abs/2007.05869" rel="noopener ugc nofollow" target="_blank"><strong class="kz jd"><em class="lv">https://arxiv.org/abs/2007.05869</em></strong></a>阅读我们的论文。</p><h1 id="82e8" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">什么是对抗性训练？</h1><p id="c352" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对抗性训练通过在每个输入图像<em class="lv"> xᵢ </em>上添加对抗性扰动<em class="lv"> δᵢ </em>来修改典型的DNN训练过程，目的是生成对对抗性攻击鲁棒的dnn。特别地，对于具有<em class="lv"> m </em>个输入图像的DNN，损失函数<em class="lv"> ℓ( </em> ⋅ <em class="lv"> ) </em>，由<em class="lv"> θ </em>参数化的模型预测响应<em class="lv"> h( </em> ⋅ <em class="lv"> ) </em>，以及iᵗʰ图像<em class="lv"> yᵢ </em>的真实标签，对抗训练的优化目标在等式1中进行了数学描述。更多详情，请参见我们的论文 中<a class="ae lw" href="https://arxiv.org/abs/2007.05869" rel="noopener ugc nofollow" target="_blank"> <strong class="kz jd">的第3节:“讲解对抗性训练过程”。</strong></a></p><div class="lx ly lz ma gt ab cb"><figure class="mb mc md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><img src="../Images/f18f767f87e6836abdba15bd1b3b0ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*xMN0vQJWtq6S3z8khxMhwA.png"/></div></figure><figure class="mb mc mo me mf mg mh paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><img src="../Images/252080c0c196846b45d529e039163602.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*rNSFWajN-d5zuhoVPtcm3A.png"/></div></figure><figure class="mb mc md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><img src="../Images/f18f767f87e6836abdba15bd1b3b0ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*xMN0vQJWtq6S3z8khxMhwA.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk mt di mu mv translated">等式1:对抗性训练中的优化目标</figcaption></figure></div><h1 id="ec08" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">什么是DNNs背景下的迁移学习？</h1><p id="fd95" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">迁移学习通常在ImageNet等丰富的数据集上训练dnn，然后用目标数据集重新训练(<strong class="kz jd">微调</strong>)一些最后的层。经验上，这种方法允许我们获得比从零开始训练的DNNs更高的精度，正如Yosinski等人首先展示的那样。艾尔。在<a class="ae lw" href="https://arxiv.org/abs/1411.1792" rel="noopener ugc nofollow" target="_blank"> <em class="lv">中，深度神经网络中的特征有多大的可转移性？</em> </a> [2]。此外，与从零开始训练的DNNs相比，这种更高的精度获得得更快。</p><h1 id="18dc" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">是什么激励了我们？</h1><p id="7617" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">齐普拉斯等人解释了两个关键观点。艾尔。<a class="ae lw" href="https://arxiv.org/abs/1805.12152" rel="noopener ugc nofollow" target="_blank"> <em class="lv">鲁棒性可能与准确性不一致</em></a>【1】:</p><ol class=""><li id="17d3" class="mw mx it kz b la my le mz li na lm nb lq nc lu nd ne nf ng bi translated">对抗训练的dnn通常比自然训练的模型具有更低的测试准确度。然而，</li><li id="31bf" class="mw mx it kz b la nh le ni li nj lm nk lq nl lu nd ne nf ng bi translated">正如我们在下面的“为什么这样做”一节中解释的那样，经过对抗性训练的dnn往往具有更符合人类的特征。因此，我们想知道:</li></ol><blockquote class="nm"><p id="b212" class="nn no it bd np nq nr ns nt nu nv lu dk translated">当这些人类对齐的表示被微调到新的数据集时，有没有可能使对抗性训练的模型比自然训练的模型更有优势？</p></blockquote><h1 id="49b5" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk nw km kn ko nx kq kr ks ny ku kv kw bi translated">我们的实验是什么，我们的结果是什么？</h1><p id="99f0" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们全面研究了通过自然微调ResNet-50模型获得的测试准确性，这些模型最初是在ImageNet上以对抗或自然方式训练的。在对6个不同变量的14，400个模型进行微调后，我们得出结论<strong class="kz jd">经过对抗性训练的dnn比自然训练的dnn学习得更快，数据更少</strong>。在某些情况下，我们看到训练图像减少了4倍，训练速度加快了10倍，以达到与自然训练的源模型相同的精度基准。</p><p id="1141" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated"><strong class="kz jd">图1(a)和图2(a) </strong>显示了每个目标数据集上的测试精度分别作为用于微调的训练图像和训练时期的数量的函数。<strong class="kz jd">图1(b)和2(b) </strong>显示了测试精度增量，定义为稳健性减去目标数据集上的自然测试精度。<strong class="kz jd"> (a) </strong>和<strong class="kz jd"> (b) </strong>都由数据集一致着色，阴影区域显示95%置信区间，点是多个随机种子的平均值。更多详情，请参见我们论文 中<a class="ae lw" href="https://arxiv.org/abs/2007.05869" rel="noopener ugc nofollow" target="_blank"> <strong class="kz jd">的第5节:“结果与讨论”。</strong></a></p><figure class="lx ly lz ma gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi oc"><img src="../Images/f7aaa9839d3cd43800eb6dab4afee34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQVC4JaJPGx47JTcxAYFwQ.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">图1:对抗训练的模型比自然训练的模型用更少的数据传输得更好</figcaption></figure><figure class="lx ly lz ma gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi oc"><img src="../Images/854f8b3cb88ae57622cc05d2578aeb2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gccMfOYGFvfXYp_d9IFDDA.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">图2:对抗性训练的模型比自然训练的模型学得更快</figcaption></figure><h1 id="01d6" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">还有人研究过这个吗？</h1><p id="4294" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">据我们所知，只有两篇论文直接研究了这一现象:</p><ol class=""><li id="b2d7" class="mw mx it kz b la my le mz li na lm nb lq nc lu nd ne nf ng bi translated"><a class="ae lw" href="https://arxiv.org/pdf/1905.08232.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">在</em></a><em class="lv"/>【3】中，沙法希等人。艾尔。发现对抗性训练的模型比自然训练的模型传递得更差。然而，这个看似矛盾的结论可以用一个事实来解释:他们使用了比我们大得多的鲁棒性<strong class="kz jd"> </strong>水平:他们使用ε=5，而我们使用ε=3。</li><li id="32f8" class="mw mx it kz b la nh le ni li nj lm nk lq nl lu nd ne nf ng bi translated">微软研究院和麻省理工学院在他们最近的论文<a class="ae lw" href="https://arxiv.org/abs/2007.08489" rel="noopener ugc nofollow" target="_blank"> <em class="lv">中得出了类似的结论:对抗性强的ImageNet模型传输得更好吗？</em></a>【4】和Salman等人的一篇相关<a class="ae lw" href="https://gradientscience.org/transfer-learning/" rel="noopener ugc nofollow" target="_blank"> blogpost </a>他们专注于不同网络架构的影响、固定特征转移，以及对比对抗性鲁棒性和纹理鲁棒性。</li></ol><h1 id="30fd" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">为什么会这样？</h1><figure class="lx ly lz ma gt mc gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0b5de18063e05baad34a363424783d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*n3SJOBUCjal2UUrKmTST_A.jpeg"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">图3:德国牧羊犬的形象</figcaption></figure><p id="1184" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">为了深入了解为什么强大的迁移学习有效，我们可以看看人类的感知。当我们感知现实生活中的物体时，我们从被观察物体的语义属性来推断物体的标签。因此，举例来说，当被赋予识别图3中的物体的任务时，大多数人识别出高耳朵、拉长的鼻子和棕色皮毛图案暗示该物体是德国牧羊犬。这种认知方法的强大之处在于，它允许人类学会识别大量的物体。学会识别德国牧羊犬后，人们可能会增强识别狼的能力，因为它们有共同的视觉特征。</p><p id="0c22" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">考虑一下，图3中的狗图像被输入一只自然训练的DNN和一只敌对训练的DNN。每个网络如何在内部表现这种形象？这可以在图4中可视化，图4示出了给定狗的输入图像的像素中的小变化的损失函数的灵敏度。损失函数衡量我们模型的预测有多“好”。左图显示了一个非常不规则和不平滑的表示，而右图显示了一个人类可以识别为狗的表示。直观地，我们可以说，对抗性训练的模型看到了森林(即，诸如耳朵、眼睛、手臂等特征。)，而自然训练的模型在树(即图像像素)中丢失。</p><figure class="lx ly lz ma gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi oe"><img src="../Images/e97a8d6b7c7b1dfaa4fc0c09810a2100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ciyo-p4k1l4ZtL_13qIAHA.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">图4:对抗性训练的模型包含人类一致的表示。这些图像是利用损失函数对自然训练(左)和对抗训练(右)的DNN模型的输入图像的像素的微小变化的敏感性而创建的。</figcaption></figure><p id="d398" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">我们也可以从<strong class="kz jd"> <em class="lv">影响函数</em> </strong>的角度来考察这一现象，如Koh和梁的《【通过影响函数理解黑箱预测》论文[5]中所述，其实质是问:</p><blockquote class="of og oh"><p id="83e0" class="kx ky lv kz b la my lc ld le mz lg lh oi nz lk ll oj oa lo lp ok ob ls lt lu im bi translated">训练数据集中的哪些图像对分类该输入图像最有帮助？</p></blockquote><p id="2d22" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">由于对抗性训练的模型包含人类对齐的特征，我们期望输入图像的最有影响力的图像实际上看起来与输入图像相似。我们获得了与这一理论相一致的结果。</p><figure class="lx ly lz ma gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ol"><img src="../Images/722aec77dbe61e26ce4280adefe7c976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7RWzz49NVGk9kWv2qwnVWQ.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">图5:对抗性训练的源模型在目标数据集中有更直观的有影响力的图像。最上面一行包含测试图像，10个类别中各有一个。剩余的行包含训练集中最有影响力的图像，分别用于预训练的鲁棒模型和预训练的自然模型</figcaption></figure><p id="26a7" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">图5示出了转移的DNN的测试图像的集合，其中针对对抗和自然训练的源模型示出了目标数据集中最有影响力的训练图像。很明显，与自然训练的源模型相比，鲁棒模型产生看起来更类似于测试图像的最有影响力的图像。图6扩展了图5中所示的概念，以定量测量对抗训练的DNN比自然训练的DNN好多少。特别是，它表明高影响力的图像在鲁棒模型中更经常被正确分类。这些结果表明，相似的图像在对立训练的源模型的特征空间中聚集在一起，这表明对立训练的模型具有更好地与人类认知一致的内部特征表示。更多详情，请参见我们的论文 中<a class="ae lw" href="https://arxiv.org/abs/2007.05869" rel="noopener ugc nofollow" target="_blank"> <strong class="kz jd">的第6节:“使用影响函数解释表征”。</strong></a></p><figure class="lx ly lz ma gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi om"><img src="../Images/a222a52085c0d49392e75e040a8b5208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZwQ_jS_gz-T8KuJcxvfsBw.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">图6:与自然训练的图像相比，对抗性训练的源模型的最有影响力的图像的类别更经常地匹配目标图像的类别。(a)示出了使用训练图像匹配目标的前1到前100个有影响力的图像的比例，而(b)进行了相同的操作，但是使用测试图像作为目标</figcaption></figure><h1 id="8645" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">你怎么能使用这个？</h1><p id="726b" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与许多其他复杂和不规范的培训程序相比，<strong class="kz jd">我们的</strong> <strong class="kz jd">对抗性</strong> <strong class="kz jd">迁移学习程序易于实施</strong>。</p><p id="de45" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">第一步是通过第三方或通过scratch对抗性SGD训练来获得对抗性训练的ImageNet模型(有关更多详细信息，请参见我们论文<a class="ae lw" href="https://arxiv.org/abs/2007.05869" rel="noopener ugc nofollow" target="_blank"><strong class="kz jd"/></a>中的第3节:“对抗性训练过程的解释”)。我们建议使用README.txt中链接的<a class="ae lw" href="https://github.com/MadryLab/robustness" rel="noopener ugc nofollow" target="_blank">鲁棒性库</a>中的模型。接下来，重新初始化最后一个完全连接的层，然后使用目标数据集微调模型，如我们的论文 的第4节所述。</p><p id="2caf" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">最后，我们想分享一些我们在微调14，000多个模型时学到的技巧和诀窍:</p><ul class=""><li id="3c1e" class="mw mx it kz b la my le mz li na lm nb lq nc lu on ne nf ng bi translated">如果训练数据较少，请微调较少的层以避免过度拟合</li><li id="12ef" class="mw mx it kz b la nh le ni li nj lm nk lq nl lu on ne nf ng bi translated">考虑使用我们的超参数作为起点，并执行网格搜索，以提高约1–3%的精度</li><li id="4033" class="mw mx it kz b la nh le ni li nj lm nk lq nl lu on ne nf ng bi translated">具有ℓ-2标准的对抗性训练模型对我们最有效</li><li id="c2f7" class="mw mx it kz b la nh le ni li nj lm nk lq nl lu on ne nf ng bi translated">请记住，这种方法即使在低数据状态下也应该工作得相当好</li></ul><p id="e9c2" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">要查看与我们的研究相关的代码，请查看我们的<a class="ae lw" href="https://github.com/utrerf/robust_transfer_learning" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p><h1 id="d782" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">下一步是什么？</strong></h1><p id="d109" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从<strong class="kz jd">实用</strong>的角度来看，我们感兴趣的是找到进一步提高迁移学习准确性和/或减少对抗性训练源模型所需的计算开销的方法。不同的源模型和架构会对迁移学习过程的行为产生很大的影响。此外，虽然ImageNet作为源数据集在迁移学习中已经变得很常见，但该数据集存在许多问题，可能与迁移学习不一致。ImageNet的一些问题包括图像中的重叠标签、数据集长度不足、低分辨率照片和过时的图像(按照今天的标准)。一些数据集试图解决这些问题。2019年，腾讯发布了一个包含1800万张图像和1.1万个类别的公开数据集，成为世界上最大的带注释的图像数据集。</p><p id="0648" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">从理论角度来看，我们应该首先探究为什么会出现这种现象。即使我们已经通过影响函数的透镜观察了健壮的迁移学习行为，我们也不能明确地解释为什么对抗性训练的模型迁移得更好。这种现象可能与基于表征的学习和/或半监督学习有关。也许我们可以利用这些相关领域的一些理论来获得更深入的理解，解释为什么经过对抗性训练的dnn传输得更好。</p><h1 id="6a84" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">致谢</strong></h1><p id="2ef0" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我要感谢我的博客合著者Evan Kravitz，以及来自Benjamin Erichson、Rajiv Khanna和Michael Mahoney的有益评论和编辑。我们还要感谢CLTC对这一项目的部分支持。</p><h1 id="3b22" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">来源</h1><p id="3945" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]季米特里斯·齐普拉斯、什巴尼·桑图尔卡尔、洛根·恩斯特罗姆、亚历山大·特纳和亚历山大·马德瑞。<a class="ae lw" href="https://arxiv.org/abs/1805.12152" rel="noopener ugc nofollow" target="_blank"> <em class="lv">鲁棒性可能与准确性</em> </a>不一致。ICLR (2019年)。<br/> <strong class="kz jd">与我们工作的相关性— </strong>当在相同的源数据集(即未转移)上进行评估时，对抗性训练的模型表现不如自然训练的模型。然而，“稳健模型学习到的表示往往更符合显著的数据特征和人类感知”。</p><p id="fca5" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">[2]杰森·约辛斯基、杰夫·克鲁恩、约舒阿·本吉奥和霍德·利普森。<a class="ae lw" href="https://arxiv.org/abs/1411.1792" rel="noopener ugc nofollow" target="_blank"> <em class="lv">深度神经网络中特征的可转移性如何？</em> </a> NeurIPS (2014)。<br/></p><p id="f88c" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">[3]阿里·沙法希、帕萨·萨阿达特帕纳、诸宸、阿明·吉亚西、克里斯托夫·斯图德。<a class="ae lw" href="https://arxiv.org/pdf/1905.08232.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">对抗性强的迁移学习</em> </a> <em class="lv">。</em> ICLR(2020)。<br/> <strong class="kz jd">与我们的工作相关— </strong>研究敌对训练的dnn的迁移学习的第一篇论文表明，对敌对扰动具有高容忍度(即ε=5)的dnn的迁移不如自然训练的模型[表3]。</p><p id="6615" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">[4]哈迪·萨勒曼、安德鲁·易勒雅斯、洛根·恩斯特罗姆、阿希什·卡普尔、亚历山大·马德瑞。<a class="ae lw" href="https://arxiv.org/abs/2007.08489" rel="noopener ugc nofollow" target="_blank"> <em class="lv">对抗性强的ImageNet模型传输得更好吗？</em> </a> ArXiv (2020) <br/> <strong class="kz jd">与我们工作的相关性— </strong>最近的工作进一步验证了我们的结论，即通过分析不同的网络架构宽度、微调整个DNN和固定特征转移，对抗性训练的模型转移得更好。</p><p id="3a9f" class="pw-post-body-paragraph kx ky it kz b la my lc ld le mz lg lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">[5]庞维高和佩西梁。<a class="ae lw" href="https://arxiv.org/abs/1703.04730" rel="noopener ugc nofollow" target="_blank"> <em class="lv">通过影响函数</em> </a> <em class="lv">理解黑盒预测。</em> ICML (2017)。<br/> <strong class="kz jd">与我们的工作相关— </strong>提供了我们用来更好地理解为什么对抗性训练模型转移得更好的影响函数框架。</p></div></div>    
</body>
</html>