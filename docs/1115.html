<html>
<head>
<title>Principal Component Analysis (PCA) with Python Examples — Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带Python示例的主成分分析(PCA)——教程</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/principal-component-analysis-pca-with-python-examples-tutorial-67a917bae9aa?source=collection_archive---------0-----------------------#2020-11-04">https://pub.towardsai.net/principal-component-analysis-pca-with-python-examples-tutorial-67a917bae9aa?source=collection_archive---------0-----------------------#2020-11-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d8a9d2df7578a2a6dddc1f0e8bcb9bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11z6aBeStz9mheLCopjPrw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">来源:由<a class="ae jg" href="https://unsplash.com/@rgrzybowski" rel="noopener ugc nofollow" target="_blank"> Radek Grzybowski </a>在<a class="ae jg" href="https://unsplash.com/photos/eBRTYyjwpRY" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上对原作的衍生</figcaption></figure><h2 id="0805" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>、<a class="ae ep" href="https://towardsai.net/p/category/editorial" rel="noopener ugc nofollow" target="_blank">编辑</a>、<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><div class=""><h2 id="a4a5" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">关于主成分分析(PCA)的深入教程，包含数学和Python编码示例</h2></div><p id="69bd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后更新，2021年1月8日</p><p id="ddfd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">作者:</strong>萨妮娅·帕维斯，<a class="ae jg" href="https://mktg.best/vguzs" rel="noopener ugc nofollow" target="_blank">罗伯托·伊里翁多</a></p><div class="is it gp gr iu md"><a href="https://members.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">加入人工智能，成为会员，你将不仅支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><p id="12e9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">本教程的代码可在</strong><a class="ae jg" href="https://github.com/towardsai/tutorials/tree/master/principal_component_analysis" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Github</strong></a><strong class="lj jt">上获得，其完整实现也可在</strong><a class="ae jg" href="https://colab.research.google.com/drive/12wfYg3mHOvVw2TZk3SPsC4naMJhcMkA2?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Google Colab</strong></a><strong class="lj jt">上获得。</strong></p><h2 id="8252" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">目录</h2><ol class=""><li id="0bb8" class="nk nl jj lj b lk nm ln nn lq no lu np ly nq mc nr ns nt nu bi translated"><a class="ae jg" href="#f641" rel="noopener ugc nofollow">简介</a></li><li id="0ac2" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#82ac" rel="noopener ugc nofollow">维度的诅咒</a></li><li id="533c" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#2000" rel="noopener ugc nofollow">降维</a></li><li id="8f28" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#08f8" rel="noopener ugc nofollow">相关性及其测量</a></li><li id="9432" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#7295" rel="noopener ugc nofollow">功能选择</a></li><li id="812e" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#85da" rel="noopener ugc nofollow">特征提取</a></li><li id="25d1" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#7524" rel="noopener ugc nofollow">线状特征提取</a></li><li id="0b7f" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#5e9a" rel="noopener ugc nofollow">主成分分析</a></li><li id="b1be" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#993b" rel="noopener ugc nofollow">PCA背后的数学</a></li><li id="f3e2" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#3628" rel="noopener ugc nofollow">PCA是如何工作的？</a></li><li id="a079" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#8885" rel="noopener ugc nofollow">PCA的应用</a></li><li id="1f78" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#5d0a" rel="noopener ugc nofollow">用Python实现PCA</a></li><li id="ff5e" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#a31f" rel="noopener ugc nofollow">结论</a></li></ol><blockquote class="oa"><p id="170d" class="ob oc jj bd od oe of og oh oi oj mc dk translated">📚查看我们用Python编写的<a class="ae jg" href="https://towardsai.net/p/deep-learning/convolutional-neural-networks-cnns-tutorial-with-python-417c29f0403f" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a>教程。📚</p></blockquote></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><h1 id="f641" class="or mt jj bd mu os ot ou mx ov ow ox na ky oy kz nd lb oz lc ng le pa lf nj pb bi translated">介绍</h1><p id="3186" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">当实现<a class="ae jg" href="https://towardsai.net/p/machine-learning/machine-learning-algorithms-for-beginners-with-python-code-examples-ml-19c6afd60daa" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">机器学习算法</strong> </a>时，包含更多的特性可能会导致性能问题恶化。增加特征的数量并不总是能提高分类精度，这也被称为维数灾难。因此，我们通过选择较低维度特征的最佳集合来应用维度缩减以提高分类精度。</p><p id="18b2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">主成分分析(PCA)对于数据科学、<a class="ae jg" href="https://mld.ai/mldcmu" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt"/></a>机器学习、数据可视化、统计学等定量领域都是必不可少的。</p><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/75bf8df4c2b05797acedc4f483442c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/0*2rXTji8EHYSI3waD.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:维数灾难。</figcaption></figure><p id="d7ee" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有两种技术可以进行降维:</p><ul class=""><li id="7163" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">特征选择</li><li id="8632" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">特征抽出</li></ul><p id="75d5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">了解向量、矩阵、转置矩阵、特征值、特征向量和其他知识对于理解降维的概念至关重要。</p><h1 id="82ac" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">维度的诅咒</h1><p id="9a05" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">对于大多数算法来说，数据集中的维数成为实现合理效率的严重障碍。增加特征的数量并不总能提高精度。当数据没有足够的特征时，模型可能会欠拟合，而当数据有太多特征时，模型可能会过拟合。因此，它被称为维数灾难。对于数据科学家来说，维数灾难是一个惊人的悖论，它基于n维空间的爆炸式增长——随着维数n的增加。</p><h2 id="b414" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">稀疏</h2><p id="0ce4" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">数据的稀疏性是指数据稀少或分散的特性。它缺乏密集性，并且它的大部分变量单元格不包含实际数据。基本上充满了“空”或“不适用”值。</p><p id="7bbb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">随着维数的增加，n维空间中的点经常变得稀疏。点与点之间的距离会随着维度数量的增加而扩大。</p><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/f971f383d492556abdaba1cb2f49a225.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/0*uSR3CC4hgnqpWUKo"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:数据稀疏。</figcaption></figure><h2 id="5547" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">维数灾难的含义</h2><p id="56dd" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">维数灾难的含义很少:</p><ul class=""><li id="477f" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">随着特征数量的增加，优化问题将不可行。</li><li id="ee9a" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">由于n维空间中固有点的绝对规模，随着n保持增长，识别特定点(或甚至附近点)的可能性继续下降。</li></ul><h1 id="2000" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">降维</h1><p id="260c" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">降维消除了数据集的某些特征，并创建了一组有限的特征，其中包含了更高效、更准确地预测目标变量所需的所有信息。</p><p id="7c32" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">减少特征的数量通常也减少了学习过程的输出可变性和复杂性。协方差矩阵是降维过程中的一个重要步骤。检查不同特征之间的相关性是一个关键过程。</p><h1 id="08f8" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">相关性及其测量</h1><p id="980f" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">机器学习中有一个相关性的概念，叫做多重共线性。当一个或多个自变量彼此高度相关时，多重共线性存在。多重共线性使得变量之间高度相关，这使得变量的系数高度不稳定[ <a class="ae jg" href="https://online.stat.psu.edu/stat501/book/export/html/981" rel="noopener ugc nofollow" target="_blank"> 8 </a> ]。</p><p id="fab8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">系数是回归的一个重要组成部分，如果这是不稳定的，那么回归结果将会很差。多重共线性通过使用方差膨胀因子(VIF)来确认。因此，如果怀疑存在多重共线性，可以使用方差膨胀因子(VIF)进行检查。</p><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/66a090626a8bd3043e0dcbf257f9d502.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/0*5YXkmukIy_JsyLAp.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3: VIF方程。</figcaption></figure><p id="19c6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">来自VIF的规则:</p><ul class=""><li id="c04a" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">VIF为1表示完全独立于任何其他变量。</li><li id="0a4a" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">介于5和10之间的VIF表示高度共线性[ <a class="ae jg" href="https://online.stat.psu.edu/stat462/node/180/" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]。</li><li id="ba9b" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">我们越接近1，预测建模的场景就越理想。</li><li id="d242" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">每个自变量对每个自变量进行回归，我们计算VIF。</li></ul><p id="1c56" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">热图在理解变量之间的相关性方面也起着至关重要的作用。</p><p id="d239" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">任何两个量之间的关系类型在一段时间内是不同的。</p><p id="6612" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">相关性从<strong class="lj jt"> -1 </strong>到<strong class="lj jt"> +1 </strong>不等</p><p id="02c8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">准确地说，</p><ul class=""><li id="8579" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">接近+1的值表示正相关。</li><li id="6f0c" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">接近-1的值表示负相关。</li><li id="84ea" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">接近0的值表示完全没有相关性。</li></ul><p id="b534" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面的热图显示了我们将如何关联哪些功能高度依赖于目标功能并考虑它们。</p><h2 id="f610" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">协方差矩阵和热图</h2><p id="e0ad" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">协方差矩阵是降维的第一步，因为它给出了强相关特征数量的概念，并且它通常是降维的第一步，因为它给出了强相关特征数量的概念，以便可以丢弃那些特征。</p><p id="7dbf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">它也给出了所有独立特征的细节。它提供了所有不同特征对之间的相关性的概念。</p><p id="2e87" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">虹膜数据集中强相关特征的识别</strong></p><p id="02ae" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">导入所有必需的包:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="ea45" class="ms mt jj pw b gy qa qb l qc qd">import numpy as np<br/>import pandas as pd<br/>from sklearn import datasets <br/>import matplotlib.pyplot as plt</span></pre><p id="bb89" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">加载虹膜数据集:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="1309" class="ms mt jj pw b gy qa qb l qc qd">iris = datasets.load_iris()<br/>iris.data</span></pre><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qe"><img src="../Images/dce01c79f2da5815003dcfa3b2ee8a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*e1rQwgTF-SulMyDc.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:虹膜数据集。</figcaption></figure><p id="edaf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">列出所有功能:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="6af1" class="ms mt jj pw b gy qa qb l qc qd">iris.feature_names</span></pre><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/5890dc031ec6962e9046ba325980e86d.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/0*5hv2e1pxgmay0y9W.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图Iris数据集的特征。</figcaption></figure><p id="7bf8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">创建协方差矩阵:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="74e4" class="ms mt jj pw b gy qa qb l qc qd">cov_data = np.corrcoef(iris.data.T)cov_data</span></pre><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/7abe54b6cb83c76191181e59a8989133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/0*JXuEMl8_E_nFUZkV.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图Iris数据集的协方差矩阵。</figcaption></figure><p id="ce57" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">绘制协方差矩阵以使用热点图识别要素之间的相关性:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="c7ef" class="ms mt jj pw b gy qa qb l qc qd">img = plt.matshow(cov_data, cmap=plt.cm.rainbow)<br/>plt.colorbar(img, ticks = [-1, 0, 1], fraction=0.045)for x in range(cov_data.shape[0]):<br/>    for y in range(cov_data.shape[1]):<br/>        plt.text(x, y, "%0.2f" % cov_data[x,y], size=12, color='black', ha="center", va="center")<br/>        <br/>plt.show()</span></pre><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/d38ec1acb10119a1a207f26a1d8ea7a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/0*k9NqXnohkhUdMSqk.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7:关联矩阵的热图。</figcaption></figure><p id="dfa4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">热图表示中的关联:</p><ul class=""><li id="ac8b" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">其中<strong class="lj jt">第一个</strong>和<strong class="lj jt">第三个</strong>特征。</li><li id="eae8" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">在第一个<strong class="lj jt">特征</strong>和第四个<strong class="lj jt">特征</strong>之间。</li><li id="2874" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">在第三<strong class="lj jt">和第四</strong>特征<strong class="lj jt">之间。</strong></li></ul><p id="9a85" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">独立功能:</p><ul class=""><li id="d308" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">第二个<strong class="lj jt">特征几乎独立于其他特征。</strong></li></ul><p id="3bd1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里，相关矩阵及其图形表示给出了关于潜在的特征数量减少的想法。因此，可以保留两个特征，并且除了这两个特征之外，可以减少其他特征。</p><p id="25a4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有两种降维方法:</p><ul class=""><li id="83b4" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">特征选择</li><li id="34fc" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">特征抽出</li></ul><p id="49c2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">降维可以忽略不太重要的成分。</p><h1 id="7295" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">特征选择</h1><p id="3b72" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">在特征选择中，通常选择原始特征的子集。</p><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/c5ab724382237046fa700fbd6c6807b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/0*eoZCf5oSJICseIPK.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:特性选择。</figcaption></figure><h1 id="85da" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">特征抽出</h1><p id="d605" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">在特征提取中，发现了一组新的特征。这是通过现有特征的一些映射找到的。此外，映射可以是线性的，也可以是非线性的。</p><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/225dffbf32d92949ae68a91b4a20f4a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/0*TVe80ILrjTdCgm05.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:特征提取。</figcaption></figure><h1 id="7524" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">线性特征提取</h1><p id="7cac" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">线性特征提取计算简单，分析可追踪。</p><p id="aff3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">广泛使用的线性特征提取方法:</p><ul class=""><li id="20f9" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated"><strong class="lj jt">主成分分析(PCA) </strong>:它寻求一种投影，尽可能多地保留数据中的信息。</li><li id="1520" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated"><strong class="lj jt">线性鉴别分析(LDA) </strong> :-它寻找一个能最好地鉴别数据的投影。</li></ul><h1 id="5e9a" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">主成分分析</h1><p id="a631" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">主成分分析(PCA)是一种探索性的方法，用于将数据集的维度降低到2D或3D，在探索性数据分析中用于制作预测模型。主成分分析是数据集的线性变换，其定义了新的坐标规则，使得:</p><ul class=""><li id="2949" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">数据集的任何投影的最大方差出现在第一轴上。</li><li id="1c1f" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">第二个轴上的第二大方差，依此类推。</li></ul><p id="4aca" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以将主成分分析(PCA)用于以下目的:</p><ul class=""><li id="2d89" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">减少数据集中的维数。</li><li id="d402" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">在高维数据集中寻找模式</li><li id="9bc4" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">为了可视化高维数据</li><li id="7958" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">忽略噪音</li><li id="3d80" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">改进分类</li><li id="416a" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">获取简洁的描述</li><li id="b86f" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">尽可能多地捕捉数据中的原始差异</li></ul><p id="937f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">总之，我们可以将<strong class="lj jt">主成分分析(PCA) </strong>定义为将大量变量转化为数量较少的不相关变量，称为主成分(PCs)，以尽可能多地捕捉数据的方差。</p><p id="6b1b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">PCA是卡尔·皮尔逊和哈罗德·霍特林在1901年发明的，作为主轴定理[<a class="ae jg" href="https://zenodo.org/record/1430636#.X6HvO4hKguU" rel="noopener ugc nofollow" target="_blank">1</a>][<a class="ae jg" href="https://pdfs.semanticscholar.org/e0be/f0bd8e07de281230ae5df28daabb4047e8f0.pdf" rel="noopener ugc nofollow" target="_blank">2</a>][<a class="ae jg" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.325.1383&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">3</a>]的类比。</p><p id="9eb0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从数学上讲，PCA的主要目标是:</p><ul class=""><li id="e26e" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">找到数据的标准正交基。</li><li id="1398" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">按重要性顺序对维度进行排序。</li><li id="4ead" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">丢弃低重要性维度。</li><li id="e908" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">关注不相关和高斯成分。</li></ul><p id="6a72" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">PCA涉及的步骤</strong></p><ul class=""><li id="39a9" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">标准化PCA。</li><li id="e0a4" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">计算协方差矩阵。</li><li id="d658" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">求协方差矩阵的特征值和特征向量。</li><li id="bc7a" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">在缩放后的数据上绘制矢量。</li></ul><h2 id="c07c" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">需要PCA的问题示例</h2><p id="bebc" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">一个班里有100名学生，他们有不同的特征，比如年级、年龄、身高、体重、头发颜色等等。</p><p id="fb9f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">大多数描述学生的特征可能是不相关的。因此，找到学生的关键特征是至关重要的。</p><p id="08eb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基于对学生不同特征观察的一些分析:</p><ul class=""><li id="cd5f" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">每个学生都有一个数据向量来定义他的身高，比如(身高，体重，发色，年级等等)。)或者(181，68，黑，99，…).</li><li id="7400" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">每一列都是一个学生向量。所以，n = 100。</li><li id="f343" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">它创建了一个<em class="qk"> m*n </em>矩阵。</li><li id="b7d5" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">每个学生都位于一个m维向量空间中。</li></ul><p id="2f8c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">要忽略的特性</strong></p><ul class=""><li id="7e82" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">共线要素或线性相关要素。例如腿的尺寸和高度。</li><li id="0f6d" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">恒定的噪声特征。例如头发的厚度</li><li id="1d12" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">不变的特征。例如齿数。</li></ul><p id="a4a3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">要保留的特征</strong></p><ul class=""><li id="c00e" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">非共线要素或低协方差。</li><li id="9457" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">变化很大的特征，高方差。例如等级。</li></ul><h1 id="993b" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">PCA背后的数学</h1><p id="a4fd" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">在启动五氯苯甲醚之前，理解相关的数学知识至关重要。特征值和特征向量在主成分分析中起着重要的作用。</p><h2 id="eed4" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">特征向量和特征值</h2><p id="87fe" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">协方差矩阵(或相关性)的特征向量和特征值描述了PCA的来源。特征向量(主成分)决定新属性空间的方向，特征值决定其大小。</p><p id="f588" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">PCA的主要目的是通过将数据投影到更小的子空间来降低数据的维数，其中特征向量形成轴。然而，特征向量只定义了新轴的方向，因为它们的大小都是<strong class="lj jt"> 1 </strong>。因此，为了决定哪个(些)特征向量，我们可以丢弃而不会在子空间构造中丢失太多信息，并检查相应的特征值。具有最高值的特征向量包含更多关于数据分布的信息。</p><h2 id="220f" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated"><strong class="ak">协方差矩阵</strong></h2><p id="0cf2" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">经典的PCA方法计算协方差矩阵，其中每个元素代表两个属性之间的协方差。两个属性之间的协方差计算如下:</p><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ql"><img src="../Images/1d1d0dff4a90c75f6b2a74a4fc0174d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Hp0vei3TJlIV73qW.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:计算两个属性之间协方差的等式。</figcaption></figure><p id="924d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">创建矩阵:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="2774" class="ms mt jj pw b gy qa qb l qc qd">import pandas as pd<br/>import numpy as npmatrix = np.array([[0, 3, 4], [1, 2, 4], [3, 4, 5]]) <br/>matrix</span></pre><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/8f6a59ed53ce2ea7ab37cf1ce60d22a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/0*gQbTlWnONgtB-EeB.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:矩阵。</figcaption></figure><p id="200d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">将矩阵转换为协方差矩阵:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="8fc6" class="ms mt jj pw b gy qa qb l qc qd">np.cov(matrix)</span></pre><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/8d283025729393ddcde9c420554c72c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*kENqmQhUcFKuzQvt.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12:协方差矩阵。</figcaption></figure><p id="5d39" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">协方差矩阵的一个令人兴奋的特征是矩阵的主对角线之和等于特征值之和。</p><h2 id="fa8c" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated"><strong class="ak">相关矩阵</strong></h2><p id="af67" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">另一种计算特征值和特征向量的方法是使用相关矩阵。尽管矩阵是不同的，但它们将产生相同的特征值和特征向量(稍后显示),因为协方差矩阵的归一化给出了相关矩阵。</p><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/87ccc7bd968a2456ea50f1fe07572199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/0*q6rzN5_QB2zJeHBj.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图13:相关矩阵的等式。</figcaption></figure><p id="ab3f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">创建矩阵:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="3858" class="ms mt jj pw b gy qa qb l qc qd">matrix_a = np.array([[0.1, .32, .2,  0.4, 0.8], <br/>             [.23, .18, .56, .61, .12], <br/>             [.9,   .3,  .6,  .5,  .3],  <br/>             [.34, .75, .91, .19, .21]])</span></pre><p id="7d6b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">转换成相关矩阵:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="e628" class="ms mt jj pw b gy qa qb l qc qd">np.corrcoef(matrix_a.T)</span></pre><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qp"><img src="../Images/57e6b8355facc444e638374f172505a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MmChj4bCGmcdwSad.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图14:相关矩阵。</figcaption></figure><h1 id="3628" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">PCA是如何工作的？</h1><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/355450feb34dd7b39abe11e18f995615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/0*2Kh-eEBNPXEfvRVK.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图15:使用PCA [ <a class="ae jg" href="https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/tutorial7.pdf" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]工作。</figcaption></figure><p id="61cc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数据从高维到低维的正交投影，使得(来自图15):</p><ul class=""><li id="5043" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">最大化投影线的方差(紫色)</li><li id="881c" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">最小化数据点和投影之间的MSE(蓝色)</li></ul><h1 id="8885" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">主成分分析的应用</h1><p id="2b2a" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">这些是PCA的典型应用:</p><ul class=""><li id="9502" class="nk nl jj lj b lk ll ln lo lq pk lu pl ly pm mc pn ns nt nu bi translated">数据可视化。</li><li id="84bd" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">数据压缩。</li><li id="810f" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">降噪。</li><li id="9750" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">数据分类。</li><li id="7053" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">图像压缩。</li><li id="3895" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pn ns nt nu bi translated">人脸识别。</li></ul><h1 id="5d0a" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">用Python实现主成分分析</h1><p id="d121" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">用Python实现虹膜数据集上的主成分分析；</p><p id="31bf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">加载虹膜数据集:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="7236" class="ms mt jj pw b gy qa qb l qc qd">import pandas as pd<br/>import numpy as np<br/>from sklearn.datasets import load_iris<br/>from sklearn.preprocessing import StandardScaleriris = load_iris()<br/>df = pd.DataFrame(data=iris.data, columns=iris.feature_names)df['class'] = iris.target<br/>df</span></pre><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qr"><img src="../Images/eec28201653dd0210ff3eb8e4cf8967b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lZ-pIBFC7pMFD3Vp.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图16:虹膜数据集。</figcaption></figure><p id="f79d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">获取x和y的值:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="f2d3" class="ms mt jj pw b gy qa qb l qc qd">x = df.drop(labels='class', axis=1).values<br/>y = df['class'].values</span></pre><p id="2e94" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用协方差矩阵实现PCA</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="f77a" class="ms mt jj pw b gy qa qb l qc qd">class convers_pca():<br/>    def __init__(self, no_of_components):<br/>        self.no_of_components = no_of_components<br/>        self.eigen_values = None<br/>        self.eigen_vectors = None<br/>        <br/>    def transform(self, x):<br/>        return np.dot(x - self.mean, self.projection_matrix.T)<br/>    <br/>    def inverse_transform(self, x):<br/>        return np.dot(x, self.projection_matrix) + self.mean<br/>    <br/>    def fit(self, x):<br/>        self.no_of_components = x.shape[1] if self.no_of_components is None else self.no_of_components<br/>        self.mean = np.mean(x, axis=0)<br/>        <br/>        cov_matrix = np.cov(x - self.mean, rowvar=False)<br/>        <br/>        self.eigen_values, self.eigen_vectors = np.linalg.eig(cov_matrix)<br/>        self.eigen_vectors = self.eigen_vectors.T<br/>        <br/>        self.sorted_components = np.argsort(self.eigen_values)[::-1]<br/>        <br/>        self.projection_matrix = self.eigen_vectors[self.sorted_components[:self.no_of_components]]self.explained_variance = self.eigen_values[self.sorted_components]<br/>        self.explained_variance_ratio = self.explained_variance / self.eigen_values.sum()</span></pre><p id="8d65" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">x的标准化:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="2c55" class="ms mt jj pw b gy qa qb l qc qd">std = StandardScaler()<br/>transformed = StandardScaler().fit_transform(x)</span></pre><p id="8050" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">具有两个组件的PCA:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="e4ec" class="ms mt jj pw b gy qa qb l qc qd">pca = convers_pca(no_of_components=2)<br/>pca.fit(transformed)</span></pre><p id="2fb9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">检查特征向量:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="a7fa" class="ms mt jj pw b gy qa qb l qc qd">cov_pca.eigen_vectors</span></pre><p id="a8a5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">检查特征值:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="b926" class="ms mt jj pw b gy qa qb l qc qd">cov_pca.eigen_values</span></pre><p id="c8ad" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">检查分类组件:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="319c" class="ms mt jj pw b gy qa qb l qc qd">cov_pca.sorted_components</span></pre><p id="7a58" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用几个成分绘制PCA图= 2:</p><pre class="pg ph pi pj gt pv pw px py aw pz bi"><span id="29c3" class="ms mt jj pw b gy qa qb l qc qd">x_std = pca.transform(transformed)plt.figure()<br/>plt.scatter(x_std[:, 0], x_std[:, 1], c=y)</span></pre><figure class="pg ph pi pj gt iv gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/2ae085e740aaa8a576d4a64f46229e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*Jl3mIShle1qf19p-.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图17: PCA可视化。</figcaption></figure><h1 id="a31f" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">结论</h1><p id="4faf" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">海量数据集在各种学科中越来越普遍。为了解释这样的数据集，我们需要降低维数以保留最相关的数据。</p><p id="4609" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">PCA解决了特征向量/值的问题。在线性回归和神经网络的训练阶段，我们应用PCA来消除共线性。PCA类似于观察值的低维表示，它解释了方差的很大一部分[ <a class="ae jg" href="http://noiselab.ucsd.edu/ECE285/lecture10.pdf" rel="noopener ugc nofollow" target="_blank"> 6 </a> ]。</p><p id="541b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以使用PCA来减少变量的数量，避免多重共线性，或者相对于观察值的数量有太多的预测值。PCA是<em class="qk"> p </em>特征的线性组合，采用这些测量的线性组合对于减少视觉分析所需的绘图数量，同时保留数据中存在的大部分信息至关重要[ <a class="ae jg" href="https://uc-r.github.io/pca" rel="noopener ugc nofollow" target="_blank"> 7 </a> ]。</p><p id="d344" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">特征约简是机器学习中必不可少的预处理步骤。因此，PCA是预处理的必要步骤，并且对于数据中的压缩和噪声去除非常有用。它通过寻找比原始变量集更小的新变量集来降低数据集的维度。</p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="3e4c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">免责声明:</strong>本文表达的观点仅代表作者个人观点，不代表卡耐基梅隆大学或其他与作者(直接或间接)相关的公司的观点。这些文章并不是最终产品，而是当前思想的反映，是讨论和改进的催化剂。</p><p id="c60b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">除非另有说明，所有图片均来自作者。</strong></p><p id="9872" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过<a class="ae jg" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向AI </a>发布</p><h1 id="6ef5" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">资源</h1><p id="58ef" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated"><a class="ae jg" href="https://colab.research.google.com/drive/12wfYg3mHOvVw2TZk3SPsC4naMJhcMkA2?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google colab实现</a>。</p><p id="6f19" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://github.com/towardsai/tutorials/tree/master/principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> Github库</a>。</p><h1 id="c270" class="or mt jj bd mu os po ou mx ov pp ox na ky pq kz nd lb pr lc ng le ps lf nj pb bi translated">参考</h1><p id="8aa6" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">[1] <a class="ae jg" href="https://en.wikipedia.org/wiki/Karl_Pearson" rel="noopener ugc nofollow" target="_blank">皮尔逊，K. </a> (1901)。<a class="ae jg" href="https://zenodo.org/record/1430636" rel="noopener ugc nofollow" target="_blank">“在最接近空间点系统的直线和平面上”</a>。<em class="qk">哲学杂志</em>。2(11):559–572。<a class="ae jg" href="https://en.wikipedia.org/wiki/Doi_(identifier)" rel="noopener ugc nofollow" target="_blank">doi</a>:<a class="ae jg" href="https://doi.org/10.1080%2F14786440109462720" rel="noopener ugc nofollow" target="_blank">10.1080/14786440109462720</a>。</p><p id="477c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]主成分分析与优化:一个教程，Robert Reris和J. Paul Brooks，<a class="ae jg" href="https://pdfs.semanticscholar.org/e0be/f0bd8e07de281230ae5df28daabb4047e8f0.pdf" rel="noopener ugc nofollow" target="_blank">https://pdfs . semantic scholar . org/e0be/f0b d8e 07 de 281230 AE 5 df 28 da abb 4047 e8f 0 . pdf</a></p><p id="3ca1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3]主成分分析，马丁·休厄尔，伦敦大学学院，<a class="ae jg" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.325.1383&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">https://citeseerx.ist.psu.edu/viewdoc/download?doi = 10 . 1 . 1 . 325 . 1383&amp;rep = re P1&amp;type = pdf</a></p><p id="0f73" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4]使用方差膨胀因子检测多重共线性，https://online.stat.psu.edu/stat462/node/180/<a class="ae jg" href="https://online.stat.psu.edu/stat462/node/180/" rel="noopener ugc nofollow" target="_blank">宾夕法尼亚州立大学</a></p><p id="7062" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]拉格朗日对偶与PCA CS 411教程，罗文洁多伦多大学，<a class="ae jg" href="https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/tutorial7.pdf" rel="noopener ugc nofollow" target="_blank">https://www . CS . Toronto . edu/~ urta sun/courses/CSC 411 _ fall 16/Tutorial 7 . pdf</a></p><p id="5d03" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6] ECE 285和SIO 209物理应用的机器学习，第10讲，彼得·格斯托夫特，加州大学圣地亚哥分校，<a class="ae jg" href="http://noiselab.ucsd.edu/ECE285/lecture10.pdf" rel="noopener ugc nofollow" target="_blank">http://noiselab.ucsd.edu/ECE285/lecture10.pdf</a></p><p id="07c3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[7]主成分分析，UC Business Analytics R编程指南，<a class="ae jg" href="https://uc-r.github.io/pca" rel="noopener ugc nofollow" target="_blank">https://uc-r.github.io/pca</a></p><p id="5e78" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[8]第12课:多重共线性和其他回归陷阱，回归方法，宾夕法尼亚州立大学，<a class="ae jg" href="https://online.stat.psu.edu/stat501/book/export/html/981" rel="noopener ugc nofollow" target="_blank">https://online.stat.psu.edu/stat501/book/export/html/981</a></p><p id="a4eb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[9] Python数据科学手册，Jake VanderPlas，O'Reilly，<a class="ae jg" href="https://github.com/jakevdp/PythonDataScienceHandbook" rel="noopener ugc nofollow" target="_blank">https://www . oreilly . com/library/view/Python-Data-Science/9781491912126/</a></p></div></div>    
</body>
</html>