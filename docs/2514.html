<html>
<head>
<title>Attention Visualizer Package: Showcase Highest Scored Words Using RoBERTa Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意力可视化工具包:使用罗伯塔模型展示得分最高的单词</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/attention-visualizer-package-showcase-highest-scored-words-using-roberta-model-8218658b4447?source=collection_archive---------3-----------------------#2022-01-24">https://pub.towardsai.net/attention-visualizer-package-showcase-highest-scored-words-using-roberta-model-8218658b4447?source=collection_archive---------3-----------------------#2022-01-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f0b9" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><p id="289d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><a class="ae kx" href="https://github.com/AlaFalaki/AttentionVisualizer" rel="noopener ugc nofollow" target="_blank"> <em class="ky">这个库</em> </a> <em class="ky">可以让你很容易地看到哪些单词在每一层的自我关注分数方面贡献最大，并使用预先训练的罗伯塔模型走向输出。</em></p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi kz"><img src="../Images/2cb12b3992ee1b9833a2bc2e562dff8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OyHxf77nWaUd096N6uHaRw.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk translated">图一。使用来自CNN/DM数据集的文章的模型输出示例。(启用“忽略特殊记号、点、停用字词”功能)</figcaption></figure><p id="ca7e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">自我关注机制相对于上下文中的所有其他标记给每个标记分配一个分数。每个头部中的得分张量形状将是[N×N]，其中N是输入长度。理论上，我们可以将这些分数可视化，以查看哪些单词对模型的最终决策贡献最大。该库与RoBERTa [1]兼容，并有不同的选项来显示详细视图，如图2所示。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi lp"><img src="../Images/b22f4822fb458c84e0eb779ca2c8534a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1JsKjZUEMsurCgiBwSuo8A.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk translated">图二。用户界面元素。</figcaption></figure><p id="226f" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">第一步是输入一篇长度不超过512个单词的文章；然后有三个不同的选项，在计算分数时忽略不同的单词。这些选项允许您忽略1) BOS/EOS标记、2)点和3)停用词。为什么这些选项可用的细节将在后面讨论。最后，下拉菜单选择<em class="ky">所有</em>、<em class="ky">一个范围</em>或<em class="ky">特定的</em>层和头。单击visualize按钮后会显示输出(图1 ),如果将鼠标悬停在每个单词上一两秒钟，就会显示每个单词的分数。</p><p id="714d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">这个包托管在<a class="ae kx" href="https://github.com/AlaFalaki/AttentionVisualizer" rel="noopener ugc nofollow" target="_blank"> Github </a>上，你可以使用Python包管理器(pip)来安装它，它会负责下载Huggingface、NLTK和ipywidgets等依赖项。</p><pre class="la lb lc ld gt lq lr ls lt aw lu bi"><span id="a4a5" class="lv lw it lr b gy lx ly l lz ma">pip install git+https://github.com/AlaFalaki/AttentionVisualizer.git</span></pre><p id="7919" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">导入该类，并使用<em class="ky"> AttentionVisualizer() </em>函数初始化它，该函数将下载RoBERTa模型和tokenizer。在笔记本中，可以通过调用<em class="ky"> show_controllers() </em>方法来处理任何文本。</p><figure class="la lb lc ld gt le"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="02e1" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">或者，使用<a class="ae kx" href="https://colab.research.google.com/github/AlaFalaki/AttentionVisualizer/blob/main/demo.ipynb" rel="noopener ugc nofollow" target="_blank">这个演示链接</a>在Google Colab环境中使用这个模型。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="ce62" class="mk lw it bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">实施细节</h1><p id="402b" class="pw-post-body-paragraph jz ka it kb b kc nh ke kf kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw im bi translated">有几点和设计选择值得讨论。我将解释这些问题以及解决这些问题的方法。</p><ul class=""><li id="8d80" class="nm nn it kb b kc kd kg kh kk no ko np ks nq kw nr ns nt nu bi translated"><strong class="kb jd">高维度</strong></li></ul><p id="9267" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">自我注意分数张量具有[L×H×N×N]的大小，其中N×N表示自我注意机制，即通过将句子与其自身进行比较来计算每个标记的分数；同样，变量L和H代表层数和自我关注头数。正如你所看到的，它远不是一个简单的大小为[N]的秩1张量，它为每个令牌保存一个分数！最简单的解决方案是将分数与层和头一起平均。(该包中选择的方法)</p><p id="770b" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">想到的更高级的想法，如头部修剪以找到最有价值的头部，或结合维度减少算法以提取重要的特征，目前还没有实现。</p><ul class=""><li id="18aa" class="nm nn it kb b kc kd kg kh kk no ko np ks nq kw nr ns nt nu bi translated"><strong class="kb jd">标记化过程</strong></li></ul><p id="5491" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">RoBERTa模型使用字节级字节对编码，将一些单词分解成更小的比特。(比如像<strong class="kb jd">【去】</strong>这样的单词会转换成<strong class="kb jd">【go+ing】</strong>)词汇量小一点会有帮助。但是既然每个标记(“go”、“ing”)得到的是分数而不是单词，我们应该如何处理呢？这里实现的解决方案是选择得分最高的标记作为单词的得分。(例如，如果第一个单词“go”得分为0.05，第二个单词“ing”得分为0.01，则0.05将被选为单词“going”的得分)</p><ul class=""><li id="b34a" class="nm nn it kb b kc kd kg kh kk no ko np ks nq kw nr ns nt nu bi translated"><strong class="kb jd">忽略令牌</strong></li></ul><p id="a64e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">如前所述，有一个选项可以在显示单词组时忽略它们的分数。如果您使用这个包并尝试不同的配置，那么这些选项存在的原因就很清楚了。事实证明，平均所有的分数并显示它们会将一个很大的数字分配给句子([CLS])令牌的乞求，这是有意义的。同样明显的是，忽略BOS标记会增加[dot]标记的分数！只有在忽略等式中的bo和[dot]之后，才能获得合理的输出。</p><p id="daaf" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">需要注意的是，如果选择忽略选项，标记不会从文本中删除。无论如何，相同的文本将被传递给RoBERTa模型。唯一的区别是，我们将这些令牌的分数(平均后)更改为最小分数，并进行简单的最小-最大归一化来放大所有分数。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="e644" class="mk lw it bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">最后的话，</h1><p id="8df6" class="pw-post-body-paragraph jz ka it kb b kc nh ke kf kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw im bi translated">这个包的创建是作为一个实验开始的，但我认为添加一个UI并发布在Github上可能会对某些人有所帮助，甚至会带来乐趣。无论如何，它都不应该是完美的，这就是开源社区的魅力所在！如果你不同意设计选择或者认为有更好的方法(我知道有！)要反制提到的问题，就来说说吧，或者你可以简单的发一个拉取请求！对我来说这是一个有趣的项目，它教会了我如何使用IPythonWidgets在IPython笔记本上制作一个简单的UI。<em class="ky">(如果有人喜欢，我可以写下来，在</em> <a class="ae kx" href="https://twitter.com/NLPiation" rel="noopener ugc nofollow" target="_blank"> <em class="ky">推特</em> </a> <em class="ky">上告诉我)</em>我希望这对你来说也是一本好书。</p><p id="3716" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">⚠️ <em class="ky">免责声明:我是软件包作者。</em></p><blockquote class="nv"><p id="f0bc" class="nw nx it bd ny nz oa ob oc od oe kw dk translated">我每周给NLP的书呆子发一份时事通讯。如果您想了解自然语言处理的最新发展，可以考虑订阅。<br/> <a class="ae kx" href="https://nlpiation.github.io/" rel="noopener ugc nofollow" target="_blank">阅读更多，订阅</a> —加入酷孩子俱乐部，立即报名！</p></blockquote><h1 id="9594" class="mk lw it bd ml mm of mo mp mq og ms mt mu oh mw mx my oi na nb nc oj ne nf ng bi translated">参考</h1><p id="77f1" class="pw-post-body-paragraph jz ka it kb b kc nh ke kf kg ni ki kj kk nj km kn ko nk kq kr ks nl ku kv kw im bi translated">[1]刘，y .，奥特，m .，戈亚尔，n .，杜，j .，乔希，m .，陈，d .，… &amp;斯托扬诺夫，V. (2019)。Roberta:稳健优化的bert预训练方法。arXiv预印本arXiv:1907.11692</p></div></div>    
</body>
</html>