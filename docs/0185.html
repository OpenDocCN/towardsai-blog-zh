<html>
<head>
<title>Review: IEF — Iterative Error Feedback (Human Pose Estimation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:IEF——迭代误差反馈(人体姿态估计)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/review-ief-iterative-error-feedback-human-pose-estimation-a56add160fa5?source=collection_archive---------1-----------------------#2019-10-16">https://pub.towardsai.net/review-ief-iterative-error-feedback-human-pose-estimation-a56add160fa5?source=collection_archive---------1-----------------------#2019-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0346" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">评| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向艾</a></h2><div class=""/><div class=""><h2 id="a8c0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">胜过<a class="ae ko" href="https://towardsdatascience.com/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c" rel="noopener" target="_blank">汤普森·尼普斯的《14 </a>，和<a class="ae ko" href="https://towardsdatascience.com/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c" rel="noopener" target="_blank">汤普森·CVPR的《15 </a></h2></div><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/86b453a18d2c96bbd991a44697104634.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuhMlmjT_125IGtWXARSdw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">用迭代误差反馈从左到右越来越好(IEF) </strong></figcaption></figure><p id="076e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi mc translated"><span class="l md me mf bm mg mh mi mj mk di">在</span>这个故事中，简要回顾了加州大学伯克利分校的<strong class="li ja"> IEF(迭代误差反馈)</strong>。<strong class="li ja">自校正模型</strong>用于<strong class="li ja">通过反馈误差预测</strong>逐步改变初始解，而不是一次性直接预测输出。这是一篇<strong class="li ja"> 2016 CVPR </strong>论文，引用<strong class="li ja"> 300多篇</strong>。(<a class="ml mm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----a56add160fa5--------------------------------" rel="noopener" target="_blank">曾植和</a> @中)</p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h1 id="29c5" class="mu mv iq bd lf mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">概述</h1><ol class=""><li id="c6fd" class="nl nm iq li b lj nn lm no lp np lt nq lx nr mb ns nt nu nv bi translated"><strong class="li ja"> IEF建筑概述</strong></li><li id="16a9" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb ns nt nu nv bi translated"><strong class="li ja">培训</strong></li><li id="970b" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb ns nt nu nv bi translated"><strong class="li ja">消融研究</strong></li><li id="7fde" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb ns nt nu nv bi translated"><strong class="li ja">与最先进方法的比较</strong></li></ol></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h1 id="d239" class="mu mv iq bd lf mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">1.IEF建筑概述</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ob"><img src="../Images/a53a612dc79871f1c45164c6aadbf599.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nKubufo0yDWzfKN3mpffQA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf"> IEF建筑概述</strong></figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1a9c976e7dbaf6ef2bcb98c0d3f26369.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*xZTM7uxOdWH59kHIh8zIMg.png"/></div></figure><ul class=""><li id="23a9" class="nl nm iq li b lj lk lm ln lp od lt oe lx of mb og nt nu nv bi translated"><strong class="li ja"> <em class="oh"> xt </em> </strong>，输入图像<em class="oh"> I </em>与视觉表示g的拼接，输入到模型<em class="oh"> f </em>。有了3通道RGB图像和<em class="oh"> K </em>关键点的<em class="oh"> K </em>热图，还有<em class="oh"> K </em> +3通道的<em class="oh"> xt </em>。</li><li id="96ef" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">然后模型<em class="oh"> f </em>输出/预测一个“修正”<em class="oh"> et。</em></li><li id="5a61" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated"><em class="oh">yt</em>+1 =<em class="oh">et</em>+<em class="oh">yt</em>获得新的<em class="oh"> y </em>。</li><li id="871b" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated"><em class="oh"> yt </em> +1转换成<em class="oh"> g </em>的可视化表示。<em class="oh"> g </em>是以关键点位置为中心的具有固定标准偏差的2D高斯。</li><li id="4a3a" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated"><strong class="li ja">该程序通过猜测输出(<em class="oh"> y </em> 0)进行初始化，并重复进行，直到满足预定的终止标准。</strong></li><li id="93eb" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">y  0是训练图像上真实2D关键点位置的中值</li></ul></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h1 id="b549" class="mu mv iq bd lf mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">2.培养</h1><h2 id="d3a0" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated">2.1.损失函数</h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/0a2cbc948db37468c7ba7c363f90a73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*HFoKEGiqZR_hMOoPwJD5Ng.png"/></div></figure><ul class=""><li id="4659" class="nl nm iq li b lj lk lm ln lp od lt oe lx of mb og nt nu nv bi translated"><em class="oh"> et </em>和<em class="oh"> e </em> ( <em class="oh"> y </em>，<em class="oh"> yt </em>)分别是预测和目标有界修正。</li><li id="16cf" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated"><em class="oh"> h </em>是距离的度量，比如二次损耗。</li><li id="130c" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">t是模型采取的校正步骤的数量。<em class="oh"> T </em> =4。</li></ul><h2 id="3a07" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated">2.2.固定路径整合(FPC)</h2><ul class=""><li id="1bd9" class="nl nm iq li b lj nn lm no lp np lt nq lx nr mb og nt nu nv bi translated">以上是迭代过程，但我们只得到最后的地面——真相。我们没有中间的事实真相。</li><li id="361f" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">最简单的策略是<strong class="li ja">从<em class="oh"> y </em> 0开始，使用一组固定的修正值<em class="oh"> e </em> ( <em class="oh"> y </em>，<em class="oh"> yt </em>)为每次迭代预定义<em class="oh"> yt </em>，得到(<em class="oh"> y </em> 0，<em class="oh"> y </em> 1，…，<em class="oh"> y </em> ) </strong>。</li><li id="684d" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">使用一个函数来计算每次迭代的目标有界校正:</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/0a3e2bdc926e175ce9d0bbc936209986.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*FbZZhTkAML0nywshBfxfpA.png"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/958c4b770c413d72d9f108d0710fe34b.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*b7EqCdrYR8jl1N7OZQelkg.png"/></div></figure><ul class=""><li id="6460" class="nl nm iq li b lj lk lm ln lp od lt oe lx of mb og nt nu nv bi translated">其中<em class="oh"> k </em>是第<em class="oh"> k </em>个关键点。<em class="oh"> L </em>表示每个关键点位置的最大位移。<em class="oh"> L </em> = 20像素。<em class="oh"> u^ </em>是<em class="oh"> u </em>的单位向量。</li></ul><h2 id="32e4" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated">2.3.ConvNet</h2><ul class=""><li id="5ecb" class="nl nm iq li b lj nn lm no lp np lt nq lx nr mb og nt nu nv bi translated"><strong class="li ja"> ImageNet预训练</strong> <a class="ae ko" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7?source=post_page---------------------------" rel="noopener"> <strong class="li ja">使用GoogLeNet </strong> </a> <strong class="li ja"> </strong>。</li><li id="3242" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">conv-1滤波器被修改为在20个通道输入上操作。前三个conv-1通道的权重使用通过在Imagenet上预训练学习的权重来初始化。对应于剩余17个通道的权重用方差为0.1的高斯噪声随机初始化。</li><li id="c0ee" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">预测Imagenet类的1000个单元的最后一层被丢弃。它被替换为包含32个单元的层，对用笛卡尔坐标表示的连续2D校正进行编码(第17个“关键点”是在训练和测试中使用的人体内任何地方的一个点的位置)。</li></ul></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h1 id="f0d5" class="mu mv iq bd lf mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">3.消融研究</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ow"><img src="../Images/c148f46584b12ae9db469d051e17b4a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2GPME3qSiyGByUCHzdrVuQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">MPII验证集的PCKh-0.5结果</strong></figcaption></figure><h2 id="e73c" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated"><strong class="ak"> 3.1。迭代v/s直接预测</strong></h2><ul class=""><li id="aa27" class="nl nm iq li b lj nn lm no lp np lt nq lx nr mb og nt nu nv bi translated"><strong class="li ja">加性回归到关键点位置的IEF </strong>达到了<strong class="li ja"> 81.0 </strong>的PCKh-0.5，相比之下<strong class="li ja"> 74.8 </strong>的PCKh是由<strong class="li ja">直接回归</strong>到关键点得到的。</li></ul><h2 id="b770" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated">3.2.迭代误差反馈v/s迭代直接预测</h2><ul class=""><li id="8b89" class="nl nm iq li b lj nn lm no lp np lt nq lx nr mb og nt nu nv bi translated"><strong class="li ja"> IEF </strong>通过<strong class="li ja">迭代直接预测</strong>实现了<strong class="li ja"> 81.0 </strong>的PCKh-0.5，而<strong class="li ja"> 73.4 </strong>的PCKh。</li></ul><h2 id="e1b7" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated">3.3.固定路径整合的重要性(FPC)</h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/6c16abdbac710c6d245108f31d6b47a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*FTj0cX_p_Hm6dlFBK8_oYQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">验证PCKh-0.5分数</strong></figcaption></figure><ul class=""><li id="e5ed" class="nl nm iq li b lj lk lm ln lp od lt oe lx of mb og nt nu nv bi translated">没有FPC，验证集上的性能下降了近10个PCKh点。</li></ul><h2 id="b0d8" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated">3.4.学习结构输出</h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oy"><img src="../Images/31927a17f5fe23e82aecbfc5bf0230a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YywAuFCfjmKYl-9q_i2aiQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf"> MPII验证PCKh-0.5结果</strong></figcaption></figure><ul class=""><li id="de6e" class="nl nm iq li b lj lk lm ln lp od lt oe lx of mb og nt nu nv bi translated">作为<strong class="li ja">基线</strong>，回归得到<strong class="li ja"> 64.6 </strong>。</li><li id="bab0" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">为<strong class="li ja">左膝</strong>多了一个输入通道的left】车型得到了<strong class="li ja"> 69.2 </strong>的PCKh。</li><li id="f0e4" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">此外，<strong class="li ja">左膝和左臀</strong>的IEF车型得到<strong class="li ja"> 72.8 </strong>的PCKh。</li><li id="77fd" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">最后，<strong class="li ja">用图像将所有关节</strong>建模在一起，得到一个<strong class="li ja"> 73.8 </strong>的PCKh。</li></ul><h2 id="c3b4" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated">3.5.形象化</h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oz"><img src="../Images/308d08f9c96cdf5bf1e393ef788e2038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6MeceBN7Wqnom-48IJbTyw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf"> MPII验证集</strong></figcaption></figure></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h1 id="f6a2" class="mu mv iq bd lf mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">4.<strong class="ak">与最先进方法的比较</strong></h1><h2 id="4a86" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated">4.1.MPII</h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pa"><img src="../Images/3ce13b7b230e375648f0ce06937d6fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JiAhadsORLBme7KIaJHG8Q.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf"> MPII测试集PCKh-0.5结果</strong></figcaption></figure><ul class=""><li id="bdce" class="nl nm iq li b lj lk lm ln lp od lt oe lx of mb og nt nu nv bi translated">IEF胜过SOTA，比如汤普森·CVPR的《T21》。</li></ul><h2 id="31ce" class="oi mv iq bd lf oj ok dn mz ol om dp nd lp on oo nf lt op oq nh lx or os nj iw bi translated">4.2.骶左后期（left sacro-posterior的缩写）</h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/a7d3bc2648fa11d2edf18489766de7cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*Yh1nKBf_ovzKDODmddYDzg.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">LSP数据集测试集上以人为中心的PCP分数</strong></figcaption></figure><ul class=""><li id="536c" class="nl nm iq li b lj lk lm ln lp od lt oe lx of mb og nt nu nv bi translated">躯干上也没有标记点，因此第17个关键点被初始化为图像的中心。</li><li id="3e5f" class="nl nm iq li b lj nw lm nx lp ny lt nz lx oa mb og nt nu nv bi translated">IEF的表现超过了SOTA，比如汤普森·尼普斯。</li></ul></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><h1 id="f8ee" class="mu mv iq bd lf mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">参考</h1><p id="f842" class="pw-post-body-paragraph lg lh iq li b lj nn ka ll lm no kd lo lp pc lr ls lt pd lv lw lx pe lz ma mb ij bi translated">【2016 CVPR】【IEF】<br/><a class="ae ko" href="https://arxiv.org/abs/1507.06550" rel="noopener ugc nofollow" target="_blank">带迭代误差反馈的人体姿态估计</a></p><h1 id="e471" class="mu mv iq bd lf mw pf my mz na pg nc nd kf ph kg nf ki pi kj nh kl pj km nj nk bi translated">我以前的评论</h1><p id="6cde" class="pw-post-body-paragraph lg lh iq li b lj nn ka ll lm no kd lo lp pc lr ls lt pd lv lw lx pe lz ma mb ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(但)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(想)(要)(让)(这)(些)(人)(都)(有)(这)(些)(情)(况)(,)(我)(们)(还)(不)(想)(要)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(就)(是)(这)(些)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(了)(,)(我)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。</p><p id="753f" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">物体检测</strong> [ <a class="ae ko" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------" rel="noopener">过食</a> ] [ <a class="ae ko" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a> ] [ <a class="ae ko" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------" rel="noopener">快R-CNN </a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快R-CNN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a>][<a class="ae ko" href="https://towardsdatascience.com/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=post_page---------------------------" rel="noopener" target="_blank">CRAFT</a>][<a class="ae ko" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank">R-FCN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------" rel="noopener" target="_blank">离子</a><a class="ae ko" href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------" rel="noopener" target="_blank"> [</a><a class="ae ko" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank">G-RMI</a>][<a class="ae ko" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------" rel="noopener">TDM</a>][<a class="ae ko" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae ko" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>][<a class="ae ko" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolo v1</a>][<a class="ae ko" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolo v2/yolo 9000</a>][<a class="ae ko" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolo v3</a>][<a class="ae ko" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank">FPN</a>[<a class="ae ko" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank">retina net</a>[<a class="ae ko" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank">DCN</a></p><p id="b54b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">)(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。 [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]</p><p id="a2a6" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">)(他)(们)(都)(不)(是)(真)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)( )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(。</p><p id="881b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">实例分割</strong> [ <a class="ae ko" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b?source=post_page---------------------------" rel="noopener"> SDS </a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-hypercolumn-instance-segmentation-367180495979?source=post_page---------------------------" rel="noopener" target="_blank">超列</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339?source=post_page---------------------------" rel="noopener" target="_blank">深度掩码</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61?source=post_page---------------------------" rel="noopener" target="_blank">清晰度掩码</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------" rel="noopener" target="_blank">多路径网络</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34?source=post_page---------------------------" rel="noopener" target="_blank"> MNC </a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92?source=post_page---------------------------" rel="noopener" target="_blank">实例中心</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=post_page---------------------------" rel="noopener" target="_blank"> FCIS </a></p><p id="8d49" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">超分辨率</strong>[<a class="ae ko" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c?source=post_page---------------------------" rel="noopener">SR CNN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-fsrcnn-super-resolution-80ca2ee14da4?source=post_page---------------------------" rel="noopener" target="_blank">fsr CNN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-vdsr-super-resolution-f8050d49362f?source=post_page---------------------------" rel="noopener" target="_blank">VDSR</a>][<a class="ae ko" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350?source=post_page---------------------------" rel="noopener">ESPCN</a>][<a class="ae ko" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e?source=post_page---------------------------" rel="noopener">红网</a>][<a class="ae ko" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20?source=post_page---------------------------" rel="noopener">DRCN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994?source=post_page---------------------------" rel="noopener" target="_blank">DRRN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8?source=post_page---------------------------" rel="noopener" target="_blank">LapSRN&amp;MS-LapSRN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8?source=post_page---------------------------" rel="noopener" target="_blank">SRDenseNet</a>][【T20</p><p id="962c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">人体姿态估计</strong><a class="ae ko" href="https://towardsdatascience.com/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------" rel="noopener" target="_blank">深度姿态</a><a class="ae ko" href="https://towardsdatascience.com/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=post_page---------------------------" rel="noopener" target="_blank">汤普逊·尼普斯【14】T5】</a><a class="ae ko" href="https://towardsdatascience.com/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------" rel="noopener" target="_blank">汤普逊·CVPR【15】T7】</a><a class="ae ko" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener">CPM</a><a class="ae ko" href="https://medium.com/@sh.tsang/review-fcgn-fully-convolutional-google-net-human-pose-estimation-52022a359cb3" rel="noopener">FCGN</a><a class="ae ko" href="https://medium.com/towards-artificial-intelligence/review-ief-iterative-error-feedback-human-pose-estimation-a56add160fa5" rel="noopener">IEF</a></p><p id="577e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">编解码后期处理</strong> [ <a class="ae ko" href="https://towardsdatascience.com/review-arcnn-deblocking-denoising-a098deeb792" rel="noopener" target="_blank"> ARCNN </a> ] [ <a class="ae ko" href="https://medium.com/@sh.tsang/review-cnn-for-h-264-hevc-compressed-image-deblocking-codec-post-processing-361a84e65b94" rel="noopener">林DCC ' 16</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-ifcnn-in-loop-filtering-using-convolutional-neural-network-codec-post-processing-1b89c8ddf417" rel="noopener">if CNN</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-cnn-for-compressed-image-deblocking-deblocking-44508bf99bdc" rel="noopener">李ICME ' 17</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-vrcnn-variable-filter-size-residue-learning-cnn-codec-post-processing-4a8a337ea73c" rel="noopener">VR CNN</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-dcad-deep-cnn-based-auto-decoder-codec-post-processing-e05a8f15f0c6" rel="noopener">DCAD</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-ds-cnn-decode-side-scalable-cnn-codec-post-processing-4bd85a4cfcfd" rel="noopener">DS-CNN</a></p><p id="3720" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">生成对抗网络</strong><a class="ae ko" href="https://medium.com/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75" rel="noopener">甘</a></p></div></div>    
</body>
</html>