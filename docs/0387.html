<html>
<head>
<title>Stateless vs Stateful LSTMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无状态与有状态LSTMs</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/stateless-vs-stateful-lstms-265c83116c49?source=collection_archive---------1-----------------------#2020-04-04">https://pub.towardsai.net/stateless-vs-stateful-lstms-265c83116c49?source=collection_archive---------1-----------------------#2020-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2453" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在机器学习中，通常假设训练样本是独立且同分布的(IID)。就序列数据而言，这并不总是正确的。如果序列值之间具有时间相关性，例如时间序列数据，则IID假设失败。</p><p id="e353" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，序列建模算法有两种风格，<strong class="jp ir">无状态</strong>和<strong class="jp ir">有状态</strong>，这取决于训练时使用的架构。下面以LSTM为例进行讨论，但是该概念也适用于其他变体，即RNN、GRU等。</p><p id="5eb9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当IID假设成立时，使用这种架构。在为培训创建批次时，这意味着批次之间没有相互关系，并且每个批次都是相互独立的。</p><p id="ffaa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">无状态LSTM架构中的典型培训过程如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/e915b7a26b92b97258fa502e940dd583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8qyYR1Yp9LvygdHN.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</figcaption></figure><p id="b9e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这两种体系结构的不同之处在于，随着训练从一批进展到另一批，模型(对应于每一批)的状态(单元和隐藏状态)被初始化的方式。这不要与参数/权重混淆，参数/权重无论如何都会在整个训练过程中传播(这是训练的全部)</p><p id="16b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上图中，每次接收和处理新批次时，LSTM的初始状态被重置为零，因此没有利用已经学习的内部激活(状态)。这迫使模型忘记来自先前批次的学习。</p><p id="e727" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">时序数据(如时间序列)包含非IID样本，因此假设划分的批次是独立的(实际上不是),这不是一个好主意。因此，直观地将学习到的状态传播到后续批次，使得模型不仅捕获每个样本序列内的时间相关性，还捕获批次间的时间相关性。<br/>(注意，对于文本数据，当一个句子代表一个序列时，通常假设语料库由独立的句子组成，它们之间没有联系。因此，选择无状态架构是安全的。每当这个假设不成立时，有状态是首选。)<br/>下面是有状态LSTM架构的样子:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lb"><img src="../Images/a771dc17076b1b847f38fb74998928cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qp7T1BeOyzcSV9fW.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</figcaption></figure><p id="60ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，使用来自前一批的学习状态来初始化每一批的LSTM的单元和隐藏状态，从而使模型学习跨批的依赖性。然而，状态在每个时期开始时被复位。下面是一个更细粒度的可视化图，显示了批次间的这种传播:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lc"><img src="../Images/ed546fb8f126892e3d39726d04f39218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oX2DCZ_9zMzNhTZ6.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</figcaption></figure><p id="49fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，位于索引I，X[i]的样本的状态将用于下一批中样本X[i + b s]的计算，其中b s是批大小。更准确地说，一批中索引I处的每个样本的最后状态将被用作下一批中索引I的样本的初始状态。在图中，每个样本序列的长度是4(时间步长)，时间步长t=4时的LSTM状态的值用于下一批中的初始化。</p><p id="96ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">观察:</strong> <br/> 1。随着批处理大小的增加，无状态LSTM倾向于有状态LSTM。<br/> 2。对于有状态的体系结构，批处理不会在内部被打乱(否则在无状态的情况下，这是默认的步骤)</p><p id="44dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献:</strong></p><ol class=""><li id="645f" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk li lj lk ll bi translated"><a class="ae lm" href="http://philipperemy.github.io/keras-stateful-lstm/" rel="noopener ugc nofollow" target="_blank">Keras中的有状态LSTM</a></li><li id="7644" class="ld le iq jp b jq ln ju lo jy lp kc lq kg lr kk li lj lk ll bi translated"><a class="ae lm" href="https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/" rel="noopener ugc nofollow" target="_blank">Python中时序预测的有状态和无状态LSTM</a></li></ol></div></div>    
</body>
</html>