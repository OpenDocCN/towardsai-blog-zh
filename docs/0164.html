<html>
<head>
<title>A Quick Guide to Gradient Descent and its Variants</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降及其变体快速指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-quick-guide-to-gradient-descent-and-its-variants-afa181d5b97b?source=collection_archive---------0-----------------------#2019-09-19">https://pub.towardsai.net/a-quick-guide-to-gradient-descent-and-its-variants-afa181d5b97b?source=collection_archive---------0-----------------------#2019-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7021" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">朝向AI的梯度下降及其变体|<a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank"/></h2><div class=""/><div class=""><h2 id="e424" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如果你不知道从哪里开始，优化学习速度</h2></div><p id="69fa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在机器学习中，人们处理一些数学模型<em class="ln"> z </em> = <em class="ln"> F </em> ( <em class="ln"> x，θ </em>)，它是一些输入变量x和一组参数<em class="ln"> θ </em>的函数。该模型可以是例如人工的并且可能是深度的神经网络。游戏的名字是从某个定义好的输入<em class="ln"> x </em>，<em class="ln">即</em>开始，寻找使预测某些目标<em class="ln"> y </em>(或分类问题中的标签)的误差最小的参数集，以尽可能减少数量<em class="ln"> F </em> ( <em class="ln"> x，θ </em> )- <em class="ln"> y </em>。借用<a class="ae lo" href="http://mathworld.wolfram.com/OptimizationTheory.html" rel="noopener ugc nofollow" target="_blank"> <em class="ln">最优化理论</em> </a>中的行话和一些思路，如果模型<em class="ln"> F </em> ( <em class="ln"> x </em>，<em class="ln"> θ </em>)是一个可微函数，<em class="ln">即</em>它<em class="ln"> </em>对其定义域中的每一点都有导数，原则上只要遵循它的梯度∇ <em class="ln"> F </em> ( <em class="ln"> x </em>，)这相当于到达了物理学中某个能量函数的地平面，或者走下坡路，直到我们最终到达一个山谷。问题是，复杂的模型通常有许多"<em class="ln">假"</em>极小值:如果我们继续寻找，最终我们可能会找到<em class="ln">真</em>或<em class="ln">全局</em>极小值。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/0da52b80dbac39abcc1931a276854407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*09kq2L23D9XM_9Xtr8gc8Q.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">梯度下降是一种在<em class="mf"> N </em>维中寻找函数最小值的算法。然而，一些函数是复杂的，并显示多个最小值。</figcaption></figure><h2 id="f945" class="mg mh it bd mi mj mk dn ml mm mn dp mo la mp mq mr le ms mt mu li mv mw mx iz bi translated">基本梯度下降算法</h2><p id="701b" class="pw-post-body-paragraph kr ks it kt b ku my kd kw kx mz kg kz la na lc ld le nb lg lh li nc lk ll lm im bi translated">在最基本的形式中，GD被应用于所有训练示例，因此它被称为<strong class="kt jd"> <em class="ln">批次</em>梯度下降</strong>。关键要素是学习率参数λ，它表示当我们从参数空间中的一个点转到另一个点时，更新参数<em class="ln"> θ </em>所采取的步长:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/2af289a9c239e6583ca19fcf4789b266.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*rzK0vNI1M4TdLT_C4sGaOw.png"/></div></figure><p id="d5be" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">批量梯度下降保证对于凸函数(想象抛物线)收敛到全局最小值，对于非凸函数收敛到局部最小值。但是，我们如何跳到全局最小值呢？此外，如果数据集非常大，此过程可能会非常慢，或者根本不适合内存。</p><p id="354d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="ln">随机</em>梯度下降</strong> ( <strong class="kt jd"> SGD </strong>)前来救援:对每个训练样本进行参数更新。SGD是嘈杂的，但最初看起来是一个问题的东西实际上是它成功的关键:它允许从最小值“跳出来”,并继续探索参数空间。实践经验表明，如果我们缓慢地降低学习速率，在某一点上，SGD收敛到一个最小值比原始的批量GD更有效。但现在我们有了另一个问题:我们如何找出λ的最佳值，或者当我们向最小值下降时，我们应该如何改变这个参数？</p><h2 id="8a90" class="mg mh it bd mi mj mk dn ml mm mn dp mo la mp mq mr le ms mt mu li mv mw mx iz bi translated">优化的梯度下降算法</h2><p id="5b6d" class="pw-post-body-paragraph kr ks it kt b ku my kd kw kx mz kg kz la na lc ld le nb lg lh li nc lk ll lm im bi translated">众所周知，SGD在某些区域梯度快速变化的情况下表现不佳，尤其是在某些维度上(因此得名<em class="ln">峡谷</em>)。一个聪明的想法是从冶金学借鉴来的，在冶金学中，一系列的加热和冷却被用来模拟某些属性。在最优化理论中，这被称为<strong class="kt jd"> <em class="ln">动量</em> </strong>:它通过阻尼振荡在相关方向上加速SGD。动量参数γ(通常设置为0.9)类似于在保守力场中穿过粘性介质的牛顿粒子的质量。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/be24b8eb55241362f20c5eda184bb16f.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*UAp4W9rCJRy5n1OZJwbV4g.png"/></div></figure><p id="a38e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所有这些看起来都很聪明，但是如果我们到达谷底并继续加速会发生什么呢？我们会在别的地方结束，也许那不是我们想要的。所以我们应该更好地改变动量的值，来考虑我们离最小值有多远。这是用<strong class="kt jd">内斯特罗夫加速梯度</strong> ( <strong class="kt jd"> NAG </strong>)获得的，它在下降方向更新之前“向前看”:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/560359e66c8a53bc03222894a1607ed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*vntJrys2bdqcJUojFUuPMQ.png"/></div></figure><h2 id="0e8a" class="mg mh it bd mi mj mk dn ml mm mn dp mo la mp mq mr le ms mt mu li mv mw mx iz bi translated">自适应算法</h2><p id="505d" class="pw-post-body-paragraph kr ks it kt b ku my kd kw kx mz kg kz la na lc ld le nb lg lh li nc lk ll lm im bi translated">最终的智能水平是能够根据参数<em class="ln"> θ </em>微调学习速率λ。例如，我们可能希望在梯度变化不大的地方使用较小的λ，而在梯度变化较频繁的地方使用较大的λ。<a class="ae lo" href="https://medium.com/konvergen/an-introduction-to-adagrad-f130ae871827" rel="noopener"> <strong class="kt jd"> Adagrad </strong> </a>算法更新为每个参数分配一个学习率。学习率被自适应地更新，<em class="ln">，即</em>相对于在每个维度的每次迭代中累积的平方梯度进行缩放。<strong class="kt jd"> Adadelta </strong>通过将过去的梯度累积到固定的历史深度来改进算法。<strong class="kt jd"> RMSprop </strong>将学习率除以梯度平方的指数衰减平均值:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/2581c2b992eedfb39a283e09e5146917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*92wYoSlinKsPkZ6mEMBHAA.png"/></div></figure><p id="a353" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后，<strong class="kt jd"> Adam </strong>(自适应矩估计)跟踪平均平方梯度(如在RMSprop中)以及梯度，<em class="ln">，即梯度分布的一阶(均值)和二阶(方差)矩的</em>。最近，Adam的进一步改进版本被提出，称为<a class="ae lo" href="https://openreview.net/pdf?id=Bkg3g2R9FX" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> AdaBound </strong> </a>，声称能够训练出“和Adam一样快，和SGD一样好”的模型。它采用动态上限和下限的学习率。</p><h2 id="8dda" class="mg mh it bd mi mj mk dn ml mm mn dp mo la mp mq mr le ms mt mu li mv mw mx iz bi translated">用什么？</h2><p id="01f2" class="pw-post-body-paragraph kr ks it kt b ku my kd kw kx mz kg kz la na lc ld le nb lg lh li nc lk ll lm im bi translated">答案，不言而喻，取决于问题。毫不奇怪，实践经验暗示了使用RMSprop和Adam等自适应算法的优势。有趣的是，有些人声称，当训练一个生成-对抗网络时，SGD会更好。如果您不知道从哪里开始，请使用Adam并记住:如果您没有太多时间，并且您只能优化一个超参数，请关注学习率。</p></div></div>    
</body>
</html>