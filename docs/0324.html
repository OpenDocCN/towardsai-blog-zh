<html>
<head>
<title>BatchNorm for Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迁移学习的批处理方式</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/batchnorm-for-transfer-learning-df17d2897db6?source=collection_archive---------0-----------------------#2020-02-22">https://pub.towardsai.net/batchnorm-for-transfer-learning-df17d2897db6?source=collection_archive---------0-----------------------#2020-02-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e622" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让您的网络在TensorFlow 2+中更快地学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/04127660b12731f87b3acc733883e357.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v6uw_sUrE0c0Jn_L1zGjOA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">资料来源:Pexels</figcaption></figure><p id="dd99" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">仍然有很多模型使用批量标准化图层。如果你想在这样的模型上进行迁移学习，你会遇到很多问题。微调AlexNet或VGG要容易得多，因为它们不包含批处理规范层。许多开发人员都很好奇为什么更现代的CNN架构不能像旧的架构一样运行良好。</p><p id="d2c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我跑了好几次批定额层的问题。我以为是模型优化出了问题。然后我就找到了Datumbox关于Keras中<a class="ae lr" href="https://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/" rel="noopener ugc nofollow" target="_blank">批次定额</a>层坏掉的文章。问题是当层被冻结时，它在训练期间继续使用小批量统计。作者请求的拉动未被<a class="ae lr" href="https://github.com/keras-team/keras/pull/9965" rel="noopener ugc nofollow" target="_blank">合并</a>。然而，新的TensorFlow 2应该<a class="ae lr" href="https://github.com/tensorflow/tensorflow/blob/eda53c63dab8b364872ede8e423e4fed5d1686f7/tensorflow/python/keras/layers/normalization_v2.py#L26-L65" rel="noopener ugc nofollow" target="_blank">在处理批处理规范层时表现得更好一点</a>。有些人仍然对TF2的<em class="ls"> tf.keras.applications </em>有<a class="ae lr" href="https://github.com/keras-team/keras/pull/9965#issuecomment-549126009" rel="noopener ugc nofollow" target="_blank">的问题</a>，需要向模型重新注入新的批量规范层。</p><p id="1c09" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其他人还在<a class="ae lr" href="https://forums.fast.ai/t/transfer-learning-in-fast-ai-how-does-the-magic-work/55620/20" rel="noopener ugc nofollow" target="_blank">疑惑</a>为什么他们的TensorFlow模型在微调他们的模型时比PyTorch差。默认的伽玛参数导致了TF2的一个常见问题。PyTorch中的默认gamma参数是0.9。但是在TensorFlow中是<strong class="kx ir"> 0.99 </strong>。</p><p id="0816" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">如果您在预训练的imagenet模型上进行迁移学习，并且:</strong>，此数字不是最佳值</p><ul class=""><li id="6269" class="lt lu iq kx b ky kz lb lc le lv li lw lm lx lq ly lz ma mb bi translated">您的数据集来自不同于“Imagenet”的域</li><li id="7d24" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">您的数据集只有几千幅图像</li><li id="85e8" class="lt lu iq kx b ky mc lb md le me li mf lm mg lq ly lz ma mb bi translated">您使用的是小批量</li></ul><p id="90db" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">TF2和PyTorch之间的另一个区别是当冻结/解冻预训练主干模型时，框架是如何表现的。</strong></p><h1 id="9e7f" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">冻结层时出现批次异常</h1><p id="ddb5" class="pw-post-body-paragraph kv kw iq kx b ky mz jr la lb na ju ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">如果你正在冻结预训练的主干模型，那么我推荐你看看Keras创建者Franç ois Chollet的这个<a class="ae lr" href="https://colab.research.google.com/drive/17vHSAj7no7RMdJ18MJomTf8twqw1suYC#scrollTo=-TwK3BeMO5FF" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir"> colab </strong> </a>页面。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="39a5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">设置base_model(inputs，training=False)将使批范数层在训练期间停止更新不可训练的参数，这在冻结和解冻期间是至关重要的。<strong class="kx ir">请注意，此设置不会冻结您的base_model </strong>中的可训练重量。如果要冻结基础模型的可训练权重，则设置base _ model.trainable = False。但是不要忘记首先调用base_model(inputs，training=False ),因为BatchNormalization层包含不可训练的参数，即使base _ model.trainable = False，这些参数仍在变化。只要看看<a class="ae lr" href="https://colab.research.google.com/drive/17vHSAj7no7RMdJ18MJomTf8twqw1suYC" rel="noopener ugc nofollow" target="_blank"> colab </a>文件，就更有意义了！</p><h1 id="a463" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">让我们用动量做实验</h1><p id="30eb" class="pw-post-body-paragraph kv kw iq kx b ky mz jr la lb na ju ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">让我们看看当你在模型中进行迁移学习时，动量参数是什么意思。我们将挑选两个数据集用于我们的实验"<a class="ae lr" href="https://www.tensorflow.org/datasets/catalog/imagenette" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir"> imagenette </strong> </a>"和"<a class="ae lr" href="https://www.tensorflow.org/datasets/catalog/colorectal_histology" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">结肠直肠_组织学</strong> </a> <strong class="kx ir">"</strong>我们在这个实验中的模型是未冻结的，这意味着所有层/权重都是可训练的。</p><p id="770f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第一个数据集“<strong class="kx ir"> imagenette，</strong>”具有类似于Imagenet的图像。这就是为什么该模型可以很容易地拟合，因为它是在Imagenet上预先训练的。图像的BatchNorm统计类似于原始数据集。我们可以在下图中看到。这只是一个简单的例子，表明模型可以轻松实现几乎100 %的准确性，因此忽略过度拟合:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/8c5c18010b862d1e6ee911ca451e9416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q37805BmPVPZEf8rDOZ0WA.png"/></div></div></figure><p id="96cd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们将尝试拟合“<strong class="kx ir">结肠直肠_组织学，</strong>”图看起来是这样的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/a92ade1e8aa416dff0a815daf3b489a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYu3vKrK6xKCGFDeOfAPIQ.png"/></div></div></figure><p id="fc68" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们应用此修复将批规范的参数从0.99更改为0.9，这样层将学习得更快一些。我们将创建BatchNorm层，并将其注入到模型中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="9038" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">用固定的0.9动量参数重新注入批规范层。</p><p id="f1a9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在这种模式的学习看起来好多了:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/e6fa1d1c89ebd13543302bca6daec824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDSO7YoI9_p8S3BMOLAGAw.png"/></div></div></figure><h1 id="3933" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">小批量</h1><p id="e1ae" class="pw-post-body-paragraph kv kw iq kx b ky mz jr la lb na ju ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">一些项目和团队使用批量重整化[ <a class="ae lr" href="https://arxiv.org/pdf/1702.03275.pdf" rel="noopener ugc nofollow" target="_blank">源</a>层，而不是批量规格化。这项技术看起来非常有希望用于大型网络(对象检测等)，因为你不能使用大批量，因为你无法在GPU卡上安装更大的批量。<strong class="kx ir">批量重正化</strong>层实现为张量流批量规格化层中的参数'<strong class="kx ir">重正化</strong>'。该论文的作者用第一个5000步来拟合模型，其中参数rmax = 1，dmax = 0(经典的批处理规范化)。经过这些初始步骤后，它们逐渐增加到rmax =3和dmax = 5。你可以在TensorFlow 2+中通过编写自己的<strong class="kx ir">回调</strong>来做到这一点。如果你想知道这种技术是如何工作的，那就去读原文吧。还有其他有前途的规范化技术，看起来比经典的BatchNorm更好，如<strong class="kx ir">组规范化</strong> [ <a class="ae lr" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">源</a>。GroupNorm在TF Addons库中实现。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/ec180dd3b6f1a7e05f22a06c1f25fd3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHvgqhtqYv8yNlAKRJHXGw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">批次重正化:减少批次标准化模型中的小批次依赖性[ <a class="ae lr" href="https://arxiv.org/pdf/1702.03275.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="ab3c" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">综上</h1><p id="3fdb" class="pw-post-body-paragraph kv kw iq kx b ky mz jr la lb na ju ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">如果您的模型包含批处理规范化层，请注意它会在培训期间或在生产环境中部署时给您带来许多问题。要了解更多关于最佳实践的信息，我推荐一个很棒的<a class="ae lr" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>课程和论坛。另见我们的<a class="ae lr" href="https://medium.com/@michallukac/preprocessing-layer-in-cnn-models-for-tf2-d471e61ddc2e" rel="noopener">上一篇关于预处理层的文章</a>，它可以节省你很多时间。</p><p id="17ed" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从Imagenet上的预训练模型开始有时(例如，大多数医疗保健图片)不是最佳的，不同的<a class="ae lr" href="https://www.fast.ai/2020/01/13/self_supervised/" rel="noopener ugc nofollow" target="_blank">预训练</a>技术非常有意义。</p><p id="f42c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">米哈尔</p></div></div>    
</body>
</html>