<html>
<head>
<title>How to Get Deterministic word2vec/doc2vec/paragraph Vectors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何获得确定性的word2vec/doc2vec/paragraph向量</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-get-same-word2vec-doc2vec-paragraph-vectors-in-every-time-of-training-335bac809c83?source=collection_archive---------1-----------------------#2018-11-06">https://pub.towardsai.net/how-to-get-same-word2vec-doc2vec-paragraph-vectors-in-every-time-of-training-335bac809c83?source=collection_archive---------1-----------------------#2018-11-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/70acedde766255842350113467c63b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sXNXYfAqfLUeiDXPCo130w.png"/></div></div></figure><figure class="iz ja jb jc gt is"><div class="bz fp l di"><div class="jd je l"/></div></figure><h2 id="24f0" class="jf jg jh bd b dl ji jj jk jl jm jn dk jo translated" aria-label="kicker paragraph">面向AI的文字嵌入和语言建模|<a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank"/></h2><div class=""/><p id="6ad4" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">好了，欢迎来到我们的<strong class="kp jr">嵌字系列</strong>。这篇文章是这个系列的第一个故事。你可能会发现这个故事适合中级以上，在<a class="ae ll" href="https://skymind.ai/wiki/word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec，或doc2vec/paragraph vectors </a>上训练过或至少尝试过一次的人。但是不要担心，我会在接下来的文章中介绍背景、先决条件和知识以及代码如何实现它。</p><p id="d2be" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我会尽力不把你重定向到其他一些要求你阅读乏味教程并以放弃告终的链接(相信我，我是大量在线教程的受害者:)。我想让你<strong class="kp jr">和我一起从编码层面</strong>理解单词向量，这样我们就可以知道<strong class="kp jr">如何设计和实现我们的单词嵌入和语言模型</strong>。</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="a390" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">如果你有机会自己训练单词向量，你可能会发现，即使你输入相同的训练数据，每次训练的模型和向量表示也是不同的。这是因为在训练时间中引入了随机性。代码会自己说话，我们来看看随机性从何而来，如何彻底消除。我将使用<a class="ae ll" href="https://deeplearning4j.org/" rel="noopener ugc nofollow" target="_blank"> DL4j </a>的<a class="ae ll" href="https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/paragraphvectors/ParagraphVectors.java" rel="noopener ugc nofollow" target="_blank">实现</a>的<a class="ae ll" href="https://deeplearning4j.org/docs/latest/deeplearning4j-nlp-doc2vec" rel="noopener ugc nofollow" target="_blank">段向量</a>来显示代码。如果你想看看另一个包，去<a class="ae ll" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank"> gensim的doc2vec </a>，它有相同的实现方法。</p><h1 id="42b9" class="lt lu jh bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">随机性从何而来</h1><h2 id="e3b4" class="mr lu jh bd lv ms mt dn lz mu mv dp md ky mw mx mh lc my mz ml lg na nb mp jn bi translated">模型权重和向量表示的初始化</h2><p id="bd6c" class="pw-post-body-paragraph kn ko jh kp b kq nc ks kt ku nd kw kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">我们知道在训练之前，模型和向量表示的权重会随机初始化，而随机性是由<a class="ae ll" href="https://en.wikipedia.org/wiki/Random_seed" rel="noopener ugc nofollow" target="_blank">种子</a>控制的。因此，如果我们将种子设置为0，那么每次都会得到完全相同的初始化。这里是<a class="ae ll" href="https://github.com/deeplearning4j/deeplearning4j/blob/3c66853115e0d500cf1cafc0e060478766de4f03/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/inmemory/InMemoryLookupTable.java#L141" rel="noopener ugc nofollow" target="_blank">种子生效的地方</a>。这里<code class="fe nh ni nj nk b">syn0</code>是模型权重，由<code class="fe nh ni nj nk b">Nd4j.rand</code>初始化</p><pre class="iz ja jb jc gt nl nk nm nn aw no bi"><span id="8d02" class="mr lu jh nk b gy np nq l nr ns">// Nd4j takes seed configuration here<br/>Nd4j.<em class="nt">getRandom</em>().setSeed(configuration.getSeed());</span><span id="19da" class="mr lu jh nk b gy nu nq l nr ns">// Nd4j initializes a random matrix for syn0<br/>syn0 = Nd4j.rand(new int[] {vocab.numWords(), vectorLength}, rng).subi(0.5).divi(vectorLength);</span></pre><h2 id="5c9f" class="mr lu jh bd lv ms mt dn lz mu mv dp md ky mw mx mh lc my mz ml lg na nb mp jn bi translated">PV-DBOW算法</h2><p id="9273" class="pw-post-body-paragraph kn ko jh kp b kq nc ks kt ku nd kw kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">如果我们使用<a class="ae ll" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank"> PV-DBOW </a>算法(我将在下面的帖子中详细解释)来训练段落向量，在训练的迭代过程中，它会从文本窗口中随机地对单词进行二次采样，以计算并更新权重。但这种随机并不是真正的随机。让我们看看<a class="ae ll" href="https://github.com/deeplearning4j/deeplearning4j/blob/3c66853115e0d500cf1cafc0e060478766de4f03/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/sequencevectors/SequenceVectors.java#L1205" rel="noopener ugc nofollow" target="_blank">代码</a>。</p><pre class="iz ja jb jc gt nl nk nm nn aw no bi"><span id="bcfe" class="mr lu jh nk b gy np nq l nr ns">// next random is an AtomicLong initialized by thread id<br/>this.nextRandom = new AtomicLong(this.threadId);</span></pre><p id="e41d" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><code class="fe nh ni nj nk b">nextRandom</code>用于</p><pre class="iz ja jb jc gt nl nk nm nn aw no bi"><span id="cb1e" class="mr lu jh nk b gy np nq l nr ns">trainSequence(sequence, <strong class="nk jr">nextRandom</strong>, alpha);</span></pre><p id="9478" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">在<code class="fe nh ni nj nk b">trainSequence</code>里面，它会做什么</p><pre class="iz ja jb jc gt nl nk nm nn aw no bi"><span id="cfc1" class="mr lu jh nk b gy np nq l nr ns">nextRandom.set(nextRandom.get() * 25214903917L + 11);</span></pre><p id="0c33" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">如果我们在训练步骤上更深入，我们会发现它以同样的方式产生<code class="fe nh ni nj nk b">nextRandom</code>，即做同样的数学运算(去<a class="ae ll" href="https://en.wikipedia.org/wiki/Linear_congruential_generator" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae ll" href="http://www.cburch.com/logisim/docs/2.3.0/libs/mem/random.html" rel="noopener ugc nofollow" target="_blank">这个</a>知道为什么)，所以这个数字只依赖于线程id，其中线程id是0，1，2，3，…。因此，它不再是随机的。</p><h2 id="4db8" class="mr lu jh bd lv ms mt dn lz mu mv dp md ky mw mx mh lc my mz ml lg na nb mp jn bi translated">并行令牌化</h2><p id="1965" class="pw-post-body-paragraph kn ko jh kp b kq nc ks kt ku nd kw kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">由于复杂文本的处理可能是耗时的，所以它用于并行标记，并行标记可以帮助提高性能，但不能保证训练之间的一致性。由tokenizer处理的序列可以以随机顺序送入线程进行训练。从<a class="ae ll" href="https://github.com/deeplearning4j/deeplearning4j/blob/3c66853115e0d500cf1cafc0e060478766de4f03/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/word2vec/wordstore/VocabConstructor.java#L257-L264" rel="noopener ugc nofollow" target="_blank">代码</a>中可以看出，如果我们将<code class="fe nh ni nj nk b">allowParallelBuilder</code>设置为false，那么正在进行标记化的<code class="fe nh ni nj nk b">runnable</code>将一直等待，直到标记化完成，这样就可以保持给料数据的顺序。</p><pre class="iz ja jb jc gt nl nk nm nn aw no bi"><span id="0437" class="mr lu jh nk b gy np nq l nr ns"><strong class="nk jr">if </strong>(!<strong class="nk jr">allowParallelBuilder</strong>) {<br/>    <strong class="nk jr">try </strong>{<br/>        runnable.awaitDone();<br/>    } <strong class="nk jr">catch </strong>(InterruptedException e) {<br/>        Thread.<em class="nt">currentThread</em>().interrupt();<br/>        <strong class="nk jr">throw new </strong>RuntimeException(e);<br/>    }<br/>}</span></pre><h2 id="3302" class="mr lu jh bd lv ms mt dn lz mu mv dp md ky mw mx mh lc my mz ml lg na nb mp jn bi translated">为每个要训练的线程提供序列的队列</h2><p id="831e" class="pw-post-body-paragraph kn ko jh kp b kq nc ks kt ku nd kw kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">这个<a class="ae ll" href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingQueue.html" rel="noopener ugc nofollow" target="_blank"> LinkedBlockingQueue </a>从训练文本的迭代器中获取序列，并将这些序列提供给每个线程。由于每个线程可以随机出现，所以在每次训练中，每个线程可以得到不同的序列进行训练。让我们看看这个数据提供者的<a class="ae ll" href="https://github.com/deeplearning4j/deeplearning4j/blob/3c66853115e0d500cf1cafc0e060478766de4f03/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/sequencevectors/SequenceVectors.java#L1037-L1130" rel="noopener ugc nofollow" target="_blank">实现</a>。</p><pre class="iz ja jb jc gt nl nk nm nn aw no bi"><span id="3d1d" class="mr lu jh nk b gy np nq l nr ns">// initialize a sequencer to provide data to threads<br/>val sequencer = new AsyncSequencer(this.iterator, this.stopWords);</span><span id="8f70" class="mr lu jh nk b gy nu nq l nr ns">// each threads are pointing to the same sequencer <br/>// worker is the number of threads we want to use<br/>for (int x = 0; x &lt; workers; x++) {<br/>    threads.add(x, new VectorCalculationsThread(x, ..., sequencer);                <br/>    threads.get(x).start();            <br/>}</span><span id="61b2" class="mr lu jh nk b gy nu nq l nr ns">// sequencer will initialize a LinkedBlockingQueue buffer<br/>// and maintain the size between [limitLower, limitUpper]<br/>private final LinkedBlockingQueue&lt;Sequence&lt;T&gt;&gt; buffer;<br/>limitLower = workers * batchSize;<br/>limitUpper = workers * batchSize * 2;</span><span id="2aaa" class="mr lu jh nk b gy nu nq l nr ns">// threads get data from the queue through<br/>buffer.poll(3L, TimeUnit.SECONDS);</span></pre><p id="3280" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">因此，如果我们将一个工人的数量设置为1，它将在一个单独的线程中运行，并且在每次训练中具有完全相同的数据输入顺序。但是请注意，单线程会极大地降低训练速度。</p><h2 id="23cc" class="mr lu jh bd lv ms mt dn lz mu mv dp md ky mw mx mh lc my mz ml lg na nb mp jn bi translated">总结</h2><p id="47c4" class="pw-post-body-paragraph kn ko jh kp b kq nc ks kt ku nd kw kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">总结一下，下面是我们要彻底排除随机性需要做的:<br/> <strong class="kp jr"> 1。将种子设置为0；<br/> 2。将allowParallelTokenization设置为false<br/> 3。将工作线程数设置为1。</strong> <br/>那么如果馈入相同的数据，我们会得到完全相同的单词向量和段落向量的结果。</p><p id="8622" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">最后，我们训练的代码是这样的:</p><pre class="iz ja jb jc gt nl nk nm nn aw no bi"><span id="9b4c" class="mr lu jh nk b gy np nq l nr ns">ParagraphVectors vec = <strong class="nk jr">new</strong> ParagraphVectors.Builder()<br/>                .minWordFrequency(1)<br/>                .labels(labelsArray)<br/>                .layerSize(100)<br/>                .stopWords(<strong class="nk jr">new</strong> ArrayList&lt;String&gt;())<br/>                .windowSize(5)<br/>                .iterate(iter)<br/>                .allowParallelTokenization(false)<br/>                .workers(1)<br/>                .seed(0)<br/>                .tokenizerFactory(t)<br/>                .build();<br/><br/>vec.fit();</span></pre></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="d446" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">如果你觉得</p><figure class="iz ja jb jc gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1fd31262d2c594aab8802a466788c5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*fpOKv2N_R2F-_TXDUhea_A.gif"/></div></figure><p id="313e" class="pw-post-body-paragraph kn ko jh kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">请关注接下来关于<strong class="kp jr">单词嵌入和语言模型</strong>的故事，我为你准备了盛宴。</p><h2 id="087b" class="mr lu jh bd lv ms mt dn lz mu mv dp md ky mw mx mh lc my mz ml lg na nb mp jn bi translated">参考</h2><blockquote class="nw nx ny"><p id="62ab" class="kn ko nt kp b kq kr ks kt ku kv kw kx nz kz la lb oa ld le lf ob lh li lj lk ij bi translated">[1] Deeplearning4j、ND4J、DataVec等——面向Java/Scala的深度学习和线性代数，采用GPU+Spark——来自sky mind<a class="ae ll" href="http://deeplearning4j.org/" rel="noopener ugc nofollow" target="_blank">http://deeplearning4j.org</a><a class="ae ll" href="https://github.com/deeplearning4j/deeplearning4j" rel="noopener ugc nofollow" target="_blank">https://github.com/deeplearning4j/deeplearning4j</a></p><p id="792d" class="kn ko nt kp b kq kr ks kt ku kv kw kx nz kz la lb oa ld le lf ob lh li lj lk ij bi translated">[2] Java平台，标准版8 API规范<a class="ae ll" href="https://docs.oracle.com/javase/8/docs/api/" rel="noopener ugc nofollow" target="_blank">https://docs.oracle.com/javase/8/docs/api/</a></p><p id="5f3d" class="kn ko nt kp b kq kr ks kt ku kv kw kx nz kz la lb oa ld le lf ob lh li lj lk ij bi translated">[3]https://giphy.com/<a class="ae ll" href="https://giphy.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="fde7" class="kn ko nt kp b kq kr ks kt ku kv kw kx nz kz la lb oa ld le lf ob lh li lj lk ij bi translated">[4]https://images.google.com/<a class="ae ll" href="https://images.google.com/" rel="noopener ugc nofollow" target="_blank"/></p></blockquote></div></div>    
</body>
</html>