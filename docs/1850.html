<html>
<head>
<title>A Gentle Introduction to Hint Learning &amp; Knowledge Distillation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">暗示学习的温和介绍&amp;知识蒸馏</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-gentle-introduction-to-knowledge-distillation-6240bf8eb8ea?source=collection_archive---------1-----------------------#2021-05-16">https://pub.towardsai.net/a-gentle-introduction-to-knowledge-distillation-6240bf8eb8ea?source=collection_archive---------1-----------------------#2021-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c477" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="1c57" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">引导学生学习老师的行为。</h2></div><p id="9e6e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">知识提取是一种从复杂的模型集合中提取知识并将其压缩到一个模型中的方法，目的是使现实生活中的应用成为可能。知识蒸馏是由人工智能教父杰弗里·辛顿(Geoffrey Hinton)和他在谷歌的两位同事奥里奥尔·维尼亚尔斯(Oriol Vinyals)和杰夫·迪恩(Jeff Dean)在2015年提出的。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/1ed0b021acc32aa561d5e5122aa38865.png" data-original-src="https://miro.medium.com/v2/format:webp/1*i9J0_ZeRTfRpAWn7ERhgMg.jpeg"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">图片来自Prakhar的帖子[ <a class="ae lw" href="https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="3535" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">知识升华是指将一个繁琐的模型(教师)的学习行为转移到一个更小的模型(学生)中，其中教师产生的输出被用作训练学生的“软目标”。通过应用这种方法，作者揭示了他们在MNIST数据集上获得了令人惊讶的结果，并表明通过将模型集合中的知识提取到单个模型中可以获得显著的改进。</p><h1 id="d3f2" class="lx ly iq bd lz ma mb mc md me mf mg mh kf mi kg mj ki mk kj ml kl mm km mn mo bi translated">用于图像分类的知识蒸馏</h1><p id="1fef" class="pw-post-body-paragraph ko kp iq kq b kr mp ka kt ku mq kd kw kx mr kz la lb ms ld le lf mt lh li lj ij bi translated">Hinton和他的两位合著者首先在论文中介绍了他们对图像分类任务的知识提取:<a class="ae lw" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">在神经网络</a>中提取知识。如文中所述，知识提取的最简单形式是在具有软目标分布的转移集上训练提取的模型。到目前为止，我们应该知道有两个目标用于训练学生模型。一个是正确的标签(硬目标)，另一个是从教师网生成的软标签(软目标)。因此，目标函数是两个不同目标函数的加权平均值。第一个目标函数是学生预测和软目标之间的交叉熵损失，第二个目标函数是学生输出和正确标签之间的交叉熵损失。作者还提到，最好的结果通常是通过对第二目标函数使用较低的权重而获得的。</p><p id="2727" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从该论文中获得的一些令人惊讶的结果如下所示，更多详情，请参见原文<a class="ae lw" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">此处</a>:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/de4f9afb30d49432a4b4512f8c82ebfe.png" data-original-src="https://miro.medium.com/v2/format:webp/1*jQ6nBhNBD8ATbuzHT0a4uQ.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">图片来自纸张[ <a class="ae lw" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">来源</a> ]</figcaption></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/263e9268b4235b4bde1a8ec91136a75d.png" data-original-src="https://miro.medium.com/v2/format:webp/1*8oUXQFQYq2Z-WDxMvFJ0_w.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">图片来自纸张[ <a class="ae lw" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="fc46" class="lx ly iq bd lz ma mb mc md me mf mg mh kf mi kg mj ki mk kj ml kl mm km mn mo bi translated">用于物体检测的知识蒸馏</h1><p id="7204" class="pw-post-body-paragraph ko kp iq kq b kr mp ka kt ku mq kd kw kx mr kz la lb ms ld le lf mt lh li lj ij bi translated">在NeurIPS 2017中，陈国斌和他的合著者在论文中发表了他们关于结合提示学习的知识提炼用于对象检测的研究:<a class="ae lw" href="https://paperswithcode.com/paper/learning-efficient-object-detection-models" rel="noopener ugc nofollow" target="_blank">用知识蒸馏</a>学习高效的对象检测模型。在他们的方法中，他们进一步使用了一个<em class="mu">提示</em>，这是从教师的中间层获得的特征图，用于指导学生尽可能接近地学习教师的行为。此外，适应层对于提取知识的最佳性能是必要的，这个适应层将在后面讨论。fast-RCNN是本文实验中使用的目标检测网络。他们的学习方案可以形象化为下图:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/b9eb857e91a297529d3f68ad7735e94e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MJt8cSXRqddoEmtDI8gyZg.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">图片来自纸张。【<a class="ae lw" href="https://paperswithcode.com/paper/learning-efficient-object-detection-models" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="90d5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">学习目标函数写如下:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/529c82d1fadaf2ca7be5120d76b9c136.png" data-original-src="https://miro.medium.com/v2/format:webp/1*AENI4wosAKlsNZc1uuNYwA.png"/></div></figure><p id="712b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中RCN和RPN分别代表回归分类网络和区域建议网络；n和M分别是RCN和RPN的批量；L_RCN、L_RPN和L_Hint分别是RCN、RPN和Hint的损耗；λ(通常为1)和γ(通常设置为0.5)是控制最终损耗的超参数。</p><h1 id="4352" class="lx ly iq bd lz ma mb mc md me mf mg mh kf mi kg mj ki mk kj ml kl mm km mn mo bi translated">暗示学习</h1><p id="82af" class="pw-post-body-paragraph ko kp iq kq b kr mp ka kt ku mq kd kw kx mr kz la lb ms ld le lf mt lh li lj ij bi translated">Adriana Romero在论文<a class="ae lw" href="https://arxiv.org/abs/1412.6550" rel="noopener ugc nofollow" target="_blank">fit Nets:Hints for Thin Deep Nets</a>中已经证明，当教师网络的中间表示被用作<em class="mu">提示</em>来帮助学生的训练过程时，学生网络的性能可以得到提高。在这个意义上，使用L1距离计算<em class="mu">提示特征Z </em>(从教师的中间层获得的特征图)和<em class="mu">引导特征V </em>(学生的中间层的特征图)之间的损失，</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/9cc7008c8d38fc8ef6190b59286da5f4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*37zJVAd4KAtoR-oFf5vwJg.png"/></div></figure><p id="08b3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">或者L2距离，</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/bf1768fe09a8122ff79ef7b870bdd46b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*fhSU9-Uj1aG9Pko8KeO0Ow.png"/></div></figure><p id="0598" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下图显示了从在WAYMO数据集上训练的预训练YOLOv4模型中提取的特征图，这是我的一个关于利用知识提取进行对象检测的项目。在这些示例中，输入图像的大小调整为800x800。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/2a1142a3e7e16f56230dbedb3a79d1c4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*W34yPolSPL_-ZEJ764WsMg.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">图片作者。</figcaption></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/e04065f7601a99616c3a11ddc8a87d56.png" data-original-src="https://miro.medium.com/v2/format:webp/1*r39Gq0gdAx-hK3yGHIIUPg.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">图片作者。</figcaption></figure><h1 id="f062" class="lx ly iq bd lz ma mb mc md me mf mg mh kf mi kg mj ki mk kj ml kl mm km mn mo bi translated">知识蒸馏+暗示学习</h1><p id="98e1" class="pw-post-body-paragraph ko kp iq kq b kr mp ka kt ku mq kd kw kx mr kz la lb ms ld le lf mt lh li lj ij bi translated">使用提示学习要求<em class="mu">提示特征</em>和<em class="mu">引导特征</em>具有相同的形状(高度x宽度x通道)。此外，<em class="mu">提示特征</em>和<em class="mu">引导特征</em>将总是不在相似的特征空间中，因此，使用适配层(通常是1x1卷积层)来帮助改善从教师到学生的知识传递。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/3ad0d9d9bbbbde57c5a823cd32e67673.png" data-original-src="https://miro.medium.com/v2/format:webp/1*E_RaZ2yqUR_s0rI1kLoRZg.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">图片改编自Prakhar的帖子[ <a class="ae lw" href="https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="68f1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下图描述了我在对象检测项目中工作的学习方案，其中，我使用了一个具有三个检测级别的小型网络来从预训练的YOLOv4中提取知识。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/6ede8f31dc397e2622dbce6c6126e8b4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*19aqzAIOgBYOqr8J7L_iwg.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">图片作者。</figcaption></figure><p id="93ba" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当将知识提炼和提示学习结合起来用于目标检测时，陈国斌显示出优异的结果。有关进一步的详细信息，请在原始论文<a class="ae lw" href="https://paperswithcode.com/paper/learning-efficient-object-detection-models" rel="noopener ugc nofollow" target="_blank">中找到与知识蒸馏</a>一起学习高效的对象检测模型。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl lp"><img src="../Images/ddcc391420773d2c579af81840e9461d.png" data-original-src="https://miro.medium.com/v2/format:webp/1*F9z6owJDBTzjfgevh2mwTg.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">图片来自纸张。[ <a class="ae lw" href="https://paperswithcode.com/paper/learning-efficient-object-detection-models" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="d100" class="lx ly iq bd lz ma mb mc md me mf mg mh kf mi kg mj ki mk kj ml kl mm km mn mo bi translated">结论</h1><p id="12a1" class="pw-post-body-paragraph ko kp iq kq b kr mp ka kt ku mq kd kw kx mr kz la lb ms ld le lf mt lh li lj ij bi translated">在这篇文章中，我简要介绍了知识提炼和提示学习。知识提取被认为是一种有效的方法，可以将复杂模型中的知识转化为更小、更精的知识。提示学习和知识提取的结合是提高神经网络性能的一个非常有效的方案。</p><p id="d3dd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">欢迎读者访问我的脸书粉丝页面，这是一个分享关于机器学习的东西的页面:<a class="ae lw" href="https://www.facebook.com/diveintomachinelearning" rel="noopener ugc nofollow" target="_blank">深入机器学习</a>。</p><p id="e27e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">感谢您抽出时间！</p></div></div>    
</body>
</html>