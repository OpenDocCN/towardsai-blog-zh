<html>
<head>
<title>PyTorch Tutorial 101</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch教程101</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pytorch-basics-be7e7ac11603?source=collection_archive---------5-----------------------#2021-07-19">https://pub.towardsai.net/pytorch-basics-be7e7ac11603?source=collection_archive---------5-----------------------#2021-07-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="98f8" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><p id="f95d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">Pytorch是一个用于张量操作的Python框架，也可以用于机器学习。其主要特点包括</p><ul class=""><li id="d25e" class="kx ky it kb b kc kd kg kh kk kz ko la ks lb kw lc ld le lf bi translated">GPU加速计算</li><li id="84fa" class="kx ky it kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">自动微分</li><li id="82b1" class="kx ky it kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">神经网络模块</li></ul><p id="13b5" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">本教程将教你操作PyTorch张量的基本原理。</p><p id="a72a" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">有关其他教程，请参见<a class="ae ll" href="http://pytorch.org/tutorials/" rel="noopener ugc nofollow" target="_blank">http://pytorch.org/tutorials/</a></p><p id="7996" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们还将使用用于科学计算的关键python模块<a class="ae ll" href="http://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"> numpy </a>。它为科学算法的分析和开发提供了数据结构和复杂的方法</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="5e53" class="lv lw it lr b gy lx ly l lz ma">import torch<br/>import numpy as np</span></pre><h2 id="fbbc" class="lv lw it bd mb mc md dn me mf mg dp mh kk mi mj mk ko ml mm mn ks mo mp mq iz bi translated">张量</h2><p id="1a12" class="pw-post-body-paragraph jz ka it kb b kc mr ke kf kg ms ki kj kk mt km kn ko mu kq kr ks mv ku kv kw im bi translated">张量是数组数据的基本对象。把它们想象成一个多维矩阵。您将使用的最常见类型是<code class="fe mw mx my lr b">IntTensor</code>和<code class="fe mw mx my lr b">FloatTensor</code>(这是默认类型)。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="8758" class="lv lw it lr b gy lx ly l lz ma"># Create uninitialized tensor of size (2,3)<br/>x = torch.FloatTensor(2,3)<br/>print(x)<br/># Initialize to zeros<br/>x.zero_()<br/>print(x)</span><span id="1849" class="lv lw it lr b gy mz ly l lz ma">tensor([[1.7561e+22, 2.1230e-10, 2.2232e-10],<br/>        [4.1585e+21, 2.6336e+20, 1.7732e+28]])<br/>tensor([[0., 0., 0.],<br/>        [0., 0., 0.]])</span><span id="32b0" class="lv lw it lr b gy mz ly l lz ma"># Create from numpy array (seed for repeatability)<br/>np.random.seed(123)<br/>np_array = np.random.random((2,3))<br/>print(torch.FloatTensor(np_array))<br/>print(torch.from_numpy(np_array))</span><span id="68dd" class="lv lw it lr b gy mz ly l lz ma">tensor([[0.6965, 0.2861, 0.2269],<br/>        [0.5513, 0.7195, 0.4231]])<br/>tensor([[0.6965, 0.2861, 0.2269],<br/>        [0.5513, 0.7195, 0.4231]], dtype=torch.float64)</span><span id="76d0" class="lv lw it lr b gy mz ly l lz ma"># Create random tensor (seed for repeatability)<br/>torch.manual_seed(123)<br/>x=torch.randn(2,3)<br/>print(x)</span><span id="ebbe" class="lv lw it lr b gy mz ly l lz ma">tensor([[-0.1115,  0.1204, -0.3696],<br/>        [-0.2404, -1.1969,  0.2093]])</span><span id="40af" class="lv lw it lr b gy mz ly l lz ma"># special tensors (see documentation)<br/>#Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.<br/>print(torch.eye(3)) <br/>#Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.<br/>print(torch.ones(2,3))<br/>#Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.<br/>print(torch.zeros(2,3))<br/>#Returns a 1-D tensor of size (end-start/step) with values in interval 0,1 with common difference of step.<br/>print(torch.arange(0,3))</span><span id="f695" class="lv lw it lr b gy mz ly l lz ma">tensor([[1., 0., 0.],<br/>        [0., 1., 0.],<br/>        [0., 0., 1.]])<br/>tensor([[1., 1., 1.],<br/>        [1., 1., 1.]])<br/>tensor([[0., 0., 0.],<br/>        [0., 0., 0.]])<br/>tensor([0, 1, 2])</span></pre><p id="c9bf" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">所有张量都有一个<code class="fe mw mx my lr b">size</code>和<code class="fe mw mx my lr b">type</code></p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="14c8" class="lv lw it lr b gy lx ly l lz ma">x=torch.FloatTensor(3,4)<br/>print(x.size())<br/>print(x.type())</span><span id="7743" class="lv lw it lr b gy mz ly l lz ma">torch.Size([3, 4])<br/>torch.FloatTensor</span></pre><p id="8d86" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">例1: </strong>我们将创建一个10x10的张量，其对角线范围从0到9，所有其他元素都是0</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="7731" class="lv lw it lr b gy lx ly l lz ma">torch.diag(torch.arange(10))</span><span id="2967" class="lv lw it lr b gy mz ly l lz ma">tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br/>        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],<br/>        [0, 0, 2, 0, 0, 0, 0, 0, 0, 0],<br/>        [0, 0, 0, 3, 0, 0, 0, 0, 0, 0],<br/>        [0, 0, 0, 0, 4, 0, 0, 0, 0, 0],<br/>        [0, 0, 0, 0, 0, 5, 0, 0, 0, 0],<br/>        [0, 0, 0, 0, 0, 0, 6, 0, 0, 0],<br/>        [0, 0, 0, 0, 0, 0, 0, 7, 0, 0],<br/>        [0, 0, 0, 0, 0, 0, 0, 0, 8, 0],<br/>        [0, 0, 0, 0, 0, 0, 0, 0, 0, 9]])</span></pre><h2 id="ee6e" class="lv lw it bd mb mc md dn me mf mg dp mh kk mi mj mk ko ml mm mn ks mo mp mq iz bi translated">随机张量</h2><p id="ce6f" class="pw-post-body-paragraph jz ka it kb b kc mr ke kf kg ms ki kj kk mt km kn ko mu kq kr ks mv ku kv kw im bi translated">我们可以从均匀分布中取样</p><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi na"><img src="../Images/653c94de91db93b0efc64fa32641e798.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*Fo9fkb4U9RqoV9Xq2Y0GYg.png"/></div></figure><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="5e40" class="lv lw it lr b gy lx ly l lz ma">nSamples = 10<br/>z = torch.rand((nSamples))<br/>print(z)</span><span id="bdb4" class="lv lw it lr b gy mz ly l lz ma">tensor([0.0756, 0.1966, 0.3164, 0.4017, 0.1186, 0.8274, 0.3821, 0.6605, 0.8536,<br/>        0.5932])</span></pre><p id="f565" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">正态分布也是如此:</p><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/53f5a2859092a8ada8a0ab1711b4f0bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*Enx5y1hWJDDkxIvCkSBphA.png"/></div></figure><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="ff76" class="lv lw it lr b gy lx ly l lz ma">z = torch.randn((nSamples))<br/>print(z)</span><span id="3d0a" class="lv lw it lr b gy mz ly l lz ma">tensor([ 0.5490,  0.3671,  0.1219,  0.6466, -1.4168,  0.8429, -0.6307,  1.2340,<br/>         0.3127,  0.6972])</span></pre><p id="59a5" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">例2: </strong>我们将创建一个10×10的随机张量，遵循平均值为3的正态分布</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="7b54" class="lv lw it lr b gy lx ly l lz ma">t = 3 + torch.randn((10,10))<br/>t</span><span id="14ee" class="lv lw it lr b gy mz ly l lz ma">tensor([[2.5155, 0.9071, 2.1801, 2.5790, 2.0380, 4.2825, 3.8768, 4.6221, 2.0113,<br/>         1.2982],<br/>        [2.2502, 1.8715, 3.4135, 3.2892, 5.2473, 2.1964, 2.0867, 2.5796, 4.3111,<br/>         2.7801],<br/>        [3.2190, 3.2045, 3.5146, 3.9938, 3.8218, 3.1512, 3.1036, 0.8004, 2.9115,<br/>         2.4388],<br/>        [3.6716, 3.6933, 1.7215, 1.8560, 3.2436, 2.9433, 3.3784, 4.6863, 3.2553,<br/>         2.4504],<br/>        [2.0917, 3.3507, 4.5434, 3.1406, 4.0617, 2.0071, 1.3975, 1.9236, 3.4046,<br/>         1.3539],<br/>        [4.0720, 4.5026, 2.1810, 3.2686, 0.7850, 1.6807, 2.7099, 4.2767, 2.0052,<br/>         4.2176],<br/>        [2.7718, 4.3382, 4.9929, 4.3708, 1.9393, 0.6756, 4.2311, 1.9027, 2.0331,<br/>         3.6712],<br/>        [2.0595, 2.5319, 3.6455, 2.1043, 4.1124, 2.5832, 1.2894, 2.6710, 4.3966,<br/>         2.0051],<br/>        [1.7904, 2.4440, 0.2798, 3.5421, 3.6557, 1.5944, 1.7257, 3.4513, 2.7720,<br/>         3.9224],<br/>        [3.2056, 2.5030, 4.2782, 5.5501, 2.6982, 2.3297, 2.3829, 2.1666, 3.4839,<br/>         2.8651]])</span></pre><h2 id="fa6a" class="lv lw it bd mb mc md dn me mf mg dp mh kk mi mj mk ko ml mm mn ks mo mp mq iz bi translated">重塑张量</h2><p id="b957" class="pw-post-body-paragraph jz ka it kb b kc mr ke kf kg ms ki kj kk mt km kn ko mu kq kr ks mv ku kv kw im bi translated">我们可以用<code class="fe mw mx my lr b">reshape</code>方法重塑张量:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="ee5b" class="lv lw it lr b gy lx ly l lz ma">z = torch.randn((12))<br/>print(z)</span><span id="5297" class="lv lw it lr b gy mz ly l lz ma">tensor([-1.7799,  0.6474,  0.5460,  0.8050, -1.3467, -0.6418, -0.1514, -2.4449,<br/>        -0.0952,  0.5930, -1.6707,  0.1656])</span></pre><p id="1902" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">如果我们想把z塑造成一个(3，4)张量，我们可以这样做:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="607c" class="lv lw it lr b gy lx ly l lz ma">y = z.reshape((3,4))<br/>print(y)</span><span id="a78d" class="lv lw it lr b gy mz ly l lz ma">tensor([[-1.7799,  0.6474,  0.5460,  0.8050],<br/>        [-1.3467, -0.6418, -0.1514, -2.4449],<br/>        [-0.0952,  0.5930, -1.6707,  0.1656]])</span></pre><p id="e448" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们还可以将-1指定为应该自动填充的维度:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="d093" class="lv lw it lr b gy lx ly l lz ma">y = z.reshape((2,2,-1))<br/>print(y)<br/>print(y.shape)</span><span id="8b28" class="lv lw it lr b gy mz ly l lz ma">tensor([[[-1.7799,  0.6474,  0.5460],<br/>         [ 0.8050, -1.3467, -0.6418]],<br/><br/>        [[-0.1514, -2.4449, -0.0952],<br/>         [ 0.5930, -1.6707,  0.1656]]])<br/>torch.Size([2, 2, 3])</span></pre><p id="b251" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><code class="fe mw mx my lr b">reshape</code>可以用指定的形状创建新的数据结构。然而，如果我们不想创建一个新的数据结构，而只是以不同的形式呈现数据(保持相同的底层数据表示)，我们可以使用<code class="fe mw mx my lr b">view</code>:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="0c7d" class="lv lw it lr b gy lx ly l lz ma">y = z.view((4,3))<br/>print(y)</span><span id="e642" class="lv lw it lr b gy mz ly l lz ma">tensor([[-1.7799,  0.6474,  0.5460],<br/>        [ 0.8050, -1.3467, -0.6418],<br/>        [-0.1514, -2.4449, -0.0952],<br/>        [ 0.5930, -1.6707,  0.1656]])</span></pre><p id="f140" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">示例3: </strong>生成50个训练样本的随机数据集(使用随机分布)，每个样本具有3个特征(大小为50×3)。然后使用<code class="fe mw mx my lr b">view</code>方法将数据集分成10批，每批5个训练样本。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="a6a1" class="lv lw it lr b gy lx ly l lz ma">dataset = torch.rand((50,3))<br/>batched = dataset.view((10,5,3))<br/>batched</span><span id="818a" class="lv lw it lr b gy mz ly l lz ma">tensor([[[0.8118, 0.0585, 0.1142],<br/>         [0.3338, 0.2122, 0.7579],<br/>         [0.8533, 0.0149, 0.0757],<br/>         [0.0131, 0.6886, 0.9024],<br/>         [0.1123, 0.2685, 0.6591]],<br/><br/>        [[0.1735, 0.9247, 0.6166],<br/>         [0.3608, 0.5325, 0.6559],<br/>         [0.3232, 0.1126, 0.5034],<br/>         [0.5091, 0.5101, 0.4270],<br/>         [0.8210, 0.3605, 0.4516]],<br/><br/>        [[0.7056, 0.1853, 0.6339],<br/>         [0.3894, 0.7398, 0.2288],<br/>         [0.5185, 0.5489, 0.0977],<br/>         [0.1364, 0.6918, 0.3545],<br/>         [0.7969, 0.0061, 0.2528]],<br/><br/>        [[0.0882, 0.6997, 0.4855],<br/>         [0.4067, 0.4168, 0.1092],<br/>         [0.6418, 0.5125, 0.1549],<br/>         [0.6881, 0.4900, 0.0164],<br/>         [0.7690, 0.7674, 0.4058]],<br/><br/>        [[0.1548, 0.5201, 0.8773],<br/>         [0.9577, 0.1226, 0.2742],<br/>         [0.8893, 0.7444, 0.8095],<br/>         [0.2511, 0.9308, 0.0890],<br/>         [0.4759, 0.5104, 0.5840]],<br/><br/>        [[0.1227, 0.9587, 0.9914],<br/>         [0.1547, 0.5185, 0.2337],<br/>         [0.9794, 0.7788, 0.7945],<br/>         [0.6613, 0.4502, 0.7815],<br/>         [0.5085, 0.3176, 0.7582]],<br/><br/>        [[0.6569, 0.3704, 0.3630],<br/>         [0.0578, 0.3629, 0.2974],<br/>         [0.2275, 0.0484, 0.8916],<br/>         [0.0532, 0.9964, 0.2377],<br/>         [0.4616, 0.9079, 0.6650]],<br/><br/>        [[0.3573, 0.0975, 0.2956],<br/>         [0.9027, 0.3112, 0.9167],<br/>         [0.4139, 0.4362, 0.6996],<br/>         [0.4265, 0.4958, 0.8463],<br/>         [0.6671, 0.4801, 0.6904]],<br/><br/>        [[0.9355, 0.6260, 0.3534],<br/>         [0.6638, 0.4563, 0.1091],<br/>         [0.3069, 0.7274, 0.5164],<br/>         [0.6845, 0.2073, 0.9727],<br/>         [0.2913, 0.6066, 0.2557]],<br/><br/>        [[0.2588, 0.7239, 0.3604],<br/>         [0.1829, 0.2956, 0.8646],<br/>         [0.8010, 0.8044, 0.0733],<br/>         [0.7355, 0.6248, 0.1638],<br/>         [0.5158, 0.6000, 0.2299]]])</span></pre><h2 id="afdb" class="lv lw it bd mb mc md dn me mf mg dp mh kk mi mj mk ko ml mm mn ks mo mp mq iz bi translated">CPU和GPU</h2><p id="2e76" class="pw-post-body-paragraph jz ka it kb b kc mr ke kf kg ms ki kj kk mt km kn ko mu kq kr ks mv ku kv kw im bi translated">张量可以在CPU和GPU之间复制。计算中涉及的所有内容都在同一台设备上，这一点很重要。</p><p id="3e15" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">这在本教程中不适用，因为笔记本电脑没有CUDA兼容的GPU。</p><p id="6279" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">PyTorch使得使用GPU设备非常简单。你可以在这里了解更多。</p><h2 id="bf74" class="lv lw it bd mb mc md dn me mf mg dp mh kk mi mj mk ko ml mm mn ks mo mp mq iz bi translated">数学、线性代数和索引</h2><p id="3639" class="pw-post-body-paragraph jz ka it kb b kc mr ke kf kg ms ki kj kk mt km kn ko mu kq kr ks mv ku kv kw im bi translated">Pytorch数学和线性代数类似于NumPy。运算符被覆盖，因此您可以使用标准的数学运算符(<code class="fe mw mx my lr b">+</code>、<code class="fe mw mx my lr b">-</code>等)。)并期望一个张量作为结果。参见<a class="ae ll" href="https://pytorch.org/docs/stable/torch.htm" rel="noopener ugc nofollow" target="_blank"> PyTorch文档</a>获取可用功能的完整列表。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="a0f6" class="lv lw it lr b gy lx ly l lz ma">x = torch.arange(0,5, dtype=torch.float)<br/>print(torch.sum(x))<br/>print(torch.sum(torch.exp(x)))<br/>print(torch.mean(x))</span><span id="26c0" class="lv lw it lr b gy mz ly l lz ma">tensor(10.)<br/>tensor(85.7910)<br/>tensor(2.)</span></pre><p id="8e84" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">Pytorch索引类似于NumPy索引。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="6020" class="lv lw it lr b gy lx ly l lz ma">x = torch.rand(3,2)<br/>print(x)<br/>print(x[1,:])</span><span id="88f4" class="lv lw it lr b gy mz ly l lz ma">tensor([[0.2890, 0.9078],<br/>        [0.4596, 0.4947],<br/>        [0.1836, 0.2010]])<br/>tensor([0.4596, 0.4947])</span></pre><p id="5267" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">例4: </strong>我们将使用例2中创建的随机张量来获取每一列的平均值。结果应该是大小为(1，10)的张量，每个列值对应于原始张量中相应列的平均值。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="d65e" class="lv lw it lr b gy lx ly l lz ma">torch.mean(t,dim=0)</span><span id="0f9a" class="lv lw it lr b gy mz ly l lz ma">tensor([2.7647, 2.9347, 3.0751, 3.3695, 3.1603, 2.3444, 2.6182, 2.9080, 3.0585,<br/>        2.7003])</span></pre><h2 id="b513" class="lv lw it bd mb mc md dn me mf mg dp mh kk mi mj mk ko ml mm mn ks mo mp mq iz bi translated">自动微分</h2><p id="1795" class="pw-post-body-paragraph jz ka it kb b kc mr ke kf kg ms ki kj kk mt km kn ko mu kq kr ks mv ku kv kw im bi translated">象征性地计算导数可能非常耗时，在某些情况下还很棘手。</p><p id="2c07" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">幸运的是，PyTorch提供了一个简单的解决方案，叫做autogradated(自动渐变)。</p><ul class=""><li id="5638" class="kx ky it kb b kc kd kg kh kk kz ko la ks lb kw lc ld le lf bi translated">你正在区分的张量是必备的<code class="fe mw mx my lr b">requires_grad=True</code></li><li id="1055" class="kx ky it kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">在变量上调用<code class="fe mw mx my lr b">.backward()</code>，你在求微分</li></ul><p id="177c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">对于PyTorch的早期版本，我们需要使用<code class="fe mw mx my lr b">torch.autograd.Variable</code>类来区分张量。在PyTorch 1.0之后，我们可以在创建需要微分的张量时简单地使用<code class="fe mw mx my lr b">requires_grad=True</code>参数。或者，我们可以使用就地操作<code class="fe mw mx my lr b">requires_grad_()</code></p><p id="ba77" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">例如，让我们计算一个简单多项式的导数:</p><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/07e2f46025deb458e8cb14b56fdcc613.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*1ZKOmcXYlNkbCCCbO2MHfA.png"/></div></figure><p id="7644" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">从微积分中，我们知道</p><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/1ff9e04eaa632296a2cc3131d549937b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HQoZR94Hf4OZpa9F_gOKtA.png"/></div></div></figure><p id="eace" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">使用PyTorch可以获得相同的结果:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="4b42" class="lv lw it lr b gy lx ly l lz ma">#Creates tensor explicitely saying that gradients are required<br/>x = torch.tensor([2.], requires_grad=True)<br/><br/>#Calculate a scalar function of the variable<br/>p = x**3<br/><br/>#Backward pass on p to calculate gradient w.r.t. all variables<br/>p.backward()<br/><br/>#Acessing the derivative on x variable<br/>x.grad</span><span id="d5ff" class="lv lw it lr b gy mz ly l lz ma">tensor([12.])</span></pre><p id="783e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">对于更复杂的函数，我们也可以这样做:</p><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/dd5197c2a8e20ad100a0b008db49bf3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*9JoElPYfsTeGSqI-qi7ePw.png"/></div></figure><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="3e70" class="lv lw it lr b gy lx ly l lz ma">x = torch.tensor([3.], requires_grad=True)<br/>p = torch.exp(torch.cos(x))<br/>p.backward()<br/>x.grad</span><span id="d770" class="lv lw it lr b gy mz ly l lz ma">tensor([-0.0524])</span></pre><p id="f302" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们得到了</p><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/3c023fa2e70c9dca073fabcaa5fc59af.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*SqwbkYjaTeKHzRhmOU8CVQ.png"/></div></figure><p id="c268" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">而不必显式地计算导数。</p><p id="5520" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">例5: </strong>我们将计算的导数</p><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/cf55ab9db80c9e766d519444d31180f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*nqJSsTOTmjWG_12wacDIcg.png"/></div></figure><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi no"><img src="../Images/16b1ddab4cd040dc1ffc01f6f093e620.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*DIG0kQgCznBkoxe7qavMSQ.png"/></div></figure><p id="b0cf" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">使用PyTorch自动签名功能</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="a44d" class="lv lw it lr b gy lx ly l lz ma">x = torch.tensor([0.8], requires_grad=True)<br/>p = torch.sin(torch.log(x))<br/>p.backward()<br/>x.grad</span><span id="ffbf" class="lv lw it lr b gy mz ly l lz ma">tensor([1.2190])</span></pre><p id="24ba" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">前面的例子使用了单个变量的函数，但是我们可以很容易地处理多个变量的函数:</p><p id="a053" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">例如，函数</p><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/743e3e5b4ca9377d396d84b450c5757a.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*0wNXdfgy3Y5-Wjwh6AWrkg.png"/></div></figure><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5d794ecde6059564deb63ca5fba617aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*2XqAj8UAq9q__A7MF0FePA.png"/></div></figure><p id="e07f" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">注意，x被初始化为向量</p><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5d794ecde6059564deb63ca5fba617aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*2XqAj8UAq9q__A7MF0FePA.png"/></div></figure><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="e74c" class="lv lw it lr b gy lx ly l lz ma"># Create variable<br/>x = torch.arange(10,14, dtype=torch.float)<br/><br/># Set requires_grad to True<br/>x.requires_grad_()<br/><br/># Calculate y=sum(x**2)<br/>y = torch.sum(x**2)<br/># Calculate gradient (dy/dx=2x)<br/>y.backward()<br/># Print values<br/>print(x,y,x.grad)<br/>print(y)<br/>print(x.grad)</span><span id="3206" class="lv lw it lr b gy mz ly l lz ma">tensor([10., 11., 12., 13.], requires_grad=True) tensor(534., grad_fn=&lt;SumBackward0&gt;) tensor([20., 22., 24., 26.])<br/>tensor(534., grad_fn=&lt;SumBackward0&gt;)<br/>tensor([20., 22., 24., 26.])</span></pre><p id="1ecc" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">微分累积梯度。这有时是你想要的，有时不是。<strong class="kb jd">如果执行SGD，请确保批次之间的梯度为零，否则会得到奇怪的结果！</strong></p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="262f" class="lv lw it lr b gy lx ly l lz ma"># Create a variable<br/>x=torch.arange(10,14, dtype=torch.float, requires_grad=True)<br/># Differentiate<br/>torch.sum(x**2).backward()<br/>print('First gradient')<br/>print(x.grad)<br/><br/># Differentiate again (accumulates gradient)<br/>torch.sum(x**2).backward()<br/>print('Second gradient without zeroing gradients')<br/>print(x.grad)<br/><br/># Zero gradient before differentiating<br/>x.grad.data.zero_()<br/>torch.sum(x**2).backward()<br/>print('Gradient after zeroing')<br/>print(x.grad)</span><span id="9f86" class="lv lw it lr b gy mz ly l lz ma">First gradient<br/>tensor([20., 22., 24., 26.])<br/>Second gradient without zeroing gradients<br/>tensor([40., 44., 48., 52.])<br/>Gradient after zeroing<br/>tensor([20., 22., 24., 26.])</span></pre><figure class="lm ln lo lp gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nq"><img src="../Images/f19e9ffb4637b0efaf0a1d7150d9ac7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IfNSvBf4ZEZbRCQK8Sjj_g.jpeg"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk translated">来源:<a class="ae ll" href="https://images.app.goo.gl/nR2ev3H7vZCJ9d729" rel="noopener ugc nofollow" target="_blank">谷歌图片</a>我的清晨散步；)</figcaption></figure></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="2532" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">最初，我是一名机械工程师，但一直对数学和人工智能充满热情，后来我在这两个领域都找到了理想的职业，成为一名数据科学家。我热衷于将这些学科的严谨性应用于复杂的分析问题。我热爱教学，我兼职辅导A-Level数学、物理和化学。我也是一个受自然启发的优化爱好者(查看我的 <a class="ae ll" href="https://www.researchgate.net/publication/317803017_An_Experimental_Study_on_Competitive_Coevolution_of_MLP_Classifiers" rel="noopener ugc nofollow" target="_blank"> <em class="oc">发表的pape </em> </a> <em class="oc"> r)和一个板球和网球的超级粉丝。在</em><a class="ae ll" href="https://www.linkedin.com/in/rahullalchandani-" rel="noopener ugc nofollow" target="_blank"><em class="oc">LinkedIn</em></a><em class="oc">上找我！</em></p></div></div>    
</body>
</html>