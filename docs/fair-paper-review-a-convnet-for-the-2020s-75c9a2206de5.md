# 公平论文评论:2020 年的会议

> 原文：<https://pub.towardsai.net/fair-paper-review-a-convnet-for-the-2020s-75c9a2206de5?source=collection_archive---------1----------------------->

## [深度学习](https://towardsai.net/p/category/machine-learning/deep-learning)

## 尝试“现代化”ConvNets，以匹配最先进的视觉变压器的性能

![](img/d5b35cbdf242b189546403efc0d04f8d.png)

由 [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

直到 2020 年，卷积神经网络一直是计算机视觉任务的标准。在过去的两年里，变形金刚已经入侵了计算机视觉和自然语言处理领域。视觉变形金刚的表现在 2021 年取代了最先进的 CNN 的表现。从那以后，CNN 和《视觉变形金刚》之间有过无数次的比较。

这篇论文是关于在不使用任何变形金刚组件(如全球注意力)的情况下，使经典 CNN 现代化，以匹配视觉变形金刚的性能。有趣的是，它揭示了 CNN 和视觉变形金刚之间的战争历史，并且它分解了用于改善 CNN 性能的零碎内容，我个人认为这些内容非常有见地和有趣。

## 介绍

这一切都是从 vanilla Vision Transformer (ViT)开始的，尽管它在图像上表现得相当好，但在对象检测和语义分割上表现不太好。之后， [Swin 变压器](https://arxiv.org/abs/2103.14030)(也称为分级变压器)作为经典 vit 的升级版推出。现在，变形金刚的问题是，整体注意力相对于输入大小具有二次复杂度[1],这意味着它们对较小的图像工作得很好，但对较高分辨率的图像来说就不太好了。

该论文的作者还认为，经典的 CNN 具有归纳偏差，这有助于它们在计算机视觉任务中表现良好，如“[滑动窗口](https://datahacker.rs/convolutional-operation-of-sliding-windows/)”策略、翻译等值。这并不意味着 CNN 只对计算机视觉有好处，因为 1D 卷积也可以用于 NLP，然而，它们在计算机视觉中占优势。

> 平移等方差或仅等方差是卷积神经网络的一个非常重要的属性，在卷积神经网络中，图像中对象的位置不应该是固定的，以便它被 CNN 检测到。这仅仅意味着如果输入改变，输出也会改变。

来源: [TDS](https://towardsdatascience.com/translational-invariance-vs-translational-equivariance-f9fbc8fca63a#:~:text=Translational%20Equivariance%20or%20just%20equivariance,changes%2C%20the%20output%20also%20changes.)

同样值得注意的是，计算机视觉不仅仅是图像分类，我认为大多数人(包括我自己)在谈到计算机视觉时主要考虑图像分类。计算机视觉包括图像分类、对象检测、语义分割和许多其他内容。虽然纯视觉变形器在图像分类方面表现出色，但它们在其他主要任务方面并不出色。

Swin transformers 试图升级普通变形金刚，以更好地执行各种各样的计算机视觉任务，它采用了视觉变形金刚和 CNN 的混合。例如，他们在局部窗口[1]中使用注意力，这基本上是一种滑动窗口策略。

## ConvNeXts 的路线图

本文主要是对 CNN 和 vit 的检查，试图将 CNN 带回到计算机视觉中。为此，他们从标准 ResNet 开始，逐步[1]提高其性能，以达到 ViT，而最终不会成为 ViT！

这里正在探索的主要设计决策有[1]:

1.  宏观设计
2.  ResNeXt
3.  倒置瓶颈
4.  大颗粒尺寸
5.  各种分层微设计

他们还关注训练过程，因为不仅结构影响神经网络的最终性能。测试了各种各样有趣的优化，如[1] [AdamW 优化器](https://www.fast.ai/2018/07/02/adam-weight-decay/)、Mixup & Cutmix 增强等等。

至于宏观设计研究，他们首先查看阶段计算比率，该比率决定了神经网络中块的数量的比率和分布。典型的 ResNets 有一个计算 1:3 比率的级，而 Swin 变压器的比率为 1:3。他们发现，将 ResNets 中的这个比率与 Swin-T 的浮点运算对齐，可以将模型精度从 78.8%提高到 79.4%。

下一步是研究一个经典的 ResNeXt 网络。与普通 resnet 相比，ResNeXt 网络具有更好的浮点运算权衡。他们还[1]使用分组卷积，其中卷积滤波器被分成不同的组，还使用深度卷积，这类似于自关注中的加权和操作。它们本质上是通过使用深度方向的卷积来重定向网络的 FLOPs，从而将网络的性能提高到 80.5%。

他们关注的下一个有趣的想法是“反向瓶颈”[1]。这意味着网络从一开始就极大地扩展了图像。经典的 MLP 块的隐藏维度比输入维度宽 4 倍。添加这种反向瓶颈设计减少了整个网络的 FLOPs，并略微提高了性能。

最后，他们开始考虑增加内核大小，试图模仿《变形金刚》中非局部自我关注的全局感受域[1]。他们将内核大小从 3 逐渐增加到 11，并观察到深度方向卷积的网络性能在 7 x7 时最佳(80.6%)。同样值得注意的是，随着这一增长，网络的失败次数几乎保持不变。

## 其他微设计技巧

神经网络设计一直是一种“艺术”，而不是如何设计它们的一步一步。当然，关于如何通过改变神经网络的设计来提高其性能，有一些标准的提示和技巧，但是我认为到目前为止，还没有什么具体的东西可以解释为什么结构的各种变化会导致性能的变化。这就是为什么许多机器学习论文是一系列从各个方面改变架构并检查性能和拟合变化的反复试验。

这篇文章最棒的一点是它的结构，它们布局了正在改变的架构的不同区域，故意详细解释了它们正在改变的内容，并给出了每项改变的结果，而不是给出所有改变的最终结果。ML 中的一些论文只是布置了一系列巨大的变化，这些变化会突然导致性能的巨大变化，但你并不真正理解每个变化的意义，但这里的情况并非如此。

为了简洁起见，我们在这里要提到的最后一个变化是减少激活函数的数量。虽然向每个卷积层添加激活函数是常见的，但他们发现，消除其中的一部分(保留 1xt 层之间的一个并消除其他层)会导致另一个性能提高到 81.3%，现在**与 Swin 转换器的性能相当。**

## 结论

请注意，并非所有的更改都是成功的，有些更改会导致性能下降。然而，为了简洁起见，我不可能包括所有的变化，所以如果你感兴趣，我强烈推荐你看看这篇文章。

如果你想定期收到关于人工智能和机器学习的最新论文的评论，请在这里添加你的电子邮件并订阅！

https://artisanal-motivator-8249.ck.page/5524b8f934

## 参考

[1][2020 年代的一篇论文](https://arxiv.org/pdf/2201.03545.pdf)