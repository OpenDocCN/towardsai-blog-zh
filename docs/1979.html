<html>
<head>
<title>Google Fights NLP Inequality with Massively Scalable, Multilingual Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌用大规模可扩展的多语言模型对抗NLP不平等</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/google-fights-language-intelligence-inequality-with-massively-scalable-multilingual-models-41d8c3fb0852?source=collection_archive---------1-----------------------#2021-07-12">https://pub.towardsai.net/google-fights-language-intelligence-inequality-with-massively-scalable-multilingual-models-41d8c3fb0852?source=collection_archive---------1-----------------------#2021-07-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="031c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="85b8" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">谷歌研究人员设计了一个通用的机器翻译系统，可以跨越100种语言。</h2></div><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="ab gu cl kw"><img src="../Images/7b148b0e86e455c9cc55c74a8d2f4517.png" data-original-src="https://miro.medium.com/v2/format:webp/1*eKmT9U-IXvVWE6RV3bZ-IQ.jpeg"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">图片来源:谷歌研究</figcaption></figure><blockquote class="ld le lf"><p id="0ac6" class="lg lh li lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过90，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="md me gp gr mf mg"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd jd gy z fp ml fr fs mm fu fw jc bi translated">序列</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到85，000多人的信任…</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">thesequence.substack.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu kx mg"/></div></div></a></div><p id="26e2" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">自然语言系统一直是过去几年人工智能(AI)复兴的中心。然而，语言智能程序的好处仅限于世界上最流行的口语。让Alexa理解几乎所有的英语是微不足道的，但尝试西非的一种<a class="ae my" href="https://en.wikipedia.org/wiki/Kru_languages" rel="noopener ugc nofollow" target="_blank">克鲁语</a>，故事就完全不同了。虽然世界上有7000多种语言，但其中只有20种占世界人口的一半以上。在人工智能的背景下，设计可以与数据稀缺的语言无缝合作的语言智能模型是维护空间平等的优先事项。最近，Google Research发布了几份白皮书，详细介绍了一些设计多语言系统的新努力，这些系统可以扩展到数百种语言。</p><p id="47b2" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">设计能有效处理数据受限语言的语言智能系统绝非易事。众所周知，语音识别或机器翻译等领域需要大量的标记数据。我们如何在不牺牲质量的情况下使这些模型适应数据稀缺的语言？我们能把数据丰富的语言中的知识推广到其他语言中吗？对于数据稀缺的语言，我们能在语言智能系统中达到像样的质量水平吗？谷歌一直在通过建立一种更有效的机器移植方法来慢慢应对这些挑战。</p><h1 id="680f" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">构建大规模多语言系统</h1><p id="2f12" class="pw-post-body-paragraph lg lh it lj b lk nr kd lm ln ns kg lp mv nt ls lt mw nu lw lx mx nv ma mb mc im bi translated">在“<a class="ae my" href="https://arxiv.org/pdf/1907.05019.pdf" rel="noopener ugc nofollow" target="_blank">野外大规模多语言神经机器翻译:发现和挑战</a>”中，谷歌研究人员提出了建立半通用神经机器翻译(NMT)模型的想法。具体来说，他们设计了一个单一的NMT模型，可以对250多亿个句子对进行训练，从100多种语言到英语，有500多亿个参数。</p><p id="fb71" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">谷歌论文的目标与其说是为NMT系统提出一个新颖的架构，不如说是理解它们的行为。尽管多语言NMT模型很有吸引力，但大多数方法都是在受限的环境下开发的；它们的功效还有待于在现实世界中证明。</p><p id="2193" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">设计通用NMT模型是一个归纳偏差问题。更具体地说，在多语言NMT的背景下，潜在的归纳偏见是，一种语言的学习信号应该有利于其他语言的质量。从这个角度来看，我们的期望是，随着语言数量的增加，学习者会因为每种语言增加的信息量而更好地概括。谷歌的研究论文总结了构建通用NMT的挑战:<em class="li">跨大量领域/任务的多领域和多任务学习，具有广泛的数据不平衡，以及由数据集噪声和主题/风格差异、不同程度的语言相似性等引起的非常异构的任务间关系。</em></p><p id="40bb" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">尽管这种描述势不可挡，但最重要的是要认识到这些挑战通常是由单个模型解决的，我们不知道是否有可能在一个通用的NMT中构建它们。一般而言，通用NMT方法应具有以下特征:</p><p id="8be9" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated"><em class="li">单个模型中考虑的语言数量的最大吞吐量。</em></p><p id="a72d" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated"><em class="li">向低资源语言的最大诱导(正)迁移。</em></p><p id="60d8" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated"><em class="li">对高资源语言的最小干扰(负迁移)。</em></p><p id="e664" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated"><em class="li">·强大的多语言NMT模型，在现实的开放领域环境中表现出色。</em></p><p id="0dbb" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">在实验中，谷歌使用了来自103种语言的250多亿个例子的数据集。谷歌实验的关键发现之一是，NMT系统可以学习跨类似语言的共享表示。这可以用来将知识从一个语言学习者传递给另一个语言学习者。下图显示了实验中涉及的不同语言的聚类分析。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/bb9dcf62057cce99ab7c8196a2a26fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ERdtJS1cvOgXsc-tpJoT7A.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">图片来源:谷歌研究</figcaption></figure><p id="176f" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">为了实现多语言效率，Google利用了传统transformer架构的适应层，该适应层允许不同任务的专门化。谷歌利用GPipe训练128层的变形金刚，参数超过60亿。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/c975f951b2eae2dcd89723dce37881cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o9c90ng8QmPICjhrtrAqAA.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">图片来源:谷歌研究</figcaption></figure><p id="05b9" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">该模型经过训练后，显示出从高资源向低资源语言的强烈正迁移，显著提高了分布尾部30+种语言的翻译质量，平均提高了5个<a class="ae my" href="https://en.wikipedia.org/wiki/BLEU" rel="noopener ugc nofollow" target="_blank"> BLEU </a>点。这一发现表明，大规模多语言模型在泛化方面是有效的，并且能够捕捉跨大量语言的代表性相似性。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/bf1323079beae4431947b88b398170b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*KofURNVkSSTklKkCpbxZnw.gif"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">图片来源:谷歌研究</figcaption></figure><p id="0125" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">最初的泛化对于少量的低资源语言来说工作得很好，但是随着这个数字开始增加，它有一些意想不到的副作用。具体来说，观察到高资源语言翻译的质量开始下降。Google使用前面提到的GPipe架构来表示我们的神经网络的能力，通过增加模型参数的数量来提高高资源语言的翻译质量，从而使它们变得更大。另一个有趣的创新是用<a class="ae my" href="https://arxiv.org/abs/1701.06538" rel="noopener ugc nofollow" target="_blank">稀疏选通的专家混合</a>替换变压器架构的基本前馈层，我们大幅提升了模型容量，允许用超过500亿个参数训练模型，这进一步全面提高了翻译质量。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/cfd9b4e32375982fb27b7a3d0c7036fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*WXa_xozvPwpyxQGh4XXXdw.gif"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">图片来源:谷歌研究</figcaption></figure><p id="ebd3" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp mv lr ls lt mw lv lw lx mx lz ma mb mc im bi translated">谷歌实现通用NMT的方法是将机器翻译能力扩展到数千种语言的第一步。世界上有超过7000种口语，在这些语言的每个领域或任务上训练单独的神经翻译模型在计算上是不现实的。从这个角度来看，通用NMT是避免机器翻译不平等的唯一途径。如果我们还考虑到，到本世纪末，7000种语言中有一半以上将不复存在，那么一个通用的NMT不仅是通向更高智能的一条途径，也是保存不同文化历史的一条途径。</p></div></div>    
</body>
</html>