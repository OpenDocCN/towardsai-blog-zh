<html>
<head>
<title>How Linear Regression Actually work (Maths In-depth Intuition)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归实际工作原理(数学深度直觉)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-linear-regression-actually-work-maths-in-depth-intuition-93530b1ad071?source=collection_archive---------1-----------------------#2021-11-20">https://pub.towardsai.net/how-linear-regression-actually-work-maths-in-depth-intuition-93530b1ad071?source=collection_archive---------1-----------------------#2021-11-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a895" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/mathematics" rel="noopener ugc nofollow" target="_blank">数学</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/d1212aa1365670b6be079484b920e970.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XvX5H2x_urnYKNS6_PCUnA.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@isaacmsmith?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">艾萨克·史密斯</a>在<a class="ae ko" href="https://unsplash.com/s/photos/graph?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="6f38" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们从我在Quora 上发现的一些东西开始吧</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="7008" class="lw lx it ls b gy ly lz l ma mb"><strong class="ls jd">Manager</strong>: Okay, we need a system for detecting pedestrians in real time on the road!</span><span id="25ea" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: I’m sure there’s an R package that does it! Alternatively, we can use Scikit-SelfDrivingCar or something. I bet we can get it done the next week!</span><span id="1c34" class="lw lx it ls b gy mc lz l ma mb">A few hours of googling later…</span><span id="c051" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Apparently, there’s nothing like that. Let’s do some heavy lifting — we will use Scikit-VisionMagic, download a dataset for it in .csv format from Kaggle, call a bunch of standard methods from tutorial and ship it to production!</span><span id="0570" class="lw lx it ls b gy mc lz l ma mb">Next day:</span><span id="4bcb" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Uuuh, there’s no readily available datasets, no out-of-the-box libraries…Sigh. Alright, let’s use Keras and someone’s project from Github!</span><span id="8a1d" class="lw lx it ls b gy mc lz l ma mb">A few denigrating comments from StackOverflow later…</span><span id="47a3" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Okay, I’ve managed to run it on our data, but this model gives us some weird artifacts that were not reported in the instructions…Guess, I’ll have to learn Keras deeper and try to fix that.</span><span id="06a8" class="lw lx it ls b gy mc lz l ma mb">A few questions on Quora later:</span><span id="afbc" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Manager</strong>: Hey, how’s that pedestrian thing going?</span><span id="8f76" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Pretty good, just a few more minor fixes!</span><span id="a682" class="lw lx it ls b gy mc lz l ma mb">*muffed sound of crash in the background*</span><span id="2464" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Turns out this model was good only for demonstration! I’ll have to write my own in TensorFlow…</span><span id="9c8f" class="lw lx it ls b gy mc lz l ma mb">A few days of copy-pasting tutorial code later:</span><span id="aaf5" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Manager</strong>: Dude, we are in no rush, but you’ve told us that you’d be done by now.</span><span id="1e64" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Yeah, there were some complications, need to make sure everything goes smoothly after the release.</span><span id="5d93" class="lw lx it ls b gy mc lz l ma mb">Do you hear that sound? That’s the sound a gradient makes when it explodes.</span><span id="15f7" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: This pre-implemented loss function is a mess! And I will have to write a couple of my custom layers, and a loss function, and the regularization that’s not shipped in the available library…so many things to study.</span><span id="f9b8" class="lw lx it ls b gy mc lz l ma mb">One ML course later.</span><span id="5bbb" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Turns out the receptive field was too narrow…if only I took a class on deep learning…</span><span id="39ca" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Oh, that batch normalization thing really rocks! Why don’t they write it in scikit-learn tutorial?</span><span id="bc34" class="lw lx it ls b gy mc lz l ma mb">One Goodfellow’s book later.</span><span id="705c" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Finally! I’ve broken a ton of things on my own and made a ton of mistakes that could be avoided, but hey, that’s the fun of learning!</span><span id="2425" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Alright, it works, the simulations look pretty good!</span><span id="b73f" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Deployment engineer</strong>: Great! But your system requires 24Gb of memory and can’t process more than 2 frames per second on a high-end Titan card. We can’t use it in a car.</span><span id="a242" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Engineer</strong>: Damn it! It looks like I’ll need a fast inference method and reduced memory consumption…if only I knew the computational complexity of every part of my network and the amount of parameters like they teach in the algorithms course…And what’s about that FP16 thing anyway?..</span><span id="40b9" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Manager</strong>: We have run out of funding and everyone is fired.</span><span id="ac81" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">Fin</strong>.</span><span id="ffab" class="lw lx it ls b gy mc lz l ma mb">The moral of the story is that you can’t outgoogle good fundamental education and relevant technical knowledge.</span><span id="189f" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">credit</strong> :- https://www.quora.com/Why-should-one-learn-machine-learning-from-scratch-rather-than-just-learning-to-use-the-available-libraries</span></pre><p id="f303" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi md translated"><span class="l me mf mg bm mh mi mj mk ml di">所以</span>简单来说，机器学习不止于”。适合“和”。预测”的方法。我们可以简单地使用”。适合“和”。预测”方法，但是如果我们不知道这些算法实际上是如何工作的，我们就不能对这些预先构建的方法进行调整...</p><p id="cdd1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">我们将在本文中讨论的内容</strong></p><ul class=""><li id="68c3" class="mm mn it kr b ks kt kw kx la mo le mp li mq lm mr ms mt mu bi translated">什么是机器学习？</li><li id="2ca1" class="mm mn it kr b ks mv kw mw la mx le my li mz lm mr ms mt mu bi translated">什么是线性回归？</li><li id="3630" class="mm mn it kr b ks mv kw mw la mx le my li mz lm mr ms mt mu bi translated">线性回归背后的数学。</li></ul><blockquote class="na nb nc"><p id="fd29" class="kp kq nd kr b ks kt ku kv kw kx ky kz ne lb lc ld nf lf lg lh ng lj lk ll lm im bi translated"><strong class="kr jd">什么是机器学习？</strong></p></blockquote><p id="db30" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi md translated"><span class="l me mf mg bm mh mi mj mk ml di">M</span><strong class="kr jd">achine learning</strong>(<strong class="kr jd">ML</strong>)是对能够通过经验和使用数据自动改进的计算机算法的研究。它被视为人工智能的一部分。机器学习算法基于样本数据(称为训练数据)建立模型，以便在没有明确编程的情况下进行预测或决策。机器学习算法用于各种各样的应用中，例如在医学、电子邮件过滤、语音识别和计算机视觉中，在这些应用中，开发常规算法来执行所需的任务是困难的或不可行的。[维基]</p><p id="7be7" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们主要有3种机器学习技术</p><p id="647e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">监督学习</strong> - &gt;在监督收益中我们有- &gt;分类和回归</p><p id="a681" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">无监督学习</strong> - &gt;在无监督中我们有- &gt;降维- &gt;聚类</p><p id="3d47" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">强化学习</strong></p><blockquote class="na nb nc"><p id="ea7b" class="kp kq nd kr b ks kt ku kv kw kx ky kz ne lb lc ld nf lf lg lh ng lj lk ll lm im bi translated"><strong class="kr jd">什么是线性回归？</strong></p></blockquote><p id="2d6d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi md translated">线性回归是一种用来模拟观察变量之间关系的技术。简单线性回归背后的思想是将两个变量的观察值“拟合”成它们之间的线性关系。它提供了预测(称之为‘Y’)和输入(称之为‘X’)之间的线性关系。预测变量也称为因变量，输入X称为自变量。</p><p id="a8f3" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果只有一个自变量，它被称为简单线性回归，如果有一个以上的自变量，那么它被称为多元线性回归。</p><blockquote class="na nb nc"><p id="d1eb" class="kp kq nd kr b ks kt ku kv kw kx ky kz ne lb lc ld nf lf lg lh ng lj lk ll lm im bi translated"><strong class="kr jd">让我们学习线性回归背后的数学</strong></p></blockquote><p id="9f67" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi md translated"><span class="l me mf mg bm mh mi mj mk ml di"> S </span>假设我们将‘X’作为自变量，将‘Y’作为因变量。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="df7f" class="lw lx it ls b gy ly lz l ma mb">X = [1,2,3,4,5]</span><span id="8c6b" class="lw lx it ls b gy mc lz l ma mb">Y = [3,2,2,4,5]</span></pre><figure class="ln lo lp lq gt kd gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/27edaa86bc5ac89d285e8cbf0e5d05f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/0*xVUZ-ILpDxLcXfcd"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">x和Y坐标图</figcaption></figure><p id="368d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们需要找到X和y的平均值<strong class="kr jd"/></p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="feef" class="lw lx it ls b gy ly lz l ma mb"><strong class="ls jd">Mean = sum of total observation / total no of observation</strong></span></pre><figure class="ln lo lp lq gt kd gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c1a11183893ac8dbf5d82cd42f2a0f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/0*iGRk0E_jgVqPTMie"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">x和y的平均值</figcaption></figure><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="2cda" class="lw lx it ls b gy ly lz l ma mb"><strong class="ls jd">Now Xi = 3 and Yi = 2.8</strong></span></pre><p id="1150" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们都知道直线的方程式是</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="902e" class="lw lx it ls b gy ly lz l ma mb"><strong class="ls jd">y = mx + c </strong></span></pre><p id="1093" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了找到m和x的值，我们需要做一些计算</p><figure class="ln lo lp lq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nj"><img src="../Images/824543cc33ad7d515893b88511b8fbc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QPHQ78fLUMJSStrF"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">一点计算</figcaption></figure><p id="b699" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了找到m的值</p><figure class="ln lo lp lq gt kd gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/582c168357cd9b2904e1208c92ef00bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/0*GmUn3nO4qFkqBMCl"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">为了找到m的值</figcaption></figure><p id="01e5" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以如果你在我们的方程中放入数值，我们得到</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="7322" class="lw lx it ls b gy ly lz l ma mb"><strong class="ls jd">m = 2/10 = 0.2</strong></span></pre><blockquote class="na nb nc"><p id="d6c4" class="kp kq nd kr b ks kt ku kv kw kx ky kz ne lb lc ld nf lf lg lh ng lj lk ll lm im bi translated">为了找到c，我们有y = mx + c </p></blockquote><p id="e26f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">将x，y和m的值放入我们得到的等式中</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="477d" class="lw lx it ls b gy ly lz l ma mb">y = mx + c</span><span id="3b92" class="lw lx it ls b gy mc lz l ma mb">2.8 = 0.2 * 3 + c</span><span id="8cd6" class="lw lx it ls b gy mc lz l ma mb">2.8 = 0.6 + c</span><span id="2663" class="lw lx it ls b gy mc lz l ma mb">c = 2.8–0.6</span><span id="8112" class="lw lx it ls b gy mc lz l ma mb">c = 2.2</span></pre><p id="d635" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以我们有c= 2.2，我们的平均值是(3，2.8)，让我们用这些点画一条线。</p><p id="85f6" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们有了m和c的值，所以我们可以找到我们的初始回归线</p><figure class="ln lo lp lq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nl"><img src="../Images/9d6daf8f0d7f780fb99311d089810cb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/0*Fo4KOZN-P0lk0-zn"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">初始回归线</figcaption></figure><p id="ee46" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们用上面的等式来预测y '的值</p><p id="3a4d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">给定X = [1，2，3，4，5]，c = 2.2，m = 0.2，我们需要预测y '(预测y的值)</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="e8d9" class="lw lx it ls b gy ly lz l ma mb"><strong class="ls jd">For x = 1</strong></span><span id="3c61" class="lw lx it ls b gy mc lz l ma mb">y’1 = mx + c</span><span id="2b8a" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’1 = (0.2 * 1) + 2.2</span><span id="459e" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’1 = 2.4</span><span id="b604" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">For x = 2</strong></span><span id="cb24" class="lw lx it ls b gy mc lz l ma mb">y’2 = mx + c</span><span id="2c81" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’2 = (0.2 * 2) + 2.2</span><span id="4d61" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’2 = 2.6</span><span id="d33e" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">For x = 3</strong></span><span id="65ba" class="lw lx it ls b gy mc lz l ma mb">y’3 = mx + c</span><span id="f5fe" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’3 = (0.2 * 3) + 2.2</span><span id="8584" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’3 = 2.8</span><span id="1d10" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">For x = 4</strong></span><span id="146c" class="lw lx it ls b gy mc lz l ma mb">y’4 = mx + c</span><span id="b67f" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’4 = (0.2 * 4) + 2.2</span><span id="f1b4" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’4 = 3</span><span id="2e27" class="lw lx it ls b gy mc lz l ma mb"><strong class="ls jd">For x = 5</strong></span><span id="6c02" class="lw lx it ls b gy mc lz l ma mb">y’5 = mx + c</span><span id="1665" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’5 = (0.2 * 5) + 2.2</span><span id="5c12" class="lw lx it ls b gy mc lz l ma mb">=&gt; y’5 = 3.2</span></pre><p id="e073" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以在计算y '的值后</p><figure class="ln lo lp lq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nm"><img src="../Images/bd929acce68649c35bb86ee349828c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/0*vhsoHCApgcBennbI"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">预测y值</figcaption></figure><p id="19a7" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们绘制这些数据，我们会得到</p><figure class="ln lo lp lq gt kd gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6a624abc6896bb43a734e6ae6830b932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/0*C-wuKK4BF4LTvowd"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">实际值(y)和预测值(y’)之间的图</figcaption></figure><p id="3533" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如你所见，我们已经得到了预测的y '，<strong class="kr jd">现在我们的目标是缩小实际y '值和预测y '值之间的距离。</strong></p><p id="2e26" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">有很多方法可以最小化实际值和预测值之间的距离，如“误差平方和”、“绝对误差”、“均方根误差”、“梯度下降”等</p><p id="0361" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我不打算在本文中讨论这些方法，但是在我以后的文章中，我们肯定会了解这些方法。</p><p id="8162" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">查看本文的<a class="ae ko" rel="noopener ugc nofollow" target="_blank" href="/how-linear-regression-actually-work-maths-in-depth-intuition-part-2-c49a8db03013">部分-2 </a>,在这里我们用Python实现了迄今为止我们学到的任何东西</p><div class="no np gp gr nq nr"><a rel="noopener  ugc nofollow" target="_blank" href="/how-linear-regression-actually-work-maths-in-depth-intuition-part-2-c49a8db03013"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">线性回归如何实际工作(数学深度直觉)-第2部分</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">让我们从头开始实现线性回归……</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">pub.towardsai.net</p></div></div><div class="oa l"><div class="ob l oc od oe oa of ki nr"/></div></div></a></div><p id="4264" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">就这样吧，下一篇文章再见。</p><blockquote class="na nb nc"><p id="55a6" class="kp kq nd kr b ks kt ku kv kw kx ky kz ne lb lc ld nf lf lg lh ng lj lk ll lm im bi translated"><strong class="kr jd">看看我以前的文章:</strong></p></blockquote><div class="no np gp gr nq nr"><a rel="noopener  ugc nofollow" target="_blank" href="/dimensional-reduction-feature-selection-part-1-d5e4fac63a11"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">降维—特征选择第1部分</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">特征选择是一个过程，在这个过程中，我们试图减少特征的数量，只找到相关的特征…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">pub.towardsai.net</p></div></div><div class="oa l"><div class="og l oc od oe oa of ki nr"/></div></div></a></div><div class="no np gp gr nq nr"><a rel="noopener  ugc nofollow" target="_blank" href="/lets-learn-about-dimensionality-reduction-df4622f30c84"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">让我们来学习降维</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">降维，或称降维，是将数据从高维空间转换到多维空间</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">pub.towardsai.net</p></div></div><div class="oa l"><div class="oh l oc od oe oa of ki nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://medium.com/nerd-for-tech/machine-learning-automation-1c112e099005" rel="noopener follow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">机器学习自动化…</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">"仅仅因为你能使某件事自动化，并不意味着它就应该自动化."</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">medium.com</p></div></div><div class="oa l"><div class="oi l oc od oe oa of ki nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://medium.com/@iamhimanshutripathi0/product-recommendation-based-on-visual-similarity-on-the-web-machine-learning-project-end-to-end-6d38d68d414f" rel="noopener follow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">基于网页视觉相似性的产品推荐:机器学习项目…</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">众所周知，谷歌、亚马逊、网飞等大型科技公司都在使用推荐系统…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">medium.com</p></div></div><div class="oa l"><div class="oj l oc od oe oa of ki nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://medium.com/datadriveninvestor/natural-langauge-processing-nlp-for-indian-language-hindi-on-web-64d83f16544a" rel="noopener follow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">Web上印度语言(印地语)的自然语言处理(NLP)</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">"语言是一个秘密，每个人都可以处理，对我来说，这是美丽的."</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">medium.com</p></div></div><div class="oa l"><div class="ok l oc od oe oa of ki nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5" rel="noopener follow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">什么是平衡和不平衡数据集？</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">不平衡数据集到平衡数据集的转换技术及其比较</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">medium.com</p></div></div><div class="oa l"><div class="ol l oc od oe oa of ki nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://medium.com/analytics-vidhya/brain-tumor-classification-transfer-learning-e04f84f96443" rel="noopener follow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">基于迁移学习的脑肿瘤分类</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">迁移学习的详细解释以及如何使用它进行分类</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">medium.com</p></div></div><div class="oa l"><div class="om l oc od oe oa of ki nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://medium.com/analytics-vidhya/different-type-of-feature-engineering-encoding-techniques-for-categorical-variable-encoding-214363a016fb" rel="noopener follow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd jd gy z fp nw fr fs nx fu fw jc bi translated">用于分类变量编码的不同类型的特征工程编码技术</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">"让我们从现有的功能创建新的功能."</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">medium.com</p></div></div><div class="oa l"><div class="on l oc od oe oa of ki nr"/></div></div></a></div><p id="183d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你觉得这篇文章有趣，有帮助，如果你从这篇文章中学到了什么，请鼓掌(👏👏)<strong class="kr jd">并留下反馈。</strong></p><p id="e4ca" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">我们连线上</strong><a class="ae ko" href="https://www.linkedin.com/in/iamhimanshu0/" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd">Linkedin</strong></a><strong class="kr jd">，</strong><a class="ae ko" href="https://twitter.com/iam_himanshu0" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd">Twitter</strong></a><strong class="kr jd">，</strong><a class="ae ko" href="https://instagram.com/iamhimanshu0/" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd">insta gram</strong></a><strong class="kr jd">，</strong><a class="ae ko" href="https://github.com/iamhimanshu0" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd">Github</strong></a><strong class="kr jd">，以及</strong> <a class="ae ko" href="https://www.facebook.com/iamhimanshu0" rel="noopener ugc nofollow" target="_blank"> <strong class="kr jd">脸书</strong> </a> <strong class="kr jd">。</strong></p><p id="eb74" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">感谢阅读！</strong></p></div></div>    
</body>
</html>