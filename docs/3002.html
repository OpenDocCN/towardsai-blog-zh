<html>
<head>
<title>Explained: Reverse Attention Network (RAN) in Image Segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释:图像分割中的反向注意网络(RAN)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/explained-reverse-attention-network-in-image-segmentation-baa6bdf08ac4?source=collection_archive---------2-----------------------#2022-07-31">https://pub.towardsai.net/explained-reverse-attention-network-in-image-segmentation-baa6bdf08ac4?source=collection_archive---------2-----------------------#2022-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6a669b29260b63de6390ba7294c6a59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LWT3cEJhFZIZ-e-t"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">德文·艾弗里在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="a7d2" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">目录</h1><p id="443d" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><a class="ae kc" href="#c6cf" rel="noopener ugc nofollow"> ⭐️问题</a> <br/> <a class="ae kc" href="#85f3" rel="noopener ugc nofollow"> ⭐️一解</a> <br/> <a class="ae kc" href="#cc2a" rel="noopener ugc nofollow"> ⭐ ️Reverse关注网(冉)</a> <br/> ∘ <a class="ae kc" href="#cf42" rel="noopener ugc nofollow">反向关注分支(RB) </a> <br/> ∘ <a class="ae kc" href="#563c" rel="noopener ugc nofollow">反向关注分支(RAB) </a> <br/> ∘ <a class="ae kc" href="#db70" rel="noopener ugc nofollow">结合成绩</a> <br/> <a class="ae kc" href="#dd68" rel="noopener ugc nofollow"> ⭐️训练</a> <br/> <a class="ae kc" href="#0f67" rel="noopener ugc nofollow"> ⭐️表演</a> <br/> <a class="ae kc" href="#7a83" rel="noopener ugc nofollow">引用</a></p><h1 id="c6cf" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">⭐️问题</h1><ul class=""><li id="13a1" class="lz ma iq ld b le lf li lj lm mb lq mc lu md ly me mf mg mh bi translated">大多数基于CNN的语义分割方法专注于简单地获得正确的预测，而没有<strong class="ld ir">机制教导模型辨别类别之间的差异。</strong> ( <em class="mi">因此不太常见的类别的特征可能会被忽略</em>)</li><li id="1173" class="lz ma iq ld b le mj li mk lm ml lq mm lu mn ly me mf mg mh bi translated">由于类别之间的视觉相似性，高级特征在不同的类别中共享，这可能在包含不同类别的 <strong class="ld ir">边界的<strong class="ld ir">区域中产生混淆的结果(例如，具有对象的背景，因为它们具有相似的<em class="mi">激活强度</em>】<strong class="ld ir">)或者当它们混合在一起时。</strong></strong></strong></li></ul><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/6e882c20938d3b2f9ae360ce754b3def.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J4linz91OO-yf09GTqOUPg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图一</figcaption></figure><p id="c225" class="pw-post-body-paragraph lb lc iq ld b le mt lg lh li mu lk ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">为了更好地理解这个问题，请参见图1。从注意力热图中可以看出，很明显，大多数当今的编码器-解码器模型在两个对象“混合”在一起的部分会有强烈的神经激活(<em class="mi">又名。具有模糊的边界或区域，其中2+个对象共享相似的空间模式</em>)，在预测期间，模型根本不应该过多关注那些“混合”部分。</p><h1 id="85f3" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">⭐️的解决方案</h1><ul class=""><li id="da2e" class="lz ma iq ld b le lf li lj lm mb lq mc lu md ly me mf mg mh bi translated">作者设计了一种机制来识别这些混合的特殊区域，并放大较弱的激活以捕捉目标对象，因此<strong class="ld ir">网络不仅学习辨别背景类别，还学习辨别图像中存在的不同对象。</strong></li></ul><p id="cec0" class="pw-post-body-paragraph lb lc iq ld b le mt lg lh li mu lk ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">因此，他们提出了一种新颖的架构，并将其命名为“<strong class="ld ir">反向注意网络</strong>”(<strong class="ld ir">冉</strong>)来解决上述问题。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/ffe9effd33b367b1160460bbf366befc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eKZ2STmu59n94NGlN1f5IA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图2:他们提议的网络:RAN</figcaption></figure><p id="ebd6" class="pw-post-body-paragraph lb lc iq ld b le mt lg lh li mu lk ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">在RAN中，有两个不同的分支(<em class="mi">一个用红色圈出，一个用蓝色圈出</em>)，分别用于学习背景特征和对象特征。</p><p id="4845" class="pw-post-body-paragraph lb lc iq ld b le mt lg lh li mu lk ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">为了进一步<strong class="ld ir">突出从对象类学到的知识</strong>，一个<strong class="ld ir">反向注意结构</strong>被指定生成<strong class="ld ir">每类掩码</strong>以放大混乱区域中对象类的激活。</p><p id="e938" class="pw-post-body-paragraph lb lc iq ld b le mt lg lh li mu lk ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">最后，将预测融合在一起以产生最终预测。</p><h1 id="cc2a" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">⭐ ️Reverse关注网络</h1><p id="2f91" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了更详细地理解所提出的模型，请参见图3。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mz"><img src="../Images/f222ea5620cf8dc086d82157fe77d4a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vru8mRZYhVyd1aTEqp93HA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图RAN的整体视图。有三种颜色的树枝，分别是黄色、蓝色和绿色。</figcaption></figure><p id="5017" class="pw-post-body-paragraph lb lc iq ld b le mt lg lh li mu lk ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated">给出输入图像后，将该过程分解为几个步骤:</p><ul class=""><li id="5118" class="lz ma iq ld b le mt li mu lm na lq nb lu nc ly me mf mg mh bi translated">使用选定的模型架构生成特征图(<em class="mi">通常是ResNet-101或VGG16，但也可以变化</em> ) <strong class="ld ir">来学习对象特征。</strong></li><li id="b026" class="lz ma iq ld b le mj li mk lm ml lq mm lu mn ly me mf mg mh bi translated">然后，地图被分成两个分支。</li></ul><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/5dde720a27c44a38a1482280f25851ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*COwzgzT0pfMwKHraXCRygw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图4:反向分支(从图3中截取)。</figcaption></figure><h2 id="cf42" class="ne ke iq bd kf nf ng dn kj nh ni dp kn lm nj nk kr lq nl nm kv lu nn no kz np bi translated"><strong class="ak">反向分支</strong> (RB)</h2><ul class=""><li id="3a0c" class="lz ma iq ld b le lf li lj lm mb lq mc lu md ly me mf mg mh bi translated">用黄色着色，该模型首先训练一个<strong class="ld ir"> CONV_rev </strong>层，以明确地学习“<strong class="ld ir">反向对象类</strong>”(<em class="mi">反向对象类是对象类</em>的反向基础事实)。</li><li id="aeda" class="lz ma iq ld b le mj li mk lm ml lq mm lu mn ly me mf mg mh bi translated">为了得到反向的对象类，背景和其他类设置为1，而对象类设置为0。</li><li id="5324" class="lz ma iq ld b le mj li mk lm ml lq mm lu mn ly me mf mg mh bi translated">然而，当它是多类分割问题时，通常使用一种替代方法，即在馈送到基于softmax的分类器之前，反转所有类激活的符号(<em class="mi"/><strong class="ld ir"><em class="mi">负</em> </strong> <em class="mi">块</em>)。这种方法允许<strong class="ld ir"> CONV_rev </strong>层使用相同的基于类别的基本事实标签进行训练。</li></ul><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/12b44b6dc01f3777f94e3d08445f8301.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*kUyqAKpyZaHzQbCFnxfsPA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图5:反向注意分支(从图3中截取)。</figcaption></figure><h2 id="563c" class="ne ke iq bd kf nf ng dn kj nh ni dp kn lm nj nk kr lq nl nm kv lu nn no kz np bi translated"><strong class="ak">反向注意分支(RAB) </strong></h2><ul class=""><li id="cf9a" class="lz ma iq ld b le lf li lj lm mb lq mc lu md ly me mf mg mh bi translated">由于较差的性能，<strong class="ld ir">反向注意分支</strong>被提出来突出被原始预测忽略的区域(<em class="mi">包括</em> <em class="mi">混合区域和背景区域</em>)，而不是通过激活反向分支直接对原始预测应用逐元素减法。反向注意的输出会产生一个面向类别的面具来放大反向激活图。</li><li id="aaca" class="lz ma iq ld b le mj li mk lm ml lq mm lu mn ly me mf mg mh bi translated">如图3和图5所示，来自输入图像的初始特征图被馈送到<strong class="ld ir">conv _组织</strong>层。</li><li id="96aa" class="lz ma iq ld b le mj li mk lm ml lq mm lu mn ly me mf mg mh bi translated">然后，结果特征图的像素值被<strong class="ld ir">负</strong>块翻转。</li><li id="e093" class="lz ma iq ld b le mj li mk lm ml lq mm lu mn ly me mf mg mh bi translated">然后，<strong class="ld ir"> sigmoid </strong>函数被应用于在[0，1]之间转换像素值，然后将特征图馈送到注意图，在那里应用注意遮罩。</li><li id="981a" class="lz ma iq ld b le mj li mk lm ml lq mm lu mn ly me mf mg mh bi translated">上述步骤可以总结为公式1，其中I，j表示像素位置。</li><li id="f258" class="lz ma iq ld b le mj li mk lm ml lq mm lu mn ly me mf mg mh bi translated">因此，具有小的或负面反应的区域将通过<strong class="ld ir">否定</strong>和<strong class="ld ir">s形</strong>操作来突出显示，但是正面激活(或置信分数)的区域将在反向注意分支中被抑制。</li></ul><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e0675e33a5ed5b409aeee32eb4e91ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*t2DqRX2QTmxbUK4aIVfXag.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">一级方程式</figcaption></figure><h2 id="db70" class="ne ke iq bd kf nf ng dn kj nh ni dp kn lm nj nk kr lq nl nm kv lu nn no kz np bi translated">结合结果</h2><ul class=""><li id="1ca4" class="lz ma iq ld b le lf li lj lm mb lq mc lu md ly me mf mg mh bi translated">然后，来自<strong class="ld ir">反向注意分支</strong>的映射被<strong class="ld ir">反向分支逐元素相乘。</strong>从原始预测中减去结果图，生成最终预测。</li></ul><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/9684b8f5391e081b12ef5589f4c39dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKjH1EOCyIz8oY9YPwGfSA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图RAN的整体视图(为了便于参考，复制了它！).</figcaption></figure><h1 id="dd68" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">⭐️培训</h1><p id="92ac" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这超出了本文的范围，所以我们只向您展示本文的原文:</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/80bb8eda841f3afd4cffff3792340346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qj9PQVLk5oUrc19uo0bExw.png"/></div></div></figure><h1 id="0f67" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">⭐️表演</h1><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/d2102aeabbbee5d3670f76e5f007189d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXc4nMdzyu5-VMOIoX1Yjw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">表1:在流行数据集上与流行语义分割架构的性能比较。</figcaption></figure><p id="3e98" class="pw-post-body-paragraph lb lc iq ld b le mt lg lh li mu lk ll lm mv lo lp lq mw ls lt lu mx lw lx ly ij bi translated"><strong class="ld ir">谢谢！❤️ </strong></p><h1 id="7a83" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">引用</h1><p id="78b5" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">[1] <a class="ae kc" href="https://arxiv.org/pdf/1707.06426.pdf" rel="noopener ugc nofollow" target="_blank">具有反向注意的语义分割</a></p></div></div>    
</body>
</html>