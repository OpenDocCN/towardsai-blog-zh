<html>
<head>
<title>Dreamer: A State-of-the-art Model-Based Reinforcement Learning Agent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Dreamer:一个基于模型的强化学习代理</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/dreamer-8b5a42acebbf?source=collection_archive---------2-----------------------#2020-05-31">https://pub.towardsai.net/dreamer-8b5a42acebbf?source=collection_archive---------2-----------------------#2020-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8798" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">强化学习</h2><div class=""/><div class=""><h2 id="19fd" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">简要介绍一种基于模型的强化学习算法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/6672071f9d12bcf007da5d21f72f6330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1EeiWxfNgoJ4SOU5g4Q4uA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来自<a class="ae lh" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2682017" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的莱安德罗·德·卡瓦略</figcaption></figure><p id="e983" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们讨论一种基于模型的强化学习代理，称为Dreamer，由Hafner等人在DeepMind提出，它在各种基于图像的控制任务上实现了最先进的性能，但需要的样本比当代的无模型方法少得多。</p><h2 id="c3c4" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">概观</h2><p id="a95d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">梦者由三部分组成:</p><ul class=""><li id="69ee" class="nb nc it lk b ll lm lo lp lr nd lv ne lz nf md ng nh ni nj bi translated"><strong class="lk jd">动力学学习:</strong>作为一种基于模型的RL方法，Dreamer学习由四个部分组成的动力学模型:表征模型、转移模型、重建模型、奖励模型。</li><li id="09a9" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">行为学习:</strong>基于动力学模型，梦想家学习一种演员-评论家架构，以最大化<em class="np">想象轨迹</em>上的回报。</li><li id="9621" class="nb nc it lk b ll nk lo nl lr nm lv nn lz no md ng nh ni nj bi translated"><strong class="lk jd">环境交互:</strong>造梦者使用动作模型与环境交互，进行数据收集。</li></ul><p id="df36" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章的其余部分，我们将把我们的注意力集中在前两部分，因为最后一部分很快就会变得明显。</p><h2 id="0409" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">动力学学习</h2><p id="1183" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">梦想家将动态表现为一个由以下部分组成的连续模型</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/52bff6ee8923b0ee501c81d94597f345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*qL9L1VsAd-rC4q9SpyCzYw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">所有这些模型都由具有高斯头部的(去)卷积/全连接网络来参数化。</figcaption></figure><p id="3cca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文的其余部分，我们将转换模型称为先验模型，将表示模型称为后验模型，因为后者额外取决于观察结果。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/8bf1370fbd2c6febaf6c35ae00ed136a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4b2l7Fr4eavQV5ZMouCT-g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图一。单个时间步长中的递归状态空间模型(RSSM)。h是确定性状态，s是先验随机状态，s '是后验随机状态。</figcaption></figure><p id="8603" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">像它的前身PlaNet一样，Dreamer采用递归状态空间模型(RSSM)作为世界模型，它将状态空间分为随机和确定性组件。图1显示了RSSM模型是如何在一个时间步长内展开的。在训练动力学模型时，RNN将来自前一时间步的动作<em class="np"> a_{t-1} </em>和后验随机状态<em class="np">s’_ { t-1 }</em>作为输入，并输出确定性状态<em class="np"> h_t </em>。确定性状态<em class="np"> h_t </em>然后被a)馈送到具有单个隐藏层的MLP中，以计算先前的随机状态<em class="np">s _ t</em>；b)与图像嵌入<em class="np"> e_t </em>连接，并馈入另一个单层MLP以计算后验随机状态。之后，我们使用<em class="np"> h_t </em>和<em class="np">s’_ t</em>的串联作为起始潜变量来重构图像、奖励等。数学上，我们可以把世界模型分成以下几个部分。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/2372d42c79489eb12cabd65ac9a8105a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nCv43_4rzYlW141M4u_YgA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">实际上，f是一个RNN，所有的分布都是高斯分布，参数由相应的网络产生。</figcaption></figure><p id="e001" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">也可以将RSSM模型视为序列VAE，其中时间步<em class="np"> t </em>处的潜在变量通过转移模型与前一时间步<em class="np"> t-1 </em>的潜在变量相关联。这给了我们下面的变分证据下界(ELBO)目标。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/e6f8aaf5bc8a80085930cf291b8198d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rykeMoyT4rFkW562qiOyOA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">训练动力学模型的目标，其中𝛽正则化从o_t到s_t的信息</figcaption></figure><p id="65cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最大化这个目标导致模型状态预测观察和奖励的顺序，同时限制在每个时间步提取的信息量。这鼓励模型通过尽可能依赖于在先前时间步骤提取的信息来重建每幅图像，并且仅在必要时从当前图像访问附加信息。结果，信息正则化器鼓励模型<em class="np">学习长期依赖性</em>。</p><p id="bc92" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们继续之前，值得强调的是确定性和随机状态模型服务于不同的目的:确定性部分可靠地保留跨许多时间步骤的信息，基于此，随机部分建立环境的紧凑信念状态。后者尤其重要，因为环境对代理人来说通常是部分可观察的——图4显示，没有随机部分，代理人什么也学不到！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/a1f6a90f6204b35308fb1d1815713da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E4FdovXufcZRC2OpUTzrMQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">从:梦想到控制:通过潜在想象学习行为</figcaption></figure><h2 id="44fe" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">行为学习</h2><p id="a703" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">梦想家在行为学习的状态空间上训练演员-批评家模型。更具体地说，梦想家首先根据代理人过去的经验，从一些真实的模型状态<em class="np"> s_𝜏 </em>开始想象轨迹，遵循过渡模型<em class="np"> q(s_{t+1}|s_t，a_t) </em>、策略<em class="np"> q(a_t|s_t) </em>和奖励模型<em class="np"> q(r_t|s_t) </em> —见右图1。注意，在想象过程中，RNN把先前的随机状态作为输入，因为没有给出更多的观察。然后，我们通过最大化沿着这些轨迹的预期回报来训练演员-评论家模型，目标如下</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/b93f0e2a3ad47e2287b80f37f4a99c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ir4hHriFku-CTHWtNQv80g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">其中，𝜃是动态模型的参数，𝜙、𝜓分别是动作模型和价值模型的参数。V_𝝀(s)是TD(𝝀的𝝀-return)，其中𝝀控制n步目标的权重</figcaption></figure><p id="fb27" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其实这些RL物镜的选择还是挺出彩的。我曾试图将一些其他非政策方法应用于梦想家学习的潜在空间，如retrace(𝝀的SAC)，因为我认为世界模型(即动力学、奖励和折扣模型)引入的函数逼近误差可能会导致对想象轨迹的不准确预测，从而削弱AC模型的性能。然而，实验结果表明了一个相反的故事:从想象的轨迹学习在学习速度和最终表现方面优于在潜在空间应用偏离策略的方法。这是因为从想象的轨迹学习提供了更丰富的训练信号，促进了学习过程；如果我们减少想象轨迹的长度<em class="np"> H </em>到<em class="np"> 1 </em>，做梦者的表现比将SAC应用于潜在空间更差。</p><h1 id="a853" class="nw mf it bd mg nx ny nz mj oa ob oc mm ki od kj mp kl oe km ms ko of kp mv og bi translated">参考</h1><p id="f392" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">丹尼尔·哈夫纳，蒂莫西·莉莉卡普，吉米·巴，穆罕默德·诺鲁齐。通过潜在想象学习行为。2020年的ICLR</p><p id="ea4a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">亚历山大·阿莱米、伊恩·费希尔、约书亚·狄龙、凯文·墨菲。深度变化的信息瓶颈。在2017年的ICLR</p><h1 id="079d" class="nw mf it bd mg nx ny nz mj oa ob oc mm ki od kj mp kl oe km ms ko of kp mv og bi translated">感谢</h1><p id="b123" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我要特别感谢Danijar Hafner对代码的讨论。</p></div></div>    
</body>
</html>