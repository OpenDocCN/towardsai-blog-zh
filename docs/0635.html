<html>
<head>
<title>Content-Based Recommendation System using Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于内容的词嵌入推荐系统</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/content-based-recommendation-system-using-word-embeddings-c1c15de1ef95?source=collection_archive---------1-----------------------#2020-06-29">https://pub.towardsai.net/content-based-recommendation-system-using-word-embeddings-c1c15de1ef95?source=collection_archive---------1-----------------------#2020-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/afa133634ec8acd5912f03adbee7b4ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9dv8CzZC3i9x30EG8J6Y7A.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图片归功于<a class="ae jg" href="https://thewordpoint.com/" rel="noopener ugc nofollow" target="_blank">thewordpoint.com</a></figcaption></figure><h2 id="e8f7" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="ef78" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">平均Word2Vec和TF-IDF Word2Vec</h2></div><p id="c33c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://towardsdatascience.com/building-a-content-based-book-recommendation-engine-9fd4d57a4da" rel="noopener" target="_blank">在我之前的文章</a>中，我写了一个基于内容的推荐引擎，它使用TF-IDF来处理Goodreads数据。在本文中，我使用相同的Goodreads数据，并使用word2vec构建推荐引擎。</p><p id="9e8a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">和上一篇一样，我准备用同样的图书描述来推荐图书。我们使用的算法总是很难处理原始文本数据，它只能理解数字形式的数据。为了让它明白，我们需要将原始文本转换成数字。这可以在NLP的帮助下通过将原始文本转换成向量来实现。</p><p id="9e63" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在之前的推荐引擎中，我使用TF-IDF将原始文本转换为向量。然而，它没有捕获语义，并且给出了稀疏矩阵。NLP领域的研究和突破正以前所未有的速度发展。神经网络架构因理解单词表示而闻名，这也被称为单词嵌入。</p><p id="3947" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">单词嵌入特征创建密集的低维特征，而TF-IDF创建稀疏的高维特征。它也很好地抓住了语义。</p><p id="f924" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中一个重大突破是Google在2013年推出的word2vec嵌入。使用嵌入Word2vec在很多方面都优于TF-IDF。NLP的另一个转折点是2017年推出的变压器网络。继多项研究、BERT(变压器的双向编码器表示法)之后，引入了许多其他被认为是NLP中最先进的算法。</p><p id="3715" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">本文探讨了如何使用<strong class="lj jt"> average Word2Vec </strong>和<strong class="lj jt"> TF-IDF Word2Vec </strong>构建推荐引擎。我将在下一篇文章中探讨如何使用BERT嵌入。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="cfaf" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">Word2Vec是什么？</h1><p id="a72f" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">Word2Vec是一个简单的神经网络模型，只有一个隐藏层。它预测句子或语料库中每个单词的相邻单词。我们需要得到由模型的隐藏层学习的权重，并且同样可以用作单词嵌入。</p><p id="f5e7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看看下面的句子是如何工作的:</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/5a8e4ad33205cc3679e253ca7e47486e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHCu34KJUpgPBa7FZutolA.png"/></div></div></figure><p id="89b6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">根据以上内容，我们假设单词“理论家”是我们的输入单词。它有一个大小为2的上下文窗口。这意味着我们只将输入单词两侧的两个相邻单词视为相邻单词。</p><p id="5bda" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，任务是逐个挑选邻近的单词(上下文窗口中的单词)，并找到词汇表中每个单词是所选邻近单词的概率。在这里，上下文窗口可以根据我们的要求进行更改。</p><p id="4106" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Word2Vec有两种模型架构变体1)连续词包(CBoW)和SkipGram。互联网上充斥着大量关于Word2Vec的文章，因此我没有详细解释。关于Word2Vec的更多细节，请查看此处的。</p><blockquote class="nm"><p id="a156" class="nn no jj bd np nq nr ns nt nu nv mc dk translated">简单来说，Word2Vec取单词，给出D维空间的向量。</p></blockquote><p id="16b6" class="pw-post-body-paragraph lh li jj lj b lk nw kt lm ln nx kw lp lq ny ls lt lu nz lw lx ly oa ma mb mc im bi translated">请注意，Word2Vec提供了低维(50–500)和稠密(不是稀疏的，大多数值是非零的)的单词嵌入。这个推荐引擎我用了300维向量。正如我上面提到的，Word2Vec善于捕捉语义含义和关系。</p><p id="ae96" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">训练我们自己的单词嵌入是昂贵的过程，并且还需要大的数据集。我没有一个大的数据集，因为我丢弃了Goodreads的数据，这些数据只与类型有关:商业和烹饪。使用Google预先训练的单词嵌入，这些单词嵌入是在大型语料库(如维基百科、新闻文章等)上训练的。</p><p id="3458" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">预先训练的嵌入有助于获得你想要的单词的向量。它是一个大型的键-值对集合，其中键是词汇表中的单词，值是它们对应的单词向量。</p><p id="6ec5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们的问题中，我们需要将书籍描述转换成一个向量，并找出这些向量之间的相似性来推荐书籍。每本书的描述都是一个<em class="ob">句子或单词序列。</em>我试过平均Word2vec和TF-IDF Word2Vec等两种方法。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="cd2e" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">平均Word2Vec</h1><p id="3808" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">让我们在数据集中举一些随机描述的例子</p><p id="2aa0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">书名:<em class="ob">投资的四大支柱</em></p><pre class="ni nj nk nl gt oc od oe of aw og bi"><span id="764e" class="oh ml jj od b gy oi oj l ok ol">Book Description:</span><span id="c81a" class="oh ml jj od b gy om oj l ok ol">william bernstein american financial theorist neurologist research field modern portfolio theory research financial books individual investors wish manage equity field lives portland oregon</span></pre><p id="24d1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">好了，怎么把上面的描述转换成向量呢？如你所知，word2vec取单词，给出一个d维向量。首先，我们需要将句子分割成单词，并找到句子中每个单词的向量表示。</p><p id="f05c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面的例子有23个单词。让我们用w1，w2，w3，w4 …w23来表示这些单词。(w1 =威廉，w2 =伯恩斯坦……w23 =俄勒冈州)。计算所有23个单词的Word2Vec。</p><p id="e9d3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，对所有向量求和，并除以描述中的总字数(n)。它可以表示为v1，计算如下</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/083627f71dd3a39815b06c96be89eabf.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*yZtMrZlG4SeemjHQIAtffg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">平均word2vec</figcaption></figure><p id="7995" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里，向量是d维的(使用300维)</p><p id="c7da" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">N =描述1中的字数(总计:23)</p><p id="12b8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">v1 =图书描述1的矢量表示。</p><p id="bf32" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是我们计算平均word2vec的方法。同样，其他书籍的描述也可以转换成向量。我用Python实现了同样的功能，下面给出了代码片段。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="7e64" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">TF-IDF Word2Vec</h1><p id="8050" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">TF-IDF是一个术语频率-逆文档频率。它有助于计算给定单词相对于文档和语料库中其他单词的重要性。它计算两个量，TF和IDF。将两者结合将得到TF-IDF分数。对TF-IDF如何工作的详细解释超出了本文的范围，因为互联网上充斥着许多文章。请点击此处查看更多关于TF-IDF的详细信息。</p><p id="65a0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们来看看TF-IDF Word2Vec的工作原理，</p><p id="378b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">考虑相同的描述示例</p><pre class="ni nj nk nl gt oc od oe of aw og bi"><span id="1be8" class="oh ml jj od b gy oi oj l ok ol">Book Description:</span><span id="6e6b" class="oh ml jj od b gy om oj l ok ol">william bernstein american financial theorist neurologist research field modern portfolio theory research financial books individual investors wish manage equity field lives portland oregon</span></pre><p id="eb3b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">同样，描述有23个单词。让我们用w1，w2，w3，w4 …w23来表示这些单词。(w1 =威廉，w2 =伯恩斯坦……..w23 =俄勒冈州)。</p><h2 id="c4aa" class="oh ml jj bd mm oo op dn mq oq or dp mu lq os ot mw lu ou ov my ly ow ox na jp bi translated">计算TF-IDF Word2Vec的步骤:</h2><ol class=""><li id="e4bf" class="oy oz jj lj b lk nc ln nd lq pa lu pb ly pc mc pd pe pf pg bi translated">为上面描述中的每个单词计算TF-IDF向量。让我们称TF-IDF向量为tf1，tf2，tf3…tf23。请注意TF-IDF向量不会给出d维向量。</li><li id="b87c" class="oy oz jj lj b lk ph ln pi lq pj lu pk ly pl mc pd pe pf pg bi translated">计算描述中每个单词的单词2Vec</li><li id="8779" class="oy oz jj lj b lk ph ln pi lq pj lu pk ly pl mc pd pe pf pg bi translated">将每个单词的TF-IDF分数和Word2Vec向量表示相乘，总和相同。</li><li id="137d" class="oy oz jj lj b lk ph ln pi lq pj lu pk ly pl mc pd pe pf pg bi translated">然后，将总和除以TF-IDF向量之和。它可以被称为v1，并写成如下形式</li></ol><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/831c3b967660eb4f53a82027b5b5dd78.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*WvyAn4dQq3rxvPot8-dvsg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">tf-idf word2vec</figcaption></figure><p id="344b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">v1 =图书描述1的矢量表示。这是计算TF-IDF Word2Vec的方法。同样，我们可以将其他描述转换成向量。我用Python实现了同样的功能，下面给出了代码片段。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="3a24" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">基于内容的推荐系统</h1><p id="22aa" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">基于内容的推荐系统通过获取书籍的相似性来向用户推荐书籍。这个推荐系统根据图书描述推荐图书。它根据书籍的描述来识别它们之间的相似性。它还会考虑用户以前的图书历史，以便推荐类似的图书。</p><p id="b781" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:如果用户喜欢西德尼·谢尔顿的小说“告诉我你的梦”，那么推荐系统推荐用户阅读西德尼·谢尔顿的其他小说，或者推荐体裁为“非小说”的小说。(西德尼·谢尔顿小说属于非小说类)。</p><p id="8ccb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们需要找到与给定书籍相似的书籍，然后向用户推荐这些相似的书籍。如何发现给定的书是相似还是不相似？使用相似性度量来找到相同的结果。在我们的推荐系统中使用了<strong class="lj jt"> <em class="ob">余弦相似度</em> </strong>来推荐书籍。有关相似性度量的更多详细信息，请参考此<a class="ae jg" href="https://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><p id="1f4d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如我上面提到的，我们使用goodreads.com的数据，没有用户阅读历史。因此，我不能使用协作推荐引擎。</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pn"><img src="../Images/fe80c0fc020f43c578ae7607e111ec21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGo2V6OK-NOnq_0O0niDNw.png"/></div></div></figure></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="092a" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">数据</h1><p id="4348" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">我从goodreads.com那里搜集了有关商业、非小说和烹饪类型的书籍细节。</p><h2 id="51b4" class="oh ml jj bd mm oo op dn mq oq or dp mu lq os ot mw lu ou ov my ly ow ox na jp bi translated">密码</h2><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><h2 id="7657" class="oh ml jj bd mm oo op dn mq oq or dp mu lq os ot mw lu ou ov my ly ow ox na jp bi translated">输出</h2><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/39f9e51c3527ef2fd553f8ab9093880d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iHXSyHasww5eoQEdNemLcw.png"/></div></div></figure><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/087abf827a67f67e0be330f2cb6b901c.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*OJthL3kY8Gr49BjJjffzMA.png"/></div></figure><p id="16c6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该数据由2382条记录组成。它有两种类型，1)商业(1185项记录)2)非小说(1197项记录)。此外，它还包括书名、描述、作者姓名、评级和图书图片链接。</p><h1 id="1c50" class="mk ml jj bd mm mn ps mp mq mr pt mt mu ky pu kz mw lb pv lc my le pw lf na nb bi translated">文本预处理</h1><p id="a3bf" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">清理图书描述并将清理后的描述存储在一个名为“cleaned”的新变量中</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="a64a" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">推荐引擎</h1><p id="cf09" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">使用平均Word2Vec和TF-IDF Word2Vec单词嵌入构建两个推荐引擎。</p><h1 id="8e31" class="mk ml jj bd mm mn ps mp mq mr pt mt mu ky pu kz mw lb pv lc my le pw lf na nb bi translated">平均Word2Vec</h1><p id="5672" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">将描述拆分成单词并存储在一个名为“语料库”的列表中，用于训练我们的word2vec模型</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="07f6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用Google预训练的Word2Vec模型训练我们的语料库。</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="36ec" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">创建了一个名为vectors的函数，用于生成平均Word2Vec嵌入，并将其存储为一个名为“word _ embeddings”的列表。代码遵循我在上面的word2vec解释中所写的步骤。</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><h2 id="428d" class="oh ml jj bd mm oo op dn mq oq or dp mu lq os ot mw lu ou ov my ly ow ox na jp bi translated">使用平均Word2Vec的前5项建议</h2><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="eea5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们推荐丹·布朗的书《达芬奇密码》</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="6300" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该模型基于书籍描述中存在的相似性来推荐其他丹·布朗的书籍。</p><p id="8b61" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们推荐阿加莎·克里斯蒂的书《罗杰·艾克罗伊德谋杀案》</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="5db5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这本书属于悬疑惊悚类，推荐一种类似的小说。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="b061" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">构建TF-IDF Word2Vec模型</h1><p id="ae0c" class="pw-post-body-paragraph lh li jj lj b lk nc kt lm ln nd kw lp lq ne ls lt lu nf lw lx ly ng ma mb mc im bi translated">代码解释了我在上面提到的TF-IDF Word2Vec模型过程中的相同步骤。我使用的是同一个语料库，只是在单词嵌入上有所变化</p><h2 id="3d92" class="oh ml jj bd mm oo op dn mq oq or dp mu lq os ot mw lu ou ov my ly ow ox na jp bi translated">构建TF-IDF模型</h2><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><h2 id="8604" class="oh ml jj bd mm oo op dn mq oq or dp mu lq os ot mw lu ou ov my ly ow ox na jp bi translated">构建TF-IDF Word2Vec嵌入</h2><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><h2 id="dc03" class="oh ml jj bd mm oo op dn mq oq or dp mu lq os ot mw lu ou ov my ly ow ox na jp bi translated">使用TF-IDF Word2Vec的前5项建议</h2><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="ca1d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们推荐丹·布朗的书<em class="ob">《达芬奇密码》</em></p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="9b94" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以看到，该模型推荐夏洛克家园小说和输出不同于平均word2vec。</p><p id="fd15" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们推荐阿加莎·克里斯蒂的《罗杰·艾克罗伊德谋杀案》</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="po pp l"/></div></figure><p id="58f3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">同样，它推荐不同于一般的word2vec。但它给出了一个类似阿加莎克里斯蒂的小说。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="1477" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">似乎TF-IDF Word2Vec给出了比一般Word2Vec更强有力的推荐。本文只探讨如何使用平均Word2Vec和TF-IDF Word2Vec构建推荐引擎。没有比较这两个模型的结果。我使用的数据非常少，而且当我们处理更大的数据集时，结果肯定会改变。此外，我们可以使用Fast Text(来自脸书)和Glove(来自斯坦福)预先训练的单词嵌入来代替Google one。</p><p id="cd93" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">真实世界的推荐系统更加强大和先进。A/B测试用于评估推荐引擎，业务领域在评估和挑选最佳推荐引擎中也起着重要作用。</p><p id="b716" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我的下一篇文章中，我将展示如何使用BERT嵌入来构建相同类型的推荐引擎。你可以在我的<a class="ae jg" href="https://github.com/sdhilip200/Content-Based-Recommendation---Good-Reads-data" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中找到完整的代码和数据。</p><p id="534e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢阅读。请继续学习，并关注更多内容！</p></div></div>    
</body>
</html>