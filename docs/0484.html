<html>
<head>
<title>Reading: ESRGAN — Enhanced Super-Resolution Generative Adversarial Networks (Super Resolution &amp; GAN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阅读:增强的超分辨率生成对抗网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/reading-esrgan-enhanced-super-resolution-generative-adversarial-networks-super-resolution-e8533ad006b5?source=collection_archive---------1-----------------------#2020-05-12">https://pub.towardsai.net/reading-esrgan-enhanced-super-resolution-generative-adversarial-networks-super-resolution-e8533ad006b5?source=collection_archive---------1-----------------------#2020-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3d3f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="7c01" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">胜过<a class="ae ko" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c?source=post_page---------------------------" rel="noopener"> SRCNN </a>、<a class="ae ko" href="https://medium.com/@sh.tsang/review-edsr-mdsr-enhanced-deep-residual-networks-for-single-image-super-resolution-super-4364f3b7f86f" rel="noopener"> EDSR </a>和<a class="ae ko" href="https://medium.com/@sh.tsang/review-rcan-deep-residual-channel-attention-networks-super-resolution-fbbf04224c22" rel="noopener"> RCAN </a>，以及<a class="ae ko" href="https://medium.com/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490" rel="noopener"> SRGAN </a>。还获得PIRM2018-SR挑战赛第一名</h2></div><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/c03969d56731e22144ccf8e9e71b3cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IS1rXV5jWxKPfeCkILvoZw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf"> ESRGAN可以比</strong> <a class="ae ko" href="https://medium.com/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490" rel="noopener"> <strong class="bd lf"> SRGAN </strong> </a>有更清晰的结果</figcaption></figure><p id="33a2" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi mc translated"><span class="l md me mf bm mg mh mi mj mk di">在</span>这个故事中，描述了由香港中文大学、中国科学院、中国科学院大学和南洋理工大学合作的增强型超分辨率生成对抗网络(ESRGAN)。在ESRGAN中:</p><ul class=""><li id="094d" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated"><strong class="li ja">引入无批量归一化</strong>的残差中残差密集块(RRDB)。</li><li id="ab05" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">来自<strong class="li ja">相对论甘</strong>的想法是让鉴别器预测相对真实度而不是绝对值。</li><li id="2c96" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">使用激活前使用<strong class="li ja">特征的<strong class="li ja">感知损失</strong>，为亮度一致性和纹理恢复提供更强的监督。</strong></li></ul><p id="3bcc" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">最终，<strong class="li ja"> ESRGAN获得PIRM2018-SR挑战赛</strong>第一名。这是一篇在<strong class="li ja"> 2018 ECCVW (ECCV研讨会)</strong>中超过<strong class="li ja"> 300次引用</strong>的论文。(<a class="mz na ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----e8533ad006b5--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="0b1c" class="ni nj iq bd lf nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">概述</h1><ol class=""><li id="4b11" class="ml mm iq li b lj nz lm oa lp ob lt oc lx od mb oe mr ms mt bi translated"><strong class="li ja">感性质量和客观质量</strong></li><li id="f3c4" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb oe mr ms mt bi translated"><strong class="li ja">残中残致密块(RRDB) </strong></li><li id="534d" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb oe mr ms mt bi translated"><strong class="li ja">相对论甘</strong></li><li id="fc74" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb oe mr ms mt bi translated"><strong class="li ja">知觉丧失</strong></li><li id="74ff" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb oe mr ms mt bi translated"><strong class="li ja">网络插值</strong></li><li id="fff3" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb oe mr ms mt bi translated"><strong class="li ja">消融研究</strong></li><li id="e3e1" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb oe mr ms mt bi translated"><strong class="li ja"> SOTA比较</strong></li></ol></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="6907" class="ni nj iq bd lf nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">1.感性质量和客观质量</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi of"><img src="../Images/2fe9ec1ce219d1396ec976d9c1e051e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*jvULrnzpgHDLEjzMeGRdng.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">感性质量和客观质量</strong></figcaption></figure><ul class=""><li id="c008" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">RMSE是对客观质量的一种衡量。RMSE越高，客观质量越低。它是以客观的方式来衡量的。</li><li id="14e5" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">感知指数(PI)是感知质量的量度。PI越高，感知质量越低。感知质量是一种非常接近人眼的质量。</li><li id="3e88" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><strong class="li ja"> ESRGAN意在提高感性质量而非客观质量，如PSNR。</strong></li></ul></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="cb87" class="ni nj iq bd lf nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">2.残余致密岩块中的残余(RRDB)</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi og"><img src="../Images/bb33dec3b293fe18192f417f94cff86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4LJaE6zGZarK28A0nNFjqg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf"/><a class="ae ko" href="https://medium.com/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490" rel="noopener"><strong class="bd lf">SRResNet/SRGAN</strong></a>的基本架构</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oh"><img src="../Images/66602eab9c354eef83e85b4f8d318e22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t0xDF0aJO5q6txmZPEZu2A.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf"> RRDB被用作ESRGAN中的基本块</strong></figcaption></figure><ul class=""><li id="db9b" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">首先，<a class="ae ko" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"><strong class="li ja">【BN】</strong></a><strong class="li ja">批量归一化被删除。</strong>移除<a class="ae ko" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN </a>层已被证明在不同的面向PSNR的任务中提高了性能并降低了计算复杂度，包括SR。因为<a class="ae ko" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN </a>会带来伪像。</li><li id="dd69" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">第二，<strong class="li ja">密集块，起源于</strong><a class="ae ko" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="li ja">DenseNet</strong></a><strong class="li ja">，用于替换剩余块</strong>来增强网络。(如果有兴趣，请随意阅读我关于<a class="ae ko" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>的故事。)</li></ul></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="f9ad" class="ni nj iq bd lf nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">3.相对论GAN</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oi"><img src="../Images/b1dd3005b026fa8c688fd0e7417207cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-79tX33DCUKAY-FZz6aiCQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">标准甄别器和相对论甄别器的区别</strong></figcaption></figure><ul class=""><li id="29ea" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated"><a class="ae ko" href="https://medium.com/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490" rel="noopener"> SRGAN </a>中的标准鉴别器<em class="oj"> D </em>，用于估计一幅输入图像<em class="oj"> x </em>真实自然的概率。</li><li id="b254" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">(如果有兴趣，请阅读我关于<a class="ae ko" href="https://medium.com/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75" rel="noopener"> GAN </a>和<a class="ae ko" href="https://medium.com/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490" rel="noopener"> SRGAN </a>的故事。)</li><li id="99d2" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">相比之下，<strong class="li ja">相对论性鉴别器试图预测真实图像XR比假图像<em class="oj"> xf </em>相对更真实的概率。</strong></li><li id="b7b6" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">带有相对论平均甄别器的标准甄别器<em class="oj"> Dra </em>。</li><li id="f0ec" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><strong class="li ja"> <em class="oj"> E </em>是对小批量中所有伪数据取平均值的操作。</strong>鉴频器损耗为:</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ok"><img src="../Images/adea7acf1f071810c82e804c08f2e1e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dRRScNhGCg9mbe6tsvyN-g.png"/></div></div></figure><ul class=""><li id="6e6a" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">发电机的不利损失是对称的:</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ol"><img src="../Images/7a2bae5d92337a92785a4386ca78f856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tpe1GB8WLWDJCXy8dZWVUQ.png"/></div></div></figure><ul class=""><li id="e61b" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">观察到<strong class="li ja">发电机的不利损失包含XR和<em class="oj"> xf </em> </strong>。因此，<strong class="li ja">生成器在对抗训练中受益于来自生成数据和真实数据的梯度</strong>，而在<a class="ae ko" href="https://medium.com/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490" rel="noopener"> SRGAN </a>中，只有生成部分起作用。</li></ul></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="e012" class="ni nj iq bd lf nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">4.知觉丧失</h1><ul class=""><li id="2d11" class="ml mm iq li b lj nz lm oa lp ob lt oc lx od mb mq mr ms mt bi translated"><strong class="li ja">在ESRGAN中，使用激活层之前的特性。</strong></li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi om"><img src="../Images/b663cae6bc03fc6cb5b0681bf1e705af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gtxN_QNZsw6Z6wPE_wn0HQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">激活前后特征图示例</strong></figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi on"><img src="../Images/31bdb7977da1db0346756bb6c9ae04ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*no2qgNjByseQsq3pXVUplQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">激活前后对特征地图的影响示例</strong></figcaption></figure><ul class=""><li id="7538" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">如果使用激活层之后的特征，将会有缺点:</li></ul><ol class=""><li id="5e24" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb oe mr ms mt bi translated">第一，<strong class="li ja">激活的特征非常稀疏</strong>，如上图，特别是在非常深的网络之后。例如，在vgg 19–546层之后，图像“狒狒”的激活神经元的平均百分比仅为11.17%。稀疏激活<strong class="li ja">提供弱监管</strong>并因此导致低性能。</li><li id="bea3" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb oe mr ms mt bi translated">第二，<strong class="li ja">使用激活后的特征也会导致重建亮度</strong>与地面实况图像不一致。</li></ol><ul class=""><li id="a25d" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">因此，总损失为:</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/a4763c51a11dde7aaada013c46b32cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*Gk6shANGnrB8N-kHFsnOpQ.png"/></div></figure><ul class=""><li id="2f77" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">其中<em class="oj"> Lpercep </em>为VGG损失，<em class="oj"> LGRA </em>为对抗性损失，L1为标准L1损失，即作为内容损失。</li><li id="a605" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">(在PIRM-SR挑战中，使用了另一种称为MINC损失的损失。但是我没有详细提到它，因为它也没有在论文中提到。)</li></ul></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="4421" class="ni nj iq bd lf nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">4.网络插值</h1><ul class=""><li id="d678" class="ml mm iq li b lj nz lm oa lp ob lt oc lx od mb mq mr ms mt bi translated">首先，我们训练一个面向PSNR的网络<em class="oj"> GPSNR，</em>，然后我们通过微调获得一个基于GAN的网络<em class="oj"> GGAN </em>。</li><li id="f304" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">对这两个网络的所有相应参数进行插值，得到插值模型<em class="oj"> GINTERP </em>:</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi op"><img src="../Images/67e10da8951d2f6ccee1567a6af8c4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*tvJB2fBy3Wmz7qSwxFjkkg.png"/></div></figure><ul class=""><li id="3849" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">其中<em class="oj"> α </em>在0到1之间。有两个优点:</li><li id="5691" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">首先，插值模型能够产生任何可行的有意义的结果，而不会引入伪像。</li><li id="c8a6" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">第二，我们可以在不重新训练模型的情况下持续平衡感知质量和保真度。</li></ul></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="72f7" class="ni nj iq bd lf nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">5.消融研究</h1><h2 id="f0dc" class="oq nj iq bd lf or os dn nn ot ou dp nr lp ov ow nt lt ox oy nv lx oz pa nx iw bi translated">5.1.数据</h2><ul class=""><li id="e36d" class="ml mm iq li b lj nz lm oa lp ob lt oc lx od mb mq mr ms mt bi translated">训练数据:使用包含800幅图像的DIV2K、由2650幅2K高分辨率图像组成的Flickr2K数据集和OutdoorSceneTraining (OST)数据集。</li><li id="7580" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">模型在RGB通道中训练，训练数据集用随机水平翻转和90度旋转来扩充。</li><li id="4bb4" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">基准数据集:Set5，Set14，BSD100，Urban100和PIRM自验证数据集。</li></ul><h2 id="e02c" class="oq nj iq bd lf or os dn nn ot ou dp nr lp ov ow nt lt ox oy nv lx oz pa nx iw bi translated">5.2.消融实验</h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pb"><img src="../Images/e9f958fe8f24093a4133a1aa1b905cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7WGOHvc0_gJiD73qIJ6gqQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">消融实验</strong></figcaption></figure><ul class=""><li id="69cc" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated"><a class="ae ko" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> <strong class="li ja"> BN </strong> </a> <strong class="li ja">移除(第3列)</strong>:观察到比第2列略有改善。</li><li id="5f2b" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><strong class="li ja">感知损失激活前(第4列)</strong>:与第3列相比，产生了更清晰的边缘和更丰富的纹理。</li><li id="df60" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><strong class="li ja"> RaGAN(第5列)</strong>:与第4列相比，再次学习生成更锐利的边缘和更详细的纹理。</li><li id="7c20" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><strong class="li ja">具有RRDB的更深的网络(第6列)</strong>:提出的RRDB可以进一步改善恢复的纹理，特别是对于像图像6的屋顶这样的规则结构，因为深度模型具有捕捉语义信息的强大表示能力。</li></ul><h2 id="0990" class="oq nj iq bd lf or os dn nn ot ou dp nr lp ov ow nt lt ox oy nv lx oz pa nx iw bi translated">5.3.网络插值</h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pc"><img src="../Images/515cae37660c33ed67b6102ad988ef5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j9Th3FGcS2e-DvCDUpJZHg.png"/></div></div></figure><ul class=""><li id="d2ba" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated"><em class="oj"> α </em>从0到1中选取，间隔为0.2。</li><li id="5a22" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">基于<strong class="li ja">纯GAN </strong>的方法产生<strong class="li ja">锐利的边缘和更丰富的纹理，但带有一些令人不快的伪像</strong>，而面向<strong class="li ja">纯PSNR </strong>的方法输出<strong class="li ja">卡通风格的模糊图像</strong>。</li><li id="8d3a" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><strong class="li ja">通过采用网络插值，在保持纹理的同时减少了不愉快的伪像。</strong></li></ul></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="88b6" class="ni nj iq bd lf nk nl nm nn no np nq nr kf ns kg nt ki nu kj nv kl nw km nx ny bi translated">7.SOTA比较</h1><h2 id="4d39" class="oq nj iq bd lf or os dn nn ot ou dp nr lp ov ow nt lt ox oy nv lx oz pa nx iw bi translated">7.1.基准数据集</h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pd"><img src="../Images/e5d13f4afe20e3f02c26b6fb1063d3fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w_Fm9_Z6ou4W195hmsOiWQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated"><strong class="bd lf">左边的数字:PSNR，右边的数字:感知指数(PI) </strong></figcaption></figure><ul class=""><li id="d7a5" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">面向PSNR的方法，如<a class="ae ko" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c?source=post_page---------------------------" rel="noopener"> SRCNN </a>、<a class="ae ko" href="https://medium.com/@sh.tsang/review-edsr-mdsr-enhanced-deep-residual-networks-for-single-image-super-resolution-super-4364f3b7f86f" rel="noopener"> EDSR </a>和<a class="ae ko" href="https://medium.com/@sh.tsang/review-rcan-deep-residual-channel-attention-networks-super-resolution-fbbf04224c22" rel="noopener"> RCAN </a>，往往会产生模糊的结果</li><li id="c1e9" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">以前基于GAN的方法，即EnhanceNet和<a class="ae ko" href="https://medium.com/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490" rel="noopener"> SRGAN </a>，往往会产生不自然的纹理，并且包含令人不愉快的噪声。</li><li id="193b" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">ESRGAN消除了这些伪像，产生了自然的结果。</li></ul><h2 id="9a32" class="oq nj iq bd lf or os dn nn ot ou dp nr lp ov ow nt lt ox oy nv lx oz pa nx iw bi translated">7.2.PIRM-SR挑战</h2><ul class=""><li id="132a" class="ml mm iq li b lj nz lm oa lp ob lt oc lx od mb mq mr ms mt bi translated">使用具有MINC损失的16个剩余块的ESRGAN。(还有其他细节。)</li><li id="6c4d" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">并且它<strong class="li ja">以最佳感知指数(PI)获得PIRM-SR挑战赛(3区)</strong>第一名。</li></ul></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><blockquote class="pe pf pg"><p id="04f8" class="lg lh oj li b lj lk ka ll lm ln kd lo ph lq lr ls pi lu lv lw pj ly lz ma mb ij bi translated"><em class="iq">在冠状病毒肆虐的日子里，给我一个挑战，这个月再写30个故事..？好吃吗？这是本月的第16个故事。感谢访问我的故事..</em></p></blockquote></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h2 id="3971" class="oq nj iq bd lf or os dn nn ot ou dp nr lp ov ow nt lt ox oy nv lx oz pa nx iw bi translated">参考</h2><p id="a580" class="pw-post-body-paragraph lg lh iq li b lj nz ka ll lm oa kd lo lp pk lr ls lt pl lv lw lx pm lz ma mb ij bi translated">【2018 ECC VW】【ESR gan】<br/><a class="ae ko" href="https://arxiv.org/abs/1809.00219" rel="noopener ugc nofollow" target="_blank">ESR gan:增强型超分辨率生成对抗网络</a></p><h2 id="b8b4" class="oq nj iq bd lf or os dn nn ot ou dp nr lp ov ow nt lt ox oy nv lx oz pa nx iw bi translated">超分辨率</h2><p id="07ac" class="pw-post-body-paragraph lg lh iq li b lj nz ka ll lm oa kd lo lp pk lr ls lt pl lv lw lx pm lz ma mb ij bi translated">)(我)(们)(都)(不)(想)(要)(让)(这)(些)(人)(都)(有)(这)(些)(的)(情)(况)(,)(我)(们)(都)(不)(想)(会)(有)(什)(么)(情)(况)(,)(我)(们)(都)(不)(想)(会)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><h2 id="03f4" class="oq nj iq bd lf or os dn nn ot ou dp nr lp ov ow nt lt ox oy nv lx oz pa nx iw bi translated">生成对抗网络</h2><p id="1f1f" class="pw-post-body-paragraph lg lh iq li b lj nz ka ll lm oa kd lo lp pk lr ls lt pl lv lw lx pm lz ma mb ij bi translated">[ <a class="ae ko" href="https://medium.com/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75" rel="noopener">甘</a>[<a class="ae ko" href="https://medium.com/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41" rel="noopener">CGAN</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-lapgan-laplacian-generative-adversarial-network-gan-e87200bbd827" rel="noopener">拉普甘</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-dcgan-deep-convolutional-generative-adversarial-network-gan-ec390cded63c" rel="noopener">DCGAN</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490" rel="noopener">SRGAN&amp;SRResNet</a>]</p><h2 id="5e38" class="oq nj iq bd lf or os dn nn ot ou dp nr lp ov ow nt lt ox oy nv lx oz pa nx iw bi translated"><a class="ae ko" href="https://medium.com/@sh.tsang/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我之前的其他阅读材料</a></h2></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="8105" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">通过<a class="ae ko" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向AI </a>发布</p></div></div>    
</body>
</html>