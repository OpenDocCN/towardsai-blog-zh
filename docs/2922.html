<html>
<head>
<title>Top 5 Approaches to Named Entity Recognition (NER) in 2022</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2022年命名实体识别的五大方法(NER)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/top-5-approaches-to-named-entity-recognition-ner-in-2022-38afdf022bf1?source=collection_archive---------0-----------------------#2022-07-09">https://pub.towardsai.net/top-5-approaches-to-named-entity-recognition-ner-in-2022-38afdf022bf1?source=collection_archive---------0-----------------------#2022-07-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="46e3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对于NER，前5名的方法，前7名的python库，和10个最常见的功能。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4c737a24d75094942ec588a7c2e67ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X3qyMc6LBq6Ufx6nSNidBA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Unsplash的Bradley Pisney</figcaption></figure><p id="b734" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">命名实体识别(NER)是一个识别文本中的命名实体并将它们分类到预先构建的类别中的过程(这些类别根据其详细程度因型号而异)，例如人名、位置、组织、时间表达式、数量、货币值和百分比。<strong class="lb iu"> NER是自然语言处理(NLP)的一个领域，专注于识别和分类文本中的专有名词。</strong></p><p id="266e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在高层次上，NER是对文本中的命名实体进行识别和分类的过程；这个过程可以通过几种方式实现，每种方式都有其优点和缺点。首先，可以通过查找特定的关键字或关键短语来识别命名实体。这种方法实现起来很简单，但是如果文本包含许多不知名的命名实体，其准确性就会受到限制。另一种方法是使用监督机器学习算法，该算法可以从标记数据的训练集中学习识别命名实体。在这种情况下，需要花费更多的精力来设置，但这比单独使用关键字更准确。最后，另一种实现方法是使用基于规则的系统，该系统基于系统开发者定义的一组规则来识别命名实体。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/66c5859353d9534135bc28eedcd48657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7gVGbMjT87jCGqFuTAbZmA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由来自Pexels的<a class="ae ky" href="https://www.pexels.com/@karolina-grabowska/" rel="noopener ugc nofollow" target="_blank">卡罗琳娜·格拉博斯卡</a></figcaption></figure><h1 id="e487" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">以下是10种最常见的NER功能，并简要说明了它们的工作原理:</strong></h1><p id="f5e8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">1.标记化:这是将文本分解成单个标记(单词、短语等)的过程。).这是大多数NLP任务的必要步骤，包括NER。</p><p id="dc17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.词性标注:这涉及到给文本中的每个单词分配一个词性标签。这对于NER是有帮助的，因为某些专有名词更有可能作为某些词类(例如，名词)出现。</p><p id="e695" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.词形分析:用于识别单词的词根形式(其词条)——许多专有名词都是从其他单词中派生出来的(例如，“John”来自“Johns”)。</p><p id="670d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.句法分析:这是分析句子句法结构的过程；例如，许多专有名词在它们的句法短语中是中心词(例如，“约翰是医生”中的“约翰”)。</p><p id="56aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.语义分析:分析文本的意义；例如，许多专有名称与特定的概念或实体相关联(例如，“John”作为人名)。</p><p id="d00f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">6.语篇分析:这是分析一篇文章是如何组织的过程——许多专有名词被用来指语篇中的特定实体(例如，“约翰”是故事中主角的名字)。</p><p id="0a3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">7.语用分析分析一篇文章如何在语境中使用。这对NER很有帮助，因为在使用的上下文中，许多专有名词被用来指代特定的实体(例如，“约翰”是在对话中被称呼的人的名字)。</p><p id="7910" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">8.世界知识使用关于世界的背景知识来帮助解释文本。这里的分析是有多少专有名词与可以通过世界知识识别的特定实体相关联(例如，“约翰”作为美国总统的名字)。</p><p id="9fcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">9.领域知识:如何使用特定领域的背景知识来帮助解释文本—许多专有名称都与不同的实体相关联，这些实体可以通过领域知识来识别(例如，“John”是电影中某个角色的名称)。</p><p id="aa8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">10.统计方法(下一节还会有更多的介绍):使用统计方法识别数据模式的过程；例如，许多专有名称与可以通过统计分析识别的特定模式相关联(例如，“John”是更可能是男性的人的名字)。</p><p id="c769" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NER有几种方法，包括基于规则的系统、统计模型和神经网络。每种方法都有其优点和缺点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/7a87b7f15b225800ac25e338d36f8168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sm6EaQvsXx0Ed4cSEtBjDg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的Karolina Grabowska</figcaption></figure><h1 id="c43c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">前往NER的5条途径:</strong></h1><p id="6b1a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">1.基于规则的系统通常基于由领域专家编写的手工规则。这些规则可以基于文本模式、词汇信息或句法结构。虽然规则在某些领域可能非常有效，但它们的开发和维护可能具有挑战性，并且它们通常不能很好地推广到新的领域或语言。</p><p id="3d0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.统计模型基于这样一种想法，即命名实体可以通过它们所保持的公司(即，往往出现在它们周围的单词)来与文本中的其他单词区分开来。已经为NER开发了几种统计模型，包括隐马尔可夫模型(HMMs)、最大熵(Maxent)模型和支持向量机(SVMs)。这些模型可能是有效的，但它们通常需要大量的训练数据，这可能具有挑战性。</p><p id="4d73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.神经网络是可用于NER的机器学习算法的子集。神经网络非常适合这种类型的分析，因为它们可以学习数据中的复杂模式。然而，神经网络需要大量的训练数据来学习这些模式。</p><p id="ab35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.混合系统:这些系统结合了多种NER方法。例如，系统可以使用基于规则的方法来识别人名，使用统计方法来识别组织。混合系统可能是有效的，因为它们可以利用每种方法的优势。</p><p id="a824" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.语义角色标注:语义角色标注(SRL)是一项相关的任务，旨在识别每个单词在句子中的角色。这通常通过分析句子的句法结构和句子中实体的上下文来完成。例如，在句子“蒂姆把工具给了朱莉”中，蒂姆是“给予者”，工具是“对象”，朱莉是“接受者”通过为命名实体提供额外的上下文，SRL可用于改进NER。</p><p id="ba22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，实体链接是将文本中的命名实体与其在数据库或本体中的对应条目相连接的过程。这通常通过将文本中实体的上下文与已知实体的数据库进行比较来完成。作为实体链接的一部分，实体解析是一个必须了解的过程(当多个实体具有相同的名称时，如何确定一个实体的正确身份)。通过将文本中的实体的上下文与已知实体的数据库进行比较来完成由于多个实体而导致的这种级别的确定。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/2cad462efabe02f0c26fa07b1347a4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KLYvgA2ohOc1aH22jdC-sg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的Karolina Grabowska</figcaption></figure><h1 id="610d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">以下是使用Python实现的几个NER库:</h1><p id="9a24" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">斯坦福大学核心实验室:standfordnlp.github.io/CoreNLP/ner.html</p><p id="58cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">空间:<a class="ae ky" href="https://spacy.io/usage/linguistic-features" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage/linguistic-features</a></p><p id="1455" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NLTK:【https://www.nltk.org/book/ch07.html T2】</p><p id="7007" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">https://textblob.readthedocs.io/en/dev/<a class="ae ky" href="https://textblob.readthedocs.io/en/dev/" rel="noopener ugc nofollow" target="_blank"/></p><p id="5054" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Polyglot:<a class="ae ky" href="http://polyglot.readthedocs.io/en/latest/NamedEntityRecognition.html" rel="noopener ugc nofollow" target="_blank">http://Polyglot . readthe docs . io/en/latest/namedentityrecognition . html</a></p><p id="552c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Apache OpenNLP:<a class="ae ky" href="https://opennlp.apache.org/" rel="noopener ugc nofollow" target="_blank">https://opennlp.apache.org/</a></p><p id="e196" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经ref:<a class="ae ky" href="https://huggingface.co/coref" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/coref</a></p><p id="7f23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还添加了Gensim，这是一个用于Python的主题建模库，实现了流行的技术，如潜在的Dirichlet分配(LDA)和Word2Vec。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="5b92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有任何编辑/修改建议或关于进一步扩展此主题的建议，请考虑与我分享您的想法。</p><h1 id="3592" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">另外，请考虑订阅我的每周简讯:</strong></h1><div class="na nb gp gr nc nd"><a href="https://pventures.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">周日报告#1</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">设计思维与AI的共生关系设计思维能向AI揭示什么，AI又能如何拥抱…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">pventures.substack.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr ks nd"/></div></div></a></div><p id="fdf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参考资料:</p><p id="c69c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.纳多，大卫，和关，聪。命名实体识别和分类综述。Lingvisticae Investigiones 30(1)，2007年。</p><p id="79e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.拉蒂诺夫，列夫和罗斯，丹。命名实体识别中的设计挑战和误解。2011年社交媒体语言研讨会会议录。</p><p id="9ad6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.曼宁，克里斯托弗d，苏尔迪努，米哈伊，鲍尔，约翰，芬克尔，珍妮，贝瑟德，史蒂文j。和麦克罗斯基，大卫。斯坦福CoreNLP自然语言处理工具包。计算机械协会通讯56(2)，2013。</p><p id="0901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.朱拉夫斯基，丹，马丁，詹姆斯h。语音和语言处理。培生教育，2014年。</p><p id="6917" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.什么是命名实体识别(NER)？—来自WhatIs.com的定义。<a class="ae ky" href="https://www.techtarget.com/whatis/definition/named-entity-recognition-NER" rel="noopener ugc nofollow" target="_blank">https://www . techtarget . com/what is/definition/named-entity-recognition-NER</a></p><p id="fdf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">6.事件提取的神经方法:【https://tel.archives-ouvertes.fr/tel-01943841/document T2】</p><p id="841f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">7.事件提取的简要综述:方法和应用。【https://arxiv.org/pdf/2107.02126.pdf T4】</p></div></div>    
</body>
</html>