# 支持在金融服务中负责任地使用人工智能

> 原文：<https://pub.towardsai.net/supporting-responsible-use-of-ai-in-financial-services-63a65f5d50d5?source=collection_archive---------6----------------------->

![](img/ff2d63a02cbf3a88827b3bdbefdb1ded.png)

(艾蒂安·马丁在 [Unsplash](https://unsplash.com/s/photos/bank?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片)

## [人工智能](https://towardsai.net/p/category/artificial-intelligence)

美联储州长莱尔·布雷纳德最近在人工智能研讨会上谈到了负责任的人工智能在金融服务中的应用。这篇演讲提供了重要的见解，可以作为美联储指南可能如何寻找人工智能治理的早期指标。已经在利用人工智能提供新的或增强的客户体验的金融服务公司可以查看这些评论，以抢占先机，并确保他们的人工智能运营得到更好的准备。演讲的完整文本可以在[这里](https://www.federalreserve.gov/newsevents/speech/brainard20210112a.htm)找到。这篇文章总结了这次演讲的要点，以及团队应该如何思考它对他们的 ML 实践(即 MLOps)的适用性。

## 人工智能给金融服务带来的好处

美联储广泛接受人工智能在打击欺诈和提高信贷可用性方面的优势。人工智能使公司能够更快、更好地应对欺诈，随着金融服务数字化程度的提高，欺诈正在升级。用传统和替代数据构建的信用风险和信用决策的机器学习(ML)模型为当前信用框架之外的更多人提供了更准确和公平的信用决策([美联储联合声明](https://blog.fiddler.ai/2019/12/fed-opens-up-alternative-data-more-credit-more-algorithms-more-regulation/)开放替代数据)。然而，美联储警告说，如果在没有适当护栏和保护的不透明人工智能模型中使用，带有种族偏见的历史数据可能会使偏见永久化。人工智能系统需要产生积极的影响，并保护以前被边缘化的阶层。

## 艾的黑箱问题

关键问题是 ML 模型缺乏透明度，美联储概述了其背后的原因:

1.  与人类设计的统计模型不同，ML 模型是通过算法自动对数据进行训练的。
2.  作为这种自动生成的结果，ML 模型可以从人类无法识别的数据中吸收复杂的非线性相互作用。

这种复杂性掩盖了模型如何将输入转换为输出，对于深度学习模型来说，这种复杂性变得更糟，很难解释和推理。

## 语境的重要性

美联储概述了背景对理解和解释模型的重要性。尽管人工智能研究社区在解释模型方面取得了进展，但解释取决于询问它们的人和模型预测的类型。例如，给技术模型开发人员的解释要比给合规官的解释详细得多。此外，最终用户需要收到一个易于理解和可操作的模型解释。例如，如果一个贷款申请人被拒绝了，了解这个决定是如何作出的，并建议采取行动来增加他们获得批准的机会，这将使他们能够做出改变并重新申请。

对于采用人工智能的金融服务团队来说，这突出了需要有一个 ML 系统来迎合人工智能的所有利益相关者，而不仅仅是模型开发人员。它需要解决这些涉众对 ML 理解的不同程度，并允许模型解释正确地呈现给最终用户。

主要的银行使用案例，尤其是信用贷款，受许多法律的监管，包括平等信用机会法(ECOA)、公平住房法(FHA)、民权法、移民改革法等。法律要求人工智能模型和驱动它们的数据得到理解和评估，以解决任何不必要的偏见。即使在模型开发中没有使用像种族这样的受保护属性，模型也可以不知不觉地从相关数据输入中吸收与受保护类的关系，即代理偏差。因此，在这些严格的限制条件下开发模型以促进金融普惠的公平结果是一个积极的研究课题。

金融服务已经很好地建立了评估偏见的统计模型。为了满足 ML 模型的相同需求，人工智能团队需要一个更新的偏差测试过程，使用工具来评估和减轻用例环境中的人工智能偏差。

银行管理层在做出关键决策时，需要相信他们的模型是稳健的。他们需要确保模型在面对可能具有更复杂交互的真实世界数据时能够正确运行。解释是为模型开发和评估团队提供这种信心的关键工具。然而，并不是所有的 ML 系统都需要相同水平的理解。例如，对于与主 AI 系统结合使用的次级挑战者系统，较低的透明度阈值就足够了。

随着团队扩展他们的 ML 开发，该过程将需要提供一个强大的验证和监控工具集合，以允许模型开发人员和 IT 部门确保符合监管和风险要求，如[SR 11–7](https://www.federalreserve.gov/boarddocs/srletters/2011/sr1107a1.pdf)和 [OCC 公告 2011–12](https://www.occ.gov/news-issuances/bulletins/2011/bulletin-2011-12a.pdf)。银行[已经开始在他们的第二道防线中引入人工智能验证器](https://blog.fiddler.ai/2020/11/rise-of-banking-ai-validator/)来实现模型验证。

## 解释的形式

演讲概述了解释如何根据模型的复杂性和结构而有所不同。建议银行考虑在基于用例的模型中使用适当的透明度。例如，有些模型可以被开发成完全“可解释”的，但可能不够精确。例如，逻辑回归模型决策可以用输入的权重来解释。其他模型更加复杂和准确，但不具备内在的可解释性。在这种情况下，通过使用模型不可知技术来获得解释，该技术通过用变化的输入探测模型并观察其输出的变化来提供解释。虽然这些“事后”解释可以在某些用例中实现理解，但它们可能不像来自内在可解释模型的解释那样可靠。因此，银行将面临的一个关键问题是，模型不可知论的解释是否可以接受，或者一个可解释的模型是否必要。然而，一个准确的模型解释并不能保证一个健壮和公平的模型，这个模型只能随着时间的推移和经验的积累而发展。

可解释的人工智能是最近的一项研究进展，是一种打开人工智能黑匣子的技术，因此人类可以了解人工智能模型内部发生了什么，以确保人工智能驱动的决策是透明、负责和可信的。这种可解释性为模型输出的解释提供了动力。金融服务公司需要有适当的平台，以允许他们的团队为各种各样的模型生成解释，这些模型可以在各种内部和外部团队中使用。

## 对银行的期望

美联储的讲话以承诺支持负责任的人工智能的发展为结尾，并呼吁该领域的专家就透明度技术及其风险影响提供反馈。

随着美联储寻求投入，很明显，部署人工智能模型的金融服务团队需要探索用更新的流程和工具来支持他们的 ML 开发的方法，以实现模型理解、稳健性和公平性的透明度，以便他们为即将到来的指导方针做好更好的准备。