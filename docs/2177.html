<html>
<head>
<title>Feature Selection and Removing in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的特征选择和去除</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/feature-selection-and-removing-in-machine-learning-dd3726f5865c?source=collection_archive---------1-----------------------#2021-09-14">https://pub.towardsai.net/feature-selection-and-removing-in-machine-learning-dd3726f5865c?source=collection_archive---------1-----------------------#2021-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ca6a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/predictive-analytics" rel="noopener ugc nofollow" target="_blank">预测分析</a></h2><div class=""/><div class=""><h2 id="09fe" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">高维数据模型及其精度的改进</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/eebfb6a3c71e6f02adf2ddac96c25cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iZJW9TvF7_y1DeVs"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@franki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗兰基·查马基</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="fd47" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">众所周知，特征在机器学习算法中的重要性，在任何领域的预测分析中都起着非常关键的作用。</p><p id="ddba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当数据要素变得非常复杂时，出现多重共线性情况或两个或更多要素之间高度相关的可能性非常高。这种情况严重影响了数据的训练，可能会使数据过拟合或欠拟合。</p><p id="9e03" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有一些选择和移除特征的方法，如下所示:</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="1dd1" class="mj mk it mf b gy ml mm l mn mo"><strong class="mf jd">Feature Selection Methods</strong><br/>1. Uni-variate Selection<br/>2. Selecting from Model</span><span id="365d" class="mj mk it mf b gy mp mm l mn mo"><strong class="mf jd">Feature removing Methods</strong><br/>1. Low variance method<br/>2. Recursive method</span></pre><blockquote class="mq mr ms"><p id="e0f4" class="li lj mt lk b ll lm kd ln lo lp kg lq mu ls lt lu mv lw lx ly mw ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">单变量选择</em> </strong></p></blockquote><p id="55bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">单变量方法是一组检查特征关系强度的方法。我们可以用任何方法来理解单个特征到目标特征。</p><p id="de6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">单变量特征选择方法</p><ul class=""><li id="6168" class="mx my it lk b ll lm lo lp lr mz lv na lz nb md nc nd ne nf bi translated">选择K最佳</li><li id="da7c" class="mx my it lk b ll ng lo nh lr ni lv nj lz nk md nc nd ne nf bi translated">选择百分点</li><li id="3a78" class="mx my it lk b ll ng lo nh lr ni lv nj lz nk md nc nd ne nf bi translated">通用单变量选择</li><li id="f40a" class="mx my it lk b ll ng lo nh lr ni lv nj lz nk md nc nd ne nf bi translated">皮尔逊相关</li></ul><h2 id="db6f" class="mj mk it bd nl nm nn dn no np nq dp nr lr ns nt nu lv nv nw nx lz ny nz oa iz bi translated">k最佳方法</h2><p id="ce88" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">在这种方法中，我们基于特征的最高分数和p值来选择要选择的特征的数量。</p><p id="8a81" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用python的此方法的示例如下所示:</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="a6d9" class="mj mk it mf b gy ml mm l mn mo">from sklearn.datasets import load_iris<br/>from sklearn.feature_selection import SelectKBest<br/>from sklearn.feature_selection import chi2</span><span id="ceff" class="mj mk it mf b gy mp mm l mn mo">feature, target= load_iris(return_X_y=True)<br/>feature.shape</span><span id="5de6" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>(150, 4)</span><span id="43d5" class="mj mk it mf b gy mp mm l mn mo">feature_new = SelectKBest(chi2, k=2).fit_transform(feature, target)<br/>feature_new.shape</span><span id="f823" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>(150, 2)</span></pre><p id="48c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们在这里注意到的一件事是，我们使用chi2特性，因为它用于查找一个特性如何依赖于目标特性的统计数据。</p><h2 id="6495" class="mj mk it bd nl nm nn dn no np nq dp nr lr ns nt nu lv nv nw nx lz ny nz oa iz bi translated">选择百分点</h2><p id="2d5f" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">在这种方法中，我们基于特征的最高百分位分数来选择特征的数量。</p><p id="2468" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用python的此方法的示例如下所示:</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="5b2f" class="mj mk it mf b gy ml mm l mn mo">from sklearn.datasets import load_iris<br/>from sklearn.feature_selection import SelectPercentile<br/>from sklearn.feature_selection import chi2</span><span id="84e6" class="mj mk it mf b gy mp mm l mn mo">feature, target= load_iris(return_X_y=True)<br/>feature.shape</span><span id="bf88" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>(1797, 64)</span><span id="eb4a" class="mj mk it mf b gy mp mm l mn mo">feature_new = SelectPercentile(chi2,<br/>                       percentile=10).fit_transform(feature, target)<br/>feature_new.shape</span><span id="a199" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>(1797, 7)</span></pre><p id="09f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以选择百分位数，如上面的代码所示，我们还可以看到，从64个特性中选择的特性数量现在只有7个。</p><h2 id="c0ba" class="mj mk it bd nl nm nn dn no np nq dp nr lr ns nt nu lv nv nw nx lz ny nz oa iz bi translated">通用单变量选择</h2><p id="7e9c" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">在这种方法中，我们选择模式的类型来选择特征的数量。</p><p id="4f98" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用python的此方法的示例如下所示:</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="d42b" class="mj mk it mf b gy ml mm l mn mo">from sklearn.datasets import load_breast_cancer<br/>from sklearn.feature_selection import GenericUnivariateSelect<br/>from sklearn.feature_selection import chi2</span><span id="d5a0" class="mj mk it mf b gy mp mm l mn mo">feature, target= load_iris(return_X_y=True)<br/>feature.shape</span><span id="8fed" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>(569, 30)</span><span id="64e5" class="mj mk it mf b gy mp mm l mn mo">transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)</span><span id="2c5a" class="mj mk it mf b gy mp mm l mn mo">feature_new = transformer.fit_transform(feature, target)<br/>feature_new.shape</span><span id="bab8" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>(569, 20)</span></pre><h2 id="faa5" class="mj mk it bd nl nm nn dn no np nq dp nr lr ns nt nu lv nv nw nx lz ny nz oa iz bi translated">皮尔逊相关</h2><p id="8c7d" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">这种方法很容易知道输入特征和目标特征之间的特征相关性。我们得到的分数在[-1到1]的范围内，即负相关到强正相关。</p><p id="863b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用python的此方法的示例如下所示:</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="a462" class="mj mk it mf b gy ml mm l mn mo">import<!-- --> <!-- -->numpy as np<br/>from<!-- --> <!-- -->scipy.stats import<!-- --> <!-- -->pearsonr</span><span id="43c5" class="mj mk it mf b gy mp mm l mn mo">np.random.seed(0)</span><span id="0088" class="mj mk it mf b gy mp mm l mn mo">count =<!-- --> <!-- -->300<br/>x =<!-- --> <!-- -->np.random.normal(0, 1, count)</span><span id="a8d0" class="mj mk it mf b gy mp mm l mn mo">print("Less random data",pearsonr(x, x +<!-- --> <!-- -->np.random.normal(0, 1,<br/>                         size)))</span><span id="9498" class="mj mk it mf b gy mp mm l mn mo">print("More random data", pearsonr(x, x +<!-- --> <!-- -->np.random.normal(0, 10,<br/>                         size)))</span><span id="a167" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>Less random data (0.618, 4.743e-49)<br/>More random data (0.078, 0.23)</span></pre><p id="3869" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种方法的一个主要缺点是它对特征的线性关系起作用。</p><div class="og oh gp gr oi oj"><a rel="noopener  ugc nofollow" target="_blank" href="/python-examples-to-make-algorithm-more-robust-with-exception-handling-6bff7a127786"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jd gy z fp oo fr fs op fu fw jc bi translated">Python示例通过异常处理使算法更加健壮</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">程序正常流程中的错误中断</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">pub.towardsai.net</p></div></div><div class="os l"><div class="ot l ou ov ow os ox lb oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://medium.com/pythoneers/forget-html-and-flask-start-using-streamlit-1b394cfe4595" rel="noopener follow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jd gy z fp oo fr fs op fu fw jc bi translated">忘记HTML和Flask，开始使用Streamlit</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">数据科学和机器学习的WebApp框架</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">medium.com</p></div></div><div class="os l"><div class="oy l ou ov ow os ox lb oj"/></div></div></a></div><blockquote class="mq mr ms"><p id="b309" class="li lj mt lk b ll lm kd ln lo lp kg lq mu ls lt lu mv lw lx ly mw ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">从</em> </strong>型号中选择</p></blockquote><p id="c0e1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该方法用于在拟合模型后发现重要特征。它就像是模型的一个附件，根据某个阈值来选择重要的特性。</p><p id="b943" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用python的此方法的示例如下所示:</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="8838" class="mj mk it mf b gy ml mm l mn mo">from sklearn.feature_selection import SelectFromModel<br/>from sklearn.linear_model import LogisticRegression</span><span id="3771" class="mj mk it mf b gy mp mm l mn mo">X = [[ 0.27, -2.34,  0.31 ],<br/>    [-2.79, -0.09, -0.85 ],<br/>    [-0.34, 1.34, -2.55 ],<br/>    [ 1.77,  1.28,  0.54 ]]</span><span id="3d15" class="mj mk it mf b gy mp mm l mn mo">y = [1, 0, 1, 0]</span><span id="2d99" class="mj mk it mf b gy mp mm l mn mo">selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)<br/>selector.estimator_.coef_</span><span id="65ef" class="mj mk it mf b gy mp mm l mn mo">output:<br/>array([[ 0.35531238, -0.56881882, -0.70144451]])</span><span id="1e44" class="mj mk it mf b gy mp mm l mn mo">-------------------------------------------------------------</span><span id="07ef" class="mj mk it mf b gy mp mm l mn mo">selector.threshold_</span><span id="11af" class="mj mk it mf b gy mp mm l mn mo">output:<br/>0.5524527319086916</span><span id="fe80" class="mj mk it mf b gy mp mm l mn mo">-------------------------------------------------------------</span><span id="2ef6" class="mj mk it mf b gy mp mm l mn mo">selector.get_support()</span><span id="d02c" class="mj mk it mf b gy mp mm l mn mo">output:<br/>array([False,  True, False])</span></pre><blockquote class="mq mr ms"><p id="4573" class="li lj mt lk b ll lm kd ln lo lp kg lq mu ls lt lu mv lw lx ly mw ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">低方差法</em> </strong></p></blockquote><p id="7c8f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该方法基于对特征的方差方法，以满足给定的某个阈值，从而找到数据中的重要重要性。</p><p id="d5cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">阈值的一般公式如下所示:</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="2d8a" class="mj mk it mf b gy ml mm l mn mo">.8 * (1 - .8)</span></pre><p id="b602" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用python的此方法的示例如下所示:</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="2d5a" class="mj mk it mf b gy ml mm l mn mo">from sklearn.feature_selection import VarianceThreshold</span><span id="764d" class="mj mk it mf b gy mp mm l mn mo">X = [[1,0,1], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 0], [0, 1, 1]]</span><span id="3c8d" class="mj mk it mf b gy mp mm l mn mo">sel = VarianceThreshold(threshold=(.5 * (1 - .5)))<br/>sel.fit_transform(X)</span><span id="21a0" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>No feature in X meets the variance threshold 0.25000</span><span id="5e32" class="mj mk it mf b gy mp mm l mn mo">-------------------------------------------------------------</span><span id="6da2" class="mj mk it mf b gy mp mm l mn mo">sel = VarianceThreshold(threshold=(.6 * (1 - .6)))<br/>sel.fit_transform(X)</span><span id="19ad" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>array([[1],<br/>       [0],<br/>       [1],<br/>       [0],<br/>       [0],<br/>       [1]])</span></pre><p id="c92a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，如果我们给定一个低阈值，那么这些特征就不会被选择。但是如果我们给某个点一个阈值，那么它会从数据中删除一些列。</p><blockquote class="mq mr ms"><p id="5224" class="li lj mt lk b ll lm kd ln lo lp kg lq mu ls lt lu mv lw lx ly mw ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">递归法</em> </strong></p></blockquote><p id="ddc2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这种方法中，特征的排序是基于权重的。该方法将特征分成更小的特征集，然后从更小的特征集到更大的特征集进行训练。</p><p id="87ae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用python的此方法的示例如下所示:</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="cbd6" class="mj mk it mf b gy ml mm l mn mo">from sklearn.datasets import make_friedman1<br/>from sklearn.feature_selection import RFE<br/>from sklearn.svm import SVR</span><span id="ffb1" class="mj mk it mf b gy mp mm l mn mo">feature, target = make_friedman1(n_samples=30, n_features=8, <br/>                                                 random_state=0)</span><span id="779c" class="mj mk it mf b gy mp mm l mn mo">estimator = SVR(kernel="linear")<br/>selector = RFE(estimator, n_features_to_select=5, step=1)<br/>selector = selector.fit(feature, target)</span><span id="52c9" class="mj mk it mf b gy mp mm l mn mo">selector.ranking_</span><span id="af8c" class="mj mk it mf b gy mp mm l mn mo">#output:<br/>array([1, 2, 3, 1, 1, 1, 1, 4])</span></pre><p id="f76f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从上面的例子中，我们可以看到在n_features参数中给出的8个特性上有一个排名。</p><blockquote class="mq mr ms"><p id="7ede" class="li lj mt lk b ll lm kd ln lo lp kg lq mu ls lt lu mv lw lx ly mw ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">结论</em> </strong></p></blockquote><p id="7041" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文讨论了机器学习中的一些特征选择方法。特性选择是每个项目中的一项重要任务。</p><p id="5d3c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae lh" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="5fd5" class="oz mk it bd nl pa pb pc no pd pe pf nr ki pg kj nu kl ph km nx ko pi kp oa pj bi translated">推荐文章</h1><p id="be4f" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">1.<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/8-active-learning-insights-of-python-collection-module-6c9e0cc16f6b?source=friends_link&amp;sk=4a5c9f9ad552005636ae720a658281b1">8 Python的主动学习见解收集模块</a> <br/> 2。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/numpy-linear-algebra-on-images-ed3180978cdb?source=friends_link&amp;sk=d9afa4a1206971f9b1f64862f6291ac0"> NumPy:图像上的线性代数</a>T5】3。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/pandas-dealing-with-categorical-data-7547305582ff?source=friends_link&amp;sk=11c6809f6623dd4f6dd74d43727297cf">熊猫:处理分类数据</a> <br/> 5。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/hyper-parameters-randomseachcv-and-gridsearchcv-in-machine-learning-b7d091cf56f4?source=friends_link&amp;sk=cab337083fb09601114a6e466ec59689">超参数:机器学习中的RandomSeachCV和GridSearchCV</a><br/>6。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/data-distribution-using-numpy-with-python-3b64aae6f9d6?source=friends_link&amp;sk=809e75802cbd25ddceb5f0f6496c9803">数据分发使用Numpy与Python </a> <br/> 9。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/decision-trees-vs-random-forests-in-machine-learning-be56c093b0f?source=friends_link&amp;sk=91377248a43b62fe7aeb89a69e590860">机器学习中的决策树vs随机森林</a> <br/> 10。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/standardization-in-data-preprocessing-with-python-96ae89d2f658?source=friends_link&amp;sk=f348435582e8fbb47407e9b359787e41">用Python实现数据预处理的标准化</a></p></div></div>    
</body>
</html>