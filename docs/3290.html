<html>
<head>
<title>Deal With an Imbalanced Dataset With TensorFlow, LightGBM, and CatBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow、LightGBM和CatBoost处理不平衡数据集</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/deal-with-an-imbalanced-dataset-with-tensorflow-lightgbm-and-catboost-b2476996d145?source=collection_archive---------1-----------------------#2022-11-08">https://pub.towardsai.net/deal-with-an-imbalanced-dataset-with-tensorflow-lightgbm-and-catboost-b2476996d145?source=collection_archive---------1-----------------------#2022-11-08</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="7bf6" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">定制模型时，向工具箱中添加新仪器</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj kj"><img src="../Images/b58cd4dd556a8f71bfd8581ea4e7a53a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*Le6NX_okaJ-eKT6_8buAYg.jpeg"/></div><figcaption class="kr ks gk gi gj kt ku bd b be z dk translated">来源:照片由flickr.com<a class="ae kv" href="http://flickr.com" rel="noopener ugc nofollow" target="_blank">的</a><a class="ae kv" href="https://www.flickr.com/photos/x1brett/" rel="noopener ugc nofollow" target="_blank">布雷特·乔丹</a>拍摄</figcaption></figure><p id="cd8c" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">您的数据集不平衡；您希望减少假阴性(FN)甚至假阳性(FP)的数量。也许你喜欢定制的东西，想练习给标准模型增加变化。如果是这样，这篇文章是给你的。</p><p id="424e" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">一种方法是用特定的系数定制模型的损失函数。本文旨在展示TensorFlow、LightGBM和Catboost中的定制方法。如果你想用相关的数学对整体概念有个大概的感觉，对XGBoost也看到同样的概念，看看我在Medium上的<a class="ae kv" rel="noopener ugc nofollow" target="_blank" href="/outline-a-smaller-class-with-the-custom-loss-function-94ff00359698"> <strong class="ky iv">文章</strong> </a> <strong class="ky iv"> </strong>。</p><p id="9df3" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">此外，我的目标是提供一种将自定义超参数嵌入到自定义函数中的方法，这为像普通参数一样对新参数进行高级调优打开了大门。</p><p id="0899" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">我使用泰坦尼克号数据集进行演示，因为它是可接近的和不平衡的。基本款，还有定制款，都在我的<a class="ae kv" href="https://github.com/kpluzhnikov/binary_classification_custom_loss" rel="noopener ugc nofollow" target="_blank"> <strong class="ky iv"> GitHub库</strong> </a> <strong class="ky iv">里。</strong></p></div><div class="ab cl ls lt hy lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="in io ip iq ir"><h2 id="afc3" class="lz ma iu bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated"><strong class="ak"> LightGBM </strong></h2><p id="a3ed" class="pw-post-body-paragraph kw kx iu ky b kz ms jv lb lc mt jy le lf mu lh li lj mv ll lm ln mw lp lq lr in bi translated">这是微软开发的最有效的梯度推进算法之一。它在速度上超过XGBoost，在精度上不相上下。更多详情，请查看BexBoost的<a class="ae kv" href="https://medium.com/towards-data-science/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997" rel="noopener"> <strong class="ky iv">这篇文章</strong> </a> <strong class="ky iv">。</strong> LightGBM是XGBoost的弟弟，所以有它所有的成就。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mx my l"/></div></figure><p id="2e92" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">我已经使用嵌入式用户定义函数引入了<code class="fe mz na nb nc b">beta</code>作为logloss函数的核心部分(它不再是一个外部超参数)。</p><p id="47b8" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">您可以看到外部函数将<code class="fe mz na nb nc b">beta</code>呈现给内部函数，内部函数计算导数。这同样适用于自定义指标。现在，您可以使用其他带有特殊包(如Optuna库)的超参数对其进行调优。</p><blockquote class="nd ne nf"><p id="6653" class="kw kx ng ky b kz la jv lb lc ld jy le nh lg lh li ni lk ll lm nj lo lp lq lr in bi translated"><code class="fe mz na nb nc b">beta</code>应该是&lt; 1.0来惩罚FN。要惩罚FP，应该是1.0以上。详情请看我的<a class="ae kv" rel="noopener ugc nofollow" target="_blank" href="/outline-a-smaller-class-with-the-custom-loss-function-94ff00359698"> <strong class="ky iv">文章</strong> </a> <strong class="ky iv"> </strong>上媒。</p></blockquote><p id="f48b" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">与XGBoost自定义损失函数相比有一些不同。首先，LightGBM将<code class="fe mz na nb nc b">y_pred</code>放入logit_raw格式，需要进行logit转换。其次，LightGBM定制度量输出三个结果(定制度量的名称(例如，“logreg_error”)、度量的值以及应该设置的布尔参数<code class="fe mz na nb nc b">False</code>，因为我们的目标是减少定制度量值)。</p><p id="e0b8" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">在<code class="fe mz na nb nc b">predt</code>的一个logit转换中还有一个更有趣的细节；在处理负数logit_raw时，我使用了<code class="fe mz na nb nc b">np.where</code>函数来保证稳定性和避免溢出。在Stackoverflow和模型文档的不同例子中，它被作为最佳实践提到。</p><p id="6527" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">让我们绘制一个标准LightGBM模型和一个有定制损耗的模型的结果的混淆矩阵:</p><div class="kk kl km kn gu ab cb"><figure class="nk ko nl nm nn no np paragraph-image"><img src="../Images/316d10ebc18fb27bc6b49f58d2a683ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*zOcukPAAQYRiXSVj2XRPdA.png"/></figure><figure class="nk ko nq nm nn no np paragraph-image"><img src="../Images/5ec0822bc5c0bde3964541e36171f7e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*BB7FIagvdiQzAAGXp_I_HA.png"/><figcaption class="kr ks gk gi gj kt ku bd b be z dk nr di ns nt translated">(<strong class="bd mb">左</strong>)基本LightGBM模型| ( <strong class="bd mb">右</strong>)beta = 0.4的定制LightGBM模型，来源:图片由作者提供</figcaption></figure></div><p id="c864" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">与<code class="fe mz na nb nc b">beta</code>1的自定义丢失导致了FPs和TPs的增长；到FN和TN的耗尽。</p></div><div class="ab cl ls lt hy lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="in io ip iq ir"><h2 id="02de" class="lz ma iu bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">CatBoost</h2><p id="50c1" class="pw-post-body-paragraph kw kx iu ky b kz ms jv lb lc mt jy le lf mu lh li lj mv ll lm ln mw lp lq lr in bi translated">全称是Categorical boosting，由Yandex开发。与其他算法相比，它具有巨大的优势，因为您不需要对数据集的分类特征进行编码；您将它们列在模型中，它会自己处理它们。<a class="nu nv ep" href="https://medium.com/u/cc9b2dac6f8b?source=post_page-----b2476996d145--------------------------------" rel="noopener" target="_blank"> Dmytro Iakubovskyi </a>在他对不同数据集(IMDB、葡萄酒、啤酒和更多带有统计数据的表格)的分析中广泛使用了它。CatBoost继承了XGBoost和LightGBM的大部分额外功能。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mx my l"/></div></figure><p id="f3c4" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">您可以看到Catboost(使用面向对象编程)和LightGBM(一个标准的用户定义函数)实现之间的区别。我从官方文档中获取了CatBoost类的代码。我只将<code class="fe mz na nb nc b">beta</code>添加到类的初始化中。你可以用你喜欢的任何形式(OOP或UDF)为这些函数编写代码。选择权在你！</p><p id="2f19" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">绘制结果:</p><div class="kk kl km kn gu ab cb"><figure class="nk ko nw nm nn no np paragraph-image"><img src="../Images/1fa6af81bbe26749ca7374cfab4aafc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*BmR01TJ-ttxGNZPiVncQoQ.png"/></figure><figure class="nk ko nx nm nn no np paragraph-image"><img src="../Images/6d710bcdc111b5d4446b729350c8f185.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*UqdUmlhrbWyjVTKYQwZ4RQ.png"/><figcaption class="kr ks gk gi gj kt ku bd b be z dk nr di ns nt translated">(<strong class="bd mb">左</strong>基本CatBoost模型| ( <strong class="bd mb">右</strong>)beta = 0.4的定制CatBoost模型，来源:图片由作者提供</figcaption></figure></div><p id="25d6" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">结果的逻辑与LightGBM模型相同。</p></div><div class="ab cl ls lt hy lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="in io ip iq ir"><h2 id="c997" class="lz ma iu bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">张量流</h2><p id="e7b8" class="pw-post-body-paragraph kw kx iu ky b kz ms jv lb lc mt jy le lf mu lh li lj mv ll lm ln mw lp lq lr in bi translated">它是谷歌著名的超级强大的算法家族。在这里建立一个自定义损失是一个不同的故事。你不需要明确地写下导数和自定义指标；不再有' beta '了(<code class="fe mz na nb nc b">beta</code>已死，<code class="fe mz na nb nc b">pos_weight</code>万岁！).TF有一个合适的功能，<code class="fe mz na nb nc b">tf.nn.weighted_cross_entropy_with_logits</code>让事情变得容易管理得多。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mx my l"/></div></figure><blockquote class="nd ne nf"><p id="4c96" class="kw kx ng ky b kz la jv lb lc ld jy le nh lg lh li ni lk ll lm nj lo lp lq lr in bi translated"><code class="fe mz na nb nc b">pos_weight</code>应该是&gt; 1.0惩罚FN，&lt; 1.0惩罚FP。和<code class="fe mz na nb nc b">beta</code>相比是相反的情况。<code class="fe mz na nb nc b">pos_weight</code>是logloss的FN部分相乘的系数，而<code class="fe mz na nb nc b">beta</code>是FP部分的因子。</p></blockquote><p id="40c6" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">绘制结果:</p><div class="kk kl km kn gu ab cb"><figure class="nk ko nl nm nn no np paragraph-image"><img src="../Images/2a2eb1fb82e8794442eb608a1d236032.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*MEIF75y2GZy6zonqYQqxzg.png"/></figure><figure class="nk ko nq nm nn no np paragraph-image"><img src="../Images/dab00f0b2d9dedf54cc77e705230db8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*DQhj9jMX75z9wvDmKr-wOg.png"/><figcaption class="kr ks gk gi gj kt ku bd b be z dk nr di ns nt translated">(<strong class="bd mb">左</strong>)基本张量流模型| ( <strong class="bd mb">右</strong>)定制的张量流模型，pos_weight = 3.5，来源:图片由作者提供</figcaption></figure></div><p id="d535" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">我的定制模型表现相当差，而TF标准模型表现很好；我希望你原谅我糟糕的结果，因为这里的主要目标是演示。</p></div><div class="ab cl ls lt hy lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="in io ip iq ir"><p id="a5b9" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated"><strong class="ky iv">结论</strong></p><p id="4e5b" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">所有模型的总体结果都是可比的。FN和FP之间的权衡也很到位。但是如果减少FN是你的目标，这些自定义损失由你支配。</p><p id="9884" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated"><strong class="ky iv">优点</strong></p><ul class=""><li id="56c9" class="ny nz iu ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">易于快速应用(使用四个用户定义的函数和测试版，仅此而已)。</li><li id="8de1" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">在建模之前，不需要对底层数据进行操作(如果数据集不是高度不平衡的话)</li><li id="6c91" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">它可以作为数据探索的一部分或模型叠加的一部分来应用。</li><li id="d415" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">我们可以将它添加到最流行的机器学习包中。</li><li id="558e" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">通过嵌入的<code class="fe mz na nb nc b">beta</code>或<code class="fe mz na nb nc b">pos_weight</code>，我们可以像往常一样调整它们的超参数。</li></ul><p id="56df" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated"><strong class="ky iv">快捷键</strong></p><ul class=""><li id="ec2d" class="ny nz iu ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">我们应该调整<code class="fe mz na nb nc b">beta</code>以获得最佳的FN到FP的平衡。</li><li id="f784" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">当数据集高度不平衡时(次要类别少于所有样本的10%的数据集)，它可能不会提供有意义的结果。探索性数据分析对于模型的运行至关重要。</li><li id="982d" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">如果我们惩罚新生力量，它通常会导致大量的FP增长，反之亦然。您可能需要额外的资源来弥补这种增长。</li></ul><p id="248f" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">希望这篇文章对用UDF和OOPs编写自定义损耗甚至改编Tensorflow官方实现有所指导。此外，您可以使用这些示例作为函数开发的起点。</p><p id="0c9c" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">保持安全和健康。不允许战争。</p></div><div class="ab cl ls lt hy lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="in io ip iq ir"><h2 id="bfc0" class="lz ma iu bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated"><strong class="ak">参考文献</strong></h2><ol class=""><li id="6962" class="ny nz iu ky b kz ms lc mt lf om lj on ln oo lr op oe of og bi translated">关于如何在Stackoverflow上实现LightGBM的讨论--&gt;<a class="ae kv" href="https://stackoverflow.com/questions/58572495/how-to-implement-custom-logloss-with-identical-behavior-to-binary-objective-in-l/58573112#58573112" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/58572495/how-to-implementation-custom-log loss-with-identical-behavior-to-binary-objective-in-l/58573112 # 58573112</a></li><li id="311f" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr op oe of og bi translated">自定义loss -&gt; <a class="ae kv" href="https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function" rel="noopener ugc nofollow" target="_blank">官方CatBoost文档https://CatBoost . ai/en/docs/concepts/python-usages-examples #用户自定义loss-function </a></li><li id="546e" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr op oe of og bi translated">自定义指标的CatBoost官方文档--&gt;<a class="ae kv" href="https://catboost.ai/en/docs/concepts/python-usages-examples#custom-loss-function-eval-metric" rel="noopener ugc nofollow" target="_blank">https://CatBoost . ai/en/docs/concepts/python-usages-examples # custom-loss-function-eval-metric</a></li><li id="01c0" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr op oe of og bi translated">带逻辑的加权交叉熵的TensorFlow官方文档-&gt;<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/nn/weighted _ cross _ entropy _ with _ logits</a></li><li id="04a3" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr op oe of og bi translated">关于如何在TensorFlow中组装自定义损失函数的优秀文章-&gt;<a class="ae kv" href="https://medium.com/swlh/custom-loss-and-custom-metrics-using-keras-sequential-model-api-d5bcd3a4ff28" rel="noopener">https://medium . com/swlh/custom-loss-and-custom-metrics-using-keras-sequential-model-API-D5 BCD 3 a4 ff 28</a></li><li id="309b" class="ny nz iu ky b kz oh lc oi lf oj lj ok ln ol lr op oe of og bi translated">我的GitHub储存库，包含所有提到的自定义损失-&gt;<a class="ae kv" href="https://github.com/kpluzhnikov/binary_classification_custom_loss" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/kpluzhnikov/binary _ classification _ custom _ loss</a></li></ol></div><div class="ab cl ls lt hy lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="in io ip iq ir"><p id="fe17" class="pw-post-body-paragraph kw kx iu ky b kz la jv lb lc ld jy le lf lg lh li lj lk ll lm ln lo lp lq lr in bi translated">如果你喜欢这篇文章，请毫不犹豫地喜欢、评论并分享它。或者甚至:</p><div class="oq or gq gs os ot"><a href="https://medium.com/@kplz/membership" rel="noopener follow" target="_blank"><div class="ou ab fp"><div class="ov ab ow cl cj ox"><h2 class="bd iv gz z fq oy fs ft oz fv fx it bi translated">通过我的推荐链接-康斯坦丁·普鲁申尼科夫加入媒体</h2><div class="pa l"><h3 class="bd b gz z fq oy fs ft oz fv fx dk translated">阅读康斯坦丁·普鲁申尼科夫的每一个故事(以及媒体上成千上万的其他作家)。您的会员费直接…</h3></div><div class="pb l"><p class="bd b dl z fq oy fs ft oz fv fx dk translated">medium.co</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph kp ot"/></div></div></a></div></div></div>    
</body>
</html>