<html>
<head>
<title>Insights Regarding Pose Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于姿态估计的见解</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/insights-regarding-pose-estimation-c8b37f5df660?source=collection_archive---------2-----------------------#2022-12-01">https://pub.towardsai.net/insights-regarding-pose-estimation-c8b37f5df660?source=collection_archive---------2-----------------------#2022-12-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="096a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你为什么不深入研究描绘人体的技术艺术呢？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5d4e1536cbeaace9f9efb27be2c374ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BIp6jEoTxhWw5vl7yA5yog.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:https://arxiv.org/abs/1812.08008</figcaption></figure><p id="eb8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> T </span>他的帖子的目的是分享我在<a class="ae ky" href="https://www.neosperience.com/" rel="noopener ugc nofollow" target="_blank"> Neosperience </a>实习期间研究<strong class="lb iu">姿态估计</strong>时学到的知识和实验。这份文件可能对那些计划开发与估计姿态相关的项目的人有用。</p><p id="294e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以查看<strong class="lb iu">技术细节</strong>、<strong class="lb iu">实验方法、</strong>和<strong class="lb iu">代码实现</strong>进行本帖推理。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><blockquote class="ml mm mn"><p id="8603" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">技术细节</em> </strong></p></blockquote><p id="b5d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">姿态估计</strong>是计算机视觉中使用的一项技术，涉及从图像或视频帧中估计<strong class="lb iu">物体</strong>的<strong class="lb iu">位置</strong>、<strong class="lb iu">方位</strong>和<strong class="lb iu">比例</strong>。<strong class="lb iu">姿态估计</strong>在很多领域都有应用，比如<strong class="lb iu">机器人</strong>、<strong class="lb iu">增强现实</strong>、<strong class="lb iu">动作捕捉</strong>、<strong class="lb iu">计算机视觉</strong>。</p><p id="8139" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当前最先进的姿态估计方法是基于<strong class="lb iu">卷积神经网络(CNN) </strong>。完成这项任务的方法主要有两种:一种是<strong class="lb iu">自上而下的方法</strong>和一种<strong class="lb iu">自下而上的方法</strong>。两种方法都有各自的<strong class="lb iu">优点</strong>和<strong class="lb iu">缺点</strong>与监控区域相关。</p><ul class=""><li id="ecd0" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">自上而下</strong>的方法包括两个步骤。第一步是检测图像中的所有人，然后在这些检测中估计部分并计算姿态。它也被称为两阶段<strong class="lb iu">方法</strong>。</li></ul><p id="34bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些<strong class="lb iu">自顶向下</strong>的方法有:<a class="ae ky" href="https://github.com/Microsoft/human-pose-estimation.pytorch" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">用于人体姿态估计和跟踪的单基线</strong> </a> <strong class="lb iu">，</strong><a class="ae ky" href="https://github.com/MVIG-SJTU/AlphaPose" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">alpha Pose</strong></a><strong class="lb iu">，Mask-RCNN，</strong><a class="ae ky" href="https://github.com/GengDavid/pytorch-cpn" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">py torch CPN</strong></a></p><ul class=""><li id="6ea2" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">自下而上</strong>的方法首先预测图像中的<strong class="lb iu">关键点</strong>，然后将这些关键点组合成图像中人的姿势。</li></ul><p id="150a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些<strong class="lb iu">自下而上</strong>的方法有<strong class="lb iu"/><a class="ae ky" href="https://www.tensorflow.org/hub/tutorials/movenet" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">MoveNet</strong></a><strong class="lb iu">，</strong><a class="ae ky" href="https://arxiv.org/pdf/1803.08225.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">PersonLab</strong></a><strong class="lb iu">，</strong><a class="ae ky" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">open pose</strong></a>。</p><blockquote class="nb"><p id="7228" class="nc nd it bd ne nf ng nh ni nj nk lu dk translated"><strong class="ak">那么，在什么条件下应该首选哪种方法呢？</strong></p></blockquote><ul class=""><li id="561f" class="ms mt it lb b lc nl lf nm li nn lm no lq np lu mx my mz na bi translated"><strong class="lb iu">自顶向下</strong>的方法主要用于<strong class="lb iu">单人</strong>位姿估计。</li><li id="91bb" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated"><strong class="lb iu">自下而上的</strong>方式更适合<strong class="lb iu">多人</strong>的情况。</li><li id="bae1" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated"><strong class="lb iu">自上而下</strong>方式比<strong class="lb iu">自下而上</strong>方式精度<strong class="lb iu">高</strong>但速度<strong class="lb iu">低。</strong></li><li id="1a49" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated"><strong class="lb iu">自底向上的</strong>方法提供了<strong class="lb iu">常量运行时。</strong></li></ul><p id="4ffc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种<strong class="lb iu">在<strong class="lb iu">精度</strong>和<strong class="lb iu">速度</strong>之间权衡</strong>的主要原因是<strong class="lb iu">分辨率</strong>。如上所述，<strong class="lb iu">自上而下</strong>的方法首先检测到这个人，并使用图像的这一部分来反馈给他们的网络。它具有高分辨率的姿态估计。另一方面，整个图像用于为<strong class="lb iu">自下而上</strong>方法的网络提供信息。</p><p id="69ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，<strong class="lb iu">自底向上的</strong>方法似乎更适合于<strong class="lb iu">实时应用</strong>。然而，<strong class="lb iu">根据客户的情况，可以首选自上而下的</strong>方法。</p><p id="48af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在人数较少的情况下，这种方式极有可能带来更<strong class="lb iu">准确的</strong>结果，并且<strong class="lb iu">推断时间</strong>可能不成问题。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><blockquote class="ml mm mn"><p id="695f" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">实验</em> </strong></p></blockquote><p id="10df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经详细介绍了我在实施过程中所使用的方法。关于这些方法的简短解释和对样本视频的推断结果可以在这一部分进行检查。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/fce34fe21285e0b7c8866f6e9a17debc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fd4s9YUVwVkioKGbJxqXNQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">样本视频中的单帧。来源:来自<a class="ae ky" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=video&amp;utm_content=32937" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的<a class="ae ky" href="https://pixabay.com/users/stuck1-15406262/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=video&amp;utm_content=32937" rel="noopener ugc nofollow" target="_blank">拉蒙·斯塔基</a>的视频</figcaption></figure><p id="4696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1812.08008" rel="noopener ugc nofollow" target="_blank">T5】open poseT7】</a></p><ul class=""><li id="531a" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">自下而上</strong>的方法。</li><li id="fc32" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated"><strong class="lb iu"> OpenPose </strong>是为<strong class="lb iu">实时多人</strong> 2D姿态估计而设计的。</li><li id="d1b3" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">它比<strong class="lb iu">自上而下</strong>的方法精度低。然而，它在演示中表现出了良好的性能。</li><li id="c812" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated"><strong class="lb iu">推断时间</strong>与其他一些估计量的比较可以在下图中查到。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/40b1160288b23a670eec567e533a14f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/0*kHMSE7OCrT7tDxCM"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://arxiv.org/abs/1812.08008" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1812.08008</a></figcaption></figure><ul class=""><li id="5629" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">与某些<strong class="lb iu">自上而下的</strong>方法不同，如<strong class="lb iu"> Mask-RCNN </strong>和<strong class="lb iu">Alpha Pose</strong>,<strong class="lb iu">open Pose</strong>在每张图像的<strong class="lb iu">人数</strong>和<strong class="lb iu">运行时间</strong>之间没有线性关系。</li><li id="fb74" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">你可以在下面看到演示结果。这个<a class="ae ky" href="https://github.com/mgultekin/PoseEstimation/blob/main/notebooks/OpenPose.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>可以用来对<strong class="lb iu">打开姿势</strong>进行推断。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/8ad50dde1bceee304b6158a7e898bc37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9h4Uh28WtUOE-Gb5hnVjAw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:作者图片</figcaption></figure><p id="56cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">脸书探测仪2 </strong> </a></p><ul class=""><li id="0b85" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">它是由<strong class="lb iu">脸书</strong>创建的基于<strong class="lb iu"> PyTorch的</strong>对象检测和分割<strong class="lb iu">库</strong>。</li><li id="41ba" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">他们不断用新任务更新他们的Github。<strong class="lb iu">人体姿态预测</strong>就是其中之一。</li><li id="210c" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">他们的模型基于<strong class="lb iu">面具R-CNN </strong>。对于姿态估计，他们提供了一种<strong class="lb iu">自上而下</strong>的方法，首先检测人类，然后估计每个盒子中的关键点，包括人类。</li><li id="58cb" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">它包括针对不同视觉识别任务的不同预训练模型。这些模型可以在他们的<a class="ae ky" href="https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md" rel="noopener ugc nofollow" target="_blank"> GitHub模型动物园</a>部分找到。</li><li id="62a2" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">你可以通过Colab笔记本看到<a class="ae ky" href="https://github.com/mgultekin/PoseEstimation/blob/main/Detectron2Inference.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">我的实现</strong> </a>。</li><li id="490e" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">下面是演示结果的截图。视频演示结果显示<strong class="lb iu">探测器2 </strong>在<strong class="lb iu">精度</strong>和<strong class="lb iu">速度</strong>两方面都没有给<strong class="lb iu">带来高性能</strong>。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/b362a55a7a5e3f3953e3621bebad391b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LlGd1PGdw3byYQuh6KH7iw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:作者图片</figcaption></figure><p id="9f8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/2207.02696" rel="noopener ugc nofollow" target="_blank">T13】yolov 7T15】</a></p><ul class=""><li id="5697" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">高性能</strong>在<strong class="lb iu">速度</strong>和<strong class="lb iu">精度</strong>方面。</li><li id="b350" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">在相似的推理速度下，它们提供比其他<strong class="lb iu"> YOLOs </strong>更高的精度。</li><li id="d32e" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">导出模型可能很容易。他们支持出口<strong class="lb iu"> Pytorch到ONNX </strong>和<strong class="lb iu">tensort</strong>。所有的实现都可以在他们的<a class="ae ky" href="https://github.com/WongKinYiu/yolov7" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e7759a88c5029bce471cc6d03305b1a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/0*n_aNUT6m0zbNJ0ov"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://arxiv.org/abs/2207.02696" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2207.02696</a></figcaption></figure><ul class=""><li id="5838" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">与其他<strong class="lb iu">实时物体探测器</strong>的对比从上面可以看出，<strong class="lb iu"> YOLOv7 </strong>达到了<strong class="lb iu">最先进的性能</strong>。</li><li id="eb16" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">你可以通过Colab笔记本看到<a class="ae ky" href="https://github.com/mgultekin/PoseEstimation/blob/main/yolov7PoseEstimation.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">我的实现</strong> </a>。</li><li id="5f67" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">你可以从下面的演示结果中看到截图。就<strong class="lb iu">速度</strong>和<strong class="lb iu">精度</strong>而言，该型号在样本视频上表现最佳。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/ab04e61f160e0ebb451dc3119db36837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4H1RBu9-RHEWLGxM8jebA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:作者图片</figcaption></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><blockquote class="ml mm mn"><p id="7c1e" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">结论</em> </strong></p></blockquote><ul class=""><li id="a28e" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">我已经展示了我在<strong class="lb iu">姿态估计</strong>算法上的工作的概述。根据你的<strong class="lb iu">问题</strong>和不同方法的<strong class="lb iu">能力</strong>，你要找到合适的方法。</li><li id="ef01" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">我建议你先确定你的<strong class="lb iu">问题</strong>的<strong class="lb iu">困境</strong>，然后想出解决办法。<strong class="lb iu"> <em class="mo">没有一种方法可以解决所有可能的场景，例如多人、高速推理需求或关于安全性和可伸缩性的硬件兼容性需求</em>。</strong></li></ul><blockquote class="ml mm mn"><p id="386d" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>姿态估计模型，它们的基准结果，它们的论文，以及实现细节可以从<a class="ae ky" href="https://paperswithcode.com/task/multi-person-pose-estimation" rel="noopener ugc nofollow" target="_blank">的论文中查阅，代码</a>。它还包括研究和实施之间的性能比较。你的项目可以有一个起点。</p></blockquote><p id="4828" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">各行各业的人都在估算一个物体或人的姿态。他们包括<strong class="lb iu">建筑师</strong>，他们用它来<strong class="lb iu">建造建筑</strong>和<strong class="lb iu">家具</strong>，<strong class="lb iu">公司</strong>，他们用它来<strong class="lb iu">工作场所安全</strong>，甚至<strong class="lb iu">人</strong>，他们只是想从不同的角度知道自己的<strong class="lb iu">身体</strong>有多少是合适的，以便他们能够<strong class="lb iu">针对每种情况适当地</strong>或<strong class="lb iu">跳舞</strong>。此外,<strong class="lb iu">政府</strong>需要维护<strong class="lb iu">人类的安全</strong>,就像人们在公共场所<strong class="lb iu">摔倒</strong>或<strong class="lb iu">受伤</strong>一样。这些例子只是沙漠中的沙粒。</p><p id="c6c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">幸运的是</strong>，我们能够通过使用<strong class="lb iu">深度学习</strong>和<strong class="lb iu">计算机视觉</strong>技术来实现这一切。这仅仅是开始…我们有足够的资源和能力去做这件事。</p><p id="7ff6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mo">让我们一起打造让人们生活更轻松的产品……感谢阅读。</em></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><blockquote class="ml mm mn"><p id="29d2" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated"><em class="it">关于我</em></p></blockquote><ul class=""><li id="0fee" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">我是一个<strong class="lb iu">深度学习</strong>和<strong class="lb iu">计算机视觉</strong>爱好者<strong class="lb iu">。我正在帕维亚大学<a class="ae ky" href="https://web-en.unipv.it/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a>攻读<strong class="lb iu">数据科学</strong>的<strong class="lb iu">硕士学位。</strong></strong></li><li id="1f62" class="ms mt it lb b lc nq lf nr li ns lm nt lq nu lu mx my mz na bi translated">你可以通过<a class="ae ky" href="https://www.linkedin.com/in/gultekinmustafa/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae ky" href="https://twitter.com/MRgultekinn" rel="noopener ugc nofollow" target="_blank"> Twitter、</a>和<a class="ae ky" href="https://medium.com/@mustafagultekinn01" rel="noopener"> Medium </a>联系到我。</li></ul></div></div>    
</body>
</html>