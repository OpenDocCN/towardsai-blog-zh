<html>
<head>
<title>EfficientDet: When Object Detection Meets Scalability and Efficiency</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EfficientDet:当对象检测满足可扩展性和效率时</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/efficientdet-when-object-detection-meets-scalability-and-efficiency-551e263719aa?source=collection_archive---------1-----------------------#2020-01-12">https://pub.towardsai.net/efficientdet-when-object-detection-meets-scalability-and-efficiency-551e263719aa?source=collection_archive---------1-----------------------#2020-01-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2ce7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">E</span><a class="ae ku" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank">efficientdet</a>，这是一个由谷歌研究团队Brain开发的高效且可扩展的先进对象检测模型。它不仅仅是一个单一的模型。它拥有一系列探测器，与以前的物体探测器相比，其参数和触发器数量级更少，精度更高。</p><p id="bc52" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">EfficientDet报纸提到了它的7个家族成员。</p><blockquote class="kv"><p id="67e3" class="kw kx iq bd ky kz la lb lc ld le kk dk translated">EfficientDet探测器[0–6]与其他SOTA物体探测模型的比较。</p></blockquote><figure class="lg lh li lj lk ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lf"><img src="../Images/ec33356f3c9fd97ae3897d90e2bdc11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wVXzRV58CNjHMV24FMFz7g.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">来源:<a class="ae ku" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank"> arXiv:1911.09070v1 </a></figcaption></figure><h1 id="5dca" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">论文的快速概述</h1><ol class=""><li id="72f0" class="mu mv iq jp b jq mw ju mx jy my kc mz kg na kk nb nc nd ne bi translated"><a class="ae ku" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> EfficientNet </strong> </a>是模型中使用的主干架构。EfficientNet也是由谷歌的同一批作者编写的。传统的CNN模型任意缩放网络尺寸- <em class="nf">宽度、深度和分辨率</em>。EfficientNet使用一组固定的缩放系数统一缩放每个维度。它以10倍的效率超越了SOTA精确度。</li><li id="f0c4" class="mu mv iq jp b jq ng ju nh jy ni kc nj kg nk kk nb nc nd ne bi translated"><strong class="jp ir"> BiFPN: </strong>而<em class="nf">融合</em>(应用<a class="ae ku" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">残差或跳过连接</a>)不同的输入特征，大部分作品只是简单的总结，没有任何区分。由于两个输入要素的分辨率不同，因此它们对融合输出图层的贡献也不同。提出了一种加权双向特征金字塔网络(BiFPN ),该网络引入了可学习的权重来学习不同输入特征的重要性。</li><li id="400b" class="mu mv iq jp b jq ng ju nh jy ni kc nj kg nk kk nb nc nd ne bi translated"><strong class="jp ir">复合缩放:</strong>对于更高的精度，先前的对象检测模型依赖于更大的主干或更大的输入图像尺寸。复合缩放是一种使用简单的复合系数<strong class="jp ir"> φ </strong>来联合放大主干网络、BiFPN网络、类/箱网络和分辨率的所有维度的方法。</li></ol><blockquote class="nl nm nn"><p id="43fc" class="jn jo nf jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">将EfficientNet主干与我们提出的BiFPN和复合缩放相结合，我们开发了一个新的对象检测器系列，名为EfficientDet，它始终以比以前的对象检测器少一个数量级的参数和FLOPS实现更好的准确性。</p></blockquote><h1 id="bded" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">BiFPN</h1><figure class="ns nt nu nv gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nr"><img src="../Images/b2d0b170e8e4355db72a50c014e94dcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZqkRP7n8Xh-PILCuX1510A.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">来源:<a class="ae ku" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank"> arXiv:1911.09070v1 </a> —图2</figcaption></figure><p id="5723" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">传统的FPN(特征金字塔网络)受到单向信息流的限制。PANet 为信息流增加了一个额外的自下而上的路径。PANet实现了更好的准确性，但是成本和更多的参数和计算。本文针对跨规模连接提出了几种优化方案:</p><ol class=""><li id="8a3f" class="mu mv iq jp b jq jr ju jv jy nw kc nx kg ny kk nb nc nd ne bi translated">移除只有一条输入边的结点。<br/> <em class="nf">如果一个节点只有一条输入边没有进行特征融合，那么它对旨在融合不同特征的特征网络的贡献就比较小。</em></li><li id="2b77" class="mu mv iq jp b jq ng ju nh jy ni kc nj kg nk kk nb nc nd ne bi translated">如果原始输入节点和输出节点处于同一级别，则从原始输入节点到输出节点添加一条额外的边，以便在不增加太多成本的情况下融合更多功能。</li><li id="df8b" class="mu mv iq jp b jq ng ju nh jy ni kc nj kg nk kk nb nc nd ne bi translated">将每个双向(自上而下和自下而上)路径视为一个要素网络图层，并多次重复相同的图层以实现更高级别的要素融合。</li></ol><h1 id="3157" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">加权特征融合</h1><p id="513a" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy nz ka kb kc oa ke kf kg ob ki kj kk ij bi translated">而多尺度融合，输入特征不是简单的相加。作者建议在特征融合过程中为每个输入增加额外的权重，并让网络学习每个输入特征的重要性。三种加权融合方法中— <br/> <strong class="jp ir">无界融合:</strong></p><figure class="ns nt nu nv gt ll gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/74fcbaa12001393a5acc729d0ac7f44b.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*rv-MpuUiv-uVs5Trx4XI1Q.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">来源:<a class="ae ku" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1911.09070</a></figcaption></figure><p id="cbf9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="nf"> W </em>是可学习的权重，可以是标量(每特征)、向量(每通道)或多维张量(每像素)。由于标量权重是无限的，这可能会导致训练不稳定。因此，基于Softmax的融合被尝试用于归一化权重。<br/>T13】基于Softmax的融合:</p><figure class="ns nt nu nv gt ll gh gi paragraph-image"><div class="gh gi od"><img src="../Images/2326e65194921592371c1cc28067a4dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*AhA54rA3NcNFEuosOac_YA.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">来源:https://arxiv.org/abs/1911.09070</figcaption></figure><p id="cb33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为softmax将权重归一化为0到1范围内的概率，这可以表示每个输入的重要性。softmax导致GPU速度变慢。<br/> <strong class="jp ir">快速归一化融合:</strong></p><figure class="ns nt nu nv gt ll gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7ef17e9cd41ca7c3f53f55afc3d17149.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*AO0A5dDSJl0GKeUvYYc4cQ.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">来源:<a class="ae ku" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1911.09070</a></figcaption></figure><p id="1a63" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">添加є是为了数字的稳定性。它在GPU上快了30%,给出的结果几乎和softmax一样准确。</p><blockquote class="nl nm nn"><p id="282c" class="jn jo nf jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">最终的BiFPN集成了双向跨尺度连接和快速归一化融合。</p></blockquote><h1 id="3e73" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">高效检测架构</h1><figure class="ns nt nu nv gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi of"><img src="../Images/ebb99f9a5e9938678ba7dd621fb05bf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nP0LlBoz0Uqhd17T4bINzg.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated">来源:<a class="ae ku" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank"> arXiv:1911.09070v1 </a> —图3</figcaption></figure><p id="0834" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">EfficientDet遵循一阶段检测范式。预先训练的EfficientNet主干与BiFPN一起用作特征提取器。BiFPNN从EfficientNet主干网络获取{P3、P4、P5、P6、P7}特征，并重复应用双向特征融合。<br/>融合的特征被馈送到用于预测对象类别和包围盒的类别和包围盒网络。</p><div class="og oh gp gr oi oj"><a href="https://arxiv.org/abs/1911.09070" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">EfficientDet:可扩展且高效的对象检测</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">模型效率在计算机视觉中变得越来越重要。在本文中，我们系统地研究了各种模型</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://arxiv.org/abs/1905.11946" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">EfficientNet:重新思考卷积神经网络的模型缩放</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">卷积神经网络(ConvNets)通常是在固定的资源预算下开发的，然后按比例放大用于…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://arxiv.org/abs/1803.01534" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">用于实例分割的路径聚合网络</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">信息在神经网络中传播的方式非常重要。在本文中，我们提出了路径…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://arxiv.org/abs/1512.03385" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">用于图像识别的深度残差学习</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">更深层次的神经网络更难训练。我们提出了一个剩余学习框架，以减轻训练…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">arxiv.org</p></div></div></div></a></div><blockquote class="kv"><p id="1ac2" class="kw kx iq bd ky kz os ot ou ov ow kk dk translated">希望你喜欢这篇文章。</p></blockquote><p id="8ed9" class="pw-post-body-paragraph jn jo iq jp b jq ox js jt ju oy jw jx jy oz ka kb kc pa ke kf kg pb ki kj kk ij bi translated">👉<a class="ae ku" href="https://twitter.com/aniketmaurya" rel="noopener ugc nofollow" target="_blank">推特</a>:<a class="ae ku" href="https://twitter.com/aniketmaurya" rel="noopener ugc nofollow" target="_blank">https://twitter.com/aniketmaurya</a>T5】👉邮件:aniketmaurya@outlook.com</p></div></div>    
</body>
</html>