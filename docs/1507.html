<html>
<head>
<title>What is CLIP (Contrastive Language — Image Pre-training) and how it can be used for semantic image search?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CLIP(对比语言—图像预训练)是什么，如何用于语义图像搜索？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/what-is-clip-contrastive-language-image-pre-training-and-how-it-can-be-used-for-semantic-image-b02ccf49414e?source=collection_archive---------0-----------------------#2021-02-09">https://pub.towardsai.net/what-is-clip-contrastive-language-image-pre-training-and-how-it-can-be-used-for-semantic-image-b02ccf49414e?source=collection_archive---------0-----------------------#2021-02-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ac7b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="2f47" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">深度学习搜索图像的方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/2addf57e184359ae01d7cb98d8ff5c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CuzMp1faYWMDvIbUKl_DvA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@miteneva?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">玛利亚·特内娃</a>在<a class="ae lh" href="https://unsplash.com/s/photos/deep-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="c0f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最近，OpenAI的研究人员发表了一种多模态架构，一旦在大约4亿个图像-文本对上进行预训练，就可以用于30种不同的任务。这种方法并不新鲜，以前许多其他研究人员试图使用文本转换器和预训练CNN模型的组合来预训练图像-文本对的模型，然后将其用于不同的向下任务。但是由于种种原因，这些方法并不像在<a class="ae lh" href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中讨论的那样成功。尝试了各种预训练方法，包括预测和对比；在不同的向下任务中达到SOTA水平的精确度。在预测方法中，训练多模态架构来基于图像预测字幕。但这种方法在向下的任务中并不奏效，因为模型试图匹配文本中的精确单词。因此，使用对比方法通过联合训练图像编码器和文本编码器来从多模态表示中学习，以最大化正确(图像-文本)对之间的余弦相似性，并最小化不正确(图像-文本)对之间的余弦相似性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/98334f5848aaa1f9664854fd9ab4cccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*ZqjY8IvfM0UDqIp7LVKFIw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf" rel="noopener ugc nofollow" target="_blank">回形针</a></figcaption></figure><p id="5e76" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们如何使用这个预先训练好的模型进行语义图像搜索呢？我们有一个图像查询文本T和一堆不同的图像。文本编码器提供查询文本t的文本特征(<code class="fe mf mg mh mi b">Tfeat</code>)。然后，我们遍历图像Is，并使用图像编码器为每个图像I计算图像特征(<code class="fe mf mg mh mi b">Ifeat</code>)，并计算图像特征和文本特征之间的点积相似度。由于预训练目标最大化了正确(图像、文本)对的相似性得分，因此我们可以认为最大点积值意味着最大相似性。所以对于每张图片，我们计算点积相似度，然后按照分数降序排列。余弦相似性也可以用来计算相似性得分，但GPU在矩阵乘法上更快，因此，这里我将使用点积。让我们进入编码。在这篇博客中，我们将使用Unsplash API从查询文本中获取图像。使用来自Unsplash API的查询文本和图像，我们将根据上面讨论的技术对它们进行排序。</p><h1 id="9978" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">下载剪辑模型和包</h1><pre class="ks kt ku kv gt nb mi nc nd aw ne bi"><span id="4225" class="nf mk it mi b gy ng nh l ni nj">pip install git+https://github.com/openai/CLIP.git</span></pre><h1 id="f240" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">加载模型并计算相似性</h1><p id="e916" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">这里，我们将从<strong class="lk jd"> COCO Captions </strong>数据集中随机选择三张图片，并提供一个标题，然后检查它是如何工作的。</p><pre class="ks kt ku kv gt nb mi nc nd aw ne bi"><span id="e5a3" class="nf mk it mi b gy ng nh l ni nj">import torch<br/>import clip<br/>from PIL import Image</span><span id="839f" class="nf mk it mi b gy np nh l ni nj">images = ['391895.png', '522418.png', '318219.png']<br/>text = 'a man with red helmet on a moped'</span><span id="d99d" class="nf mk it mi b gy np nh l ni nj">simScore = []<br/>tokenizedText = clip.tokenize(text).to(device)<br/>for img in images:<br/>    image = preprocess(Image.open(os.path.join(path, img))).unsqueeze(0).to(device)<br/>    with torch.no_grad():<br/>        image_features = model.encode_image(image)<br/>        text_features = model.encode_text(tokenizedText)<br/>    <br/>		# append image name with similarity score<br/>    simScore.append((img, torch.matmul(text_features, image_features.T)[0][0]))</span><span id="2eb5" class="nf mk it mi b gy np nh l ni nj">print(simScore)</span><span id="e209" class="nf mk it mi b gy np nh l ni nj"># output<br/>[('391895.png', tensor(21.2141)),<br/> ('522418.png', tensor(11.7804)),<br/> ('318219.png', tensor(10.6959))]</span></pre><div class="ks kt ku kv gt ab cb"><figure class="nq kw nr ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/2762df0cab87dc5b856ebf3f4d08e4a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*oigr6byhBbsuwezP6Fbbjg.png"/></div></figure><figure class="nq kw nw ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/1d9f921c3b7e0a8c5aed5a30ab814500.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*xmteQAEynNxDfbqNGVC6Qw.png"/></div></figure><figure class="nq kw nx ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/3b0f286a23b8a09311507b039ec04ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*UwBpc7jBWz8_Ll3HVKkFww.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk ny di nz oa translated">来源:可可字幕数据集(391895.png，391895.png)</figcaption></figure></div><h1 id="92b0" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">使用Streamlit share部署图像语义搜索应用程序</h1><p id="7727" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">在<a class="ae lh" href="https://unsplash.com/join" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上注册一个开发者账户，创建一个应用程序并获得访问密钥。</p><p id="7ca0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">创建</strong> <code class="fe mf mg mh mi b"><strong class="lk jd">streamlitcliputils.py</strong></code> <strong class="lk jd">文件并跟随</strong></p><ul class=""><li id="f01a" class="ob oc it lk b ll lm lo lp lr od lv oe lz of md og oh oi oj bi translated">导入和模型加载</li></ul><pre class="ks kt ku kv gt nb mi nc nd aw ne bi"><span id="b661" class="nf mk it mi b gy ng nh l ni nj">import torch<br/>import clip<br/>from PIL import Image<br/>import os<br/>import re<br/>from tqdm import tqdm, trange<br/>import random<br/>import requests<br/>import numpy as np<br/>import streamlit as st</span><span id="68ef" class="nf mk it mi b gy np nh l ni nj">global model, preprocess, device</span><span id="072d" class="nf mk it mi b gy np nh l ni nj">device = 'cpu'</span><span id="7913" class="nf mk it mi b gy np nh l ni nj">model, preprocess = clip.load("ViT-B/32", device = 'cpu')</span></pre><ul class=""><li id="1a78" class="ob oc it lk b ll lm lo lp lr od lv oe lz of md og oh oi oj bi translated">使用API从Unsplash获取基于查询文本的图像</li></ul><pre class="ks kt ku kv gt nb mi nc nd aw ne bi"><span id="8dec" class="nf mk it mi b gy ng nh l ni nj">def getImagesFromUnsplash(total: int, query_text: str):<br/>    '''<br/>    Images from query text<br/>    '''<br/>    num_page = 1<br/>    imgs_total = total<br/>    query = query_text<br/>    url = f"&lt;https://api.unsplash.com/search/photos?query={query_text}&amp;page={num_page}&amp;per_page={imgs_total}&gt;"</span><span id="cada" class="nf mk it mi b gy np nh l ni nj">    headers = {<br/>        "Authorization": f"Bearer Client-ID {UPSPLASH_API_KEY}",</span><span id="28f2" class="nf mk it mi b gy np nh l ni nj">    }</span><span id="d6b0" class="nf mk it mi b gy np nh l ni nj">    req = requests.get(url, headers = headers)</span><span id="39f5" class="nf mk it mi b gy np nh l ni nj">    resp = req.json()</span><span id="5bda" class="nf mk it mi b gy np nh l ni nj">    regUrls = [r['urls']['regular'] for r in resp['results']]</span><span id="13a5" class="nf mk it mi b gy np nh l ni nj">    return regUrls</span></pre><ul class=""><li id="dac8" class="ob oc it lk b ll lm lo lp lr od lv oe lz of md og oh oi oj bi translated">图片链接到<code class="fe mf mg mh mi b">PIL.Image</code></li></ul><pre class="ks kt ku kv gt nb mi nc nd aw ne bi"><span id="dc35" class="nf mk it mi b gy ng nh l ni nj">def linkToImage(link):<br/>    '''<br/>    Image URL to PIL.Image<br/>    '''<br/>    content = requests.get(link, stream = True)<br/>    content = content.raw<br/>    img = Image.open(content)<br/>    return img</span></pre><ul class=""><li id="cfc0" class="ob oc it lk b ll lm lo lp lr od lv oe lz of md og oh oi oj bi translated">计算相似性得分</li></ul><pre class="ks kt ku kv gt nb mi nc nd aw ne bi"><span id="9362" class="nf mk it mi b gy ng nh l ni nj">@st.cache(show_spinner=False)<br/>def getImageTextSimScore(link, text):<br/>    '''<br/>    Compute similarity score from image feature<br/>    and text feature<br/>    '''<br/>    image = preprocess(linkToImage(link)).unsqueeze(0).to(device)<br/>    tokenizedText = clip.tokenize(text).to(device)<br/>    with torch.no_grad():<br/>        image_features = model.encode_image(image)<br/>        text_features = model.encode_text(tokenizedText)<br/>    <br/>    simScore = torch.matmul(text_features, image_features.T)[0][0]</span><span id="c752" class="nf mk it mi b gy np nh l ni nj">    return simScore.item()</span></pre><ul class=""><li id="6220" class="ob oc it lk b ll lm lo lp lr od lv oe lz of md og oh oi oj bi translated">从Unsplash获取图像并排序</li></ul><pre class="ks kt ku kv gt nb mi nc nd aw ne bi"><span id="4f2b" class="nf mk it mi b gy ng nh l ni nj">@st.cache(show_spinner=False)<br/>def getSortedQuery(text):<br/>    '''<br/>    Get images from text and sort<br/>    using similarity score<br/>    '''</span><span id="b83c" class="nf mk it mi b gy np nh l ni nj">    upSplashImages = getImagesFromUnsplash(10, text)<br/>    <br/>    imgSimScore = []</span><span id="0f46" class="nf mk it mi b gy np nh l ni nj">    for ix, img in enumerate(tqdm(upSplashImages)):</span><span id="e4a6" class="nf mk it mi b gy np nh l ni nj">        imgSimScore.append((img, getImageTextSimScore(img, text)))<br/>    <br/>    imgSimScore = sorted(imgSimScore, key = lambda x: x[1], reverse=True)</span><span id="8d99" class="nf mk it mi b gy np nh l ni nj">    return imgSimScore, upSplashImages</span></pre><p id="fb20" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">创建</strong> <code class="fe mf mg mh mi b"><strong class="lk jd">streamlitapp.py</strong></code> <strong class="lk jd">文件</strong></p><pre class="ks kt ku kv gt nb mi nc nd aw ne bi"><span id="c1ad" class="nf mk it mi b gy ng nh l ni nj">from streamlitcliputils import *<br/>import streamlit as st<br/>import io<br/>from PIL import Image</span><span id="8cdf" class="nf mk it mi b gy np nh l ni nj">st.set_page_config(<br/>    page_title = 'CLIP',<br/>    page_icon = '🎑'<br/>)</span><span id="60df" class="nf mk it mi b gy np nh l ni nj">st.header("CLIP - Semantic Image Search")<br/>imageText = st.text_input("Search Image")</span><span id="3155" class="nf mk it mi b gy np nh l ni nj">if imageText:<br/>    with st.spinner(text = 'Getting Images from Unsplash and sorting with clip ...'):<br/>        <br/>        imgSimScore, upSplashImages = getSortedQuery(imageText)</span><span id="d565" class="nf mk it mi b gy np nh l ni nj">        images = [linkToImage(img) for img, score in imgSimScore]<br/>        simScore = [f'Sim Score: {score:.2f}' for img, score in imgSimScore]</span><span id="49df" class="nf mk it mi b gy np nh l ni nj">        upSplashImages = [linkToImage(img) for img in upSplashImages]<br/>        upSplashIx = [i+1 for i in range(len(upSplashImages))]</span><span id="4505" class="nf mk it mi b gy np nh l ni nj">        col1, col2 = st.beta_columns(2)</span><span id="d458" class="nf mk it mi b gy np nh l ni nj">        col1.header("Semantic Search")<br/>        col1.image(images, width = 300, caption = simScore)</span><span id="06d3" class="nf mk it mi b gy np nh l ni nj">        col2.header("Images from Unsplash")<br/>        col2.image(upSplashImages, width = 300, caption = upSplashIx)</span></pre><p id="98f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，要使用Streamlit share部署应用程序，您需要有一个经过批准的Streamlit Share帐户，您可以在此处注册一个<a class="ae lh" href="https://share.streamlit.io/" rel="noopener ugc nofollow" target="_blank"/>。要使用streamlit share部署您的应用程序，您需要使用一个<code class="fe mf mg mh mi b">requirements.txt</code>文件将代码推送到GitHub。</p><ul class=""><li id="5e85" class="ob oc it lk b ll lm lo lp lr od lv oe lz of md og oh oi oj bi translated">将包添加到<code class="fe mf mg mh mi b">requirements.txt</code></li></ul><pre class="ks kt ku kv gt nb mi nc nd aw ne bi"><span id="439b" class="nf mk it mi b gy ng nh l ni nj">tqdm==4.56.0<br/>git+https://github.com/openai/CLIP.git<br/>requests==2.25.1<br/>streamlit==0.74.1<br/>torch==1.7.1<br/>numpy==1.20.0rc2<br/>Pillow==8.1.0</span></pre><p id="8624" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以创建一个GitHub存储库并推送您的文件。然后转到streamlit share控制台，单击新应用程序下拉菜单，选择“From Existing Repo”。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/0e79074c8c390d6c08d36a8243692744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*36TqVenSsxFOzJ3IxDggpA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:作者</figcaption></figure><p id="afeb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">选择repo名称，并在主文件路径输入中提供文件名</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/bf91b4b98040129cb77ad171cb94c3b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0K_ULffOBjy8Lj6P_JYXgQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:作者</figcaption></figure><p id="f51e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">单击部署，您将进入部署屏幕，您的应用将在几分钟内完成部署。语义搜索的实时版本可从<a class="ae lh" href="https://share.streamlit.io/vatsalsaglani/clipsemanticimagesearch/streamlitClip1/streamlitapp.py" rel="noopener ugc nofollow" target="_blank">这里</a>获得</p><p id="3123" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以看看这个视频，看看这个模型的表现。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:作者</figcaption></figure><p id="9e6e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为博客的总结，我们讨论了CLIP的训练方法以及如何将其用于语义图像搜索，然后我们使用Unsplash API来查询图像，并使用使用CLIP计算的相似性得分进行排序。我希望你喜欢看这个博客。</p></div></div>    
</body>
</html>