<html>
<head>
<title>ECCV 2020 Best Paper Award | A New Architecture For Optical Flow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ECCV 2020最佳论文奖|光流的新架构</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/eccv-2020-best-paper-award-a-new-architecture-for-optical-flow-3298c8a40dc7?source=collection_archive---------0-----------------------#2020-09-06">https://pub.towardsai.net/eccv-2020-best-paper-award-a-new-architecture-for-optical-flow-3298c8a40dc7?source=collection_archive---------0-----------------------#2020-09-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ffed" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>，<a class="ae ep" href="https://towardsai.net/p/category/research" rel="noopener ugc nofollow" target="_blank">研究</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/44a302f3098e21d97b6db4dec1c999fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wQRP6gBvTQJpVtLvr_nhw.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">照片由<a class="ae ko" href="https://unsplash.com/@crisovalle?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">克里斯·奥瓦尔</a>在<a class="ae ko" href="https://unsplash.com/s/photos/movement?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="eb94" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">普林斯顿团队获得ECCV 2020年最佳论文奖。<br/>他们为光流开发了一种新的端到端可训练模型。<br/>他们的方法在多个数据集上击败了最先进架构的准确性，并且更加高效。</p><p id="bc30" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">他们甚至在Github上向所有人开放代码！让我们看看他们是如何做到的。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="964c" class="lu lv it bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml iz bi translated">论文简介</h2><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mm"><img src="../Images/d439850d66ea349d93d04b50a5981f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmLvbXfPd6_OXbLrp-EFzQ.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank">https://github.com/princeton-vl/RAFT</a></figcaption></figure><p id="057e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">ECCV2020大会于上周举行。为了这次会议，计算机视觉领域的大量新研究论文问世。<br/>在这里，我将报道他们授予普林斯顿团队的“最佳论文奖”。</p><p id="a122" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">简而言之，他们为光流开发了一种新的端到端可训练模型，称为“RAFT:光流的循环所有对场变换”<br/>他们的方法在多个数据集上实现了最先进的准确性，并且效率更高。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="fb67" class="lu lv it bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml iz bi translated">什么是光流？</h2><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/71c521ed2b643fedb91df06011ba5314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*9cEJJUiKjT7_rxO-QcK8hg.gif"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">Gif作者:<a class="ae ko" href="https://gfycat.com/fr/wetcreepygecko" rel="noopener ugc nofollow" target="_blank">https://gfycat.com/fr/wetcreepygecko</a></figcaption></figure><p id="1037" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，我将快速解释什么是光流。<br/>定义为视频中物体的表观运动模式。<br/>换句话说，这意味着物体在一个序列的连续帧之间的运动。它计算物体和场景之间的相对运动。<br/>它通过使用视频中的时间结构和每帧中的空间结构来实现。</p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/0a24772c348900f736a8aec78642a6ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*dw5CGQmH1QPTSpPGg_Gzcg.gif"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">Gif作者:<a class="ae ko" href="https://nanonets.com/blog/optical-flow/" rel="noopener ugc nofollow" target="_blank">https://nanonets.com/blog/optical-flow/</a></figcaption></figure><p id="ef90" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如您所见，您可以使用OpenCV的函数轻松计算视频的光流:</p><pre class="mn mo mp mq gt mt mu mv mw aw mx bi"><span id="e40e" class="lu lv it mu b gy my mz l na nb">import cv2<br/>import numpy as np<br/>cap = cv2.VideoCapture("vtest.avi")<br/><br/>ret, frame1 = cap.read()<br/>prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)<br/>hsv = np.zeros_like(frame1)<br/>hsv[...,1] = 255<br/><br/>while(1):<br/>    ret, frame2 = cap.read()<br/>    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)<br/><br/>    flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)<br/><br/>    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])<br/>    hsv[...,0] = ang*180/np.pi/2<br/>    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)<br/>    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)<br/><br/>    cv2.imshow('frame2',rgb)<br/>    k = cv2.waitKey(30) &amp; 0xff<br/>    if k == 27:<br/>        break<br/>    elif k == ord('s'):<br/>        cv2.imwrite('opticalfb.png',frame2)<br/>        cv2.imwrite('opticalhsv.png',rgb)<br/>    prvs = next<br/><br/>cap.release()<br/>cv2.destroyAllWindows()</span></pre><p id="0c7a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">只需要几行代码就可以在实时提要中生成它。以下是使用此短代码从普通视频帧中获得的结果:</p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nc"><img src="../Images/6f891f28c66e0744be785f6b212e73c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_0pLAsL8XTiOtFgJjprHiQ.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">代码和图片作者:<a class="ae ko" href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html" rel="noopener ugc nofollow" target="_blank">https://opencv-python-tutro als . readthedocs . io/en/latest/py _ tutorials/py _ video/py _ Lucas _ kanade/py _ Lucas _ kanade . html</a></figcaption></figure><p id="d4ac" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">它非常酷，对许多应用程序非常有用。</p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/0a24772c348900f736a8aec78642a6ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*dw5CGQmH1QPTSpPGg_Gzcg.gif"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">Gif作者:【https://nanonets.com/blog/optical-flow/ T2】</figcaption></figure><p id="f695" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">例如交通分析、车辆跟踪、对象检测和跟踪、机器人导航等等。唯一的问题是它相当慢，需要大量的计算资源。</p><p id="4720" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这篇新论文有助于解决这两个问题，同时产生更准确的结果！</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="393d" class="lu lv it bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml iz bi translated">这是什么纸？研究人员到底做了什么？</h2><p id="ac35" class="pw-post-body-paragraph kp kq it kr b ks nd ku kv kw ne ky kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">现在，让我们更深入地了解这篇文章的内容，以及它如何改进了当前最先进的方法。<br/>他们从四个方面改进了最先进的方法。</p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ni"><img src="../Images/f70ad38429eebe2896318eeb37a92e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3EA9o7IsDY2cVkFecJN7w.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">图片作者:<a class="ae ko" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.12039.pdf</a></figcaption></figure><p id="48e2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，它可以直接在光流上训练，而不需要使用像素之间的嵌入损失来训练网络，这使得它更加有效。<br/>然后，关于流量预测，当前的方法直接在一对帧之间进行预测。<br/>相反，他们通过维护和更新单个高分辨率流场来优化计算时间。</p><p id="be6e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于这个流场，他们不得不使用类似于LSTM块的GRU块，以便像当前最好的方法一样迭代地改进他们的光流。</p><div class="mn mo mp mq gt ab cb"><figure class="nj kd nk nl nm nn no paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/df575c9616c3f5a08efdf663d5a1cf27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*nkKAUHBlohMBj_eiIITTJw.png"/></div></figure><figure class="nj kd np nl nm nn no paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/64901dbe592b296f2e02b45348a89e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*msvz3-XjMCAFdfAftsGx7w.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk nq di nr ns translated">图片作者:<a class="ae ko" href="http://dprogrammer.org/rnn-lstm-gru" rel="noopener ugc nofollow" target="_blank">http://dprogrammer.org/rnn-lstm-gru</a></figcaption></figure></div><p id="99c5" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该块允许它们在这些迭代之间共享权重，同时允许在训练时使用它们的固定流场进行收敛。</p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/11c9e09e1334da9d9eb93dc99142a1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*iABpt6yCEToNZNMO-cCyrw.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">图片来自:<a class="ae ko" href="https://blog.clairvoyantsoft.com/the-ascent-of-gradient-descent-23356390836f?gi=9b683d504450" rel="noopener ugc nofollow" target="_blank">https://blog . clairvoyantsoft . com/the-ascent-of-gradient-descent-23356390836 f？gi=9b683d504450 </a></figcaption></figure><p id="b311" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">他们的技术与其他方法之间的最后一个区别是，他们不是明确定义关于优化目标的梯度，而是使用反向传播，从相关体积中检索特征以提出下降方向。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="75a7" class="lu lv it bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml iz bi translated">他们是如何做到的？</h2><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nu"><img src="../Images/dde32817f1849354d1daeba03282b2b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M1RAARrCI81sAgwjYuD39g.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">图片作者:<a class="ae ko" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.12039.pdf</a></figcaption></figure><p id="de68" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所有这些改进都是使用他们的新架构实现的。<br/>它基本上由3个主要部件组成。</p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nu"><img src="../Images/fdfca22517d083187ca38aff82ae4dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EFHwf2HNm7Gxd5CZTsl-eQ.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">图片作者:<a class="ae ko" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.12039.pdf</a></figcaption></figure><p id="8dcc" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，一个编码器从两个不同的帧中提取每像素特征，另一个编码器仅从第一帧中提取特征，以便理解</p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nu"><img src="../Images/f817bf8d568ac9fa171046f1e1dc5592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IdVxSPDUwdvSCZH5wsk8ng.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">图片作者:【https://arxiv.org/pdf/2003.12039.pdf T2】</figcaption></figure><p id="6657" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">形象。<br/>然后，使用所有的特征向量对，他们使用两帧的宽度和高度生成一个四维体积。</p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nu"><img src="../Images/7ff62d91eac7de3dfdcc5e49db743a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qCpOs5cXsB0bkid3qPer9g.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">图片作者:<a class="ae ko" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.12039.pdf</a></figcaption></figure><p id="393c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后，他们使用一个更新算子循环更新光流。这就是GRU街区所在的地方。<br/>它从以前的相关体积中检索值，并迭代更新流场。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><h2 id="3436" class="lu lv it bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml iz bi translated">结果</h2><p id="469a" class="pw-post-body-paragraph kp kq it kr b ks nd ku kv kw ne ky kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">看看结果有多尖锐就知道了。同时比当前的方法更快！</p><p id="1f3f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">观看显示结果的视频:</strong></p><figure class="mn mo mp mq gt kd"><div class="bz fp l di"><div class="nv nw l"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://www.youtube.com/watch?v=OSEuYBwOSGI" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=OSEuYBwOSGI</a></figcaption></figure><p id="832e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">他们甚至在Github上向所有人开放代码！如果你想试试，我把它链接在下面。</p><p id="30f2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当然，这只是对ECCV2020最佳论文奖得主的简单概述。<br/>我强烈推荐阅读下面链接的论文以获取更多信息。</p><blockquote class="nx ny nz"><p id="ae8b" class="kp kq oa kr b ks kt ku kv kw kx ky kz ob lb lc ld oc lf lg lh od lj lk ll lm im bi translated"><strong class="kr jd"> OpenCV光流教程</strong>:<a class="ae ko" href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html" rel="noopener ugc nofollow" target="_blank">https://OpenCV-python-tutro als . readthedocs . io/en/latest/py _ tutorials/py _ video/py _ Lucas _ kanade/py _ Lucas _ kanade . html</a><br/><strong class="kr jd">论文</strong>:<a class="ae ko" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.12039.pdf</a><br/><strong class="kr jd">GitHub，代码</strong>:<a class="ae ko" href="https://github.com/princeton-vl/RAFT" rel="noopener ugc nofollow" target="_blank">https://github.com/princeton-vl/RAFT</a></p></blockquote></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="f1ac" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你喜欢我的工作并想支持我，我会非常感谢你在我的社交媒体频道上关注我:</p><ul class=""><li id="564e" class="oe of it kr b ks kt kw kx la og le oh li oi lm oj ok ol om bi translated">支持我最好的方式就是在<a class="ae ko" href="https://medium.com/@whats_ai" rel="noopener"> <strong class="kr jd">中</strong> </a>关注我。</li><li id="4bab" class="oe of it kr b ks on kw oo la op le oq li or lm oj ok ol om bi translated">订阅我的<a class="ae ko" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"> <strong class="kr jd"> YouTube频道</strong> </a>。</li><li id="b299" class="oe of it kr b ks on kw oo la op le oq li or lm oj ok ol om bi translated">在<a class="ae ko" href="https://www.linkedin.com/company/what-is-artificial-intelligence" rel="noopener ugc nofollow" target="_blank"> <strong class="kr jd"> LinkedIn </strong> </a>上关注我的项目</li><li id="75e9" class="oe of it kr b ks on kw oo la op le oq li or lm oj ok ol om bi translated">一起学习AI，加入我们的<a class="ae ko" href="https://discord.gg/SVse4Sr" rel="noopener ugc nofollow" target="_blank"> <strong class="kr jd"> Discord社区</strong> </a>，<em class="oa">分享你的项目、论文、最佳课程、寻找kaggle队友等等！</em></li></ul></div></div>    
</body>
</html>