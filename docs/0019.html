<html>
<head>
<title>Bad and Good Regression Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">好坏回归分析</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/bad-and-good-regression-analysis-700ca9b506ff?source=collection_archive---------3-----------------------#2019-02-01">https://pub.towardsai.net/bad-and-good-regression-analysis-700ca9b506ff?source=collection_archive---------3-----------------------#2019-02-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6360" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">审视机器学习回归模型| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向人工智能</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/04b9e260ed3e08bf3cb10284378479a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lwils93aT3t9c99l0Fiyww.png"/></div></div></figure><p id="37e8" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">回归模型是最流行的机器学习模型。回归模型用于连续预测目标变量。回归模型在几乎每个研究领域都有应用，因此，它是使用最广泛的机器学习模型之一。本文将讨论构建回归模型的好的和坏的实践。</p><p id="3661" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">我们将构建一个简单的线性回归模型(内点和外点之间没有区别，可以使用Lasso回归等更稳健的正则化回归模型来处理)，然后使用它来预测住房数据集的房价。我们使用模型的输出来突出回归分析中好的和坏的实践。</p><p id="6e2a" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">关于住房数据集的更多信息可以从<a class="ae li" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库</a>中找到。包含所有代码的Jupyter笔记本可以在<a class="ae li" href="https://github.com/bot13956/python-linear-regression-estimator/blob/master/simple_regression_analysis_housing_prices.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><h1 id="5b42" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">使用梯度下降的线性回归估计量</h1><p id="c42e" class="pw-post-body-paragraph kk kl it km b kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh im bi translated">在上一篇文章中，我们讨论了如何使用包含单个特征(X)的一维数据集构建简单的线性回归模型来预测连续的结果变量(y):<a class="ae li" href="https://medium.com/@benjaminobi/machine-leaning-python-linear-regression-estimator-using-gradient-descent-b0b2c496e463" rel="noopener">https://medium . com/@ benjamobi/machine-leaving-python-linear-regression-estimator-using-gradient-descent-b 0b 2c 496 e 463</a></p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mm"><img src="../Images/ec4c1073d5b78a4e1034ba63dbee2786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KTY0U4104a-mfw9tHZdFjA.png"/></div></div></figure><h1 id="d31e" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">用Python实现简单的线性回归估计器</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="cda4" class="mw lk it ms b gy mx my l mz na">class GradientDescent(object):<br/>    """Gradient descent optimizer.<br/>    Parameters<br/>    ------------<br/>    eta : float<br/>        Learning rate (between 0.0 and 1.0)<br/>    n_iter : int<br/>        Passes over the training dataset.<br/>        <br/>    Attributes<br/>    -----------<br/>    w_ : 1d-array<br/>        Weights after fitting.<br/>    errors_ : list<br/>        Error in every epoch.<br/>        <br/>    Methods<br/>    -----------<br/>    fit(X,y): fit the linear regression model using the data.</span><span id="1abf" class="mw lk it ms b gy nb my l mz na">    predict(X): Predict outcome for samples in X.</span><span id="e030" class="mw lk it ms b gy nb my l mz na">    Rsquare(X,y): Returns the R^2 value.<br/>    """</span><span id="72eb" class="mw lk it ms b gy nb my l mz na">    def __init__(self, eta=0.01, n_iter=10):<br/>        self.eta = eta<br/>        self.n_iter = n_iter<br/>        <br/>    def fit(self, X, y):<br/>        """Fit the data.<br/>        <br/>        Parameters<br/>        ----------<br/>        X : {array-like}, shape = [n_points]<br/>        Independent variable or predictor.<br/>        y : array-like, shape = [n_points]<br/>        Outcome of prediction.<br/>        Returns<br/>        -------<br/>        self : object<br/>        """<br/>        self.w_ = np.zeros(2)<br/>        self.errors_ = []<br/>        <br/>        for i in range(self.n_iter):<br/>            errors = 0<br/>            for j in range(X.shape[0]):<br/>                self.w_[1:] += self.eta*X[j]*(y[j] - self.w_[0] - self.w_[1]*X[j])<br/>                self.w_[0] += self.eta*(y[j] - self.w_[0] - self.w_[1]*X[j])<br/>                errors += 0.5*(y[j] - self.w_[0] - self.w_[1]*X[j])**2<br/>            self.errors_.append(errors)<br/>        return self</span><span id="be4c" class="mw lk it ms b gy nb my l mz na">    def predict(self, X):<br/>        """Return predicted y values"""<br/>        return self.w_[0] + self.w_[1]*X <br/>    <br/>    def Rsquare(self, X,y):<br/>        """Return the Rsquare value"""<br/>        y_hat = self.predict(X)<br/>        return  1-((y_hat - y)**2).sum()/((y-np.mean(y))**2).sum()</span></pre><h1 id="7137" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">Python估算器的应用:预测房价</h1><h1 id="3ffc" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">a)导入必要的库</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="f7ba" class="mw lk it ms b gy mx my l mz na">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import seaborn as sns<br/>np.set_printoptions(precision=4)<br/></span></pre><h1 id="65a6" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">b)探索住房数据集</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="ce93" class="mw lk it ms b gy mx my l mz na">df = pd.read_csv('<a class="ae li" href="https://raw.githubusercontent.com/rasbt/'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/rasbt/'</a><br/>                 'python-machine-learning-book-2nd-edition'<br/>                 '/master/code/ch10/housing.data.txt',<br/>                 header=None,<br/>                 sep='\s+')</span><span id="10a5" class="mw lk it ms b gy nb my l mz na">df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', <br/>              'NOX', 'RM', 'AGE', 'DIS', 'RAD', <br/>              'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']<br/>df.head()</span></pre><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nc"><img src="../Images/626d96526f32eabe78648d9d8a9e0550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38l3dTBFjwMwnT5do2Y4VA.png"/></div></div></figure><h1 id="3005" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">c)特征选择和标准化</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="7aae" class="mw lk it ms b gy mx my l mz na">cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV']<br/>from sklearn.preprocessing import StandardScaler<br/>stdsc = StandardScaler()<br/>X_std = stdsc.fit_transform(df[cols].iloc[:,range(0,5)].values)</span><span id="9d26" class="mw lk it ms b gy nb my l mz na"># Evaluate the covariance matrix</span><span id="cf8d" class="mw lk it ms b gy nb my l mz na">cov_mat =np.cov(X_std.T)<br/>hm = sns.heatmap(cov_mat,<br/>                 cbar=True,<br/>                 annot=True,<br/>                 square=True,<br/>                 fmt='.2f',<br/>                 annot_kws={'size': 15},<br/>                 yticklabels=cols,<br/>                 xticklabels=cols)</span><span id="359a" class="mw lk it ms b gy nb my l mz na">plt.tight_layout()<br/>plt.savefig('images/10_04.png', dpi=300)<br/>plt.show()</span></pre><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/cb44df1d2c8266948dc777b04c60ee53.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*p9c2s6etZmTju2lsPg5x0Q.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk translated"><strong class="bd ll">显示特征间相关性的协方差矩阵。</strong></figcaption></figure><p id="5954" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">由于我们对预测MEDV(房屋的中值)感兴趣，我们发现与RM(每个住宅的平均房间数)的相关性最强。因此，在我们的模型中，我们将使用RM作为预测变量，MEDV作为目标变量:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="b976" class="mw lk it ms b gy mx my l mz na">X=X_std[:,3] # we use RM as our predictor variable<br/>y=X_std[:,4] # we use MEDV as our target variable</span></pre><h1 id="9d05" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">d)计算不同学习率的R平方值</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="0ce4" class="mw lk it ms b gy mx my l mz na">[GradientDescent(eta=k, n_iter=100).fit(X,y).Rsquare(X,y) for k in [0.1,0.01,0.001,0.0001,0.00001,0.000001]]</span></pre><p id="5fd8" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">我们获得了以下输出:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="447a" class="mw lk it ms b gy mx my l mz na">[0.297,0.312,0.482,0.483,0.308,0.047]</span></pre><h1 id="77f1" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">e)拟合、预测和超参数调整</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="6ba8" class="mw lk it ms b gy mx my l mz na">np.set_printoptions(precision=1)<br/># plot with various axes scales<br/>plt.figure(figsize=(10,8))</span><span id="7651" class="mw lk it ms b gy nb my l mz na"># fig 1<br/>plt.subplot(231)<br/>plt.scatter(X,y,c='steelblue', edgecolor='white', s=70,label='data')<br/>plt.plot(X, GradientDescent(eta=0.1, n_iter=100).fit(X,y).predict(X),color='black', lw=2,label='fit')<br/>plt.title('$\eta = 10^{-1}, R^2 = 0.297$ ',size=14)<br/>plt.grid(False)<br/>plt.xlabel('RM (std)',size=14)<br/>plt.ylabel('MEDV (std)',size=14)<br/>plt.legend()</span><span id="e79e" class="mw lk it ms b gy nb my l mz na"># fig 2<br/>plt.subplot(232)<br/>plt.scatter(X,y,c='steelblue', edgecolor='white', s=70)<br/>plt.plot(X, GradientDescent(eta=0.01, n_iter=100).fit(X,y).predict(X),color='black', lw=2)<br/>plt.title('$\eta = 10^{-2},R^2 = 0.312$',size=14)<br/>plt.grid(False)<br/>plt.xlabel('RM (std)',size=14)<br/>plt.ylabel('MEDV (std)',size=14)</span><span id="51d8" class="mw lk it ms b gy nb my l mz na"># fig 3<br/>plt.subplot(233)<br/>plt.scatter(X,y,c='steelblue', edgecolor='white', s=70)<br/>plt.plot(X,GradientDescent(eta=0.001, n_iter=100).fit(X,y).predict(X),color='black', lw=2)<br/>plt.title('$\eta =10^{-3},R^2 = 0.482$',size=14)<br/>plt.grid(False)<br/>plt.xlabel('RM (std)',size=14)<br/>plt.ylabel('MEDV (std)',size=14)</span><span id="5e0c" class="mw lk it ms b gy nb my l mz na"># fig 4<br/>plt.subplot(234)<br/>plt.scatter(X,y,c='steelblue', edgecolor='white', s=70)<br/>plt.plot(X, GradientDescent(eta=0.0001, n_iter=100).fit(X,y).predict(X),color='black', lw=2)<br/>plt.title('$\eta = 10^{-4}, R^2 = 0.483$ ',size=14)<br/>plt.grid(False)<br/>plt.xlabel('RM (std)',size=14)<br/>plt.ylabel('MEDV (std)',size=14)</span><span id="e581" class="mw lk it ms b gy nb my l mz na"># fig 5<br/>plt.subplot(235)<br/>plt.scatter(X,y,c='steelblue', edgecolor='white', s=70)<br/>plt.plot(X, GradientDescent(eta=0.00001, n_iter=100).fit(X,y).predict(X),color='black', lw=2)<br/>plt.title('$\eta = 10^{-5},R^2 = 0.308$',size=14)<br/>plt.grid(False)<br/>plt.xlabel('RM (std)',size=14)<br/>plt.ylabel('MEDV (std)',size=14)</span><span id="ea3f" class="mw lk it ms b gy nb my l mz na"># fig 6<br/>plt.subplot(236)<br/>plt.scatter(X,y,c='steelblue', edgecolor='white', s=70)<br/>plt.plot(X,GradientDescent(eta=0.000001, n_iter=100).fit(X,y).predict(X),color='black', lw=2)<br/>plt.title('$\eta =10^{-6},R^2 = 0.047$',size=14)<br/>plt.grid(False)<br/>plt.xlabel('RM (std)',size=14)<br/>plt.ylabel('MEDV (std)',size=14)</span><span id="522b" class="mw lk it ms b gy nb my l mz na"><br/>plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.4, wspace=0.35)</span><span id="1a64" class="mw lk it ms b gy nb my l mz na">plt.show()</span></pre><p id="d719" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">以下是输出:</p><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/04b9e260ed3e08bf3cb10284378479a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lwils93aT3t9c99l0Fiyww.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk translated"><strong class="bd ll">使用不同学习率参数值的回归分析。</strong></figcaption></figure><h1 id="bc35" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">总论和结论</h1><p id="9a26" class="pw-post-body-paragraph kk kl it km b kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh im bi translated">使用我们的简单回归模型，我们可以看到，我们的模型的可靠性取决于超参数调整。如果我们只是为学习率选择一个随机值，比如eta = 0.1，这将导致一个糟糕的模型。为eta选择一个太小的值，比如eta = 0.00001，也会产生一个不好的模型。我们的分析表明，最佳选择是当eta = 0.0001时，从R平方值可以看出。</p><p id="5aa8" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">一个好的和一个坏的回归分析之间的差异取决于一个人理解模型的所有细节的能力，包括关于不同超参数的知识以及如何调整这些参数以获得具有最佳性能的模型。在没有完全理解模型的错综复杂的情况下，将任何机器学习模型作为黑箱，都会导致模型被证伪。</p><h1 id="9b73" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">参考资料:</h1><ol class=""><li id="9586" class="ni nj it km b kn mh kr mi kv nk kz nl ld nm lh nn no np nq bi translated">《机器学习:使用梯度下降的Python线性回归估计器》，Benjamin o . Tayo(<a class="ae li" href="https://medium.com/@benjaminobi/machine-leaning-python-linear-regression-estimator-using-gradient-descent-b0b2c496e463" rel="noopener">https://medium . com/@ Benjamin bi/Machine-Learning-Python-Linear-Regression-Estimator-Using-Gradient-Descent-b 0b2c 496 e 463</a>)。</li></ol><p id="7d58" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">2.《Python机器学习》，第二版，塞巴斯蒂安·拉什卡。</p><p id="2dc6" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><em class="nr"> 3。UCI机器学习资源库</em>at<em class="nr"/><a class="ae li" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/machine-learning-databases/housing/</a>。</p><p id="902f" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">4.包含本文使用的全部代码的Jupyter notebook可以在这里找到:<a class="ae li" href="https://github.com/bot13956/python-linear-regression-estimator" rel="noopener ugc nofollow" target="_blank">https://github . com/bot 13956/python-linear-regression-estimator</a>。</p></div></div>    
</body>
</html>