<html>
<head>
<title>Let’s learn about Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们来学习降维</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/lets-learn-about-dimensionality-reduction-df4622f30c84?source=collection_archive---------1-----------------------#2021-09-28">https://pub.towardsai.net/lets-learn-about-dimensionality-reduction-df4622f30c84?source=collection_archive---------1-----------------------#2021-09-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6a28" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/5a3819e0464d6da40af8cbe68977df93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XiQR9iuI1iomPscAwUdOug.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@hharritt?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">亨特·哈里特</a>在<a class="ae ko" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><blockquote class="kp kq kr"><p id="6b41" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">什么是维度？</strong></p></blockquote><p id="83b8" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">统计学中的维数指的是“一个数据集有多少属性。”</p><p id="8c7a" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">例如:-我们有电子表格格式的数据，并且我们有大量的变量(年龄、姓名、性别、Id等等..).</p><p id="f7f0" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">简而言之，“数据集的输入变量或特征的数量被称为其维度。”</p><blockquote class="kp kq kr"><p id="ee99" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">为什么要降维？</strong></p></blockquote><p id="7169" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated"><strong class="kv jd">降维</strong>或<strong class="kv jd">降维</strong>，是将数据从高维空间转换到低维空间，使低维表示保留原始数据的一些有意义的属性，在高维空间工作可能出于多种原因不可取；由于维数灾难，原始数据通常是稀疏的，并且分析数据通常在计算上是困难的。降维在处理大量观察值和/或大量变量的领域中很常见，例如信号<a class="ae ko" href="https://en.wikipedia.org/wiki/Signal_processing" rel="noopener ugc nofollow" target="_blank"> </a>处理、语音<a class="ae ko" href="https://en.wikipedia.org/wiki/Speech_recognition" rel="noopener ugc nofollow" target="_blank"> </a>识别、神经信息学和生物信息学。</p><p id="3489" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">在现实世界中，我们会遇到一些问题，真实数据中我们经常会有高达数百万的高维数据，在这种高维结构中，数据代表了数据本身，但有时我们需要降低其维数</p><blockquote class="kp kq kr"><p id="87c7" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">功能多有什么不好？</strong></p></blockquote><p id="877d" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated"><strong class="kv jd">高维数据</strong></p><figure class="lv lw lx ly gt kd gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/a76a52c04cd6ddef9d2a713bb74c564b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*sLOoWxBL3MO7hagW8mYPQg.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="http://vis.pku.edu.cn/hdvis/images/gallery/Jigsaw_13.png" rel="noopener ugc nofollow" target="_blank">http://vis.pku.edu.cn/hdvis/images/gallery/Jigsaw_13.png</a></figcaption></figure><p id="cdf1" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">高维意味着维数很高——高到计算变得极其困难。在高维数据中，特征的数量可以超过观察的数量。</p><p id="beef" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">“在完美的世界中，列数就是数据集的维数。然而，一些列是相似的，一些是相关的，一些在某些方面是重复的，一些是无用的。所以使用所有这些列对于我们的机器学习算法或深度学习算法来建立模型和预测输出没有任何意义。</p><p id="396d" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">更多的输入特征通常使预测建模任务更具建模挑战性，它被称为“维数灾难”。</p><blockquote class="kp kq kr"><p id="cde0" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">维度的诅咒:</strong></p></blockquote><p id="d299" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated"><strong class="kv jd">维度诅咒</strong>指的是在高维空间中分析和组织数据时出现的各种现象，这些现象不会在低维设置中出现，例如日常经验的三维物理空间。这个表达是由Richard E. Bellman在考虑动态编程中的问题时创造的。[维基]</p><p id="b3c5" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">听起来很复杂，对吧？<strong class="kv jd">我们来分解一下</strong></p><p id="e953" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">基本上，维数灾难指的是“误差随着特征数量的增加而增加”,意味着更多的特征趋向于更多的误差，这意味着算法在高维空间中更难设计，并且通常在多维空间中具有指数运行时间</p><blockquote class="kp kq kr"><p id="c68e" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">应用降维的好处</strong></p></blockquote><figure class="lv lw lx ly gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi lz"><img src="../Images/73898d092287a1381c315426d4667f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oycv0Uz4bb9EbcWDWIYgYA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://d3i71xaburhd42.cloudfront.net/8dc7a7af1685d6667d24f013ecc5fceeb2bcc689/250px/7-Figure2-1.png" rel="noopener ugc nofollow" target="_blank">https://d3i 71 xaburhd 42 . cloudfront . net/8d C7 a 7 af 1685d 6667d 24 f 013 ECC 5 fceeb 2 bcc 689/250 px/7-figure 2-1 . png</a></figcaption></figure><ul class=""><li id="221e" class="ma mb it kv b kw kx la lb lr mc ls md lt me lq mf mg mh mi bi translated">通过减少要素的维度，存储数据集所需的空间也会减少。</li><li id="f13f" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">特征维数的减少需要较少的计算训练时间。</li><li id="dfd4" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">数据集要素的降维有助于快速可视化数据集。</li><li id="4786" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">它移除了多余的特征，并在多重共线性中帮助我们。</li></ul><blockquote class="kp kq kr"><p id="93ac" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">降维的缺点</strong></p></blockquote><figure class="lv lw lx ly gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mo"><img src="../Images/a90b35a03b42aedabb8244806c66b076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oy6-Ti0E-YfqVXlIn2IreQ.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">马库斯·温克勒在<a class="ae ko" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="a9e8" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">虽然我们在使用降维方面有很多进步，但也有一些缺点</p><ul class=""><li id="055f" class="ma mb it kv b kw kx la lb lr mc ls md lt me lq mf mg mh mi bi translated">一些数据可能会丢失，我们不知道要保留多少主成分。</li><li id="53fe" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">在降维技术中的一些方法，如PCA，有时需要考虑的主成分是未知的。</li><li id="6e8f" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">此外，PCA在许多情况下会失败，例如，如果均值和协方差不足以定义数据集。</li></ul><blockquote class="kp kq kr"><p id="eba2" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">用于降维的技术:</strong></p></blockquote><p id="2ff6" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">在本文中，我们将学习降维技术的各种方法，因此在接下来的文章中，我们将学习如何使用Python实现它们。</p><blockquote class="kp kq kr"><p id="779c" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">特征选择方法</strong></p></blockquote><ul class=""><li id="3f1c" class="ma mb it kv b kw kx la lb lr mc ls md lt me lq mf mg mh mi bi translated">缺失值比率</li><li id="e2c9" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">低方差滤波器</li><li id="8b0c" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">高相关滤波器</li><li id="38e7" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">随机森林</li><li id="deaf" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">反向特征消除</li><li id="04a7" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">正向特征选择</li></ul><blockquote class="kp kq kr"><p id="d682" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">降维方法</strong></p></blockquote><ul class=""><li id="c204" class="ma mb it kv b kw kx la lb lr mc ls md lt me lq mf mg mh mi bi translated">要素分析</li><li id="cea4" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">主成分分析*</li><li id="1ab6" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">线性判别分析(LDA) *</li><li id="e2e2" class="ma mb it kv b kw mj la mk lr ml ls mm lt mn lq mf mg mh mi bi translated">t分布随机邻居嵌入(t-SNE) *</li></ul><p id="9b52" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">如果这些方法/特性现在对你没有任何意义，不要担心。我将通过实际的实现来解释每一种特征选择和降维的方法</p><p id="0e04" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">暂时就这样了👏👏。下一篇文章再见。</p><blockquote class="kp kq kr"><p id="e264" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">参考:-</p></blockquote><p id="f607" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated"><a class="ae ko" href="https://www.kdnuggets.com/2015/05/7-methods-data-dimensionality-reduction.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2015/05/7-methods-data-dimensionally-reduction . html</a></p><p id="94b5" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated"><a class="ae ko" href="https://analyticsindiamag.com/curse-of-dimensionality-and-what-beginners-should-do-to-overcome-it/" rel="noopener ugc nofollow" target="_blank">https://analyticsindiamag . com/cur-of-dimensionality-and-what-success-it/</a></p><p id="dff5" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated"><strong class="kv jd">在我的YouTube上查看更多有趣的机器学习、深度学习、数据科学项目👉:-</strong><a class="ae ko" href="https://www.youtube.com/c/himanshutripathi" rel="noopener ugc nofollow" target="_blank"><strong class="kv jd">YouTube</strong></a><strong class="kv jd"/>(👍)</p><blockquote class="kp kq kr"><p id="ebfb" class="ks kt ku kv b kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kv jd">看看我以前的文章:</strong></p></blockquote><div class="mp mq gp gr mr ms"><a href="https://medium.com/nerd-for-tech/machine-learning-automation-1c112e099005" rel="noopener follow" target="_blank"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd jd gy z fp mx fr fs my fu fw jc bi translated">机器学习自动化…</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">"仅仅因为你能使某件事自动化，并不意味着它就应该自动化."</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">medium.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng ki ms"/></div></div></a></div><div class="mp mq gp gr mr ms"><a href="https://medium.com/@iamhimanshutripathi0/product-recommendation-based-on-visual-similarity-on-the-web-machine-learning-project-end-to-end-6d38d68d414f" rel="noopener follow" target="_blank"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd jd gy z fp mx fr fs my fu fw jc bi translated">基于网页视觉相似性的产品推荐:机器学习项目…</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">众所周知，谷歌、亚马逊、网飞等大型科技公司都在使用推荐系统…</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">medium.com</p></div></div><div class="nb l"><div class="nh l nd ne nf nb ng ki ms"/></div></div></a></div><div class="mp mq gp gr mr ms"><a href="https://medium.com/datadriveninvestor/natural-langauge-processing-nlp-for-indian-language-hindi-on-web-64d83f16544a" rel="noopener follow" target="_blank"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd jd gy z fp mx fr fs my fu fw jc bi translated">Web上印度语言(印地语)的自然语言处理(NLP)</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">"语言是一个秘密，每个人都可以处理，对我来说，这是美丽的."</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">medium.com</p></div></div><div class="nb l"><div class="ni l nd ne nf nb ng ki ms"/></div></div></a></div><div class="mp mq gp gr mr ms"><a href="https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5" rel="noopener follow" target="_blank"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd jd gy z fp mx fr fs my fu fw jc bi translated">什么是平衡和不平衡数据集？</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">不平衡数据集到平衡数据集的转换技术及其比较</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">medium.com</p></div></div><div class="nb l"><div class="nj l nd ne nf nb ng ki ms"/></div></div></a></div><div class="mp mq gp gr mr ms"><a href="https://medium.com/analytics-vidhya/brain-tumor-classification-transfer-learning-e04f84f96443" rel="noopener follow" target="_blank"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd jd gy z fp mx fr fs my fu fw jc bi translated">基于迁移学习的脑肿瘤分类</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">迁移学习的详细解释以及如何使用它进行分类</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">medium.com</p></div></div><div class="nb l"><div class="nk l nd ne nf nb ng ki ms"/></div></div></a></div><div class="mp mq gp gr mr ms"><a href="https://medium.com/analytics-vidhya/different-type-of-feature-engineering-encoding-techniques-for-categorical-variable-encoding-214363a016fb" rel="noopener follow" target="_blank"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd jd gy z fp mx fr fs my fu fw jc bi translated">用于分类变量编码的不同类型的特征工程编码技术</h2><div class="mz l"><h3 class="bd b gy z fp mx fr fs my fu fw dk translated">“让我们从现有功能中创建新功能。”</h3></div><div class="na l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">medium.com</p></div></div><div class="nb l"><div class="nl l nd ne nf nb ng ki ms"/></div></div></a></div><p id="183d" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated">如果你觉得这篇文章有趣，有帮助，如果你从这篇文章中学到了什么，请鼓掌👏👏)<strong class="kv jd">并留下反馈。</strong></p><p id="e4ca" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated"><strong class="kv jd">我们连线上</strong><a class="ae ko" href="https://www.linkedin.com/in/iamhimanshu0/" rel="noopener ugc nofollow" target="_blank"><strong class="kv jd">Linkedin</strong></a><strong class="kv jd"/><a class="ae ko" href="https://twitter.com/iam_himanshu0" rel="noopener ugc nofollow" target="_blank"><strong class="kv jd">Twitter</strong></a><strong class="kv jd"/><a class="ae ko" href="https://instagram.com/iamhimanshu0/" rel="noopener ugc nofollow" target="_blank"><strong class="kv jd">insta gram</strong></a><strong class="kv jd"/><a class="ae ko" href="https://github.com/iamhimanshu0" rel="noopener ugc nofollow" target="_blank"><strong class="kv jd">Github</strong></a><strong class="kv jd"/><a class="ae ko" href="https://www.facebook.com/iamhimanshu0" rel="noopener ugc nofollow" target="_blank"><strong class="kv jd">脸书</strong> </a> <strong class="kv jd">。</strong></p><p id="eb74" class="pw-post-body-paragraph ks kt it kv b kw kx ky kz la lb lc ld lr lf lg lh ls lj lk ll lt ln lo lp lq im bi translated"><strong class="kv jd">感谢阅读！</strong></p></div></div>    
</body>
</html>