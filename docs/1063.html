<html>
<head>
<title>Detecting Traffic Lights using the Monk Object Detection Library</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Monk对象检测库检测交通灯</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/detecting-traffic-lights-using-the-monk-object-detection-library-1321b113f37d?source=collection_archive---------5-----------------------#2020-10-19">https://pub.towardsai.net/detecting-traffic-lights-using-the-monk-object-detection-library-1321b113f37d?source=collection_archive---------5-----------------------#2020-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c4e3" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="c2f0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">这篇文章是一个如何使用MONK对象检测库的例子。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/af2bef9ca286f03d9fbc4baf04b174c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*R9t67SAWYg_dFfEY"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@crisovalle?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克里斯·奥瓦尔</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="eace" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于这个项目的笔记本访问<a class="ae le" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection/blob/master/application_model_zoo/Example%20-%20Lara%20Traffic%20Lights%20Detection%20Dataset.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="95e9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">欲了解更多关于僧侣对象检测的示例，请访问<a class="ae le" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection/tree/master/application_model_zoo" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><h1 id="ab70" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">关于项目:</h1><p id="bc6e" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">在一个无人驾驶汽车、交通灯🚦会成为过去。但只要人类在旁边行驶，自动驾驶车辆就必须遵循人类制定的规则。这些规则之一是遵守交通信号灯。自动驾驶汽车🚗必须检测和识别交通灯，以避免事故和街道上的混乱。<br/>这是我最近对Tessellate Imaging Monk对象检测库的贡献。期待将其部署到边缘设备。</p><h1 id="8155" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">关于数据集:</h1><p id="400e" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">在此可以查看数据集<a class="ae le" href="http://www.lara.prd.fr/benchmarks/trafficlightsrecognition" rel="noopener ugc nofollow" target="_blank">。数据集文件包含<strong class="lh ja">9168个交通灯实例，手工标记为</strong>。红绿灯细节如下:<strong class="lh ja">3381“绿色”</strong>(叫<em class="my">‘行’</em>)<strong class="lh ja">58“橙色”</strong>(叫<em class="my">‘警告’</em>)<strong class="lh ja">5280“红色”</strong>(叫<em class="my">‘停’</em>)<strong class="lh ja">449“暧昧”。</strong></a></p><h1 id="db53" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">项目目标:</h1><ol class=""><li id="58e6" class="mz na iq lh b li mt ll mu lo nb ls nc lw nd ma ne nf ng nh bi translated">安装<a class="ae le" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection" rel="noopener ugc nofollow" target="_blank">和尚物体检测库</a></li><li id="8e14" class="mz na iq lh b li ni ll nj lo nk ls nl lw nm ma ne nf ng nh bi translated">使用预先训练的模型来检测交通灯。</li><li id="ec6d" class="mz na iq lh b li ni ll nj lo nk ls nl lw nm ma ne nf ng nh bi translated">从头开始训练交通灯检测模型。</li></ol><h2 id="4fa6" class="nn mc iq bd md no np dn mh nq nr dp ml lo ns nt mn ls nu nv mp lw nw nx mr iw bi translated">1.安装僧侣图书馆</h2><p id="5ec6" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">运行这些命令</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="3073" class="nn mc iq nz b gy od oe l of og">!git clone <a class="ae le" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection.git" rel="noopener ugc nofollow" target="_blank">https://github.com/Tessellate-Imaging/Monk_Object_Detection.git</a></span><span id="9c62" class="nn mc iq nz b gy oh oe l of og">!cd Monk_Object_Detection/16_mmdet/installation</span></pre><p id="cbe9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">选择正确的文件并运行</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="076b" class="nn mc iq nz b gy od oe l of og">!chmod +x install.sh &amp;&amp; ./install.sh</span></pre><h2 id="fa3a" class="nn mc iq bd md no np dn mh nq nr dp ml lo ns nt mn ls nu nv mp lw nw nx mr iw bi translated">2.使用预先训练的模型来检测交通灯。</h2><p id="8a49" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">要使用预先训练的模型在测试图像上加载和运行推理，请遵循以下代码。</p><p id="e6af" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">导入MONK对象检测并创建一个推断类的对象。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="0401" class="nn mc iq nz b gy od oe l of og">import os<br/>import sys<br/>sys.path.append("Monk_Object_Detection/16_mmdet/lib")</span><span id="8a7d" class="nn mc iq nz b gy oh oe l of og">from infer_engine import Infer<br/>gtf = Infer();</span></pre><p id="fa6e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从驱动器下载预先训练好的模型并解压。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="d04f" class="nn mc iq nz b gy od oe l of og">! wget --load-cookies /tmp/cookies.txt "<a class="ae le" href="https://docs.google.com/uc?export=download&amp;confirm=$(wget" rel="noopener ugc nofollow" target="_blank">https://docs.google.com/uc?export=download&amp;confirm=$(wget</a> --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate '<a class="ae le" href="https://docs.google.com/uc?export=download&amp;id=1XLSAp7YAYBY8xwKURiehj_uerHTJeq5n'" rel="noopener ugc nofollow" target="_blank">https://docs.google.com/uc?export=download&amp;id=1XLSAp7YAYBY8xwKURiehj_uerHTJeq5n'</a> -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&amp;id=1XLSAp7YAYBY8xwKURiehj_uerHTJeq5n" -O obj_traffic2_trained.zip &amp;&amp; rm -rf /tmp/cookies.txt</span><span id="f8b9" class="nn mc iq nz b gy oh oe l of og">! unzip -qq obj_traffic2_trained.zip</span></pre><p id="82a3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">加载模型参数。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="4b20" class="nn mc iq nz b gy od oe l of og">gtf.Model_Params("work_dirs/config_updated/config_updated.py", <br/>                 "work_dirs/config_updated/latest.pth")</span></pre><p id="0a3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对您想要的图像运行Predict()函数。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="65e5" class="nn mc iq nz b gy od oe l of og">result = gtf.Predict(img_path="Test_images/test1.jpg",<br/>           out_img_path="result.jpg",<br/>           thresh=0.8);<br/>from IPython.display import Image<br/>Image(filename='result.jpg')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/0305a880e1b2c67c014eb2d992afc624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qt4YtIQs1l-6KRFgVZ1Pjw.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/2ef6f0f00e2ccccf6de4822936ba013f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1DJGkAgMOaosEV91c64ZTQ.png"/></div></div></figure><h1 id="ba57" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">从头开始训练交通灯检测模型</h1><h2 id="2887" class="nn mc iq bd md no np dn mh nq nr dp ml lo ns nt mn ls nu nv mp lw nw nx mr iw bi translated">涉及的步骤</h2><ol class=""><li id="f069" class="mz na iq lh b li mt ll mu lo nb ls nc lw nd ma ne nf ng nh bi translated">下载数据集</li><li id="e9e4" class="mz na iq lh b li ni ll nj lo nk ls nl lw nm ma ne nf ng nh bi translated">将CVML标注格式转换为COCO格式</li><li id="ae16" class="mz na iq lh b li ni ll nj lo nk ls nl lw nm ma ne nf ng nh bi translated">使用<a class="ae le" href="https://github.com/open-mmlab/mmdetection" rel="noopener ugc nofollow" target="_blank"> MMDet </a>训练模型</li></ol><h2 id="60d8" class="nn mc iq bd md no np dn mh nq nr dp ml lo ns nt mn ls nu nv mp lw nw nx mr iw bi translated">1.下载数据集</h2><p id="d07f" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">数据集学分:<a class="ae le" href="http://www.lara.prd.fr/benchmarks/trafficlightsrecognition" rel="noopener ugc nofollow" target="_blank">红绿灯识别(TLR)公共基准</a>。使用下面的代码数据集可以下载到你的机器上。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="b1c7" class="nn mc iq nz b gy od oe l of og">!wget <a class="ae le" href="http://s150102174.onlinehome.fr/Lara/files/Lara_UrbanSeq1_JPG.zip" rel="noopener ugc nofollow" target="_blank">http://s150102174.onlinehome.fr/Lara/files/Lara_UrbanSeq1_JPG.zip</a></span></pre><p id="9753" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">解压缩数据集:</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="fe18" class="nn mc iq nz b gy od oe l of og">!unzip Lara_UrbanSeq1_JPG.zip</span></pre><h2 id="5e02" class="nn mc iq bd md no np dn mh nq nr dp ml lo ns nt mn ls nu nv mp lw nw nx mr iw bi translated">2.转换CVML到可可格式。</h2><p id="1d6d" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">这一步是必需的，因为数据集的标注是CVML格式，因此需要转换为COCO格式。</p><p id="f0dc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">更多详情请参考本博客:<a class="ae le" href="https://medium.com/towards-artificial-intelligence/cvml-annotation-what-it-is-and-how-to-convert-it-7b818dc30c9f" rel="noopener"> CVML注解——什么是注解，如何转换？</a></p><p id="5ea3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">将CVML转换为僧侣类型的代码</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="08f6" class="nn mc iq nz b gy od oe l of og">import os<br/>import sys<br/>import numpy as np<br/>import pandas as pd<br/>import xmltodict<br/>import json<br/>from tqdm.notebook import tqdm<br/>import collections<br/>from pycocotools.coco import COCO</span><span id="a473" class="nn mc iq nz b gy oh oe l of og">img_dir = "Lara3D_UrbanSeq1_JPG/";</span><span id="d069" class="nn mc iq nz b gy oh oe l of og">annoFile="Lara_UrbanSeq1_GroundTruth_cvml.xml"<br/>f = open(annoFile, 'r');<br/>my_xml = f.read();<br/>anno = dict(dict(xmltodict.parse(my_xml))["dataset"])</span><span id="82de" class="nn mc iq nz b gy oh oe l of og">combined=[]<br/>count=0;<br/>for frame in tqdm(anno['frame']):<br/>    fname=file_content[count].strip()<br/>    count+=1<br/>    label_str = "";<br/>    width=640<br/>    height=480<br/>    if(type(frame["objectlist"]) ==collections.OrderedDict):<br/>        if(type(frame["objectlist"]['object']) == list):<br/>            for j,i in enumerate(frame['objectlist']['object']):<br/>                x1=max(int(i['box']['<a class="ae le" href="http://twitter.com/xc" rel="noopener ugc nofollow" target="_blank">@xc</a>'])-int(i['box']['<a class="ae le" href="http://twitter.com/w" rel="noopener ugc nofollow" target="_blank">@w</a>'])/2,0)<br/>                y1=max(int(i['box']['<a class="ae le" href="http://twitter.com/yc" rel="noopener ugc nofollow" target="_blank">@yc</a>'])-int(i['box']['<a class="ae le" href="http://twitter.com/h" rel="noopener ugc nofollow" target="_blank">@h</a>'])/2,0)<br/>                x2=min(int(i['box']['<a class="ae le" href="http://twitter.com/xc" rel="noopener ugc nofollow" target="_blank">@xc</a>'])+int(i['box']['<a class="ae le" href="http://twitter.com/w" rel="noopener ugc nofollow" target="_blank">@w</a>'])/2,width)<br/>                y2=min(int(i['box']['<a class="ae le" href="http://twitter.com/yc" rel="noopener ugc nofollow" target="_blank">@yc</a>'])+int(i['box']['<a class="ae le" href="http://twitter.com/h" rel="noopener ugc nofollow" target="_blank">@h</a>'])/2,height)<br/>                label=i['hypothesislist']['hypothesis']['subtype']['#text']<br/>                label_str+=str(x1)+" "+str(y1)+" "+str(x2)+" "+str(y2)+" "+label+" "</span><span id="ab68" class="nn mc iq nz b gy oh oe l of og">else:<br/>            x1=max(0,int(frame["objectlist"]['object']['box']['<a class="ae le" href="http://twitter.com/xc" rel="noopener ugc nofollow" target="_blank">@xc</a>'])-int(frame["objectlist"]['object']['box']['<a class="ae le" href="http://twitter.com/w" rel="noopener ugc nofollow" target="_blank">@w</a>'])/2)<br/>            y1=max(0,int(frame["objectlist"]['object']['box']['<a class="ae le" href="http://twitter.com/yc" rel="noopener ugc nofollow" target="_blank">@yc</a>'])-int(frame["objectlist"]['object']['box']['<a class="ae le" href="http://twitter.com/h" rel="noopener ugc nofollow" target="_blank">@h</a>'])/2)<br/>            x2=min(width,int(frame["objectlist"]['object']['box']['<a class="ae le" href="http://twitter.com/xc" rel="noopener ugc nofollow" target="_blank">@xc</a>'])+int(frame["objectlist"]['object']['box']['<a class="ae le" href="http://twitter.com/w" rel="noopener ugc nofollow" target="_blank">@w</a>'])/2)<br/>            y2=min(height,int(frame["objectlist"]['object']['box']['<a class="ae le" href="http://twitter.com/yc" rel="noopener ugc nofollow" target="_blank">@yc</a>'])+int(frame["objectlist"]['object']['box']['<a class="ae le" href="http://twitter.com/h" rel="noopener ugc nofollow" target="_blank">@h</a>'])/2)<br/>            label=frame["objectlist"]['object']['hypothesislist']['hypothesis']['subtype']['#text']<br/>            label_str += str(x1)+" "+str(y1)+" "+str(x2)+" "+str(y2)+" " + label<br/>        <br/>    combined.append([fname,label_str.strip()])</span><span id="1fb2" class="nn mc iq nz b gy oh oe l of og">df = pd.DataFrame(combined, columns = ['ID', 'Label']);<br/>df.to_csv("train_labels.csv", index=False);</span></pre><p id="61a2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">和尚到可可类型</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="f7bc" class="nn mc iq nz b gy od oe l of og">import os<br/>import numpy as np <br/>import cv2<br/>import dicttoxml<br/>import xml.etree.ElementTree as ET<br/>from xml.dom.minidom import parseString<br/>from tqdm import tqdm<br/>import shutil<br/>import json<br/>import pandas as pd</span><span id="befc" class="nn mc iq nz b gy oh oe l of og">root = "./";<br/>img_dir = "Lara3D_UrbanSeq1_JPG/";<br/>anno_file = "train_labels.csv";</span><span id="2133" class="nn mc iq nz b gy oh oe l of og">dataset_path = root;<br/>images_folder = root + "/" + img_dir;<br/>annotations_path = root + "/annotations/";</span><span id="7961" class="nn mc iq nz b gy oh oe l of og">if not os.path.isdir(annotations_path):<br/>    os.mkdir(annotations_path)<br/>    <br/>input_images_folder = images_folder;<br/>input_annotations_path = root + "/" + anno_file;</span><span id="5360" class="nn mc iq nz b gy oh oe l of og">output_dataset_path = root;<br/>output_image_folder = input_images_folder;<br/>output_annotation_folder = annotations_path;</span><span id="7147" class="nn mc iq nz b gy oh oe l of og">tmp = img_dir.replace("/", "");<br/>output_annotation_file = output_annotation_folder + "/instances_" + tmp + ".json";<br/>output_classes_file = output_annotation_folder + "/classes.txt";</span><span id="2183" class="nn mc iq nz b gy oh oe l of og">if not os.path.isdir(output_annotation_folder):<br/>    os.mkdir(output_annotation_folder);</span><span id="c8f7" class="nn mc iq nz b gy oh oe l of og">df = pd.read_csv(input_annotations_path);<br/>columns = df.columns</span><span id="e378" class="nn mc iq nz b gy oh oe l of og">delimiter = " ";</span><span id="6e10" class="nn mc iq nz b gy oh oe l of og">list_dict = [];<br/>anno = [];<br/>for i in range(len(df)):<br/>    img_name = df[columns[0]][i];<br/>    labels = df[columns[1]][i];<br/>    tmp = str(labels).split(delimiter);<br/>    for j in range(len(tmp)//5):<br/>        label = tmp[j*5+4];<br/>        if(label not in anno):<br/>            anno.append(label);<br/>    anno = sorted(anno)<br/>    <br/>for i in tqdm(range(len(anno))):<br/>    tmp = {};<br/>    tmp["supercategory"] = "master";<br/>    tmp["id"] = i;<br/>    tmp["name"] = anno[i];<br/>    list_dict.append(tmp);</span><span id="7051" class="nn mc iq nz b gy oh oe l of og">anno_f = open(output_classes_file, 'w');<br/>for i in range(len(anno)):<br/>    anno_f.write(anno[i] + "\n");<br/>anno_f.close();</span><span id="3be4" class="nn mc iq nz b gy oh oe l of og">coco_data = {};<br/>coco_data["type"] = "instances";<br/>coco_data["images"] = [];<br/>coco_data["annotations"] = [];<br/>coco_data["categories"] = list_dict;<br/>image_id = 0;<br/>annotation_id = 0;</span><span id="5eee" class="nn mc iq nz b gy oh oe l of og">for i in tqdm(range(len(df))):<br/>    img_name = df[columns[0]][i];<br/>    labels = df[columns[1]][i];<br/>    tmp = str(labels).split(delimiter);<br/>    image_in_path = input_images_folder + "/" + img_name;<br/>    img = cv2.imread(image_in_path, 1);<br/>    h, w, c = img.shape;</span><span id="7f25" class="nn mc iq nz b gy oh oe l of og">images_tmp = {};<br/>    images_tmp["file_name"] = img_name;<br/>    images_tmp["height"] = h;<br/>    images_tmp["width"] = w;<br/>    images_tmp["id"] = image_id;<br/>    coco_data["images"].append(images_tmp);</span><span id="022d" class="nn mc iq nz b gy oh oe l of og">for j in range(len(tmp)//5):<br/>        x1 = float(tmp[j*5+0]);<br/>        y1 = float(tmp[j*5+1]);<br/>        x2 = float(tmp[j*5+2]);<br/>        y2 = float(tmp[j*5+3]);<br/>        label = tmp[j*5+4];<br/>        annotations_tmp = {};<br/>        annotations_tmp["id"] = annotation_id;<br/>        annotation_id += 1;<br/>        annotations_tmp["image_id"] = image_id;<br/>        annotations_tmp["segmentation"] = [];<br/>        annotations_tmp["ignore"] = 0;<br/>        annotations_tmp["area"] = (x2-x1)*(y2-y1);<br/>        annotations_tmp["iscrowd"] = 0;<br/>        annotations_tmp["bbox"] = [x1, y1, x2-x1, y2-y1];<br/>        annotations_tmp["category_id"] = anno.index(label);</span><span id="a7e7" class="nn mc iq nz b gy oh oe l of og">coco_data["annotations"].append(annotations_tmp)<br/>    image_id += 1;</span><span id="dacd" class="nn mc iq nz b gy oh oe l of og">outfile =  open(output_annotation_file, 'w');<br/>json_str = json.dumps(coco_data, indent=4);<br/>outfile.write(json_str);<br/>outfile.close();</span></pre><h1 id="b987" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">2.培训模式</h1><p id="55d6" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">加载monk并创建一个检测器类的对象。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="de8b" class="nn mc iq nz b gy od oe l of og">import os<br/>import sys<br/>sys.path.append(“Monk_Object_Detection/16_mmdet/lib”)</span><span id="328d" class="nn mc iq nz b gy oh oe l of og">from train_engine import Detector<br/>gtf = Detector();</span></pre><p id="9793" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">查找所需的文件。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="d367" class="nn mc iq nz b gy od oe l of og">img_dir = "Lara3D_UrbanSeq1_JPG";<br/>annofile = "annotations/instances_Lara3D_UrbanSeq1_JPG.json"<br/>class_file = "annotations/classes.txt"</span></pre><p id="b9d9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">加载数据集、给定数据集参数、加载模型架构以及调整超参数:</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="015d" class="nn mc iq nz b gy od oe l of og">gtf.Train_Dataset(img_dir, annofile, class_file);<br/>gtf.Dataset_Params(batch_size=8, num_workers=2);<br/>gtf.Model_Params(model_name="faster_rcnn_fpn50");<br/>gtf.Hyper_Params(lr=0.02, momentum=0.9, weight_decay=0.0001);</span></pre><p id="c32f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">开始培训:</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="9aae" class="nn mc iq nz b gy od oe l of og">gtf.Training_Params(num_epochs=50, val_interval=1);<br/>gtf.Train();</span></pre><h2 id="d30d" class="nn mc iq bd md no np dn mh nq nr dp ml lo ns nt mn ls nu nv mp lw nw nx mr iw bi translated">3.对测试图像进行推理</h2><p id="b140" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">要使用预先训练的模型在测试图像上加载和运行推理，请遵循以下代码。</p><p id="ef83" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">导入MONK对象检测并创建一个推断类的对象。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="5836" class="nn mc iq nz b gy od oe l of og">import os<br/>import sys<br/>sys.path.append(“Monk_Object_Detection/16_mmdet/lib”)</span><span id="8160" class="nn mc iq nz b gy oh oe l of og">from infer_engine import Infer<br/>gtf=Infer();</span></pre><p id="13f6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">加载模型参数。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="4f52" class="nn mc iq nz b gy od oe l of og">gtf.Model_Params("work_dirs/config_updated/config_updated.py", <br/>                 "work_dirs/config_updated/latest.pth")</span></pre><p id="80b5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对您想要的图像运行Predict()函数。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="5c75" class="nn mc iq nz b gy od oe l of og">result = gtf.Predict(img_path="Test_images/test1.jpg",<br/>           out_img_path="result.jpg",<br/>           thresh=0.8);<br/>from IPython.display import Image<br/>Image(filename='result.jpg')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/0305a880e1b2c67c014eb2d992afc624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qt4YtIQs1l-6KRFgVZ1Pjw.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/2ef6f0f00e2ccccf6de4822936ba013f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1DJGkAgMOaosEV91c64ZTQ.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/ab523682207907cd22c025f0d67798ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b3SzmagRDQq814-fIIWCyw.png"/></div></div></figure><p id="f88b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如需更多测试图片，或查看完整笔记本，请访问<a class="ae le" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection/blob/master/application_model_zoo/Example%20-%20Lara%20Traffic%20Lights%20Detection%20Dataset.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><h1 id="f85a" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">结论</h1><ol class=""><li id="fa93" class="mz na iq lh b li mt ll mu lo nb ls nc lw nd ma ne nf ng nh bi translated">Monk library让学生、研究人员和竞争对手可以非常轻松地创建深度学习模型，并尝试不同的超参数调整来提高模型的准确性。</li><li id="dc49" class="mz na iq lh b li ni ll nj lo nk ls nl lw nm ma ne nf ng nh bi translated">预先训练的模型可以直接下载使用，而无需进入模型创建部分。</li><li id="7b7c" class="mz na iq lh b li ni ll nj lo nk ls nl lw nm ma ne nf ng nh bi translated">使用Monk使这个过程变得简单，耗时更少。</li></ol><h1 id="1f16" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">感谢阅读。</h1><p id="66e6" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">如果你读到这里，请为这篇文章鼓掌。还有，在<a class="ae le" href="https://www.linkedin.com/in/rohit96/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上和我联系。</p></div></div>    
</body>
</html>