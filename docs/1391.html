<html>
<head>
<title>Fully Explained Logistic Regression with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python全面解释逻辑回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=collection_archive---------0-----------------------#2021-01-16">https://pub.towardsai.net/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=collection_archive---------0-----------------------#2021-01-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="29ae" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="d17f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">机器学习算法中的统计非线性方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/9f9c5ff15492510e60c1a648c083b3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FpMomUQRNBvPX-GZ377Vw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">线性和非线性图。作者的照片</figcaption></figure><p id="8caf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们从线性回归模型切换到非线性回归模型的原因是因为输出特征变量。上周学习线性回归时，我得到了一个因变量有类别的数据集。</p><p id="3db5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这篇文章中，我们将讨论有关逻辑回归的基本概念，并了解我们将如何最大似然估计和日志(赔率)。很好的理解是非常重要的，它会节省我们很多时间。</p><div class="md me gp gr mf mg"><a href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd jd gy z fp ml fr fs mm fu fw jc bi translated">用Python全面解释线性回归</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">如何用一个真实的例子解决回归问题。</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">medium.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu lb mg"/></div></div></a></div><p id="d4df" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，我们需要知道为什么线性回归不适合数据的类别。从下图中，我们观察到第一个是线性回归，第二个也是线性的，但有二进制类别值。从这两幅图中我们可以看出，第一幅图的值呈线性逼近，即自变量增加，因变量也增加。但是，第二张图没有说明这种类型的行为，而是仅在两个值(即“0”和“1”)上标出了因变量的值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/7fafa49258a21b8ef339e873a51972db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*ljIljgqILHWqh00xUx07kQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">不同类型因变量的线性逼近。作者的照片</figcaption></figure><p id="05d3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们对第二个值使用线性方法，误差率将会增加，我们的模型将不会很好地拟合，还有一点需要注意，线性线在我们不需要预测的数据点的上方和下方。因此，我们需要一种方法，在这种方法中，预测将只在“0”和“1”中。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/9564fb80ead751b0c9ca74867582fe28.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*U6eXRwv9GAZzVOelB8q4ag.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">逻辑回归曲线。作者的照片</figcaption></figure><p id="18f6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从这个思路出发，我们可以思考概率值在“0”到“1”范围内的概率。好吧，但是我们也需要改变我们的预测线。许多函数基于某个阈值给出“0”和“1”的值。这条曲线可以称为逻辑回归曲线或逻辑函数。</p><p id="769a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">logit回归模型如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/eac3c4e176eef796758ead1490380d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*JNIYBgEOTwkBYEqvelwYOw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">逻辑模型。照片来自sphweb.bumc.bu.edu<a class="ae my" href="https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704-ep713_multivariablemethods/BS704-EP713_MultivariableMethods4.html#headingtaglink_1" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="228b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所以，概率的对数等于线性模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/71237157da19f5a5629cf6307982c75f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*n22AnaRFvaS4fQP8UzcCjQ.png"/></div></figure><p id="29ad" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">logit函数比普通的逻辑函数更容易理解。所以，这个函数只不过是给出“0”和“1”中的值的sigmoid函数。</p><p id="d154" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当我们试图拟合我们的模型时，它会在内部计算迭代和函数值。这两个词的意思都是，在多次迭代之后，模型优化将不起作用，在函数值中获得的值是目标函数的值，通过该值我们得到收敛。</p><div class="md me gp gr mf mg"><a href="https://medium.com/towards-artificial-intelligence/fundamentals-of-series-and-data-frame-in-pandas-with-python-6e0b8a168a0d" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd jd gy z fp ml fr fs mm fu fw jc bi translated">用python实现熊猫系列和数据框架的基础</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">数据框中常用参数的基础知识</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">medium.com</p></div></div><div class="mp l"><div class="na l mr ms mt mp mu lb mg"/></div></div></a></div><p id="6a36" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在让我们用python做一些实际的事情。</p><p id="210f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们创建了一个小数据集来解释逻辑回归中二进制输出的分类方法。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="ebbb" class="ng nh it nc b gy ni nj l nk nl">#importing the libraries<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import warnings<br/>warnings.filterwarnings("ignore")</span></pre><p id="e83b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在读取excel文件并查看它的前5行。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="178f" class="ng nh it nc b gy ni nj l nk nl">df = pd.read_excel("logistic.xlsx")<br/>df.head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/cd005fdda036c7ea8f0670ea05ad124c.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*1OhV0jgfgoR_58kJwXeIeg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">数据集的前5行。作者的照片</figcaption></figure><p id="ada9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">将数据集分成独立变量和因变量。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="afd1" class="ng nh it nc b gy ni nj l nk nl">x = df.iloc[:,[0,1]].values<br/>y = df.iloc[:,2].values</span></pre><p id="4af6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，将数据分为训练和测试数据。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="081b" class="ng nh it nc b gy ni nj l nk nl">from sklearn.model_selection import train_test_split <br/>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size =<br/>                                           0.25, random_state = 0)</span></pre><p id="8db7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">标准化数据，使数字的变化成为正常。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="4a2f" class="ng nh it nc b gy ni nj l nk nl">from sklearn.preprocessing import StandardScaler <br/>sc_x = StandardScaler() <br/>x_train = sc_x.fit_transform(x_train)  <br/>x_test = sc_x.transform(x_test)</span></pre><p id="db07" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使训练集适合模型。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="aa3e" class="ng nh it nc b gy ni nj l nk nl">from sklearn.linear_model import LogisticRegression <br/>classifier = LogisticRegression(random_state = 0) <br/>classifier.fit(x_train, y_train)</span></pre><p id="9450" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用分类器进行预测。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="38e1" class="ng nh it nc b gy ni nj l nk nl">y_pred = classifier.predict(x_test)</span></pre><p id="31ed" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">生成混淆矩阵。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="050a" class="ng nh it nc b gy ni nj l nk nl">from sklearn.metrics import confusion_matrix <br/>conf_matrix = confusion_matrix(y_test, y_pred) <br/>  <br/>print ("Confusion Matrix : \n", conf_matrix)</span><span id="3029" class="ng nh it nc b gy nn nj l nk nl">#output:<br/>Confusion Matrix : <br/> [[10  0]<br/> [ 0 10]]</span></pre><p id="4c6e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">检查逻辑模型的准确性。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="be25" class="ng nh it nc b gy ni nj l nk nl">from sklearn.metrics import accuracy_score <br/>print ("Accuracy : ", accuracy_score(y_test, y_pred))</span><span id="9492" class="ng nh it nc b gy nn nj l nk nl">#output:<br/>Accuracy :  1.0</span></pre><p id="d62e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">绘制二元分类模型。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="c9a4" class="ng nh it nc b gy ni nj l nk nl">from matplotlib.colors import ListedColormap <br/>X_set, y_set = x_test, y_test <br/>X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1,  <br/>                               stop = X_set[:, 0].max() + 1, step = 0.01), <br/>                     np.arange(start = X_set[:, 1].min() - 1,  <br/>                               stop = X_set[:, 1].max() + 1, step = 0.01)) <br/>  <br/>plt.contourf(X1, X2, classifier.predict( <br/>             np.array([X1.ravel(), X2.ravel()]).T).reshape( <br/>             X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green'))) <br/>  <br/>plt.xlim(X1.min(), X1.max()) <br/>plt.ylim(X2.min(), X2.max()) <br/>  <br/>for i, j in enumerate(np.unique(y_set)): <br/>    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], <br/>                c = ListedColormap(('red', 'green'))(i), label = j) <br/>      <br/>plt.title('Classifier (Test set)') <br/>plt.xlabel('Age') <br/>plt.ylabel('Bank Saving') <br/>plt.legend() <br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/31afe12c8535ff976871f466f3fc3bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*n7gfAQA16ZqrKtq7amw0Qw.png"/></div></figure><p id="7c79" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">统计模型的逻辑回归。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="4ee5" class="ng nh it nc b gy ni nj l nk nl">import statsmodels.api as sm</span></pre><p id="5119" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">符合逻辑回归</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="b27c" class="ng nh it nc b gy ni nj l nk nl">x1 = sm.add_constant(x)<br/>log_reg = sm.logit(y,x1)<br/>log_output = log_reg.fit()</span></pre><p id="c2dd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在检查统计模型的摘要。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="a367" class="ng nh it nc b gy ni nj l nk nl">log_output.summary()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/043d3505949e61626abfdf1d355d317f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*WufTwfOZL1YFTKCg0AtyWg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">逻辑模型摘要的一部分。作者的照片</figcaption></figure><p id="93d7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这个逻辑总结中，我们有伪R-square。一般来说，我们有一些像AIC，BIC和麦克法登的R平方。在此拟合中，它使用了麦克法登，其值为0.3458。良好伪r平方的良好范围值在0.2到0.4值之间。logit模型如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4e2960ea55c8b8a897a36c0600bedd69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*5UIU616a7-jYQDhztEMpCA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">Logit模型。作者的照片</figcaption></figure><p id="0c64" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们用logit模型创建了逻辑回归的一般模型。</p><blockquote class="nr ns nt"><p id="b22d" class="lh li nu lj b lk ll kd lm ln lo kg lp nv lr ls lt nw lv lw lx nx lz ma mb mc im bi translated"><strong class="lj jd"> <em class="it">结论:</em> </strong></p></blockquote><p id="a880" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">本文展示了在二元分类问题中使用逻辑回归的基本思想。结果值可能因数据集和运行模型的机器速度而异。</p><p id="012f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae my" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae my" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="4c47" class="ny nh it bd nz oa ob oc od oe of og oh ki oi kj oj kl ok km ol ko om kp on oo bi translated">推荐文章</h1><ol class=""><li id="6fd5" class="op oq it lj b lk or ln os lq ot lu ou ly ov mc ow ox oy oz bi translated"><a class="ae my" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> NLP —用Python从零到英雄</a></li></ol><p id="bc5e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">2.<a class="ae my" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a></p><p id="9946" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">3.<a class="ae my" href="https://medium.com/towards-artificial-intelligence/mysql-zero-to-hero-with-syntax-of-all-topics-92e700762c7b?source=friends_link&amp;sk=35a3f8dc1cf1ebd1c4d5008a5d12d6a3" rel="noopener"> MySQL:零到英雄</a></p><p id="73b9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">4.<a class="ae my" href="https://medium.com/towards-artificial-intelligence/basic-of-time-series-with-python-a2f7cb451a76?source=friends_link&amp;sk=09d77be2d6b8779973e41ab54ebcf6c5" rel="noopener">Python时间序列基础</a></p><p id="cfb6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">5.<a class="ae my" href="https://medium.com/towards-artificial-intelligence/numpy-zero-to-hero-with-python-d135f57d6082?source=friends_link&amp;sk=45c0921423cdcca2f5772f5a5c1568f1" rel="noopener"> NumPy:用Python零到英雄</a></p></div></div>    
</body>
</html>