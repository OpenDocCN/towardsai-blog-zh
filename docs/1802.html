<html>
<head>
<title>MorphNet is a Google Model to Build Faster and Smaller Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MorphNet是Google的一个模型，用来建立更快更小的神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/morphnet-is-a-google-model-to-build-faster-and-smaller-neural-networks-f890276da456?source=collection_archive---------0-----------------------#2021-04-30">https://pub.towardsai.net/morphnet-is-a-google-model-to-build-faster-and-smaller-neural-networks-f890276da456?source=collection_archive---------0-----------------------#2021-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f960" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="56f8" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">该模型在优化神经网络结构方面取得了进展。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/5afd3b3a1cecb1d101597eb14a61ab85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*51JhNmQI6Ft_EObIpwTZ0w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://blog.usejournal.com/compress-optimize-your-deep-neural-network-with-pruning-97a010321637" rel="noopener ugc nofollow" target="_blank">https://blog . use journal . com/compress-optimize-your-deep-neural-network-with-pruning-97a 010321637</a></figcaption></figure><blockquote class="li lj lk"><p id="364b" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过80，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到85，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="6a2e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">如今设计深度神经网络更像是艺术而非科学。在深度学习领域，任何给定的问题都可以通过大量的神经网络架构来解决。从这个意义上说，为一个给定的问题从头开始设计一个深度神经网络，在时间和计算资源方面可能会非常昂贵。此外，由于在这个领域缺乏指导，我们经常最终产生对于手头的任务来说不太理想的神经网络架构。大约两年前，来自谷歌<a class="ae lh" href="https://arxiv.org/pdf/1711.06798.pdf" rel="noopener ugc nofollow" target="_blank">的人工智能(AI)研究人员发表了一篇论文，提出了一种叫做MorphNet </a>的方法来优化深度神经网络的设计。</p><p id="a238" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">自动神经网络设计是深度学习领域中最活跃的研究领域之一。神经网络架构设计的最传统的方法涉及使用诸如L1的方法的稀疏正则化器。虽然这种技术已被证明在减少神经网络中的连接数量方面是有效的，但经常最终产生次优的架构。另一种方法涉及使用搜索技术来为给定的问题找到最佳的神经网络结构。该方法已经能够产生高度优化的神经网络结构，但是它需要大量的反复试验，这经常导致计算上被禁止。因此，神经网络架构搜索只在非常特殊的情况下才被证明是有效的。考虑到前面方法的局限性，我们可以得出有效的自动神经网络设计技术的三个关键特征:</p><p id="bea5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">a) <strong class="lo jd">可扩展性:</strong>自动化设计方法应该可扩展到大型数据集和模型。</p><p id="db53" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">b) <strong class="lo jd">多因素优化:</strong>自动化的方法应该能够优化针对特定资源的深度神经网络的结构。</p><p id="07b2" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">c) <strong class="lo jd">最优:</strong>一个自动化的神经网络设计应该产生一个提高性能同时减少目标资源使用的架构。</p><h1 id="3361" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">MorphNet</h1><p id="c238" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">Google的MorphNet从一个稍微不同的角度探讨了自动化神经网络架构设计的问题。MorphNet没有尝试在一个大的设计空间中使用多种架构，而是针对类似的问题从一个现有的架构开始，然后针对手头的任务进行优化。</p><p id="25c1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">MorphNet通过交互式收缩和扩展其结构来优化深度神经网络。在收缩阶段，MorphNet识别低效神经元，并通过应用<a class="ae lh" href="https://en.wikipedia.org/wiki/Structured_sparsity_regularization" rel="noopener ugc nofollow" target="_blank">稀疏正则化器</a>将它们从网络中删除，这样网络的总损失函数包括每个神经元的成本。仅仅这样做通常会导致神经网络消耗更少的目标资源，但是通常会实现较低的性能。然而，MorphNet应用了一个特定的收缩模型，该模型不仅突出了神经网络的哪些层是过度参数化的，而且还突出了哪些层是瓶颈。MorphNet不是对每个神经元应用统一的成本，而是计算相对于目标资源的神经元成本。随着训练的进行，优化器在计算梯度时会意识到资源成本，从而了解哪些神经元是资源高效的，哪些神经元可以被移除。</p><p id="4a9d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">MorphNet的收缩阶段对于产生优化特定资源成本的神经网络是有用的。然而，这种优化可能会以准确性为代价。这就是为什么MorphNet使用基于宽度乘数的扩展阶段来扩展所有图层的大小。例如，50%的扩展将导致效率低下的层，该层从100个神经元开始并收缩到10个，将仅扩展回15个，而仅收缩到80个神经元的重要层可能扩展到120个并具有更多的资源来工作。净效应是将计算资源从效率较低的网络部分重新分配到它们可能更有用的网络部分。</p><p id="c151" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">收缩阶段和扩展阶段的组合产生了比原始神经网络更精确的神经网络，同时对于特定的资源仍有所优化。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/ea5beb7a4b1a9201c6bd87b81634b8f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AY3cBUggbC_KLt1JqDNACQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://arxiv.org/pdf/1711.06798.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.06798.pdf</a></figcaption></figure><p id="77e1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在最初的迭代中，MorphNet可以在几个领域提供神经网络架构的即时价值。</p><p id="bb2c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd">目标正则化:</strong> MorphNet优化深度神经网络的结构，专注于特定资源的缩减。从概念上讲，该模型提供了比传统正则化技术更有针对性的方法。下图显示了MorphNet使用两个标准优化的传统RestNet-101架构:FLOPs和模型大小。MorphNet针对FLOPs(中间，FLOPs减少40%)或model size(右侧，重量减少43%)生成的结构有很大不同。当针对计算成本进行优化时，网络较低层中的较高分辨率神经元往往比较高层中的较低分辨率神经元被修剪得更多。当目标是较小的模型尺寸时，修剪权衡是相反的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f36217de0676a32f5951a65f6d0ba344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*09tucrXF9EcrQjbOQRHOhw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:https://arxiv.org/pdf/1711.06798.pdf<a class="ae lh" href="https://arxiv.org/pdf/1711.06798.pdf" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="16e3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd">拓扑变形:</strong>MorphNet创建的一些优化可能会产生全新的拓扑。例如，当一个层有0个神经元时，MorphNet可以通过从网络中剪切受影响的分支来有效地改变网络的拓扑。让我们看看下图，它再次显示了RestNet架构的变化。在该示例中，MorphNet可能会保留跳过连接，但会移除剩余的块，如下图所示(左图)。对于盗梦空间风格的架构，MorphNet可能会移除整个平行塔，如右图所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/18b1ce58dd0a5ec4712721bbdf77e0e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*RWz3ZQSpxIybRQ60oGJ49w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://arxiv.org/pdf/1711.06798.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.06798.pdf</a></figcaption></figure><p id="0e86" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd">可扩展性:</strong>MorphNet的最大优势之一是，它可以在单次训练中学习新的结构，从而最大限度地减少训练所需的计算资源，并可以扩展到非常复杂的架构。</p><p id="8a18" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd">可移植性:</strong>MorphNet生产的网络在技术上是可移植的，并且可以从头开始重新训练，因为权重与学习过程无关。</p><p id="aaa4" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">谷歌将MorphNet应用于各种场景，包括使用翻牌优化的<a class="ae lh" href="https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models" rel="noopener ugc nofollow" target="_blank">盗梦V2 </a>在<a class="ae lh" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>上训练。与侧重于缩小输出数量的传统正则化方法相比，MorphNet方法直接针对FLOPs，并在缩小模型时产生更好的权衡曲线(蓝色)。在这种情况下，与基线相比，在精度相同的情况下，翻牌成本降低了11%至15%。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/bec1c7d1f9413e46f288e3be09604ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*1_Jn2P175uz71EspopCOgg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://arxiv.org/pdf/1711.06798.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.06798.pdf</a></figcaption></figure><h1 id="8da4" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">使用MorphNet</h1><p id="4759" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">谷歌在GitHub 上发布了<a class="ae lh" href="https://github.com/google-research/morph-net" rel="noopener ugc nofollow" target="_blank">开源版本的MorphNet。简而言之，使用MorphNet包括以下步骤:</a></p><p id="b427" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">1)从morphnet.network _ regularizers中选择一个正则项，并使用特定的优化度量对其进行初始化。MorphNet的当前实现包括几种正则化算法。</p><p id="985f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">2)训练目标模型。</p><p id="8942" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">3)用StructureExporter保存建议的模型结构。</p><p id="96c6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">4)在没有MorphNet正则化器的情况下从零开始重新训练模型。</p><p id="5914" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">以下代码说明了这些步骤:</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="3f8c" class="oh ne it od b gy oi oj l ok ol">from morph_net.network_regularizers import flop_regularizer<br/>from morph_net.tools import structure_exporter</span><span id="94c8" class="oh ne it od b gy om oj l ok ol">logits = build_model()</span><span id="d607" class="oh ne it od b gy om oj l ok ol">network_regularizer = flop_regularizer.GammaFlopsRegularizer(<br/>    [logits.op], gamma_threshold=1e-3)<br/>regularization_strength = 1e-10<br/>regularizer_loss = (network_regularizer.get_regularization_term() * regularization_strength)</span><span id="04e6" class="oh ne it od b gy om oj l ok ol">model_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)</span><span id="98b6" class="oh ne it od b gy om oj l ok ol">optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)</span><span id="cba3" class="oh ne it od b gy om oj l ok ol">train_op = optimizer.minimize(model_loss + regularizer_loss)</span></pre><p id="6036" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">自动化神经网络架构设计是使深度学习更加主流的关键领域。最好的神经网络架构是使用人类程序员和机器学习算法的组合产生的。MorphNet为深度学习生态系统的这个新的热门领域带来了一个非常创新的角度。</p></div></div>    
</body>
</html>