<html>
<head>
<title>PCA Clearly Explained -When, Why, How To Use It and Feature Importance: A Guide in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA清楚地解释了——何时、为什么、如何使用它以及特性的重要性:Python指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pca-clearly-explained-when-why-how-to-use-it-and-feature-importance-a-guide-in-python-56b3da72d9d1?source=collection_archive---------0-----------------------#2020-12-26">https://pub.towardsai.net/pca-clearly-explained-when-why-how-to-use-it-and-feature-importance-a-guide-in-python-56b3da72d9d1?source=collection_archive---------0-----------------------#2020-12-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8c06" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="4a43" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在这篇文章中，我解释了什么是PCA，何时以及为什么使用它，以及如何使用scikit-learn在Python中实现它。此外，我解释了如何在PCA分析后得到特征的重要性。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ab677f86b284ccd513591b51364f617f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ba0XpZtJrgh7UpzWcIgZ1Q.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd lh">手工制作</strong>草图<strong class="bd lh">作者制作。</strong></figcaption></figure><h1 id="7170" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">1.简介和背景</h1><p id="72b6" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">主成分分析</strong> (PCA)是一种众所周知的<strong class="mb jd">无监督</strong><strong class="mb jd"/><strong class="mb jd">降维</strong>技术，通过原始变量(特征)的线性(线性PCA)或非线性(核PCA) <strong class="mb jd">组合</strong>来构造<strong class="mb jd">相关</strong>特征/变量。在本帖中，我们将只关注著名且广泛使用的<strong class="mb jd">线性PCA </strong>方法。</p><p id="b5c4" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">相关特征的构建是通过将相关变量线性转换成较少数量的<strong class="mb jd">不相关变量</strong>来实现的。这是通过<strong class="mb jd">使用协方差/相关矩阵的特征向量(也称为主分量(PCs ))将</strong>(点积)原始数据投影到<strong class="mb jd">缩减的PCA空间</strong>中来完成的。</p><p id="2686" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">结果</strong> <strong class="mb jd">预计</strong> <strong class="mb jd">数据</strong>实质上是<strong class="mb jd">原始</strong>数据<strong class="mb jd">捕捉</strong> <strong class="mb jd">数据</strong> ( <a class="ae na" href="https://www.springer.com/gp/book/9780387954424" rel="noopener ugc nofollow" target="_blank"> Jolliffe 2002 </a>)中方差最大的 <strong class="mb jd">。</strong></p><p id="e037" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">总之，<strong class="mb jd"> PCA </strong>是数据到一系列<strong class="mb jd">不相关</strong>数据的<strong class="mb jd">正交</strong> <strong class="mb jd">变换，这些数据存在于缩减的PCA空间中，使得第一个分量解释数据中的最大差异，而每个后续分量解释的较少。</strong></p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><p id="3592" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:<a class="ae na" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="262b" class="li lj it bd lh lk ni lm ln lo nj lq lr ki nk kj lt kl nl km lv ko nm kp lx ly bi translated">2.何时/为何使用PCA</h1><ul class=""><li id="261a" class="nn no it mb b mc md mf mg mi np mm nq mq nr mu ns nt nu nv bi translated">PCA技术在处理<strong class="mb jd">多</strong> - <strong class="mb jd">共线性</strong>存在于<strong class="mb jd">特征</strong> / <strong class="mb jd">变量</strong>之间的数据时特别有用。</li><li id="919b" class="nn no it mb b mc nw mf nx mi ny mm nz mq oa mu ns nt nu nv bi translated">当<strong class="mb jd">输入特征的尺寸较大</strong>(如大量变量)时，可使用PCA。</li><li id="c007" class="nn no it mb b mc nw mf nx mi ny mm nz mq oa mu ns nt nu nv bi translated">PCA还可以用于<strong class="mb jd">去噪</strong>和<strong class="mb jd">数据</strong>压缩。</li></ul><h1 id="4d64" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">3.主成分分析方法的核心</h1><p id="c299" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">设<code class="fe ob oc od oe b"><strong class="mb jd">X</strong></code>是包含形状为<code class="fe ob oc od oe b">[n_samples, n_features]</code>的原始数据的矩阵。</p><p id="6c75" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">简而言之，<strong class="mb jd"> PCA </strong>分析由以下<strong class="mb jd">步骤</strong>组成:</p><ul class=""><li id="a2a0" class="nn no it mb b mc mv mf mw mi of mm og mq oh mu ns nt nu nv bi translated">首先，<code class="fe ob oc od oe b"><strong class="mb jd">X</strong></code>中存储的原始输入变量是<strong class="mb jd"> z得分</strong>，这样每个原始变量(<code class="fe ob oc od oe b"><strong class="mb jd">X</strong></code>列)的平均值和单位标准差为零。</li><li id="78d6" class="nn no it mb b mc nw mf nx mi ny mm nz mq oa mu ns nt nu nv bi translated">下一步涉及<strong class="mb jd">协方差</strong>矩阵<code class="fe ob oc od oe b"><strong class="mb jd">Cx= (1/n)X'X</strong></code>的构造和<a class="ae na" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd">特征分解</strong> </a>(在z得分数据的情况下，协方差等于相关矩阵，因为所有特征的标准偏差为1)。</li><li id="6141" class="nn no it mb b mc nw mf nx mi ny mm nz mq oa mu ns nt nu nv bi translated"><strong class="mb jd">特征值</strong>然后按照<strong class="mb jd">递减</strong>的顺序<strong class="mb jd">排序</strong>，表示数据中的递减方差(特征值等于方差——我将在下面第6段使用Python来证明这一点)。</li><li id="86a8" class="nn no it mb b mc nw mf nx mi ny mm nz mq oa mu ns nt nu nv bi translated">最后，<strong class="mb jd">原始</strong> <strong class="mb jd">归一化</strong> <strong class="mb jd">数据</strong>到<strong class="mb jd">约简PCA空间</strong>的<strong class="mb jd">投影</strong>(变换)通过<strong class="mb jd">乘以</strong>(点积)<strong class="mb jd">原始归一化数据</strong>与协方差矩阵即PCs的<strong class="mb jd">前导</strong> <strong class="mb jd">特征向量</strong>得到。</li><li id="1263" class="nn no it mb b mc nw mf nx mi ny mm nz mq oa mu ns nt nu nv bi translated">新的<strong class="mb jd">减少的</strong> PCA空间<strong class="mb jd">最大化</strong>原<strong class="mb jd">数据的<strong class="mb jd">方差</strong>。为了<strong class="mb jd">可视化</strong>投影数据以及原始变量的贡献，在联合绘图中，我们可以使用<strong class="mb jd">双绘图</strong>。</strong></li></ul><h1 id="8282" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">4.有意义组件的最大数量</h1><p id="ac49" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">有意义的</strong> <strong class="mb jd">成分</strong>有一个<strong class="mb jd">上</strong> <strong class="mb jd">界</strong>，可以用<strong class="mb jd"> PCA </strong>提取。这与<strong class="mb jd">协方差/相关性</strong>矩阵(<code class="fe ob oc od oe b"><strong class="mb jd">Cx</strong></code>)的<a class="ae na" href="https://stattrek.com/matrix-algebra/matrix-rank.aspx" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd">秩</strong> </a>有关。具有形状为<code class="fe ob oc od oe b">[n_samples, n_features/n_variables]</code>的数据矩阵<code class="fe ob oc od oe b"><strong class="mb jd">X</strong></code>，则<strong class="mb jd">协方差</strong> / <strong class="mb jd">相关性</strong>矩阵将为<code class="fe ob oc od oe b">[n_features, n_features]</code>，其中<strong class="mb jd">最大秩</strong>等于<code class="fe ob oc od oe b">min(n_samples, n_features)</code>。</p><p id="45f5" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">因此，我们可以在<strong class="mb jd">处拥有最</strong> <code class="fe ob oc od oe b">min(n_samples, n_features)</code> <strong class="mb jd">有意义的</strong> PC <strong class="mb jd">组件/维度</strong>归因于协方差/相关矩阵的<strong class="mb jd">最大值</strong> <a class="ae na" href="https://stattrek.com/matrix-algebra/matrix-rank.aspx" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd">秩</strong> </a>。</p><h1 id="8402" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">5.使用scikit-learn和Iris数据集的Python示例</h1><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="d398" class="om lj it oe b gy on oo l op oq">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn import datasets<br/>from sklearn.decomposition import PCA<br/>import pandas as pd<br/>from sklearn.preprocessing import StandardScaler<br/>plt.style.use('ggplot')</span><span id="6c20" class="om lj it oe b gy or oo l op oq"># Load the data<br/>iris = datasets.load_iris()<br/>X = iris.data<br/>y = iris.target</span><span id="8b59" class="om lj it oe b gy or oo l op oq"># Z-score the features<br/>scaler = StandardScaler()<br/>scaler.fit(X)<br/>X = scaler.transform(X)</span><span id="3af2" class="om lj it oe b gy or oo l op oq"># The PCA model<br/>pca = PCA(n_components=2) # estimate only 2 PCs<br/>X_new = pca.fit_transform(X) # project the original data into the PCA space</span></pre><p id="fa8f" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">让我们在PCA变换之前和<strong class="mb jd">变换之后绘制数据，并使用花</strong> <code class="fe ob oc od oe b">(y)</code>的相应<strong class="mb jd">类对每个点(样本)进行<strong class="mb jd">颜色</strong>编码。</strong></p><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="37c9" class="om lj it oe b gy on oo l op oq">fig, axes = plt.subplots(1,2)</span><span id="2d46" class="om lj it oe b gy or oo l op oq">axes[0].scatter(X[:,0], X[:,1], c=y)<br/>axes[0].set_xlabel('x1')<br/>axes[0].set_ylabel('x2')<br/>axes[0].set_title('Before PCA')</span><span id="3e6c" class="om lj it oe b gy or oo l op oq">axes[1].scatter(X_new[:,0], X_new[:,1], c=y)<br/>axes[1].set_xlabel('PC1')<br/>axes[1].set_ylabel('PC2')<br/>axes[1].set_title('After PCA')</span><span id="3b90" class="om lj it oe b gy or oo l op oq">plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/231b65c6df061b1af8a5b7caf7f4b638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RsKGvLlewFt5kktQhbaO7w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">上面的PCA输出代码。</figcaption></figure><p id="311e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们可以看到在PCA空间中，<strong class="mb jd">方差</strong>是沿着<strong class="mb jd">PC1</strong><strong class="mb jd"> PC2 </strong><strong class="mb jd">最大化</strong>(解释了73%的方差)和<strong class="mb jd">PC2</strong>(解释了22%的方差)。两者加起来解释了95%。</p><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="1a8a" class="om lj it oe b gy on oo l op oq">print(pca.explained_variance_ratio_)<br/># array([0.72962445, 0.22850762])</span></pre><h1 id="b7a8" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">6.原协方差矩阵特征值等于约简空间方差的证明</h1><h2 id="66ad" class="om lj it bd lh ot ou dn ln ov ow dp lr mi ox oy lt mm oz pa lv mq pb pc lx iz bi translated">数学公式和证明</h2><p id="c1d5" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">假设存储在<code class="fe ob oc od oe b"><strong class="mb jd">X</strong></code>中的原始输入变量是<strong class="mb jd"> z得分的</strong>，这样每个原始变量(<code class="fe ob oc od oe b"><strong class="mb jd">X</strong></code>列)具有零均值和单位标准偏差，我们有:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/72c3d956556b5c2bf349b81bcd9766ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_fDA0M4piVQGaWujmbiHSQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者写的Latex代码。</figcaption></figure><p id="8e45" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">上面的<code class="fe ob oc od oe b"><strong class="mb jd">Λ</strong></code>矩阵存储了原始<strong class="mb jd">空间/数据集的<strong class="mb jd">协方差</strong>矩阵的<strong class="mb jd">特征值</strong>。</strong></p><h2 id="59cf" class="om lj it bd lh ot ou dn ln ov ow dp lr mi ox oy lt mm oz pa lv mq pb pc lx iz bi translated">使用Python验证</h2><p id="2a1b" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">最大方差证明</strong>也可以通过估计<strong class="mb jd">约简</strong> <strong class="mb jd">空间</strong>的<strong class="mb jd">协方差</strong>矩阵来看:</p><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="18e1" class="om lj it oe b gy on oo l op oq">np.cov(X_new.T)</span><span id="7d1d" class="om lj it oe b gy or oo l op oq">array([[<strong class="oe jd">2.93808505e+00</strong>, 4.83198016e-16],<br/>       [4.83198016e-16, <strong class="oe jd">9.20164904e-01</strong>]])</span></pre><p id="82d4" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们观察到这些值(对角线上有方差)等于存储在<code class="fe ob oc od oe b">pca.explained_variance_</code>中的协方差的<strong class="mb jd">实际</strong> <strong class="mb jd">特征值</strong>:</p><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="a917" class="om lj it oe b gy on oo l op oq">pca.explained_variance_<br/>array([2.93808505, 0.9201649 ])</span></pre><h1 id="2221" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">7.特征重要性</h1><p id="ba5f" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">每个<strong class="mb jd">特征</strong>的<strong class="mb jd">重要性</strong>由特征向量中<strong class="mb jd">对应值的<strong class="mb jd">量级</strong>来反映(量级越高，重要性越高)。</strong></p><p id="cd52" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">让我们找到<strong class="mb jd">最重要的特征:</strong></p><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="5b16" class="om lj it oe b gy on oo l op oq">print(abs( pca.components_ ))</span><span id="005f" class="om lj it oe b gy or oo l op oq">[[0.52106591 0.26934744 0.5804131 0.56485654]<br/> [0.37741762 0.92329566 0.02449161 0.06694199]]</span></pre><p id="1d75" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">这里，<code class="fe ob oc od oe b">pca.components_</code>具有形状<code class="fe ob oc od oe b">[n_components, n_features]</code>因此，通过查看<strong class="mb jd"> PC1 </strong>(第一主成分)即<strong class="mb jd">第一</strong>行<strong class="mb jd">行</strong></p><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="2f88" class="om lj it oe b gy on oo l op oq">[[0.52106591 0.26934744 0.5804131 0.56485654]</span></pre><p id="2f3e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们可以得出结论，对于<strong class="mb jd"> PC1来说，<strong class="mb jd">特性1、3和4 </strong>是<strong class="mb jd">最重要的</strong>。</strong>同样，我们可以说<strong class="mb jd">特征2 </strong>和<strong class="mb jd"> </strong>那么<strong class="mb jd"> 1 </strong>就是<strong class="mb jd"/><strong class="mb jd">对于<strong class="mb jd"> PC2来说最重要的</strong>。</strong></p><p id="649b" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><em class="pe">综上所述，我们看特征向量的分量的绝对值对应于</em> <strong class="mb jd"> <em class="pe"> k </em> </strong> <em class="pe">最大特征值。在sklearn中，组件按解释的方差排序。</em> <strong class="mb jd"> <em class="pe">这些绝对值越大，特定特征对该主成分的贡献越大。</em> </strong></p><h1 id="20fa" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">8.双地块</h1><p id="e010" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">双标图</strong>是在<strong class="mb jd"> PCA </strong>分析后可视化<strong class="mb jd">一体化</strong>的最佳方式。</p><p id="381a" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">在<a class="ae na" href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/biplot" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd"> R </strong> </a>中有一个实现，但是在<strong class="mb jd"> python </strong>中没有标准实现，所以我决定为此编写自己的<strong class="mb jd">函数</strong>:</p><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="93aa" class="om lj it oe b gy on oo l op oq">def biplot(score, coeff , y):<br/>    '''<br/>    Author: Serafeim Loukas, serafeim.loukas@epfl.ch<br/>    Inputs:<br/>       score: the projected data<br/>       coeff: the eigenvectors (PCs)<br/>       y: the class labels<br/>   '''</span><span id="01e3" class="om lj it oe b gy or oo l op oq">xs = score[:,0] # projection on PC1<br/>    ys = score[:,1] # projection on PC2<br/>    n = coeff.shape[0] # number of variables<br/>    plt.figure(figsize=(10,8), dpi=100)<br/>    classes = np.unique(y)<br/>    colors = ['g','r','y']<br/>    markers=['o','^','x']<br/>    for s,l in enumerate(classes):<br/>        plt.scatter(xs[y==l],ys[y==l], c = colors[s], marker=markers[s]) # color based on group<br/>    for i in range(n):<br/>        #plot as arrows the variable scores (each variable has a score for PC1 and one for PC2)<br/>        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'k', alpha = 0.9,linestyle = '-',linewidth = 1.5, overhang=0.2)<br/>        plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, "Var"+str(i+1), color = 'k', ha = 'center', va = 'center',fontsize=10)<br/><br/>    plt.xlabel("PC{}".format(1), size=14)<br/>    plt.ylabel("PC{}".format(2), size=14)<br/>    limx= int(xs.max()) + 1<br/>    limy= int(ys.max()) + 1<br/>    plt.xlim([-limx,limx])<br/>    plt.ylim([-limy,limy])<br/>    plt.grid()<br/>    plt.tick_params(axis='both', which='both', labelsize=14)</span></pre><p id="7e50" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">调用函数(确保首先运行加载虹膜数据和执行PCA分析的初始代码块):</p><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="3f14" class="om lj it oe b gy on oo l op oq">import matplotlib as mpl<br/>mpl.rcParams.update(mpl.rcParamsDefault) # reset ggplot style</span><span id="cec5" class="om lj it oe b gy or oo l op oq"># Call the biplot function for only the first 2 PCs<br/>biplot(X_new[:,0:2], np.transpose(pca.components_[0:2, :]), y)<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/5c02f96c187c47a61dd0b90f761948c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6VxrEZWEr4k7ZljKp-OUEw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">使用我的自定义函数的PCA双图。</figcaption></figure><p id="dd03" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">我们可以再次从视觉上验证<strong class="mb jd"/><strong class="mb jd">a)</strong>方差最大化<strong class="mb jd"> b) </strong>特征1、3和4 是<strong class="mb jd"> PC1最重要的<strong class="mb jd">。</strong>同样，<strong class="mb jd">功能2 </strong>和<strong class="mb jd">功能1</strong>和<strong class="mb jd">功能1 </strong>都是<strong class="mb jd">功能</strong>功能<strong class="mb jd">对于<strong class="mb jd"> PC2最重要的</strong>。</strong></strong></p><p id="3654" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">此外，指向<strong class="mb jd">相同</strong> <strong class="mb jd">方向</strong>的<strong class="mb jd">箭头</strong>(变量/特征)表示它们所代表的变量之间的<strong class="mb jd">相关性</strong>，而指向与 <strong class="mb jd">方向</strong>相反<strong class="mb jd">的箭头表示它们所代表的变量之间的<strong class="mb jd">对比</strong>。</strong></p><p id="98b2" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">使用<strong class="mb jd">代码</strong>验证</strong>上述内容:</p><pre class="ks kt ku kv gt oi oe oj ok aw ol bi"><span id="620d" class="om lj it oe b gy on oo l op oq"># <strong class="oe jd">Var 3 and Var 4</strong> are extremely <strong class="oe jd">positively</strong> correlated<br/>np.corrcoef(X[:,2], X[:,3])[1,0]<br/>0.9628654314027957</span><span id="9ba5" class="om lj it oe b gy or oo l op oq"># <strong class="oe jd">Var 2and Var 3</strong> are <strong class="oe jd">negatively</strong> correlated<br/>np.corrcoef(X[:,1], X[:,2])[1,0]<br/>-0.42844010433054014</span></pre><p id="fc35" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">那都是乡亲们！希望你喜欢这篇文章！</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="9d89" class="li lj it bd lh lk ni lm ln lo nj lq lr ki nk kj lt kl nl km lv ko nm kp lx ly bi translated">最新帖子</h1><div class="pg ph gp gr pi pj"><a href="https://towardsdatascience.com/time-series-forecasting-predicting-stock-prices-using-facebooks-prophet-model-9ee1657132b5" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">时间序列预测:用脸书的先知模型预测股票价格</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">使用可从《先知脸书》公开获得的预测模型预测股票价格</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px lb pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a href="https://towardsdatascience.com/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">用新冠肺炎假设的例子解释ROC曲线:二分类和多分类…</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我清楚地解释了什么是ROC曲线以及如何阅读它。我用一个新冠肺炎的例子来说明我的观点，我…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="py l pu pv pw ps px lb pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a href="https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">支持向量机(SVM)解释清楚:分类问题的python教程…</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何绘制支持…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="pz l pu pv pw ps px lb pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a href="https://towardsdatascience.com/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">关于Python中的最小-最大规范化，您需要知道的一切</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我将解释什么是最小-最大缩放，什么时候使用它，以及如何使用scikit在Python中实现它</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qa l pu pv pw ps px lb pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a href="https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">Scikit-Learn的标准定标器如何工作</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在这篇文章中，我将解释为什么以及如何使用scikit-learn应用标准化</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qb l pu pv pw ps px lb pj"/></div></div></a></div><h1 id="6b86" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">敬请关注并支持这一努力</h1><p id="7a46" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">如果你喜欢并发现这篇文章有用，<strong class="mb jd">关注我吧！</strong></p><p id="0d60" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">有问题吗？把它们作为评论贴出来，我会尽快回复。</p><h1 id="2722" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">参考</h1><p id="8b87" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">[1] Jolliffe，I. T. <em class="pe">主成分分析。纽约州纽约市:斯普林格出版社，2002年。</em></p><p id="eb67" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><a class="ae na" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Principal_component_analysis</a></p><p id="dd3a" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><a class="ae na" href="https://stattrek.com/matrix-algebra/matrix-rank.aspx" rel="noopener ugc nofollow" target="_blank">https://stattrek.com/matrix-algebra/matrix-rank.aspx</a></p><h1 id="d7ce" class="li lj it bd lh lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">和我联系</h1><ul class=""><li id="54cc" class="nn no it mb b mc md mf mg mi np mm nq mq nr mu ns nt nu nv bi translated"><strong class="mb jd">LinkedIn</strong>:【https://www.linkedin.com/in/serafeim-loukas/ T2】</li><li id="4c1e" class="nn no it mb b mc nw mf nx mi ny mm nz mq oa mu ns nt nu nv bi translated">https://www.researchgate.net/profile/Serafeim_Loukas<strong class="mb jd">研究之门</strong>:<a class="ae na" href="https://www.researchgate.net/profile/Serafeim_Loukas" rel="noopener ugc nofollow" target="_blank">T7】</a></li></ul></div></div>    
</body>
</html>