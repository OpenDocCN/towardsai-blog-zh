<html>
<head>
<title>Deploy HuggingFace NLP Models in Java With Deep Java Library</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度Java库在Java中部署HuggingFace NLP模型</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/deploy-huggingface-nlp-models-in-java-with-deep-java-library-e36c635b2053?source=collection_archive---------1-----------------------#2022-04-27">https://pub.towardsai.net/deploy-huggingface-nlp-models-in-java-with-deep-java-library-e36c635b2053?source=collection_archive---------1-----------------------#2022-04-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8d02" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用HuggingFace问答模型一步步演示。</h2></div><p id="70a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者:冯可欣，李正哲</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/128ddbf7ef4fb7969ddebbbf47034fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*ILAGzbfXzuOcpoUKkruabQ.jpeg"/></div></figure><p id="3c62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lj" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>是基于PyTorch和TensorFlow构建的最流行的自然语言处理(NLP)工具包之一。它有各种预训练的Python模型用于NLP任务，如问题回答和令牌分类。它还提供了强大的记号赋予器工具来处理现成的输入。</p><p id="8255" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">HuggingFace的API使初学者只需几行代码就能实现机器学习模型。此外，它为高级用户提供了定制和微调基于变压器的模型的灵活性。然而，由于这个工具包是用Python实现的，机器学习(ML)工程师很少有机会将这些模型集成到生产Java环境中。今天，如果ML工程师要从零开始重构Java代码，他们将需要实现数据处理，如超过10行代码的图像到数组的转换，以及性能很差的N维数组操作。现在有了Deep Java Library (DJL)，他们只需要一个line函数<code class="fe lk ll lm ln b">Image.toNDArray</code>来转换图像，并利用高性能的阵列操作，这些操作利用了多个CPU内核和GPU。</p><p id="b83e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DJL为Java开发人员提供了一个易于使用的模型加载API。它为用户提供了从各种来源访问模型工件的灵活性，包括我们预加载的模型动物园、<a class="ae lj" href="https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html" rel="noopener ugc nofollow" target="_blank"> HDFS </a>、S3桶和您的本地文件系统。DJL还通过捆绑实现所需的标记器和词汇工具来简化数据处理，以实现HuggingFace模型。配备了这些功能，HuggingFace用户可以在10分钟内使用HuggingFace工具包带来自己的问题回答模型。在这篇博文中，我们一步一步地介绍了如何部署你自己的HuggingFace问答模型。</p><p id="f537" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">完整源代码</strong>可在<a class="ae lj" href="https://gist.github.com/KexinFeng/97e6344556f88822650d023acfbdf4f5" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">这里</strong> </a> <strong class="kh ir">。</strong></p><h2 id="55b9" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">设置</h2><p id="ae36" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">要开始使用DJL，请将下面的代码片段添加到您的<code class="fe lk ll lm ln b">build.gradle</code>文件中，该代码片段定义了必要的依赖关系。</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="87a0" class="mq lp iq ln b be mr ms l mt mu">plugins {<br/>  id 'java'<br/>}<br/>repositories {                           <br/>  mavenCentral()<br/>}<br/>dependencies {<br/>  implementation "org.apache.logging.log4j:log4j-slf4j-impl:2.17.1"<br/>  implementation platform("ai.djl:bom:0.21.0")<br/>  implementation "ai.djl:api"<br/>  runtimeOnly "ai.djl.pytorch:pytorch-engine"<br/>  runtimeOnly "ai.djl.pytorch:pytorch-model-zoo"<br/>}</span></pre><p id="e63c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还要注意，在运行这段代码时，可能还需要用VM选项指定默认引擎:<code class="fe lk ll lm ln b">-Dai.djl.default_engine=PyTorch</code>，它与模型和标记器兼容。</p><h2 id="af72" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">把你自己的问答模式带到DJL</h2><p id="7d39" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">推理工作流与输入预处理、模型前向和输出后处理相结合。DJL将输入和输出处理封装到翻译器中，并使用预测器进行模型转发。要使用HuggingFace API运行问答任务，以BERT为例，您需要创建一个<code class="fe lk ll lm ln b">BertTokenizer</code>来将您的文本输入转换为机器可理解的张量，这是数据预处理的一部分。后期处理主要包括结果索引的转换。DJL引入了<code class="fe lk ll lm ln b">Translator</code>结构来封装预处理和后处理数据的工作流。这个例子<code class="fe lk ll lm ln b">Translator</code>是由<code class="fe lk ll lm ln b">BertTranslator</code>实现的。</p><p id="bd56" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后你可以加载一个特定的模型，例如<code class="fe lk ll lm ln b">BertForQuestionAnswering</code>，来运行推理。然后在logits的顶部应用<code class="fe lk ll lm ln b">argmax()</code>来获得结果索引。</p><h2 id="c4ca" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">翻译器概述</h2><p id="2364" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated"><code class="fe lk ll lm ln b">Translator</code>用于组织预处理和后处理。您定义输入和输出对象。它包含以下两个覆盖类:</p><ul class=""><li id="1e56" class="mv mw iq kh b ki kj kl km ko mx ks my kw mz la na nb nc nd bi translated"><code class="fe lk ll lm ln b">public NDList processInput(TranslatorContext ctx, I)</code></li><li id="6cce" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><code class="fe lk ll lm ln b">public String processOutput(TranslatorContext ctx, O)</code></li></ul><p id="ce7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个翻译器接受输入，并以通用对象的形式返回输出。在这种情况下，翻译器以<code class="fe lk ll lm ln b">QAInput</code> (I)的形式接受输入，并以<code class="fe lk ll lm ln b">String</code> (O)的形式返回输出。<code class="fe lk ll lm ln b">QAInput</code>只是一个保存问题和答案的对象。我们为您准备了输入类。输出<code class="fe lk ll lm ln b">String</code>是您期望从模型中得到的答案。在实现我们的第一个翻译器之前，您需要样本输入。</p><h2 id="1dd7" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">创建一个样本输入</h2><p id="87e8" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">以下代码示例使用<code class="fe lk ll lm ln b">QAInput</code>为问答任务创建了一个示例输入:</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="7bdc" class="mq lp iq ln b be mr ms l mt mu">String question = "When did BBC Japan start broadcasting?";<br/>String resourceDocument = <br/>  "BBC Japan was a general entertainment Channel.\n" + <br/>  "Which operated between December 2004 and April 2006.\n" + <br/>  "It ceased operations after its Japanese distributor folded.";<br/>QAInput input = new QAInput(question, resourceDocument);</span></pre><h2 id="6bb8" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">标记你的输入</h2><p id="2cb5" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">DJL提供了一个内置的<code class="fe lk ll lm ln b">BertTokenizer</code>来把你的字符串分割成令牌。这个记号赋予器实现如下:</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="3289" class="mq lp iq ln b be mr ms l mt mu">BertTokenizer tokenizer = new BertTokenizer();<br/>List&lt;String&gt; tokenQ = tokenizer.tokenize(question.toLowerCase());<br/>List&lt;String&gt; tokenA = tokenizer.tokenize(resourceDocument.toLowerCase());</span></pre><p id="7388" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么输出将是:</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="08e2" class="mq lp iq ln b be mr ms l mt mu">// tokenQ: [when, did, bbc, japan, start, broadcasting, ?]<br/><br/>// tokenA: [bbc, japan, was, a, general, entertainment, channel, ., which, operated, between, december, 2004, and, april, 2006, ., it, ceased, operations, after, its, japanese, distributor, folded, .]</span></pre><p id="17d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">tokenizer还可以用于将问题和资源文档编码在一起，这增加了用于训练BERT模型的特殊标记。您获得了BERT模型输入所需的所有元数据。下面的代码示例演示了用于BERT模型的三种类型的输入:编码标记、标记类型和attentionMask。</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="6d92" class="mq lp iq ln b be mr ms l mt mu">BertToken token = tokenizer.encode(question.toLowerCase(), resourceDocument.toLowerCase());<br/>List&lt;String&gt; tokens = token.getTokens();<br/>List&lt;Long&gt; tokenTypes = token.getTokenTypes();<br/>List&lt;Long&gt; attentionMask = token.getAttentionMask();</span></pre><p id="c762" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么输出将是:</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="f5d6" class="mq lp iq ln b be mr ms l mt mu">tokens: [[CLS], when, did, bbc, japan, start, broadcasting, ?, [SEP], bbc, japan, was, a, general, entertainment, channel, ., which, operated, between, december, 2004, and, april, 2006, ., it, ceased, operations, after, its, japanese, distributor, folded, ., [SEP]]<br/>toeknTypes: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]<br/>attentionMask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span></pre><p id="5be6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当你使用hugging face<a class="ae lj" href="https://huggingface.co/transformers/model_doc/bert.html#berttokenizer" rel="noopener ugc nofollow" target="_blank">bertokenizer</a>时，它会下载词汇表。您可以通过在<code class="fe lk ll lm ln b">from_pretrained</code>方法中指定<code class="fe lk ll lm ln b">cache_dir</code>来轻松找到该文件。呼叫<code class="fe lk ll lm ln b">PtBertVocabulary.parse(InputStream)</code>得到<code class="fe lk ll lm ln b">BertVocabulary</code>。然后，用<code class="fe lk ll lm ln b">getIndex(token)</code>将令牌转换为索引，用<code class="fe lk ll lm ln b">getToken(index)</code>将令牌转换为索引，如下所示。在这个例子中，文件<code class="fe lk ll lm ln b">vocab.txt</code>可以从我们的公共图片<a class="ae lj" href="https://mlrepo.djl.ai/model/nlp/question_answer/ai/djl/pytorch/bertqa/trace_cased_bertqa/0.0.1/bert-base-cased-vocab.txt.gz" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">这里</strong> </a>或者HuggingFace repo <a class="ae lj" href="https://huggingface.co/bert-base-uncased/tree/main" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="c9e1" class="mq lp iq ln b be mr ms l mt mu">Path file = Paths.get("/YOUR PATH/vocab.txt");<br/>Vocabulary vocabulary = DefaultVocabulary.builder()<br/>                        .optMinFrequency(1)<br/>                        .addFromTextFile(file)<br/>                        .optUnknownToken("[UNK]")<br/>                        .build();<br/><br/>// index: 2482<br/>long index = vocabulary.getIndex("car");<br/><br/>// token: car<br/>String token = vocabulary.getToken(2482);</span></pre><h2 id="2df5" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">对输出进行后处理</h2><p id="ac34" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在模型前向调用之后，您会得到一系列NDArrays作为输出。我们将这些打包成一个对象:<code class="fe lk ll lm ln b">NDList</code>。您可以使用<code class="fe lk ll lm ln b">get(index)</code>从<code class="fe lk ll lm ln b">NDList</code>中提取NDArray。要获得概率最高的索引，您可以应用argMax，后跟<code class="fe lk ll lm ln b">getLong</code>，这将标量NDArray转换为Java原语类型，如下所示:</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="65a2" class="mq lp iq ln b be mr ms l mt mu">// list is NDList which is the output from the model<br/>NDArray startLogits = list.get(0);<br/>NDArray endLogits = list.get(1);<br/>int startIdx = (int) startLogits.argMax().getLong();<br/>int endIdx = (int) endLogits.argMax().getLong();<br/><br/>// token(BertToken) is generated by the encode method.<br/>List&lt;String&gt; tokens = token.getTokens();<br/><br/>// get the answer<br/>tokens.subList(startIdx, endIdx + 1).toString();</span></pre><h2 id="14ee" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">实现BertTranslator</h2><p id="a36d" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">现在，您可以将上述预处理和后处理结合在一起，创建自己的翻译器<code class="fe lk ll lm ln b">BertTranslator</code>。接下来，它将用于构建<code class="fe lk ll lm ln b">Criteria</code>和<code class="fe lk ll lm ln b">predictor</code>。</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="28af" class="mq lp iq ln b be mr ms l mt mu">public class BertTranslator implements Translator&lt;QAInput, String&gt; {<br/>  private List&lt;String&gt; tokens;<br/>  private Vocabulary vocabulary;<br/>  private BertTokenizer tokenizer;<br/>  <br/>  @Override<br/>  public void prepare(TranslatorContext ctx) {<br/>    Path path = Paths.get("/YOUR PATH/vocab.txt");<br/>    vocabulary = DefaultVocabulary.builder()<br/>                .optMinFrequency(1)<br/>                .addFromTextFile(path)<br/>                .optUnknownToken("[UNK]")<br/>                .build();<br/>    tokenizer = new BertTokenizer();<br/>  }<br/>    <br/>  @Override<br/>  public NDList processInput(TranslatorContext ctx, QAInput input){<br/>    BertToken token =<br/>      tokenizer.encode(<br/>        input.getQuestion().toLowerCase(),<br/>        input.getParagraph().toLowerCase()<br/>      );<br/>    // get the encoded tokens used in precessOutput<br/>    tokens = token.getTokens();<br/>    NDManager manager = ctx.getNDManager();<br/>    // map the tokens(String) to indices(long)<br/>    long[] indices =<br/>      tokens.stream().mapToLong(vocabulary::getIndex).toArray();<br/>    long[] attentionMask = <br/>      token.getAttentionMask().stream().mapToLong(i -&gt; i).toArray();<br/>    long[] tokenType = token.getTokenTypes().stream()<br/>      .mapToLong(i -&gt; i).toArray();<br/>    NDArray indicesArray = manager.create(indices);<br/>    NDArray attentionMaskArray =<br/>      manager.create(attentionMask);<br/>    NDArray tokenTypeArray = manager.create(tokenType);<br/>    // The order matters<br/>    return new NDList(indicesArray, attentionMaskArray,<br/>      tokenTypeArray);<br/>  }<br/>    <br/>  @Override<br/>  public String processOutput(TranslatorContext ctx, NDList list) {<br/>    NDArray startLogits = list.get(0);<br/>    NDArray endLogits = list.get(1);<br/>    int startIdx = (int) startLogits.argMax().getLong();<br/>    int endIdx = (int) endLogits.argMax().getLong();<br/>    return tokenizer.tokenToString(tokens.subList(startIdx, endIdx + 1));<br/>  }<br/>    <br/>  @Override<br/>  public Batchifier getBatchifier() {<br/>    return Batchifier.STACK;<br/>  }<br/>}</span></pre><h2 id="57cf" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">从本地文件系统加载您自己的模型</h2><p id="c13b" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在这一步，我们将构建<code class="fe lk ll lm ln b">Criteria</code> API，它被用作搜索ZooModel的搜索标准。在这个应用程序中，将指定本地TorchScript模型的目录，以便相应地加载ZooModel，使用<code class="fe lk ll lm ln b">.optModelPath()</code>。下面的代码片段用文件路径加载模型:<code class="fe lk ll lm ln b">/YOUR PATH/trace_cased_bertqa.pt</code>。这个TorchScript的模型文件可以在我们的<a class="ae lj" href="https://mlrepo.djl.ai/model/nlp/question_answer/ai/djl/pytorch/bertqa/trace_cased_bertqa/0.0.1/trace_cased_bertqa.pt.gz" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">公众号这里</strong> </a>找到。也可以从<a class="ae lj" href="https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>下载然后按照<a class="ae lj" href="https://docs.djl.ai/docs/pytorch/how_to_convert_your_model_to_torchscript.html" rel="noopener ugc nofollow" target="_blank"> djl教程</a>或者<a class="ae lj" href="https://huggingface.co/transformers/torchscript.html" rel="noopener ugc nofollow" target="_blank">官方教程</a>用TorchScript格式保存模型。</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="ebc4" class="mq lp iq ln b be mr ms l mt mu">BertTranslator translator = new BertTranslator();<br/>Criteria&lt;QAInput, String&gt; criteria = Criteria.builder()<br/>  .setTypes(QAInput.class, String.class)<br/>  .optModelPath(Paths.get("/YOUR PATH/trace_cased_bertqa.pt"))<br/>  .optTranslator(translator)<br/>  .optProgress(new ProgressBar()).build();<br/>ZooModel&lt;QAInput, String&gt; model = criteria.loadModel();<br/>Predictor&lt;QAInput, String&gt; predictor = model.newPredictor(tranlator));<br/>return predictor.predict(input);</span></pre><p id="7af5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Criteria API之前在<a class="ae lj" href="https://towardsdatascience.com/implement-object-detection-with-pytorch-in-java-in-5-minutes-c3ba5769e7aa" rel="noopener" target="_blank">在5分钟内用PyTorch在Java中实现对象检测的博文</a>中介绍过，它用于从预先上传的模型动物园中加载模型。对于问答模型，我们将BERT模型上传到我们的模型动物园，该模型通过<a class="ae lj" href="https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>的小队进行了微调。有关该型号的更多信息，请参见<a class="ae lj" href="https://github.com/awslabs/djl/blob/master/examples/docs/BERT_question_and_answer.md" rel="noopener ugc nofollow" target="_blank"> BERT QA示例</a>。</p><h2 id="a40e" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">把所有东西放在一起</h2><p id="ab68" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">现在，将所有东西放在一起，您就可以使用与上面创建的翻译器捆绑在一起的模型来运行推理了。</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="677f" class="mq lp iq ln b be mr ms l mt mu">public static void main(String[] args) {<br/>  String question = "When did BBC Japan start broadcasting?";<br/>  String paragraph =<br/>    "BBC Japan was a general entertainment Channel. "<br/>    + "Which operated between December 2004 and April 2006. "<br/>    + "It ceased operations after its Japanese distributor folded.";<br/>  QAInput input = new QAInput(question, paragraph);<br/>        <br/>  String answer = HuggingFaceQaInference.qa_predict(input);<br/>  System.out.println("The answer is: \n" + answer);<br/>}</span></pre><p id="8adb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是演示输入和输出:</p><pre class="lc ld le lf gt mm ln mn bn mo mp bi"><span id="8549" class="mq lp iq ln b be mr ms l mt mu">String paragraph =<br/>  "BBC Japan was a general entertainment Channel. "<br/>  + "Which operated between December 2004 and April 2006. "<br/>  + "It ceased operations after its Japanese distributor folded.";<br/><br/>String question = "When did BBC Japan start broadcasting?";<br/>// The answer is:<br/>// december 2004<br/>String question = "What is BBC Japan?"<br/>// The answer is:<br/>// a general entertainment channel<br/>String question = "When did it cease operations?"<br/>// The answer is: <br/>// april 2006</span></pre><p id="9dfc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你是Java程序员，恭喜你！现在，您可以轻松访问HuggingFace QA模型。点击<a class="ae lj" href="https://gist.github.com/KexinFeng/97e6344556f88822650d023acfbdf4f5" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">此处</strong> </a>查看<strong class="kh ir">完整源代码</strong>。</p><p id="ceac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您还可以轻松地将推理代码片段与<a class="ae lj" href="https://github.com/deepjavalibrary/djl-demo/tree/master/apache-spark/image-classification" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>、<a class="ae lj" href="https://github.com/deepjavalibrary/djl-demo/tree/master/apache-flink/sentiment-analysis" rel="noopener ugc nofollow" target="_blank"> Apache Flink </a>和<a class="ae lj" href="https://github.com/aws-samples/djl-demo/tree/master/quarkus/example" rel="noopener ugc nofollow" target="_blank"> Quarkus </a>集成。</p><h2 id="acf9" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">结论</h2><p id="7709" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在这篇博文中，我们展示了如何使用Deep Java库实现自己的拥抱脸翻译器，以及如何对更复杂的模型进行推理的示例。有了这些知识，您应该能够在Java应用程序上从HuggingFace部署自己的基于transformer的模型，包括SpringBoot和Apache Spark。</p><p id="9d29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你是Python用户，AWS SageMaker最近宣布与HuggingFace合作，推出一种新的拥抱脸深度学习容器(DLCs)。它提供了一个强大的Python SDK，以减少在API可用性和性能方面最先进的HuggingFace模型从科学到生产的差距。你可以找到更多关于合作关系的细节:亚马逊SageMaker和拥抱脸和<a class="ae lj" href="https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html" rel="noopener ugc nofollow" target="_blank">使用亚马逊SageMaker的拥抱脸——亚马逊SageMaker </a>。</p><p id="7bdb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">免责声明—本文仅代表作者个人观点。这不是该组织的正式文件。</p></div></div>    
</body>
</html>