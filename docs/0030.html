<html>
<head>
<title>Importance of Choosing the Correct Hyper-parameters While Defining a Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">定义模型时选择正确超参数的重要性</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/importance-of-choosing-the-correct-hyper-parameters-while-defining-a-model-c6084bc49fc2?source=collection_archive---------3-----------------------#2019-04-24">https://pub.towardsai.net/importance-of-choosing-the-correct-hyper-parameters-while-defining-a-model-c6084bc49fc2?source=collection_archive---------3-----------------------#2019-04-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/3455f2e554a33c29b622e4a1ad983033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BZx6pKa6Q98umO5C_zux1Q.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">资料来源:联合国人类住区规划署</figcaption></figure><h2 id="68c0" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph">面向AI的超参数优化|<a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank"/></h2><div class=""/><blockquote class="kl"><p id="c41e" class="km kn jf bd ko kp kq kr ks kt ku kv dk translated">通常被认为是优化机器学习算法最棘手的部分。正确的超参数调整可以节省大量时间，并有助于更快地部署ML模型</p></blockquote><p id="5811" class="pw-post-body-paragraph kw kx jf ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls kv ij bi translated">我们所有的机器学习爱好者肯定都参加过黑客马拉松，以测试我们在机器学习方面的技能。嗯，我们需要解决的一些问题陈述可能与回归有关，一些可能与分类有关。让我们假设，我们在一个，我们已经完成了<strong class="ky jp"> <em class="lt">预处理数据的所有艰苦工作，努力生成新的特征，将它们应用于我们的数据以获得基础模型。</em> </strong></p><p id="6de5" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">所以，现在我们有了基本模型，我们想在看不见的数据上测试这个模型，我们这样做了。我们看到一个<strong class="ky jp"><em class="lt"/></strong><em class="lt">(可能是基于问题陈述的其他东西)</em>，我们觉得还可以。但是，我们仍然可以通过调整参数来改善模型的性能。<strong class="ky jp"> <em class="lt">熟悉这种感觉？</em>T15】</strong></p><p id="58d9" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">我们该怎么做？<strong class="ky jp"> <em class="lt">参数</em> </strong> <em class="lt">和</em> <strong class="ky jp"> <em class="lt">超参数</em> </strong>。我相信我们在机器学习中一定听过很多这些术语，在深度学习中更是如此。让我们看看这些术语是什么意思。</p><p id="f9f1" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"> <em class="lt">模型参数</em> </strong>是分类器或其他ML模型在训练过程中学习到的训练数据的属性。<em class="lt">例如</em>，在一些NLP任务的情况下:<em class="lt">词频、句子长度、每句话的名词或动词分布、每个单词的特定字符n元文法的数量、词汇多样性</em>等。<em class="lt">每个实验的模型参数</em>不同，取决于数据类型和手头的任务。</p><p id="c699" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"> <em class="lt">另一方面，模型超参数</em> </strong>对于类似的模型是通用的，不能在训练期间学习，而是预先设置的。<strong class="ky jp"> NN </strong> ( <em class="lt">神经网络</em>)的一组典型超参数包括<em class="lt">隐层的数量和大小、权重初始化方案、学习率及其衰减、漏失和梯度限幅阈值等。</em></p><p id="1188" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">换句话说，<strong class="ky jp"> <em class="lt">参数</em> </strong>是那些像<em class="lt">权重和偏差</em>一样被机器学习的参数。<strong class="ky jp"> <em class="lt">超参数</em> </strong>是我们提供给神经网络的参数，例如- <em class="lt">隐藏层数、输入特征、学习率、激活函数等</em>。</p><blockquote class="kl"><p id="4ac5" class="km kn jf bd ko kp lz ma mb mc md kv dk translated">超参数是我们在构建机器/深度学习模型时可以调整的旋钮。简单的..！！！</p></blockquote><figure class="mf mg mh mi mj is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/e006c81ce3112d87f84701b941a844ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hS-23KpsFLNupv4CkCYwQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">超参数设计的简要描述</figcaption></figure><p id="83db" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">现在，让我们来看看机器学习算法的超参数调整的类型。我们将一个一个地看到他们。</p><p id="f70a" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"> <em class="lt">网格搜索(GS)</em>:</strong>——顾名思义，<strong class="ky jp"> <em class="lt">网格搜索是在</em> </strong>网格中扫描数据为给定模型配置最优参数的过程。这意味着参数搜索是在所选数据的整个网格中进行的。</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/f09ef0bfe77c6900e00b65f78bbf13be.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*ZQ-j10BjS246KljbAx7HRw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">上图显示了使用SVM模型的网格搜索</figcaption></figure><p id="06b3" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">这很重要，因为模型的整体性能取决于指定的超参数。<em class="lt">让我们看一个在超参数调整模型上执行网格搜索的基本代码片段。</em></p><pre class="ml mm mn mo gt mq mr ms mt aw mu bi"><span id="6e48" class="mv mw jf mr b gy mx my l mz na"><strong class="mr jp"><em class="lt">#Example of Grid Search<br/></em></strong><em class="lt"># Load the dataset</em><br/>x, y = load_dataset()</span><span id="426b" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Create model for KerasClassifier</em><br/>def create_model(hparams1=dvalue, hparams2=dvalue, ... hparamsn=dvalue):</span><span id="f5dd" class="mv mw jf mr b gy nb my l mz na">    ...</span><span id="a879" class="mv mw jf mr b gy nb my l mz na">model = KerasClassifier(build_fn=create_model) </span><span id="a849" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Define the range</em><br/>hparams1 = [2, 4, ...]<br/>hparams2 = ['elu', 'relu', ...]<br/>...<br/>hparamsn = [1, 2, 3, 4, ...]</span><span id="ef70" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Prepare the Grid</em><br/>param_grid = dict(hparams1=hparams1, <br/>                  hparams2=hparams2, <br/>                  ...<br/>                  hparamsn=hparamsn)</span><span id="0a69" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># GridSearch in action</em><br/>grid = GridSearchCV(estimator=model, <br/>                    param_grid=param_grid, <br/>                    n_jobs=, <br/>                    cv=,<br/>                    verbose=)<br/>grid_result = grid.fit(x, y)</span><span id="03dc" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Show the results</em><br/>print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))<br/>means = grid_result.cv_results_['mean_test_score']<br/>stds = grid_result.cv_results_['std_test_score']<br/>params = grid_result.cv_results_['params']<br/>for mean, stdev, param in zip(means, stds, params):<br/>    print("%f (%f) with: %r" % (mean, stdev, param))</span></pre><p id="f23a" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"> <em class="lt">执行网格搜索时要记住的一点</em> </strong> <em class="lt"> </em>就是我们的参数越多，参数执行搜索所占用的时间和空间就越多。这也是<strong class="ky jp"> <em class="lt">维度的诅咒</em> </strong> <em class="lt"> </em>出现的原因。这意味着我们添加的维度越多，搜索的时间复杂度就越大(<em class="lt">增加一个指数因子</em>，最终使这种策略变得不可行。</p><p id="c2bb" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp">T41】</strong></p><p id="c73b" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"> <em class="lt">随机搜索(RS): </em> </strong> <em class="lt"> </em>另一种超参数调优叫做随机搜索。随机搜索完成随机选择参数的工作。它类似于网格搜索，但它比网格搜索产生更好的结果。<strong class="ky jp"> <em class="lt">随机搜索的缺点是，在计算</em>时会产生很高的方差。</strong>由于参数的选择是完全随机的；由于没有智力被用来对这些组合进行取样，所以运气发挥了它的作用。</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/f7b0917b9c676efc6d57a47ccba22a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*xAwDW5CdlqMg79VPiacpuQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><em class="mp">上图显示了随机搜索的基本直觉</em></figcaption></figure><p id="91e0" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">它擅长测试大范围的值，通常它很快达到一个非常好的组合，但问题是它不能保证给出最佳的参数组合。</p><pre class="ml mm mn mo gt mq mr ms mt aw mu bi"><span id="7c5a" class="mv mw jf mr b gy mx my l mz na"><strong class="mr jp"><em class="lt">## Example of Random Search</em></strong><br/><em class="lt"># Load the dataset</em><br/>X, Y = load_dataset()</span><span id="00d2" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Create model for KerasClassifier</em><br/>def create_model(hparams1=dvalue, hparams2=dvalue, ... hparamsn=dvalue):</span><span id="0f5d" class="mv mw jf mr b gy nb my l mz na">    ...</span><span id="24be" class="mv mw jf mr b gy nb my l mz na">model = KerasClassifier(build_fn=create_model) </span><span id="ae46" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Specify parameters and distributions to sample from</em><br/>hparams1 = randint(1, 100)<br/>hparams2 = ['elu', 'relu', ...]<br/>...<br/>hparamsn = uniform(0, 1)</span><span id="87e8" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Prepare the Dict for the Search</em><br/>param_dist = dict(hparams1=hparams1, <br/>                  hparams2=hparams2, <br/>                  ...<br/>                  hparamsn=hparamsn)</span><span id="42ad" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Search in action!</em><br/>n_iter_search = 16 <em class="lt"># Number of parameter settings that are sampled.</em><br/>random_search = RandomizedSearchCV(estimator=model, <br/>                                   param_distributions=param_dist,<br/>                                   n_iter=n_iter_search,<br/>                                   n_jobs=, <br/>								   cv=, <br/>								   verbose=)<br/>random_search.fit(X, Y)</span><span id="7226" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Show the results</em><br/>print("Best: %f using %s" % (random_search.best_score_, random_search.best_params_))<br/>means = random_search.cv_results_['mean_test_score']<br/>stds = random_search.cv_results_['std_test_score']<br/>params = random_search.cv_results_['params']<br/>for mean, stdev, param in zip(means, stds, params):<br/>    print("%f (%f) with: %r" % (mean, stdev, param))</span></pre><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/4eeea1cc5d3d8f2232d2ea8f6ce4745e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*wY_foZme4zE-IndvN6-peQ.png"/></div></div></figure><p id="5edd" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><em class="lt">上图通过在两个超参数空间上搜索最佳配置来比较这两种方法。</em></p><p id="5456" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">但是，不幸的是，网格搜索和随机搜索都有一个共同的缺点，那就是<strong class="ky jp"> <em class="lt">“每个新的猜测都独立于之前的运行！”</em> </strong> <em class="lt"> </em>这意味着<strong class="ky jp"> GS </strong>和<strong class="ky jp"> RS </strong>都不考虑超参数搜索的先前计算。</p><p id="6360" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">因此，需要一种新型的超参数搜索，其既有效又通过使用过去作为资源来改进接下来的运行，从而有效地驱动搜索和实验。<em class="lt">欢迎贝叶斯优化。</em></p><p id="3eac" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"> <em class="lt">贝叶斯优化</em> : </strong>贝叶斯优化试图用最少的步骤找到全局最优。基于称为代理的目标函数的模型选择下一组超参数。为了清楚地理解贝叶斯优化，首先，我们需要理解代理函数的概念。<em class="lt">替代函数，也称为响应面，是使用先前评估的目标函数的概率表示</em>。代理函数比目标函数更容易优化。</p><p id="a92c" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"> <em class="lt">贝叶斯优化</em> </strong> <em class="lt">的工作原理是构建一个最能描述你想要优化的函数的后验(</em>可能未观测值分布，以观测值为条件<em class="lt">)函数分布(高斯过程)。随着观察值数量的增加，后验分布得到改善，算法变得更加确定参数空间中的哪些区域值得探索，哪些区域不值得探索。</em></p><p id="33e4" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><em class="lt">高斯过程是一种随机过程(</em>由时间或空间索引的随机变量的集合<em class="lt">)，使得这些随机变量的每个有限集合都具有多元正态分布，即它们的每个有限线性组合都是正态分布的。</em>在概率论和统计学中，<em class="lt">多元正态分布、多元高斯分布或联合正态分布是一维(单变量)正态分布向更高维度的推广。</em></p><p id="1f2c" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">高斯过程属于称为<em class="lt"> </em> <strong class="ky jp"> <em class="lt">【基于模型的连续优化(SMBO) </em> </strong> <em class="lt">的算法类别。</em> <a class="ae nd" href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky jp">序贯模型优化(SMBO)方法(SMBO) </strong> </a> <strong class="ky jp"> </strong>是贝叶斯优化的一种形式化。序列是指一个接一个地运行试验，每次都通过应用贝叶斯推理和更新概率模型(代理)来尝试更好的超参数。替代模型的几种常见选择是<a class="ae nd" href="https://en.wikipedia.org/wiki/Gaussian_process" rel="noopener ugc nofollow" target="_blank"><strong class="ky jp"/></a><strong class="ky jp"/><a class="ae nd" href="http://aad.informatik.uni-freiburg.de/papers/13-GECCO-BBOB_SMAC.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ky jp">随机森林回归</strong></a><strong class="ky jp"/>和树Parzen估计器(TPE)。</p><pre class="ml mm mn mo gt mq mr ms mt aw mu bi"><span id="8b08" class="mv mw jf mr b gy mx my l mz na"><strong class="mr jp"><em class="lt">The Steps in Bayesian Distribution(Sequential Model-Based Optimization (SMBO)):-</em></strong></span><span id="02fc" class="mv mw jf mr b gy nb my l mz na"><strong class="mr jp"><em class="lt">1.A domain of hyperparameters over which to search</em></strong></span><span id="42eb" class="mv mw jf mr b gy nb my l mz na"><strong class="mr jp"><em class="lt">2.An objective function which takes in hyperparameters and outputs a score that we want to minimize (or maximize)</em></strong></span><span id="1447" class="mv mw jf mr b gy nb my l mz na"><strong class="mr jp"><em class="lt">3.The surrogate model of the objective function</em></strong></span><span id="1f52" class="mv mw jf mr b gy nb my l mz na"><strong class="mr jp"><em class="lt">4.A criteria, called a selection function, for evaluating which hyperparameters to choose next from the surrogate model</em></strong></span><span id="2e30" class="mv mw jf mr b gy nb my l mz na"><strong class="mr jp"><em class="lt">5.A history consisting of (score, hyperparameter) pairs used by the algorithm to update the surrogate model</em></strong></span></pre><p id="2a71" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><em class="lt">代码中的贝叶斯优化:- </em></p><pre class="ml mm mn mo gt mq mr ms mt aw mu bi"><span id="8d0e" class="mv mw jf mr b gy mx my l mz na"><strong class="mr jp"><em class="lt">#Bayesian Optimization in code</em></strong></span><span id="6805" class="mv mw jf mr b gy nb my l mz na">def data():"""<br/>    Defining a function that defines the data<br/>    """<em class="lt"># Load / Cleaning / Preprocessing</em><br/>    ...<br/>    return x_train, y_train, x_test, y_test<br/>    <br/>def model(x_train, y_train, x_test, y_test):"""<br/>    Model providing function:<br/>    Create Keras model with double curly brackets dropped-in as needed.<br/>    <br/>    """<br/>    <br/>    return {'loss': &lt;metrics_to_minimize&gt;, 'status': STATUS_OK, 'model': model}<br/>    <br/><em class="lt"># SMBO - TPE in action</em><br/>best_run, best_model = optim.minimize(model=model,<br/>                                      data          =data,<br/>                                      algo=tpe.suggest,<br/>                                      max_evals=,<br/>                                      trials=Trials())</span><span id="d8ad" class="mv mw jf mr b gy nb my l mz na"><em class="lt"># Show the results</em><br/>x_train, y_train, x_test, y_test = data()<br/>print("Evalutation of best performing model:")<br/>print(best_model.evaluate(x_test, y_test))<br/>print("Best performing model chosen hyper-parameters:")<br/>print(best_run)</span></pre><p id="397e" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"><em class="lt"/></strong></p><p id="d502" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"> <em class="lt"> Bayes SMBO </em> </strong>只要资源对我们来说不是约束，大概是最好的候选，但是也要考虑用随机搜索建立基线。毕竟，超参数调整的最终目标是减少搜索最佳参数的时间，我们希望在尽可能短的时间内实现这一目标。<strong class="ky jp"> <em class="lt">那么，可以优化训练时间吗？”。</em> </strong></p><p id="51bb" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated"><strong class="ky jp"> <em class="lt">是的，我们可以。</em> <em class="lt">进入提前停止</em> </strong> <em class="lt">。</em> <em class="lt">当训练没有朝着正确的方向进行时，提前停止提供了防止资源浪费的伟大机制。</em></p><p id="f954" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">许多著名的深度学习框架都提供了提前停止的功能，甚至更好的是一套超级有用的<strong class="ky jp"> <em class="lt">回调</em> </strong> <em class="lt">。</em></p><pre class="ml mm mn mo gt mq mr ms mt aw mu bi"><span id="4a3f" class="mv mw jf mr b gy mx my l mz na"><strong class="mr jp"><em class="lt">A callback is a set of functions to be applied at given stages of the training procedure. </em></strong></span><span id="1df9" class="mv mw jf mr b gy nb my l mz na"><strong class="mr jp">#Example in Keras</strong><br/>keras.callbacks.Callback()</span><span id="481b" class="mv mw jf mr b gy nb my l mz na"><strong class="mr jp">#EarlyStopping</strong><br/>keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, <br/>              verbose=0, mode='auto', baseline=None, restore_best_weights=False)</span></pre><p id="b03c" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">如果我们还记得，在文章的开始，我们将其命名为<strong class="ky jp"> <em class="lt">定义模型时选择正确超参数的重要性。</em> </strong> <em class="lt">我想现在可以看出背后的想法了。</em>由于超参数调整占用了处理模型的大部分时间，并且它们决定了性能，从长远来看，我们需要采取措施来调整它们，这反过来又决定了模型的稳定性。<em class="lt">为我们的模型选择正确的超参数不仅会决定模型达到收敛所需的时间，还会决定更快获得更好结果所需的决策和迭代</em>。</p><p id="4b6c" class="pw-post-body-paragraph kw kx jf ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls kv ij bi translated">下次见…再见..！！！</p></div></div>    
</body>
</html>