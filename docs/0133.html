<html>
<head>
<title>Address Limitation of RNN in NLP Problems by Using Transformer-XL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用Transformer-XL解决自然语言处理问题中RNN的局限性</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/address-limitation-of-rnn-in-nlp-problems-by-using-transformer-xl-866d7ce1c8f4?source=collection_archive---------0-----------------------#2019-08-12">https://pub.towardsai.net/address-limitation-of-rnn-in-nlp-problems-by-using-transformer-xl-866d7ce1c8f4?source=collection_archive---------0-----------------------#2019-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d3d2050f8a332c252bd588209a6a5b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y9CyuRjQBgbeHuuD"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@josephgardnerphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔·加德纳</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><h2 id="9e5e" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">通过使用Transformer-XL | <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">朝向AI </a>解决rnn的限制</h2><div class=""/><div class=""><h2 id="d698" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">递归神经网络的局限性</h2></div><p id="bf16" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">递归神经网络(RNN)提供了一种学习输入序列的方法。缺点是很难优化，由于消失梯度问题。引入Transformer (Al-Rfou等人，2018)是为了克服RNN的局限性。通过设计，定义了固定长度的数据段以减少资源消耗。</p><p id="233a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是，还有一个问题叫做上下文碎片。如果输入序列大于预定义的段长度，则需要将输入序列分开，并且不能跨段捕获信息。戴等人(2019)引入Transformer-XL来克服这一限制</p><h1 id="3f41" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">香草变压器</h1><p id="aaa3" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">为了减少计算资源，输入序列被固定长度分割。戴等人将其命名为香草变压器。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/e6ac03811eb200eb4823311803234bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*pbpGCCLNKaAqvQqPVyDPVA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">在普通变压器架构下，信息不能跨部门共享(戴等人，2019年)</figcaption></figure><p id="fdc5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第一个限制是信息不能跨网段共享。虽然<code class="fe nf ng nh ni b">Transformer</code>受消失梯度问题的影响较小，但如果输入序列的长度固定，它会限制其能力。第二个限制是由填充引起的。由于需要固定长度的输入，如果输入的长度短于预定义的长度，则需要填充。它不尊重句子和语义的界限。</p><h1 id="7362" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">变压器-XL</h1><p id="867e" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">Transformer-XL(超长)的诞生是为了解决普通变压器的局限性。</p><p id="554b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在计算下一个片段时，将使用前一个片段的隐藏状态序列，而不是在片段之间断开连接。理论上，我们可以添加多个先前的段，使得当前的段可以跨段到达更多的信息。</p><p id="b07e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另一个输入特性是位置编码。利用相对位置编码而不是绝对位置来防止误导。因此，任何单词都有每个单词的相对距离，这有助于改进模型训练。</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/a7dc85baa03678d476b2ead8e7018b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rhSWVOaVYBga-e7lpYOS6g.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">Transformer-XL架构(戴等，2019)</figcaption></figure><h1 id="c77b" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">拿走</h1><ul class=""><li id="b0aa" class="nk nl jj lj b lk mv ln mw lq nm lu nn ly no mc np nq nr ns bi translated">克服了以前NLP模型的一些限制，如最大信息长度。</li><li id="731d" class="nk nl jj lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated">如果使用PyTorch，您可以使用<a class="ae jg" href="https://github.com/huggingface/pytorch-transformers" rel="noopener ugc nofollow" target="_blank">humping Face的PyTorch-transformers </a>库来训练模型。对于Keras的用户，你可能想试试这个<a class="ae jg" href="https://github.com/kpot/keras-transformer" rel="noopener ugc nofollow" target="_blank">库</a>。</li></ul><h1 id="94ec" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph lh li jj lj b lk mv kt lm ln mw kw lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。在<a class="ae jg" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae jg" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上随时联系<a class="ae jg" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>。</p><h1 id="1470" class="md me jj bd mf mg mh mi mj mk ml mm mn ky mo kz mp lb mq lc mr le ms lf mt mu bi translated">参考</h1><ul class=""><li id="8cbb" class="nk nl jj lj b lk mv ln mw lq nm lu nn ly no mc np nq nr ns bi translated">R.f-r-fou、d-Choe、n-Constant、M. Guo和L. Jones。<a class="ae jg" href="https://arxiv.org/pdf/1808.04444.pdf" rel="noopener ugc nofollow" target="_blank">具有更深自我关注的角色级语言建模</a>。2018</li><li id="2674" class="nk nl jj lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated">Z.戴、杨振宁、杨、卡波内尔、乐庆伟和萨拉胡季诺夫。<a class="ae jg" href="https://arxiv.org/pdf/1901.02860.pdf" rel="noopener ugc nofollow" target="_blank"> Transformer-XL:超越固定长度上下文的注意力语言模型</a>。2019</li></ul></div></div>    
</body>
</html>