<html>
<head>
<title>Bayesian Classification Algorithm From Scratch in Python Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python熊猫中从零开始的贝叶斯分类算法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/bayesian-classification-algorithm-from-scratch-in-python-pandas-f573c3376088?source=collection_archive---------1-----------------------#2022-06-09">https://pub.towardsai.net/bayesian-classification-algorithm-from-scratch-in-python-pandas-f573c3376088?source=collection_archive---------1-----------------------#2022-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e2bcc10d8504a92286f3fc5bcab2904b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*trU9xyzr6Q02E2SI"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://unsplash.com/@jeshoots?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">JESHOOTS.COM</a>在<a class="ae jg" href="https://unsplash.com/s/photos/mathematics?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><div class=""/><h1 id="1d0e" class="kg kh jj bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">目录:</h1><p id="5554" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">理解贝叶斯分类器的工作原理</p><p id="c4fd" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">二。预处理您的数据</p><ul class=""><li id="3f0a" class="mh mi jj lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated">使用训练数据计算数据集中每个要素朝向目标变量类的条件概率。</li><li id="4e8a" class="mh mi jj lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated">使用训练数据计算先验概率</li></ul><p id="4188" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">三。应用于您的测试数据</p><ul class=""><li id="40e1" class="mh mi jj lg b lh mc ll md lp mj lt mk lx ml mb mm mn mo mp bi translated">给定行上的属性值，使用先验概率乘以类的条件概率来计算类得分</li><li id="4ed0" class="mh mi jj lg b lh mq ll mr lp ms lt mt lx mu mb mm mn mo mp bi translated">使用最高分数预测班级</li></ul><p id="da34" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">四。计算结果</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="bac1" class="nc kh jj bd ki nd ne dn km nf ng dp kq lp nh ni ku lt nj nk ky lx nl nm lc nn bi translated">I .了解贝叶斯分类器的工作原理</h2><p id="dd5c" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae jg" href="https://www.youtube.com/watch?v=O2L2Uv9pdDA&amp;t=550s" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯，解释得很清楚！！！— YouTube </a></p><p id="6356" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">请观看上述视频并订阅Josh Stramer的Statquest频道，以便了解贝叶斯分类背后的统计数据。这也将在我下面的代码中解释。</p><p id="d8fd" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这篇文章不是关于文本分类的。事实上，本文中构建的分类器将处理连续变量。</p><h2 id="01d5" class="nc kh jj bd ki nd ne dn km nf ng dp kq lp nh ni ku lt nj nk ky lx nl nm lc nn bi translated">二。预处理您的数据</h2><p id="785e" class="pw-post-body-paragraph le lf jj lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于我的工作，我使用谷歌协作来编码。但是，您可以使用任何您选择的Python Ide或Jupyter笔记本。</p><p id="102d" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">首先，我导入熊猫，然后在我的代码中加入iris CSV:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="d535" class="nc kh jj nt b gy nx ny l nz oa">import pandas as pd</span><span id="a2cf" class="nc kh jj nt b gy ob ny l nz oa">df = pd.read_csv(r'/content/iris.csv')</span></pre><p id="2a9c" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">接下来，我将把数据分成两个数据帧。您在下面看到的是，80%的总行数将被</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="7618" class="nc kh jj nt b gy nx ny l nz oa">#seperate train test)</span><span id="310d" class="nc kh jj nt b gy ob ny l nz oa">df_train = df.sample(frac=0.8, random_state=1)</span><span id="31a3" class="nc kh jj nt b gy ob ny l nz oa">df_test=df.drop(df_train.index)</span><span id="60d3" class="nc kh jj nt b gy ob ny l nz oa">df.head()</span></pre><p id="41d9" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">df.head将允许您查看iris数据的前五行以及列:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/3aad07c352cf1dea6b8c8a906128dea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*Q7d0DhN4Q87USPfdYhM6EQ.png"/></div></figure><p id="4098" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如您所见，iris数据集包含四列:</p><p id="77c1" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">1 —萼片长度</p><p id="0691" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">2-萼片宽度</p><p id="0dcf" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">3-花瓣长度</p><p id="43e8" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">4-花瓣宽度</p><p id="3cab" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">注意，所有这些都是连续的数值变量。它们在本质上不是绝对的，比如单词。</p><p id="80ff" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们继续将这些离散化成桶。对于该算法，我们将使用四个桶:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="c7bb" class="nc kh jj nt b gy nx ny l nz oa"><em class="od">#set 4 buckets for each column in train set</em><br/>df_train['quantile_SL_Disc'] <strong class="nt jk">=</strong> pd<strong class="nt jk">.</strong>qcut(df['sepal.length'], q<strong class="nt jk">=</strong>4,labels<strong class="nt jk">=None</strong>, precision<strong class="nt jk">=</strong>2)<br/>df_train['quantile_SW_Disc'] <strong class="nt jk">=</strong> pd<strong class="nt jk">.</strong>qcut(df['sepal.width'], q<strong class="nt jk">=</strong>4,labels<strong class="nt jk">=None</strong>,precision<strong class="nt jk">=</strong>2)<br/>df_train['quantile_PL_Disc'] <strong class="nt jk">=</strong> pd<strong class="nt jk">.</strong>qcut(df['petal.length'], q<strong class="nt jk">=</strong>4,labels<strong class="nt jk">=None</strong>,precision<strong class="nt jk">=</strong>2)<br/>df_train['quantile_PW_Disc2'] <strong class="nt jk">=</strong> pd<strong class="nt jk">.</strong>qcut(df['petal.width'], q<strong class="nt jk">=</strong>4,labels<strong class="nt jk">=None</strong>,precision<strong class="nt jk">=</strong>2)</span></pre><p id="03a9" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们分解上面的一行代码，看看发生了什么:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="b0b4" class="nc kh jj nt b gy nx ny l nz oa">df_train['quantile_SL_Disc'] <strong class="nt jk">=</strong> pd<strong class="nt jk">.</strong>qcut(df['sepal.length'], q<strong class="nt jk">=</strong>4,labels<strong class="nt jk">=None</strong>, precision<strong class="nt jk">=</strong>2)</span></pre><p id="5c39" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是怎么回事？嗯，在等号的左侧，我在训练数据帧中创建了一个名为“quantile_SL_Disc”的列。</p><blockquote class="oe of og"><p id="61e7" class="le lf od lg b lh mc lj lk ll md ln lo oh me lr ls oi mf lv lw oj mg lz ma mb im bi translated">分位数_SL_Disc是离散化分位数萼片长度的缩写。</p></blockquote><p id="41ee" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在右边，我们通过使用pd.qcut(开始应用方法qcut。</p><p id="a437" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">qcut是pandas中的一种方法，允许您通过传递参数q来设置桶的数量。您可以键入标签名称，还可以设置桶边框中的小数精度。</p><blockquote class="oe of og"><p id="61a7" class="le lf od lg b lh mc lj lk ll md ln lo oh me lr ls oi mf lv lw oj mg lz ma mb im bi translated">在这种情况下，我设置q=4，因此它会给我四个桶，在第25、50、75个百分比截止。</p></blockquote><p id="b343" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如上所示(再一次在下图中)，我对所有四列都这样做了:</p><p id="2ac2" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg jk">注意:我必须对其中一列使用quantile_PW_Disc2，因为我稍后必须对该列进行少量的数据清理。</strong></p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/ef8a696482ff69199c5e08eff665f3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ggUvaopJMbVbG6zXcQRzQA.png"/></div></div></figure><p id="7e30" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是它看起来的样子。正如您在quantile_PW_Disc2中看到的，我们看到一个. 090000…的数字，它大于我们选择的两个精度。这是一个bug，这就是为什么我在最后用2来命名这个列，这样我可以清理它。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/7f5e0086901e7ac5f09d6d629909ead6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TTgIPpfGMn-6yhV2A44Ohg.png"/></div></div></figure><p id="9eda" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们有了上面的内容，让我们使用正则表达式将qcut范围分为低部分和高部分，用于所有四个qcut:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="ee1a" class="nc kh jj nt b gy nx ny l nz oa"><em class="od">#split the quantile low high into two parts </em><br/>df_train[['SL_low','SL_high']] <strong class="nt jk">=</strong> df_train['quantile_SL_Disc']<strong class="nt jk">.</strong>astype(str)<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>split(',',expand<strong class="nt jk">=True</strong>)<br/>df_train[['SW_low','SW_high']] <strong class="nt jk">=</strong> df_train['quantile_SW_Disc']<strong class="nt jk">.</strong>astype(str)<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>split(',',expand<strong class="nt jk">=True</strong>)<br/>df_train[['PL_low','PL_high']] <strong class="nt jk">=</strong> df_train['quantile_PL_Disc']<strong class="nt jk">.</strong>astype(str)<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>split(',',expand<strong class="nt jk">=True</strong>)<br/>df_train[['PW_low','PW_high']] <strong class="nt jk">=</strong> df_train['quantile_PW_Disc2']<strong class="nt jk">.</strong>astype(str)<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>split(',',expand<strong class="nt jk">=True</strong>)</span></pre><p id="74cd" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">好了，让我们创建一个PW_low_2列，正则表达式去掉除小数点后前两位以外的所有数字。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="c78a" class="nc kh jj nt b gy nx ny l nz oa">df_train['PW_low_2']<strong class="nt jk">=</strong>df_train['PW_low']<strong class="nt jk">.</strong>str[:5]</span></pre><p id="1f6b" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在让我们将低位和高位一起放回quantile_PW_Disc:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="aa74" class="nc kh jj nt b gy nx ny l nz oa">df_train['quantile_PW_Disc']<strong class="nt jk">=</strong>df_train['PW_low_2']<strong class="nt jk">+</strong>df_train['PW_high']</span></pre><p id="4e35" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们通过键入以下命令来看看这将是什么样子:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="36f5" class="nc kh jj nt b gy nx ny l nz oa">df_train<strong class="nt jk">.</strong>head()</span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/5b38d6bb940a5093192036dfa71533ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x8jQbidYyNHkeOE1vywZow.png"/></div></div></figure><p id="7ae8" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">正如你所看到的，我们把柱子分成了高低两部分。</p><p id="72c5" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在我们需要更多的正则表达式来去掉括号:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="1b0b" class="nc kh jj nt b gy nx ny l nz oa">df_train['SL_low']<strong class="nt jk">=</strong>df_train['SL_low']<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>replace('(','')<br/>df_train['SL_high']<strong class="nt jk">=</strong>df_train['SL_high']<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>replace(']','')<br/>df_train['SW_low']<strong class="nt jk">=</strong>df_train['SW_low']<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>replace('(','')<br/>df_train['SW_high']<strong class="nt jk">=</strong>df_train['SW_high']<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>replace(']','')<br/>df_train['PL_low']<strong class="nt jk">=</strong>df_train['PL_low']<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>replace('(','')<br/>df_train['PL_high']<strong class="nt jk">=</strong>df_train['PL_high']<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>replace(']','')<br/>df_train['PW_low_2']<strong class="nt jk">=</strong>df_train['PW_low_2']<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>replace('(','')<br/>df_train['PW_high']<strong class="nt jk">=</strong>df_train['PW_high']<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>replace(']','')<br/>df_train<strong class="nt jk">.</strong>head()</span></pre><p id="0f69" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">df_train.head()将打印出下面的内容，并给出我们需要的内容。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/58747b18440c47f799dab31b40c2908c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nkhOIuaGDmWteH5rhS1QSg.png"/></div></div></figure><p id="0c39" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们可以分组来得到概率。这些概率是我们的第一个猜测:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="b940" class="nc kh jj nt b gy nx ny l nz oa"><em class="od">#group by low high and count for each flower</em><br/><em class="od">#setosa probability</em><br/>df_setosa <strong class="nt jk">=</strong> df_train[df_train['variety']<strong class="nt jk">.</strong>isin(['Setosa'])]<br/>df_setosaSL <strong class="nt jk">=</strong> df_setosa<strong class="nt jk">.</strong>groupby(['quantile_SL_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/>df_setosaSW <strong class="nt jk">=</strong> df_setosa<strong class="nt jk">.</strong>groupby(['quantile_SW_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/>df_setosaPL <strong class="nt jk">=</strong> df_setosa<strong class="nt jk">.</strong>groupby(['quantile_PL_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/>df_setosaPW <strong class="nt jk">=</strong> df_setosa<strong class="nt jk">.</strong>groupby(['quantile_PW_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/><br/><em class="od">#probability of setosa given that it has that sepal length quantile group i.e p(that quantile group | setosa)</em><br/>df_setosaSL['probability_setosa_sepal_length'] <strong class="nt jk">=</strong> df_setosaSL['count']<strong class="nt jk">/</strong>df_setosaSL['count']<strong class="nt jk">.</strong>sum()<br/>df_setosaSW['probability_setosa_sepal_width'] <strong class="nt jk">=</strong> df_setosaSW['count']<strong class="nt jk">/</strong>df_setosaSW['count']<strong class="nt jk">.</strong>sum()<br/>df_setosaPL['probability_setosa_petal_length'] <strong class="nt jk">=</strong> df_setosaPL['count']<strong class="nt jk">/</strong>df_setosaPL['count']<strong class="nt jk">.</strong>sum()<br/>df_setosaPW['probability_setosa_petal_width'] <strong class="nt jk">=</strong> df_setosaPW['count']<strong class="nt jk">/</strong>df_setosaPW['count']<strong class="nt jk">.</strong>sum()<br/><br/></span></pre><p id="ce79" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们仔细检查其中一个:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="4786" class="nc kh jj nt b gy nx ny l nz oa">df_setosaPW<strong class="nt jk">.</strong>head()</span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/862c1f52bd1adeb256c24183a8035055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*TSq-EGPG1TOc4ekxGytTqw.png"/></div></figure><blockquote class="oe of og"><p id="8ef9" class="le lf od lg b lh mc lj lk ll md ln lo oh me lr ls oi mf lv lw oj mg lz ma mb im bi translated">正如你所看到的，这告诉我们当花瓣宽度在0.09和0.3之间时，一朵花被衬托的概率是0.83或83%。</p></blockquote><p id="9d8d" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在条件概率术语中，这被写成P(Setosa |花瓣宽度在. 09和. 3之间)= .83。</p><p id="480f" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们为virginica做同样的事情:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="dbc2" class="nc kh jj nt b gy nx ny l nz oa"><em class="od">#group by low high and count for each flower</em><br/><em class="od">#virginica probability</em><br/>df_virginica <strong class="nt jk">=</strong> df_train[df_train['variety']<strong class="nt jk">.</strong>isin(['Virginica'])]<br/>df_virginicaSL <strong class="nt jk">=</strong> df_virginica<strong class="nt jk">.</strong>groupby(['quantile_SL_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/>df_virginicaSW <strong class="nt jk">=</strong> df_virginica<strong class="nt jk">.</strong>groupby(['quantile_SW_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/>df_virginicaPL <strong class="nt jk">=</strong> df_virginica<strong class="nt jk">.</strong>groupby(['quantile_PL_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/>df_virginicaPW <strong class="nt jk">=</strong> df_virginica<strong class="nt jk">.</strong>groupby(['quantile_PW_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/><br/><em class="od">#probability of virginica given that it has that sepal length quantile group i.e p(that quantile group | setosa)</em><br/>df_virginicaSL['probability_virginica_sepal_length'] <strong class="nt jk">=</strong> df_virginicaSL['count']<strong class="nt jk">/</strong>df_virginicaSL['count']<strong class="nt jk">.</strong>sum()<br/>df_virginicaSW['probability_virginica_sepal_width'] <strong class="nt jk">=</strong> df_virginicaSW['count']<strong class="nt jk">/</strong>df_virginicaSW['count']<strong class="nt jk">.</strong>sum()<br/>df_virginicaPL['probability_virginica_petal_length'] <strong class="nt jk">=</strong> df_virginicaPL['count']<strong class="nt jk">/</strong>df_virginicaPL['count']<strong class="nt jk">.</strong>sum()<br/>df_virginicaPW['probability_virginica_petal_width'] <strong class="nt jk">=</strong> df_virginicaPW['count']<strong class="nt jk">/</strong>df_virginicaPW['count']<strong class="nt jk">.</strong>sum()<br/><br/>df_virginicaPW<strong class="nt jk">.</strong>head()</span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ef846e2b7dfe3aabbb8f7a5a2e8aaaba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*PQip6WAVlyEVIBrP8wJlsQ.png"/></div></figure><p id="f419" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们可以为云芝做这件事:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="7028" class="nc kh jj nt b gy nx ny l nz oa"><em class="od">#group by low high and count for each flower</em><br/><em class="od">#Versicolor probability</em><br/>df_Versicolor <strong class="nt jk">=</strong> df_train[df_train['variety']<strong class="nt jk">.</strong>isin(['Versicolor'])]<br/>df_VersicolorSL <strong class="nt jk">=</strong> df_Versicolor<strong class="nt jk">.</strong>groupby(['quantile_SL_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/>df_VersicolorSW <strong class="nt jk">=</strong> df_Versicolor<strong class="nt jk">.</strong>groupby(['quantile_SW_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/>df_VersicolorPL <strong class="nt jk">=</strong> df_Versicolor<strong class="nt jk">.</strong>groupby(['quantile_PL_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/>df_VersicolorPW <strong class="nt jk">=</strong> df_Versicolor<strong class="nt jk">.</strong>groupby(['quantile_PW_Disc'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count')<br/><br/><em class="od">#probability of Versicolor given that it has that sepal length quantile group i.e p(that quantile group | setosa)</em><br/>df_VersicolorSL['probability_Versicolor_sepal_length'] <strong class="nt jk">=</strong> df_VersicolorSL['count']<strong class="nt jk">/</strong>df_VersicolorSL['count']<strong class="nt jk">.</strong>sum()<br/>df_VersicolorSW['probability_Versicolor_sepal_width'] <strong class="nt jk">=</strong> df_VersicolorSW['count']<strong class="nt jk">/</strong>df_VersicolorSW['count']<strong class="nt jk">.</strong>sum()<br/>df_VersicolorPL['probability_Versicolor_petal_length'] <strong class="nt jk">=</strong> df_VersicolorPL['count']<strong class="nt jk">/</strong>df_VersicolorPL['count']<strong class="nt jk">.</strong>sum()<br/>df_VersicolorPW['probability_Versicolor_petal_width'] <strong class="nt jk">=</strong> df_VersicolorPW['count']<strong class="nt jk">/</strong>df_VersicolorPW['count']<strong class="nt jk">.</strong>sum()<br/><br/>df_VersicolorPW<strong class="nt jk">.</strong>head()</span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/b4acdbae971a3e48d67c0289e32501e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*e1LalhdP0Jh5wqTWV-h5lw.png"/></div></figure><p id="27cf" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们还需要训练集中的先验概率。这是我们对贝叶斯分类器将开始的每朵花的猜测:</p><p id="d987" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为此，首先，我们可以统计每个类在训练集中的出现次数。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="5189" class="nc kh jj nt b gy nx ny l nz oa"><em class="od"># probability of each class - prior probability in training</em><br/><br/>df_prior<strong class="nt jk">=</strong>pd<strong class="nt jk">.</strong>DataFrame(df_train<strong class="nt jk">.</strong>groupby(['variety'])<strong class="nt jk">.</strong>size()<strong class="nt jk">.</strong>reset_index(name<strong class="nt jk">=</strong>'count'))</span><span id="2846" class="nc kh jj nt b gy ob ny l nz oa">df_prior<strong class="nt jk">.</strong>head()</span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/51c3507a58aa13621bf9fa3fad1c6889.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*4vB6EpM1vMocaVG-C4Sn8g.png"/></div></figure><p id="5898" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">接下来，我们可以使用这个计数来计算每个类的百分比:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="d1bd" class="nc kh jj nt b gy nx ny l nz oa">df_prior['prior']<strong class="nt jk">=</strong>df_prior['count']<strong class="nt jk">/</strong>df_prior['count']<strong class="nt jk">.</strong>sum()</span><span id="dd5d" class="nc kh jj nt b gy ob ny l nz oa">df_prior<strong class="nt jk">.</strong>head()</span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d21a86b9c6fd709c488116c0db0c841f.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*IGj5Xe--6iDEG3ZB3De4mQ.png"/></div></figure><p id="c3b2" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是我们的先验概率。例如，P(Setosa) = .333333。</p><p id="68cd" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们将转置这个数据集，使它看起来像下面。我们转置数据的原因是为了将这些概率包含在我们要分类的测试数据的行中。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="b2f3" class="nc kh jj nt b gy nx ny l nz oa">df_priorT<strong class="nt jk">.</strong>columns <strong class="nt jk">=</strong> df_priorT<strong class="nt jk">.</strong>iloc[0] <br/><br/>df_priorprob<strong class="nt jk">=</strong>df_priorT<strong class="nt jk">.</strong>drop(['count','variety'], inplace <strong class="nt jk">=</strong> <strong class="nt jk">True</strong>)<br/><br/>df_priorT<strong class="nt jk">.</strong>head()</span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/113c6f4d396dbda39d2e37ea26cdf9e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*kERIiyimPrKfzwGHz_1Pvw.png"/></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="914c" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">三。应用于您的测试数据</p><p id="4c88" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们需要将我们的先验概率数据推送到测试数据集的每一行。为此，我们可以做一个交叉连接。交叉连接基本上返回一个表，其中包含两个表之间的所有行组合。在这种情况下，由于我们的先验概率数据有1行，结果将返回测试数据中的行数。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="e59e" class="nc kh jj nt b gy nx ny l nz oa"><em class="od">#cross join the prior probability of training with test</em><br/>df_test['key'] <strong class="nt jk">=</strong> 0<br/>df_priorT['key'] <strong class="nt jk">=</strong> 0<br/><br/>df_test<strong class="nt jk">=</strong>df_test<strong class="nt jk">.</strong>merge(df_priorT, on<strong class="nt jk">=</strong>'key', how<strong class="nt jk">=</strong>'outer')<br/>len(df_test)</span></pre><p id="b4e7" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">输出如下图所示<strong class="lg jk">。正如您所看到的，我们在前几列中有测试数据，然后我们有三列，在这三列上我们有来自我们的训练数据的先验概率。</strong></p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/c6600e723b5e1b2971236dda0102b73b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*byLDohf4QVJT1dFqjv8bsQ.png"/></div></div></figure><p id="ef8e" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg jk">现在，我们要削减我们的测试数据特征。但是，这一次，我们不会做qcut，我们会做cut。cut允许我们指定箱的截止值，这一次，我们将使用qcut从上面的训练数据集中给我们的截止值。这样做的原因是，当涉及到测试集中的特征时，我们可以从训练数据中识别哪个箱匹配哪个行。</strong></p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="ef25" class="nc kh jj nt b gy nx ny l nz oa"><br/>df_test['SL_bins'] <strong class="nt jk">=</strong> pd<strong class="nt jk">.</strong>cut(x<strong class="nt jk">=</strong>df_test['sepal.length'], bins<strong class="nt jk">=</strong>[4.29, 5.1, 5.8, 6.4, 7.9])<br/>df_test['SW_bins'] <strong class="nt jk">=</strong> pd<strong class="nt jk">.</strong>cut(x<strong class="nt jk">=</strong>df_test['sepal.width'], bins<strong class="nt jk">=</strong>[1.99, 2.8, 3.0, 3.3, 4.4])<br/>df_test['PL_bins'] <strong class="nt jk">=</strong> pd<strong class="nt jk">.</strong>cut(x<strong class="nt jk">=</strong>df_test['petal.length'], bins<strong class="nt jk">=</strong>[0.99, 1.6, 4.35, 5.1, 6.9])<br/>df_test['PW_bins'] <strong class="nt jk">=</strong> pd<strong class="nt jk">.</strong>cut(x<strong class="nt jk">=</strong>df_test['petal.width'], bins<strong class="nt jk">=</strong>[0.09, 0.3, 1.3, 1.8, 2.5])</span></pre><p id="6a19" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">结果集现在将如下所示:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/cbe33b68d5085cc15f36b2b4765446a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uL6i-dGlTK0Bh5dheDAcLQ.png"/></div></div></figure><p id="aa07" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们将在测试帧上进行重复连接，以获得一个条件概率，即假设一个流的特征属于一个特定的桶，那么这个流就是一个特定的类。例如P(刚毛|萼片长度bin为4.29至5.1)等。</p><p id="aff5" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为此，代码将如下所示。每个连接都基于作为主键的存储桶。<strong class="lg jk">我们正在加入的是我们对概率表的测试集，这些概率表来自本文第二部分的训练集。</strong></p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="3257" class="nc kh jj nt b gy nx ny l nz oa">df_test1<strong class="nt jk">=</strong>df_test<strong class="nt jk">.</strong>merge(df_setosaSL[['quantile_SL_Disc','probability_setosa_sepal_length']], left_on<strong class="nt jk">=</strong>'SL_bins',right_on<strong class="nt jk">=</strong>'quantile_SL_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_SL_Disc'])<br/>df_test2<strong class="nt jk">=</strong>df_test1<strong class="nt jk">.</strong>merge(df_virginicaSL[['quantile_SL_Disc','probability_virginica_sepal_length']], left_on<strong class="nt jk">=</strong>'SL_bins',right_on<strong class="nt jk">=</strong>'quantile_SL_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_SL_Disc'])<br/>df_test3<strong class="nt jk">=</strong>df_test2<strong class="nt jk">.</strong>merge(df_VersicolorSL[['quantile_SL_Disc','probability_Versicolor_sepal_length']], left_on<strong class="nt jk">=</strong>'SL_bins',right_on<strong class="nt jk">=</strong>'quantile_SL_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_SL_Disc'])<br/>df_test4<strong class="nt jk">=</strong>df_test3<strong class="nt jk">.</strong>merge(df_setosaSW[['quantile_SW_Disc','probability_setosa_sepal_width']], left_on<strong class="nt jk">=</strong>'SW_bins',right_on<strong class="nt jk">=</strong>'quantile_SW_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_SW_Disc'])<br/>df_test5<strong class="nt jk">=</strong>df_test4<strong class="nt jk">.</strong>merge(df_virginicaSW[['quantile_SW_Disc','probability_virginica_sepal_width']], left_on<strong class="nt jk">=</strong>'SW_bins',right_on<strong class="nt jk">=</strong>'quantile_SW_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_SW_Disc'])<br/>df_test6<strong class="nt jk">=</strong>df_test5<strong class="nt jk">.</strong>merge(df_VersicolorSW[['quantile_SW_Disc','probability_Versicolor_sepal_width']], left_on<strong class="nt jk">=</strong>'SW_bins',right_on<strong class="nt jk">=</strong>'quantile_SW_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_SW_Disc'])<br/>df_test7<strong class="nt jk">=</strong>df_test6<strong class="nt jk">.</strong>merge(df_setosaPL[['quantile_PL_Disc','probability_setosa_petal_length']], left_on<strong class="nt jk">=</strong>'PL_bins',right_on<strong class="nt jk">=</strong>'quantile_PL_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_PL_Disc'])<br/>df_test8<strong class="nt jk">=</strong>df_test7<strong class="nt jk">.</strong>merge(df_virginicaPL[['quantile_PL_Disc','probability_virginica_petal_length']], left_on<strong class="nt jk">=</strong>'PL_bins',right_on<strong class="nt jk">=</strong>'quantile_PL_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_PL_Disc'])<br/>df_test9<strong class="nt jk">=</strong>df_test8<strong class="nt jk">.</strong>merge(df_VersicolorPL[['quantile_PL_Disc','probability_Versicolor_petal_length']], left_on<strong class="nt jk">=</strong>'PL_bins',right_on<strong class="nt jk">=</strong>'quantile_PL_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_PL_Disc'])<br/>df_test10<strong class="nt jk">=</strong>df_test9<strong class="nt jk">.</strong>merge(df_setosaPW[['quantile_PW_Disc','probability_setosa_petal_width']], left_on<strong class="nt jk">=</strong>'PW_bins',right_on<strong class="nt jk">=</strong>'quantile_PW_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_PW_Disc'])<br/>df_test11<strong class="nt jk">=</strong>df_test10<strong class="nt jk">.</strong>merge(df_virginicaPW[['quantile_PW_Disc','probability_virginica_petal_width']], left_on<strong class="nt jk">=</strong>'PW_bins',right_on<strong class="nt jk">=</strong>'quantile_PW_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_PW_Disc'])<br/><em class="od"># #df_test11.head()</em><br/>df_test12<strong class="nt jk">=</strong>df_test11<strong class="nt jk">.</strong>merge(df_VersicolorPW[['quantile_PW_Disc','probability_Versicolor_petal_width']], left_on<strong class="nt jk">=</strong>'PW_bins',right_on<strong class="nt jk">=</strong>'quantile_PW_Disc', how<strong class="nt jk">=</strong>'left')<strong class="nt jk">.</strong>drop(columns <strong class="nt jk">=</strong> ['quantile_PW_Disc'])<br/><em class="od"># #df_test12.head()</em><br/>df_test12<strong class="nt jk">.</strong>head()<br/><br/></span></pre><p id="ba32" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">接下来，我们有一些nans。这样做的原因是，如果一个桶中有0朵花，上面的qcut不会创建桶。所以我们需要用1来填充我们的nans。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="b0db" class="nc kh jj nt b gy nx ny l nz oa"><strong class="nt jk">import</strong> numpy <strong class="nt jk">as</strong> np<br/>df_test12<strong class="nt jk">=</strong>df_test12<strong class="nt jk">.</strong>replace(np<strong class="nt jk">.</strong>nan, 1)</span></pre><p id="60f6" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们用1填充NANs的原因是这样的。记住贝叶斯分类器是这样工作的:</p><blockquote class="oe of og"><p id="5c75" class="le lf od lg b lh mc lj lk ll md ln lo oh me lr ls oi mf lv lw oj mg lz ma mb im bi translated"><strong class="lg jk"> P(来自训练集的第一次猜测)* P(类|特征1在特定桶中)* P(类|特征2在特定桶中)* P(类|特征3在特定桶中)* P(类|特征4在特定桶中)</strong></p></blockquote><p id="8b3a" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是我们在每个类的每一行上计算的。得分最高的将是预测的班级。这就是为什么我们不能有0的条件概率，所以我们使用1，如果这个组不存在，所以我们乘以1，保持我们的分数不变。</p><p id="440b" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们来计算分数:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="100c" class="nc kh jj nt b gy nx ny l nz oa"><em class="od">#calculate score of setosa for each row</em><br/><em class="od">#prior probability of setosa in training set * probability_setosa_sepal_length_x (this second term is the probability of setosa given the sepal length is in that bucket)</em><br/>df_test12['Setosa_score'] <strong class="nt jk">=</strong> df_test12['Setosa'] <strong class="nt jk">*</strong> df_test12['probability_setosa_sepal_length']<strong class="nt jk">*</strong>df_test12['probability_setosa_sepal_width']<strong class="nt jk">*</strong>df_test12['probability_setosa_petal_width']<strong class="nt jk">*</strong>df_test12['probability_setosa_petal_length']<br/><br/><em class="od">#calculate score of virginica for each row</em><br/><em class="od">#prior probability of virginica in training set * probability_virginica_sepal_length_x (this second term is the probability of virginica given the sepal length is in that bucket)</em><br/>df_test12['Virginica_score'] <strong class="nt jk">=</strong> df_test12['Virginica'] <strong class="nt jk">*</strong> df_test12['probability_virginica_sepal_length']<strong class="nt jk">*</strong>df_test12['probability_virginica_sepal_width']<strong class="nt jk">*</strong>df_test12['probability_virginica_petal_width']<strong class="nt jk">*</strong>df_test12['probability_virginica_petal_length']<br/><br/><br/><em class="od">#calculate score of virginica for each row</em><br/><em class="od">#prior probability of setosa in training set * probability_setosa_sepal_length_x (this second term is the probability of setosa given the sepal length is in that bucket)</em><br/>df_test12['Versicolor_score'] <strong class="nt jk">=</strong> df_test12['Versicolor'] <strong class="nt jk">*</strong> df_test12['probability_Versicolor_sepal_length']<strong class="nt jk">*</strong>df_test12['probability_Versicolor_sepal_width']<strong class="nt jk">*</strong>df_test12['probability_Versicolor_petal_width']<strong class="nt jk">*</strong>df_test12['probability_Versicolor_petal_length']</span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/bd66c1140b713436deedeb44d1661acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7jVXd1O1CX7rBhJTL__AFw.png"/></div></div></figure><p id="125f" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们需要添加一个预测列，其类文本与max score的列名相匹配。例如，如果Setosa_score列是最大值，它将在该列中打印出setosa_score。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="dd20" class="nc kh jj nt b gy nx ny l nz oa">df_test12['Setosa_score'] <strong class="nt jk">=</strong> df_test12['Setosa_score']<strong class="nt jk">.</strong>astype(float)<br/>df_test12['Virginica_score'] <strong class="nt jk">=</strong> df_test12['Virginica_score']<strong class="nt jk">.</strong>astype(float)<br/>df_test12['Versicolor_score'] <strong class="nt jk">=</strong> df_test12['Versicolor_score']<strong class="nt jk">.</strong>astype(float)<br/><br/><br/>df_test12['predicted'] <strong class="nt jk">=</strong> df_test12[['Setosa_score','Virginica_score','Versicolor_score']]<strong class="nt jk">.</strong>idxmax(axis<strong class="nt jk">=</strong>'columns')</span></pre><p id="02c0" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们接下来会看到这一点:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/80587244e6aac4b26abddbe8fc305c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wZZc70L_muKnpU8_HxGi9g.png"/></div></div></figure><p id="cd14" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，我们需要分离预测列，以便只保留花名。分离之后，我将数据帧推送到一个CSV。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="0ddc" class="nc kh jj nt b gy nx ny l nz oa"><em class="od">#df_test12[['Predicted','seperatestuff']] = df_test12['predicted'].astype(str).str.split('_',expand=True)</em><br/>df_test12['predicted'] <strong class="nt jk">=</strong> df_test12['predicted']<strong class="nt jk">.</strong>astype(str)<strong class="nt jk">.</strong>str<strong class="nt jk">.</strong>split('_')<strong class="nt jk">.</strong>str[0]<br/>df_test12<strong class="nt jk">.</strong>head()<br/>df_test12<strong class="nt jk">.</strong>to_csv('/content/testanswers.csv', index <strong class="nt jk">=</strong> <strong class="nt jk">False</strong>)</span></pre><p id="5df9" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">结果:</p><p id="1a8d" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">对于测试数据集中的30行，我们看到准确率为87%:</p><p id="04b3" class="pw-post-body-paragraph le lf jj lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">真26 87%假4 13%</p></div></div>    
</body>
</html>