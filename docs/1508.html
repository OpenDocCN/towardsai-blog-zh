<html>
<head>
<title>Popular Datasets for 3D Human Pose Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">三维人体姿态估计的流行数据集</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/popular-datasets-for-3d-human-pose-estimation-a309b5700f9c?source=collection_archive---------1-----------------------#2021-02-09">https://pub.towardsai.net/popular-datasets-for-3d-human-pose-estimation-a309b5700f9c?source=collection_archive---------1-----------------------#2021-02-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="d0c3" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="97c5" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">获取解释和创建您自己的数据集的知识</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b2479b2bdcf2c63bb11dcf04ca14ed34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hz1CliIZpnwTVnK6"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@camstejim?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卡米洛·希门尼斯</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><blockquote class="lf lg lh"><p id="caa5" class="li lj lk ll b lm ln ka lo lp lq kd lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">“有了3D，你就能沉浸在这个世界中”<br/>——李安(2003年《绿巨人》的导演)</p></blockquote><h1 id="3d6b" class="mf mg iq bd mh mi mj mk ml mm mn mo mp kf mq kg mr ki ms kj mt kl mu km mv mw bi translated">介绍</h1><p id="0398" class="pw-post-body-paragraph li lj iq ll b lm mx ka lo lp my kd lr mz na lu lv nb nc ly lz nd ne mc md me ij bi translated">人体姿态估计(HPE)任务旨在从给定的传感器输入中获得人体的姿态。<br/>这一研究领域可应用于许多应用，如<em class="lk">动作/活动</em> <em class="lk">识别</em>、<em class="lk">动作检测</em>、<em class="lk">人体跟踪、</em> <a class="ae le" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> <em class="lk">移动应用</em> </a> <em class="lk"> </em>和<em class="lk">虚拟现实</em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/e8d78baf822e2d6fa5f75338046f3b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XmmeAS9C03bCKOm-ZNOuJw.gif"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由<a class="ae le" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> OpenPose </a>提供</figcaption></figure><p id="007c" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mz lt lu lv nb lx ly lz nd mb mc md me ij bi translated">基于建立我们的对象的位置所必需的参数，这个研究领域可以分为<strong class="ll ja"> 2D </strong>和<strong class="ll ja"> 3D </strong>人体姿态估计。</p><ul class=""><li id="a49a" class="ng nh iq ll b lm ln lp lq mz ni nb nj nd nk me nl nm nn no bi translated"><strong class="ll ja">2D·HPE</strong>从单目图像和视频中计算出人体关节的位置。</li><li id="1b80" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated"><strong class="ll ja"> 3D HPE </strong>从图像或其他输入源计算3D空间中人体关节的位置。</li></ul><p id="83cd" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mz lt lu lv nb lx ly lz nd mb mc md me ij bi translated">得益于深度学习技术，HPE实现快速进步(<a class="ae le" href="https://www.researchgate.net/publication/287815055_SMPL_a_skinned_multi-person_linear_model" rel="noopener ugc nofollow" target="_blank"> Loper et al .，2015</a>；<a class="ae le" href="https://arxiv.org/abs/1712.06584" rel="noopener ugc nofollow" target="_blank">金泽等人，2018 </a>。<br/>数据集和所有深度学习技术一样，扮演着重要的角色。<br/> <em class="lk">在本文中，我们将一起来看看最流行的可用公共数据集的概述。</em> <br/> <em class="lk">这将让我们看到，在以后的时间里，这种评价方法能够理解SOTA最近的表现。</em></p><blockquote class="lf lg lh"><p id="1cb1" class="li lj lk ll b lm ln ka lo lp lq kd lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated"><strong class="ll ja">免责声明:</strong>在本文中，我们将仅限于分析3D案例</p></blockquote></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h1 id="a526" class="mf mg iq bd mh mi ob mk ml mm oc mo mp kf od kg mr ki oe kj mt kl of km mv mw bi translated">三维人体姿态估计数据集</h1><p id="f761" class="pw-post-body-paragraph li lj iq ll b lm mx ka lo lp my kd lr mz na lu lv nb nc ly lz nd ne mc md me ij bi translated">下面呈现的数据集是使用<strong class="ll ja">运动捕捉</strong> <strong class="ll ja"> (MoCap)系统</strong>捕捉的。但是，还有其他方法可以为HPE 3D创建数据集。举几个例子:</p><ul class=""><li id="3339" class="ng nh iq ll b lm ln lp lq mz ni nb nj nd nk me nl nm nn no bi translated"><a class="ae le" href="https://www.di.ens.fr/willow/research/surreal/data/" rel="noopener ugc nofollow" target="_blank"><strong class="ll ja"/></a>:包含单个合成人的视频，真实背景不变。它包含身体部分分割、深度、光流和表面法线的注释。该数据集采用SMPL身体模型来生成身体姿势和形状</li><li id="1eae" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated"><a class="ae le" href="https://amass.is.tue.mpg.de/" rel="noopener ugc nofollow" target="_blank"> <strong class="ll ja">聚敛数据集</strong> </a>:是人体运动的大型数据库，通过在公共框架和参数化内表示不同的基于光学标记的运动捕捉数据集来统一它们。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/58027eedb25dae003d2e8aded08e3d86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/1*BvPgTws5rlelMgXaUjmz0g.gif"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:<a class="ae le" href="https://www.youtube.com/channel/UCqNJuPO0tyV6eWfYB7lcsvw" rel="noopener ugc nofollow" target="_blank">迈克尔·布莱克</a></figcaption></figure><h2 id="f932" class="oh mg iq bd mh oi oj dn ml ok ol dp mp mz om on mr nb oo op mt nd oq or mv iw bi translated">使用MoCap系统创建的数据集</h2><p id="5d11" class="pw-post-body-paragraph li lj iq ll b lm mx ka lo lp my kd lr mz na lu lv nb nc ly lz nd ne mc md me ij bi translated">下面我将展示一些使用MoCap系统创建的最流行的数据集。这些系统由几个摄像头组成，可以记录人体的运动，以便立即或推迟分析，这要归功于复制。<br/>这些将分为:</p><p id="ba6f" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mz lt lu lv nb lx ly lz nd mb mc md me ij bi translated"><strong class="ll ja"> I)单人</strong> : HumanEva-I &amp; II，Human3.6，TNT15，MPI-INF-3DHP</p><p id="70ca" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mz lt lu lv nb lx ly lz nd mb mc md me ij bi translated"><strong class="ll ja"> II)多人</strong>:全景，3DPW</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h2 id="229b" class="oh mg iq bd mh oi oj dn ml ok ol dp mp mz om on mr nb oo op mt nd oq or mv iw bi translated">I)单人数据集</h2><h2 id="e000" class="oh mg iq bd mh oi oj dn ml ok ol dp mp mz om on mr nb oo op mt nd oq or mv iw bi translated">HumanEva-I和II数据集</h2><p id="65cc" class="pw-post-body-paragraph li lj iq ll b lm mx ka lo lp my kd lr mz na lu lv nb nc ly lz nd ne mc md me ij bi translated"><a class="ae le" href="http://humaneva.is.tue.mpg.de/" rel="noopener ugc nofollow" target="_blank"> "HumanEva-I </a> "[1]数据集包含7个校准的视频序列(4个灰度和3个彩色)，这些序列与从ViconPeak的运动捕捉系统获得的3D身体姿态同步。<br/>该数据库包含4名受试者执行的6种常见动作(如行走、慢跑、打手势等)。)在3m×2m的捕获区域中。数据集包含训练集、验证集和测试集。<br/>“HumanEva-II”是用于测试的human EVA-I数据集的扩展，包含2个执行动作组合的对象。</p><h2 id="699b" class="oh mg iq bd mh oi oj dn ml ok ol dp mp mz om on mr nb oo op mt nd oq or mv iw bi translated">Human3.6数据集</h2><p id="47ae" class="pw-post-body-paragraph li lj iq ll b lm mx ka lo lp my kd lr mz na lu lv nb nc ly lz nd ne mc md me ij bi translated"><a class="ae le" href="http://vision.imar.ro/human3.6m/description.php" rel="noopener ugc nofollow" target="_blank">该数据集</a> [2]是使用基于精确标记的运动捕捉系统(<a class="ae le" href="https://www.vicon.com/" rel="noopener ugc nofollow" target="_blank"> Vicon </a>)在室内实验室设置(4m x 3m)中收集的，记录了11名专业演员(6名男性和5名女性)进行的17项日常活动，例如:等待、坐在椅子上、打电话、坐着时的活动等。<br/>主要捕获设备包括10台运动摄像机、1台飞行时间传感器和4台数码摄像机。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi os"><img src="../Images/0f7d8a93292a19106c4f7db25b7cf717.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*FKbhjyygbn_bJCORobMRfw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">显示拍摄区域以及视频、MoCAP和t of摄像机位置的平面图。来源:图片由卡特林·约内斯库等人提供。</figcaption></figure><p id="ab7d" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mz lt lu lv nb lx ly lz nd mb mc md me ij bi translated">所提供的注释包括每个动作的3D关节位置、关节角度、人边界框和3D激光扫描。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ot"><img src="../Images/b93dfbbc05aee96d23517d17cb49db88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQXoNVFYgtMbXCtk_a7yFQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">从左到右:RGB图像、人的轮廓(边界框也是可用的)、飞行时间(深度)数据、3D姿势数据。来源:图片由卡特林·约内斯库等人提供。</figcaption></figure><h2 id="6929" class="oh mg iq bd mh oi oj dn ml ok ol dp mp mz om on mr nb oo op mt nd oq or mv iw bi translated">TNT15数据集</h2><p id="3d25" class="pw-post-body-paragraph li lj iq ll b lm mx ka lo lp my kd lr mz na lu lv nb nc ly lz nd ne mc md me ij bi translated"><a class="ae le" href="https://www.tnt.uni-hannover.de/project/TNT15/" rel="noopener ugc nofollow" target="_blank"> TNT15数据集</a> [3]由视频数据、从8个校准的RGB摄像机获得的多视图序列组成。轮廓，通过背景减法获得的二值分割图像。IMU数据，10个IMU的方位和加速度数据。投影矩阵，所有8台摄像机的摄像机参数。以及每个演员的网格、3D激光扫描和配准的网格。<br/>这个数据集记录了4个演员进行的5项活动(动态出拳、跳跃和滑雪练习、旋转手臂、原地跑步、行走)。<br/>该数据集包含13k帧(每个人的二值分割图像、3D激光扫描和注册网格)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/52e28097e6037c9c4b8fc5e81712ed43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b0R1Hy3fx-xzf9mbWd8XjQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片来自<a class="ae le" href="https://www.tnt.uni-hannover.de/project/TNT15/" rel="noopener ugc nofollow" target="_blank"> tnt </a></figcaption></figure><p id="a369" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mz lt lu lv nb lx ly lz nd mb mc md me ij bi translated"><strong class="ll ja">MPI-INF-3d HP<br/></strong><a class="ae le" href="http://gvv.mpi-inf.mpg.de/3dhp-dataset/" rel="noopener ugc nofollow" target="_blank">数据集</a>【4】是用无标记多摄像机MoCap系统(TheCaptury)在室内和室外场景中采集的。它包含来自14个不同视图的超过130万个帧。记录八名受试者(4名女性和4名男性)进行的8项活动(例如，行走/站立、锻炼、坐着、蹲下/伸展、在地板上、运动、其他)。)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/9eef8aa93d658b4df9e75c2698aa383e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppaera22RF2C7PsJjnqp5g.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">MPI-INF-3DHP数据集。来源:图片由Mehta等人提供。</figcaption></figure><h2 id="7031" class="oh mg iq bd mh oi oj dn ml ok ol dp mp mz om on mr nb oo op mt nd oq or mv iw bi translated">II)多人数据集</h2><h2 id="6c00" class="oh mg iq bd mh oi oj dn ml ok ol dp mp mz om on mr nb oo op mt nd oq or mv iw bi translated">全景数据集</h2><p id="a2af" class="pw-post-body-paragraph li lj iq ll b lm mx ka lo lp my kd lr mz na lu lv nb nc ly lz nd ne mc md me ij bi translated"><a class="ae le" href="http://domedb.perception.cs.cmu.edu/" rel="noopener ugc nofollow" target="_blank">全景</a>【5】是使用多视图系统通过无标记运动捕捉进行捕捉的，该系统包含:</p><ul class=""><li id="1ff5" class="ng nh iq ll b lm ln lp lq mz ni nb nj nd nk me nl nm nn no bi translated">480个VGA摄像机，640 x 480分辨率，25 fps，它们之间使用硬件时钟进行同步</li><li id="113c" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">31台高清摄像机，1920 x 1080分辨率，30 fps，使用硬件时钟相互同步，时序与VGA摄像机一致</li><li id="2db3" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">10个Kinect传感器。1920 x 1080 (RGB)、512 x 424(深度)、30 fps，它们之间以及与其他传感器之间的时序一致</li><li id="7ee5" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">5台DLP投影仪。与高清摄像机同步</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a854b20396b76653ab7059efb5f8aba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/1*JEIeEvZTicOsFBfUXV9rew.gif"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:<a class="ae le" href="https://www.youtube.com/watch?v=wb32z_xwk0c" rel="noopener ugc nofollow" target="_blank">韩星</a></figcaption></figure><p id="3d1a" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mz lt lu lv nb lx ly lz nd mb mc md me ij bi translated">目前panoptic有65个序列和150万个3D骨架。</p><h2 id="8ab2" class="oh mg iq bd mh oi oj dn ml ok ol dp mp mz om on mr nb oo op mt nd oq or mv iw bi translated">3DPW数据集</h2><p id="71ed" class="pw-post-body-paragraph li lj iq ll b lm mx ka lo lp my kd lr mz na lu lv nb nc ly lz nd ne mc md me ij bi translated">野外数据集[6]中的3D姿态是野外中第一个具有用于评估的精确3D姿态的数据集。它是在自然环境中用一个手持相机拍摄的。三维注释估计从IMU附在受试者的肢体与提出的方法视频惯性Poser。<br/>数据集包括:60个视频序列，2D姿态注释，用我们的方法获得的3D姿态，序列中每一帧的相机姿态，3D身体扫描&amp; 3D人物模型，以及18个不同服装变化的3D模型。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/3cec9ba4d4fa6df5177dec438bc97ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7Jqlv5AvJurabq7Go1rDw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">3DPW数据集。来源:图片由V.Marcard等人提供。</figcaption></figure><h1 id="ef02" class="mf mg iq bd mh mi mj mk ml mm mn mo mp kf mq kg mr ki ms kj mt kl mu km mv mw bi translated">结论</h1><p id="3862" class="pw-post-body-paragraph li lj iq ll b lm mx ka lo lp my kd lr mz na lu lv nb nc ly lz nd ne mc md me ij bi translated">在本文中，我们看到了最流行的三维人体姿态估计数据集。<br/>我希望这篇文章对那些计划创建自己的3D人体姿态估计数据集的人有所帮助和启发。</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="8ee8" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mz lt lu lv nb lx ly lz nd mb mc md me ij bi translated">感谢阅读我的文章。如有任何问题或信息，您可以通过<a class="ae le" href="https://www.linkedin.com/in/kouatemuhamed/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h2 id="3dc2" class="oh mg iq bd mh oi oj dn ml ok ol dp mp mz om on mr nb oo op mt nd oq or mv iw bi translated">参考资料:</h2><ul class=""><li id="caec" class="ng nh iq ll b lm mx lp my mz ox nb oy nd oz me nl nm nn no bi translated">[1] <a class="ae le" href="https://link.springer.com/article/10.1007%2Fs11263-009-0273-6" rel="noopener ugc nofollow" target="_blank"> Signal等，“Humaneva:用于评估关节式人体运动的同步视频和运动捕捉数据集和基线算法”(2010) </a></li><li id="a1b0" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">[2] <a class="ae le" href="http://vision.imar.ro/human3.6m/pami-h36m.pdf" rel="noopener ugc nofollow" target="_blank">约内斯库等，“自然环境中三维人体感知的大规模数据集和预测方法”(2014) </a></li><li id="4269" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">[3] V. Marcard等人，“<a class="ae le" href="https://www.tnt.uni-hannover.de/project/TNT15/TNT15_documentation.pdf" rel="noopener ugc nofollow" target="_blank">多模态运动捕捉数据集TNT 15</a>”(2016)</li><li id="1ef3" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">[4] <a class="ae le" href="https://arxiv.org/pdf/1611.09813.pdf" rel="noopener ugc nofollow" target="_blank"> Mehta等人，“使用改进的CNN监督在野外进行单目3D人体姿态估计”(2017) </a></li><li id="f8f1" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">[5] <a class="ae le" href="https://ieeexplore.ieee.org/abstract/document/8187699?casa_token=tbxHnb8pCs0AAAAA:pTOouYYGYdVK2oyHPxGsksjWt6hQth6UX2My7IL84HjqwNON1eJ2SWd_N8aNyzlJMZm0nbg9gA" rel="noopener ugc nofollow" target="_blank"> Joo等人，“全景工作室:用于社交互动捕捉的大规模多视角系统”(2017) </a></li><li id="b22c" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">[6] <a class="ae le" href="https://virtualhumans.mpi-inf.mpg.de/papers/vonmarcardECCV18/vonmarcardECCV18.pdf" rel="noopener ugc nofollow" target="_blank"> V. Marcard等人，“使用IMUs和移动摄像机恢复野外精确的3D人体姿势”(2018) </a></li><li id="78bc" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated"><a class="ae le" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank">打开姿势</a></li><li id="e3ef" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated"><a class="ae le" href="https://www.youtube.com/watch?v=cceRrlnTCEs&amp;feature=emb_title" rel="noopener ugc nofollow" target="_blank">积累三维人体运动数据集</a></li><li id="75bc" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated"><a class="ae le" href="https://www.youtube.com/watch?v=wb32z_xwk0c" rel="noopener ugc nofollow" target="_blank">CMU全景工作室:简介(短版)</a></li><li id="3bd3" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">【陈等.“单目人体姿态估计:基于深度学习的方法综述”(2020) </li><li id="d8b8" class="ng nh iq ll b lm np lp nq mz nr nb ns nd nt me nl nm nn no bi translated">【王等。【基于RGB-D的深度学习人体运动识别研究综述】(2018) </li></ul></div></div>    
</body>
</html>