<html>
<head>
<title>Diverse Generation from a Single Video Made Possible — No dataset or deep learning required!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从单个视频生成不同的内容成为可能—不需要数据集或深度学习！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/diverse-generation-from-a-single-video-made-possible-no-dataset-or-deep-learning-required-f4377c0c56bf?source=collection_archive---------3-----------------------#2021-09-27">https://pub.towardsai.net/diverse-generation-from-a-single-video-made-possible-no-dataset-or-deep-learning-required-f4377c0c56bf?source=collection_archive---------3-----------------------#2021-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="bdb2" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="50ca" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">这种模式可以做任何视频操作或视频生成应用程序，你记住了！</h2></div><blockquote class="kr ks kt"><p id="21fe" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/vgpnn-generate-video-variations/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/vgpnn-generate-video-variations/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><h2 id="9c5d" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">观看视频并查看更多示例！</h2><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="ms mt l"/></div></figure><div class="mn mo mp mq gt ab cb"><figure class="mu mr mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/0d950fb2ca80d6f27890a13892ed8ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*lKh_K7PUHaVNHrgBhIp8MQ.gif"/></div></figure><figure class="mu mr mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/ca868780b6064005b6a806626f10dca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*jfcThszlOQ8D1JmqxWxyZQ.gif"/></div></figure></div><div class="ab cb"><figure class="mu mr mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/2c44742d1895eb0297eb59ded05deb67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*tPTuNtWhlbzeYEKJqAwlNg.gif"/></div></figure><figure class="mu mr mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/6158c763380b2ff8d0e6c4060874df0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*svivaK0CYMf1gW9_t7JyWA.gif"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk nk di nl nm translated">原始视频的视频变体(左上角)。图片示例来自<a class="ae lr" href="https://nivha.github.io/vgpnn/" rel="noopener ugc nofollow" target="_blank"> VGPNN </a> [1]。</figcaption></figure></div><p id="6fc4" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">你有没有想过剪辑一个视频？</p><p id="0629" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">删除或添加某人，更改背景，使其持续更长时间，或者更改分辨率以适应特定的纵横比，而不压缩或拉伸它。对于那些已经开展过广告活动的人来说，你肯定希望有不同的视频来进行AB测试，看看什么效果最好。Niv Haim等人的这项新研究可以帮助你在一个高清视频中完成所有这些事情！的确，使用一个简单的视频，你可以在几秒钟或几分钟内完成我刚才提到的任何高质量视频的任务。你基本上可以把它用于任何你想到的视频操作或视频生成应用。它甚至在所有方面都优于GANs，并且不使用任何深度学习的花哨研究，也不需要庞大而不切实际的数据集！最棒的是，这项技术可以扩展到高分辨率视频。它不仅仅是为了256x256像素视频的研究目的！哦，当然，你也可以用它来处理图像！让我们看看它是如何工作的。</p><h2 id="4b07" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">它是如何工作的？</h2><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nn"><img src="../Images/00e173cbaf3a749a690e689d20159536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bI-8E2XUEexXgWSZDLNGOA.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk translated">图来自<a class="ae lr" href="https://nivha.github.io/vgpnn/" rel="noopener ugc nofollow" target="_blank">VGPNN</a>【1】。</figcaption></figure><p id="4bcd" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">该模型被称为基于视频的生成式面片最近邻，简称VGPNN。开发VGPNN的研究人员没有使用复杂的算法和模型，如GANs或transformers，而是选择了一种更简单的方法，但重新进行了研究。最近邻算法。</p><p id="45ae" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">首先，他们以金字塔的方式缩小图像，每一层的分辨率都比上一层低。然后，他们将随机噪声添加到最粗糙的级别，以生成不同的图像，类似于GANs在编码图像后在压缩空间中所做的事情。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi no"><img src="../Images/b4fe685b9935c244c69affd9eb3de39a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xjCWKfNC6Nae3HuM.png"/></div></a></figure><p id="c9c6" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">请注意，为了简单起见，这里我将说图像，但在这种情况下，由于它适用于视频，该过程是在3帧上同时进行的，增加了一个时间维度，但解释保持不变，只是在最后增加了一个额外的步骤。</p><p id="fee5" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">添加了噪声的最粗比例的图像被分成多个小的正方形小块。添加了噪声的图像中的所有补片被替换为来自没有噪声的初始缩小图像的最相似的补片。正如我们将看到的，这个最相似的小块是用最近邻算法测量的。大多数这些补丁将保持不变，但根据添加的噪声，一些补丁将发生足够的变化，使它们看起来更类似于初始图像中的另一个补丁。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><a href="https://www.louisbouchard.ai/learnai/"><div class="gh gi np"><img src="../Images/d6d4f598ae72cf7f2fb082a3e0a0d220.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*fqRa4yjYoXrzncvQ.png"/></div></a></figure><p id="0e6d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">这是你在上面看到的VPNN输出。这些变化足以生成新版本的图像。然后，该第一输出被放大并用于与下一尺度的输入图像进行比较，以充当其噪声版本，并且在该下一次迭代中重复相同的步骤。我们将这些图像分割成小块，并用当前步骤中最相似的图像替换先前生成的图像。</p><p id="a9fb" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">让我们进入刚刚讨论过的VPNN模块:</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nq"><img src="../Images/6c3b20e9345650608dd0062b87d28a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpjWLBRzTWImdgT9-3WpQA.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk translated">图来自<a class="ae lr" href="https://nivha.github.io/vgpnn/" rel="noopener ugc nofollow" target="_blank">VGPNN</a>【1】。</figcaption></figure><p id="947a" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">正如您在这里看到的，与添加噪声的初始步骤的唯一区别是，我们将放大生成的图像(这里表示为Q)与之前图像的放大版本进行比较，以便它具有相同的细节和锐度级别(表示为K)。基本上，使用下面的级别作为比较，我们比较Q和K，然后从当前级别V中选择图像中的相应补片，以生成该步骤的新图像，该图像将用于下一次迭代。正如你在这里看到的小箭头，K只是我们在这个算法的初始步骤中缩小V所创建的图像的放大版本，在这个步骤中，我们创建了图像的金字塔缩放版本。这样做是为了比较两幅图像中的相同清晰度水平，因为来自前一层(Q)的放大生成的图像将比当前步骤(V)的图像更加模糊，并且很难找到相似的补片。如此反复，直到我们回到金字塔的顶端，得到高分辨率的结果。然后，将所有这些生成的补丁折叠成一个视频。</p><h2 id="e501" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">摘要</h2><p id="fe5d" class="pw-post-body-paragraph ku kv it kx b ky nr kd la lb ns kg ld mb nt lg lh mf nu lk ll mj nv lo lp lq im bi translated">让我们做一个快速的总结:<br/>图像在多个尺度下缩小。噪声被添加到最粗糙尺度的图像中，该图像被分成小的正方形小块。然后，每个有噪声的补片被来自相同的无噪声压缩图像的最相似的补片所替换，从而在保持真实性的同时在图像中引起很少的随机变化。新生成的图像和没有该步骤的噪声的图像都被放大并进行比较，以再次找到与最近邻最相似的片。然后，从当前分辨率的图像中选择这些最相似的小块，以生成该步骤的新图像。我们重复这些升级和比较步骤，直到我们回到金字塔的顶端，得到高分辨率的结果。</p><p id="7eff" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">瞧！你可以用不同的噪声或修改重复这个过程，在你的视频上产生任何你想要的变化！</p><h2 id="9e5b" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">限制</h2><div class="mn mo mp mq gt ab cb"><figure class="mu mr mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/53304524cefe0c26ad57fdfba7573577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*Zf1r9AHeiqUOQCgmsszWDQ.gif"/></div></figure><figure class="mu mr mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/0b121775ee796f2e4ebe58fbd60a9f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*vKLUNPtjzMEalGV82zS-gQ.gif"/></div></figure></div><div class="ab cb"><figure class="mu mr mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/5ea0e68736e5aac81b3a0a6e8fb1dcf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*quVH51VArdeVWhDeh88wSQ.gif"/></div></figure><figure class="mu mr mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><img src="../Images/70447713e4b316b3f8c1b18c1c1c3c29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*RiMZUfbCVT_rmW3cQ9DJMQ.gif"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk nk di nl nm translated">最初视频(左上)中各代的限制。图片来自<a class="ae lr" href="https://nivha.github.io/vgpnn/" rel="noopener ugc nofollow" target="_blank"> VGPNN </a> [1]。</figcaption></figure></div><p id="a7fa" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">当然，结果并不完美。你仍然可以看到一些人工制品，比如人在奇怪的地方出现和消失，或者在某些情况下简单地复制粘贴某人，如果你专注于它，它会变得非常明显。尽管如此，这只是第一篇用最近邻算法攻击视频操作并使其可扩展到高分辨率视频的论文。看到不同的方法总是很棒。我非常兴奋地看到下一篇论文在这篇基础上有所改进！</p><p id="c18a" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">此外，结果仍然令人印象深刻，由于运行时间非常低，它们可以用作处理视频的模型的数据增强工具。允许其他模型在更大和更多样化的数据集上进行训练，而无需太多成本。</p><p id="53d8" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">如果你有兴趣学习更多关于这项技术的知识，我强烈推荐你阅读他们的论文[1]。</p><p id="3922" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">感谢您的阅读！</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><p id="6059" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)，并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">简讯</strong> </a>！</p><h2 id="420f" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">支持我:</h2><ul class=""><li id="8d50" class="od oe it kx b ky nr lb ns mb of mf og mj oh lq oi oj ok ol bi translated">支持我的最好方式是成为这个网站<strong class="kx jd"> </strong>的会员，或者如果你喜欢视频格式，在<a class="ae lr" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"><strong class="kx jd">YouTube</strong></a><strong class="kx jd"/>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="1451" class="od oe it kx b ky om lb on mb oo mf op mj oq lq oi oj ok ol bi translated">在经济上支持我在T21的工作</li><li id="d8c6" class="od oe it kx b ky om lb on mb oo mf op mj oq lq oi oj ok ol bi translated">跟我来这里上<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a></li><li id="b551" class="od oe it kx b ky om lb on mb oo mf op mj oq lq oi oj ok ol bi translated">想进入AI或者提升技能，<a class="ae lr" href="https://www.louisbouchard.ai/learnai/" rel="noopener ugc nofollow" target="_blank">看这个</a>！</li></ul><h2 id="58b1" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">参考</h2><p id="a150" class="pw-post-body-paragraph ku kv it kx b ky nr kd la lb ns kg ld mb nt lg lh mf nu lk ll mj nv lo lp lq im bi translated">[1]论文包括:哈伊姆，n .，费恩斯坦，b .，格拉诺特，n .，肖彻，a .，巴贡，s .，戴克尔，t .，伊拉尼，M. (2021)。从一个视频中产生不同的一代成为可能。ArXiv，abs/2109.08591。</p><p id="6db8" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">[2]从图像改编成视频的技术:尼夫·格拉诺特、本·范斯坦、阿萨夫·肖彻、沙伊·巴贡和迈克尔·伊拉尼。丢弃gan:作为单一图像生成模型的补丁最近邻的防御。arXiv预印本arXiv:2103.15545，2021。</p><p id="33c6" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">[3]代码(即将推出):<a class="ae lr" href="https://nivha.github.io/vgpnn/" rel="noopener ugc nofollow" target="_blank">https://nivha.github.io/vgpnn/</a></p></div></div>    
</body>
</html>