<html>
<head>
<title>GANsformers: Generate complex scenes using GANs and Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GANsformers:使用GANs和变形金刚生成复杂的场景</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/generative-adversarial-transformers-gansformers-explained-bf1fa76ef58d?source=collection_archive---------2-----------------------#2021-03-06">https://pub.towardsai.net/generative-adversarial-transformers-gansformers-explained-bf1fa76ef58d?source=collection_archive---------2-----------------------#2021-03-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="c31b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="2ea5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">他们基本上利用了强大的StyleGAN2架构中变形金刚的注意力机制，使其更加强大！</h2></div><h2 id="d1c9" class="kr ks it bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll iz bi translated">观看视频:</h2><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/3eeb34dd18ff898565330fba570c8fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJ3fKrHWHG-08av-nKXvbA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">使用注意力地图生成卧室场景的结果示例。图片来自:德鲁·a·哈德森(Drew A. Hudson)和c·劳伦斯·兹尼克(C. Lawrence Zitnick)，生成式对抗性变形金刚(2021)。</figcaption></figure><p id="19b8" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">上周我们看了DALL-E，OpenAI的最新论文。<br/>它使用与GPT 3相似的架构，包括从文本生成图像的变形金刚。这是一个超级有趣和复杂的任务，称为文本到图像的翻译。正如你在下面的视频中所看到的，与之前最先进的技术相比，结果令人惊讶地好。这主要是由于使用了变压器和大量的数据。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="f4b4" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">本周我们将看一个非常相似的任务，叫做视觉生成模型。其中目标是生成高分辨率的完整场景，如道路或房间，而不是单张人脸或特定对象。这与DALL-E不同，因为我们不是从文本中生成这个场景，而是从一个特定风格的场景的训练模型中生成的。这里是卧室。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mx"><img src="../Images/f5c74c761f56eef974d5a97a70289e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxSfmc22_f_TQI80EZwPJg.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">生成卧室场景的结果示例。图片来自:德鲁·a·哈德森(Drew A. Hudson)和c·劳伦斯·兹尼克(C. Lawrence Zitnick)，生成式对抗性变形金刚(2021)。</figcaption></figure><p id="6683" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">相反，它就像StyleGAN一样，能够在真实人脸数据集上训练生成独特而不存在的人脸。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi my"><img src="../Images/e95162e3f3899cc5bf5f8e572297b09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M54Ehz6C-WjMDe0L3ddX_A.png"/></div></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mz"><img src="../Images/2f33ffa5932a1ced83e3015701550045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odfeir2vWsLVpds3lF8Axw.png"/></div></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi na"><img src="../Images/fc9844409d7ad16f26a27620f4d0c5b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MIJ9FLwDiYHhd_LeWrLAXQ.png"/></div></div></figure><p id="4bb5" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">不同之处在于，它以传统的生成和区分方式将这种GAN架构用于卷积神经网络。经典的GAN架构会有一个经过训练的生成器来生成图像，还有一个鉴别器，用于通过猜测图像是来自数据集的真实图像还是由第一个网络生成的伪图像来测量所生成图像的质量。这两种网络通常都由卷积神经网络组成。其中生成器看起来像这样，主要包括使用卷积对图像进行下采样以对其进行编码，然后再次使用卷积对图像进行上采样，以基于编码生成具有相同风格的图像的新“版本”，这就是为什么它被称为StyleGAN。然后，鉴别器获取这个生成的图像，或者来自你的数据集中的图像，并试图判断它是真实的还是生成的，称为假的。</p><p id="235a" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">相反，他们利用强大的StyleGAN2架构内的变形金刚注意力机制，使其更加强大。注意力是这个网络的一个基本特征，允许网络在输入和输出之间画出全局依赖关系。在这种情况下，在架构的当前步骤的输入和先前编码的潜在代码之间，正如我们将在一分钟内看到的。</p><p id="91cb" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">在一头扎进去之前，如果你对变形金刚不熟悉或者不关注，建议你看看我做的关于变形金刚的视频。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="1674" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">要了解更多细节，更好地理解注意力，你应该看看视频“注意力是你所需要的”,它来自于我的一个同事，Yannic Kilcher，他报道了这篇惊人的论文。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="b623" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">好的，我们知道他们一起使用变形金刚和甘斯来产生更好更真实的场景，这解释了这篇论文的名字，甘斯弗勒，但是他们为什么和如何做呢？</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nb"><img src="../Images/11f9de5464be7453378f308682818031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lEt3IaajVTSo8X0-iwHtbw.png"/></div></div></figure><p id="72bc" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">至于为什么，他们这样做是为了自动生成像这样复杂而真实的场景。对于电影或视频游戏等许多行业来说，这可能是一个强大的应用程序，比让艺术家在计算机上创建它们，甚至在现实生活中为它们拍照需要更少的时间和精力。此外，想象一下，当结合文本到图像的翻译，从一个单一的文本输入生成许多不同的场景，并按下一个随机按钮时，它会对设计师多么有用！</p><p id="1780" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">他们使用了最先进的StyleGAN架构，因为当我们谈论一般图像时，GAN是强大的生成器。由于GANs使用卷积神经网络工作，它们天生使用像素的本地信息，将它们合并以获得关于图像的一般信息，出于同样的原因，错过了远处像素的长程相互作用。这使得GANs成为图像整体风格的强大生成器。出于同样的原因，它们在生成的图像中的小细节的质量方面仍然没有那么强大。不能控制生成的图像本身内的局部区域的样式。</p><p id="a0e5" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">这就是为什么他们有想法将变压器和GANs结合在一个架构中，他们称之为“二分变压器”。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nc"><img src="../Images/d0a70d59acbddce49e88c08802c22ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JP7IzuuVJCqhKZVYPoVZFQ.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">图片来自:德鲁·a·哈德森(Drew A. Hudson)和c·劳伦斯·兹尼克(C. Lawrence Zitnick)，生成式对抗性变形金刚(2021)。</figcaption></figure><p id="1fc5" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">正如GPT-3和许多其他论文已经证明的那样，变形金刚在长距离互动方面非常强大，可以绘制它们之间的依赖关系，并理解文本或图像的上下文。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/aeff91ac7b51740b2d9bc5e424583199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*Jr5Ahd-nuXjQh3xZ8i0WeA.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated"><code class="fe ne nf ng nh b">StyleGAN2 architecture. Image from: Tero Karras et al., Analyzing and Improving the Image Quality of StyleGAN, (2020).</code></figcaption></figure><p id="3e27" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">我们可以说，他们只是在生成器和鉴别器的卷积层之间添加了关注层，这是变压器网络的基础。因此，它们不是像卷积天生所做的那样集中于使用全局信息和全局控制所有特征，而是利用这种注意力将信息从局部像素传播到全局高级表示，反之亦然。与应用于图像的其他变形器一样，该注意力层采用像素位置和StyleGAN2潜在空间W和Z。潜在空间W是在网络开始处完成的对中间潜在空间的输入的编码，这里表示为a。而编码Z只是网络中当前步骤的输入的结果特征。</p><p id="b93b" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">这使得生成整个图像更加“有表现力”，尤其是在生成描绘多对象场景的图像时。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ni"><img src="../Images/e25125705cc273c711e6eb57b167cc83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kyanvplBfezCvqBjJJy02w.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">生成道路场景的结果示例。图片来自:德鲁·a·哈德森(Drew A. Hudson)和c·劳伦斯·兹尼克(C. Lawrence Zitnick)，生成式对抗性变形金刚(2021)。</figcaption></figure><p id="1609" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">当然，这只是脸书人工智能研究所和斯坦福大学的这篇新论文的概述。我强烈建议阅读这篇文章，以便更好地理解这种方法。这是下面引用中的第一个链接。代码也是可用的，并且将在参考文献中链接。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="8236" class="pw-post-body-paragraph me mf it mg b mh mi kd mj mk ml kg mm la mn mo mp le mq mr ms li mt mu mv mw im bi translated">如果你喜欢我的工作，并想了解最新的人工智能技术，你绝对应该在我的社交媒体频道上关注我。</p><ul class=""><li id="d476" class="nq nr it mg b mh mi mk ml la ns le nt li nu mw nv nw nx ny bi translated">订阅我的<a class="ae nz" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"> <strong class="mg jd"> YouTube频道</strong> </a>。</li><li id="56d4" class="nq nr it mg b mh oa mk ob la oc le od li oe mw nv nw nx ny bi translated">关注我的项目上<a class="ae nz" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="mg jd"> LinkedIn </strong> </a> <strong class="mg jd"> </strong>和这里上<strong class="mg jd"> </strong> <a class="ae nz" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="mg jd">中</strong> </a> <strong class="mg jd">。</strong></li><li id="4e2a" class="nq nr it mg b mh oa mk ob la oc le od li oe mw nv nw nx ny bi translated">一起学习AI，加入我们的<a class="ae nz" href="https://discord.gg/learnaitogether" rel="noopener ugc nofollow" target="_blank"> <strong class="mg jd">不和谐社区</strong> </a>，<em class="of">分享你的项目、论文、最佳课程，寻找Kaggle队友，等等！</em></li></ul></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><h1 id="4646" class="og ks it bd kt oh oi oj kw ok ol om kz ki on kj ld kl oo km lh ko op kp ll oq bi translated">参考</h1><p id="d879" class="pw-post-body-paragraph me mf it mg b mh or kd mj mk os kg mm la ot mo mp le ou mr ms li ov mu mv mw im bi translated"><strong class="mg jd">论文</strong>:<a class="ae nz" href="https://arxiv.org/pdf/2103.01209.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.01209.pdf</a><br/>T5】代码:<a class="ae nz" href="https://github.com/dorarad/gansformer" rel="noopener ugc nofollow" target="_blank">https://github.com/dorarad/gansformer</a><br/><strong class="mg jd">完整参考</strong>:德鲁·a·哈德森和c·劳伦斯·兹尼克(Drew a . Hudson and c . Lawrence Zitnick)，Generative Adversarial Transformers，(2021)，发表在Arxiv上，摘要:“我们介绍了一种新颖高效的变形金刚(GANsformer)，并对其进行探索，以完成可视化生成建模的任务。该网络采用二分结构，能够在图像上进行远程交互，同时保持线性效率的计算，可以很容易地扩展到高分辨率合成。它反复地将信息从一组潜在变量传播到进化的视觉特征，反之亦然，以支持根据另一个对每一个的改进，并鼓励对象和场景的合成表示的出现。与经典变压器架构不同，它利用乘法积分实现灵活的基于区域的调制，因此可以视为成功的StyleGAN网络的推广。我们通过对一系列数据集(从模拟的多对象环境到丰富的真实世界室内和室外场景)的仔细评估，展示了该模型的强度和鲁棒性，表明它在图像质量和多样性方面实现了最先进的结果，同时享受了快速学习和更好的数据效率。进一步的定性和定量实验为我们提供了对模型内部工作的深入了解，揭示了改进的可解释性和更强的解缠性，并说明了我们方法的好处和功效。该模型的实施可在<a class="ae nz" href="https://github.com/dorarad/gansformer." rel="noopener ugc nofollow" target="_blank">https://github.com/dorarad/gansformer.</a>获得。</p></div></div>    
</body>
</html>