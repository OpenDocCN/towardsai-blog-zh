<html>
<head>
<title>Improve Your ML Models Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">提高您的ML模型培训</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/improve-your-ml-models-training-fc0b7a49da4?source=collection_archive---------1-----------------------#2020-09-15">https://pub.towardsai.net/improve-your-ml-models-training-fc0b7a49da4?source=collection_archive---------1-----------------------#2020-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7de2" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="2c51" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Tensorflow 2.0中的循环学习率</h2></div><p id="53f6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">深度学习在当今时代已经进入了各种研究领域，也成为了我们生活中不可或缺的一部分。吴恩达的<a class="ae ln" href="https://www.wipo.int/wipo_magazine/en/2019/03/article_0001.html" rel="noopener ugc nofollow" target="_blank">话帮助我们很好地总结了这一点，</a></p><blockquote class="lo"><p id="4463" class="lp lq it bd lr ls lt lu lv lw lx lm dk translated"><em class="ly">“人工智能是新的电力。”</em></p></blockquote><p id="c7e2" class="pw-post-body-paragraph kr ks it kt b ku lz kd kw kx ma kg kz la mb lc ld le mc lg lh li md lk ll lm im bi translated">然而，任何重大的技术突破都会带来大量的挑战。从Alexa到Google Photos，再到你的网飞推荐，一切的核心都只是深度学习，但它本身也有一些障碍:</p><ul class=""><li id="cd85" class="me mf it kt b ku kv kx ky la mg le mh li mi lm mj mk ml mm bi translated">海量数据的可用性</li><li id="ca2d" class="me mf it kt b ku mn kx mo la mp le mq li mr lm mj mk ml mm bi translated">适合高性能的硬件的可用性</li><li id="e5dc" class="me mf it kt b ku mn kx mo la mp le mq li mr lm mj mk ml mm bi translated">对可用数据的过度拟合</li><li id="7dc0" class="me mf it kt b ku mn kx mo la mp le mq li mr lm mj mk ml mm bi translated">缺乏透明度</li><li id="1402" class="me mf it kt b ku mn kx mo la mp le mq li mr lm mj mk ml mm bi translated">超参数优化</li></ul><p id="b533" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">本文将帮助您解决其中一个障碍，即<em class="ms">优化</em>。</p><h1 id="5c32" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">典型方法的问题是:</h1><p id="9230" class="pw-post-body-paragraph kr ks it kt b ku nl kd kw kx nm kg kz la nn lc ld le no lg lh li np lk ll lm im bi translated">深度神经网络通常通过使用随机梯度下降进行学习，参数θ(或权重ω)更新如下:</p><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/a16dd0fbb0b149503ed6c4bd9b2cbbe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/0*bCtmr4qFHWUA0BFI.png"/></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk translated">随机梯度下降</figcaption></figure><p id="e5a9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中L是损失函数，<em class="ms"> α </em>是学习率。</p><p id="3701" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们知道，如果我们把学习率设置得太小，算法完全收敛的时间就会太长，如果太大，算法就会发散而不是收敛。因此，重要的是<strong class="kt jd">用各种学习率</strong>和时间表进行实验，看看什么最适合我们的模型。</p><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi oc"><img src="../Images/4ffa7f99e51d38d42442b091b9fdeb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IdP-Mdzv750A-zVe.png"/></div></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk translated"><a class="ae ln" href="https://www.jeremyjordan.me/nn-learning-rate/" rel="noopener ugc nofollow" target="_blank">神经网络的学习速率行为</a></figcaption></figure><p id="baed" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">实际上，由于这种方法，还会出现一些问题:</p><ul class=""><li id="7aa1" class="me mf it kt b ku kv kx ky la mg le mh li mi lm mj mk ml mm bi translated">深度学习模型和优化器对我们的初始学习速率很敏感。初始学习速率的错误选择会从一开始就极大地妨碍我们模型的性能<strong class="kt jd">。</strong></li><li id="f53a" class="me mf it kt b ku mn kx mo la mp le mq li mr lm mj mk ml mm bi translated">这可能导致模型<strong class="kt jd">陷入局部最小值或鞍点</strong>。当这种情况发生时，即使我们继续降低学习速度，我们也可能无法下降到一个损失更低的地方。</li></ul><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/02211596dff68f656dfad235725a3dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/0*mFTfEpC3N1cWBHrk.png"/></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk translated"><a class="ae ln" href="https://www.pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/" rel="noopener ugc nofollow" target="_blank">学习率和搜索空间</a></figcaption></figure></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="75fa" class="mt mu it bd mv mw op my mz na oq nc nd ki or kj nf kl os km nh ko ot kp nj nk bi translated">循环学习率帮助我们克服这些问题</h1><p id="a056" class="pw-post-body-paragraph kr ks it kt b ku nl kd kw kx nm kg kz la nn lc ld le no lg lh li np lk ll lm im bi translated">使用循环学习率，您可以大大减少调整和找到模型的最佳学习率所需的实验次数。</p><p id="0639" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们不再单调递减学习速率，而是:</p><ol class=""><li id="039d" class="me mf it kt b ku kv kx ky la mg le mh li mi lm ou mk ml mm bi translated">在我们的学习率上定义一个<strong class="kt jd">下限</strong>(<em class="ms">base _ lr</em>)。</li><li id="0d54" class="me mf it kt b ku mn kx mo la mp le mq li mr lm ou mk ml mm bi translated">定义学习率的<strong class="kt jd">上限</strong>(<em class="ms">max _ lr</em>)。</li></ol><p id="92f9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此<strong class="kt jd">学习率在训练时在这两个界限之间振荡</strong>。每次批量更新后，它会慢慢增加和减少。</p><p id="3850" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用这种CLR方法，我们不再需要手动调整学习率，我们仍然可以达到接近最佳的分类精度。此外，与自适应学习率不同，CLR方法不需要额外的计算。</p><p id="440a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过看一个例子，你会清楚这种改进。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="94a1" class="mt mu it bd mv mw op my mz na oq nc nd ki or kj nf kl os km nh ko ot kp nj nk bi translated">在数据集上实现CLR</h1><p id="b2a3" class="pw-post-body-paragraph kr ks it kt b ku nl kd kw kx nm kg kz la nn lc ld le no lg lh li np lk ll lm im bi translated">现在，我们将训练一个简单的神经网络模型，并比较不同的优化技术。我在这里使用了一个关于心血管疾病的数据集。</p><p id="35b6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些是您在实施过程中需要的所有导入:</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="0014" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是数据的样子:</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi ox"><img src="../Images/b9538e4665a256bd166dc215c29abaa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*00Op3QE4CqqI_tOv.png"/></div></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk translated"><a class="ae ln" href="https://www.kaggle.com/sulianova/cardiovascular-disease-dataset" rel="noopener ugc nofollow" target="_blank">心血管数据集</a> —预览</figcaption></figure><p id="6efc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">列<code class="fe oy oz pa pb b">cardio</code>是目标变量，我们对数据进行一些简单的缩放，并将其分成特征(<code class="fe oy oz pa pb b">X_data</code>)和目标(<code class="fe oy oz pa pb b">y_data</code>)。</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="6ca7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们使用<code class="fe oy oz pa pb b">train_test_split</code>来获得80–20的标准训练测试比。然后我们使用Keras的<code class="fe oy oz pa pb b">Sequential</code>模型定义一个非常基本的神经网络。我在我的模型中使用了3个密集层，但是你可以选择任意数量的层或者激活函数。</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><h1 id="75e9" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">不带CLR的培训:</h1><p id="ac53" class="pw-post-body-paragraph kr ks it kt b ku nl kd kw kx nm kg kz la nn lc ld le no lg lh li np lk ll lm im bi translated">在这里，我使用基本的<a class="ae ln" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD" rel="noopener ugc nofollow" target="_blank"><em class="ms">【SGD】</em></a>优化器编译了这个模型，它的默认学习率是0.01。然后，该模型被训练超过50个时期。</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi pc"><img src="../Images/a0a8ac03e80fcae31339bb35c9d8e768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*voOSJe2D1L_vmFU_.png"/></div></div></figure><p id="d248" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了向您展示最近的几个时期，该模型每个时期花费3s，最终给出64.1%的训练准确度和64.7%的验证准确度。简而言之，这是我们的模型在大约150秒的训练后给出的结果:</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi pd"><img src="../Images/2e966287d1772fb1fd1c1e206b273c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NO9xB1toGWQWz9_R.png"/></div></div></figure><h1 id="3509" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">使用CLR进行培训:</h1><p id="f66e" class="pw-post-body-paragraph kr ks it kt b ku nl kd kw kx nm kg kz la nn lc ld le no lg lh li np lk ll lm im bi translated">现在我们使用循环学习率，看看我们的模型表现如何。TensorFlow已经内置了这个优化器，随时可供我们使用。我们从TensorFlow <a class="ae ln" href="https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/CyclicalLearningRate" rel="noopener ugc nofollow" target="_blank"> <em class="ms">插件</em> </a> <em class="ms"> </em>中调用它，定义如下:</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="85ac" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><code class="fe oy oz pa pb b">step_size</code>的值可以很容易地从一个时期内的迭代次数中计算出来。这里，每个时期的迭代次数</p><p id="221d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">=(训练样本数)/(batch_size)</p><p id="75df" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi">= 70000/350</p><p id="c17f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi">= 200.</p><blockquote class="pe pf pg"><p id="f669" class="kr ks ms kt b ku kv kd kw kx ky kg kz ph lb lc ld pi lf lg lh pj lj lk ll lm im bi translated"><em class="it">“实验表明，将步长设置为一个历元中迭代次数的2-10倍通常是好的。”</em></p></blockquote><p id="080f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在使用这个新定义的优化器编译我们的模型，</p><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi pk"><img src="../Images/992c9694be3e6c2f18b024d81dc00025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y7ggCYkb9XOoBm08.png"/></div></div></figure><figure class="nr ns nt nu gt nv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi pl"><img src="../Images/4a874bfcd4a7a7dd55a78dc69fae8388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-qVJgN3E4fowtZaV.png"/></div></div></figure><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/fb5334a5127d5bca94261b7efc427748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/0*dM7NP8CNPOwz99mU.png"/></div></figure><p id="4685" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们看到现在我们的模型火车<strong class="kt jd">比</strong>快得多，总共用时甚至不到50秒。</p><p id="0b03" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">损失值<strong class="kt jd">收敛更快</strong>并且如我们所料在CLR模型中轻微振荡。</p><figure class="nr ns nt nu gt nv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/a714c935743256fb228fdcebfc73553b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/0*pDCY621Qnd7JbUp1.png"/></div></figure><p id="d744" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">训练准确率从<strong class="kt jd"> 64.1% </strong>提高到<strong class="kt jd"> 64.3%。</strong></p><p id="2c7b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">测试精度也提高了，从<strong class="kt jd">的64.7% </strong>提高到<strong class="kt jd">的65%。</strong></p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="b669" class="mt mu it bd mv mw op my mz na oq nc nd ki or kj nf kl os km nh ko ot kp nj nk bi translated">结论</h1><p id="8622" class="pw-post-body-paragraph kr ks it kt b ku nl kd kw kx nm kg kz la nn lc ld le no lg lh li np lk ll lm im bi translated">当您开始处理任何新数据集时，您在以前的数据集中使用的相同学习率值将不适用于您的新数据。因此，您必须执行一个<strong class="kt jd"> LR范围测试</strong>，该测试将为您提供一个适合您的数据的学习率的良好范围。然后，您可以将您的CLR与固定学习率优化器进行比较，就像我们在上面看到的那样，看看什么最适合您的性能目标。因此，为了获得学习率的最佳范围，只要学习率保持线性增长，就可以在较少的时期运行模型。那么在这些界限之间摆动学习速率就足以在几次迭代中给你一个接近最优的结果。</p><p id="65f9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种优化技术显然是一个福音，因为我们不再需要自己调整学习速度。我们在更少的迭代中实现了更好的准确性。</p><h1 id="2067" class="mt mu it bd mv mw mx my mz na nb nc nd ki ne kj nf kl ng km nh ko ni kp nj nk bi translated">参考资料:</h1><p id="34db" class="pw-post-body-paragraph kr ks it kt b ku nl kd kw kx nm kg kz la nn lc ld le no lg lh li np lk ll lm im bi translated">[1] <a class="ae ln" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank"> Smith，Leslie N .“训练神经网络的循环学习率”2017年</a></p></div></div>    
</body>
</html>