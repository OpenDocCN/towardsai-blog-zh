<html>
<head>
<title>Microsoft’s New Ideas About Generative Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微软关于生成模型的新想法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/microsofts-new-ideas-about-generative-models-b883070bef67?source=collection_archive---------2-----------------------#2021-09-20">https://pub.towardsai.net/microsofts-new-ideas-about-generative-models-b883070bef67?source=collection_archive---------2-----------------------#2021-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="c09e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="6e9c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">擎天柱、FQ-甘和流行为大规模应用生成模型带来了新的思路。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b3686eb8fa942f48f40deabb2c875db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vP0xkOI6JT0kVTFq0VIzPQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://learnopencv.com/generative-and-discriminative-models/" rel="noopener ugc nofollow" target="_blank">https://learnopencv . com/generative-and-discriminal-models/</a></figcaption></figure><blockquote class="li lj lk"><p id="1598" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过10万名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到102，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="a5a9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在过去的几十年里，生成模型一直是机器学习的重要组成部分。随着深度学习的出现，生成模型开始与深度神经网络相结合，创建了深度生成模型(DGMs)领域。dgm为深度学习领域带来了很多希望，因为它们有能力从观察中综合数据。该特征可以产生改善大规模模型的训练而不需要大量数据的关键。最近几个月，微软研究院公布了几个新项目，希望推进DGMs的研究。</p><p id="7b6f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">围绕dgm的最大问题之一是它们是否可以应用于大规模数据集。近年来，我们看到了许多DGMs在相对较小的规模上应用的例子。然而，当谈到数据时，深度学习领域正朝着“越大越好”的理念发展，我们经常看到在难以理解的大数据集中训练新模型。可以在那个规模上运行的dgm的想法是该领域中最活跃的研究领域之一，也是微软研究项目的焦点。</p><h1 id="bae6" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">dgm的类型</h1><p id="4be1" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">理解dgm的一个好方法是将它们与它们最著名的补充:判别模型进行对比。通常被描述为兄弟姐妹:生成模型和判别模型包含了我们了解世界的不同方式。从概念上讲，生成模型可以试图概括他们所看到的一切，无论辨别模型是否了解他们所看到的独特属性。判别模型和生成模型各有优缺点。判别算法往往在涉及高质量数据集的分类任务中表现得非常好。然而，生成模型具有独特的优势，可以创建与现有数据相似的新数据集，并在缺乏大量标记数据集的环境中非常有效地操作。</p><p id="b5d6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">OpenAI 在2016年的一篇博客文章中出色地捕捉到了生成模型的本质，他们在文章中表示:</p><blockquote class="li lj lk"><p id="2498" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><em class="it">“为了生成数据，生成模型被迫发现并有效地内在化数据的本质。”</em></p></blockquote><p id="a933" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在同一篇博文中，OpenAI概述了对dgm进行分类的分类法，包括三个主要类别:</p><p id="8970" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><em class="ln"> I. </em> <strong class="lo jd"> <em class="ln">变型自动编码器:</em> </strong> <em class="ln">一种编码器-解码器框架，允许在概率图形模型的框架中形式化这个问题，其中我们最大化数据的对数似然的下限。</em></p><p id="cc2f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><em class="ln">二。</em> <strong class="lo jd"> <em class="ln">自回归模型:</em> </strong> <em class="ln">此类模型将训练数据的分布分解为条件分布，从而有效地从以前的维度对数据集的每个维度进行建模。</em></p><p id="860b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><em class="ln">三。</em> <strong class="lo jd"> <em class="ln">生成对抗网络:</em> </strong> <em class="ln">一个生成器-鉴别器框架，使用对抗博弈来生成数据分布。</em></p><p id="0753" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">近年来，我们已经看到了将DGMs应用于大规模模型的重大进展，如<a class="ae lh" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> OpenAI GPT-2 </a>或<a class="ae lh" href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" rel="noopener ugc nofollow" target="_blank">微软的图灵-NLG </a>。这些模型遵循相似的学习原则:自我监督的预训练，带有特定任务的微调。最大的问题仍然是DGMs是否可以系统化用于大规模的学习任务。在这方面，微软研究院最近公布了三大研究成果。</p><h1 id="5c07" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">一款过程集成与优化设计平台</h1><p id="3f75" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">在论文<a class="ae lh" href="https://www.microsoft.com/en-us/research/publication/optimus-organizing-sentences-via-pre-trained-modeling-of-a-latent-space/" rel="noopener ugc nofollow" target="_blank">Optimus:Organizing sentences with pre-trained modeling of a universal latent space</a>中，微软研究院介绍了一种用于自然语言任务的大规模VAE模型。Optimus提供了一个创新的DGM，它既是一个强大的生成模型，也是一个有效的自然语言表示学习框架。</p><p id="4f54" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">传统上，大规模预训练的自然语言模型已经被专门化为单一角色。GPT-2或威震天等型号已被证明是强大的解码器，而伯特等型号则擅长于大规模编码器。Optimus在一个新颖的架构中结合了这两种方法，如下所示:</p><p id="1195" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">Optimus架构包括一个基于BERT的编码器和一个基于GPT-2的解码器。为了连接伯特和GPT-2，擎天柱使用了两种不同的方法。在第一种方法中，潜在变量(z)被表示为解码器要关注的附加存储向量。或者，第二种方法将潜在变量(z)添加到解码器的底部嵌入层上，并直接用于每个解码步骤。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/599c05dfeb89ac6b4bc07249ecb08608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JfQv4Gne6xq5W9acTN-bfg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://www.microsoft.com/en-us/research/publication/optimus-organizing-sentences-via-pre-trained-modeling-of-a-latent-space/" rel="noopener ugc nofollow" target="_blank">https://www . Microsoft . com/en-us/research/publication/Optimus-organizing-sentences-via-pre-trained-modeling-of-a-latent-space/</a></figcaption></figure><p id="3bc4" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">Optimus中的初始测试显示了优于现有预训练语言模型的关键优势:</p><p id="2877" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">1) <strong class="lo jd">语言建模:</strong>与现有的所有小型VAEs相比，Optimus表现出了更好的表征学习性能，通过互信息和活跃单元来衡量。</p><p id="4b1e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">2) <strong class="lo jd">引导语言生成:</strong> Optimus展示了在语义层面引导语言生成的独特能力。</p><p id="2b93" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">3) <strong class="lo jd">低资源语言理解:</strong>通过学习独特的特征模式，Optimus表现出比替代模型更好的分类性能和更快的适应能力。</p><h1 id="79f7" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">FQ甘</h1><p id="b511" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">在论文<a class="ae lh" href="https://www.microsoft.com/en-us/research/publication/feature-quantization-improves-gan-training/" rel="noopener ugc nofollow" target="_blank">特征量化改进GAN训练</a>中，微软研究院提出了一种新的DGM图像生成方法。FQ-甘的创新依赖于在离散的空间而不是连续的空间中表现图像。</p><p id="33a1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">利用大数据集进行训练一直是生成对抗网络的主要挑战之一。这一挑战的部分原因是GANs依赖于非静态学习环境，这种学习环境依赖于小批量统计数据来匹配不同图像区域的特征。由于小批量只提供一个估计值，真正的底层分布只能在通过大量小批量后才能得知。</p><p id="07d3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">为了应对这一挑战，FQ-甘提出在鉴别器中使用特征量化(FQ)。首先通过真实和虚假数据样本的最近训练历史中的特征的移动平均总结来构建字典。这使得能够在运行中构建大且一致的字典，从而促进GAN训练的在线时尚。每个字典条目代表相似图像区域的唯一特征原型。通过将传统甘算法中的连续特征量化到这些字典项中，提出的FQ-甘算法在鉴别器判断时，迫使真假图像从有限的值中构造它们的特征表示。这缓解了传统gan中小批量的不良估计问题。下图说明了FQ-GAN架构的主要组件:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/8d08671955c8e65e4c68f100ff28fc02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*imdV7x5XBetjpgUvZjufHA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:https://arxiv.org/abs/2004.02088</figcaption></figure><p id="b490" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">对FQ-甘的初步测试表明，所提出的模型可以改善各种大规模任务的图像生成。事实证明，FQ模块在匹配大型训练数据集中的要素时非常有效。FQ氮化镓的原理可以很容易地结合到现有的氮化镓架构。</p><h1 id="fd98" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">流行的</h1><p id="9252" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">在《通过预训练学习视觉和语言导航的通用代理<a class="ae lh" href="https://www.microsoft.com/en-us/research/publication/towards-learning-a-generic-agent-for-vision-and-language-navigation-via-pre-training/" rel="noopener ugc nofollow" target="_blank">一文中</a>，微软研究院介绍了一个DGM代理，它可以根据语言指令在视觉环境中导航。普遍解决的挑战是一个经典的挑战:在多模态输入中训练深度学习代理无异于一场噩梦。</p><p id="da01" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">为了解决多模态输入的挑战，popular提出预训练编码器对齐语言指令和视觉状态以用于联合表示。每个时间步的图像-文本-动作三元组被独立地输入到模型中，该模型被训练以预测屏蔽单词标记和下一个动作，从而形成自学习范例中的视觉和语言导航预训练。</p><p id="bea3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">为了对视觉和语言导航任务进行建模，popular依赖于三个基本数据集:房间到房间(R2R)、合作视觉和对话导航(CVDN)以及“救命，安娜！”(汉娜)。R2R是一个领域内的任务，语言指令在开始时给出，描述完整的导航路径。CVND和汉娜是域外任务；前者是基于对话历史导航，后者是交互环境，导航中间给出中间指令。</p><p id="cf4e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">流行的架构收集图像-文本-动作三元组从R2R数据集收集，并针对R2R、CVDN和汉娜环境中的任务进行微调。结果是一个智能体不仅能够掌握这三个环境，而且能够有效地概括未知环境和任务的知识。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/a650df847c3baeaeb6247cf536a77278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G86zmaBAnrj2vzZhKrpfLA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:https://arxiv.org/abs/2002.10638</figcaption></figure><p id="4996" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">DGMs是扩展深度学习模型的关键元素。微软与Optimus、FQ-甘和previous的研究成果提出了可以纳入新一代DGM模型的新想法。微软研究院开源了与这项工作相关的代码和研究论文。</p></div></div>    
</body>
</html>