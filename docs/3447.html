<html>
<head>
<title>2022: A Year Full of Amazing AI papers — A Review 🚀</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2022:充满惊人人工智能论文的一年——综述🚀</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/2022-a-year-full-of-amazing-ai-papers-a-review-5df00bcbf75a?source=collection_archive---------0-----------------------#2022-12-25">https://pub.towardsai.net/2022-a-year-full-of-amazing-ai-papers-a-review-5df00bcbf75a?source=collection_archive---------0-----------------------#2022-12-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="83c9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">按发布日期排列的人工智能最新突破的精选列表，带有清晰的视频解释、更深入文章的链接和代码。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1e3ed1149712a44d1fdbadbeb91501df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hmRrnj3oLk6U4uOsdwwQoQ.png"/></div></div></figure><p id="5fb1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虽然世界仍在复苏，但研究并没有放缓其疯狂的步伐，尤其是在人工智能领域。此外，今年还强调了许多重要方面，如道德方面、重要偏见、治理、透明度等等。人工智能和我们对人脑及其与人工智能的联系的理解正在不断发展，显示出在不久的将来改善我们生活质量的有前途的应用。然而，我们应该小心选择应用哪种技术。</p><blockquote class="lq lr ls"><p id="d590" class="ku kv lt kw b kx ky ju kz la lb jx lc lu le lf lg lv li lj lk lw lm ln lo lp im bi translated">"科学不能告诉我们应该做什么，只能告诉我们能做什么。"——让-保罗·萨特《存在与虚无》</p></blockquote><p id="1b88" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里有一个按发布日期排列的人工智能和数据科学最新突破的精选列表，有清晰的视频解释，更深入的文章链接和代码(如果适用)。享受阅读吧！</p><p id="cb0a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">本文末尾列出了每篇论文的完整参考资料。T3】</strong></p><p id="1d64" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">订阅我的<a class="ae lx" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank">时事通讯</a>——AI的最新更新每周都有解释。</p><p id="6aca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lt">随时给我</em> <a class="ae lx" href="https://www.louisbouchard.ai/contact/" rel="noopener ugc nofollow" target="_blank"> <em class="lt">发消息</em> </a> <em class="lt">我可能错过的任何有趣的论文都可以添加到这个列表中。</em></p><p id="f5b5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lt">在</em> <strong class="kw iu"> <em class="lt">上给我贴标签</em></strong><em class="lt"/><a class="ae lx" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"><em class="lt">@ Whats _ AI</em></a><em class="lt">或</em><strong class="kw iu"><em class="lt">LinkedIn</em></strong><em class="lt"/><a class="ae lx" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"><em class="lt">@ Louis(什么是AI) Bouchard </em> </a> <em class="lt">如果分享名单！</em>来我们的<a class="ae lx" href="https://www.louisbouchard.ai/learn-ai-together/" rel="noopener ugc nofollow" target="_blank">一起学AI不和谐社区</a>和我们聊天吧！</p><p id="cee1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">👀<strong class="kw iu">如果你愿意支持我的工作</strong>，你可以查看我的<a class="ae lx" href="https://www.patreon.com/whatsai" rel="noopener ugc nofollow" target="_blank">照片</a>。你也可以支持我，关注我最喜欢的<a class="ae lx" href="https://www.syntheticmind.io/subscribe?ref=EFowuebnlZ" rel="noopener ugc nofollow" target="_blank">每日人工智能简讯</a>来获得这些新论文的频繁更新！</p><h1 id="3ec4" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">在8分钟内观看完整的2022年倒带</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="22c3" class="ly lz it bd ma mb mz md me mf na mh mi jz nb ka mk kc nc kd mm kf nd kg mo mp bi translated">完整的列表</h1><ul class=""><li id="34e6" class="ne nf it kw b kx ng la nh ld ni lh nj ll nk lp nl nm nn no bi translated">分辨率稳健的大掩模修复与傅里叶卷积[1]</li><li id="992c" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">及时缝合:基于GAN的真实视频面部编辑[2]</li><li id="09a4" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">NeROIC:在线图像集合中对象的神经渲染[3]</li><li id="769a" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">SpeechPainter:文本条件下的语音修复[4]</li><li id="0c13" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">使用生成性面部先验实现真实世界的盲面部恢复[5]</li><li id="12c5" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">用于学习的多模态比对的4D网络[6]</li><li id="8144" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">具有多分辨率散列编码的即时神经图形图元[7]</li><li id="fc28" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">带剪辑潜在时间的分层文本条件图像生成[8]</li><li id="9da9" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">MyStyle:一个个性化的生成先验[9]</li><li id="04d4" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">OPT:开放预先训练的Transformer语言模型[10]</li><li id="00b3" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">BlobGAN:空间上不纠缠的场景表示[11]</li><li id="5c95" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">多面手代理[12]</li><li id="aad8" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">具有深度语言理解的真实感文本到图像扩散模型[13]</li><li id="fc71" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">迷你Dalle</li><li id="4177" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">不让一种语言掉队:扩展以人为中心的机器翻译[15]</li><li id="f06e" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">双快门光学振动传感[16]</li><li id="cb18" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">创建一个场景:基于场景的文本到图像的人类先验生成[17]</li><li id="8b78" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">BANMo:从许多休闲视频中构建可动画化的3D神经模型[18]</li><li id="d30c" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">使用潜在扩散模型的高分辨率图像合成[19]</li><li id="f69b" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">全景场景图生成[20]</li><li id="6ee8" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">一幅图像抵得上一个词:使用文本反转个性化文本到图像的生成[21]</li><li id="cb7e" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">通用视频识别的扩展语言-图像预训练模型[22]</li><li id="4b48" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">制作视频:无需文本-视频数据的文本-视频生成[23]</li><li id="6eb0" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">通过大规模弱监督实现稳健的语音识别[24]</li><li id="d570" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">DreamFusion:使用2D扩散将文本转换为3D[25]</li><li id="c8c1" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">Imagic:基于文本的真实图像编辑与扩散模型[26]</li><li id="7451" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">eDiffi:文本到图像的扩散模型与专家降噪集成[27]</li><li id="814e" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">InfiniteNature-Zero:学习从单个图像生成自然场景的永久视图[28]</li><li id="577a" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">卡拉狄加:科学的大型语言模型[29]</li><li id="6f44" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">通过音频-空间分解的实时神经辐射谈话肖像合成[30]</li><li id="201d" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">ChatGPT:优化对话的语言模型[31]</li><li id="d9b9" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">用于视觉效果的生产就绪面部再老化[32]</li><li id="7ff0" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文参考</li></ul></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="9ebb" class="ly lz it bd ma mb mz md me mf na mh mi jz nb ka mk kc nc kd mm kf nd kg mo mp bi translated">分辨率稳健的大掩模修复与傅里叶卷积[1]</h1><p id="fd62" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">你肯定经历过这种情况:你和你的朋友拍了一张很棒的照片，有人在你身后拍照，毁了你未来的Instagram帖子。那已经不是问题了。无论是一个人还是一个垃圾桶，你在自拍前忘记拿走，都破坏了你的照片。这个人工智能会自动删除图像中不需要的物体或人，并保存你的帖子。这就像一个专业的photoshop设计师在你的口袋里，并与一个简单的点击！</p><p id="e130" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">许多人工智能研究人员已经处理了很长时间的任务，即删除图像的一部分，并用它后面应该出现的内容来替换它。这被称为图像修复，非常具有挑战性…</p><ul class=""><li id="397e" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="8532" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/lama/" rel="noopener ugc nofollow" target="_blank">这个人工智能从你的图像中移除不想要的物体！</a></li><li id="0a2a" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/pdf/2109.07161.pdf" rel="noopener ugc nofollow" target="_blank">分辨率稳健的大型蒙版修复与傅立叶卷积</a></li><li id="9dae" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/saic-mdal/lama" rel="noopener ugc nofollow" target="_blank">代号</a></li><li id="63d9" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://colab.research.google.com/github/saic-mdal/lama/blob/master/colab/LaMa_inpainting.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab演示</a></li><li id="bd85" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://cleanup.pictures/" rel="noopener ugc nofollow" target="_blank">产品使用喇嘛</a></li></ul><h1 id="fae3" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">及时缝合:基于GAN的真实视频面部编辑[2]</h1><p id="9f37" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">你肯定看过像最近的《惊奇队长》或《双子杀手》这样的电影，在这些电影中，塞谬尔·杰克森和威尔·史密斯看起来要年轻得多。这需要专业人员花费数百甚至数千个小时来手工编辑他出现的场景。相反，你可以用一个简单的人工智能在几分钟内完成。事实上，许多技术允许你添加微笑，让你看起来更年轻或更老，所有这些都是自动使用基于人工智能的算法。它被称为视频中基于人工智能的人脸操作，这是2022年的最新技术水平！</p><ul class=""><li id="82f7" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="edfc" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/stitch-it-in-time/" rel="noopener ugc nofollow" target="_blank">真实视频的AI面部剪辑！及时缝合解释</a></li><li id="d0ff" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/abs/2201.08361" rel="noopener ugc nofollow" target="_blank">及时拼接:基于GAN的真实视频面部编辑</a></li><li id="db34" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/rotemtzaban/STIT" rel="noopener ugc nofollow" target="_blank">代码</a></li></ul><h1 id="ad29" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">NeROIC:在线图像集合中对象的神经渲染[3]</h1><p id="c856" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">神经渲染。神经渲染是从感兴趣的物体、人或场景的图片中生成空间中的照片真实感模型的能力，就像这个一样。在这种情况下，你会有一些这个雕塑的照片，并要求机器理解这些照片中的物体在太空中应该是什么样子。你基本上是在要求一台机器从图像中理解物理和形状。这对我们来说很容易，因为我们只知道真实世界及其深度，但对于一台只能看到像素的机器来说，这是一个完全不同的挑战。生成的模型看起来很准确，形状逼真，这很好，但是它如何融入新的场景呢？如果拍摄的照片中的光照条件不同，生成的模型看起来也不同，这取决于您查看它的角度，该怎么办？对我们来说，这自然会显得怪异和不切实际。这些是Snapchat和南加州大学在这项新研究中应对的挑战。</p><ul class=""><li id="89ec" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="0f7d" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/neroic/" rel="noopener ugc nofollow" target="_blank">用AI创建逼真的3D效果图！</a></li><li id="c23f" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/pdf/2201.02533.pdf" rel="noopener ugc nofollow" target="_blank"> NeROIC:来自在线图像集合的对象的神经渲染</a></li><li id="aef9" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/snap-research/NeROIC" rel="noopener ugc nofollow" target="_blank">代号</a></li></ul><h1 id="2ae0" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">SpeechPainter:文本条件下的语音修复[4]</h1><p id="63b8" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">我们见过图像修复，目的是从图片中删除不需要的对象。基于机器学习的技术不仅仅是移除物体，它们还理解图片，并用背景应该看起来的样子来填充图像的缺失部分。最近的进步令人难以置信，就像结果一样，这项修复任务对许多应用程序非常有用，如广告或改善你未来的Instagram帖子。我们还讨论了一个更具挑战性的任务:视频修复，将相同的过程应用于视频以移除物体或人物。</p><p id="cf4f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">视频的挑战在于保持帧与帧之间的一致性，没有任何瑕疵。但是现在，如果我们正确地从电影中删除一个人，而声音仍然在那里，没有变化，会发生什么？嗯，我们可能会听到鬼的声音，毁了我们所有的工作。</p><p id="86b5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是我从未在我的频道上报道过的一项任务的由来:语音修复。你没听错，谷歌的研究人员刚刚发表了一篇旨在修复语音的论文，正如我们将看到的，结果令人印象深刻。好吧，我们可能更愿意听到而不是看到结果，但你明白了。它可以纠正你的语法和发音，甚至消除背景噪音。所有我肯定需要继续努力的事情，或者…简单地使用他们的新模型…听听我视频中的例子！</p><ul class=""><li id="7af3" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="9662" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/speech-inpainting-with-ai/" rel="noopener ugc nofollow" target="_blank">演讲用AI修复！</a></li><li id="0577" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">Paper: <a class="ae lx" href="https://arxiv.org/pdf/2202.07273.pdf" rel="noopener ugc nofollow" target="_blank"> SpeechPainter:文本条件语音修复</a></li><li id="47e8" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://google-research.github.io/seanet/speechpainter/examples/" rel="noopener ugc nofollow" target="_blank">多听例子</a></li></ul><h1 id="44f5" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">使用生成性面部先验实现真实世界的盲面部恢复[5]</h1><p id="de17" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">你是否也有你自己的旧照片，或者你或你父母在我们制作出高质量图像之前拍摄的老照片？是的，我觉得那些记忆被永远地破坏了。伙计，我错了！</p><p id="82f2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个全新的完全免费的AI模型可以在一瞬间修复你的大部分旧图片。即使输入质量很低或很高，它也能很好地工作，这通常是一个很大的挑战。</p><p id="1593" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本周的论文名为“利用生成性面部先验实现真实世界的盲人面部恢复”，该论文解决了照片恢复任务，并取得了出色的结果。更酷的是，你可以自己尝试，用你喜欢的方式。他们已经开源了他们的代码，并创建了一个演示和在线应用程序供你现在尝试。如果你在上面看到的结果还不够有说服力，就看看视频，在评论里告诉我你的想法，我知道它会让你大吃一惊！</p><ul class=""><li id="bfaf" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="7b30" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/gfp-gan/" rel="noopener ugc nofollow" target="_blank">AI令人印象深刻的照片复原！</a></li><li id="1d8b" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/pdf/2101.04061.pdf" rel="noopener ugc nofollow" target="_blank">面向现实世界的生成式人脸先验的盲人脸恢复</a></li><li id="e2b9" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/TencentARC/GFPGAN" rel="noopener ugc nofollow" target="_blank">代号</a></li><li id="6607" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo" rel="noopener ugc nofollow" target="_blank"> Colab演示</a></li><li id="91c1" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://huggingface.co/spaces/akhaliq/GFPGAN" rel="noopener ugc nofollow" target="_blank">在线app </a></li></ul><h1 id="1709" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">用于学习的多模态比对的4D网络[6]</h1><p id="1ccc" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">自动驾驶汽车怎么看？</p><p id="2796" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能听说过他们使用的激光雷达传感器或其他奇怪的相机。但是他们是如何工作的，他们是如何看待这个世界的，与我们相比，他们到底看到了什么？如果我们想让它们上路，主要是如果我们在政府部门工作或制定下一个法规，了解它们是如何工作的是至关重要的。也是这些服务的客户。</p><p id="5cdf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们之前报道了特斯拉autopilot是如何看待和工作的，但它们不同于传统的自动驾驶汽车。特斯拉只使用相机来了解世界，而大多数像Waymo一样，使用常规相机和3D激光雷达传感器。这些激光雷达传感器非常容易理解:它们不会像普通相机一样产生图像，而是3D点云。激光雷达相机测量物体之间的距离，计算脉冲激光投射到物体上的传播时间。</p><p id="c8f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，我们如何有效地组合这些信息并让车辆理解它呢？这辆车最后看到了什么？只有到处分？在我们的道路上行驶足够了吗？我们将通过Waymo和Google Research的一篇新研究论文对此进行研究…</p><ul class=""><li id="3f30" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="13f4" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/waymo-lidar/" rel="noopener ugc nofollow" target="_blank">结合激光雷达和相机进行3D物体检测— Waymo </a></li><li id="f7d6" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Piergiovanni_4D-Net_for_Learned_Multi-Modal_Alignment_ICCV_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank">用于学习的多模态比对的4D网络</a></li></ul><h1 id="eb73" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">具有多分辨率散列编码的即时神经图形图元[7]</h1><p id="d6ca" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">好像拍一张照片还不是一个足够具有挑战性的技术能力，我们现在正在做相反的事情:从照片中模拟世界。我已经介绍了令人惊叹的基于人工智能的模型，这些模型可以拍摄图像并将其转化为高质量的场景。一项具有挑战性的任务包括在二维图片世界中拍摄一些图像，以创建物体或人在现实世界中的样子。</p><p id="8f62" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">拍几张照片，立即有一个现实的模型插入到您的产品。多酷啊。！</p><p id="dde8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">结果比我在2020年报道的第一个模型NeRF有了显著的提高。这种改进不仅仅是关于结果的质量。NVIDIA甚至做得更好。</p><p id="4292" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不仅质量相当，甚至更好，而且在不到两年的研究时间内，速度提高了1000多倍。</p><ul class=""><li id="b9e5" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="4104" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/nvidia-photos-into-3d-scenes/" rel="noopener ugc nofollow" target="_blank"> NVIDIA在毫秒内将照片变成3D场景</a></li><li id="fba7" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf" rel="noopener ugc nofollow" target="_blank">具有多分辨率哈希编码的即时神经图形图元</a></li><li id="7e08" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/NVlabs/instant-ngp" rel="noopener ugc nofollow" target="_blank">代号</a></li></ul><h1 id="231e" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">带剪辑潜在时间的分层文本条件图像生成[8]</h1><p id="3b54" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">去年我分享了DALL E，这是OpenAI的一个神奇模型，能够从文本输入生成图像，效果令人难以置信。现在是他的大哥DALL E 2的时候了。你不会相信一年内的进步！DALL E 2不仅更擅长从文本中生成照片般逼真的图像。结果是分辨率的四倍！</p><p id="4b59" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好像这还不够令人印象深刻，最近的模型学会了一项新技能；图像修复。</p><p id="848b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">DALL E可以从文本输入中生成图像。</p><p id="b96f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">DALL E 2可以做得更好，但不止于此。它还可以编辑这些图像，使它们看起来更好！或者简单地添加一个你想要的特征，比如背景中的一些火烈鸟。</p><p id="3424" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">听起来很有趣？在视频中了解更多信息或在下面阅读更多信息！</p><ul class=""><li id="4a54" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="59eb" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/openais-new-model-dall-e-2-is-amazing/" rel="noopener ugc nofollow" target="_blank"> OpenAI的新款DALL E 2太惊艳了！</a></li><li id="f59c" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">Paper: <a class="ae lx" href="https://cdn.openai.com/papers/dall-e-2.pdf" rel="noopener ugc nofollow" target="_blank">带剪辑潜在时间的分层文本条件图像生成</a></li></ul><h1 id="38e9" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">MyStyle:一个个性化的生成先验[9]</h1><p id="24d3" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">谷歌研究和特拉维夫大学的这个新模型令人难以置信。你可以把它看做是一个非常非常强大的深度赝品，它可以做任何事情。</p><p id="53ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">给任何一个人拍一百张照片，你就可以对其角色进行编码，以修复、编辑或创建任何你想要的逼真照片。</p><p id="5e09" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你问我，这既令人惊讶又令人害怕，尤其是当你看到结果的时候。观看视频查看更多结果，了解模型的工作原理！</p><ul class=""><li id="1701" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="d3b6" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/mystyle/" rel="noopener ugc nofollow" target="_blank">你的AI个人Photoshop专家！</a></li><li id="2f34" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/abs/2203.17272" rel="noopener ugc nofollow" target="_blank"> MyStyle:一个个性化的生成先验</a></li><li id="c1cc" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://mystyle-personalized-prior.github.io/" rel="noopener ugc nofollow" target="_blank">代号(即将推出)</a></li></ul><h1 id="5357" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">OPT:开放预先训练的Transformer语言模型[10]</h1><p id="37f4" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">我们都听说过GPT 3号，对它的能力有一定的了解。您肯定已经看到了一些严格按照这种模式诞生的应用程序，其中一些我在之前关于这种模式的视频中提到过。GPT-3是由OpenAI开发的一个模型，你可以通过一个付费的API访问它，但是不能访问模型本身。</p><p id="a53f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让GPT-3如此强大的是它的结构和大小。它有1750亿个参数。我们大脑中神经元数量的两倍！这个巨大的网络几乎是在整个互联网上训练出来的，以理解我们如何书写、交换和理解文本。本周，Meta为社区向前迈进了一大步。他们刚刚发布了一个同样强大的模型，而且完全开源。</p><ul class=""><li id="a7df" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="f2d8" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/opt-meta/" rel="noopener ugc nofollow" target="_blank"> Meta的新型号OPT是GPT-3最接近的竞争对手！(而且是开源的)</a></li><li id="a963" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">Paper: <a class="ae lx" href="https://arxiv.org/pdf/2205.01068.pdf" rel="noopener ugc nofollow" target="_blank"> OPT:打开预先训练好的Transformer语言模型</a></li><li id="b534" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/facebookresearch/metaseq" rel="noopener ugc nofollow" target="_blank">代码</a></li></ul><h1 id="693c" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">BlobGAN:空间上不纠缠的场景表示[11]</h1><p id="3b65" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">BlobGAN允许对图像进行不真实的操作，非常容易控制简单的blobs。所有这些小斑点代表一个物体，你可以四处移动它们或者让它们变大，变小，甚至移除它们，它会对它在图像中所代表的物体产生相同的效果。这太酷了。</p><p id="eb1f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如作者在他们的结果中分享的那样，你甚至可以通过复制斑点来创建新颖的图像，在数据集<a class="ae lx" href="https://youtu.be/mnEzjpiA_4E" rel="noopener ugc nofollow" target="_blank">中创建看不见的图像，就像一个有两个吊扇的房间</a>！如果我错了，请纠正我，但我相信这是一篇论文，如果不是第一篇的话，让图像的修改像移动斑点一样简单，并允许在训练数据集中看不到的编辑。</p><p id="62b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">和我们都知道的一些公司相比，你实际上可以玩这个！他们公开分享了他们的代码和一个你可以马上尝试的Colab演示。更令人兴奋的是BlobGAN的工作方式。在视频中了解更多信息！</p><ul class=""><li id="5ff2" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="3ddb" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/blobgan/" rel="noopener ugc nofollow" target="_blank">这是甘斯的一大步！布洛根解释道</a></li><li id="4a13" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://dave.ml/blobgan/" rel="noopener ugc nofollow" target="_blank">布洛根:空间上解缠的场景表现</a></li><li id="0f91" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/dave-epstein/blobgan" rel="noopener ugc nofollow" target="_blank">代号</a></li><li id="5a97" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://colab.research.google.com/drive/1clvh28Yds5CvKsYYENGLS3iIIrlZK4xO?usp=sharing#scrollTo=0QuVIyVplOKu" rel="noopener ugc nofollow" target="_blank"> Colab演示</a></li></ul><h1 id="7a12" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">多面手代理[12]</h1><p id="f259" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">DeepMind的加托刚刚出版！它是一个单一的变压器，可以玩雅达利游戏，字幕图像，与人聊天，控制一个真正的机械臂，等等！事实上，它只需接受一次训练，并使用相同的重量来完成所有这些任务。根据Deepmind的说法，这不仅是一个变形人，也是一个代理人。这就是当你把变形金刚和多任务强化学习代理的进程混合在一起时会发生的事情。</p><p id="fa27" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">加托是一个多模态代理人。这意味着它可以为图像创建标题，或者像聊天机器人一样回答问题。你会说GPT-3已经可以做到这一点，但加托可以做得更多……多模态来自于这样一个事实，即加托也可以在人类层面上玩雅达利游戏，甚至可以做真实世界的任务，如控制机械臂精确移动物体。它理解文字、图像，甚至物理…</p><ul class=""><li id="371c" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="9e63" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/deepmind-gato/" rel="noopener ugc nofollow" target="_blank"> Deepmind的新模型加托太神奇了！</a></li><li id="2dd8" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://storage.googleapis.com/deepmind-media/A%20Generalist%20Agent/Generalist%20Agent.pdf" rel="noopener ugc nofollow" target="_blank">通才代理</a></li></ul><h1 id="0c96" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">具有深度语言理解的真实感文本到图像扩散模型[13]</h1><p id="72c1" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">如果你认为Dall-e 2有很好的效果，那就等着看谷歌大脑的这个新模型能做什么吧。</p><p id="f72c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Dalle-e令人惊叹，但往往缺乏真实感，这就是该团队通过名为Imagen的新模型所攻击的。</p><p id="6d10" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">他们在他们的项目页面上分享了许多结果以及一个基准，他们引入该基准是为了比较文本到图像的模型，在这方面他们明显优于Dall-E 2和以前的图像生成方法。在视频中了解更多信息…</p><ul class=""><li id="35de" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="8f18" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/google-brain-imagen/" rel="noopener ugc nofollow" target="_blank">谷歌大脑对Dalle-e 2的回答:Imagen </a></li><li id="8761" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://imagen.research.google/paper.pdf" rel="noopener ugc nofollow" target="_blank">具有深度语言理解的真实感文本到图像扩散模型</a></li><li id="8782" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://imagen.research.google/" rel="noopener ugc nofollow" target="_blank">有结果的项目页面</a></li></ul><h1 id="aac8" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">达勒和米尼[14]</h1><p id="1c0d" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">Dalle mini非常棒，你可以使用它！</p><p id="4d7c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我相信你在过去的几天里已经在你的推特上看到了类似的图片。如果你想知道它们是什么，它们是由一个叫DALL E mini的人工智能生成的图像。如果你从未看过这些，你需要看看这个视频，因为你错过了。如果你想知道这怎么可能，好吧，你在完美的视频上，不到五分钟就会知道答案。</p><p id="4ec0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Dalle mini是一个免费的开源人工智能，可以从文本输入中生成令人惊叹的图像。</p><ul class=""><li id="d1f2" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="8f27" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/dalle-mini/" rel="noopener ugc nofollow" target="_blank">dalle-mini是如何工作的？</a></li><li id="7642" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/borisdayma/dalle-mini" rel="noopener ugc nofollow" target="_blank">代码</a></li><li id="415e" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://huggingface.co/spaces/dalle-mini/dalle-mini" rel="noopener ugc nofollow" target="_blank"> Huggingface官方演示</a></li></ul><h1 id="fd27" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">不让一种语言掉队:扩展以人为中心的机器翻译[15]</h1><p id="7108" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">Meta AI的最新模型名为“不让一种语言掉队”，它做到了这一点:它以最先进的质量翻译200种不同的语言。单个模型可以处理200种语言。多不可思议啊。</p><p id="6cf0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们发现严格意义上的英语很难有很好的结果，而Meta正在用相同的模型处理200种不同的语言，以及一些最复杂和代表性较低的语言，甚至连google translate都难以应对…</p><ul class=""><li id="afdf" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="dd8e" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/no-language-left-behind/" rel="noopener ugc nofollow" target="_blank">无语言遗留</a></li><li id="ebd2" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/facebookresearch/fairseq/tree/nllb" rel="noopener ugc nofollow" target="_blank">代码</a></li><li id="2078" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://ai.facebook.com/research/no-language-left-behind/" rel="noopener ugc nofollow" target="_blank">没有留下语言</a></li></ul><h1 id="9d7c" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">双快门光学振动传感[16]</h1><p id="a73c" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">他们使用相机和激光束在任何振动表面上重建声音，使他们能够隔离乐器，专注于特定的扬声器，消除环境噪音，以及许多令人惊叹的应用。</p><ul class=""><li id="104c" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="44d1" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/cvpr-2022-best-paper/" rel="noopener ugc nofollow" target="_blank"> CVPR 2022最佳论文荣誉奖:双快门光学振动感应</a></li><li id="2fb9" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://imaging.cs.cmu.edu/vibration/" rel="noopener ugc nofollow" target="_blank">项目页面</a></li><li id="bad6" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://www.marksheinin.com/_files/ugd/a41a28_7d370603fafd419da387de85d8ecb5b4.pdf?index=true" rel="noopener ugc nofollow" target="_blank">双快门光学振动感应</a></li></ul><h1 id="694d" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">创建一个场景:基于场景的文本到图像的人类先验生成[17]</h1><p id="459a" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">大闹不是“又一场闹剧”。这个新模型的目标不是像dalle那样允许用户根据文本提示生成随机图像——这真的很酷——而是限制了用户对图像生成的控制。</p><p id="48c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相反，Meta希望推动创造性表达向前发展，将这种文本到图像的趋势与以前的草图到图像的模式相结合，导致“制作场景”:文本和草图条件下的图像生成之间的奇妙融合。</p><ul class=""><li id="3149" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="160f" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/make-a-scene/" rel="noopener ugc nofollow" target="_blank">用文字和草图创作出令人惊叹的艺术品！</a></li><li id="f23b" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/pdf/2203.13131.pdf" rel="noopener ugc nofollow" target="_blank">制作场景:具有人类先验的基于场景的文本到图像生成</a></li></ul><h1 id="3c8a" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">BANMo:从许多休闲视频中构建可动画化的3D神经模型[18]</h1><p id="4ddb" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">使用BANMo从图片创建可变形的3D模型！</p><ul class=""><li id="16c9" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="e67f" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/banmo/" rel="noopener ugc nofollow" target="_blank">用人工智能建立可动画的3D模型</a></li><li id="ca6d" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://banmo-www.github.io/banmo-cvpr.pdf" rel="noopener ugc nofollow" target="_blank"> BANMo:从许多休闲视频中构建可动画化的3D神经模型</a></li><li id="a609" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/facebookresearch/banmo" rel="noopener ugc nofollow" target="_blank">代码</a></li></ul><h1 id="6347" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">使用潜在扩散模型的高分辨率图像合成[19]</h1><p id="84c9" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">最近所有像DALLE，Imagen，或Midjourney这样的超级强大的图像模型有什么共同点？除了它们高昂的计算成本、巨大的训练时间和共享的炒作之外，它们都基于同一个机制:扩散。扩散模型最近在大多数图像任务中取得了最先进的结果，包括使用DALLE的文本到图像，以及许多其他与图像生成相关的任务，如图像修补、样式转换或图像超分辨率。</p><ul class=""><li id="91dd" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="844e" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/latent-diffusion-models/" rel="noopener ugc nofollow" target="_blank">潜在扩散模型:稳定扩散背后的架构</a></li><li id="e3ea" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/pdf/2112.10752.pdf" rel="noopener ugc nofollow" target="_blank">用潜在扩散模型合成高分辨率图像</a></li><li id="e381" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/CompVis/latent-diffusion" rel="noopener ugc nofollow" target="_blank">代码</a></li></ul><p id="3bec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">👀如果你愿意支持我的工作，你可以去<a class="ae lx" href="https://github.com/sponsors/louisfb01" rel="noopener ugc nofollow" target="_blank">赞助</a>这个库或者在<a class="ae lx" href="https://www.patreon.com/whatsai" rel="noopener ugc nofollow" target="_blank"> Patreon </a>上支持我。</p><h1 id="d766" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">全景场景图生成[20]</h1><p id="4e75" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">全景场景图生成(PSG)是一个新的问题任务，旨在基于全景分割而不是包围盒来生成图像或场景的更全面的图形表示。它可以用来理解图像，并生成描述正在发生的事情的句子。这可能是一个人工智能最具挑战性的任务！在下面了解更多信息…</p><ul class=""><li id="455b" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="f9a6" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/psg/" rel="noopener ugc nofollow" target="_blank">对AI来说最具挑战性的任务之一</a></li><li id="f783" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/abs/2207.11247" rel="noopener ugc nofollow" target="_blank">全景场景图生成</a></li><li id="9042" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/Jingkang50/OpenPSG" rel="noopener ugc nofollow" target="_blank">代码</a></li><li id="515c" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://psgdataset.org/" rel="noopener ugc nofollow" target="_blank">数据集</a></li></ul><h1 id="6d87" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">一幅图像抵得上一个词:使用文本反转个性化文本到图像的生成[21]</h1><p id="430c" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">像DALLE或stable diffusion这样的文本到图像模型真的很酷，它允许我们通过简单的文本输入来生成精彩的图片。但是，给他们一张你的照片，然后让它变成一幅画，会不会更酷呢？想象一下，能够发送任何一个物体、人甚至你的猫的图片，并要求模型将其转换为另一种风格，比如将你自己变成一个电子人或变成你喜欢的艺术风格或添加到一个新的场景中。</p><p id="d2b6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">基本上，如果有一个DALLE版本，我们可以用它来photoshop我们的图片，而不是随机生成，那该有多酷？拥有个性化的DALLE，同时让控制代变得简单得多，因为“一个图像胜过千言万语”。这就像拥有一个像抖音算法一样个性化和令人上瘾的DALLE模型。</p><p id="86c0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是特拉维夫大学和英伟达的研究人员所做的工作。他们开发了一种调节文本到图像模型的方法，就像我上周提到的稳定扩散，用一些图像通过你将随图像一起发送的文字来代表任何物体或概念。将输入图像的对象转换成你想要的任何东西！</p><ul class=""><li id="73cb" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="44ec" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/imageworthoneword/" rel="noopener ugc nofollow" target="_blank">用您的图像引导稳定扩散</a></li><li id="4315" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/pdf/2208.01618v1.pdf" rel="noopener ugc nofollow" target="_blank">一幅图像抵得上一个单词:使用文本反转个性化文本到图像的生成</a></li><li id="e3b1" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/rinongal/textual_inversion" rel="noopener ugc nofollow" target="_blank">代码</a></li></ul><h1 id="b198" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">通用视频识别的扩展语言-图像预训练模型[22]</h1><p id="ed56" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">我们已经看到AI生成文本，然后生成图像，最近甚至生成短视频，尽管它们仍然需要工作。当你想到没有人真正参与这些作品的创作过程时，结果是令人难以置信的，它只需训练一次，然后就像稳定扩散一样被成千上万的人使用。尽管如此，这些模型真的明白他们在做什么吗？他们知道他们刚刚制作的图片或视频真正代表了什么吗？当这样的模型看到这样的图片或者更复杂的视频时，它能理解什么？</p><ul class=""><li id="acf2" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="5555" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简读:<a class="ae lx" href="https://www.louisbouchard.ai/general-video-recognition/" rel="noopener ugc nofollow" target="_blank">用AI进行通用视频识别</a></li><li id="1c14" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/abs/2208.02816" rel="noopener ugc nofollow" target="_blank">扩展通用视频识别语言图像预处理模型</a></li><li id="39a7" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/microsoft/VideoX/tree/master/X-CLIP" rel="noopener ugc nofollow" target="_blank">代码</a></li></ul><h1 id="be91" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">制作视频:无需文本-视频数据的文本-视频生成[23]</h1><p id="4762" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">Meta AI的新模型make-a-video已经问世，只需一句话:它从文本中生成视频。它不仅能够生成视频，而且是新的最先进的方法，比以往任何时候都能够生成更高质量和更连贯的视频！</p><ul class=""><li id="0f1c" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="f8e7" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/make-a-video/" rel="noopener ugc nofollow" target="_blank">制作视频:人工智能电影制作者！</a></li><li id="5584" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">纸张:<a class="ae lx" href="https://makeavideo.studio/Make-A-Video.pdf" rel="noopener ugc nofollow" target="_blank">制作视频:无需文本-视频数据的文本-视频生成</a></li><li id="b329" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/lucidrains/make-a-video-pytorch" rel="noopener ugc nofollow" target="_blank">代码</a></li></ul><h1 id="3960" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">通过大规模弱监督实现稳健的语音识别[24]</h1><p id="01cd" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">你有没有梦想过一个好的转录工具，可以准确地理解你说的话并写下来？不像YouTube上的自动翻译工具…我的意思是，它们很好，但远非完美。只要尝试一下，打开视频的功能，你就会明白我在说什么了。</p><p id="229e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">幸运的是，OpenAI刚刚发布并开源了一个非常强大的人工智能模型:Whisper。</p><p id="491d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它能理解我甚至不能理解的东西，即使我不是英语母语者(听视频)，它也能用于语言翻译！</p><ul class=""><li id="b86e" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="348d" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/whisper/" rel="noopener ugc nofollow" target="_blank"> OpenAI最新型号:Whisper(已解释)</a></li><li id="bf2d" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">通过大规模弱监督的鲁棒语音识别</a></li><li id="28af" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/openai/whisper" rel="noopener ugc nofollow" target="_blank">代号</a></li></ul><h1 id="a353" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">DreamFusion:使用2D扩散将文本转换为3D[25]</h1><p id="6c09" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">我们已经看到模型能够接受一个句子并生成图像。然后，通过学习像物体或特定样式这样的特定概念来操纵生成的图像的其他方法。</p><p id="918e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上周Meta发布了我提到的制作视频模型，它允许你从一个文本句子生成一个短视频。结果还不完美，但自去年以来我们在该领域取得的进展令人难以置信。</p><p id="654b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本周我们又向前迈进了一步。</p><p id="ce38" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是DreamFusion，一个新的谷歌研究模型，它可以理解一个句子，足以生成它的3D模型。你可以看到这是一个DALLE或稳定的扩散，但在三维。</p><ul class=""><li id="a3dc" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="7b41" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/dreamfusion/" rel="noopener ugc nofollow" target="_blank">来自文字的3D模型！DreamFusion解说</a></li><li id="e0c9" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/abs/2209.14988" rel="noopener ugc nofollow" target="_blank"> DreamFusion:使用2D扩散将文本转换为3D</a></li></ul><h1 id="2165" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">Imagic:基于文本的真实图像编辑与扩散模型[26]</h1><p id="3dfb" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">如果你认为最近的图像生成模型像DALLE或稳定扩散很酷，你就不会相信这是多么不可思议。“这一个”是形象的。Imagic采用这种基于扩散的模型，能够从中提取文本并生成图像，并调整该模型以编辑图像。你可以生成一个图像，然后教模型按照你想要的方式编辑它。</p><ul class=""><li id="97ab" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="1866" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/imagic/" rel="noopener ugc nofollow" target="_blank"> AI图片编辑自文字！Imagic解释</a></li><li id="5982" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/abs/2210.09276" rel="noopener ugc nofollow" target="_blank"> Imagic:利用扩散模型进行基于文本的真实图像编辑</a></li><li id="3694" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/justinpinkney/stable-diffusion/blob/main/notebooks/imagic.ipynb" rel="noopener ugc nofollow" target="_blank">稳定扩散实现</a></li></ul><h1 id="5c48" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">eDiffi:文本到图像的扩散模型与专家降噪集成[27]</h1><p id="9514" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">eDiffi是NVIDIA的最新型号，与DALLE 2或Stable Diffusion等所有以前的方法相比，它可以生成更好看、更准确的图像。eDiffi更好地理解你发送的文本，并且更加可定制，增加了一个我们在NVIDIA以前的论文中看到的功能:画师工具。</p><ul class=""><li id="c246" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="59a8" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/ediffi/" rel="noopener ugc nofollow" target="_blank"> eDiffi讲解:新SOTA图像合成模型！</a></li><li id="6c4e" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/abs/2211.01324" rel="noopener ugc nofollow" target="_blank"> eDiffi:文本到图像的扩散模型与专家降噪集成</a></li></ul><h1 id="cd80" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">InfiniteNature-Zero:学习从单个图像生成自然场景的永久视图[28]</h1><p id="6239" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">生成无限的新帧，就好像你正在飞向你的图像！</p><ul class=""><li id="2e0e" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="bec1" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/infinitenature-zero/" rel="noopener ugc nofollow" target="_blank"> InfiniteNature-Zero:用AI飞入你的画面！</a></li><li id="fcf2" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://infinite-nature-zero.github.io/static/pdfs/InfiniteNatureZero.pdf" rel="noopener ugc nofollow" target="_blank"> InfiniteNature-Zero:学习从单幅图像生成自然场景的永久视图</a></li><li id="bcc3" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><a class="ae lx" href="https://github.com/google-research/google-research/tree/master/infinite_nature_zero" rel="noopener ugc nofollow" target="_blank">代码</a></li></ul><h1 id="8cd3" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">卡拉狄加:科学的大型语言模型[29]</h1><p id="769b" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">卡拉狄加是一个大型语言模型，大小与GPT-3相当，但专门研究科学知识。该模型可以编写白皮书、评论、维基百科页面和代码。它知道如何引用，如何写方程。这对人工智能和科学来说是一件大事。</p><ul class=""><li id="177d" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="3f7a" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/galactica/" rel="noopener ugc nofollow" target="_blank">卡拉狄加:是什么，发生了什么？</a></li><li id="23d0" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://galactica.org/static/paper.pdf" rel="noopener ugc nofollow" target="_blank">卡拉狄加:科学的大型语言模型</a></li></ul><h1 id="e289" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">通过音频-空间分解的实时神经辐射谈话肖像合成[30]</h1><p id="499b" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">从一个单独的视频中，他们可以以更好的质量实时合成人所说的几乎任何单词或句子。你可以让一个正在说话的头实时跟随任何音轨。</p><ul class=""><li id="f5f7" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="3199" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简短阅读:<a class="ae lx" href="https://www.louisbouchard.ai/rad-nerf/" rel="noopener ugc nofollow" target="_blank">从音频到人工智能实时对话头！RAD-NeRF解释道</a></li><li id="cac5" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">论文:<a class="ae lx" href="https://arxiv.org/abs/2211.12368" rel="noopener ugc nofollow" target="_blank">通过音频-空间分解的实时神经辐射说话人像合成</a></li></ul><h1 id="3877" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">ChatGPT:优化对话的语言模型[31]</h1><p id="d25e" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">由于ChatGPT的力量和它提供的迷因潜力，它已经接管了Twitter和几乎整个互联网。我们都知道创造迷因是征服互联网的最好方法，所以它成功了。</p><p id="a29d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">既然你已经看到了无数的例子，你可能已经知道ChatGPT是OpenAI最近向公众发布的一个你可以与之聊天的AI。它也被称为聊天机器人，这意味着你可以与它交谈，模仿一对一的人类讨论。</p><p id="8129" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能不知道它是什么和它是如何工作的…观看视频或阅读下面的文章或博客来了解更多！</p><ul class=""><li id="8b0d" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="2b93" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短读:<a class="ae lx" href="https://www.louisbouchard.ai/chatgpt/" rel="noopener ugc nofollow" target="_blank">什么是ChatGPT？</a></li><li id="7e0e" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">博文:<a class="ae lx" href="https://openai.com/blog/chatgpt/" rel="noopener ugc nofollow" target="_blank"> ChatGPT:优化对话的语言模型</a></li></ul><h1 id="fdee" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">用于视觉效果的生产就绪面部再老化[32]</h1><p id="1e5f" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">无论是为了在Snapchat过滤器中寻找乐趣，还是为了看电影，甚至是为了解开几个谜语，我们都有一个能够在照片中改变我们年龄的实用程序。</p><p id="e098" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这通常是由熟练的艺术家使用Photoshop或类似的工具来编辑你的图片。最糟糕的是，在一个视频中，他们必须对每一帧都进行这种人工编辑！想象一下为此需要做多少工作。好吧，这里有一个解决方案和这种情况下的新问题…👇</p><ul class=""><li id="2e77" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">短视频讲解:</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><ul class=""><li id="b2a1" class="ne nf it kw b kx ky la lb ld nx lh ny ll nz lp nl nm nn no bi translated">简读:<a class="ae lx" href="https://www.louisbouchard.ai/disney-re-age/" rel="noopener ugc nofollow" target="_blank">用AI自动再衰老！迪士尼弗兰模式讲解</a></li><li id="7b8c" class="ne nf it kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">博客文章:<a class="ae lx" href="https://studios.disneyresearch.com/2022/11/30/production-ready-face-re-aging-for-visual-effects/" rel="noopener ugc nofollow" target="_blank">为了视觉效果而进行的面部再老化</a></li></ul></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><blockquote class="lq lr ls"><p id="236e" class="ku kv lt kw b kx ky ju kz la lb jx lc lu le lf lg lv li lj lk lw lm ln lo lp im bi translated">如果你想阅读更多的论文并有更广阔的视野，这里有另一个涵盖2021年的伟大知识库:<a class="ae lx" href="https://github.com/louisfb01/best_AI_papers_2021" rel="noopener ugc nofollow" target="_blank"> 2021:充满令人惊叹的人工智能论文的一年-综述</a>，并随时订阅我的每周<a class="ae lx" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank">简讯</a>，了解2022年人工智能的最新出版物！</p></blockquote><p id="40ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lt">在Twitter上给我加标签</em><a class="ae lx" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"><em class="lt">@ Whats _ AI</em></a><em class="lt">或者LinkedIn </em> <a class="ae lx" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> <em class="lt"> @Louis(什么是AI) Bouchard </em> </a> <em class="lt">如果分享名单！</em></p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="10bd" class="ly lz it bd ma mb mz md me mf na mh mi jz nb ka mk kc nc kd mm kf nd kg mo mp bi translated">论文参考</h1><p id="95b8" class="pw-post-body-paragraph ku kv it kw b kx ng ju kz la nh jx lc ld nu lf lg lh nv lj lk ll nw ln lo lp im bi translated">[1]r .、Logacheva、e .、Mashikhin、a .、Remizova、a .、Ashukha、a .、Silvestrov、a .、Kong、n .、Goka、h .、Park、k .和Lempitsky、v .，2022年。基于傅里叶卷积的分辨率稳健的大掩模修复。IEEE/CVF计算机视觉应用冬季会议论文集(第2149-2159页)。，<a class="ae lx" href="https://arxiv.org/pdf/2109.07161.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2109.07161.pdf</a></p><p id="91b7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2] Tzaban，r .，Mokady，r .，Gal，r .，Bermano，A.H .和Cohen-Or，d .，2022年。实时拼接:基于GAN的真实视频面部编辑。<a class="ae lx" href="https://arxiv.org/abs/2201.08361" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2201.08361</a></p><p id="64ea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3] Kuang，z .，Olszewski，k .，Chai，m .，Huang，z .，Achlioptas，p .和Tulyakov，s .，2022年。NeROIC:来自在线图像集合的对象的神经渲染。https://arxiv.org/pdf/2201.02533.pdf<a class="ae lx" href="https://arxiv.org/pdf/2201.02533.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="aefe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[4]z .博尔索斯、m .沙里菲和m .塔利亚萨基，2022年。语音修复:文本条件下的语音修复。【https://arxiv.org/pdf/2202.07273.pdf T4】</p><p id="d7e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[5]王等，李等，张等，山等，2021 .基于生成人脸先验的真实世界盲人脸恢复。《IEEE/CVF计算机视觉和模式识别会议论文集》(第9168-9178页)，<a class="ae lx" href="https://arxiv.org/pdf/2101.04061.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2101.04061.pdf</a></p><p id="8000" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[6]皮尔乔瓦尼，A.J .，卡塞尔，v .，Ryoo，M.S .和安热洛娃，a .，2021年。用于学习的多模态比对的4d-net。在《IEEE/CVF计算机视觉国际会议论文集》(第15435–15445页)中，<a class="ae lx" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Piergiovanni_4D-Net_for_Learned_Multi-Modal_Alignment_ICCV_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content/iccv 2021/papers/Piergiovanni _ 4D-Net _ for _ Learned _ Multi-Modal _ Alignment _ ICCV _ 2021 _ paper . pdf</a>。</p><p id="81c9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[7]托马斯·穆勒、亚历克斯·埃文斯、克里斯托夫·席德和亚历山大·凯勒，2022，“具有多分辨率散列编码的即时神经图形原语”，<a class="ae lx" href="https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf" rel="noopener ugc nofollow" target="_blank">https://NV labs . github . io/Instant-ngp/assets/mueller 2022 Instant . pdf</a></p><p id="7053" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[8] A. Ramesh等人，2022，“具有剪辑潜在性的分层文本条件图像生成”，<a class="ae lx" href="https://cdn.openai.com/papers/dall-e-2.pdf" rel="noopener ugc nofollow" target="_blank">https://cdn.openai.com/papers/dall-e-2.pdf</a></p><p id="717b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[9] Nitzan，y .，Aberman，k .，He，q .，巴丽，o .，Yarom，m .，Gandelsman，y .，Mosseri，I .，Pritch，y .，Cohen-Or，d .，2022年。我的风格:个性化的生成先验。arXiv预印本arXiv:2203.17272。</p><p id="0083" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[10]张，苏珊等:“开放预训练转换语言模型”<a class="ae lx" href="https://arxiv.org/abs/2205.01068" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2205.01068</a></p><p id="7af5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[11] Epstein，d .，Park，t .，Zhang，r .，Shechtman，e .和Efros，A.A .，2022年。BlobGAN:空间上不纠缠的场景表现。arXiv预印本arXiv:2205.02837。</p><p id="6b36" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[12] Reed S .等人，2022年，Deemind:加托——多面手代理，<a class="ae lx" href="https://storage.googleapis.com/deepmind-media/A%20Generalist%20Agent/Generalist%20Agent.pdf" rel="noopener ugc nofollow" target="_blank">https://storage . Google APIs . com/deep mind-media/A % 20 general ist % 20 agent/general ist % 20 agent . pdf</a></p><p id="9ab0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[13] Saharia等人，2022，Google Brain，具有深度语言理解的真实感文本到图像扩散模型，【https://gweb-research-imagen.appspot.com/paper.pdf T2】</p><p id="80c7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[14] Dayma等人，2021，DALL E Mini，doi:10.5281/zenodo.5146400</p><p id="d518" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[15] NLLB团队等，2022，没有语言落后:扩大以人为中心的机器翻译</p><p id="b5d8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[16] Sheinin，Mark and Chan，Dorian and O'Toole，Matthew and Narasimhan，Srinivasa G .，2022，双快门光学振动传感，Proc .IEEE CVPR。</p><p id="52c7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[17] Gafni，o .，Polyak，a .，Ashual，o .，Sheynin，s .，Parikh，d .和Taigman，y .，2022年。有人类先验的基于场景的文本到图像生成。【https://arxiv.org/pdf/2203.13131.pdf T4】</p><p id="d001" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[18]杨(g)、沃(m)、内韦尔诺瓦(n)、拉马南(d)、韦达尔迪(a)和朱奥(h ), 2022年。Banmo:从许多休闲视频中构建可动画化的3d神经模型。IEEE/CVF计算机视觉和模式识别会议论文集(第2863-2873页)。</p><p id="48ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[19]罗姆巴赫、布拉特曼、洛伦茨、埃塞和奥默，2022年。用潜在扩散模型合成高分辨率图像。《IEEE/CVF计算机视觉和模式识别会议论文集》(第10684–10695页)，<a class="ae lx" href="https://arxiv.org/pdf/2112.10752.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2112.10752.pdf</a></p><p id="5587" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[20]杨军，杨玉珍，郭，周，张，张，刘，2022 .全景场景图生成。arXiv预印本arXiv:2207.11247。</p><p id="0cfb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[21] Gal，r .，Alaluf，y .，Atzmon，y .，Patashnik，o .，Bermano，A.H .，Chechik，g .和Cohen-Or，d .，2022年。一幅图像抵得上一个词:使用文本反转个性化文本到图像的生成。</p><p id="488a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[22]倪，b，彭，h，陈，m，张，s，孟，g，傅，j，向，s和凌，h，2022。通用视频识别的扩展语言图像预处理模型。arXiv预印本arXiv:2208.02816。</p><p id="6656" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[23]辛格等人(Meta AI)，2022，“制作视频:无文本-视频数据的文本-视频生成”，<a class="ae lx" href="https://makeavideo.studio/Make-A-Video.pdf" rel="noopener ugc nofollow" target="_blank">https://makeavideo.studio/Make-A-Video.pdf</a></p><p id="6d07" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[24]，a .，Kim，J.W .，Xu，t .，Brockman，g .，McLeavey，c .和Sutskever，I .，通过大规模弱监督的鲁棒语音识别。</p><p id="2ccf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[25]b .普尔、a .贾恩、J.T .巴伦和b .米尔登霍尔，2022年。DreamFusion:使用2D扩散将文本转换为3D。arXiv预印本arXiv:2209.14988。</p><p id="0119" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[26] Kawar，b .，Zada，s .，Lang，o .，Tov，o .，Chang，h .，Dekel，t .，Mosseri，I .，和伊拉尼，m .，2022年。基于文本的真实图像编辑与扩散模型。arXiv预印本arXiv:2210.09276。</p><p id="f983" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[27] Balaji，y .等人，2022，eDiffi:具有专家降噪器集合的文本到图像扩散模型，<a class="ae lx" href="https://arxiv.org/abs/2211.01324" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2211.01324</a></p><p id="7d06" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[28]李，z .，王，q .，斯内夫利，n .和金泽，a .，2022年。从单幅图像中学习自然场景的永久视图生成。在欧洲计算机视觉会议上(第515-534页)。斯普林格，查姆，【https://arxiv.org/abs/2207.11148 T2】</p><p id="a8c4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[29]泰勒等人，2022:卡拉狄加:科学的大型语言模型，【https://galactica.org/】T4</p><p id="0904" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[30]唐军，王，周，陈，陈，何，胡，刘，曾，王，2022 .通过音频-空间分解的实时神经辐射说话人像合成。arXiv预印本arXiv:2211.12368。</p><p id="c252" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[31] OpenAI，2022: ChatGPT:优化对话的语言模型，<a class="ae lx" href="https://openai.com/blog/chatgpt/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/chatgpt/</a></p><p id="08ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[32] Loss等人，迪士尼研究，2022年:FRAN，<a class="ae lx" href="https://studios.disneyresearch.com/2022/11/30/production-ready-face-re-aging-for-visual-effects/" rel="noopener ugc nofollow" target="_blank">https://studios . Disney research . com/2022/11/30/production-ready-face-re-aging-for-visual-effects/</a></p></div></div>    
</body>
</html>