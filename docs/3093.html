<html>
<head>
<title>TransUNet — Revolutionize Traditional Image Segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TransUNet —革新传统的图像分割</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/transunet-no-more-cnns-for-image-segmentation-278e85c81914?source=collection_archive---------0-----------------------#2022-09-04">https://pub.towardsai.net/transunet-no-more-cnns-for-image-segmentation-278e85c81914?source=collection_archive---------0-----------------------#2022-09-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="047f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak"> <em class="kf">结合CNN和Transformer更新U-Net，在图像分割任务上实现SOTA结果。</em>T3】</strong></h2></div><h1 id="5f5f" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">目录</h1><p id="0e72" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><a class="ae lu" href="#e484" rel="noopener ugc nofollow">🔥直觉</a> <br/> <a class="ae lu" href="#8e89" rel="noopener ugc nofollow">🔥TransUNet </a> <br/> ∘ <a class="ae lu" href="#9938" rel="noopener ugc nofollow">下采样(编码)</a> <br/> ∘ <a class="ae lu" href="#230c" rel="noopener ugc nofollow">上采样(解码)</a> <br/> <a class="ae lu" href="#ccf6" rel="noopener ugc nofollow">🔥结果</a> <br/> <a class="ae lu" href="#0809" rel="noopener ugc nofollow">🔥</a>实现<br/>引用<a class="ae lu" href="#900a" rel="noopener ugc nofollow">实现</a></p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi lv"><img src="../Images/6c6b2a45732c80f65ebf5a7bb15e44d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qlKlvBVExyphZmeR"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated"><a class="ae lu" href="https://unsplash.com/es/@jasperguy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">贾斯珀盖伊</a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="e484" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">🔥直觉</h1><p id="43f0" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如今，<a class="ae lu" href="https://medium.com/@mlquest0/unet-clearly-explained-a-better-image-segmentation-architecture-f48661c92df9#3e17" rel="noopener"> U-Net </a>已经主导了图像分割任务，尤其是在医学影像领域。在迄今为止提出的大多数U-网中，卷积神经网络(CNN)被广泛用作它们的底层结构。</p><p id="ab7a" class="pw-post-body-paragraph ky kz iq la b lb ml jr ld le mm ju lg lh mn lj lk ll mo ln lo lp mp lr ls lt ij bi translated">然而，<strong class="la ir">由于卷积核尺寸较小，CNN只能有效利用短程(或本地)信息</strong>，在具有远程关系特征的任务中，无法充分探索远程信息。</p><p id="10af" class="pw-post-body-paragraph ky kz iq la b lb ml jr ld le mm ju lg lh mn lj lk ll mo ln lo lp mp lr ls lt ij bi translated"><a class="ae lu" href="https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04" rel="noopener">通常用于处理自然语言处理任务的变形金刚</a>，可以有效地探索长程信息，但<strong class="la ir">它们在探索短程信息方面不如CNN</strong>。</p><p id="73ae" class="pw-post-body-paragraph ky kz iq la b lb ml jr ld le mm ju lg lh mn lj lk ll mo ln lo lp mp lr ls lt ij bi translated">在图像分割任务中，利用细胞神经网络的能力可以弥补Transformer的不足，反之亦然，陈等人提出了<strong class="la ir"> TransUNet </strong>，这也是<strong class="la ir">第一个由Transformer </strong>构建的图像分割模型。<em class="mq">同样值得一提的是，作者通过首次尝试使用纯变压器架构进行图像分割，验证了将CNN和变压器相结合的有希望的结果。然而，它的效果不如在他们的架构中引入CNN，因为变压器在利用本地特性方面不如CNN。</em></p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="8e89" class="kg kh iq bd ki kj my kl km kn mz kp kq jw na jx ks jz nb ka ku kc nc kd kw kx bi translated"><strong class="ak">🔥TransUNet </strong></h1><p id="1a24" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们知道在<a class="ae lu" href="https://medium.com/@mlquest0/unet-clearly-explained-a-better-image-segmentation-architecture-f48661c92df9#3e17" rel="noopener"> U-Net </a>(图1)中有一个<strong class="la ir">编码器</strong>(下采样路径)和一个<strong class="la ir">解码器</strong>(上采样路径)。下采样路径会将图像的特征编码到高级映射中，上采样路径会使用其细节来生成与输入维数相同的最终掩膜。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nd"><img src="../Images/a72a5883a48a23c607afd5ae02e401d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*38vydfXeaN0Nc1p7.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">图Ronneberger等人的<a class="ae lu" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">原始论文</a>中的U-Net图。</figcaption></figure><p id="5689" class="pw-post-body-paragraph ky kz iq la b lb ml jr ld le mm ju lg lh mn lj lk ll mo ln lo lp mp lr ls lt ij bi translated">类似地，TransUNet还包括一个<strong class="la ir">编码器</strong>和一个<strong class="la ir">解码器</strong>，用于编码和解码图像信息以产生一个分割。与传统的U-Nets不同，TransUNet使用一种混合CNN-Transformer架构<strong class="la ir">作为编码器，以学习来自CNN的高分辨率空间信息和来自Transformer的全局上下文信息。</strong></p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ne"><img src="../Images/ae33dcf4f2ebb9ff3d6a206d18e43ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1l_NSL1H1Evbn6IImZzx0A.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">图2: TransUNet架构设计概述。MSA代表多头自我关注，MLP代表多层感知器。</figcaption></figure><p id="6728" class="pw-post-body-paragraph ky kz iq la b lb ml jr ld le mm ju lg lh mn lj lk ll mo ln lo lp mp lr ls lt ij bi translated">要分解步骤:</p><h2 id="9938" class="nf kh iq bd ki ng nh dn km ni nj dp kq lh nk nl ks ll nm nn ku lp no np kw nq bi translated">下采样(编码)</h2><ul class=""><li id="84ad" class="nr ns iq la b lb lc le lf lh nt ll nu lp nv lt nw nx ny nz bi translated">首先，使用CNN作为特征提取器来为输入生成特征图，如图2中的粉色方框所示。</li><li id="2623" class="nr ns iq la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">对于特征提取器的每一级，输出特征图(<em class="mq">编码中间高级特征图</em>)然后被<strong class="la ir">连接</strong>到同一级的解码器路径，如图2中的虚线箭头所示。</li><li id="848a" class="nr ns iq la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">然后，通过<strong class="la ir">线性投影，</strong>将特征图表征(矢量化)成形状(n_patch，D)的2D嵌入，D是嵌入的总长度。嵌入是预先训练好的，将保留特征图的位置信息(<em class="mq">如果你不理解，现在不用担心，因为它不会太妨碍你理解trans unet</em>)。</li><li id="80fa" class="nr ns iq la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">在获得嵌入之后，它们被馈送到<strong class="la ir">12</strong>24】变换器层中，以从图像中编码较少的短程信息和较多的远程信息。每一层如图2 (a)所示，使用多头自注意(<strong class="la ir"> MSA </strong>)和多层感知器(<strong class="la ir"> MLP </strong>)模块。MSA是变压器的基本构建模块，在这里<a class="ae lu" href="https://paperswithcode.com/method/multi-head-attention" rel="noopener ugc nofollow" target="_blank">解释</a>，MLP只是由几个完全连接的层组成。</li><li id="7d7a" class="nr ns iq la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">最后，<strong class="la ir">为了准备上采样路径</strong>，输出被整形为(D，H/16，W/16)。H/16和W/16意味着由于之前的操作，此时的高度和宽度已经缩小了16倍。</li></ul><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi of"><img src="../Images/6d9a908f9e5a9d90db77e022ac1850e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PcRsf7bdNuo3HMF7ELxvvQ.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">图TransUNet架构概述的简化版本。</figcaption></figure><h2 id="230c" class="nf kh iq bd ki ng nh dn km ni nj dp kq lh nk nl ks ll nm nn ku lp no np kw nq bi translated">上采样(解码)</h2><p id="1da0" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">上采样过程非常简单(<em class="mq">没有任何花哨的技术</em>)。</p><ul class=""><li id="e269" class="nr ns iq la b lb ml le mm lh og ll oh lp oi lt nw nx ny nz bi translated">首先，来自CNN-Transformer编码器的输入由具有ReLU激活的<strong class="la ir"> 3x3卷积层运行，上采样</strong>，然后<strong class="la ir">将</strong>与<strong class="la ir">第三级</strong> CNN特征提取器的输出连接。</li><li id="073f" class="nr ns iq la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">所得到的特征图然后通过与ReLU激活层<strong class="la ir">的3×3卷积再次运行</strong>。该输出然后与来自第二级<strong class="la ir">CNN特征提取器的输出连接。</strong></li><li id="9d6d" class="nr ns iq la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">再次重复步骤<strong class="la ir">。现在，输出是形状为<strong class="la ir"> (C，H，W) </strong>的遮罩，其中C =目标类的数量，H =图像高度，W =图像宽度。</strong></li></ul><p id="57f3" class="pw-post-body-paragraph ky kz iq la b lb ml jr ld le mm ju lg lh mn lj lk ll mo ln lo lp mp lr ls lt ij bi translated">作者还指出<strong class="la ir">更密集地结合低级特征通常会导致更好的分割精度</strong>。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="ccf6" class="kg kh iq bd ki kj my kl km kn mz kp kq jw na jx ks jz nb ka ku kc nc kd kw kx bi translated">🔥结果</h1><p id="566a" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">该模型在Synapse多器官分割数据集上运行。通过骰子相似系数和Hausdorff距离来评估最终得分。</p><p id="e3d9" class="pw-post-body-paragraph ky kz iq la b lb ml jr ld le mm ju lg lh mn lj lk ll mo ln lo lp mp lr ls lt ij bi translated">如您所见，TransUNet的表现优于大多数现有的SOTA架构，如V-Net、ResNet U-Net、ResNet Attention U-Net和Vision Transformers，这表明基于Transformer的架构在利用自我关注方面优于其他基于自我关注的CNN U-Net。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oj"><img src="../Images/4dd749fbaa92b9dbd4a63e27d9d23701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-ubS9mmNfvgG7u7Ed_akA.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">表1:与其他SOTA图像分割模型相比，TransUNet的性能总结。</figcaption></figure><h1 id="0809" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">🔥履行</h1><p id="7163" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><a class="ae lu" href="https://github.com/Beckschen/TransUNet" rel="noopener ugc nofollow" target="_blank">官方TransUNet实施</a></p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="2635" class="pw-post-body-paragraph ky kz iq la b lb ml jr ld le mm ju lg lh mn lj lk ll mo ln lo lp mp lr ls lt ij bi translated">谢谢你！❤️:我们可以恳求你考虑给我们一些掌声吗！❤️ </p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="900a" class="kg kh iq bd ki kj my kl km kn mz kp kq jw na jx ks jz nb ka ku kc nc kd kw kx bi translated">参考</h1><p id="b115" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><a class="ae lu" href="https://arxiv.org/pdf/2102.04306.pdf" rel="noopener ugc nofollow" target="_blank"> TransUNet:变形金刚为医学图像分割制造强大的编码器</a></p></div></div>    
</body>
</html>