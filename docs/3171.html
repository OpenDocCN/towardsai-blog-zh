<html>
<head>
<title>Building Feedforward Neural Networks from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始构建前馈神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/building-feedforward-neural-networks-from-scratch-c5a5cf23b97b?source=collection_archive---------2-----------------------#2022-10-01">https://pub.towardsai.net/building-feedforward-neural-networks-from-scratch-c5a5cf23b97b?source=collection_archive---------2-----------------------#2022-10-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="649b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于前馈神经网络(FFNNs)你需要知道的一切。从学习什么是感知机，到深度神经网络，到梯度下降和反向传播。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/166057f88ba7c2c09a10b2ecb8d63a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1HbdwyCE8coZLetO40qdXA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@urielsc26" rel="noopener ugc nofollow" target="_blank">乌列尔资深大律师</a>在<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="6b88" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">本文将向您介绍什么是前馈神经网络。从基础开始，像什么是感知机，到达反向传播。</p><p id="8a16" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在文章的最后一部分，有一个关于如何使用Tensorflow在Python中构建FFNN的教程。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="dee9" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">由于许多主题太大，无法在一篇文章中完全解释清楚，所以在许多段落的结尾有一个部分叫做“<strong class="lf ir">推荐阅读</strong>”，在那里你可以找到真正有帮助的文章来了解这些主题的更多信息。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="8c6a" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><em class="lz">在开始阅读本文之前，我只想告诉你，如果你对深度学习、图像分析和计算机视觉感兴趣，我鼓励你看看我的另一篇文章:</em></p><div class="ma mb gp gr mc md"><a href="https://medium.com/geekculture/train-stylegan2-ada-with-custom-datasets-in-colab-11accf22ef9b" rel="noopener follow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">使用Colab中的自定义数据集训练StyleGAN2-ADA</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">在本文中，我们将使用TensorFlow 1.14在Google Colab的自定义数据集上训练NVIDIA的StyleGAN2-ADA</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">medium.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr kp md"/></div></div></a></div></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="d58c" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">目录</h1><ol class=""><li id="b939" class="nk nl iq lf b lg nm lj nn lm no lq np lu nq ly nr ns nt nu bi translated">神经网络</li><li id="da81" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">前馈神经网络</li><li id="54db" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">为什么要分层？</li><li id="3975" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">价值函数</li><li id="a94e" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">梯度下降</li><li id="66ee" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">反向传播</li><li id="daa0" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">教程-构建前馈神经网络</li><li id="f910" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">结论</li><li id="f564" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly nr ns nt nu bi translated">参考</li></ol></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="c473" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">1.神经网络</h1><p id="28e4" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated"><strong class="lf ir">神经网络</strong>，也称为<strong class="lf ir">人工神经网络(ANNs) </strong>或<strong class="lf ir">模拟神经网络(SNNs) </strong>，是机器学习的子集，是深度学习算法的核心。它们的结构受到人类大脑中神经元的启发，以及它们通过相互发送电荷来工作的方式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/761450aef7cf3feb12a44cbc599fec1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*8-b_WJLBOJ5X5OZKip_JvA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">人工神经网络，图片由<a class="ae kv" href="https://medium.com/@nicolotognoni" rel="noopener">作者</a></figcaption></figure><p id="bb3e" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">它们由节点层组成:输入层、一个或多个隐藏层以及输出层。节点相互连接，对于每个连接，都有一个相关的权重。如果该节点的输出大于某个阈值，则该节点被激活，并且数据被传递到下一层。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/b88bfaf8d0fc2de9ab84c39b7a6f587f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUJ51kA9tHh9UK33Uu_zhw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">感知器，图片作者<a class="ae kv" href="https://medium.com/@nicolotognoni" rel="noopener">作者</a></figcaption></figure><p id="140a" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了理解神经网络如何工作，我们首先必须了解一种叫做<strong class="lf ir">感知器</strong>的<strong class="lf ir">人工神经元</strong>(上图)。它由四部分组成:一个输入层、权重和偏差、净和以及激活函数。</p><ul class=""><li id="c72a" class="nk nl iq lf b lg lh lj lk lm of lq og lu oh ly oi ns nt nu bi translated"><strong class="lf ir">输入</strong>、<em class="lz"> x1、x2、… </em>被传递给<strong class="lf ir">感知器</strong>。输入和单输出都是二进制的。</li><li id="4fe2" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated"><strong class="lf ir">权重</strong>、<em class="lz"> w1、w2、…、</em>表示网络中各个输入对输出的重要性。它们是由实数组成的。</li><li id="cdc8" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">输入<em class="lz"> x </em>乘以其权重<em class="lz"> w. </em>，然后<strong class="lf ir">对所有值求和</strong>，并将结果传递给<strong class="lf ir"> <em class="lz">激活函数</em> </strong>。</li><li id="5e2d" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">激活功能根据确定的<strong class="lf ir"> <em class="lz">阈值</em> </strong> <em class="lz">确定输出值为0或1。</em></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/c1547712eeba3cd1a659c7143cd59ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BuVefnJxuzZEnX70IuZ5Gw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">阶跃激活函数</figcaption></figure><p id="dfe9" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">感知器使用<strong class="lf ir">阶跃激活函数</strong>，当<em class="lz"> x &lt; 0 </em>时为0，如果<em class="lz"> x ≥ 0则为1。</em></p><p id="693d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">同样的事情也发生在我们大脑中的神经元上:<strong class="lf ir">树突</strong>从<strong class="lf ir">突触</strong>收集电荷，既有抑制性的，也有兴奋性的。一旦超过阈值，累积的电荷被释放(神经元放电)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/b42de89cdb946533a8114ec4972efb4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*tLo64AQyyC9zIPnm3kO4Lg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">神经元由Dhp1080，<a class="ae kv" href="http://creativecommons.org/licenses/by-sa/3.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 3.0 </a>，attraverso维基共享</figcaption></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="35cb" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">2.前馈神经网络</h1><p id="19d2" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated"><strong class="lf ir">前馈神经网络(FFNNs) </strong>又称<strong class="lf ir">多层感知器(MLPs) </strong>由一个输入层，一个输出层，以及中间的许多隐含层组成。这个想法是你通过许多非线性函数的组合来转换信号。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/a142696831a8a8181f2cbfd4c596b32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDGCkDjI4e9dLQz-oy2F_g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">前馈神经网络，图片由<a class="ae kv" href="https://medium.com/@nicolotognoni" rel="noopener">作者</a></figcaption></figure><p id="a0be" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">您可以选择许多激活函数，但最受欢迎的非线性函数是<strong class="lf ir"> Sigmoid </strong>和<strong class="lf ir"> ReLU </strong>。这里有一些在神经网络中常用的激活函数的例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/4a043d03f5158453bce43281d9d03ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S5KQ8kth206cskAanXJKPw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">激活功能，图片由<a class="ae kv" href="https://medium.com/@nicolotognoni" rel="noopener">作者</a></figcaption></figure><p id="f046" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">FFNNs中的<strong class="lf ir">输出层</strong>由我们想要预测的类别一样多的神经元组成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/65d2d454a3aa59c86567a7391ae83721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8PVEvDU2Cjw59pd1aGcZnw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">输出图层，图像由<a class="ae kv" href="https://medium.com/@nicolotognoni" rel="noopener">作者</a></figcaption></figure><p id="899f" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果我们想要预测手写数字，我们将有10个类别，每个类别对应一个数字(0，1，2，…，9)，输出层将表示系统认为一个图像对应于给定类别的程度，即概率。</p><h1 id="12ac" class="ms mt iq bd mu mv oo mx my mz op nb nc jw oq jx ne jz or ka ng kc os kd ni nj bi translated">3.为什么要分层？</h1><p id="002c" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">为什么我们在神经网络中使用许多层？</p><p id="f9e7" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">某一层中每个神经元的<strong class="lf ir">激活</strong>对下一层中每个神经元的激活有影响，以此类推。<br/>一些连接可能比其他连接更强，这表明一些神经元在网络中比其他神经元具有更强的<strong class="lf ir"/><strong class="lf ir">影响力</strong>。这是反向传播的基础，我们稍后会看到。</p><p id="0f47" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><em class="lz">分层将问题分解成小块。</em></p><p id="2c22" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果我们要识别一个手写数字，问题将是<strong class="lf ir">把</strong>分成<strong class="lf ir">更小的块</strong>。</p><ul class=""><li id="5098" class="nk nl iq lf b lg lh lj lk lm of lq og lu oh ly oi ns nt nu bi translated">为了识别8，我们首先要识别8的组成部分，例如，它由两个圆组成，一个在另一个的上面。<br/>但接下来的问题是:你首先如何识别一个圆？</li><li id="b0b1" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">要做到这一点，你可以，例如，说一个圆是由四个小边组成的。即使这样，我们也可以继续问自己，你是如何识别优势的？诸如此类…</li></ul><p id="5c8b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们可以认为<strong class="lf ir">神经网络的每一层</strong>都解决了这些问题中的每一个:</p><ul class=""><li id="e73e" class="nk nl iq lf b lg lh lj lk lm of lq og lu oh ly oi ns nt nu bi translated">第一层识别小边缘，并将该信息传递给第二层。</li><li id="70ff" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">第二个，给定这个信息，通过将小边加在一起识别两个圆。</li><li id="8b64" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">然后，它将此信息传递给第三层，第三层识别两个圆圈，一个在另一个之上。</li><li id="911f" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">我们最终到达输出层，基于它从网络接收的信息，给出输入是8的高概率。</li></ul><p id="9ab8" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这是一个过于简化的过程，但它让你很好地理解一个真正的神经网络是如何工作的。</p><p id="bd32" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">神经网络的分层结构允许它们将困难的问题分解成更小更容易的问题。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="9444" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">4.价值函数</h1><p id="4078" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">但是我们如何训练一个神经网络呢？</p><p id="b6d0" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了了解它是如何工作的，我们首先需要了解如何减少线性回归中的误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b7ad12f4be35afd92a4412d4dd919691.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*LjHsKniRQyfp-iP_kO3IUg.png"/></div></figure><p id="bd8d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在线性回归中，为了优化函数并减少误差，我们使用<em class="lz">误差平方和</em>，即预测值和样本平均值之间的差值。</p><p id="99fe" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这与<strong class="lf ir">损失函数</strong>或<strong class="lf ir">成本函数</strong>的原理相同，后者的目标是评估我们的算法表现如何以及我们的模型好坏。</p><ul class=""><li id="2cff" class="nk nl iq lf b lg lh lj lk lm of lq og lu oh ly oi ns nt nu bi translated"><strong class="lf ir">损失函数</strong>计算单个训练样本的误差。</li><li id="e260" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated"><strong class="lf ir">成本函数</strong>计算所有示例的损失函数的平均值。</li></ul><p id="3423" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">目标是减少实际值和预测值之间的误差，从而最小化成本函数。</p><p id="5ff3" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">有许多损失函数可供选择。最常见的有:</p><ul class=""><li id="878a" class="nk nl iq lf b lg lh lj lk lm of lq og lu oh ly oi ns nt nu bi translated">二元交叉熵</li><li id="870d" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">范畴交叉熵</li><li id="9f7a" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">均方误差</li><li id="9047" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">对数损失</li></ul></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h2 id="61d9" class="ou mt iq bd mu ov ow dn my ox oy dp nc lm oz pa ne lq pb pc ng lu pd pe ni pf bi translated">5.梯度下降</h2><p id="5598" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">现在我们知道我们的模型有多坏或多好，但这本身并不是有用的信息。我们如何改进我们的模型？</p><p id="9157" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了做到这一点，我们希望采用我们的成本函数，并找到一个输入，使<strong class="lf ir">最小化函数的输出</strong>，即误差最小的点。我们希望找到使成本尽可能小的<strong class="lf ir">权重</strong> <strong class="lf ir">和</strong> <strong class="lf ir">偏差</strong>。</p><p id="f781" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为此，在非线性函数的情况下，我们必须使用<strong class="lf ir"> <em class="lz">梯度下降</em> </strong>，这是优化神经网络的最常见方法。它被用来寻找一个函数的最小值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/07e585e86f1bdc80d9157f06c9446cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*es0-tayrVWIn0K7W2reSmw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降f(x，y) = x * exp( -(x + y))，图片由<a class="ae kv" href="https://medium.com/@nicolotognoni" rel="noopener">作者</a></figcaption></figure><p id="6951" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">梯度下降基于凸函数，表现如下:</p><ul class=""><li id="4ed5" class="nk nl iq lf b lg lh lj lk lm of lq og lu oh ly oi ns nt nu bi translated"><strong class="lf ir">起点</strong>为随机点</li><li id="8913" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">从这个起点开始，我们找到<strong class="lf ir">导数</strong>，曲线在给定点的斜率，以了解函数增加或减少的陡峭程度。我们预计，随着时间的推移，当我们接近一个最低点时，斜率会降低。</li><li id="9b4b" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated"><strong class="lf ir">梯度</strong>告诉我们函数在哪个方向增加。因为我们想要达到一个最小值，所以我们想要沿着函数递减的方向或者梯度的相反方向前进。</li><li id="9afb" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated">为了计算下一个点的<strong class="lf ir">，我们使用当前位置的梯度，然后乘以<strong class="lf ir">学习率</strong>，最后从当前位置减去得到的值。</strong></li></ul><p id="baf4" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这个过程可以写成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/eaded6c66def5de3cd6092aa5b085d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tMwyE8skYHYm9fBMK5jQsw.png"/></div></div></figure><p id="a784" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir"> a n+1 </strong> <em class="lz"> =是我们攀登者的下一个位置<br/> </em> <strong class="lf ir"> a n </strong> <em class="lz"> =代表他当前的位置<br/> </em> <strong class="lf ir">减(-) </strong> <em class="lz"> =表示梯度下降的最小化部分<br/></em><strong class="lf ir">【ɣ</strong><em class="lz">=是学习率<br/></em><strong class="lf ir">∇f(a n)</strong><em class="lz">=梯度项表示的方向</em></p><p id="0b8d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这个过程一直持续到在函数中找到最小值。</p><p id="4e7d" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">有三种类型的梯度下降算法:</p><ul class=""><li id="8b5d" class="nk nl iq lf b lg lh lj lk lm of lq og lu oh ly oi ns nt nu bi translated"><strong class="lf ir">批量梯度下降:</strong>它对训练集中每个点的误差进行求和，但只有在评估了所有训练样本后，模型才会更新。这个过程被称为训练时期。<br/>批量梯度下降需要将所有数据存储在内存中，这可能会导致大型数据集的处理时间较长。此外，虽然它产生稳定的误差梯度和收敛性，但它可能陷入局部最小值而找不到全局最小值。</li><li id="3d8b" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated"><strong class="lf ir">随机梯度下降:</strong>这种类型的梯度为每个示例运行一个训练时期。然后，它一次更新一个示例的参数。<br/>这种梯度下降的优点是每次只需要存储一个训练样本，而不是所有的数据，所以不需要很多内存，过程可能会更快。<br/>然而，与前者相比，它可能导致计算效率的损失，并且连续更新可能导致有噪声的梯度。</li><li id="765b" class="nk nl iq lf b lg nv lj nw lm nx lq ny lu nz ly oi ns nt nu bi translated"><strong class="lf ir">小批量梯度下降:</strong>它结合了前两个概念。它将带有训练示例的数据集划分为不同的样本批量。然后，它对这些批处理中的每一个执行更新。</li></ul><p id="f337" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir"> <em class="lz">推荐阅读:<br/> </em> </strong> <em class="lz">如果你想了解更多关于梯度下降及其背后的数学，我强烈建议你阅读这篇文章:</em></p><div class="ma mb gp gr mc md"><a href="https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21" rel="noopener follow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">梯度下降算法——深度探索</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">梯度下降法为机器学习和深度学习技术奠定了基础。</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="pi l mo mp mq mm mr kp md"/></div></div></a></div></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="1c26" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">6.反向传播</h1><p id="ad23" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated"><strong class="lf ir">反向传播</strong>是神经网络中最基本的构建模块，于20世纪60年代首次推出。</p><p id="77f9" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">反向传播试图通过基于用梯度下降计算的<strong class="lf ir">梯度</strong>调整网络的<strong class="lf ir">权重和偏差</strong>来<strong class="lf ir">最小化</strong>成本函数。<br/>告诉我们<strong class="lf ir">成本函数对相应的权重和偏差有多敏感的是梯度的每个分量的<strong class="lf ir">大小</strong>。</strong></p><p id="4cc0" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">开始时，网络还没有被训练，所以输出是随机的。为了做得更好，我们需要改变最后一层的激活来做更好的预测。<br/>然而，我们不能直接改变激活，我们只能<strong class="lf ir">改变权重和偏差</strong>，所以我们必须调整它们以提高输出。<br/>有助于了解我们希望对激活进行哪些调整，以做出更好的预测。</p><p id="702c" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">下面举一个<strong class="lf ir">的例子</strong>把事情说清楚。<br/>如果我们想要识别手写数字(像MNIST数据库)，我们想要看到，对于一个确定的例子(像数字8)，我们想要改变哪个<strong class="lf ir">激活</strong>以有效地将数字识别为8。换句话说，当神经网络看到8时，我们希望模型输出层中为8的概率最高。</p><p id="149c" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">输出层(由10个神经元组成，每个类别一个:0，1，2，…，9)中的一些值必须增加，而其他值必须减少。<br/>更重要的是每个神经元这些变化的<strong class="lf ir">幅度</strong>——每个神经元的输出(预测值)离它应该有的实际值有多远。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/a9bd76993cee56571793b7015ee1f86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GkLzSerCD3BqH3TOJLYaDA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">输出图层中预测值和实际值之间的差值，图片作者<a class="ae kv" href="https://medium.com/@nicolotognoni" rel="noopener">作者</a></figcaption></figure><p id="286b" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们记住，每个神经元的激活值是前几层所有激活的加权和，加上一个偏差。</p><p id="80d1" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果我们想增加激活，我们可以增加权重，改变前一层的激活，或者增加偏差，但我们只能改变权重和偏差。</p><p id="51f5" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir"> <em class="lz">推荐阅读:<br/> </em> </strong> <em class="lz">你现在应该对反向传播有了坚实的基础，但是如果你想进一步探索这个话题，我鼓励你阅读这篇文章:</em></p><div class="ma mb gp gr mc md"><a href="https://www.3blue1brown.com/lessons/backpropagation-calculus" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">3Blue1Brown —反向传播演算</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">这里的硬性假设是，您已经阅读了前一部分，给出了反向传播的直观演示…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">www.3blue1brown.com</p></div></div><div class="mm l"><div class="pk l mo mp mq mm mr kp md"/></div></div></a></div></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="93fe" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">7.构建前馈神经网络</h1><p id="53f6" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">现在让我们把迄今为止所学到的东西付诸实践。</p><p id="4de2" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在本教程中，我们将使用TensorFlow创建一个简单的FFNN，其中包含一个输入层、一个隐藏层和一个输出层。</p><p id="0299" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们将使用手写数字的<strong class="lf ir"> MNIST </strong>每个示例是与来自10个类别的标签相关联的28x28灰度图像。</p><h2 id="75c2" class="ou mt iq bd mu ov ow dn my ox oy dp nc lm oz pa ne lq pb pc ng lu pd pe ni pf bi translated">加载数据</h2><p id="484d" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">第一步是<strong class="lf ir">加载</strong>数据。<br/>由于MNIST是最流行的图像分类数据集之一，它已经在Keras (TensorFlow)中，要加载它，我们可以直接使用Keras API。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pl pm l"/></div></figure><h2 id="4fc7" class="ou mt iq bd mu ov ow dn my ox oy dp nc lm oz pa ne lq pb pc ng lu pd pe ni pf bi translated">模型</h2><p id="7dda" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">下一步是<strong class="lf ir">创造</strong>T21T23】型号 FFNN。</p><p id="9f5e" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们将创建一个简单的FFNN，由一个输入层、一个隐藏层和一个输出层组成。</p><p id="af94" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">首先要做的是创建一个大小为28x28 (784)的<strong class="lf ir"> Flatten </strong>图层，它对应于数据集中图像的宽度乘以高度。<br/>这样做是为了在模型中表示图像的每个像素。</p><p id="6f78" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">隐藏的<strong class="lf ir">层是由1000个神经元组成的<strong class="lf ir">密集的</strong>层。</strong></p><p id="3c1f" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最后，<strong class="lf ir">输出</strong> <strong class="lf ir">层</strong>是另一个<strong class="lf ir">密集</strong>层，它需要与我们想要预测的数据集中的类别数大小相同，在MNIST数据集中是10，每个数字(0，1，2，…，9)一个</p><p id="df28" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了创建模型，我们使用Keras Sequential API。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pl pm l"/></div></figure><h2 id="146f" class="ou mt iq bd mu ov ow dn my ox oy dp nc lm oz pa ne lq pb pc ng lu pd pe ni pf bi translated">培养</h2><p id="86d5" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">最后一步是<strong class="lf ir">编译</strong>模型。<br/>要做到这一点，我们需要选择一个优化器、一个损失函数和我们希望在训练期间显示的指标，以了解模型的行为。</p><p id="53e5" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">作为<strong class="lf ir">优化器</strong>，我们将使用代表随机梯度下降的“SGD”。</p><p id="58fd" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们将为<strong class="lf ir">损失函数</strong>使用稀疏分类交叉熵。</p><p id="9e4c" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了了解我们的模型表现如何，我们将在训练集和测试集上使用模型的<strong class="lf ir">精确度</strong>。</p><p id="f96e" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最后一步是使用<strong class="lf ir">。拟合</strong>方法开始训练模型。<br/>在这一步中，您必须选择<strong class="lf ir">训练集</strong>和<strong class="lf ir">验证集</strong> <strong class="lf ir">以及您希望模型运行的<strong class="lf ir">时期</strong>的数量。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pl pm l"/></div></figure><h2 id="5965" class="ou mt iq bd mu ov ow dn my ox oy dp nc lm oz pa ne lq pb pc ng lu pd pe ni pf bi translated">估价</h2><p id="c934" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">从训练中，我们得到以下输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pn"><img src="../Images/4cc93dda913f35313ddf858acd9f263e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhrWHKnf9j5ELmCwJN1AnA.png"/></div></div></figure><p id="fb26" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们可以看到，在训练集和验证集上的<strong class="lf ir">准确率</strong>都非常高，大约在99%和98%左右。</p><p id="f2c7" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">即使模型只有三层，运行10个时期，生成的预测也是非常准确的。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="8bfe" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">8.结论</h1><p id="5be2" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">在本文中，我们从基础开始:我们学习了什么是<strong class="lf ir">神经网络</strong>。我们了解了什么是<strong class="lf ir">感知器</strong>及其组件。</p><p id="ca11" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">然后我们转向<strong class="lf ir">前馈神经网络</strong>及其激活函数。然后我们深入到<strong class="lf ir">梯度下降</strong>和<strong class="lf ir">反向传播</strong>。</p><p id="df68" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在文章的最后一部分，我们<strong class="lf ir">使用python从零开始构建了一个FFNN</strong>，并且我们使用MNIST数据集对它进行了训练。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="e422" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir">如果你觉得这篇文章很有帮助，请查看我在Medium </strong>  <strong class="lf ir">和</strong> <a class="ae kv" href="https://www.linkedin.com/in/nicolotognoni/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf ir">上的简介，在LinkedIn </strong> </a> <strong class="lf ir">上与我联系！</strong></p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="6304" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">9.参考</h1><p id="0f54" class="pw-post-body-paragraph ld le iq lf b lg nm jr li lj nn ju ll lm oa lo lp lq ob ls lt lu oc lw lx ly ij bi translated">[1] <a class="ae kv" href="https://www.amazon.com/Ian-Goodfellow/e/B01MQGN8N0/ref=dp_byline_cont_book_1" rel="noopener ugc nofollow" target="_blank">伊恩·古德菲勒</a>、<a class="ae kv" href="https://www.amazon.com/Yoshua-Bengio/e/B00IWC47MU/ref=dp_byline_cont_book_2" rel="noopener ugc nofollow" target="_blank">约舒阿·本吉奥</a>和<a class="ae kv" href="https://www.amazon.com/Aaron-Courville/e/B01N8XGWRL/ref=dp_byline_cont_book_3" rel="noopener ugc nofollow" target="_blank">亚伦·库维尔</a>、<a class="ae kv" href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?crid=1I2VKDBGNWO3G&amp;keywords=Deep+Learning+%28Adaptive+Computation+and+Machine+Learning+series%29&amp;qid=1662811685&amp;s=books&amp;sprefix=deep+learning+adaptive+computation+and+machine+learning+series+%2Cstripbooks-intl-ship%2C323&amp;sr=1-1" rel="noopener ugc nofollow" target="_blank">深度学习</a> (2016)，麻省理工学院出版社</p><p id="6f86" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">[2] Parul Pandey，<a class="ae kv" href="https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e" rel="noopener" target="_blank">理解梯度下降背后的数学原理</a>。(2019)，中</p><p id="fb27" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">【3】IBM云教育，<a class="ae kv" href="https://www.ibm.com/cloud/learn/gradient-descent#:~:text=Gradient%20descent%20is%20an%20optimization,each%20iteration%20of%20parameter%20updates." rel="noopener ugc nofollow" target="_blank">什么是梯度下降？</a> (2020)，IBM</p><p id="36e9" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">[4]格兰特·桑德森，<a class="ae kv" href="https://www.3blue1brown.com/lessons/backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播到底在做什么？</a> (2017)，3蓝1棕</p></div></div>    
</body>
</html>