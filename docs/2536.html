<html>
<head>
<title>Knowledge Distillation, a Methodology for Compressing Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">知识蒸馏，一种压缩神经网络的方法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/natural-language-processing-abcd6e13ed40?source=collection_archive---------2-----------------------#2022-02-02">https://pub.towardsai.net/natural-language-processing-abcd6e13ed40?source=collection_archive---------2-----------------------#2022-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a5a7" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="e38a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">师生架构为嵌入式系统创建一个更小的模型</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a639fe52af9e50d9ec05c64f1be7c85c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wS2PeN5hceygejoZhcYCzg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@sharonmccutcheon?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">莎伦·麦卡琴</a>在<a class="ae lh" href="https://unsplash.com/s/photos/miniature?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><p id="fd7e" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">大型神经网络非常复杂，并且很有可能执行更复杂的任务。但不幸的是，验证这样的大型网络并不容易，因此无法保证我们使用了神经网络的全部容量。相比之下，较小的神经网络更容易验证和部署。</p><p id="cca3" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lr jd">知识提炼是将知识从一个大型神经网络或一组网络转移到一个较小的神经网络的过程，其精度损失可接受。</strong>这个过程有助于压缩大型深度学习模型，有时也有可能提高准确性。<strong class="lr jd">知识提炼是一种压缩过程，如量化或模型修剪，以获得部署在边缘设备或智能手机等资源受限设备上的较小模型。</strong></p><h1 id="4375" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">师生模型</h1><p id="238a" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">大模型具有教师模型的作用，小模型是可以从大模型中学习的学生模型。目标是让学生模型达到相同或更好的精度，尽管它是一个更小的模型。学生模型从教师模型中学习知识。<strong class="lr jd">教师模型的知识可分为三类:基于反应的、基于特征的和基于关系的知识。</strong></p><h1 id="70b4" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">准确度分数</h1><p id="352a" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">根据这篇<a class="ae lh" href="https://arxiv.org/pdf/2007.09029.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，一些知识提炼方法在学生模型中提供了与教师模型中相同的准确性评级，将模型大小减少了大约80%。<strong class="lr jd">甚至有一些提炼方法可以在学生模型中获得比教师模型更好的准确率。</strong></p><h1 id="b078" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">基于响应的蒸馏</h1><p id="048e" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">教师模型输出层的知识是基于响应的提炼的核心。<strong class="lr jd">学生模型模仿教师模型输出层的预测。</strong>教师模型和学生模型之间的差异称为蒸馏损失，应在培训过程中将其降至最低。</p><h1 id="a0d9" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">基于特征的蒸馏</h1><p id="6c94" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">学生模型可以从教师模型的深层神经网络的中间层的知识中学习。特性在中间层中是不同的，学生模型使用特性属性来提供与教师模型相同的特性。在基于特征的蒸馏中，蒸馏损失可通过两个模型中应被最小化的特征激活的差异来定义。这种方法对于从深度网络学习薄网络是合理的。</p><h1 id="21dc" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">基于关系的蒸馏</h1><p id="b5e9" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated"><strong class="lr jd">这种方法使用所有层、中间层和输入或输出层之间的关系中的信息。</strong>与使用中间层或输出层知识的其他两种基于响应和基于特征的方法相比，在这种方法中，蒸馏损失由教师和学生网络中特征图的关系决定。</p><h1 id="2831" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">知识蒸馏的应用</h1><h2 id="afc2" class="ni mm it bd mn nj nk dn mr nl nm dp mv ly nn no mx mc np nq mz mg nr ns nb iz bi translated">计算机视觉</h2><p id="b2ac" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">通过知识提炼的模型压缩可以应用于计算机视觉，因为这种方法像在计算机视觉架构中一样利用深度神经网络。图像、视频、对象分类或识别等应用是知识提炼最常用的应用。<strong class="lr jd">在人脸识别应用中，可以在大容量教师模型的指导下，将特征图转移到学生模型中，以创建更小的学生模型。</strong></p><p id="45ff" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">作为自动驾驶的现实应用，将行人作为动态对象(有时可能是静态对象(站立的行人))具有挑战性，并且与安全相关。这篇<a class="ae lh" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803079&amp;casa_token=t2QkLTU_gLUAAAAA:AV8DB5rPmAP3IdRE7zVXPBdLN5YEjN7XpeO8wEQEl3W-RIEwL9TUMmTmAlP3_t0BVtR7aZRyvjL4&amp;tag=1" rel="noopener ugc nofollow" target="_blank">论文</a>提供了一个解决方案，使用高精度的知识提炼来构建一个快速的、占用空间小的模型。</p><h2 id="adc2" class="ni mm it bd mn nj nk dn mr nl nm dp mv ly nn no mx mc np nq mz mg nr ns nb iz bi translated">自然语言处理</h2><p id="b721" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">诸如BERT(来自Transformer的双向编码器表示)之类的语言模型很复杂，并且需要大量的计算资源。BERT由Google于2006年开发并发布。知识提炼可以减少内存和计算能力的需求，以适应资源受限设备的模型。这种方法有助于将深度学习模型从云转移到智能手机或其他边缘设备。<a class="ae lh" href="https://huggingface.co/docs/transformers/model_doc/distilbert" rel="noopener ugc nofollow" target="_blank"><strong class="lr jd">distil BERT</strong></a><strong class="lr jd">是BERT语言模型的压缩版本，准确率超过原始BERT模型的95%。</strong></p><h2 id="253a" class="ni mm it bd mn nj nk dn mr nl nm dp mv ly nn no mx mc np nq mz mg nr ns nb iz bi translated">语音识别</h2><p id="4071" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">深度神经网络用于语音识别应用程序(如Alexa或Siri)中的声学建模。知识提炼有助于创建轻量级声学模型。语音中的时间信息需要额外的考虑，例如RNN(递归神经网络)算法将实时行为从教师模式转移到学生模式。同样的概念可以用于智能手机神经网络中的音频传感，以检测人类行为。根据这篇<a class="ae lh" href="https://dl.acm.org/doi/abs/10.1145/2750858.2804262?casa_token=PXk1bbFEWY8AAAAA:wXSPbyB0B5zQ0uFgV7AEryapfSrCVAPY6N9lxRgK0q3PUreHr3Cce76ro6XiXTZoQQFk0MR7xeBKhoUn" rel="noopener ugc nofollow" target="_blank">论文</a>，人类的情感，如悲伤、恐惧、压力或愤怒可以在一个小设备上被识别。</p><h2 id="6b17" class="ni mm it bd mn nj nk dn mr nl nm dp mv ly nn no mx mc np nq mz mg nr ns nb iz bi translated">医疗保健领域的移动诊断</h2><p id="f20b" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated"><strong class="lr jd">通过深度神经网络的知识提炼，可以降低诊断皮肤病的成本。</strong>根据这篇<a class="ae lh" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335582" rel="noopener ugc nofollow" target="_blank">论文</a>，一个健壮的移动深度学习模型经过训练，通过智能手机拍照并应用知识提取方法，以超过93%的准确率区分特定的皮肤病。</p></div></div>    
</body>
</html>