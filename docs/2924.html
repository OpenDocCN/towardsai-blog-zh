<html>
<head>
<title>Apache Hive Hacks for a Data Scientist: Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学家的Apache Hive Hacks:第一部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/apache-hive-hacks-for-a-data-scientist-part-i-e406ad11b937?source=collection_archive---------0-----------------------#2022-07-10">https://pub.towardsai.net/apache-hive-hacks-for-a-data-scientist-part-i-e406ad11b937?source=collection_archive---------0-----------------------#2022-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="8522" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于所有的数据科学家来说，在某个时间点，我们都遇到过需要在模型开发管道中处理大规模数据的情况。如果你在大型组织的机器学习/人工智能团队中工作，在某些时候，你几乎不可避免地需要使用Hive。Hive是一个基于SQL的大数据引擎，构建在Hadoop(大数据框架)之上，使Hadoop用户可以无缝地对大规模数据运行SQL查询。它使SQL用户能够使用Hadoop来查询大数据，而无需编写Hadoop Map-Reduce代码。</p><h2 id="acf1" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">为什么要为一个数据科学家选择蜂巢？</h2><p id="163a" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">现在你可能会说，作为一名数据科学家，我可以在没有Hive的情况下生存多年，为什么是现在？并非每个分析/模型都需要针对大数据进行构建/扩展。虽然，这在许多情况下可能是正确的，但是当您与组织中的各种业务/产品团队合作时，您将很快意识到您去年构建的模型需要根据新数据进行重新校准，因为数据量比您组织去年看到的数据量增加了10倍。或者您的特征工程代码现在需要在大规模数据上运行，以生成您为微小模型开发的相同的旧特征。单节点python代码无法处理这样的场景，您需要向上扩展。这是您在数据科学职业生涯中需要大数据工具的地方。</p><p id="af80" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，有人可能会说，我们对数据进行采样，创建一个统计上相关的总体样本，并将该样本用于数据科学管道中的任务。但在这里，您还会发现必须对整个数据集进行一些分析。例如，您需要知道在过去的6个月中有多少人去了同一家餐厅。您不能依赖此处的样本，因为您可能会排除对进一步分析该餐厅是否应包含在您的最终模型中很重要的一组人。此外，如果你是一名机器学习工程师，你可能会参与模型开发/生产步骤，如特征工程或特征聚合等，这些步骤有时依赖于大数据工具，如hive、spark等来执行此类任务。</p><p id="3660" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你仍然不相信，就把它当成你帽子里一根别致的新羽毛吧。不用说，这肯定会让你的简历在所有处理万亿数据的公司面前大放异彩。等等，现在不是每个大公司都这样吗？在当今的物联网时代，组织从各种来源获取数据。随着时间的推移，这些数据只会越来越多，使得在单节点python代码上执行机器学习任务变得更加困难。这使得大数据工具与当今机器学习的集成几乎不可避免。</p><p id="9913" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">鉴于上面关于大数据工具对数据科学家的重要性的讨论，今天我想介绍一些重要的技巧，可以帮助您成为初级/中级/专家Hive用户。所以让我们开始吧。</p><ol class=""><li id="762a" class="lm ln it js b jt ju jx jy kb lo kf lp kj lq kn lr ls lt lu bi translated"><strong class="js iu">数据分割</strong></li></ol><p id="534a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我不能强调这一步骤的好处。我强烈建议从开始就对数据进行分区。例如，如果您正在获取消费者活动的每日数据，请尝试将当天的所有数据合并到如下所示的目录下:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="d4a7" class="ko kp it ma b gy me mf l mg mh">So, essentially, you should organize your data as below directories:<br/>/activity/consumer/20220707/activity.txt<br/>/activity/consumer/20220708/activity.txt<br/>/activity/consumer/20220709/activity.txt</span></pre><p id="f1e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，当需要在Hive中运行分析时，在Hive表中添加一个那天的分区。例如，您可以使用下面的语句将上述数据作为一个分区插入到您的hive表(比如cons_act)中:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="2b59" class="ko kp it ma b gy me mf l mg mh">&gt; alter table cons_act add partition(date='20220709') location '/activity/consumer/20220709/';</span></pre><p id="adf4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您将每天执行这一步，将当天的数据加载到Hive中。随着时间的推移，这一步将为您节省性能问题。当您对数据进行分区后，现在您可以在查询中将日期范围指定为where子句，Hive引擎将只加载特定的日期，而不是数据的整个数据范围，从而减少查询流形的运行时间。</p><p id="fbb1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是如何做到这一点的一些例子:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="83d7" class="ko kp it ma b gy me mf l mg mh">&gt; select * from cons_act where date='20220709'; <br/>&gt; select * from const_act where date &gt;='20220707' and date &lt;='20220709';<br/>&gt; select * from cons_act where date like '202207%'; <br/>&gt; select * from cons_act where date like '2022%'; </span></pre><p id="804b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上述查询显示了如何访问一天、几天、一个月或一整年，hive将使用我们在上面执行的alter table命令自动选择与上述日期链接的底层数据目录。</p><p id="04ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 2。指定映射器和缩减器内存</strong></p><p id="ecbf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">配置单元作业作为一组映射器和缩减器任务运行。点击查看更多关于这个<a class="ae mi" href="https://www.geeksforgeeks.org/architecture-and-working-of-hive/" rel="noopener ugc nofollow" target="_blank">的信息。默认情况下，每个任务都有一个固定的映射器和缩减器内存。当您在大数据上运行配置单元作业时，有时您可能会看到作业在映射器阶段/缩减器阶段因映射器/缩减器内存超出限制的错误而一次又一次被终止。如果您检查日志，您会在您的作业被终止之前看到mapper/reducer任务中的此类错误。</a></p><p id="a78a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这种情况下，以下配置单元设置可以帮助您增加作业的映射器/缩减器内存:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="2a3d" class="ko kp it ma b gy me mf l mg mh">&gt; set mapreduce.map.memory.mb=8000;<br/>&gt; set mapreduce.reduce.memory.mb=10000;</span></pre><p id="b90d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以上设置位于单个映射器/缩减器任务级别。在这里，我将每个映射器任务的映射器内存更改为8 GB，将缩减器的内存更改为10 GB。它受到Hadoop集群管理员配置的最大容器内存的限制。如果您设置的内存值超过此限制，您的作业将不会启动，因为Hadoop资源管理引擎不允许这样做。</p><p id="8dbd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3。处理数据的偏斜度</strong></p><p id="617a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据偏斜意味着数据中的某些列或键比其余数据具有更多的值。在大多数数据集中，偏斜是非常常见的现象。例如，如果你有来自游戏平台的数据，特定年龄段的用户会在平台上更加活跃。或者，当您查看游戏用户的活动时间时，他们可能在特定时间更活跃。</p><p id="53ea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当您尝试对这些字段运行一些分析或围绕这些字段创建要素时，您的作业将开始出现“偏斜”，这将导致所有正常工作的查询开始失败或速度变慢。这种缓慢发生在您工作的最后几个reducer中(reducer减少数据，即执行聚合步骤，例如，通过将每小时活动表与用户表连接来聚合一天中几个小时的用户活动)。</p><p id="2a87" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这里，您可以使用below hive设置，让hive引擎隐式处理join中的这种偏斜:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="f8c3" class="ko kp it ma b gy me mf l mg mh">&gt; set hive.optimize.skewjoin=true;</span></pre><p id="c9f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过以上设置，您将看到这种带有数据偏斜的连接查询再次开始进行并最终成功。另一个相关设置允许您定义倾斜键的行数:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="ae75" class="ko kp it ma b gy me mf l mg mh">&gt; set hive.skewjoin.key=60000;</span></pre><p id="1360" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这意味着，如果查询中的关键字(例如上面示例中的小时)的行数超过60000，那么hive会将其视为该连接操作中的倾斜关键字，并以特殊方式处理它，以减少因处理具有数据倾斜的关键字而导致的作业延迟。</p><p id="e523" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 4。地图侧连接</strong></p><p id="93e9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您需要在任何规范化数据库中运行的最常见操作是表连接。Hive为涉及连接的查询提供了一组广泛的优化。通常，您会有多个表中的数据，您可能需要用一个连接键连接这些数据，以便最终从两个不同的表中选择所需的列。这就是Hive“地图连接”的由来。Map join是一种操作，它允许我们将内存中两个连接中的一个小表作为一个Map，从而总体上加快查询的执行。以下语句说明了如何将较小的表指定为地图连接:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="7437" class="ko kp it ma b gy me mf l mg mh">&gt; set hive.auto.convert.join=true;<br/>&gt; select user_details.user_name, user_details.user_age, hourly_act.num_of_games, hourly_act.active_min from user_details join hourly_act on user_details.userid = hourly_act.user_id;</span></pre><p id="73d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在上面的查询中，我们的“user_details”表很小，包含id、姓名和年龄等用户详细信息，但“hourly activity”表将包含一天中所有时间所有用户的条目。通过在运行此设置之前添加hive auto convert join设置，我们指示hive将“user_details”表带入内存，这将加快我们的查询速度。</p><p id="f0b3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与上述设置相关的另一个参数可用于指定小表格的大小:</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="d4ae" class="ko kp it ma b gy me mf l mg mh">&gt; set hive.smalltable.filesize=40000000;</span></pre><p id="4f16" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我的小表大小超过默认限制时，我发现这个设置非常有用，因此，给了我在内存中放置一个“稍微大一点”的小表的灵活性。在上面的命令中，我知道我的user_details的表大小是40 MB。因此，我将hive.smalltable.filesize的值设置为40 MB，这将覆盖默认设置25 MB到40MB。</p><p id="a49a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 5。任务超时</strong></p><p id="1ebc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">处理大数据时的另一个重要设置是映射器/缩减器任务级别的超时。当使用where和group by子句运行查询时，此设置尤其重要。这些类型的查询与数据偏斜问题相结合，有时会导致一些映射器/缩减器任务长时间运行。这进一步导致正在运行的任务跳过心跳消息返回到应用程序主节点(监督整个hive作业的任务状态的主节点)，主节点期望每10分钟一次(默认值:600000)。在某些情况下，我认为作业没有问题，但任务长时间运行，作业突然被终止，我发现这就是问题所在。在下面的命令中，我可以通过将这个值设置为更大的值来解决这个问题，比如3600000毫秒，即30分钟。</p><pre class="lv lw lx ly gt lz ma mb mc aw md bi"><span id="bdb9" class="ko kp it ma b gy me mf l mg mh">&gt; set mapred.task.timeout=3600000;</span></pre><p id="3ea7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这确保了我的hive作业中的每个任务在必须向应用程序主机发送心跳消息之前可以运行30分钟。这可以确保作业不会被终止，而不是每隔10分钟就有一条消息到达应用程序主机，而是在终止作业之前等待30分钟的超时时间。</p><p id="c90c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这让我们结束了这篇关于Hive优化技巧的博客。希望你可以在你的Hive工作中使用这些，并帮助调试你在建模过程中面临的某些问题，而不需要大数据工程师的帮助。另外，从查询开始就考虑这些问题，有时会省去很多麻烦，并有助于轻松扩展这些查询。</p><p id="7eb8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你喜欢这个博客，并想了解更多关于数据科学和大数据的有趣话题，请加入<a class="ae mi" href="https://medium.com/@tangri.anurag/membership" rel="noopener">medium.com</a>。这让你可以全面了解更多关于medium.com的此类故事</p></div></div>    
</body>
</html>