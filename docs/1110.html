<html>
<head>
<title>LIME - Explaining Any Machine Learning Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释任何机器学习预测</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/lime-explaining-any-machine-learning-prediction-d663c457a740?source=collection_archive---------0-----------------------#2020-11-03">https://pub.towardsai.net/lime-explaining-any-machine-learning-prediction-d663c457a740?source=collection_archive---------0-----------------------#2020-11-03</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="6859" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/data-mining" rel="noopener ugc nofollow" target="_blank">数据科学</a></h2><div class=""/><div class=""><h2 id="08bf" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">用石灰向可解释的人工智能迈出第一步</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/e621375f5734a55de059737afe7edb42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lMAol7RL6fTaVy1d"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">来源:<a class="ae li" href="https://unsplash.com/photos/X9nMwOdYTR4" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure></div><div class="ab cl lj lk hy ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="in io ip iq ir"><h1 id="15c9" class="lq lr iu bd ls lt lu lv lw lx ly lz ma kj mb kk mc km md kn me kp mf kq mg mh bi translated">什么是石灰</h1><p id="ebc1" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">LIME包的主要目标是解释任何黑盒机器学习模型。它用于分类和回归问题。</p><blockquote class="ne"><p id="167d" class="nf ng iu bd nh ni nj nk nl nm nn nd dk translated">LIME —本地可解释的模型不可知解释</p></blockquote><h1 id="1eec" class="lq lr iu bd ls lt no lv lw lx np lz ma kj nq kk mc km nr kn me kp ns kq mg mh bi translated">为什么我们需要石灰</h1><p id="d2d6" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">让我们试着理解为什么我们需要解释机器学习预测。假设你正在为一个住房金融或银行客户工作。你的任务是建立一个机器学习模型来预测贷款违约。因此，您构建了模型并成功地在生产中实现了它。现在，相关部门正在使用该模型，他们刚刚为一个客户进行了尝试，模型预测结果为“默认”。到目前为止一切顺利。这个预测可能是100%正确的，但是你如何解释哪些特性导致这个预测是“默认的”。这就是LIME的答案，即它解释了任何机器学习模型预测。</p><h1 id="f221" class="lq lr iu bd ls lt no lv lw lx np lz ma kj nt kk mc km nu kn me kp nv kq mg mh bi translated">装置</h1><pre class="kt ku kv kw gu nw nx ny nz aw oa bi"><span id="6320" class="ob lr iu nx b gz oc od l oe of">!pip install lime</span></pre><h1 id="10bb" class="lq lr iu bd ls lt no lv lw lx np lz ma kj nt kk mc km nu kn me kp nv kq mg mh bi translated">例子</h1><p id="a4c5" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">LIME可用于文本分类(二值分类、多值分类)、回归、图像分类等。在本文中，我们将介绍如何使用LIME进行二进制文本分类。一旦你了解如何使用石灰，参考官方教程<a class="ae li" href="https://github.com/marcotcr/lime#tutorials-and-api" rel="noopener ugc nofollow" target="_blank">这里</a>的图像分类和回归任务。</p><h2 id="a267" class="ob lr iu bd ls og oh dn lw oi oj dp ma mr ok ol mc mv om on me mz oo op mg ja bi translated">二元文本分类器</h2><p id="cc31" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">我们将与<a class="ae li" href="https://www.kaggle.com/c/nlp-getting-started" rel="noopener ugc nofollow" target="_blank">合作，“真实与否？本例中的NLP与Kaggle的灾难Tweets" </a>数据集。该数据集的目标是预测哪些推文是关于真实灾难的，哪些不是。</p><p id="f344" class="pw-post-body-paragraph mi mj iu mk b ml oq ke mn mo or kh mq mr os mt mu mv ot mx my mz ou nb nc nd in bi translated">下面是相当直截了当。导入所需的库和数据集。然后，训练数据集一分为二—一个用于训练，一个用于验证。由于主要重点是解释模型预测，我们将不使用测试数据集。在应用TF-IDF和随机森林分类器后，我们得到了67.8%的f1值。</p><pre class="kt ku kv kw gu nw nx ny nz aw oa bi"><span id="328e" class="ob lr iu nx b gz oc od l oe of">import lime<br/>import os<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.metrics import f1_score<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="13ba" class="ob lr iu nx b gz ov od l oe of">train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')<br/>test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')</span><span id="11bf" class="ob lr iu nx b gz ov od l oe of">train.drop(['id','keyword','location'], axis=1, inplace=True)<br/>test.drop(['id','keyword','location'], axis=1, inplace=True)</span><span id="35c4" class="ob lr iu nx b gz ov od l oe of">from sklearn.model_selection import train_test_split<br/>X_train, X_cv, y_train, y_cv = train_test_split(train['text'], train['target'], test_size=0.25, random_state=42)</span><span id="d09b" class="ob lr iu nx b gz ov od l oe of">vectorizer = TfidfVectorizer(lowercase=False)<br/>train_vectors = vectorizer.fit_transform(X_train)<br/>valid_vectors = vectorizer.transform(X_cv)</span><span id="f3c5" class="ob lr iu nx b gz ov od l oe of">rf = RandomForestClassifier(n_estimators=250, random_state=42)<br/>rf.fit(train_vectors, y_train)</span><span id="9ab6" class="ob lr iu nx b gz ov od l oe of">pred = rf.predict(valid_vectors)<br/>print("F1-Score", f1_score(y_cv, pred))<br/>&gt;&gt; F1-Score: 0.6780162842339009</span></pre><p id="c0b7" class="pw-post-body-paragraph mi mj iu mk b ml oq ke mn mo or kh mq mr os mt mu mv ot mx my mz ou nb nc nd in bi translated">现在，让我们看看石灰的作用。首先，从LIME导入所需的模块，并使用TF-IDF和fitted随机森林分类器制作流水线。</p><pre class="kt ku kv kw gu nw nx ny nz aw oa bi"><span id="6075" class="ob lr iu nx b gz oc od l oe of">from lime import lime_text<br/>from lime.lime_text import LimeTextExplainer<br/>from sklearn.pipeline import make_pipeline</span><span id="8f68" class="ob lr iu nx b gz ov od l oe of">clf = make_pipeline(vectorizer, rf)</span></pre><p id="2518" class="pw-post-body-paragraph mi mj iu mk b ml oq ke mn mo or kh mq mr os mt mu mv ot mx my mz ou nb nc nd in bi translated">对于下面来自验证集的例子，有60%的可能性给定的推文是关于一场真正的灾难。我们也可以通过查看下面的推文来证实这一点，因为推文谈到了野火和家园的破坏。我们将使用这条推文来找出哪些特征有助于预测“真正的灾难”推文(正面类)。</p><pre class="kt ku kv kw gu nw nx ny nz aw oa bi"><span id="837d" class="ob lr iu nx b gz oc od l oe of">print(X_cv.iloc[26])<br/>&gt;&gt; '! Residents Return To Destroyed Homes As Washington Wildfire Burns on <a class="ae li" href="http://t.co/UcI8stQUg1'" rel="noopener ugc nofollow" target="_blank">http://t.co/UcI8stQUg1'</a></span><span id="4f5d" class="ob lr iu nx b gz ov od l oe of">print(clf.predict_proba([X_cv.iloc[26]]))<br/>&gt;&gt; [[0.4 0.6]]</span></pre><p id="378f" class="pw-post-body-paragraph mi mj iu mk b ml oq ke mn mo or kh mq mr os mt mu mv ot mx my mz ou nb nc nd in bi translated">接下来，通过向LimeTextExplainer传递一个类名列表来创建一个<code class="fe ow ox oy nx b">LimeTextExplainer</code>的对象<code class="fe ow ox oy nx b">explainer</code>。通过在<code class="fe ow ox oy nx b">explainer</code>上使用<code class="fe ow ox oy nx b">explain_instance</code>方法，预测的解释被生成，然后被可视化，如下所示。</p><pre class="kt ku kv kw gu nw nx ny nz aw oa bi"><span id="dfc6" class="ob lr iu nx b gz oc od l oe of">class_names = [0, 1]<br/>explainer = LimeTextExplainer(class_names=class_names, random_state=42)</span><span id="7cef" class="ob lr iu nx b gz ov od l oe of">idx = 26<br/>exp = explainer.explain_instance(X_cv.iloc[idx], clf.predict_proba, num_features=10)</span></pre><p id="55e2" class="pw-post-body-paragraph mi mj iu mk b ml oq ke mn mo or kh mq mr os mt mu mv ot mx my mz ou nb nc nd in bi translated">因为我们已经使用了<code class="fe ow ox oy nx b">num_features=10</code>，<code class="fe ow ox oy nx b">explain_instance </code>返回了前10个特性。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oz"><img src="../Images/f96b07db53145e38ac0ad27f366dde4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*q9UQvzDa8jowtfPfVwFekg.png"/></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">作者图片</figcaption></figure><p id="b125" class="pw-post-body-paragraph mi mj iu mk b ml oq ke mn mo or kh mq mr os mt mu mv ot mx my mz ou nb nc nd in bi translated">从下图中我们可以清楚的看到<code class="fe ow ox oy nx b">Wildfire</code>、<code class="fe ow ox oy nx b">Home</code>、<code class="fe ow ox oy nx b">Burns</code>等字样。有助于预测给定的推文是真正的灾难推文。这与真正的灾难推文的原始推文是一致的。有一些词是没有意义的(比如http，co等。)但是进入前10名。我们可以进一步改进模型，并在数据清理后为预测生成更好的解释。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pa"><img src="../Images/bc4562a140df481a4b4eab7c88eeea66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHIKo9mwz6aaOOgESlvKig.png"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">作者图片</figcaption></figure><h1 id="b54b" class="lq lr iu bd ls lt no lv lw lx np lz ma kj nt kk mc km nu kn me kp nv kq mg mh bi translated">结论</h1><p id="9058" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">你刚刚解释了一个使用Lime进行二进制文本分类的黑盒机器学习模型。现在，通过参考官方教程<a class="ae li" href="https://github.com/marcotcr/lime#tutorials-and-api" rel="noopener ugc nofollow" target="_blank">这里</a>一个人可以尝试图像分类，回归任务等。</p><h1 id="54e9" class="lq lr iu bd ls lt no lv lw lx np lz ma kj nt kk mc km nu kn me kp nv kq mg mh bi translated">完全码</h1><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="pb pc l"/></div></figure></div><div class="ab cl lj lk hy ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="in io ip iq ir"><h1 id="ce9a" class="lq lr iu bd ls lt lu lv lw lx ly lz ma kj mb kk mc km md kn me kp mf kq mg mh bi translated">参考</h1><div class="pd pe gq gs pf pg"><a href="https://github.com/marcotcr/lime" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab fp"><div class="pi ab pj cl cj pk"><h2 class="bd je gz z fq pl fs ft pm fv fx jd bi translated">市场营销/市场营销</h2><div class="pn l"><h3 class="bd b gz z fq pl fs ft pm fv fx dk translated">这个项目是关于解释机器学习分类器(或模型)在做什么。目前，我们支持…</h3></div><div class="po l"><p class="bd b dl z fq pl fs ft pm fv fx dk translated">github.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu lc pg"/></div></div></a></div></div><div class="ab cl lj lk hy ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="in io ip iq ir"><p id="4043" class="pw-post-body-paragraph mi mj iu mk b ml oq ke mn mo or kh mq mr os mt mu mv ot mx my mz ou nb nc nd in bi translated"><em class="pv">阅读更多关于Python和数据科学的此类有趣文章，</em> <a class="ae li" href="https://pythonsimplified.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="mk je"> <em class="pv">订阅</em> </strong> </a> <em class="pv">到我的博客</em><a class="ae li" href="http://www.pythonsimplified.com" rel="noopener ugc nofollow" target="_blank"><strong class="mk je"><em class="pv">www.pythonsimplified.com</em></strong></a><strong class="mk je"><em class="pv">。</em> </strong>你也可以通过<a class="ae li" href="https://www.linkedin.com/in/chetanambi/" rel="noopener ugc nofollow" target="_blank"> <strong class="mk je"> LinkedIn </strong> </a>联系我。</p></div></div>    
</body>
</html>