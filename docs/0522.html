<html>
<head>
<title>Mathematics of Principal Component Analysis with R Code Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用R代码实现的主成分分析数学</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/mathematics-of-principal-component-analysis-with-r-code-implementation-595a340908fa?source=collection_archive---------0-----------------------#2020-05-25">https://pub.towardsai.net/mathematics-of-principal-component-analysis-with-r-code-implementation-595a340908fa?source=collection_archive---------0-----------------------#2020-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c1972a4188301837283868208898af7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKPa5BgenJJe8Ny2kInsmg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">Benjamin O. Tayo的图片</figcaption></figure><h2 id="4854" class="jg jh ji bd b dl jj jk jl jm jn jo dk jp translated" aria-label="kicker paragraph">数据科学</h2><div class=""/><div class=""><h2 id="2a5e" class="pw-subtitle-paragraph ko jr ji bd b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf dk translated">R码实现的主成分分析的理论基础</h2></div><h1 id="bbd3" class="lg lh ji bd li lj lk ll lm ln lo lp lq kx lr ky ls la lt lb lu ld lv le lw lx bi translated">一.导言</h1><p id="310c" class="pw-post-body-paragraph ly lz ji ma b mb mc ks md me mf kv mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在机器学习中，包含特征(预测器)和离散类别标签的数据集(用于逻辑回归等分类问题)；或特征和连续结果(对于线性回归问题),用于构建预测模型，该模型可以对看不见的数据进行预测。模型的预测能力在很大程度上取决于训练数据集的质量和大小。</p><p id="730d" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">通常，数据集越大越好，但是，在数据集的大小和训练所需的计算时间之间存在权衡。事实证明，在非常大的数据集中，特征中可能存在大量冗余，或者数据集中可能存在大量不重要的特征，因此降维技术可以用于仅选择训练所需的有限数量的相关特征。</p><p id="4bc9" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma js">主成分分析(PCA) </strong>是一种用于特征提取的统计方法。PCA用于高维和高度相关的数据。PCA的基本思想是将原始的特征空间转换到主成分空间，如下图<strong class="ma js">图1 </strong>所示:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/d5257525955805a8171b4dbe4a4fe4b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*Abbtbzez3UbxdwkJ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd li">图1 </strong> : PCA算法从旧的特征空间转换到新的特征空间，去除特征相关性。Benjamin O. Tayo的图片</figcaption></figure><p id="83d0" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">PCA变换实现了以下目标:</p><p id="4355" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma js"> a) </strong>通过只关注占数据集中方差大部分的成分，减少最终模型中使用的特征数量。</p><p id="ae2d" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma js"> b) </strong>去除特征之间的相关性。</p><h1 id="71c1" class="lg lh ji bd li lj lk ll lm ln lo lp lq kx lr ky ls la lt lb lu ld lv le lw lx bi translated">二。主成分分析的数学基础</h1><p id="0938" class="pw-post-body-paragraph ly lz ji ma b mb mc ks md me mf kv mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">假设我们有一个与<em class="ne"> 4 </em>特征和<em class="ne"> n </em>观察高度相关的特征矩阵，如下面的<strong class="ma js">表1 </strong>所示:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/fc236821743826e5ca2039798382c383.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*qwfVP2fO_5ysKLiV7A-NzQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd li">表1 </strong>。具有4个变量和n个观察值的特征矩阵。</figcaption></figure><p id="82ca" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">为了可视化特征之间的相关性，我们可以生成一个散点图，如图<strong class="ma js">图1 </strong>所示。为了量化特征之间的相关程度，我们可以使用以下等式计算协方差矩阵:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/63bf28e0d0c1738a8fb0d4044370d0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F7llCoNZ85mQkPR-C0KFwA.png"/></div></div></figure><p id="a76a" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在矩阵形式中，协方差矩阵可以表示为4×4对称矩阵:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/7ecfd97f386248be54b21b7215b04a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*wNd_J9lYsAG4dn2Gq6FJIg.png"/></div></figure><p id="746a" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">可以通过执行酉变换(PCA变换)来对角化该矩阵，以获得以下结果:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/41fb3587e1c776d21ab7a1e75259eecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*YSgemfsENV7hRp0bm8Mnug.png"/></div></figure><p id="a927" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">由于矩阵的迹在酉变换下保持不变，我们观察到对角矩阵的特征值之和等于包含在特征X1、X2、X3和X4中的总方差。因此，我们可以定义以下数量:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/f04e5822fde51954a559153cbf31d16a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xf-yYARQye5pzpAQePx6PQ.png"/></div></div></figure><p id="843e" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">请注意，当p = 4时，累积方差如预期的那样等于1。</p><h1 id="7a3c" class="lg lh ji bd li lj lk ll lm ln lo lp lq kx lr ky ls la lt lb lu ld lv le lw lx bi translated">三。r .认证后活动的实施</h1><p id="5588" class="pw-post-body-paragraph ly lz ji ma b mb mc ks md me mf kv mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">为了说明PCA是如何工作的，我们通过检查虹膜数据集来展示一个例子。R代码可以从这里下载:<a class="ae nk" href="https://github.com/bot13956/principal_component_analysis_iris_dataset/blob/master/PCA_irisdataset.R" rel="noopener ugc nofollow" target="_blank"><em class="ne">https://github . com/bot 13956/principal _ component _ analysis _ iris _ dataset/blob/master/PCA _ iris dataset。r</em>T3】</a></p><p id="e610" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">让我们看看协方差矩阵:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/ebabacd43672d5fa4bf28bf63fd31903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/0*4lPMl0eIcKdeCMOa.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd li">表二</strong>。虹膜数据集的相关矩阵。</figcaption></figure><p id="0716" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma js">表2 </strong>显示了虹膜数据集中原始特征之间的强相关性。<strong class="ma js">图2 </strong>是一个成对图，显示了原始特征之间的散点图、密度图和相关系数。注意原始特征之间的强相关性。</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/18d68c4b4c4729f21f011fce27338986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*c5OfKq-kEQovQK5t.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd li">图二</strong>。原始特征空间中虹膜数据集的配对图。</figcaption></figure><p id="3cf8" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">现在让我们检查变换后的协方差矩阵:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/fe3b95197352d5ef072faa517f93936a.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/0*DA-m_PQf5ll745sd.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd li">表3 </strong>。PCA空间中的协方差矩阵。</figcaption></figure><p id="a7a7" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma js">表3 </strong>显示了变换特征之间的零相关性。<strong class="ma js">图4 </strong>显示了PCA空间中的配对图。我们看到特征之间的相关性已经被去除。</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/762fec3ffc99b5ae2ace66178f86dfbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*IfTfXii83d1C7qJD.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd li">图3 </strong>。PCA空间中虹膜数据集的配对图。</figcaption></figure><p id="4e56" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma js">表4 </strong>总结了PCA计算得出的有用指标:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b8b3e30ab40ed7ba4bd1be12a140c3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*i1u5iDWYstJgilf_.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated"><strong class="bd li">表4 </strong>。PCA计算的有用指标汇总。</figcaption></figure><p id="f548" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">基于这一总结，我们看到99.5%的方差是由前三个主成分贡献的(p = 3)。这意味着在最终模型中，第四主成分PC4可以被丢弃，因为它对方差的贡献可以忽略不计。</p><h1 id="f1d4" class="lg lh ji bd li lj lk ll lm ln lo lp lq kx lr ky ls la lt lb lu ld lv le lw lx bi translated">四。总结和结论</h1><p id="3b4d" class="pw-post-body-paragraph ly lz ji ma b mb mc ks md me mf kv mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">总之，我们解释了PCA的数学基础，并展示了如何使用iris数据集在R中实现PCA算法。用于执行计算的R代码可以从这里下载:<a class="ae nk" href="https://github.com/bot13956/principal_component_analysis_iris_dataset/blob/master/PCA_irisdataset.R" rel="noopener ugc nofollow" target="_blank"><em class="ne">https://github . com/bot 13956/principal _ component _ analysis _ iris _ dataset/blob/master/PCA _ iris dataset。r</em>T27】</a></p><h1 id="cb8d" class="lg lh ji bd li lj lk ll lm ln lo lp lq kx lr ky ls la lt lb lu ld lv le lw lx bi translated">其他数据科学/机器学习资源</h1><p id="adfb" class="pw-post-body-paragraph ly lz ji ma b mb mc ks md me mf kv mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated"><a class="ae nk" href="https://towardsdatascience.com/data-science-minimum-10-essential-skills-you-need-to-know-to-start-doing-data-science-e5a5a9be5991" rel="noopener" target="_blank">数据科学最低要求:开始从事数据科学时你需要知道的10项基本技能</a></p><p id="369a" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://medium.com/towards-artificial-intelligence/data-science-curriculum-bf3bb6805576" rel="noopener">数据科学课程</a></p><p id="65e0" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://medium.com/towards-artificial-intelligence/4-math-skills-for-machine-learning-12bfbc959c92" rel="noopener">机器学习的基本数学技能</a></p><p id="7617" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://medium.com/towards-artificial-intelligence/3-best-data-science-mooc-specializations-d58da382f628" rel="noopener"> 3个最佳数据科学MOOC专业</a></p><p id="1eae" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://towardsdatascience.com/5-best-degrees-for-getting-into-data-science-c3eb067883b1" rel="noopener" target="_blank">进入数据科学的5个最佳学位</a></p><p id="6dbc" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://towardsdatascience.com/5-reasons-why-you-should-begin-your-data-science-journey-in-2020-2b4a0a5e4239" rel="noopener" target="_blank">2020年开始数据科学之旅的5个理由</a></p><p id="06ff" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://towardsdatascience.com/theoretical-foundations-of-data-science-should-i-care-or-simply-focus-on-hands-on-skills-c53fb0caba66" rel="noopener" target="_blank">数据科学的理论基础——我应该关心还是仅仅关注实践技能？</a></p><p id="ada8" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://towardsdatascience.com/machine-learning-project-planning-71bdb3a44349" rel="noopener" target="_blank">机器学习项目规划</a></p><p id="ca19" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://towardsdatascience.com/how-to-organize-your-data-science-project-dd6599cf000a" rel="noopener" target="_blank">如何组织您的数据科学项目</a></p><p id="e8b7" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://medium.com/towards-artificial-intelligence/productivity-tools-for-large-scale-data-science-projects-64810dfbb971" rel="noopener">大型数据科学项目的生产力工具</a></p><p id="db8f" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://towardsdatascience.com/a-data-science-portfolio-is-more-valuable-than-a-resume-2d031d6ce518" rel="noopener" target="_blank">数据科学作品集比简历更有价值</a></p><p id="a082" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae nk" href="https://medium.com/towards-artificial-intelligence/data-science-101-a-short-course-on-medium-platform-with-r-and-python-code-included-3cdc9d489c6d" rel="noopener">数据科学101 —包含R和Python代码的中型平台短期课程</a></p><p id="8859" class="pw-post-body-paragraph ly lz ji ma b mb mu ks md me mv kv mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如有问题和疑问，请发邮件给我:benjaminobi@gmail.com</p></div></div>    
</body>
</html>