<html>
<head>
<title>Summarization Using Pegasus Model with the Transformers Library</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pegasus模型和变压器库进行总结</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/summarization-using-pegasus-model-with-the-transformers-library-553cd0dc5c2?source=collection_archive---------0-----------------------#2020-11-23">https://pub.towardsai.net/summarization-using-pegasus-model-with-the-transformers-library-553cd0dc5c2?source=collection_archive---------0-----------------------#2020-11-23</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="83b6" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="5e0f" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">使用Google的Pegasus模型和Huggingface transformers库生成文本摘要(提取的或抽象的)</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/93f4bb54d621606ed837dc2cc76c3d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4REIeAVLLNnVrWu4"/></div></div></figure></div><div class="ab cl le lf hy lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="in io ip iq ir"><p id="46c1" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated">PEGASUS是谷歌最近在2020年6月开源的最新抽象摘要模型。它代表<strong class="ln je"> P </strong>再训练，带有<strong class="ln je"> E </strong>提取的<strong class="ln je"> G </strong> ap语句，用于<strong class="ln je">A</strong>b提取的<strong class="ln je">SU</strong>mmalization<strong class="ln je">S</strong>序列到序列模型。关于模型的更多细节，请参考关于<a class="ae mh" href="https://arxiv.org/abs/1912.08777" rel="noopener ugc nofollow" target="_blank"> <strong class="ln je"> arXiv </strong> </a>的论文。</p><p id="fb3e" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated">这篇文章假设你知道摘要和它的两种不同风格——提取摘要和抽象摘要。</p><h1 id="01c9" class="mj mk iu bd ml mm mn mo mp mq mr ms mt kj mu kk mv km mw kn mx kp my kq mz na bi translated">装置</h1><pre class="kt ku kv kw gu nb nc nd ne aw nf bi"><span id="6481" class="ng mk iu nc b gz nh ni l nj nk">pip install transformers</span></pre><h1 id="2499" class="mj mk iu bd ml mm mn mo mp mq mr ms mt kj mu kk mv km mw kn mx kp my kq mz na bi translated">使用</h1><p id="3d37" class="pw-post-body-paragraph ll lm iu ln b lo nl ke lq lr nm kh lt lu nn lw lx ly no ma mb mc np me mf mg in bi translated">直到最近，我们不得不直接使用来自Google的Pegasus <a class="ae mh" href="https://github.com/google-research/pegasus" rel="noopener ugc nofollow" target="_blank"> Github库</a>的代码，并且不得不按照我在本文<a class="ae mh" href="https://medium.com/@chetan.ambi/generate-summaries-using-googles-pegasus-library-772633a161c2" rel="noopener">中所做的某些步骤来生成摘要输出。在</a><a class="ae mh" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>的神奇团队将Pegasus包含在<code class="fe nq nr ns nc b">transformers</code>库中之后，我们现在可以开始使用它，就像我们使用<code class="fe nq nr ns nc b">transformers</code>库中的其他模型一样。</p><p id="7ee8" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated">以下使用示例摘自<a class="ae mh" href="https://huggingface.co/transformers/model_doc/pegasus.html#usage-example" rel="noopener ugc nofollow" target="_blank">拥抱面</a>。正如您在下面看到的，只需几行代码，我们就可以使用transformers库生成最先进的摘要。如果您可以将这段代码与原始的Pegasus代码以及生成摘要所涉及的步骤进行比较，您就会体会到transformers库所提供的简单性。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="nt nu l"/></div></figure><ul class=""><li id="bae6" class="nv nw iu ln b lo lp lr ls lu nx ly ny mc nz mg oa ob oc od bi translated">作为第一步，我们将从变形金刚进口<code class="fe nq nr ns nc b">torch</code>和所需模块。既然我们对飞马型号感兴趣<code class="fe nq nr ns nc b">PegasusForConditionalGeneration</code>和<code class="fe nq nr ns nc b">PegasusTokenizer</code>都是进口的。</li><li id="8055" class="nv nw iu ln b lo oe lr of lu og ly oh mc oi mg oa ob oc od bi translated">Pegasus在<code class="fe nq nr ns nc b">C4</code> &amp; <code class="fe nq nr ns nc b">Hugenews</code>语料库上进行预训练，然后在12个下游数据集上进行微调。幸运的是，我们对所有的模型都有检查点，这样我们就可以直接使用这些预先训练好的模型来完成我们的下游任务。注意，这12个微调的模型本质上都是提取的和抽象的，因此模型的使用取决于用例(提取的或抽象的)。在上面的例子中，我们使用了在<code class="fe nq nr ns nc b">reddit-tifu </code>数据集上微调的模型，因为我们对抽象的摘要感兴趣。如果您需要使用不同于<code class="fe nq nr ns nc b">reddit-tifu</code>的型号，您可以使用以下列表中的任何一种<code class="fe nq nr ns nc b">model_name</code>。</li></ul><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oj"><img src="../Images/f0e2753b0fb7b9f0ac70cb32aef44626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*spkLUu8InZQ3TZtZxmZy-g.png"/></div><figcaption class="ok ol gk gi gj om on bd b be z dk translated">作者图片</figcaption></figure><ul class=""><li id="6710" class="nv nw iu ln b lo lp lr ls lu nx ly ny mc nz mg oa ob oc od bi translated">在接下来的步骤中，创建Pegasus tokenizer对象<code class="fe nq nr ns nc b">tokenizer</code>，然后下载模型检查点(在本例中为<code class="fe nq nr ns nc b">reddit-tifu</code>)并将其分配给<code class="fe nq nr ns nc b">model</code>。之后，我们要为其生成摘要的源文本被传递到<code class="fe nq nr ns nc b">prepare_seq2seq_batch</code>。</li><li id="5a10" class="nv nw iu ln b lo oe lr of lu og ly oh mc oi mg oa ob oc od bi translated">接下来，模型上的<code class="fe nq nr ns nc b">generate</code>方法将生成编码格式的摘要，然后在通过<code class="fe nq nr ns nc b">batch_encode</code>方法后，我们得到一个很好的摘要。<code class="fe nq nr ns nc b">generate</code>方法有许多参数可用于控制汇总输出。您可以随意使用的一些重要参数有— <code class="fe nq nr ns nc b">min_length</code>、<code class="fe nq nr ns nc b">max_length</code>、<code class="fe nq nr ns nc b">num_beams,</code>、<code class="fe nq nr ns nc b">early stopping</code>等。有关这些参数的示例用法，请参考以下示例:</li></ul><pre class="kt ku kv kw gu nb nc nd ne aw nf bi"><span id="a54b" class="ng mk iu nc b gz nh ni l nj nk">translated = model.generate(**batch, min_length=50, num_beams=5)</span></pre><p id="3855" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated">让我们尝试使用<code class="fe nq nr ns nc b">reddit-tifu</code>模型生成摘要。生成摘要时，需要传递文本。你想把它总结到上面代码中的<code class="fe nq nr ns nc b">src_text</code>字段。文章末尾还提供了完整的代码。</p><h2 id="dacf" class="ng mk iu bd ml oo op dn mp oq or dp mt lu os ot mv ly ou ov mx mc ow ox mz ja bi translated">示例1</h2><p id="e103" class="pw-post-body-paragraph ll lm iu ln b lo nl ke lq lr nm kh lt lu nn lw lx ly no ma mb mc np me mf mg in bi translated"><strong class="ln je"> <em class="mi">来源正文:</em></strong><a class="ae mh" href="https://www.nbcnews.com/politics/2020-election/georgia-secretary-state-raffensperger-says-sen-graham-asked-him-about-n1247968" rel="noopener ugc nofollow" target="_blank">https://www . NBC news . com/politics/2020-election/Georgia-secretary-state-raffensperger-says-sen-Graham-asked-him-about-n 1247968</a></p><p id="2c27" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated"><strong class="ln je"> <em class="mi">摘要文本(输出):</em> </strong>佐治亚州国务卿表示，他将一名保守派参议员的提问解释为暗示他可以在总统竞选中扔掉合法选票，但该参议员表示，他不认为事实如此，这是“荒谬的”</p><h2 id="bf13" class="ng mk iu bd ml oo op dn mp oq or dp mt lu os ot mv ly ou ov mx mc ow ox mz ja bi translated">示例2</h2><p id="f423" class="pw-post-body-paragraph ll lm iu ln b lo nl ke lq lr nm kh lt lu nn lw lx ly no ma mb mc np me mf mg in bi translated"><strong class="ln je"> <em class="mi">来源正文</em></strong>:<a class="ae mh" href="https://www.ndtv.com/india-news/sputnik-v-covid-19-vaccine-could-be-produced-in-india-china-russian-president-2326475" rel="noopener ugc nofollow" target="_blank">https://www . ndtv . com/India-news/Sputnik-v-新冠肺炎-vaccine-could-be-producted-in-India-China-Russian-president-2326475</a></p><p id="903c" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated">莫斯科:周二，弗拉基米尔·普京主席呼吁金砖国家在冠状病毒疫苗的开发上共同努力，因为他建议俄罗斯针对新冠肺炎的Sputnik V疫苗可以在中国和印度生产，这两个国家都是五国集团的成员。</p><blockquote class="oy oz pa"><p id="d3c3" class="ll lm mi ln b lo lp ke lq lr ls kh lt pb lv lw lx pc lz ma mb pd md me mf mg in bi translated">注意:即使我们正在寻找一个抽象的总结，有时模型可能会产生一个抽象的总结，就像我们在上面的例子2中看到的那样。</p></blockquote><h1 id="108b" class="mj mk iu bd ml mm mn mo mp mq mr ms mt kj mu kk mv km mw kn mx kp my kq mz na bi translated">完整代码</h1><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="pe nu l"/></div></figure><h1 id="bf23" class="mj mk iu bd ml mm mn mo mp mq mr ms mt kj mu kk mv km mw kn mx kp my kq mz na bi translated">结论</h1><p id="001b" class="pw-post-body-paragraph ll lm iu ln b lo nl ke lq lr nm kh lt lu nn lw lx ly no ma mb mc np me mf mg in bi translated">在本文中，您已经了解了如何使用Google的Pegasus模型通过<code class="fe nq nr ns nc b">transformers</code>库生成摘要。</p><p id="37fc" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated">您有兴趣使用Streamlit构建一个简单的web应用程序来进行汇总吗？参考下面的文章。</p><div class="pf pg gq gs ph pi"><a href="https://medium.com/towards-artificial-intelligence/build-a-text-summarization-web-app-using-streamlit-in-30-minutes-cbe78a8080af" rel="noopener follow" target="_blank"><div class="pj ab fp"><div class="pk ab pl cl cj pm"><h2 class="bd je gz z fq pn fs ft po fv fx jd bi translated">使用Streamlit在30分钟内创建一个文本摘要Web应用程序</h2><div class="pp l"><h3 class="bd b gz z fq pn fs ft po fv fx dk translated">使用Streamlit总结ML应用程序利用变压器模型</h3></div><div class="pq l"><p class="bd b dl z fq pn fs ft po fv fx dk translated">medium.com</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw lc pi"/></div></div></a></div><p id="50eb" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated"><em class="mi">阅读更多关于Python和数据科学的此类有趣文章，</em> <a class="ae mh" href="https://pythonsimplified.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ln je"> <em class="mi">订阅</em> </strong> </a> <em class="mi">到我的博客</em><a class="ae mh" href="http://www.pythonsimplified.com" rel="noopener ugc nofollow" target="_blank"><strong class="ln je"><em class="mi">www.pythonsimplified.com</em></strong></a><strong class="ln je"><em class="mi">。</em> </strong>你也可以通过<a class="ae mh" href="https://www.linkedin.com/in/chetanambi/" rel="noopener ugc nofollow" target="_blank"> <strong class="ln je"> LinkedIn </strong> </a>联系我。</p></div><div class="ab cl le lf hy lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="in io ip iq ir"><h1 id="325e" class="mj mk iu bd ml mm px mo mp mq py ms mt kj pz kk mv km qa kn mx kp qb kq mz na bi translated">参考</h1><p id="5508" class="pw-post-body-paragraph ll lm iu ln b lo nl ke lq lr nm kh lt lu nn lw lx ly no ma mb mc np me mf mg in bi translated">[1].<a class="ae mh" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers</a></p><p id="72cf" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated">[2].<a class="ae mh" href="https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2020/06/pegasus-state-of-art-model-for . html</a></p><p id="a21b" class="pw-post-body-paragraph ll lm iu ln b lo lp ke lq lr ls kh lt lu lv lw lx ly lz ma mb mc md me mf mg in bi translated">[3].<a class="ae mh" href="https://github.com/google-research/pegasus" rel="noopener ugc nofollow" target="_blank">https://github.com/google-research/pegasus</a></p></div></div>    
</body>
</html>