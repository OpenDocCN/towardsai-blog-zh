<html>
<head>
<title>New Model for Word Embeddings which are Resilient to Misspellings (MOE)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">抗拼写错误的单词嵌入新模型(MOE)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/new-model-for-word-embeddings-which-are-resilient-to-misspellings-moe-9ecfd3ab473e?source=collection_archive---------0-----------------------#2019-08-29">https://pub.towardsai.net/new-model-for-word-embeddings-which-are-resilient-to-misspellings-moe-9ecfd3ab473e?source=collection_archive---------0-----------------------#2019-08-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0c1a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">用一个新的模型来研究单词嵌入| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向人工智能</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/4c902acc6751dd9f21631637d4eb125f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z1ocDhYn5GMuL9he"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae ko" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="d15a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">传统的词嵌入技术擅长解决自然语言处理的许多下游问题，如文档分类和命名实体识别(NER)。然而，缺点之一是缺乏处理超词汇的能力(OOV)。</p><p id="761b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">脸书引入了拼写错误的遗忘(单词)嵌入(MOE ),克服了这一限制。MOE扩展了fastText体系结构来实现它。所以这个故事先经过fastText的训练方法和架构，再谈MOE。</p><h1 id="3412" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">负采样跳跃图</h1><p id="63d0" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">fastText扩展了word2vec的体系结构，该体系结构使用带有负采样方法的skip-gram来训练单词嵌入。Skip-gram使用上下文单词来预测周围的单词，以便学习文本表示(也称为嵌入)。负抽样方法是为上述训练挑选错误案例的一种方式。更多细节，你可以查看这些帖子(<a class="ae ko" href="https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a" rel="noopener" target="_blank">跳过图</a>和<a class="ae ko" href="https://medium.com/@makcedward/how-negative-sampling-work-on-word2vec-7bf8d545b116" rel="noopener">负采样</a>)了解更多信息。</p><p id="8509" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下图显示了word2vec中不同的训练方法。连续词袋(BOW)利用周围的词来预测上下文词，而Skip-gram使用上下文词来预测周围的词。</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mq"><img src="../Images/f023ca784c13a3ba7aca814095841852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3BDDbba1w_II07We.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">向量空间中单词表示的有效估计(Tomas等人，2013年)</figcaption></figure><h1 id="a904" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">快速文本</h1><p id="96ed" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">fastText遵循SGNS的思想，几乎没有修改。fastText的一个特点就是子字。N-gram方法用于将单词拆分成子单词。例如，n元字符的范围在3到5之间。我们可以把<code class="fe mv mw mx my b">banana</code>拆分为<code class="fe mv mw mx my b">ban</code>、<code class="fe mv mw mx my b">ana</code>、<code class="fe mv mw mx my b">nan</code>、<code class="fe mv mw mx my b">bana</code>、<code class="fe mv mw mx my b">anan</code>、<code class="fe mv mw mx my b">nana</code>、<code class="fe mv mw mx my b">banan</code>、<code class="fe mv mw mx my b">anana</code>。同时，<code class="fe mv mw mx my b">banana</code>的嵌入是这些子字嵌入的总和。</p><p id="d245" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">fastText的训练目标分类标签。模型输入是n元特征(即x1，x2 … xN)。这些要素将在隐藏图层中进行平均，并最终输入到输出图层中。</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/394d62124ef8dadb77040cc443ff96f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*37EsRn9Y0h-hOyQm3o2Y0A.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">fastText的架构(Joulin等人，2016年)</figcaption></figure><h1 id="d28e" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">拼写错误的遗忘(单词)嵌入(MOE)</h1><p id="d832" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">MOE通过引入拼写纠正损失进一步扩展了快速文本的概念。拼写校正丢失的目标是将拼写错误的单词嵌入映射到其拼写正确的变体的嵌入附近。法术修正损失是典型的逻辑函数。它是正确单词和拼写错误单词的子单词的输入向量之和的点积。</p><p id="7b5a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面显示的是<code class="fe mv mw mx my b">bird</code>(正确的单词)和<code class="fe mv mw mx my b">bwrd</code>(拼错的单词)的嵌入是靠在一起的。</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi na"><img src="../Images/44fd89ee6ee7a0316432b1a75a24abeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PZF0D0AGZbDZ6LUs"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">教育部的代表(<a class="ae ko" href="https://ai.facebook.com/blog/-a-new-model-for-word-embeddings-that-are-resilient-to-misspellings-/" rel="noopener ugc nofollow" target="_blank">脸书</a></figcaption></figure><h1 id="f806" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">拿走</h1><ul class=""><li id="f5ab" class="nb nc it kr b ks ml kw mm la nd le ne li nf lm ng nh ni nj bi translated">子词是处理拼写错误的单词和未知单词的有效方法。MOE使用n-gram字符来构建子词字典，而其他最新的NLP模型(如BERT、GPT-2)使用统计方法(如单词块、字节对编码)来构建子词字典。</li><li id="3c19" class="nb nc it kr b ks nk kw nl la nm le nn li no lm ng nh ni nj bi translated">在许多NLP系统中，处理看不见的单词是一个重要的优势。例如，聊天机器人需要处理大量的新词汇，无论是拼写错误还是新单词。</li></ul><h1 id="b064" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。在<a class="ae ko" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ko" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上随时联系<a class="ae ko" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>。</p><h1 id="be7f" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">延伸阅读</h1><ul class=""><li id="fcac" class="nb nc it kr b ks ml kw mm la nd le ne li nf lm ng nh ni nj bi translated">训练MOE的拼写错误数据集<a class="ae ko" href="https://bitbucket.org/bedizel/moe/src/master/" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="5b44" class="nb nc it kr b ks nk kw nl la nm le nn li no lm ng nh ni nj bi translated">关于<a class="ae ko" href="https://medium.com/@makcedward/how-negative-sampling-work-on-word2vec-7bf8d545b116" rel="noopener">负抽样</a>方法的故事</li><li id="a902" class="nb nc it kr b ks nk kw nl la nm le nn li no lm ng nh ni nj bi translated">关于<a class="ae ko" href="https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a" rel="noopener" target="_blank">快速文本</a>的故事</li><li id="f885" class="nb nc it kr b ks nk kw nl la nm le nn li no lm ng nh ni nj bi translated"><a class="ae ko" href="https://fasttext.cc/docs/en/english-vectors.html" rel="noopener ugc nofollow" target="_blank"> fastText </a>的官方页面。</li></ul><h1 id="f033" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">参考</h1><ul class=""><li id="8ea6" class="nb nc it kr b ks ml kw mm la nd le ne li nf lm ng nh ni nj bi translated">T.Mikolov，G. Corrado，K. Chen和J. Dean。<a class="ae ko" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计</a>。2013.</li><li id="32e9" class="nb nc it kr b ks nk kw nl la nm le nn li no lm ng nh ni nj bi translated">A.Joulin、E. Grave、P. Bojanowski和T. Mikolov。<a class="ae ko" href="https://arxiv.org/pdf/1607.01759.pdf" rel="noopener ugc nofollow" target="_blank">高效文本分类的锦囊妙计</a>。2016</li><li id="4048" class="nb nc it kr b ks nk kw nl la nm le nn li no lm ng nh ni nj bi translated">B.埃迪泽尔、a .皮克图斯、p .博亚诺斯基、r .费雷拉、e .格雷夫和f .西尔维斯特里。<a class="ae ko" href="https://arxiv.org/pdf/1905.09755.pdf" rel="noopener ugc nofollow" target="_blank">拼错遗忘词嵌入</a>。2019</li></ul></div></div>    
</body>
</html>