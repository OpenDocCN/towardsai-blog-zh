<html>
<head>
<title>The Gradient Descent Algorithm and its Variants</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降算法及其变体</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-gradient-descent-algorithm-and-its-variants-e0915796dbf2?source=collection_archive---------4-----------------------#2022-10-24">https://pub.towardsai.net/the-gradient-descent-algorithm-and-its-variants-e0915796dbf2?source=collection_archive---------4-----------------------#2022-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/2a61a2c8d2263b6a1be8c70e1a37564e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qw8TtJc267V6dUP5w2J1vg.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://pixabay.com/users/sarajuggernaut-28237/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=167089" rel="noopener ugc nofollow" target="_blank">莎拉</a>提供，来自<a class="ae jd" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=167089" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a></figcaption></figure><div class=""/><div class=""><h2 id="badc" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">梯度下降算法及python代码示例</h2></div><p id="a237" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">作者:</strong> <a class="ae jd" href="https://www.linkedin.com/in/pratik-shukla28/" rel="noopener ugc nofollow" target="_blank">普拉蒂克·舒克拉</a></p><blockquote class="lr"><p id="0b8f" class="ls lt jg bd lu lv lw lx ly lz ma lq dk translated">"教育头脑而不教育心灵根本不是教育."― <a class="ae jd" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiapLSCnOr6AhV0MlkFHZw-A4UQFnoECBoQAQ&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAristotle&amp;usg=AOvVaw0fdZoZDczxO8LfFPUTWRt4" rel="noopener ugc nofollow" target="_blank">亚里士多德</a></p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="f413" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">梯度下降系列博客:</h2><ol class=""><li id="46be" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated"><a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/the-gradient-descent-algorithm-defddd1d312e">梯度下降算法</a></li><li id="8aa3" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/mathematical-intuition-behind-the-gradient-descent-algorithm-143a051c3fa9">梯度下降算法背后的数学直觉</a></li><li id="10ef" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/the-gradient-descent-algorithm-and-its-variants-e0915796dbf2">梯度下降算法&amp;它的变种</a>(你来了！)</li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="aeb5" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">目录:</h2><ol class=""><li id="ca19" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated"><a class="ae jd" href="#4c70" rel="noopener ugc nofollow">简介</a></li><li id="6fbd" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#f380" rel="noopener ugc nofollow">批量梯度下降(BGD) </a></li><li id="83df" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#bfb4" rel="noopener ugc nofollow">随机梯度下降</a></li><li id="e7be" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#817a" rel="noopener ugc nofollow">小批量梯度下降(MBGD) </a></li><li id="196b" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#7efb" rel="noopener ugc nofollow">图形比较</a></li><li id="a77d" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#803f" rel="noopener ugc nofollow">结束注释</a></li><li id="4be1" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#168d" rel="noopener ugc nofollow">资源</a></li><li id="d50d" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#f9f3" rel="noopener ugc nofollow">参考文献</a></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="4c70" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">简介:</h2><p id="1464" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">请击鼓:欢迎来到梯度下降系列的大结局！在这篇博客中，我们将深入探讨梯度下降算法。我们将讨论所有有趣的梯度下降算法，以及它们的Python代码示例。我们还将基于每个算法中执行的计算数量来检查算法之间的差异。我们今天会想尽一切办法，所以我们要求你在阅读文档时运行<a class="ae jd" href="#168d" rel="noopener ugc nofollow"><strong class="kx jh"><em class="nu">Google Colab</em></strong></a>文件；这样做会让你对主题有更准确的理解，从而看到它的实际应用。让我们开始吧！</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="f380" class="nv mj jg bd mk nw nx ny mn nz oa ob mq km oc kn mt kp od kq mw ks oe kt mz of bi translated">批量梯度下降:</h1><figure class="oh oi oj ok gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/6c43a5e77db507fd7014a2d1b25df701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z7ygX0jbAvd3n2rFlCjRJg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">批量梯度下降(BGD)算法的工作原理</figcaption></figure><p id="85e7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">批量梯度下降(BGD)算法在每次迭代中考虑所有的训练样本。如果数据集包含大量的训练样本和大量的要素，那么实施批量梯度下降(BGD)算法在计算上就会变得非常昂贵，所以要注意预算！让我们举个例子来更好地理解它。</p><blockquote class="ol om on"><p id="71f4" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">批量梯度下降(BGD): </strong></p><p id="9c38" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">每次迭代的训练样本数= 100万= 1⁰⁶ <br/>迭代次数= 1000 = 1⁰ <br/>要训练的参数数= 10000 = 1⁰⁴ <br/>总计算量= 1⁰⁶ * 1⁰ * 1⁰⁴ = 1⁰</p></blockquote><p id="5f74" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们看看批量梯度下降(BGD)算法是如何实现的。</p><h2 id="d594" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">1.第一步:</h2><p id="9ead" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">首先，我们从GitHub库下载数据文件。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">从GitHub获取数据文件</figcaption></figure><h2 id="fd50" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">2.第二步:</h2><p id="5c72" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们导入一些必需的库来读取、操作和可视化数据。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">导入所需的库</figcaption></figure><h2 id="9894" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">3.第三步:</h2><p id="3d3e" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们读取数据文件，然后打印它的前五行。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">读取和打印数据</figcaption></figure><h2 id="ee1f" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">4.第四步:</h2><p id="5dc3" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将数据集分为要素和目标变量。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">获取特征和目标变量</figcaption></figure><blockquote class="ol om on"><p id="d4e0" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:X = (200，3)和Y = (200，)</p></blockquote><h2 id="c3ce" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">5.第五步:</h2><p id="09d4" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">为了在进一步的步骤中执行矩阵计算，我们需要对目标变量进行整形。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">重塑Y轴上的数据</figcaption></figure><blockquote class="ol om on"><p id="5dba" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:X = (200，3)和Y = (200，1)</p></blockquote><h2 id="ea62" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">6.第六步:</h2><p id="b873" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们正在规范化数据集。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">标准化数据</figcaption></figure><blockquote class="ol om on"><p id="e485" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:X = (200，3)和Y = (200，1)</p></blockquote><h2 id="debb" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">7.第七步:</h2><p id="4e85" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将获得<code class="fe ot ou ov ow b">bias</code>和<code class="fe ot ou ov ow b">weights</code>矩阵的初始值。在执行正向传播时，我们将在第一次迭代中使用这些值。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">获取随机值来初始化我们的参数</figcaption></figure><blockquote class="ol om on"><p id="1533" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:偏差= (1，1)和权重= (1，3)</p></blockquote><h2 id="3ef8" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">8.第8步:</h2><p id="3857" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们执行正向传播步骤。该步骤基于以下公式。</p><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c8a3d38affec98f6633312529683df23.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*dAtIr_YhLR5D03j-fkO3wQ.png"/></div></figure><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">预测目标变量的值</figcaption></figure><blockquote class="ol om on"><p id="847f" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:预测值= (1，1)+(200，3)*(3，1) = (1，1)+(200，1) = (200，1)</p></blockquote><h2 id="d8b9" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">9.第九步:</h2><p id="3afc" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将计算与我们的预测相关的成本。该步骤基于以下公式。</p><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/31b10971d1525d9f36667fdb31e8ffe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*iisrzz6jyS3PODo-HyPQ1g.png"/></div></figure><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">获取与预测相关的成本</figcaption></figure><blockquote class="ol om on"><p id="0dd5" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:成本=标量值</p></blockquote><h2 id="a443" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">10.第十步:</h2><p id="17c6" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们使用梯度下降算法更新权重和偏差的参数值。该步骤基于以下公式。请注意，我们没有对权重值求和的原因是我们的权重矩阵不是一个<code class="fe ot ou ov ow b">1*1</code>矩阵。</p><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/12f126b58d94f3580cca8aaf23374d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*NXAR59BuLBWTu-1m2FZY5w.png"/></div></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/ce7cb139839c648e7a886160c4fd2276.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*_cAyMbZ-wIoHqaKvxfkxTw.png"/></div></figure><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">使用梯度下降算法更新参数</figcaption></figure><blockquote class="ol om on"><p id="266e" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:db = sum(200，1) = (1，1)</p><p id="98d5" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:dw = (1，200) * (200，3) = (1，3)</p><p id="464a" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:偏差= (1，1)和权重= (1，3)</p></blockquote><h2 id="6a34" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">11.第11步:</h2><p id="a666" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将使用我们刚刚定义的所有函数来运行梯度下降算法。我们还创建了一个名为<code class="fe ot ou ov ow b">cost_list</code>的空列表来存储所有迭代的成本值。该列表将用于在后续步骤中绘制图表。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">批量梯度下降算法</figcaption></figure><h2 id="b420" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">12.第12步:</h2><p id="d273" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们实际上是在调用函数来获得最终结果。请注意，我们运行的是<code class="fe ot ou ov ow b">200 iterations</code>的全部代码。同样，这里我们指定了<code class="fe ot ou ov ow b">learning rate of 0.01</code>。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">运行批量梯度下降算法200次迭代</figcaption></figure><h2 id="5ce9" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">13.第十三步:</h2><p id="2963" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们绘制<code class="fe ot ou ov ow b">iterations vs. cost</code>的图形。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/9d7a0d1a7308f60a09f5ec69bee6a289.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*HHHJ5om8cOFhG_Ac9JeJUg.png"/></div></figure><h2 id="f1b0" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">14.第14步:</h2><p id="b2e4" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们在所有迭代完成后打印<code class="fe ot ou ov ow b">final weights</code>值。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">在200次迭代后打印权重的最终值</figcaption></figure><h2 id="7d7c" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">15.步骤15:</h2><p id="9037" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们在所有迭代完成后打印<code class="fe ot ou ov ow b">final bias</code>值。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">在200次迭代后打印偏差的最终值</figcaption></figure><h2 id="d45b" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">16.第16步:</h2><p id="5ecd" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们绘制两个具有不同学习率的图形，以查看学习率在优化中的作用。在下图中，我们可以看到学习速率较高的图<code class="fe ot ou ov ow b">(0.01)</code>比学习速率较低的图<code class="fe ot ou ov ow b">(0.001)</code>收敛得更快。正如我们在梯度下降系列的第1部分中了解到的，这是因为具有较低学习速率的图形需要较小的步长。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">绘制不同学习速率下的批量梯度下降算法图</figcaption></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/122172e1501c9efb8b8607c360fc457d.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*fgW60jt6pEwGG7sG3Em0TQ.png"/></div></figure><h2 id="eb84" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">17.第17步:</h2><p id="08bf" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">让我们把它们放在一起。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div></figure><h2 id="2e9e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">计算次数:</h2><p id="1512" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，让我们统计一下批量梯度下降算法中执行的计算次数。</p><blockquote class="ol om on"><p id="8591" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">偏差:</strong>(训练示例)x(迭代次数)x(参数)= 200 * 200 * 1 = 40000</p><p id="f63b" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">权重:</strong>(训练示例)x(迭代次数)x(参数)= 200 * 200 *3 = 120000</p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="bfb4" class="nv mj jg bd mk nw nx ny mn nz oa ob mq km oc kn mt kp od kq mw ks oe kt mz of bi translated">随机梯度下降</h1><figure class="oh oi oj ok gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/95692475c02baa5f9acec8edae39c512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9sTb1-9_A5YfTBIu9TVn9g.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">随机梯度下降算法的工作原理</figcaption></figure><p id="7410" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在批量梯度下降算法中，我们考虑算法所有迭代的所有训练样本。但是，如果我们的数据集有大量的训练示例和/或特征，那么计算参数值的计算成本就会很高。我们知道，如果我们为机器学习算法提供更多的训练示例，它将产生更高的准确性。但是，随着数据集大小的增加，与之相关的计算也会增加。让我们举个例子来更好地理解这一点。</p><blockquote class="ol om on"><p id="97b8" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">批量梯度下降(BGD) </strong></p><p id="e0fe" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">每次迭代的训练样本数= 100万= 1⁰⁶ <br/>迭代次数= 1000 = 1⁰ <br/>要训练的参数数= 10000 = 1⁰⁴ <br/>总计算量= 1⁰⁶*1⁰ *1⁰⁴=1⁰</p></blockquote><p id="d409" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，如果我们看上面的数字，它并没有给我们很好的感觉！所以我们可以说，使用批量梯度下降算法似乎效率不高。因此，为了解决这个问题，我们使用随机梯度下降(SGD)算法。“随机”这个词的意思是随机的。因此，我们不是对数据集的所有训练示例执行计算，而是选取一个随机示例并对其执行计算。听起来很有趣，不是吗？在随机梯度下降(SGD)算法中，我们每次迭代只考虑一个训练样本。让我们看看随机梯度下降基于它的计算有多有效。</p><blockquote class="ol om on"><p id="884a" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">随机梯度下降(SGD): </strong></p><p id="5d61" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">每次迭代的训练样本数= 1 <br/>迭代次数= 1000 = 1⁰ <br/>要训练的参数数= 10000 = 1⁰⁴ <br/>总计算量= 1 * 1⁰ *1⁰⁴=1⁰⁷</p><p id="ade6" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">与批量梯度下降的比较:</strong></p><p id="0ab7" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">BGD = 1⁰ <br/>中的总计算量SGD = 1⁰⁷ <br/>中的总计算量<strong class="kx jh">评估:</strong>在这个例子中SGD比BGD快⁰⁶倍。</p></blockquote><p id="e787" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">注意:</strong>请注意，我们的成本函数不一定会下降，因为我们只是在每次迭代中随机选取一个训练样本，所以不要担心。然而，随着我们执行越来越多的迭代，成本函数将逐渐减小。</p><p id="69cd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们看看随机梯度下降(SGD)算法是如何实现的。</p><h2 id="4e10" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">1.第一步:</h2><p id="a92e" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">首先，我们从GitHub库下载数据文件。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">从GitHub获取数据文件</figcaption></figure><h2 id="7603" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">2.第二步:</h2><p id="7b79" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们导入一些必需的库来读取、操作和可视化数据。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">导入所需的库</figcaption></figure><h2 id="6062" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">3.第三步:</h2><p id="a5a8" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们读取数据文件，然后打印它的前五行。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">读取和打印数据</figcaption></figure><h2 id="aafa" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">4.第四步:</h2><p id="352d" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将数据集分为要素和目标变量。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">获取特征和目标变量</figcaption></figure><blockquote class="ol om on"><p id="3456" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:X = (200，3)和Y = (200，)</p></blockquote><h2 id="8484" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">5.第五步:</h2><p id="c560" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">为了在进一步的步骤中执行矩阵计算，我们需要对目标变量进行整形。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">重塑Y轴上的数据</figcaption></figure><blockquote class="ol om on"><p id="7373" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:X = (200，3)和Y = (200，1)</p></blockquote><h2 id="b63e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">6.第六步:</h2><p id="8d24" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们正在规范化数据集。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">标准化数据</figcaption></figure><blockquote class="ol om on"><p id="3571" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:X = (200，3)和Y = (200，1)</p></blockquote><h2 id="d3e3" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">7.第七步:</h2><p id="7068" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将获得<code class="fe ot ou ov ow b">bias</code>和<code class="fe ot ou ov ow b">weights</code>矩阵的初始值。在执行正向传播时，我们将在第一次迭代中使用这些值。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">获取随机值来初始化我们的参数</figcaption></figure><blockquote class="ol om on"><p id="c33b" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:偏差= (1，1)和权重= (1，3)</p></blockquote><h2 id="9d08" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">8.第8步:</h2><p id="68bc" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们执行正向传播步骤。该步骤基于以下公式。</p><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c8a3d38affec98f6633312529683df23.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*dAtIr_YhLR5D03j-fkO3wQ.png"/></div></figure><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">预测目标变量的值</figcaption></figure><blockquote class="ol om on"><p id="0abf" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:预测值= (1，1)+(200，3)*(3，1) = (1，1)+(200，1) = (200，1)</p></blockquote><h2 id="4d51" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">9.第九步:</h2><p id="a428" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将计算与预测相关的成本。该步骤使用的公式如下。因为只有一个误差值，所以我们不需要将成本函数除以数据集的大小，也不需要将所有成本值相加。</p><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/1274178a71e31d5cb431e27e155fa639.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*5XoeX7Ab-rtL5MPLuoBeig.png"/></div></figure><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">获取与预测相关的成本</figcaption></figure><blockquote class="ol om on"><p id="020b" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:成本=标量值</p></blockquote><h2 id="53de" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">10.第十步:</h2><p id="7d12" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们使用梯度下降算法更新权重和偏差的参数值。该步骤基于以下公式。请注意，我们不对权重值求和的原因是我们的权重矩阵不是一个<code class="fe ot ou ov ow b">1*1</code>矩阵。此外，在这种情况下，由于我们只有一个训练示例，我们不需要对所有示例执行求和。更新后的公式如下。</p><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/ecbf541169c8c7f85791bd437d337646.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*P2CoBkKV1rZxRiQ4tDKxLg.png"/></div></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/8ed1cace4fca8b9c005c8c4bb13d279f.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*hVbzmoLLsSDnoxRn67wTKA.png"/></div></figure><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">使用梯度下降算法更新参数</figcaption></figure><blockquote class="ol om on"><p id="b32a" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:db = (1，1)</p><p id="7be6" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:dw = (1，200) * (200，3) = (1，3)</p><p id="955e" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:偏差= (1，1)和权重= (1，3)</p></blockquote><h2 id="aaa3" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">11.第11步:</h2><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">随机梯度下降算法</figcaption></figure><h2 id="c7b1" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">12.第12步:</h2><p id="5bfc" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们实际上是在调用函数来获得最终结果。请注意，我们运行的是<code class="fe ot ou ov ow b">200 iterations</code>的全部代码。同样，这里我们已经指定了<code class="fe ot ou ov ow b">learning rate of 0.01</code>。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">运行随机梯度下降算法200次迭代</figcaption></figure><h2 id="931f" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">13.第十三步:</h2><p id="f9c9" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们在所有迭代完成后打印<code class="fe ot ou ov ow b">final weights</code>值。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">在200次迭代后打印权重的最终值</figcaption></figure><h2 id="cd74" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">14.第14步:</h2><p id="2924" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们在所有迭代完成后打印<code class="fe ot ou ov ow b">final bias</code>值。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">在200次迭代后打印偏差的最终值</figcaption></figure><h2 id="89ba" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">15.步骤15:</h2><p id="a272" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们绘制<code class="fe ot ou ov ow b">iterations vs. cost</code>的图形。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/bad0df1db665430f82146708e82df4d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*nXmQFrrB0ox0Tud7Ixx--g.png"/></div></figure><h2 id="4b96" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">16.第16步:</h2><p id="7e55" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们绘制两个具有不同学习率的图形，以查看学习率在优化中的作用。在下图中，我们可以看到学习速率较高的图<code class="fe ot ou ov ow b">(0.01)</code>比学习速率较低的图<code class="fe ot ou ov ow b">(0.001)</code>收敛得更快。同样，我们知道这一点是因为具有较低学习率的图形需要较小的步长。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">绘制不同学习速率下的批量梯度下降算法图</figcaption></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/8abfbec5fcc66515e4134ee5c21f0cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*RfV1SkGSXx99uDjql_mNBg.png"/></div></figure><h2 id="2e59" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">17.第17步:</h2><p id="f72a" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">把这些放在一起。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div></figure><h2 id="2679" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">计算:</h2><p id="e26a" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，让我们计算在实现批量梯度下降算法中执行的计算次数。</p><blockquote class="ol om on"><p id="16f6" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">偏差:</strong>(训练样本)x(迭代次数)x(参数)= 1* 200 * 1 = 200</p><p id="2b1b" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">权重:</strong>(训练示例)x(迭代次数)x(参数)= 1* 200 *3 = 600</p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="817a" class="nv mj jg bd mk nw nx ny mn nz oa ob mq km oc kn mt kp od kq mw ks oe kt mz of bi translated">小批量梯度下降算法；</h1><figure class="oh oi oj ok gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/bcece5b7d06d07b8627af8b7900b2b84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aIASPUWpYGY66al4MdykcA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">小批量梯度下降算法的工作原理</figcaption></figure><p id="5374" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在批量梯度下降(BGD)算法中，我们考虑算法所有迭代的所有训练样本。然而，在随机梯度下降(SGD)算法中，我们只考虑一个随机训练例子。现在，在小批量梯度下降(MBGD)算法中，我们在每次迭代中考虑训练样本的随机子集。由于这不像SGD那样随机，我们更接近全局最小值。然而，MBGD容易陷入局部极小值。让我们举个例子来更好地理解这一点。</p><blockquote class="ol om on"><p id="66d5" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">批量梯度下降(BGD): </strong></p><p id="376b" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">每次迭代的训练样本数= 100万= 1⁰⁶ <br/>迭代次数= 1000 = 1⁰ <br/>要训练的参数数= 10000 = 1⁰⁴ <br/>总计算量= 1⁰⁶*1⁰ *1⁰⁴=1⁰</p><p id="5839" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">随机梯度下降(SGD): </strong></p><p id="a3a6" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">每次迭代的训练样本数= 1 <br/>迭代次数= 1000 = 1⁰ <br/>要训练的参数数= 10000 = 1⁰⁴ <br/>总计算量= 1*1⁰ *1⁰⁴ = 1⁰⁷</p><p id="7f43" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">小批量梯度下降(MBGD): </strong></p><p id="13ca" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">每次迭代的训练样本数= 100 =1⁰<br/>→这里，我们考虑1⁰⁶.以外的1⁰训练样本<br/>迭代次数= 1000 = 1⁰ <br/>待训练参数数= 10000 = 1⁰⁴ <br/>总计算量= 1⁰ *1⁰ *1⁰⁴=1⁰⁹</p><p id="526d" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">与批量梯度下降(BGD)的比较:</strong></p><p id="f6d5" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">BGD = 1⁰的总计算量<br/>mbgd =1⁰⁹的总计算量</p><p id="d641" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">评估:</strong>在本例中，MBGD比BGD快1⁰⁴倍。</p><p id="083d" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">与随机梯度下降(SGD)的比较:</strong></p><p id="0696" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">SGD = 1⁰⁷的总计算量<br/>mbgd =1⁰⁹的总计算量</p><p id="7094" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">评估:</strong>在本例中，SGD比MBGD快1⁰倍。</p><p id="a469" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">BGD、新加坡元和MBGD的比较</strong>:</p><p id="c508" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">BGD= 1⁰的总计算量<br/>SGD =1⁰⁷的总计算量<br/>mbgd =1⁰⁹的总计算量</p><p id="9e29" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">评价:</strong>新币&gt; MBGD &gt; BGD</p></blockquote><p id="e9a1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">注意:</strong>请注意，我们的成本函数不一定下降，因为我们在每次迭代中随机抽取训练样本。然而，随着我们执行越来越多的迭代，成本函数将逐渐减小。</p><p id="04b0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们看看如何在实践中实现小批量梯度下降(MBGD)算法。</p><h2 id="84f4" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">1.第一步:</h2><p id="6c89" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">首先，我们从GitHub库下载数据文件。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">从GitHub获取数据文件</figcaption></figure><h2 id="425c" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">2.第二步:</h2><p id="3273" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们导入一些必需的库来读取、操作和可视化数据。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">导入所需的库</figcaption></figure><h2 id="a6ab" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">3.第三步:</h2><p id="970e" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们读取数据文件，然后打印它的前五行。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">读取和打印数据</figcaption></figure><h2 id="0d0c" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">4.第四步:</h2><p id="1287" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将数据集分为要素和目标变量。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">获取特征和目标变量</figcaption></figure><blockquote class="ol om on"><p id="7f2a" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:X = (200，3)和Y = (200，)</p></blockquote><h2 id="702e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">5.第五步:</h2><p id="5584" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">为了在进一步的步骤中执行矩阵计算，我们需要对目标变量进行整形。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">重塑Y轴上的数据</figcaption></figure><blockquote class="ol om on"><p id="cbe5" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:X = (200，3)和Y = (200，1)</p></blockquote><h2 id="a44e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">6.第六步:</h2><p id="52ab" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将对数据集进行规范化。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">标准化数据</figcaption></figure><blockquote class="ol om on"><p id="3c8b" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:X = (200，3)和Y = (200，1)</p></blockquote><h2 id="61da" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">7.第七步:</h2><p id="84cf" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将获得<code class="fe ot ou ov ow b">bias</code>和<code class="fe ot ou ov ow b">weights</code>矩阵的初始值。在执行正向传播时，我们将在第一次迭代中使用这些值。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">获取随机值来初始化我们的参数</figcaption></figure><blockquote class="ol om on"><p id="d813" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:偏差= (1，1)和权重= (1，3)</p></blockquote><h2 id="820b" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">8.第8步:</h2><p id="e9f4" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们执行正向传播步骤。该步骤基于以下公式。</p><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c8a3d38affec98f6633312529683df23.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*dAtIr_YhLR5D03j-fkO3wQ.png"/></div></figure><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">预测目标变量的值</figcaption></figure><blockquote class="ol om on"><p id="44b5" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:预测值= (1，1)+(200，3)*(3，1) = (1，1)+(200，1) = (200，1)</p></blockquote><h2 id="9286" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">9.第九步:</h2><p id="23f2" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将计算与我们的预测相关的成本。该步骤基于以下公式。</p><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/31b10971d1525d9f36667fdb31e8ffe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*iisrzz6jyS3PODo-HyPQ1g.png"/></div></figure><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">获取与预测相关的成本</figcaption></figure><blockquote class="ol om on"><p id="fbef" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:成本=标量值</p></blockquote><h2 id="a484" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">10.第十步:</h2><p id="e173" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们使用梯度下降算法更新权重和偏差的参数值。该步骤基于以下公式。请注意，我们不对权重值求和的原因是我们的权重矩阵不是一个<code class="fe ot ou ov ow b">1*1</code>矩阵。</p><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/12f126b58d94f3580cca8aaf23374d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*NXAR59BuLBWTu-1m2FZY5w.png"/></div></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/ce7cb139839c648e7a886160c4fd2276.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*_cAyMbZ-wIoHqaKvxfkxTw.png"/></div></figure><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">使用梯度下降算法更新参数</figcaption></figure><blockquote class="ol om on"><p id="3c7c" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:db = sum(200，1) = (1，1)</p><p id="9ca4" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">尺寸:dw = (1，200) * (200，3) = (1，3)</p><p id="0c70" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated">维度:偏差= (1，1)和权重= (1，3)</p></blockquote><h2 id="56fe" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">11.第11步:</h2><p id="166a" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将使用我们刚刚定义的所有函数来运行梯度下降算法。此外，我们正在创建一个名为<code class="fe ot ou ov ow b">cost_list</code>的空列表来存储所有迭代的成本值。在接下来的步骤中，我们将使用该列表绘制图表。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">最小批量梯度下降算法</figcaption></figure><h2 id="137e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">12.第12步:</h2><p id="5cfc" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们实际上是在调用函数来获得最终结果。请注意，我们运行的是<code class="fe ot ou ov ow b">200 iterations</code>的全部代码。同样，这里我们指定了<code class="fe ot ou ov ow b">learning rate of 0.01</code>。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">运行小批量梯度下降算法200次迭代</figcaption></figure><h2 id="8b94" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">13.第十三步:</h2><p id="8cde" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们在所有迭代完成后打印<code class="fe ot ou ov ow b">final weights</code>值。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">在200次迭代后打印权重的最终值</figcaption></figure><h2 id="c293" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">14.第14步:</h2><p id="d66c" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们在所有迭代完成后打印<code class="fe ot ou ov ow b">final bias</code>值。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">在200次迭代后打印偏差的最终值</figcaption></figure><h2 id="6343" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">15.步骤15:</h2><p id="532f" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们绘制<code class="fe ot ou ov ow b">iterations vs. cost</code>的图形。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/7e9a21e51dceb2007b15917e80a42d6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*eKRoT0FUFKh-WJE7y1qL4w.png"/></div></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/4ca285ed7968c366f88b53b45f253569.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*1kE3p2bI_TlBIA3QqZ4N_g.png"/></div></figure><h2 id="ee72" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">16.第16步:</h2><p id="9079" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们绘制两个具有不同学习率的图形，以查看学习率在优化中的作用。在下图中我们可以看到，学习速率较高的图形<code class="fe ot ou ov ow b">(0.01)</code>比学习速率较低的图形<code class="fe ot ou ov ow b">(0.001)</code>收敛得更快。背后的原因是学习率越低的图，走的步数越小。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">绘制不同学习速率下的批量梯度下降算法图</figcaption></figure><figure class="oh oi oj ok gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/17b245555e02da0f39f32cf3d0bddc6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*HBepFzVaOeq5nYRbJ1ExKg.png"/></div></figure><h2 id="a0e3" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">17.第17步:</h2><p id="31fe" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">把这些放在一起。</p><figure class="oh oi oj ok gt is"><div class="bz fp l di"><div class="or os l"/></div></figure><h2 id="e376" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">计算:</h2><p id="d7fc" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，让我们计算在实现批量梯度下降算法中执行的计算次数。</p><blockquote class="ol om on"><p id="7b47" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">偏差:</strong>(训练样本)x(迭代次数)x(参数)= 20 * 200 * 1 = 4000</p><p id="b7df" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh">权重:</strong>(训练示例)x(迭代次数)x(参数)= 20 * 200 *3 = 12000</p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="7efb" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">图表比较:</h2><div class="oh oi oj ok gt ab cb"><figure class="pg is ph pi pj pk pl paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/3c714d84062db080214dbfdfc2c39dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*HHHJ5om8cOFhG_Ac9JeJUg.png"/></div></figure><figure class="pg is ph pi pj pk pl paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/767db8ac5942cd4536e0c99ba8f9e54f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*nXmQFrrB0ox0Tud7Ixx--g.png"/></div></figure><figure class="pg is ph pi pj pk pl paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/9b7b29e099a35db049ca6d32309d21be.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*eKRoT0FUFKh-WJE7y1qL4w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk pm di pn po translated">批量、随机和小批量梯度下降算法的比较</figcaption></figure></div></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="803f" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">结束注释:</h2><p id="e7cf" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">就这样，我们到达了梯度下降系列的终点！在这一部分中，我们深入研究了代码，看看三种主要类型的梯度下降算法是如何相互执行的，总结如下:</p><blockquote class="ol om on"><p id="893c" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh"> 1。批次梯度下降</strong>精确度→高<br/>时间→更多</p><p id="1d16" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh"> 2。随机梯度下降</strong> <br/>精度→低<br/>时间→少</p><p id="4f04" class="kv kw nu kx b ky kz kh la lb lc kk ld oo lf lg lh op lj lk ll oq ln lo lp lq ij bi translated"><strong class="kx jh"> 3。小批量梯度下降</strong> <br/>精度→适中<br/>时间→适中</p></blockquote><p id="2521" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们希望你喜欢这个系列，并学到一些新的东西，无论你的起点或机器学习背景如何。随着你继续你的人工智能之旅，了解这种基本算法及其变体将很有价值，并更多地了解这种令人难以置信的技术的技术和宏大方面。留意其他提供更多机器学习课程的博客，并保持好奇！</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><figure class="oh oi oj ok gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/pratu"><div class="gh gi pp"><img src="../Images/bf19b95960da2e6db6fc0dd7ba17c7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/0*jR9Jyfl35goV8nO-.png"/></div></a><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">给普拉蒂克买杯咖啡！</figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="168d" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">资源:</h2><ol class=""><li id="e5ae" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated"><a class="ae jd" href="#f380" rel="noopener ugc nofollow"><strong class="kx jh"><em class="nu"/></strong></a><strong class="kx jh"><em class="nu">——</em></strong><a class="ae jd" href="https://colab.research.google.com/drive/1E2Zhw9nujo6ARC3w1CB5cahzwwk4Ew5k?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh"><em class="nu">Google Colab</em></strong></a><strong class="kx jh"><em class="nu"/></strong><a class="ae jd" href="https://gist.github.com/Pratik-Shukla-22/8111be5605f8251d026800f15c6a229e#file-batch_gradient_descent-py" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh"><em class="nu">GitHub</em></strong></a></li><li id="b598" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#bfb4" rel="noopener ugc nofollow"> <strong class="kx jh"> <em class="nu">【随机梯度下降】</em></strong></a><strong class="kx jh"><em class="nu">——</em></strong><a class="ae jd" href="https://colab.research.google.com/drive/16Bo-05gFHFqGzQAXTK2wJ57mro5bn00A?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh"><em class="nu">Google Colab</em></strong></a><strong class="kx jh"><em class="nu">，</em></strong><a class="ae jd" href="https://gist.github.com/Pratik-Shukla-22/da462cb50e5abe42cc6710ded0b69726#file-stochastic_gradient_descent-py" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh"><em class="nu">GitHub</em></strong></a></li><li id="42e1" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#817a" rel="noopener ugc nofollow"> <strong class="kx jh"> <em class="nu">迷你批量渐变下降</em></strong></a><strong class="kx jh"><em class="nu">——</em></strong><a class="ae jd" href="https://colab.research.google.com/drive/17PXc2XdMDRdMpzIXUlaTNKGOOS1u7PoZ?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh"><em class="nu">Google Colab</em></strong></a><strong class="kx jh"><em class="nu"/></strong><a class="ae jd" href="https://gist.github.com/Pratik-Shukla-22/d728e03457f8b817d594b6bda19c27fc#file-mini_batch_gradient_descent-py" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh"><em class="nu">GitHub</em></strong></a></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="89dc" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">引用:</h2><p id="57d0" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">对于学术背景下的归属，请引用该工作为:</p><pre class="oh oi oj ok gt pq ow pr ps aw pt bi"><span id="492a" class="mi mj jg ow b gy pu pv l pw px">Shukla, et al., “The Gradient Descent Algorithm &amp; its Variants”, Towards AI, 2022</span></pre></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="24b6" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">BibTex引文:</h2><pre class="oh oi oj ok gt pq ow pr ps aw pt bi"><span id="99e5" class="mi mj jg ow b gy pu pv l pw px">@article{pratik_2022, <br/> title={The Gradient Descent Algorithm &amp; its Variants}, <br/> url={<a class="ae jd" href="https://towardsai.net/p/l/the-gradient-descent-algorithm-and-its-variants" rel="noopener ugc nofollow" target="_blank">https://towardsai.net/p/l/the-gradient-descent-algorithm-and-its-variants</a>}, <br/> journal={Towards AI}, <br/> publisher={Towards AI Co.}, <br/> author={Pratik, Shukla},<br/> editor={Lauren, Keegan},  <br/> year={2022}, <br/> month={Oct}<br/>}</span></pre></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="f9f3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">参考资料:</p><ol class=""><li id="92af" class="nb nc jg kx b ky kz lb lc le py li pz lm qa lq ni nj nk nl bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降—维基百科</a></li></ol></div></div>    
</body>
</html>