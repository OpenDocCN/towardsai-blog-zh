<html>
<head>
<title>OpenAI Brings Introspection to Reinforcement Learning Agents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI为强化学习代理带来内省</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/openai-brings-introspection-to-reinforcement-learning-agents-39cbe4cf2af3?source=collection_archive---------1-----------------------#2021-04-12">https://pub.towardsai.net/openai-brings-introspection-to-reinforcement-learning-agents-39cbe4cf2af3?source=collection_archive---------1-----------------------#2021-04-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1267" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="7071" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">围绕进化策略梯度的研究试图在强化学习模型中重建内省。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/bd95ab559455bf191daf17da8a16fd61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bRLtkq-pwK_LIxEh.jpg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://janzz.technology/the-rise-of-the-machines-cognitive-computing-disruptive-potential/" rel="noopener ugc nofollow" target="_blank">https://janzz . technology/the-rise-of-the-machines-cognitive-computing-disruptive-potential/</a></figcaption></figure><blockquote class="li lj lk"><p id="6072" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过80，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到85，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="cfaa" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">自省是人类区别于其他物种的神奇认知能力之一。从概念上讲，内省可以被定义为检查有意识的思想和感觉的能力。自省在人类如何学习中也起着关键作用。你有没有尝试过自学一项新技能，比如学习一门新语言？即使没有任何外部反馈，你也可以快速评估自己是否在词汇或发音等方面取得了进步。如果我们可以将内省的一些原则应用到人工智能(AI)学科，如强化学习(RL)，这不是很好吗？</p><p id="34b6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">内省的魔力来自于这样一个事实，即人类可以通过生物进化过程，从其他任务的先前经验中获得形状非常好的<a class="ae lh" href="http://www-anw.cs.umass.edu/legacy/pubs/2009/singh_l_b_09.pdf" rel="noopener ugc nofollow" target="_blank">内部奖励函数</a>。该模型与RL代理形成鲜明对比，RL代理的基本编码是从零开始，主要依靠外部反馈来完成任何学习任务。毫不奇怪，大多数RL模型比人类花费更多的时间来学习类似的任务。最近，来自OpenAI的研究人员发表了一篇新论文，提出了一种解决这一挑战的方法，通过创建RL模型，这些模型知道在一项新任务上取得进展意味着什么，通过过去在类似任务上取得进展的经验。</p><p id="01c3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">题为<a class="ae lh" href="https://storage.googleapis.com/epg-blog-data/epg_2.pdf" rel="noopener ugc nofollow" target="_blank">进化的政策梯度</a> (EPG)的OpenAI研究论文介绍了新的元学习技术，该技术基于限定学习过程的损失函数的概念。当在RL模型中使用时，EPG方法不通过记忆的行为显式地编码知识，而是通过学习的损失函数使用隐含的机制。EPG的最终目标是RL代理可以使用这个损失函数来学习一个新的任务。</p><p id="9e98" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在算法上，EPG由两个优化循环组成。在内部循环中，代理从零开始学习解决从一系列任务中抽取的特定任务。任务系列可以是“将夹持器移动到目标位置[x，y]”，并且该系列中的一个特定任务可以是“将夹持器移动到位置[50，100]”。内环使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a> (SGD)针对外环提出的损失函数优化代理策略。外环评估内环学习后实现的回报，并使用<a class="ae lh" href="https://blog.openai.com/evolution-strategies/" rel="noopener ugc nofollow" target="_blank">进化策略</a> (ES)调整损失函数的参数，以提出将导致更高回报的新损失。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/dcd6795c91d5701c51d6ed5d4dfae8d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*82Tr1Xv30VM1L7DzDW54jQ.gif"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ne">图片来源:OpenAI </strong></figcaption></figure><p id="07bf" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">从元学习的角度来看，损失函数由代理人最近历史的时间卷积组成，这可以带来有趣的附带好处。例如，通过检查代理的历史，损失可以激励期望的扩展行为，如探索。此外，该损失可以执行一种形式的系统识别，推断环境参数并根据这些参数调整它如何引导代理(例如，通过调整代理的有效学习速率)。</p><p id="f98f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">元学习策略在RL领域并不新鲜，但是与传统方法相比，EPG技术确实带来了一些切实的好处。EPG方法最明显的优点之一是它避免了RL模型的局部极小值的致命弱点。由于RL方法针对短期回报进行优化，而不是考虑整个学习过程，因此它们可能会陷入局部最小值，并且无法探索整个搜索空间。EPG方法允许RL模型针对真实目标进行优化，即最终训练的政策绩效，而不是短期回报。在最初的测试中，EPG似乎通过允许损失函数适应环境和代理历史来改进标准RL算法，从而导致更快的学习和在没有外部奖励的情况下学习的潜力。</p><p id="412c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在OpenAI研究人员为了测试EPG的泛化能力而进行的有趣测试中，有一个实验专注于使用EPG损失来有效地让“蚂蚁”走到竞技场右半部分随机定位的目标。在对损失函数进行初步计算后，实验给了蚂蚁一个新的目标，这次是在竞技场的左半部分。令人惊讶的是，蚂蚁学会了向左走！这是他们的学习曲线(图上的红线)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nf"><img src="../Images/644074a18379f0892b6428f89d2a0999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9f_8OP3ToVHFVTbhyekiGQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ne">图片来源:OpenAI </strong></figcaption></figure><p id="4098" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">与传统的元学习模型相比，EPG模型实现的知识概括类型非常令人鼓舞，因为它不依赖于训练分布。OpenAI团队用Github 上提供的EPG <a class="ae lh" href="https://github.com/openai/EPG" rel="noopener ugc nofollow" target="_blank">的初始实现补充了研究论文。EPG的实现基于Python和Anaconda，这使得它可以相对简单地与其他深度学习框架一起使用。</a></p></div></div>    
</body>
</html>