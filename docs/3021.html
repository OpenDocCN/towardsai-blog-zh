<html>
<head>
<title>Differences and Connections between Natural Language Processing and Computer Vision from a Machine Learning Perspective</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从机器学习的角度看自然语言处理和计算机视觉的区别和联系</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/differences-and-connections-between-natural-language-processing-and-computer-vision-from-a-machine-993a4acf3d97?source=collection_archive---------0-----------------------#2022-08-07">https://pub.towardsai.net/differences-and-connections-between-natural-language-processing-and-computer-vision-from-a-machine-993a4acf3d97?source=collection_archive---------0-----------------------#2022-08-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a532" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">自然语言处理、计算机视觉、机器学习</h2></div><p id="a07d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我最近从计算机视觉(CV)领域转移到自然语言处理(NLP)领域，发现了一些有趣的观察结果。这两个领域非常不同，但仍有一些共同的联系。开始了。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/e8967fc98f66851c8286525d8eb6e504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djjogKOuAg0quXm5D5iyhw.jpeg"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk translated">图片来自unsplash</figcaption></figure><h1 id="5195" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">差异</h1><p id="6af3" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated"><strong class="kk iu"> 1。NLP是特定于语言的，但CV不是。</strong></p><p id="984e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不同语言的NLP可能会有很大的不同。不同的语言有不同的词汇和语法。不可能训练一个适合所有语言的ML模型。</p><p id="92a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，计算机视觉要容易得多。以行人检测为例。在美国和中国拍摄的照片通常没有明显的区别。用在中国拍摄的图像训练的ML模型通常在美国拍摄的图像上也工作得很好。</p><p id="4a55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 2。NLP标注比较难，但是CV比较容易。</strong></p><p id="ee13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于两个原因，NLP标记很复杂。</p><p id="2ced" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，你需要一个特定语言的演讲者来标记特定语言的文本。你不会期望一个说德语的人用中文标注文本。此外，如果语言人口很少，如维吾尔语或丹麦语，找到一个好的发言者为你工作可能已经很难了。更不用说怎么贴标签了。</p><p id="a62d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二，你需要理解文本，才能贴标签。以情感分析为例。你需要通读文本，理解它，然后决定它是积极的，消极的，还是中性的。如果文字比较长或者需要一些背景知识，做情感分析可能要花一些时间。有时候，你甚至需要语言学家的帮助。</p><p id="e57c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相反，计算机视觉标注就容易多了。首先，这项任务不是针对语言或国家的。例如，用于对象检测或语义分割的标记可以由任何语言使用者来完成。第二，做标注不需要成为专家。一个三年级的小学生甚至可以做物体检测或语义分割的标记。</p><p id="70ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3。NLP很大程度上依赖于规则，而CV则不然。</strong></p><p id="8573" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我开始进入这个领域时，看到规则在NLP中被大量使用，我感到非常惊讶。但是过了一段时间，我明白为什么了。</p><p id="9528" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以命名实体识别为例。您希望预测文本中单词的标签，如<em class="mr"> Loc </em>(位置)或<em class="mr"> Org </em>(组织)。你会怎么做？你会用大的训练数据来训练一个LSTM网络还是简单地用一些规则来记忆一些单词？对于一些单词，比如Google、Meta、Berlin和Paris，我会创建一些规则来预测Google和Meta为Org，Berlin和Paris为Loc。为这样的单词创建规则比训练一个ML模型要容易得多。</p><p id="7281" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在CV域中，很少使用规则。人们不指望一个基于规则的系统能够预测MNIST的数字，或者从街道图像中检测出行人。ML模型在CV域中占绝对优势。</p><p id="5a1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 4。NLP数据使用的磁盘空间较少，但CV数据占用的磁盘空间较多。</strong></p><p id="1b3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在NLP中，数据是文本。它通常保存在记事本、excel或数据库中。因为它是文本，所以占用的空间通常是几兆字节，或者最多几千兆字节。</p><p id="c505" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，CV数据通常是jpg或png图像。它们通常直接保存在文件夹中。如果你有数百万张图片，它很容易占用数十亿字节甚至更多。</p><h1 id="9fd1" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">连接</h1><p id="1271" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">尽管这两个领域有许多不同之处，但它们确实有许多相似之处。这里我将介绍两种ML技术，LSTM和变形金刚，它们在这两个领域都有应用。</p><p id="4cec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> LSTM </strong></p><p id="5569" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">时间序列问题存在于这两个领域。在自然语言处理中，存在着词性和命名实体识别的问题。在这种问题中，输入文本被逐字读入ML(例如，LSTM)模型，并且该模型预测每个输入单词的标签。在CV中，存在包括场景文本识别的问题，其中给定包含文本的图像，模型预测文本。参见下面的例子。LSTM模型读入包含文本的包围盒，将包围盒从左到右转换成切片，最后预测输出字母。</p><div class="lf lg lh li gt ab cb"><figure class="ms lj mt mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/100c65fd7d2501427ee68d5a2d3e52b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Eh4Yg6ln6DaEM1xKbmKznA.png"/></div></figure><figure class="ms lj my mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/b91f4dd033da8c0fa23c1ec8d6d9096d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*xCrlpoIIWIfSmS0P74c0CQ.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk mz di na nb translated">参考文献[1]中的图片</figcaption></figure></div><p id="e3fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">变形金刚</strong></p><p id="736a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变换器起源于机器翻译领域，也用于物体检测。DETR [2]是一个基于变压器的对象检测模型。它的好处是，它是无锚的，这意味着我们不需要仔细设计锚的规模和大小。此外，由于它是无锚点的，因此没有非最大抑制作为后处理，这通常需要微调IOU阈值和分类分数阈值。下面是DETR的工作流程。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nc"><img src="../Images/c3de1a25527a2ff937af570931572768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3CCPXuBOn9lCUEj3AzX6vA.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk translated">DETR的工作流程[2]</figcaption></figure><p id="2fef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于DETR更详细的解释，请看我之前的帖子。</p><div class="nd ne gp gr nf ng"><a href="https://medium.com/analytics-vidhya/up-detr-unsupervised-pre-training-for-object-detection-with-transformers-paper-explained-84611e27a144" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">UP-DETR:用变形金刚进行物体检测的无监督预训练(论文解释)</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">CVPR 2021口头论文</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu lo ng"/></div></div></a></div><h2 id="02d4" class="nv lv it bd lw nw nx dn ma ny nz dp me kr oa ob mg kv oc od mi kz oe of mk og bi translated">参考资料:</h2><p id="9b1e" class="pw-post-body-paragraph ki kj it kk b kl mm ju kn ko mn jx kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">[1]王凯，鲍里斯·巴本科和瑟奇·贝隆吉，端到端场景文本识别，ICCV，2011。</p><p id="9264" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2] Nicolas Carion、Francisco Massa、Gabriel Synnaeve、Nicolas Usunier、Alexander基里洛夫和Sergey Zagoruyko,《利用变压器进行端到端对象检测》, ECCV，2020年。</p></div></div>    
</body>
</html>