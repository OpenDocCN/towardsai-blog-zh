<html>
<head>
<title>Mini NLP Cypher | Mini Year Review</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迷你NLP密码|迷你年度回顾</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/mini-nlp-cypher-mini-year-review-7917e12fb2e5?source=collection_archive---------5-----------------------#2020-12-31">https://pub.towardsai.net/mini-nlp-cypher-mini-year-review-7917e12fb2e5?source=collection_archive---------5-----------------------#2020-12-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/48e92667314a9dbde0a1e0b572913ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FAcMQ1REm_VpRIB8.jpg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">弗里德里希</figcaption></figure><h2 id="62bc" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph">__init__。巴拉圭</h2><div class=""/><div class=""><h2 id="b14d" class="pw-subtitle-paragraph kl jo jf bd b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc dk translated">👋👋2020年——从未有过的一年</h2></div></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="dfff" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi mg translated">终于摆脱了2020年悲惨的12个月。我们一直戴着口罩，紧张地关注着疫苗更新的新闻。当地球静止了整整一个日历年，软件(和硬件)却在前进，而且从未停止。尽管这一年即将结束，一切都很平静，也许太平静了，但我们还是忍不住见证了微软和谷歌在永无止境的强力胶之战中再次正面交锋:</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="c121" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">微软添加了DeBERTa来取代谷歌T5在基准测试中的地位，仅12小时后就被T5 + Meena的新部署取代(什么？).🤣</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="0197" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">在Quantum Stat，我们也在不断前进。除了NLP模型的数千个<a class="ae mv" href="https://models.quantumstat.com/" rel="noopener ugc nofollow" target="_blank">推理代码片段</a>之外，我们还在库存中添加了<a class="ae mv" href="https://datasets.quantumstat.com/" rel="noopener ugc nofollow" target="_blank"> 800+数据集</a>和<a class="ae mv" href="https://notebooks.quantumstat.com/" rel="noopener ugc nofollow" target="_blank"> 300+笔记本</a>。😵感谢所有促成此事的贡献者！</p><p id="ad15" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">好，那么2021年的NLP是什么样子的？超大模型与较小压缩模型的分歧？或者预训练模型在稀疏性方面的进步如何？或者，模型小到足以自然地适应越来越接近现实的边缘又如何呢？</p><p id="5e0e" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">可能以上都有。此外，我们可能会看到图形和深度学习最终联姻。2021年将是他们的蜜月。已经有几个库已经成熟了好几年，比如<a class="ae mv" href="https://github.com/rusty1s/pytorch_geometric" rel="noopener ugc nofollow" target="_blank"> PyTorch Geometric </a>、<a class="ae mv" href="https://github.com/dmlc/dgl" rel="noopener ugc nofollow" target="_blank"> DGL </a>和DeepMind的<a class="ae mv" href="https://github.com/deepmind/graph_nets" rel="noopener ugc nofollow" target="_blank"> Graph Nets </a>。以下是他们GitHub stars这些年的成长轨迹:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/de310ee8c5d3fe3072a3fc79819dd8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uwGeXA5h3ys7AA9j.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">via <a class="ae mv" href="https://blog.paperspace.com/geometric-deep-learning-framework-comparison/" rel="noopener ugc nofollow" target="_blank"> paperspace博客</a></figcaption></figure><p id="af30" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">关于模型架构，我们也看到了一些节省内存、提高处理较长文本序列的能力和改进培训目标的替代方案。几个例子:</p><p id="c272" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><a class="ae mv" href="https://github.com/allenai/longformer" rel="noopener ugc nofollow" target="_blank">龙前</a></p><p id="a475" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><a class="ae mv" href="https://github.com/google/trax/tree/master/trax/models/reformer" rel="noopener ugc nofollow" target="_blank">重整器</a></p><p id="0f4d" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><a class="ae mv" href="https://github.com/google-research/electra" rel="noopener ugc nofollow" target="_blank">伊莱克特拉</a></p><p id="79cd" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">此外，NLP模型的领域特定的适应将继续激增。就领域而言，我指的是3个维度:语言、文本格式(Twitter文本或正式文本等。)和部门(法律或医疗保健等。)</p><p id="ebb5" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">几个例子:</p><p id="296a" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><strong class="lm jp">以语言为主的</strong> : <a class="ae mv" href="https://github.com/stefan-it/turkish-bert" rel="noopener ugc nofollow" target="_blank">贝尔图克</a>，<a class="ae mv" href="https://camembert-model.fr/" rel="noopener ugc nofollow" target="_blank">卡蒙贝尔</a>，<a class="ae mv" href="https://github.com/marcopoli/AlBERTo-it" rel="noopener ugc nofollow" target="_blank">阿尔贝托</a>，<a class="ae mv" href="https://github.com/google-research/bert/blob/master/multilingual.md" rel="noopener ugc nofollow" target="_blank">姆贝尔</a></p><p id="7dfb" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><strong class="lm jp">文本聚焦</strong> : <a class="ae mv" href="https://github.com/VinAIResearch/BERTweet" rel="noopener ugc nofollow" target="_blank">伯特威</a>，<a class="ae mv" href="https://github.com/wtma/CharBERT" rel="noopener ugc nofollow" target="_blank">夏伯特</a></p><p id="6767" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><strong class="lm jp">部门聚焦</strong> : <a class="ae mv" href="https://github.com/dmis-lab/biobert" rel="noopener ugc nofollow" target="_blank">比奥伯特，</a> <a class="ae mv" href="https://github.com/ProsusAI/finBERT" rel="noopener ugc nofollow" target="_blank">芬伯特</a>，<a class="ae mv" href="https://huggingface.co/nlpaueb/legal-bert-base-uncased" rel="noopener ugc nofollow" target="_blank">法-伯特</a></p><p id="c013" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">随着几个库的发布，推理优化成为去年的大赢家。这一重点领域将有助于继续弥合研究和企业之间的绩效差距，因此，在即将到来的一年里，这一领域将会有更大的作为。以下是一些有助于优化变压器的库:</p><p id="d3c7" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><a class="ae mv" href="https://github.com/microsoft/fastformers" rel="noopener ugc nofollow" target="_blank">快速成型器</a></p><p id="1177" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><a class="ae mv" href="https://github.com/Tencent/TurboTransformers" rel="noopener ugc nofollow" target="_blank">涡轮变压器</a></p><p id="dbab" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><a class="ae mv" href="https://github.com/sacmehta/delight" rel="noopener ugc nofollow" target="_blank">欣喜</a></p><p id="4e9a" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated"><a class="ae mv" href="https://github.com/patil-suraj/onnx_transformers" rel="noopener ugc nofollow" target="_blank"> ONNX变压器</a></p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="e4bf" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">BERT现在看起来很遥远，有这么多新的模型架构和新颖的用例，使得2020年在这种情况下变得很奇怪。</p><p id="e4ae" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">但是2021年对我们所有人来说都是好的一年。所以在那之前…</p><p id="d422" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">新年快乐🎇🎆🎇，另一边见！✌✌ 2021</p><p id="7def" class="pw-post-body-paragraph lk ll jf lm b ln lo kp lp lq lr ks ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">附注，常规NLP密码周日到达。</p></div></div>    
</body>
</html>