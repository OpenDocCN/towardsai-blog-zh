<html>
<head>
<title>Feature Importance of Data in Machine Learning with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python机器学习中数据的特征重要性</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/feature-importance-of-data-in-machine-learning-with-python-76ad3b0f5845?source=collection_archive---------1-----------------------#2021-12-04">https://pub.towardsai.net/feature-importance-of-data-in-machine-learning-with-python-76ad3b0f5845?source=collection_archive---------1-----------------------#2021-12-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="de13" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="4830" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用于预测建模的减少输入特征技术</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1e8476c4cf6315897dcff7c0296b8953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LkU7qHp87jGFqVu6"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@ralexnder?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚历克斯·丘马克</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="2231" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要素重要性是一种根据某些系数值来了解输入要素重要性的技术。这种技术可能有助于大规模数据集，有时我们需要根据相关性或使用降维技术移除输入要素。</p><h2 id="d7c3" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">涵盖的主题:</h2><p id="e6ff" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><strong class="lk jd">第一部分:</strong>特征重要性介绍</p><p id="3571" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第二节:</strong>合成数据生成</p><p id="c6c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第3节:</strong>基于特征重要性的杂质均值减少</p><p id="d3b2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第4节:</strong>基于排列的特征重要性</p><p id="234b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章将帮助机器学习的学习者，他们倾向于学习更多关于机器学习的主题。</p><p id="3846" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先想到的是，回归和分类算法的不同特征重要性系数度量是什么。</p><blockquote class="nb nc nd"><p id="293b" class="li lj ne lk b ll lm kd ln lo lp kg lq nf ls lt lu ng lw lx ly nh ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">第1节:特性重要性介绍</em> </strong></p></blockquote><p id="4908" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当我们在做预测建模项目时，了解数据的特征或维度的重要性是很重要的。我们在这个世界上有很多问题，但有了适量的数据，我们可以对数据进行分类或回归，以预测结果。例如，流失问题是许多公司关心的问题，了解早期预测流失的正确特征是任何行业的主要成就。</p><blockquote class="nb nc nd"><p id="5b04" class="li lj ne lk b ll lm kd ln lo lp kg lq nf ls lt lu ng lw lx ly nh ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">第二节:合成数据生成</em> </strong></p></blockquote><p id="3241" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有时，我们需要生成合成数据用于学习和测试。在这里，我们将用一个名为<strong class="lk jd"><em class="ne">make _ classification</em></strong>的方法生成数据及其属性。数据集将被分为训练集和测试集，没有洗牌。</p><p id="506a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，我们将使用随机森林算法来检查不同方法的特征重要性。</p><p id="f1c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">导入python示例所需的库</p><pre class="ks kt ku kv gt ni nj nk nl aw nm bi"><span id="6475" class="me mf it nj b gy nn no l np nq">import matplotlib.pyplot as plt</span><span id="4655" class="me mf it nj b gy nr no l np nq">from sklearn.datasets import make_classification<br/>from sklearn.model_selection import train_test_split</span><span id="a4bf" class="me mf it nj b gy nr no l np nq">from sklearn.ensemble import RandomForestClassifier</span><span id="1d07" class="me mf it nj b gy nr no l np nq">import time<br/>import numpy as np</span><span id="97d7" class="me mf it nj b gy nr no l np nq">import pandas as pd</span></pre><p id="9659" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，我们使用sklearn库来处理二进制分类数据和训练测试分裂方法。matplotlib库用于绘制图形。随机森林分类器来自同一个sklearn库。time和NumPy库用于计算训练的时间和数值计算。</p><pre class="ks kt ku kv gt ni nj nk nl aw nm bi"><span id="fe54" class="me mf it nj b gy nn no l np nq">X, y = make_classification(n_samples=2000,n_features=15,<br/>                           n_informative=5, n_redundant=0,<br/>                           n_repeated=0, n_classes=2,<br/>                           random_state=0, shuffle=False,<br/>)<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)</span></pre><p id="9527" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们将使用获取功能名称并使用随机森林分类器</p><pre class="ks kt ku kv gt ni nj nk nl aw nm bi"><span id="0322" class="me mf it nj b gy nn no l np nq">feature_names = [f"feature {i}" for i in range(X.shape[1])]</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/cd56831385777888119c199378a23fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*K0vxf42tVMp0MdANqBjUwA.png"/></div></figure><pre class="ks kt ku kv gt ni nj nk nl aw nm bi"><span id="abcc" class="me mf it nj b gy nn no l np nq">forest = RandomForestClassifier(random_state=0)<br/>forest.fit(X_train, y_train)</span><span id="a1ef" class="me mf it nj b gy nr no l np nq">#output:<br/>RandomForestClassifier(random_state=0)</span></pre><div class="nt nu gp gr nv nw"><a rel="noopener  ugc nofollow" target="_blank" href="/why-data-science-is-booming-e240b1a64645"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jd gy z fp ob fr fs oc fu fw jc bi translated">数据科学为何蓬勃发展？</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">对他们的企业和整个社会的宝贵贡献</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">pub.towardsai.net</p></div></div><div class="of l"><div class="og l oh oi oj of ok lb nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a rel="noopener  ugc nofollow" target="_blank" href="/are-you-switching-careers-to-data-science-and-machine-learning-5fab0b75470e"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jd gy z fp ob fr fs oc fu fw jc bi translated">你要转行做数据科学和机器学习吗？</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">分析角色和成为数据科学家的路线图</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">pub.towardsai.net</p></div></div><div class="of l"><div class="ol l oh oi oj of ok lb nw"/></div></div></a></div><blockquote class="nb nc nd"><p id="2309" class="li lj ne lk b ll lm kd ln lo lp kg lq nf ls lt lu ng lw lx ly nh ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">第3节:基于特征重要性的杂质均值减少</em> </strong></p></blockquote><p id="f5b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这种方法中，每个树内的杂质减少。基于数据的标准偏差和平均值计算特征重要性。</p><p id="b206" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用python计算特征重要性的时间</p><pre class="ks kt ku kv gt ni nj nk nl aw nm bi"><span id="a1cb" class="me mf it nj b gy nn no l np nq">start_time = time.time()<br/>importances = forest.feature_importances_<br/>std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)<br/>time_taken = time.time() - start_time</span><span id="28b1" class="me mf it nj b gy nr no l np nq">print(f"Feature importance computing time: {time_taken:.3f} seconds")</span><span id="32c1" class="me mf it nj b gy nr no l np nq">#output:<br/>Feature importance computing time: 0.057 seconds</span></pre><p id="81c8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">绘制图表以了解哪些列具有较高的特性重要性。</p><pre class="ks kt ku kv gt ni nj nk nl aw nm bi"><span id="9eab" class="me mf it nj b gy nn no l np nq">forest_importances = pd.Series(importances, index=feature_names)</span><span id="cc4a" class="me mf it nj b gy nr no l np nq">fig, ax = plt.subplots()<br/>forest_importances.plot.bar(yerr=std, ax=ax)<br/>ax.set_title("Mean and standard deviation feature importance")<br/>ax.set_ylabel("Impurity with decreased mean")<br/>fig.tight_layout()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/f3c6f258c40d08b4adef739ea9ac99d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*dQbog8grf3HUiHzhY2ofDQ.png"/></div></figure><p id="412b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到，前五个特征比其他特征更重要。</p><p id="dcc6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种高基数特性方法的一个问题是，列中的类别或类更多。为了解决这个问题，我们使用了另一种叫做特征置换的方法。</p><blockquote class="nb nc nd"><p id="705e" class="li lj ne lk b ll lm kd ln lo lp kg lq nf ls lt lu ng lw lx ly nh ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">第4节:基于排列的特征重要性</em> </strong></p></blockquote><p id="a246" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种方法用于克服前一种方法的问题，但是由于数据混洗，计算特征的时间成本很高。</p><pre class="ks kt ku kv gt ni nj nk nl aw nm bi"><span id="283e" class="me mf it nj b gy nn no l np nq">from sklearn.inspection import permutation_importance</span><span id="8f68" class="me mf it nj b gy nr no l np nq">start_time = time.time()<br/>result = permutation_importance(<br/>    forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2<br/>)<br/>elapsed_time = time.time() - start_time<br/>print(f"Feature importance computing time: {elapsed_time:.3f} seconds")</span><span id="a419" class="me mf it nj b gy nr no l np nq">forest_importances = pd.Series(result.importances_mean, index=feature_names)</span></pre><p id="9138" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们在图表上标出这些特征</p><pre class="ks kt ku kv gt ni nj nk nl aw nm bi"><span id="93ee" class="me mf it nj b gy nn no l np nq">fig, ax = plt.subplots()<br/>forest_importances.plot.bar(yerr=result.importances_std, ax=ax)<br/>ax.set_title("Permutation based method")<br/>ax.set_ylabel("Decreases in mean accuracy")<br/>fig.tight_layout()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/52666a87692d242558e84bb86ff9fcc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*CfZA2JZ0O7Tm5YVNPoweNg.png"/></div></figure><p id="ddf1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们比较这两种方法，两种方法都给出了同一列的重要特征，但相对重要性不同。</p><blockquote class="nb nc nd"><p id="6e85" class="li lj ne lk b ll lm kd ln lo lp kg lq nf ls lt lu ng lw lx ly nh ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">结论</em> </strong></p></blockquote><p id="a9bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基于排列的方法仍然比杂质方法更优选，因为偏差更小且基数低。但是在基于置换的方法中仍然存在相关特征访问的问题，使得特征的重要性降低。这个问题可以通过多重共线排列从不同的聚类中选择特征来解决。</p><p id="790d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae lh" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="64e0" class="oo mf it bd mg op oq or mj os ot ou mm ki ov kj mp kl ow km ms ko ox kp mv oy bi translated">推荐文章</h1><p id="8805" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">1.<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/8-active-learning-insights-of-python-collection-module-6c9e0cc16f6b?source=friends_link&amp;sk=4a5c9f9ad552005636ae720a658281b1">8 Python的主动学习见解收集模块</a> <br/> 2。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/numpy-linear-algebra-on-images-ed3180978cdb?source=friends_link&amp;sk=d9afa4a1206971f9b1f64862f6291ac0"> NumPy:图像上的线性代数</a>T5】3。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/pandas-dealing-with-categorical-data-7547305582ff?source=friends_link&amp;sk=11c6809f6623dd4f6dd74d43727297cf">熊猫:处理分类数据</a> <br/> 5。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/hyper-parameters-randomseachcv-and-gridsearchcv-in-machine-learning-b7d091cf56f4?source=friends_link&amp;sk=cab337083fb09601114a6e466ec59689">超参数:机器学习中的RandomSeachCV和GridSearchCV</a><br/>6。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/data-distribution-using-numpy-with-python-3b64aae6f9d6?source=friends_link&amp;sk=809e75802cbd25ddceb5f0f6496c9803">数据分发使用Numpy与Python </a> <br/> 9。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/decision-trees-vs-random-forests-in-machine-learning-be56c093b0f?source=friends_link&amp;sk=91377248a43b62fe7aeb89a69e590860">机器学习中的决策树vs随机森林</a> <br/> 10。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/standardization-in-data-preprocessing-with-python-96ae89d2f658?source=friends_link&amp;sk=f348435582e8fbb47407e9b359787e41">用Python实现数据预处理的标准化</a></p></div></div>    
</body>
</html>