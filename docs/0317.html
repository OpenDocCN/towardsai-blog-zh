<html>
<head>
<title>Understanding BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解伯特</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understanding-bert-b69ce7ad03c1?source=collection_archive---------0-----------------------#2020-02-17">https://pub.towardsai.net/understanding-bert-b69ce7ad03c1?source=collection_archive---------0-----------------------#2020-02-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/664d675a7eb6cfc9c3feddf301135f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fq5MBQTZaSL5WRal0G7b8g.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:安民在<a class="ae jd" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank">像素</a>上拍摄的照片</figcaption></figure><h2 id="7061" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><p id="9b5d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">BERT(来自变形金刚的双向编码器表示)是一篇由Google AI language发表的<a class="ae jd" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a>。与以前版本的NLP架构不同，BERT概念简单，经验丰富。它在11个自然语言处理任务上取得了最新的成果。</p><p id="488e" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">BERT优于另一种标准LM，因为它应用序列的深度双向上下文训练，这意味着它在训练时同时考虑左右上下文，而其他LM模型(如开放GPT)是单向的，每个标记只能关注注意层中的先前标记。这种限制对于句子级任务(释义)或标记级任务(命名实体识别、问答)来说是次优的，在这些任务中，从两个方向结合上下文是至关重要的。</p><h2 id="a885" class="lk ll jg bd lm ln lo dn lp lq lr dp ls kx lt lu lv lb lw lx ly lf lz ma mb jm bi translated"><strong class="ak">伯特是什么？</strong></h2><p id="790a" class="pw-post-body-paragraph km kn jg ko b kp mc kr ks kt md kv kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">在LM的早期版本(如Glove)中，我们有固定的单词嵌入，例如，对于单词“<em class="mh"> right </em>”，不管它在句子中的上下文如何，嵌入都是相同的。是指“<em class="mh">正确</em>还是“<em class="mh">正确方向</em>”？然后是ELMo(双向LSTM ),它试图通过使用左右上下文来生成嵌入来解决这个问题，但它只是连接了从左到右和从右到左的信息，这意味着表示不能同时利用左右上下文。然后BERT和它的注意力层胜过了所有以前的模型。</p><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mi"><img src="../Images/7802531920ede4ec18a0c1eb66d3eb9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8hhmXwfT3-rBzppJ7X9pRA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">预训练模型架构的差异。伯特使用双向变压器。OpenAI GPT使用从左到右的变压器。ELMo使用独立训练的从左到右和从右到左LSTMs的串联来为下游任务生成特征。在这三者中，只有BERT表示在所有层中同时受到左右上下文的影响。</figcaption></figure><p id="5dd2" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Transformer是一种流行的注意力模型，其基本架构有两个主要组件:编码器和解码器。编码器部分读取输入序列并对其进行处理，解码器部分从编码器获取已处理的输入，并对其进行重新处理以执行预测任务。要了解更多关于变压器的信息，请参考:<a class="ae jd" href="https://medium.com/@shwetabaranwal20/attention-model-transformers-cbfa754c0475" rel="noopener">这里的</a>。因为这里我们感兴趣的是生成语言模型(LM)，所以只有编码器部分是必要的。BERT使用这种变压器编码器架构为输入序列生成双向自我关注。它一口气读完整个句子，注意力层从一个单词的所有左右周围单词中学习该单词的上下文。</p><h2 id="da0e" class="lk ll jg bd lm ln lo dn lp lq lr dp ls kx lt lu lv lb lw lx ly lf lz ma mb jm bi translated"><strong class="ak">预训练伯特:</strong></h2><p id="7ec4" class="pw-post-body-paragraph km kn jg ko b kp mc kr ks kt md kv kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">BERT的预训练是在未标记的数据集上完成的，因此本质上是无监督的。BERT中有两个预训练步骤:</p><ol class=""><li id="2a3b" class="mn mo jg ko b kp kq kt ku kx mp lb mq lf mr lj ms mt mu mv bi translated"><strong class="ko jq">蒙面语言模特(MLM) </strong></li></ol><p id="d232" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">a)模型使用[MASK]标记随机屏蔽15%的标记，然后在输出层预测这些屏蔽的标记。丢失仅基于屏蔽记号的预测，而不是基于所有记号的预测。</p><p id="5807" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">b)在模型的微调期间,[MASK]令牌不出现，从而产生不匹配，为了减轻这种情况，如果在预训练期间选择第I个令牌进行屏蔽，则用以下令牌替换它:</p><p id="c71f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">80%倍[面具] token: <em class="mh">我的狗是毛→我的狗是[面具] </em></p><p id="e7e8" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">来自语料库的10%倍随机词:<em class="mh">我的狗有毛→我的狗是苹果</em></p><p id="2656" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">10%倍不变:<em class="mh">我的狗有毛→我的狗有毛</em></p><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/1a0db7c3cef77906055a9412ae222917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6MBarD0dujEwZXVFz6X1VA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-bert/</a></figcaption></figure><p id="bc37" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.<strong class="ko jq">下一句预测</strong></p><p id="91a4" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">A)在这种预训练方法中，给定两个句子A和B，无论句子是否相关，模型都在二进制化的输出上进行训练。</p><p id="4d3e" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">B)在为预训练示例选择句子A和B时，B有50%的时间是跟在A后面的实际的下一个句子(label: <em class="mh"> IsNext </em>)，有50%的时间是从语料库中随机抽取的句子(label: <em class="mh"> NotNext </em>)。</p><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mx"><img src="../Images/a17bf756de86ae309b7e5afb988f128d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dMitvrkA8J5eWYujfTlb5g.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-bert/</a></figcaption></figure><p id="3b0d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">训练损失是平均屏蔽LM可能性和平均下一句预测可能性的总和。</p><p id="3d09" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在NLP的早期工作中，只有句子嵌入被转移到下游任务，而BERT转移预训练的所有参数来初始化不同下游任务的模型。预先训练的BERT模型由Google提供，可以直接用于微调下游任务。</p><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/5663068fd9900377084da222b0fb4015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*27Y8sW-YY6AaiYexft_Z9A.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/pretrained_models.html</a></figcaption></figure><h2 id="2a35" class="lk ll jg bd lm ln lo dn lp lq lr dp ls kx lt lu lv lb lw lx ly lf lz ma mb jm bi translated"><strong class="ak">伯特建筑:</strong></h2><p id="1b05" class="pw-post-body-paragraph km kn jg ko b kp mc kr ks kt md kv kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">BERT的模型架构是基于Google的<a class="ae jd" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mh">注意力是你所需要的全部</em> </a>论文的多层双向变压器编码器。它有两种模型形式:</p><p id="c4db" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq"> BERT BASE </strong> : <em class="mh">更少的变压器块和隐藏层尺寸，具有与OpenAI GPT相同的模型尺寸。【12个变压器块，12个注意头，768个隐藏层尺寸】</em></p><p id="0ff9" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq"> BERT LARGE </strong> : <em class="mh">具有两倍于BERT基础的注意力层的巨大网络，在NLP任务上实现了最先进的结果。【24个变压器块，16个注意头，1024个隐藏层尺寸】</em></p><p id="ac30" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在模型微调期间，这些层的参数(<em class="mh">变压器块、注意头、隐藏层)</em>以及下游任务的附加层被端到端地微调。</p><h2 id="aace" class="lk ll jg bd lm ln lo dn lp lq lr dp ls kx lt lu lv lb lw lx ly lf lz ma mb jm bi translated"><strong class="ak">伯特输入表示:</strong></h2><ol class=""><li id="ef45" class="mn mo jg ko b kp mc kt md kx mz lb na lf nb lj ms mt mu mv bi translated">每个序列的第一个记号总是一个特殊的分类记号[CLS]。对应于该令牌的最终隐藏状态用于分类任务。</li><li id="6f33" class="mn mo jg ko b kp nc kt nd kx ne lb nf lf ng lj ms mt mu mv bi translated">这两个句子用[SEP]符号分隔。</li><li id="7a63" class="mn mo jg ko b kp nc kt nd kx ne lb nf lf ng lj ms mt mu mv bi translated">在句子对的情况下，添加了一个段嵌入，该段嵌入指示标记是属于句子A还是句子b。</li><li id="692f" class="mn mo jg ko b kp nc kt nd kx ne lb nf lf ng lj ms mt mu mv bi translated">对于给定的标记，其输入表示是通过添加相应的标记、段和位置嵌入来构建的。</li></ol><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/8b4dd20ab7057e6f79339161a837a23e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EKzyGf_l0e57XN491_YAyg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">BERT输入表示。输入嵌入是标记嵌入、分段嵌入和位置嵌入的总和</figcaption></figure><h2 id="6ea3" class="lk ll jg bd lm ln lo dn lp lq lr dp ls kx lt lu lv lb lw lx ly lf lz ma mb jm bi translated"><strong class="ak">微调伯特:</strong></h2><p id="1d22" class="pw-post-body-paragraph km kn jg ko b kp mc kr ks kt md kv kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">微调BERT简单明了。根据手头的任务修改模型。对于每个任务，我们只需将特定于任务的输入和输出插入到BERT中，并端到端地微调所有参数。</p><p id="4363" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在输入端，来自预训练的句子A和句子B类似于</p><ol class=""><li id="4240" class="mn mo jg ko b kp kq kt ku kx mp lb mq lf mr lj ms mt mu mv bi translated">释义中的句子对</li><li id="f8f3" class="mn mo jg ko b kp nc kt nd kx ne lb nf lf ng lj ms mt mu mv bi translated">蕴涵中的假设前提对</li><li id="b75c" class="mn mo jg ko b kp nc kt nd kx ne lb nf lf ng lj ms mt mu mv bi translated">问答中的问题-段落对</li><li id="d98d" class="mn mo jg ko b kp nc kt nd kx ne lb nf lf ng lj ms mt mu mv bi translated">文本分类或序列标记中的退化text-∅对。</li></ol><p id="3bfc" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在输出端，表征表示被馈送到输出层用于表征级任务，例如序列标记或问题回答，而[CLS]表示被馈送到输出层用于分类，例如情感分析。</p><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/584dbdc0b18c8c2f2b743a0d0bc91fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aKkeQTgmShGWUcILqTXSPw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-bert/</a></figcaption></figure><p id="0cdc" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">HuggingFace也提供了一个微调特定任务模型的框架。</p><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/490ab807279e2e4bb71bef55f986372b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5k4O9L9QfReayFQkK7MELw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/bert.html#bertforpretraining" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/transformers/model _ doc/Bert . html # bertforpreparating</a></figcaption></figure><p id="1e30" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">MaskedLM、下一句预测、序列分类、多项选择等的模型框架。以及针对BERT的预训练参数。这些实现起来既简单又有趣。</p><h2 id="2b16" class="lk ll jg bd lm ln lo dn lp lq lr dp ls kx lt lu lv lb lw lx ly lf lz ma mb jm bi translated"><strong class="ak">伯特记号赋予器:</strong></h2><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/33f73df8a1697a035e880fabbe1790c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GzkqHW-vJnjkrBEdFjgnbg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/bert.html#berttokenizer" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/transformers/model _ doc/Bert . html # bertokenizer</a></figcaption></figure><p id="9edd" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在BERT输入表示中，我们已经看到我们需要三种类型的嵌入(令牌、段、位置)。HuggingFace的Transformers包为每个嵌入需求构造了令牌(<a class="ae jd" href="https://huggingface.co/transformers/main_classes/tokenizer.html" rel="noopener ugc nofollow" target="_blank"> <em class="mh"> encode_plus </em> </a>)。这里既可以使用预先训练好的标记器，也可以使用来自给定vocab文件的标记器。</p><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/46503d0aca18e910353081353dfbd14a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dO1S3dhHQaO1hkv-DZ3USQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来自预训练“bert-base-uncased”的BERT记号化器</figcaption></figure><p id="92a9" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">BERT tokenizer使用词块模型进行标记化。它将单词分解成子单词，以增加词汇的覆盖范围。</p><p id="4c6e" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">单词</strong> : <em class="mh">喷气式飞机制造商因座位宽度争执不休，大订单岌岌可危</em></p><p id="bfcd" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">word pieces</strong>:<em class="mh">_ J et _ makers _ Fe ud _ over _ seat _ width _ with _ big _ orders _ at _ stake</em></p><p id="fdbb" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在上面的例子中，单词“<em class="mh"> Jet </em>”被拆分为两个词块“_J”和“et”，“世仇”被拆分为两个词块“_fe”和“ud”。其他单词保持为单个单词片段。“_”是添加的特殊字符，用于标记单词的开头。</p><div class="ip iq gp gr ir nm"><a href="https://github.com/ShwetaBaranwal/BERT" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jq gy z fp nr fr fs ns fu fw jp bi translated">ShwetaBaranwal/BERT</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">在GitHub上创建一个帐户，为ShwetaBaranwal/BERT开发做出贡献。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">github.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa ix nm"/></div></div></a></div><h2 id="a1d9" class="lk ll jg bd lm ln lo dn lp lq lr dp ls kx lt lu lv lb lw lx ly lf lz ma mb jm bi translated"><strong class="ak">参考文献</strong>:</h2><div class="ip iq gp gr ir nm"><a href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jq gy z fp nr fr fs ns fu fw jp bi translated">BERT -变压器2.4.1文件</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">BERT模型是在BERT:用于语言理解的深度双向转换器的预训练中提出的…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">huggingface.co</p></div></div></div></a></div><div class="ip iq gp gr ir nm"><a href="http://jalammar.github.io/illustrated-bert/" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jq gy z fp nr fr fs ns fu fw jp bi translated">有插图的伯特、埃尔莫等人(NLP如何破解迁移学习)</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">讨论:黑客新闻(98分，19条评论)，Reddit r/MachineLearning (164分，20条评论)翻译…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">jalammar.github.io</p></div></div></div></a></div><div class="ip iq gp gr ir nm"><a href="https://arxiv.org/abs/1810.04805" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jq gy z fp nr fr fs ns fu fw jp bi translated">BERT:用于语言理解的深度双向转换器的预训练</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">我们介绍了一种新的语言表示模型，称为BERT，代表双向编码器表示…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="ip iq gp gr ir nm"><a href="https://arxiv.org/abs/1609.08144" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jq gy z fp nr fr fs ns fu fw jp bi translated">谷歌的神经机器翻译系统:弥合人类和机器翻译之间的差距</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">神经机器翻译(NMT)是一个端到端的自动翻译学习方法，有潜力…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="ip iq gp gr ir nm"><a href="https://github.com/google-research/bert" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jq gy z fp nr fr fs ns fu fw jp bi translated">谷歌研究/bert</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">这是几个新模型的发布，是预处理代码改进的结果。在…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">github.com</p></div></div><div class="nv l"><div class="ob l nx ny nz nv oa ix nm"/></div></div></a></div></div></div>    
</body>
</html>