<html>
<head>
<title>Bias Matters! What’s Fairlearn, and why should I care?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">偏见很重要！什么是Fairlearn，我为什么要关心？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/bias-matters-whats-fairlearn-and-why-should-i-care-76db53ab45cc?source=collection_archive---------4-----------------------#2020-10-09">https://pub.towardsai.net/bias-matters-whats-fairlearn-and-why-should-i-care-76db53ab45cc?source=collection_archive---------4-----------------------#2020-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="64a0" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="cde7" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">提高机器学习模型的公平性。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/3933de9631bd4b0362fc32f3e48e8029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-RDTW5vMTCN-sfIWpeVyg.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">多元文化矢量作者<a class="ae le" href="https://www.vecteezy.com/free-vector/multicultural" rel="noopener ugc nofollow" target="_blank"> Vecteezy </a></figcaption></figure><p id="3319" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在过去的几年里，我们见证了机器学习的快速发展。新技术在技术性能方面表现出了巨大的进步，越来越多的公司在决策过程中依赖人工智能，或者将其作为产品的一部分。</p><p id="46f1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，随着这些进步，不公平和歧视性的结果也以同样的速度增加。在本文中，我们将讨论其中的一些问题以及减少这些问题的可能解决方案。</p><p id="5639" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文中，您将了解到:</p><ul class=""><li id="8ecf" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma mg mh mi mj bi translated">偏见的麻烦在于</li><li id="180a" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">什么是负责任的ML？</li><li id="617e" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">什么是Fairlearn？</li><li id="0905" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">定义人工智能中的公平。</li><li id="443f" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">有哪些用例？</li></ul></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="8ff2" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">偏见的麻烦在于</h1><p id="4697" class="pw-post-body-paragraph lf lg iq lh b li no ka lk ll np kd ln lo nq lq lr ls nr lu lv lw ns ly lz ma ij bi translated">对人工智能偏见的担忧并不新鲜。凯特·克劳福德在2017年NIPS 2017主题演讲中提出了她的担忧。如果想看完整视频，可以访问<a class="ae le" href="https://www.youtube.com/watch?v=fMym_BKWQzk" rel="noopener ugc nofollow" target="_blank">带偏的麻烦</a>。</p><blockquote class="nt nu nv"><p id="32a5" class="lf lg nw lh b li lj ka lk ll lm kd ln nx lp lq lr ny lt lu lv nz lx ly lz ma ij bi translated">部分问题在于，那些被培养成数据科学家、建立模型和处理数据的人，很多时候并没有与民权倡导者很好地联系在一起。<br/>——亚伦·里克</p></blockquote><p id="8a52" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">机器学习工程师在开发模型时必须注意避免复制人类过程，这在历史上是偏差的来源。</p><p id="c2a3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">2012年的一篇论文研究了人口统计对人脸识别算法性能的影响，发现它们在识别黑人、女性和30岁以下成年人的人脸时准确性较低。</p><p id="cc6f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">避免偏见需要理解非常复杂的技术和非常复杂的社会问题，从而理解<strong class="lh ja">技术-社会系统</strong>。</p><blockquote class="nt nu nv"><p id="7884" class="lf lg nw lh b li lj ka lk ll lm kd ln nx lp lq lr ny lt lu lv nz lx ly lz ma ij bi translated"><strong class="lh ja">技术-社会系统</strong>指的是不联系人类社会领域就无法定义网络的情况。<br/>——塞利纳·拉夫尔</p></blockquote></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="21db" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">什么是负责任的ML？</h1><p id="83eb" class="pw-post-body-paragraph lf lg iq lh b li no ka lk ll np kd ln lo nq lq lr ls nr lu lv lw ns ly lz ma ij bi translated">负责任的机器学习是将负责任的人工智能原则付诸实践，以确保这些系统的公平性、可解释性、隐私性和安全性。</p><p id="5d99" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里有一些公司通过与ML社区分享最佳实践和工具，帮助在负责任的AI开发方面取得进展。</p><p id="0e86" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">微软:<a class="ae le" href="https://www.microsoft.com/en-us/ai/responsible-ai" rel="noopener ugc nofollow" target="_blank">https://www.microsoft.com/en-us/ai/responsible-ai</a></p><p id="3ce5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">谷歌:<a class="ae le" href="https://ai.google/responsibilities/responsible-ai-practices/" rel="noopener ugc nofollow" target="_blank">https://ai . Google/responsible/responsible-ai-practices/</a></p><p id="019b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">https://www.h2o.ai/responsible-ai/<a class="ae le" href="https://www.h2o.ai/responsible-ai/" rel="noopener ugc nofollow" target="_blank">H2O</a></p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="93c0" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">什么是Fairlearn？</h1><p id="e412" class="pw-post-body-paragraph lf lg iq lh b li no ka lk ll np kd ln lo nq lq lr ls nr lu lv lw ns ly lz ma ij bi translated">Fairlearn是微软创建的一个开源工具包，旨在让数据科学家和机器学习工程师能够评估和改善他们的人工智能系统的公平性。</p><p id="21d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它提供了度量标准和一个交互式仪表板，以帮助评估哪些人群可能受到模型的不公平对待。此外，Fairlearn还提供算法，可以帮助减轻分类和回归模型中的不公平性。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/7c81e7db364ce6cb1b105ebd61535737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b_n-Ow1_zISziXbt2uqAyg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图片由<a class="ae le" href="https://techcommunity.microsoft.com/" rel="noopener ugc nofollow" target="_blank">https://techcommunity.microsoft.com/</a></figcaption></figure><p id="45b8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你有兴趣开始使用Fairlearn，你可以访问官方网站<a class="ae le" href="https://fairlearn.github.io/" rel="noopener ugc nofollow" target="_blank">https://fairlearn.github.io/</a>。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="d8c4" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">定义人工智能中的公平。</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/61a3dcf33a50cd954cf20b360b0bdd2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-OwRcgNsRqfT7oTZMr0PAA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图片由<a class="ae le" href="https://fairmlclass.github.io/" rel="noopener ugc nofollow" target="_blank">https://fairmlclass.github.io/</a></figcaption></figure><p id="a883" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当机器学习算法的结果没有显示出与被认为敏感的变量(如性别、种族、性取向、残疾、年龄和其他人口统计因素)的相关性时，它被认为具有<strong class="lh ja">公平性</strong>。</p><p id="00b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在我们已经定义了公平的概念，我们如何识别一个人工智能系统是否表现不公平？</p><p id="b545" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在Fairlearn中，当有人受到伤害时，就会出现不公平行为。它关注两种类型的伤害:</p><ul class=""><li id="48c1" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma mg mh mi mj bi translated"><strong class="lh ja"> <em class="nw">分配危害:</em> </strong>当机会或资源因算法干预而被不公平分配时发生。工作申请、学校录取和贷款都可能发生这种情况。</li><li id="b195" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><strong class="lh ja"> <em class="nw">服务质量危害:</em> </strong>当机会、资源或信息同样可用，但对一个人的效果不如对另一个人时，就会出现这种情况。</li></ul><p id="52e2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Fairlearn使用<strong class="lh ja">群体公平</strong>的概念，试图识别<strong class="lh ja"> <em class="nw">有遭受伤害风险的个体群体。</em> </strong></p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="9208" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">有哪些用例？</h1><p id="0b60" class="pw-post-body-paragraph lf lg iq lh b li no ka lk ll np kd ln lo nq lq lr ls nr lu lv lw ns ly lz ma ij bi translated">人工智能的发展为改善全球人民的生活创造了新的机会，然而，过去的几年表明，我们在确保公平方面还有很长的路要走。一些由人工智能驱动且易受歧视行为影响的系统有:</p><ul class=""><li id="ccd7" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma mg mh mi mj bi translated"><strong class="lh ja">自动资格系统:</strong>当租赁和销售市场存在对少数种族和少数民族的歧视时。</li><li id="1fb8" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><strong class="lh ja">排名算法:</strong>当结果排名较高时，即使它们不是最相关的。</li><li id="94f9" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><strong class="lh ja">贷款的预测风险模型:</strong>当来自某些社区的贷款申请人没有资格获得贷款时。</li><li id="229f" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><strong class="lh ja">欺诈系统的风险评分模型:</strong>当欺诈防范系统标记或拒绝来自贫困或少数族裔社区的持卡人的合法交易，而不是来自富裕的白人社区的持卡人。</li><li id="d4f9" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">学校招生:当招生显示出对女性申请者有偏见的误导模式时。</li></ul></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="4911" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">最后的想法</h1><p id="15ea" class="pw-post-body-paragraph lf lg iq lh b li no ka lk ll np kd ln lo nq lq lr ls nr lu lv lw ns ly lz ma ij bi translated">将人工智能创新与人权和伦理联系起来需要参与性，从数据科学家和机器学习工程师开始，接受对每个人都公平和包容的实践，以实现以人为中心的机器学习方法。</p><p id="e995" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你喜欢这篇文章，请分享给你的队友和同事。让我们通过传播知识来帮助建立一个更加公平的世界。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="41dd" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">参考</h1><ul class=""><li id="3521" class="mb mc iq lh b li no ll np lo oc ls od lw oe ma mg mh mi mj bi translated"><strong class="lh ja">偏见的麻烦</strong> [ <a class="ae le" href="https://www.youtube.com/watch?v=fMym_BKWQzk" rel="noopener ugc nofollow" target="_blank">链接</a> ] <br/>克劳福德，k，2017。神经信息处理系统会议，特邀演讲人。</li><li id="ab9c" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><strong class="lh ja">作为技术-社会系统的网络:网络3.0的出现</strong>【<a class="ae le" href="http://fuchs.uti.at/wp-content/uploads/web3.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>】塞利纳·拉夫尔等人，2008。萨尔茨堡大学。</li><li id="a87c" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><strong class="lh ja"> Fairlearn:一个用于评估和提高人工智能公平性的工具包</strong>【<a class="ae le" href="https://www.microsoft.com/en-us/research/uploads/prod/2020/05/Fairlearn_whitepaper.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>】莎拉伯德等人，2020。微软。</li><li id="239a" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><strong class="lh ja">人脸识别绩效:人口统计学信息的作用</strong>【链接】Brendan F. Klare等人，2012。</li><li id="8bd8" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">研究生入学中的性别偏见:来自伯克利的数据，P. J .比克尔等人，1975年。科学。</li></ul></div></div>    
</body>
</html>