<html>
<head>
<title>How Linear Regression Actually work (Maths In-depth Intuition)- Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归如何实际工作(数学深度直觉)-第2部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-linear-regression-actually-work-maths-in-depth-intuition-part-2-c49a8db03013?source=collection_archive---------1-----------------------#2021-11-23">https://pub.towardsai.net/how-linear-regression-actually-work-maths-in-depth-intuition-part-2-c49a8db03013?source=collection_archive---------1-----------------------#2021-11-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e4a1" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/cd67685ce668c5c8a4f085b3d1bbd48f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U7cvBr3LJDF_VrTBb3eekQ.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@hudsoncrafted?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">王思然·哈德森</a>在<a class="ae ko" href="https://unsplash.com/s/photos/pen-paper-computer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="c05a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">欢迎来到第2部分“线性回归实际上是如何工作的(数学深度直觉)”……在<a class="ae ko" rel="noopener ugc nofollow" target="_blank" href="/how-linear-regression-actually-work-maths-in-depth-intuition-93530b1ad071">上一篇文章</a>中，我们已经了解了实现线性回归需要知道的一切，在本文中，我们将学习最重要的主题，称为“<strong class="kr jd">梯度下降算法</strong>”，我们将仅使用NumPy和明显的数学在python中实现线性回归算法。</p><p id="c250" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">我们将在本文中讨论的内容</strong></p><ul class=""><li id="36da" class="ln lo it kr b ks kt kw kx la lp le lq li lr lm ls lt lu lv bi translated">线性回归的误差是多少？</li><li id="b586" class="ln lo it kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv bi translated">什么是梯度下降算法(数学解释)…</li></ul><p id="aac0" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在阅读本文之前，我强烈建议您阅读本文的第1部分，这样我们就能达成共识。</p><blockquote class="mb"><p id="75f1" class="mc md it bd me mf mg mh mi mj mk lm dk translated">我们开始吧</p></blockquote><blockquote class="ml mm mn"><p id="ec39" class="kp kq mo kr b ks mp ku kv kw mq ky kz mr ms lc ld mt mu lg lh mv mw lk ll lm im bi translated"><strong class="kr jd">什么是线性回归中的误差？</strong></p></blockquote><figure class="my mz na nb gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mx"><img src="../Images/08e4bd4a62a1693a32b6d0c7e2d19d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*983ojUI6YnA7VXVT7-iPYA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">实际值(y)和预测值(y’)</figcaption></figure><p id="831c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以在我们之前的文章中，我们已经用公式预测了y '的值</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="ae33" class="nh ni it nd b gy nj nk l nl nm">y = mx + c</span></pre><p id="2de1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">画出了实际值(y)和预测值(y’)之间的图表，我们得到了这种图表</p><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/72c54a7bf22bf7cab18798ef8bb2f660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*x2dqc7LZkEMpim9UfIVHJg.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">y和y '之间的图形</figcaption></figure><p id="f92b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如我们可以看到的，我们的预测值(y’)不同于实际值(y)，所以我们的任务是减少实际值和预测值之间的距离。</p><blockquote class="ml mm mn"><p id="6b06" class="kp kq mo kr b ks kt ku kv kw kx ky kz mr lb lc ld mt lf lg lh mv lj lk ll lm im bi translated"><strong class="kr jd">我们先来了解一下什么是损失函数？</strong></p></blockquote><p id="0f0c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">线性回归最常用的损失函数是最小二乘误差，其代价函数也称为均方误差(MSE)。</p><p id="6f4b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">那么我们如何找到<strong class="kr jd"> MSE </strong></p><ul class=""><li id="05ea" class="ln lo it kr b ks kt kw kx la lp le lq li lr lm ls lt lu lv bi translated">找出实际值(y)和预测值(y’)之间的差异</li><li id="4b2a" class="ln lo it kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv bi translated">求差额的平方</li><li id="b9bf" class="ln lo it kr b ks lw kw lx la ly le lz li ma lm ls lt lu lv bi translated">求平均值</li></ul><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e4ed2d90262214fdef44d4a386a07a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*2bbebMl3hQhChGMmdapm2g.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">均方误差</figcaption></figure><blockquote class="ml mm mn"><p id="7e91" class="kp kq mo kr b ks kt ku kv kw kx ky kz mr lb lc ld mt lf lg lh mv lj lk ll lm im bi translated"><strong class="kr jd">什么是梯度下降算法(数学解释)</strong></p></blockquote><p id="3884" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">梯度下降是机器学习中使用的迭代优化算法，用于最小化损失函数。损失函数描述了在给定当前参数集(权重和偏差)的情况下模型的表现，梯度下降用于找到最佳参数集</p><blockquote class="ml mm mn"><p id="00ef" class="kp kq mo kr b ks kt ku kv kw kx ky kz mr lb lc ld mt lf lg lh mv lj lk ll lm im bi translated">数学上我们需要做什么？</p></blockquote><p id="87e8" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">计算损失函数相对于‘m’和‘c’的<strong class="kr jd">偏导数，并更新数值。</strong></p><ul class=""><li id="004f" class="ln lo it kr b ks kt kw kx la lp le lq li lr lm ls lt lu lv bi translated">损失函数关于“m”的偏导数</li></ul><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi np"><img src="../Images/34328efded83090b257aa278df813e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*RTIdrAY5erAGtOgIYK5rYw.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">偏导数</figcaption></figure><ul class=""><li id="a7c6" class="ln lo it kr b ks kt kw kx la lp le lq li lr lm ls lt lu lv bi translated">损失函数关于“c”的偏导数</li></ul><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/71975c4b30e3cb3f3c56f9ec9d0fa1b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*CXJkJw6oLwzuIEAcaXZpXQ.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">偏导数</figcaption></figure><p id="5cf3" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，在得到“Dm”(偏导数w.r.t 'm ')和“Dc”(偏导数w.r.t 'c ')的值后，我们只需更新变量“m”和“c”</p><p id="c1c5" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以最初m=0，c=0，学习率(l)</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="6598" class="nh ni it nd b gy nj nk l nl nm">m = m - l * Dm<br/>c = c - l * Dc</span></pre><p id="2b2b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">基本上，这就是我们实现梯度下降算法所需要的。</p><p id="3f56" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们对目前为止所学的内容进行编码，我将使用一个假数据集进行演示。</p><p id="5c24" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以从我的<a class="ae ko" href="https://github.com/iamhimanshu0/Ml_Linear_Regression_Gradient_Descent" rel="noopener ugc nofollow" target="_blank"> Github </a>下载数据集和代码</p><p id="5705" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">导入库</strong></p><figure class="my mz na nb gt kd"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="2bfd" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">预处理输入数据并将数据分成因变量(y)和自变量(x)</p><figure class="my mz na nb gt kd"><div class="bz fp l di"><div class="nr ns l"/></div></figure><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a29b6966b99d137be200d4bb87db235d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*BraVe6Sh8L4P4IYKm9sPNA.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">数据分布</figcaption></figure><p id="883b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">寻找(x-xi)、(y-yi)、(x-xi)和(y-yi)的值，如果这些变量没有任何意义，请查看<a class="ae ko" rel="noopener ugc nofollow" target="_blank" href="/how-linear-regression-actually-work-maths-in-depth-intuition-93530b1ad071">以前的文章</a>。</p><figure class="my mz na nb gt kd"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="44f0" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">寻找m和c的值</strong></p><figure class="my mz na nb gt kd"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="a2ac" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">让我们绘制初始回归线</strong></p><figure class="my mz na nb gt kd"><div class="bz fp l di"><div class="nr ns l"/></div></figure><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/92b012aa2107e38e5e3448ba9ee54920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*SvnBSU4r-qCxL44xjt0Org.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">初始回归线</figcaption></figure><p id="f6b8" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们执行<strong class="kr jd">梯度下降法并初始化数值</strong></p><figure class="my mz na nb gt kd"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="03bf" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">梯度计算</strong></p><figure class="my mz na nb gt kd"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="2046" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">预测并可视化梯度计算后的回归线</strong></p><figure class="my mz na nb gt kd"><div class="bz fp l di"><div class="nr ns l"/></div></figure><figure class="my mz na nb gt kd gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1ba553d2a35e3fa0d9f902250d067496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*RVpRIfU4QbTpKgd8wXBNvQ.png"/></div></figure><p id="6867" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">那么让我们比较一下结果</strong></p><div class="my mz na nb gt ab cb"><figure class="nu kd nv nw nx ny nz paragraph-image"><img src="../Images/92b012aa2107e38e5e3448ba9ee54920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*SvnBSU4r-qCxL44xjt0Org.png"/></figure><figure class="nu kd nv nw nx ny nz paragraph-image"><img src="../Images/1ba553d2a35e3fa0d9f902250d067496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*RVpRIfU4QbTpKgd8wXBNvQ.png"/></figure></div><p id="dda7" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如我们可以看到的，我们的回归线略有移动，并猜测该数据集的最佳拟合线</p><p id="37ff" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以我希望这能帮助你理解线性回归实际上是如何工作的，以及我们如何应用梯度下降算法来解决回归问题。</p><p id="935c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">就这样吧，下一篇文章再见。</p><p id="5980" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">感谢阅读！</strong></p></div></div>    
</body>
</html>