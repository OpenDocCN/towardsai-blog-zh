<html>
<head>
<title>Mathematical Intuition behind the Gradient Descent Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降算法背后的数学直觉</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/mathematical-intuition-behind-the-gradient-descent-algorithm-143a051c3fa9?source=collection_archive---------3-----------------------#2022-10-24">https://pub.towardsai.net/mathematical-intuition-behind-the-gradient-descent-algorithm-143a051c3fa9?source=collection_archive---------3-----------------------#2022-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c8894d2b322a85b6bb44a96a934daa4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1OsPgY9vuwuHOxUSOsmzQA.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=6784617" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的Gerd Altmann 提供</figcaption></figure><div class=""/><div class=""><h2 id="bf71" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">均方误差的梯度下降算法</h2></div><p id="9201" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">作者:</strong> <a class="ae jd" href="https://www.linkedin.com/in/pratik-shukla28/" rel="noopener ugc nofollow" target="_blank">普拉蒂克·舒克拉</a></p><blockquote class="lr"><p id="ddb1" class="ls lt jg bd lu lv lw lx ly lz ma lq dk translated">"头脑不是一个需要被填满的容器，而是一团需要被点燃的火."— <a class="ae jd" href="https://en.wikipedia.org/wiki/Plutarch" rel="noopener ugc nofollow" target="_blank">普鲁塔克</a></p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="46be" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">梯度下降系列博客:</h2><ol class=""><li id="de36" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated"><a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/the-gradient-descent-algorithm-defddd1d312e">梯度下降算法</a></li><li id="8aa3" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/mathematical-intuition-behind-the-gradient-descent-algorithm-143a051c3fa9">梯度下降算法背后的数学直觉</a>(你来了！)</li><li id="10ef" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/the-gradient-descent-algorithm-and-its-variants-e0915796dbf2">梯度下降算法&amp;及其变种</a></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="05a5" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">目录:</h2><ol class=""><li id="a779" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated"><a class="ae jd" href="#e135" rel="noopener ugc nofollow">简介</a></li><li id="33fe" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#a99a" rel="noopener ugc nofollow">均方误差梯度下降算法的推导</a></li><li id="bfd5" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#23a9" rel="noopener ugc nofollow">梯度下降算法的工作示例</a></li><li id="d764" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#dc1b" rel="noopener ugc nofollow">结束注释</a></li><li id="056e" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#3080" rel="noopener ugc nofollow">参考资料和资源</a></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="e135" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">简介:</h2><p id="8b2a" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">欢迎光临！今天，我们正致力于发展一种强大的数学直觉，关于梯度下降算法如何找到其参数的最佳值。拥有这种感觉可以帮助你抓住机器学习输出中的错误，并对梯度下降算法如何使机器学习变得如此强大感到更加舒服。在接下来的几页中，我们将推导均方误差函数的梯度下降算法的方程。我们将使用本博客的结果来编写梯度下降算法。让我们开始吧！</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="a99a" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">均方误差梯度下降算法的推导；</h2><h2 id="1d64" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">1.第一步:</h2><p id="408f" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">输入数据显示在下面的矩阵中。在这里，我们可以观察到有<strong class="kx jh"> <em class="nu"> m </em> </strong>个训练例子和<strong class="kx jh"> <em class="nu"> n </em> </strong>个特征。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4721ea13d5fbad11bb71e9521cfc2878.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*lU8vZWRfh52PuAKwP3olsg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图1:输入特征</figcaption></figure><blockquote class="oa ob oc"><p id="11da" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong> X = (m，n)</p></blockquote><h2 id="5376" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">2.第二步:</h2><p id="5a9b" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">预期输出矩阵如下所示。我们的预期输出矩阵的大小将是<strong class="kx jh"> <em class="nu"> m*1 </em> </strong>，因为我们有<strong class="kx jh"> <em class="nu"> m </em> </strong>个训练示例。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi og"><img src="../Images/fdf223f30bb72f7b6283748cf6d98cea.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*fRsvMUopHPJOb_POki5zMg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图2:预期的输出</figcaption></figure><blockquote class="oa ob oc"><p id="1d69" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong> Y = (m，1)</p></blockquote><h2 id="39f5" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">3.第三步:</h2><p id="f148" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们将在要训练的参数中添加一个偏差元素。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d6e21d86e3c62475f9d61ba81b862310.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/format:webp/1*sflcYpHY5v0BrXMyhBbv6A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图3:偏置元件</figcaption></figure><blockquote class="oa ob oc"><p id="cfa5" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong> α = (1，1)</p></blockquote><h2 id="d9c5" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">4.第四步:</h2><p id="fb19" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">在我们的参数中，我们有权重矩阵。权重矩阵将具有<strong class="kx jh"> <em class="nu"> n </em> </strong>个元素。这里，<strong class="kx jh"> <em class="nu"> n </em> </strong>是我们训练数据集的特征数。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/19b58e23960596188c08ccbb7ab73fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*z0-F--JyLKyqSigfmwXgIA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图4:输入的权重</figcaption></figure><blockquote class="oa ob oc"><p id="98f4" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维数:</strong> β = (1，n)</p></blockquote><h2 id="c5f3" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">5.第五步:</h2><figure class="nw nx ny nz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/61bb5c2d074e2a2e501d299bad115d86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v7oR4EkpqGiiz3IXF5EmNg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图5:简单线性回归中的正向传播</figcaption></figure><p id="449c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个训练示例的预测值由下式给出:</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f28dbe1720745afa637c2d4f9dd61de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*YxqK_4DqSd2IQIbLsurMaQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图6:预测值</figcaption></figure><p id="56ca" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，我们正在对权重矩阵(β)进行转置，以使维度符合矩阵乘法规则。</p><blockquote class="oa ob oc"><p id="5f7d" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong>预测值= (1，1) + (m，n) * (1，n)</p><p id="73e3" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated">—对权重矩阵(β)进行转置</p><p id="9553" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong>预测值= (1，1) + (m，n) * (n，1) = (m，1)</p></blockquote><h2 id="9576" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">6.第六步:</h2><p id="827c" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">均方误差定义如下。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/3aa23afaa6bd161c1500ae4e7bce8815.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*nGvcTnyQCFJIYnwuXinlww.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图7:成本函数</figcaption></figure><blockquote class="oa ob oc"><p id="7101" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong>成本=标量函数</p></blockquote><h2 id="3954" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">7.第七步:</h2><p id="f30b" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们将使用下面的梯度下降规则来确定这种情况下的最佳参数。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi om"><img src="../Images/f43a0bda430b5fddbc3488c779912559.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*yzVFACxEKSdRN8_jNQqZxA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图8:使用梯度下降算法更新参数</figcaption></figure><blockquote class="oa ob oc"><p id="5dc8" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong> α = (1，1) &amp; β = (1，n)</p></blockquote><h2 id="2c2b" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">8.第8步:</h2><p id="1d24" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，我们来求代价函数关于偏差元素(<strong class="kx jh"> <em class="nu"> α </em> </strong>)的偏导数。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/8e546c8260b513581fef7581883bfabd.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*D_sGwXtH-MZY4H-T8g3ICg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图9:成本函数相对于偏差的偏导数</figcaption></figure><blockquote class="oa ob oc"><p id="4b6d" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong> (1，1)</p></blockquote><h2 id="6453" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">9.第九步:</h2><p id="2aa0" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，我们正试图简化上述方程，以找到偏导数。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/1570565a682b65f3766a802909c71288.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*qKQ6R1ys_WkNHgXdycJoeQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图10:简化计算</figcaption></figure><blockquote class="oa ob oc"><p id="bb21" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong> u = (m，1)</p></blockquote><h2 id="4772" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">10.第十步:</h2><p id="06f6" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">基于<a class="ae jd" href="#6453" rel="noopener ugc nofollow"> Step — 9 </a>，我们可以将成本函数写成:</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi op"><img src="../Images/550878895c730fa8d660ee35b1a54923.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*9KKmTMOxA4CFYdqRwdniLA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图11:成本函数</figcaption></figure><blockquote class="oa ob oc"><p id="5c76" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong>标量函数</p></blockquote><h2 id="d650" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">11.第11步:</h2><p id="467e" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将使用链式法则来计算成本函数相对于截距(<strong class="kx jh"> <em class="nu"> α </em> </strong>)的偏导数。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/7cd1ceda553049682c702c82cf06fa7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*AOz3hM8P7M9ryS72DTNLgg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图12:寻找成本函数相对于偏差的偏导数</figcaption></figure><blockquote class="oa ob oc"><p id="f1de" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong>(米，1)</p></blockquote><h2 id="bc41" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">12.第12步:</h2><p id="5e9a" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们正在计算<a class="ae jd" href="#d650" rel="noopener ugc nofollow"> Step — 11 </a>的偏导数的第一部分。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi or"><img src="../Images/d07ebea349b9fc99d7242a4441525af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*B_003FSCdtk7Bk7ctnbiuQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图13:寻找成本函数的偏导数</figcaption></figure><blockquote class="oa ob oc"><p id="603a" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong> (m，1)</p></blockquote><h2 id="5fad" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">13.第十三步:</h2><p id="a16a" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们计算<a class="ae jd" href="#d650" rel="noopener ugc nofollow">步骤的偏导数的第二部分— 11 </a>。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi os"><img src="../Images/2ca73460174077854067bce8725b20ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*IbTxwyCLQrUD65MgaybXlQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图-14:寻找紫外线偏差的偏导数</figcaption></figure><blockquote class="oa ob oc"><p id="a106" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong>标量函数</p></blockquote><h2 id="bef0" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">14.第14步:</h2><p id="e295" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将<a class="ae jd" href="#bc41" rel="noopener ugc nofollow">步骤— 12 </a>和<a class="ae jd" href="#5fad" rel="noopener ugc nofollow">步骤— 13 </a>的结果相乘，得到最终结果。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/2aaeab19b821b7cf7f7ed13846e7fbec.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*AY9VrdGKSrK7O3IWcVsRbQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图15:寻找成本函数相对于偏差的偏导数</figcaption></figure><blockquote class="oa ob oc"><p id="3bea" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong>(米，1)</p></blockquote><h2 id="aedf" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">15.步骤15:</h2><p id="09ee" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将使用链式法则来计算成本函数相对于权重的偏导数(<strong class="kx jh"> <em class="nu"> β </em> </strong>)。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/9845084981cafd45d08b245f115f9994.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*UorQBxbMGW0U-UBIDH7bBQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图16:寻找成本函数相对于重量的偏导数</figcaption></figure><blockquote class="oa ob oc"><p id="6197" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong> (1，n)</p></blockquote><h2 id="4746" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">16.第16步:</h2><p id="6b44" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们计算<a class="ae jd" href="#aedf" rel="noopener ugc nofollow">步骤— 15 </a>的偏导数的第二部分。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ov"><img src="../Images/fc00410e356d64021d2fae9bbf05f60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*cOizwlssMLx4Rzz7JOnECg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图-17:寻找紫外线重量的偏导数</figcaption></figure><blockquote class="oa ob oc"><p id="7797" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong> (m，n)</p></blockquote><h2 id="c0a8" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">17.第17步:</h2><p id="450c" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将<a class="ae jd" href="#bc41" rel="noopener ugc nofollow"> Step — 12 </a>和<a class="ae jd" href="#4746" rel="noopener ugc nofollow"> Step — 16 </a>的结果相乘，得到偏导数的最终结果。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/860e171771dd752d9a800cb8e8c685ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*C9LC6dmbVsm007Lwmi3qlg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图18:寻找成本函数相对于重量的偏导数</figcaption></figure><p id="235c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，既然我们想要有<strong class="kx jh"> <em class="nu"> n </em> </strong>个权重值，我们就从上面的等式中去掉求和部分。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/b3c7b18f06c59ba28789a81955c40fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*Wsj3CY6hIgCQhqGXQFo0Iw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图19:寻找成本函数相对于重量的偏导数</figcaption></figure><p id="1c4a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，这里我们将不得不转置计算的第一部分，以使其符合矩阵乘法规则。</p><blockquote class="oa ob oc"><p id="2859" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">尺寸:</strong> (m，1) * (m，n)</p><p id="2918" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated">—对误差部分进行转置—</p><p id="45bf" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong> (1，m) * (m，n) = (1，n)</p></blockquote><h2 id="6696" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">18.第18步:</h2><p id="cf8b" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将所有计算的值放入<a class="ae jd" href="#3954" rel="noopener ugc nofollow"> Step — 7 </a>中，以计算用于更新<strong class="kx jh"> α的梯度规则。</strong></p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/0d6ffcca162f83cf0933be0e20809a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*2AMDVChWSsnBFZvXhM01CA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图20:使用梯度下降更新偏差</figcaption></figure><blockquote class="oa ob oc"><p id="c967" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong> α = (1，1)</p></blockquote><h2 id="d352" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">19.第19步:</h2><p id="c6c8" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将所有计算出的值放入<a class="ae jd" href="#3954" rel="noopener ugc nofollow"> Step — 7 </a>中，计算出更新<strong class="kx jh"> <em class="nu"> β </em> </strong>的梯度规则。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/b078f73991a0b39aac45f173b54b1d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*LOpzgk-Y9lOaDwkxuK7jUg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图21:使用梯度下降更新权重</figcaption></figure><p id="35e4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，我们必须转置误差值，以使函数与矩阵乘法规则兼容。</p><blockquote class="oa ob oc"><p id="79d6" class="kv kw nu kx b ky kz kh la lb lc kk ld od lf lg lh oe lj lk ll of ln lo lp lq ij bi translated"><strong class="kx jh">维度:</strong> β = (1，n) — (1，n) = (1，n)</p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="23a9" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">梯度下降算法的工作示例:</h2><p id="e4b3" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，我们举个例子，看看梯度下降算法是如何找到最佳参数值的。</p><h2 id="a45a" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">1.第一步:</h2><p id="8c0b" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">输入数据显示在下面的矩阵中。在这里，我们可以观察到有<strong class="kx jh"> <em class="nu"> 4 </em> </strong>的训练实例和<strong class="kx jh"> <em class="nu"> 2 </em> </strong>的特点。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/70a0360f17499c580d666e73f41eeef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*is1vO503YO0QRbA7OERTTQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图22:输入矩阵</figcaption></figure><h2 id="e55f" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">2.第二步:</h2><p id="72ec" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">预期输出矩阵如下所示。我们预期的输出矩阵大小为<strong class="kx jh"> <em class="nu"> 4*1 </em> </strong>，因为我们有<strong class="kx jh"> <em class="nu"> 4 </em> </strong>个训练示例。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi op"><img src="../Images/bf9cc8309722fd61851be6f30f55886c.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*wEOOKkYOuAbhRGCP3fm48A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图23:预期的输出</figcaption></figure><h2 id="1048" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">3.第三步:</h2><p id="e387" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们将在要训练的参数中添加一个偏差元素。这里，我们选择0作为偏差的初始值。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/122e27af5e6aab3d72c79f0e4c1134c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*ZUSC0EQU4gm2mshwRwcAbg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图24:偏置元件</figcaption></figure><h2 id="69a1" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">4.第四步:</h2><p id="ede1" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">在我们的参数中，我们有权重矩阵。权重矩阵将有2个元素。这里，2是我们的训练数据集的特征数量。最初，我们可以为权重矩阵选择任意随机数。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/c16c71a90df61510d3fddee89df7d2bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*KUNVExsHn8LZt6g84uN_1A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图25:权重矩阵</figcaption></figure><h2 id="81f3" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">5.第五步:</h2><p id="13e3" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将使用输入矩阵、权重矩阵和偏差来预测值。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pd"><img src="../Images/6dcbc836ca411028f86227c8230f2780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*7FYZzWCw7guSuKl1WvX_Bw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图26:预测值</figcaption></figure><h2 id="be22" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">6.第六步:</h2><p id="7102" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们使用下面的等式计算成本。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/dd26f0235b0eeb083315fb68328528a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*3BvU7wSnXvqg5q0LrJBkvw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图27:在预测中计算成本</figcaption></figure><h2 id="3ced" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">7.第七步:</h2><p id="3846" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们计算成本函数相对于偏差元素的偏导数。我们将在梯度下降算法中使用这个结果来更新偏差参数的值。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/492479b978e5e826531f6c053edb6d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*aaZ7qXfLKzLPLx07SiwL6Q.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图28:成本函数相对于偏差元素的偏导数</figcaption></figure><h2 id="de31" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">8.第8步:</h2><p id="7892" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们计算成本函数相对于权重矩阵的偏导数。我们将在梯度下降算法中使用这个结果来更新权重矩阵的值。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/74e56af8d95ed23a67216c744ad92840.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*c_aIzriQhg0SS18DL_xx5A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图29:成本函数相对于权重矩阵的偏导数</figcaption></figure><h2 id="a0a9" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">9.第九步:</h2><p id="de0b" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们定义学习率的值。学习率是控制模型学习速度的参数。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/49c4df6a8b894664464f7869f8ef8430.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*BMq13d_g3J7_YiVZ6KhDjg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图30:学习率</figcaption></figure><h2 id="d9d5" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">10.第十步:</h2><p id="26e0" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们使用梯度下降规则来更新bias元素的参数值。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pi"><img src="../Images/4dbd6d5cb69d8addbd9c6331b4e9752a.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*mGg8EiF30XDxuuD1mDHBiQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图-31:使用梯度下降算法更新偏差元素的值</figcaption></figure><h2 id="f5d9" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">11.第11步:</h2><p id="4266" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们使用梯度下降规则来更新权重矩阵的参数值。</p><figure class="nw nx ny nz gt is gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/5db40640e5851d5d14b42c886a21d740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*1O19AdxEkchpvmFIoi5EHA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图32:使用梯度下降算法更新权重矩阵的值</figcaption></figure><h2 id="197d" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">12.第12步:</h2><p id="18bf" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，我们重复这个过程多次，以找到我们模型的最佳参数。在每次迭代中，我们使用参数的更新值。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="d560" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">结束注释:</h2><p id="2283" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">这就是我们如何使用均方误差的梯度下降算法找到更新的规则。我们希望这引发了你的好奇心，让你渴望更多的机器学习知识。在以后的博客中，我们将使用我们在这里得到的规则来实现梯度下降算法，所以不要错过梯度下降系列的第三部分，这是它的大结局！</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><figure class="nw nx ny nz gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/pratu"><div class="gh gi pk"><img src="../Images/bf19b95960da2e6db6fc0dd7ba17c7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/0*jR9Jyfl35goV8nO-.png"/></div></a><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">给普拉蒂克买杯咖啡！</figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="90cb" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">引用:</h2><p id="57d0" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">对于学术背景下的归属，请引用该工作为:</p><pre class="nw nx ny nz gt pl pm pn po aw pp bi"><span id="492a" class="mi mj jg pm b gy pq pr l ps pt">Shukla, et al., “Mathematical Intuition behind the Gradient Descent Algorithm”, Towards AI, 2022</span></pre></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="24b6" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">BibTex引文:</h2><pre class="nw nx ny nz gt pl pm pn po aw pp bi"><span id="99e5" class="mi mj jg pm b gy pq pr l ps pt">@article{pratik_2022, <br/> title={Mathematical Intuition behind the Gradient Descent Algorithm}, <br/> url={<a class="ae jd" href="https://towardsai.net/p/l/mathematical-intuition-behind-the-gradient-descent-algorithm" rel="noopener ugc nofollow" target="_blank">https://towardsai.net/p/l/mathematical-intuition-behind-the-gradient-descent-algorithm</a>}, <br/> journal={Towards AI}, <br/> publisher={Towards AI Co.}, <br/> author={Pratik, Shukla},<br/> editor={Lauren, Keegan},  <br/> year={2022}, <br/> month={Oct}<br/>}</span></pre></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="3080" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">参考资料和资源:</h2><ol class=""><li id="9349" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降—维基百科</a></li></ol></div></div>    
</body>
</html>