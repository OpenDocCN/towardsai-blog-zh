<html>
<head>
<title>Checking the Sentiment of a Tweet Using Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习检查推文的情感</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/checking-the-sentiment-of-a-tweet-using-machine-learning-9492b7e4aa67?source=collection_archive---------4-----------------------#2020-04-19">https://pub.towardsai.net/checking-the-sentiment-of-a-tweet-using-machine-learning-9492b7e4aa67?source=collection_archive---------4-----------------------#2020-04-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/593d5677b37d052df2ba95e661cb28f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdzMnTpQjvftm10xe7DGxw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图片来源:Unsplash</figcaption></figure><div class=""/><div class=""><h2 id="3794" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">让我们看看一些推文，并将它们归类为积极或消极情绪</h2></div><p id="4e82" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi lq translated">当我们听到Twitter这个词时，我们会想到什么？对一些人来说，它是一个信息来源，人们可以在那里即时获得新闻；对一些人来说，它是一个发表意见的地方；对一些人来说，它只是他们手机上的一个应用程序。</p><p id="fec7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">随着过去十年智能手机的出现，Twitter逐渐从最初作为短信替代品推出的微博网站转变为现在作为内置应用程序安装在我们的手机上。现在，我们所有人都有了这个触手可及的应用程序，我们可以用它来查看新闻，跟踪出版物，跟踪名人和名人，他们已经将它作为他们公关的一部分，与Instagram和脸书一起在Twitter上更新。</p><blockquote class="ma"><p id="e418" class="mb mc jf bd md me mf mg mh mi mj lp dk translated">简而言之，如果你是个名人，Twitter上的更新就意味着新闻。</p></blockquote><p id="03f1" class="pw-post-body-paragraph ku kv jf kw b kx mk kg kz la ml kj lc ld mm lf lg lh mn lj lk ll mo ln lo lp ij bi translated">现在，由于所有大小公司、信托机构、机构、电影首映式、艺术或精品公司、电影制作公司、餐馆都有自己的Twitter账号，他们经常让他们的追随者发布他们认为有必要通过他们的账号知道的任何事情。</p><p id="5633" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一旦他们这样做了，人们往往会倾向于在平台上表达他们的观点、感受、咆哮、厌恶和他们想发泄的一切。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mp"><img src="../Images/7e6c32228017b0eff91bf47d4db945c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*18DxiO1T7xosCVCHeIj_rw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">各种情绪意味着各种心情</figcaption></figure><p id="cf83" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi lq translated"><span class="l lr ls lt bm lu lv lw lx ly di">第二家公司知道这一点，所以他们使用Twitter来衡量公众的情绪，以了解他们的产品发布，以及他们如何进一步改进他们的产品，以使其到达更广泛的社会阶层，这反过来会影响他们的业务创收。</span></p><blockquote class="mu mv mw"><p id="cdbf" class="ku kv lz kw b kx ky kg kz la lb kj lc mx le lf lg my li lj lk mz lm ln lo lp ij bi translated">从下面的图表中我们可以看到，Twitter世界上所有的推文内容都被分成了不同的类别。</p></blockquote><figure class="mq mr ms mt gt is gh gi paragraph-image"><div class="gh gi na"><img src="../Images/eb3cb2bdf12408f316cda00ba25f7685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*UOjZbhYtQGIig1yJtARdsw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:谷歌。根据Pear Analytics News的推文内容(3.6%)垃圾邮件(3.8%)自我推销(5.9%)无意义的胡言乱语(40.1%)对话(37.6%)传递价值(8.7%)</figcaption></figure></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="6aee" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> <em class="lz">在我之前的</em> </strong> <a class="ae ni" href="https://medium.com/towards-artificial-intelligence/lets-see-the-sales-in-a-big-mart-store-250fb7e1f704?source=---------2------------------" rel="noopener"> <strong class="kw jg"> <em class="lz">文章</em> </strong> </a> <strong class="kw jg"> <em class="lz">中，我们看到了我们如何使用机器学习来检查某个特定商店中某个商品的销售情况。</em>T19】</strong></p><blockquote class="ma"><p id="37a5" class="mb mc jf bd md me mf mg mh mi mj lp dk translated">在这一篇中，我们将看到如何使用机器学习来检测特定推文的情绪。</p></blockquote><blockquote class="mu mv mw"><p id="581b" class="ku kv lz kw b kx mk kg kz la ml kj lc mx mm lf lg my mn lj lk mz mo ln lo lp ij bi translated">情感分析是自然语言处理广泛应用的关键问题之一。我们收到客户发来的关于制造和销售手机、电脑、笔记本电脑等各种技术公司的推文，任务是识别推文是否对这些公司或产品有负面情绪。我们将解决这个问题，然后将推文分类为积极或消极的情绪。</p></blockquote><p id="6820" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为此，我们将通过在<a class="ae ni" href="https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/" rel="noopener ugc nofollow" target="_blank"> AV平台</a>上可用的黑客马拉松，看看我们如何将推文分类成各种情绪。</p><p id="4d27" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">这个的完整代码可以在我的</strong><a class="ae ni" href="https://github.com/SaiBiswas/Sentiment-Analysis/blob/master/Sentiment_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="kw jg">GitHub repo</strong></a><strong class="kw jg">上找到。</strong></p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="7793" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">所以，我们走吧。</p><p id="1466" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">我们将代码分为3类。</strong></p><ol class=""><li id="afb7" class="nj nk jf kw b kx ky la lb ld nl lh nm ll nn lp no np nq nr bi translated"><strong class="kw jg"> EDA(探索性数据分析)</strong>:将训练和测试数据结合起来，作为一个整体进行一些探索性分析。</li><li id="8e3c" class="nj nk jf kw b kx ns la nt ld nu lh nv ll nw lp no np nq nr bi translated"><strong class="kw jg">特征工程</strong>:利用特征更好的预测情感。</li><li id="0f83" class="nj nk jf kw b kx ns la nt ld nu lh nv ll nw lp no np nq nr bi translated"><strong class="kw jg">建模</strong>:机器学习脱颖而出，我们看到预测结果的部分。</li></ol></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><blockquote class="ma"><p id="e70e" class="mb mc jf bd md me mf mg mh mi mj lp dk translated">说够了。给我看看代码..！！</p></blockquote></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="3d0e" class="oc od jf ny b gy oe of l og oh"><em class="lz"># importing the libraries for data processing and analysis</em><br/><strong class="ny jg">import</strong> <strong class="ny jg">pandas</strong> <strong class="ny jg">as</strong> <strong class="ny jg">pd</strong><br/><strong class="ny jg">import</strong> <strong class="ny jg">numpy</strong> <strong class="ny jg">as</strong> <strong class="ny jg">np</strong><br/><strong class="ny jg">import</strong> <strong class="ny jg">matplotlib.pyplot</strong> <strong class="ny jg">as</strong> <strong class="ny jg">plt</strong><br/><strong class="ny jg">import</strong> <strong class="ny jg">seaborn</strong> <strong class="ny jg">as</strong> <strong class="ny jg">sns</strong><br/><strong class="ny jg">import</strong> <strong class="ny jg">warnings</strong><br/>%matplotlib inline<br/>plt.style.use('fivethirtyeight')<br/>warnings.filterwarnings('ignore')</span></pre><p id="b433" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一旦完成，我们将继续装载列车和测试数据。</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="5f01" class="oc od jf ny b gy oe of l og oh">train = pd.read_csv('train_sentiment.csv')<br/>test = pd.read_csv('test_sentiment.csv')</span></pre></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><ol class=""><li id="b28c" class="nj nk jf kw b kx ky la lb ld nl lh nm ll nn lp no np nq nr bi translated"><strong class="kw jg"> EDA(探索性数据分析)</strong></li></ol><p id="df02" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们可以检查tweet计数在训练和测试数据中的分布。</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="e226" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg"># checking the distribution of label of tweets in the dataset<br/></strong>train[train['label'] == 0].head(10)<br/>train[train['label'] == 1].head(10)</span></pre><p id="ad7f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">接下来，我们继续对tweet数据进行计数矢量化。计数矢量化是计算每个单词在文档中出现的次数的过程(例如，不同的文本，如一篇文章、一本书，甚至一个段落！).</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="5ad5" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg"><em class="lz"># count vectorization for the text</em><br/>from</strong> <strong class="ny jg">sklearn.feature_extraction.text</strong> <strong class="ny jg">import</strong> CountVectorizer<br/><br/>cv = CountVectorizer(stop_words = 'english')<br/>words = cv.fit_transform(train.tweet)<br/><br/>sum_words = words.sum(axis=0)<br/><br/>words_freq = [(word, sum_words[0, i]) <strong class="ny jg">for</strong> word, i <strong class="ny jg">in</strong> cv.vocabulary_.items()]<br/>words_freq = sorted(words_freq, key = <strong class="ny jg">lambda</strong> x: x[1], reverse = <strong class="ny jg">True</strong>)<br/><br/>frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])<br/><br/>frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'brown')<br/>plt.title("Most Frequently Occuring Words - Top 30")</span></pre><p id="9f29" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">另一个重要的EDA步骤是检查数据中最重要的单词。因此，为训练数据中出现最多的单词生成单词云。</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="8f39" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg"><em class="lz"># generating word cloud for the most common occuring words in the train data</em></strong><br/><strong class="ny jg">from</strong> <strong class="ny jg">wordcloud</strong> <strong class="ny jg">import</strong> WordCloud<br/><br/>wordcloud = WordCloud(background_color = 'white', width = 1000, height = 1000).generate_from_frequencies(dict(words_freq))<br/><br/>plt.figure(figsize=(10,8))<br/>plt.imshow(wordcloud)<br/>plt.title("WordCloud - Vocabulary from Reviews", fontsize = 22)</span></pre><figure class="mq mr ms mt gt is gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/3af32658e109566ae327f7d9c2af2bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*pQSfdi4aGrfxY_e8pPs_0A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">tweet数据的Wordcloud</figcaption></figure><p id="c420" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们也检查一下tweet数据中出现的中性词。</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="aac5" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg"><em class="lz"># wordcloud for words that are neutral</em><br/></strong>normal_words =' '.join([text <strong class="ny jg">for</strong> text <strong class="ny jg">in</strong> train['tweet'][train['label'] == 0]])<br/><br/>wordcloud = WordCloud(width=800, height=500, random_state = 0, max_font_size = 110).generate(normal_words)<br/>plt.figure(figsize=(10, 7))<br/>plt.imshow(wordcloud, interpolation="bilinear")<br/>plt.axis('off')<br/>plt.title('The Neutral Words')<br/>plt.show()</span></pre><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/07eb39559d64715c99d5f7af359351a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*JfsJNnHFpghGXttA_ywI4w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">当前中性词的词云</figcaption></figure><p id="21b7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了检查数据中存在的标签，我创建了一个函数来检查和收集数据中存在的标签，并将它们存储在一个列表中。</p><ol class=""><li id="81c4" class="nj nk jf kw b kx ky la lb ld nl lh nm ll nn lp no np nq nr bi translated">我们还可以看到tweet数据中最重要的标签。</li><li id="f1b0" class="nj nk jf kw b kx ns la nt ld nu lh nv ll nw lp no np nq nr bi translated">我们还可以检查tweets类型，并将它们存储在一个列表中。</li></ol><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="9a8f" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg"><em class="lz"># defining a function to collect the hashtags from the train data</em><br/>def</strong> hashtags_extract(x):<br/>    hashtags = []<br/>    <strong class="ny jg">for</strong> i <strong class="ny jg">in</strong> x:<br/>        ht = re.findall(r"#(\w+)", i)<br/>        hashtags.append(ht)<br/>    <br/>    <strong class="ny jg">return</strong> hashtags</span></pre></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><pre class="nx ny nz oa aw ob bi"><span id="87ca" class="oc od jf ny b gy ok ol om on oo of l og oh"><strong class="ny jg"><em class="lz">#extracting hashtags from racist/sexist tweet</em><br/></strong>ht_normal = hashtags_extract(train['tweet'][train['label'] == 0])<br/><br/><em class="lz">#extracting hashtags from normal tweet</em><br/>ht_negetive = hashtags_extract(train['tweet'][train['label'] == 1])<br/><br/><em class="lz">#unnesting list</em><br/><br/>ht_normal = sum(ht_normal , [])<br/>ht_negetive = sum(ht_negetive , [])</span></pre><p id="f0c6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一旦解决了这个问题，我们就继续对数据中出现的单词进行标记。标记化是自然语言处理中的一个重要步骤，因为它有助于将单词分解为其词根形式。</p><p id="d4eb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在标记化之后，我们使用流行的Gensim库创建一个单词到向量的模型，对于skip-gram模型，保持上下文窗口大小为5，窗口大小为1。</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="faa4" class="oc od jf ny b gy oe of l og oh"><em class="lz">#tokenizing the words present in the training set</em><br/>tokenized_tweet = train['tweet'].apply(<strong class="ny jg">lambda</strong> x: x.split()) <br/><br/><em class="lz"># importing gensim</em><br/><strong class="ny jg">import</strong> <strong class="ny jg">gensim</strong><br/><br/><strong class="ny jg"><em class="lz"># creating a word to vector model</em><br/></strong>model_w2v = gensim.models.Word2Vec(<br/>            tokenized_tweet,<br/>            size=200, <em class="lz"># desired no. of features/independent variables </em><br/>            window=5, <em class="lz"># context window size</em><br/>            min_count=2,<br/>            sg = 1, <em class="lz"># 1 for skip-gram model</em><br/>            hs = 0,<br/>            negative = 10, <em class="lz"># for negative sampling</em><br/>            workers= 2, <em class="lz"># no.of cores</em><br/>            seed = 34)<br/><br/>model_w2v.train(tokenized_tweet, total_examples= len(train['tweet']), epochs=20)</span></pre><p id="2dde" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一旦做到这一点，我们就可以看到自己的话；具有与数据集中出现的单词相似的含义。例如:- " <a class="ae ni" href="https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/" rel="noopener ugc nofollow" target="_blank">索尼</a>"</p><div class="mq mr ms mt gt ab cb"><figure class="op is oq or os ot ou paragraph-image"><img src="../Images/c44a8023e9603eb321c9417b8b163785.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*jOje2EW9w5UmqRb3jmrEOg.png"/></figure><figure class="op is ov or os ot ou paragraph-image"><img src="../Images/05dbf22ad7044a44ce0262d311a4f136.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*Y0-q5JFlm1TtRO6t5bcorg.png"/><figcaption class="iz ja gj gh gi jb jc bd b be z dk ow di ox oy translated">Gensim word to vec模型，用于检查与单词Sony最相似的单词。</figcaption></figure></div><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="f6d5" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg">from</strong> <strong class="ny jg">tqdm</strong> <strong class="ny jg">import</strong> tqdm<br/>tqdm.pandas(desc="progress-bar")<br/><strong class="ny jg">from</strong> <strong class="ny jg">gensim.models.doc2vec</strong> <strong class="ny jg">import</strong> LabeledSentence</span><span id="3083" class="oc od jf ny b gy oz of l og oh"># Adding a label to the tweets</span><span id="d381" class="oc od jf ny b gy oz of l og oh"><strong class="ny jg">def</strong> add_label(twt):<br/>    output = []<br/>    <strong class="ny jg">for</strong> i, s <strong class="ny jg">in</strong> zip(twt.index, twt):<br/>        output.append(LabeledSentence(s, ["tweet_" + str(i)]))<br/>    <strong class="ny jg">return</strong> output<br/><br/><em class="lz"># label all the tweets</em><br/>labeled_tweets = add_label(tokenized_tweet)<br/><br/>labeled_tweets[:6]</span></pre><p id="1a98" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们也应该从数据中删除不需要的模式，因为保留它们会给我们的数据集增加噪声，我们应该尽可能保持我们的模型没有噪声，以便更好地预测。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="7c6a" class="oc od jf ny b gy oe of l og oh"><em class="lz"># removing unwanted patterns from the data</em><br/><br/><strong class="ny jg">import</strong> <strong class="ny jg">re</strong><br/><strong class="ny jg">import</strong> <strong class="ny jg">nltk</strong><br/><br/>nltk.download('stopwords')<br/><strong class="ny jg">from</strong> <strong class="ny jg">nltk.corpus</strong> <strong class="ny jg">import</strong> stopwords<br/><strong class="ny jg">from</strong> <strong class="ny jg">nltk.stem.porter</strong> <strong class="ny jg">import</strong> PorterStemmer</span><span id="7d3b" class="oc od jf ny b gy oz of l og oh"># collecting the train data and forming a corpus<br/>train_corpus = []<br/><br/><strong class="ny jg">for</strong> i <strong class="ny jg">in</strong> range(0, 7920):<br/>  review = re.sub('[^a-zA-Z]', ' ', train['tweet'][i])<br/>  review = review.lower()<br/>  review = review.split()<br/>  <br/>  ps = PorterStemmer()<br/>   <em class="lz"># stemming</em><br/>  review = [ps.stem(word) <strong class="ny jg">for</strong> word <strong class="ny jg">in</strong> review <strong class="ny jg">if</strong> <strong class="ny jg">not</strong> word <strong class="ny jg">in</strong> set(stopwords.words('english'))]<br/>  <br/>  <em class="lz"># joining them back with space</em><br/>  review = ' '.join(review)<br/>  train_corpus.append(review)</span><span id="7ba9" class="oc od jf ny b gy oz of l og oh"><br/><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.feature_extraction.text</strong> <strong class="ny jg">import</strong> CountVectorizer<br/><br/>cv = CountVectorizer(max_features = 1500)<br/>x = cv.fit_transform(train_corpus).toarray()<br/>y = train.iloc[:, 1]<br/><br/>print(x.shape)<br/>print(y.shape)</span><span id="3e6a" class="oc od jf ny b gy oz of l og oh"><em class="lz"># creating bag of words for test</em><br/><br/><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.feature_extraction.text</strong> <strong class="ny jg">import</strong> CountVectorizer<br/><br/>cv = CountVectorizer(max_features = 1500)<br/>x_test = cv.fit_transform(test_corpus).toarray()<br/>y = train.iloc[:, 1]<br/><br/>print(x_test.shape)</span></pre><p id="4369" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，对测试数据做同样的事情。</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="2d85" class="oc od jf ny b gy oe of l og oh">test_corpus = []<br/><br/><strong class="ny jg">for</strong> i <strong class="ny jg">in</strong> range(0, 1953):<br/>  review = re.sub('[^a-zA-Z]', ' ', test['tweet'][i])<br/>  review = review.lower()<br/>  review = review.split()<br/>  <br/>  ps = PorterStemmer()<br/>  <br/>  <em class="lz"># stemming</em><br/>  review = [ps.stem(word) <strong class="ny jg">for</strong> word <strong class="ny jg">in</strong> review <strong class="ny jg">if</strong> <strong class="ny jg">not</strong> word <strong class="ny jg">in</strong> set(stopwords.words('english'))]<br/>  <br/>  <em class="lz"># joining them back with space</em><br/>  review = ' '.join(review)<br/>  test_corpus.append(review) </span></pre><p id="d9fb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">去除停用词&amp;词汇化(<strong class="kw jg"> <em class="lz">)词汇化与词干化不同，它适当地减少了词形变化，确保词根属于该语言。在引理化中，词根称为引理。一个引理(复数引理或引理数据)是一组词</em> </strong> <em class="lz">的规范形式、词典形式或引用形式。</em>)也是自然语言处理的重要一步。</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="2e16" class="oc od jf ny b gy oe of l og oh"><em class="lz"># removing unwanted patterns from the data</em><br/><br/><strong class="ny jg">import</strong> <strong class="ny jg">re</strong><br/><strong class="ny jg">import</strong> <strong class="ny jg">nltk</strong><br/><br/>nltk.download('stopwords')<br/><strong class="ny jg">from</strong> <strong class="ny jg">nltk.corpus</strong> <strong class="ny jg">import</strong> stopwords<br/><strong class="ny jg">from</strong> <strong class="ny jg">nltk.stem.porter</strong> <strong class="ny jg">import</strong> PorterStemmer</span><span id="cd1c" class="oc od jf ny b gy oz of l og oh"># for train data<br/>train_corpus = []<br/><br/><strong class="ny jg">for</strong> i <strong class="ny jg">in</strong> range(0, 7920):<br/>  review = re.sub('[^a-zA-Z]', ' ', train['tweet'][i])<br/>  review = review.lower()<br/>  review = review.split()<br/>  <br/>  ps = PorterStemmer()<br/>   <em class="lz"># stemming</em><br/>  review = [ps.stem(word) <strong class="ny jg">for</strong> word <strong class="ny jg">in</strong> review <strong class="ny jg">if</strong> <strong class="ny jg">not</strong> word <strong class="ny jg">in</strong> set(stopwords.words('english'))]<br/>  <br/>  <em class="lz"># joining them back with space</em><br/>  review = ' '.join(review)<br/>  train_corpus.append(review)</span><span id="5806" class="oc od jf ny b gy oz of l og oh"># for test data<br/>test_corpus = []<br/><br/><strong class="ny jg">for</strong> i <strong class="ny jg">in</strong> range(0, 1953):<br/>  review = re.sub('[^a-zA-Z]', ' ', test['tweet'][i])<br/>  review = review.lower()<br/>  review = review.split()<br/>  <br/>  ps = PorterStemmer()<br/>  <br/>  <em class="lz"># stemming</em><br/>  review = [ps.stem(word) <strong class="ny jg">for</strong> word <strong class="ny jg">in</strong> review <strong class="ny jg">if</strong> <strong class="ny jg">not</strong> word <strong class="ny jg">in</strong> set(stopwords.words('english'))]<br/>  <br/>  <em class="lz"># joining them back with space</em><br/>  review = ' '.join(review)<br/>  test_corpus.append(review)</span></pre><p id="5957" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在，我们几乎完成了数据的清理，然后我们将通过将语料库中的单词转换为数组来创建一个单词包，以便机器理解和处理数据。</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="2d3e" class="oc od jf ny b gy oe of l og oh"><em class="lz"># creating bag of words for train</em><br/><em class="lz"># creating bag of words</em><br/><br/><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.feature_extraction.text</strong> <strong class="ny jg">import</strong> CountVectorizer<br/><br/>cv = CountVectorizer(max_features = 1500)<br/>x = cv.fit_transform(train_corpus).toarray()<br/>y = train.iloc[:, 1]<br/><br/>print(x.shape)<br/>print(y.shape)</span><span id="59e9" class="oc od jf ny b gy oz of l og oh">-------<br/>(7920, 1500)<br/>(7920,)</span><span id="7030" class="oc od jf ny b gy oz of l og oh"><em class="lz"># creating bag of words for test</em><br/><br/><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.feature_extraction.text</strong> <strong class="ny jg">import</strong> CountVectorizer<br/><br/>cv = CountVectorizer(max_features = 1500)<br/>x_test = cv.fit_transform(test_corpus).toarray()<br/>y = train.iloc[:, 1]<br/><br/>print(x_test.shape)</span><span id="f770" class="oc od jf ny b gy oz of l og oh">-------<br/>(1953, 1500)</span></pre></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><pre class="nx ny nz oa aw ob bi"><span id="2ef0" class="oc od jf ny b gy ok ol om on oo of l og oh"><em class="lz"># standardization</em><br/><br/><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.preprocessing</strong> <strong class="ny jg">import</strong> StandardScaler<br/><br/>sc = StandardScaler()<br/><br/>x_train = sc.fit_transform(x_train)<br/>x_valid = sc.transform(x_valid)</span></pre><p id="a752" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">标准化也是我们进行建模过程之前的一个重要步骤。</p><p id="a1a6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">标准化之后，我们为了训练和测试的目的将数据分开。</p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="a8c7" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.model_selection</strong> <strong class="ny jg">import</strong> train_test_split<br/><br/>x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.25, random_state = 42)<br/><br/>print(x_train.shape)<br/>print(x_valid.shape)<br/>print(y_train.shape)<br/>print(y_valid.shape)</span><span id="35b4" class="oc od jf ny b gy oz of l og oh">---------------<br/>(5940, 1500)<br/>(1980, 1500)<br/>(5940,)<br/>(1980,)</span></pre></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="50b4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">本次比赛使用的评估标准是F1分数，公式为2*(精度*召回率)/(精度+召回率)。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pa"><img src="../Images/8026f6222794fc1cd67f4b28ecbeaf13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQPsgMDhZC0lwlnOWZIA2A.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">F1分数的公式</figcaption></figure><p id="96d4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">一旦我们完成了这个，我们将继续机器学习建模。</p><p id="9910" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们在数据上尝试一些建模技术，然后自己看看结果。</p><ol class=""><li id="aaac" class="nj nk jf kw b kx ky la lb ld nl lh nm ll nn lp no np nq nr bi translated"><strong class="kw jg">随机森林</strong></li></ol><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="2d9a" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.ensemble</strong> <strong class="ny jg">import</strong> RandomForestClassifier<br/><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.metrics</strong> <strong class="ny jg">import</strong> confusion_matrix<br/><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.metrics</strong> <strong class="ny jg">import</strong> f1_score<br/><br/>model = RandomForestClassifier()<br/>model.fit(x_train, y_train)<br/><br/>y_pred = model.predict(x_valid)<br/><br/>print("Training Accuracy :", model.score(x_train, y_train))<br/>print("Validation Accuracy :", model.score(x_valid, y_valid))<br/><br/><em class="lz"># calculating the f1 score for the validation set</em><br/>print("F1 score :", f1_score(y_valid, y_pred))</span><span id="35a4" class="oc od jf ny b gy oz of l og oh">---------------<br/>Training Accuracy : 0.9994949494949495<br/>Validation Accuracy : 0.8853535353535353<br/>F1 score : 0.7926940639269406</span><span id="c9e6" class="oc od jf ny b gy oz of l og oh">---------------<br/><em class="lz"><br/># confusion matrix</em> <br/>cm = confusion_matrix(y_valid, y_pred) <br/>print(cm)</span><span id="c130" class="oc od jf ny b gy oz of l og oh">---------------<br/>[[1319  120]<br/> [ 107  434]]</span></pre><p id="fe92" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">2.<strong class="kw jg">决策树分类器</strong></p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="e7d1" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.tree</strong> <strong class="ny jg">import</strong> DecisionTreeClassifier<br/><br/>model = DecisionTreeClassifier()<br/>model.fit(x_train, y_train)<br/><br/>y_pred = model.predict(x_valid)<br/><br/>print("Training Accuracy :", model.score(x_train, y_train))<br/>print("Validation Accuracy :", model.score(x_valid, y_valid))<br/><br/><em class="lz"># calculating the f1 score for the validation set</em><br/>print("f1 score :", f1_score(y_valid, y_pred))<br/><br/><em class="lz"># confusion matrix</em><br/>cm = confusion_matrix(y_valid, y_pred)<br/>print(cm)<br/>----------------</span><span id="4009" class="oc od jf ny b gy oz of l og oh">Training Accuracy : 0.9994949494949495<br/>Validation Accuracy : 0.8378787878787879<br/>f1 score : 0.700280112044818</span><span id="91e4" class="oc od jf ny b gy oz of l og oh">[[1284  155]<br/> [ 166  375]]</span></pre><p id="dc05" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">3.<strong class="kw jg"> SVM造型</strong></p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="3ff0" class="oc od jf ny b gy oe of l og oh"><em class="lz"># trying SVC algorithm on the data</em><br/><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.svm</strong> <strong class="ny jg">import</strong> SVC<br/><br/>model = SVC()<br/>model.fit(x_train, y_train)<br/><br/>y_pred = model.predict(x_valid)<br/><br/>print("Training Accuracy :", model.score(x_train, y_train))<br/>print("Validation Accuracy :", model.score(x_valid, y_valid))<br/><br/><em class="lz"># calculating the f1 score for the validation set</em><br/>print("f1 score :", f1_score(y_valid, y_pred))<br/><br/><em class="lz"># confusion matrix</em><br/>cm = confusion_matrix(y_valid, y_pred)<br/>print(cm)</span><span id="37b8" class="oc od jf ny b gy oz of l og oh">----------------</span><span id="c368" class="oc od jf ny b gy oz of l og oh">Training Accuracy : 0.9644781144781145<br/>Validation Accuracy : 0.8621212121212121<br/>f1 score : 0.7222787385554426<br/>[[1352   87]<br/> [ 186  355]]</span></pre><p id="b1e4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">4.<strong class="kw jg"> XG增强分类器</strong></p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="0a39" class="oc od jf ny b gy oe of l og oh"><em class="lz"># trying xgboost classifier on the data</em><br/><strong class="ny jg">from</strong> <strong class="ny jg">xgboost</strong> <strong class="ny jg">import</strong> XGBClassifier<br/><br/>model = XGBClassifier()<br/>model.fit(x_train, y_train)<br/><br/>y_pred = model.predict(x_valid)<br/><br/>print("Training Accuracy :", model.score(x_train, y_train))<br/>print("Validation Accuracy :", model.score(x_valid, y_valid))<br/><br/><em class="lz"># calculating the f1 score for the validation set</em><br/>print("f1 score :", f1_score(y_valid, y_pred))<br/><br/><em class="lz"># confusion matrix</em><br/>cm = confusion_matrix(y_valid, y_pred)<br/>print(cm)</span><span id="a82a" class="oc od jf ny b gy oz of l og oh">-------------------</span><span id="6f13" class="oc od jf ny b gy oz of l og oh">Training Accuracy : 0.8882154882154882<br/>Validation Accuracy : 0.8792929292929293<br/>f1 score : 0.7912663755458514<br/>[[1288  151]<br/> [  88  453]]</span></pre><p id="baad" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">5.<strong class="kw jg">逻辑回归</strong></p><pre class="mq mr ms mt gt nx ny nz oa aw ob bi"><span id="ae13" class="oc od jf ny b gy oe of l og oh"><strong class="ny jg">from</strong> <strong class="ny jg">sklearn.linear_model</strong> <strong class="ny jg">import</strong> LogisticRegression<br/><br/>model = LogisticRegression()<br/>model.fit(x_train, y_train)<br/><br/>y_pred = model.predict(x_valid)<br/><br/>print("Training Accuracy :", model.score(x_train, y_train))<br/>print("Validation Accuracy :", model.score(x_valid, y_valid))<br/><br/><em class="lz"># calculating the f1 score for the validation set</em><br/>print("f1 score :", f1_score(y_valid, y_pred))<br/><br/><em class="lz"># confusion matrix</em><br/>cm = confusion_matrix(y_valid, y_pred)<br/>print(cm)</span><span id="ca16" class="oc od jf ny b gy oz of l og oh">----------------</span><span id="800b" class="oc od jf ny b gy oz of l og oh">Training Accuracy : 0.9757575757575757<br/>Validation Accuracy : 0.8323232323232324<br/>f1 score : 0.6897196261682242<br/>[[1279  160]<br/> [ 172  369]]</span></pre><p id="3e9e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">因此，在对训练数据使用5种算法并对测试数据进行测试后，我们可以得出一个结论，即随机森林算法在所有剩余算法中具有最高的准确性，它成功地将推文分类为正面和负面，准确率为79.2%。</p><figure class="mq mr ms mt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pb"><img src="../Images/0d22e011563ae0e3681bb4310df17f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VT7AxioAGXplMe7RAEYfSA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图片来源:谷歌</figcaption></figure><h2 id="4cc0" class="oc od jf bd pc pd pe dn pf pg ph dp pi ld pj pk pl lh pm pn po ll pp pq pr ps bi translated"><strong class="ak">结论</strong></h2><p id="9d3c" class="pw-post-body-paragraph ku kv jf kw b kx pt kg kz la pu kj lc ld pv lf lg lh pw lj lk ll px ln lo lp ij bi translated">Twitter现在已经成为我们大多数人生活中不可或缺的一部分。一条推文有能力影响一家公司产品的销售，或者在twitter社区造成严重破坏，如果这条推文像病毒一样传播，我的意思是，人们每次都发布各种各样充满各种情绪的推文。因此，作为数据科学家，我们应该能够将推文中的负面情绪解读为积极情绪，这反过来被公司用来衡量产品发布或营销广告的有效性。</p><blockquote class="ma"><p id="3403" class="mb mc jf bd md me mf mg mh mi mj lp dk translated">未来，随着越来越多的人变得精通技术，这种对情感进行分类的需求将会成倍增长。</p></blockquote><p id="e77e" class="pw-post-body-paragraph ku kv jf kw b kx mk kg kz la ml kj lc ld mm lf lg lh mn lj lk ll mo ln lo lp ij bi translated">对于公司来说，这是一个在竞争中领先的机会，通过评估产品评论的反应，在竞争日益激烈的市场中获得更多收入。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><blockquote class="ma"><p id="85eb" class="mb mc jf bd md me mf mg mh mi mj lp dk translated">这就是全部。下次见。再见..！！！</p></blockquote></div></div>    
</body>
</html>