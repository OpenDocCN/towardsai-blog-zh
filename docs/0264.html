<html>
<head>
<title>Generate Quotes with Web Scrapping, Glove Embeddings, and LSTM in Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pytorch中的网络报废、手套嵌入和LSTM生成报价</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/quotes-generation-for-social-media-with-web-scrapping-glove-embeddings-training-and-lstm-in-e07c03491f15?source=collection_archive---------4-----------------------#2020-01-06">https://pub.towardsai.net/quotes-generation-for-social-media-with-web-scrapping-glove-embeddings-training-and-lstm-in-e07c03491f15?source=collection_archive---------4-----------------------#2020-01-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/b2d9165e3b1f17d352c9fc1aebef2858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0oQqMCR0D4VFWVBr5UXWzg.png"/></div></div></figure><h1 id="601c" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">介绍</h1><p id="c9a4" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">随着自然语言处理特别是语言模型研究的进展，文本生成——一个经典的机器学习任务使用递归网络来解决。</p><p id="e0db" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在本文中，我们将使用单词级语言模型从头开始生成英语引语。Web报废以获得数据集<br/> 2。手套嵌入训练<br/> 3。LSTM型号<br/> 4。预言；预测；预告</p><h1 id="788f" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">Web报废</h1><p id="8ac4" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们使用网络报废从网站<a class="ae lz" href="http://www.wiseoldsayings.com/" rel="noopener ugc nofollow" target="_blank">收集名言警句</a>。这个网站有近4000个不同类别的报价。我不会在这篇文章中讲述如何做到这一点，因为这超出了本文的范围。但在“robots.txt”允许的情况下，从这个网站上删除是合法的。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="efd6" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">由于这个网站的主页列出了所有类别，每个类别的爬行不同于主页URL的唯一后缀，首先，收集所有类别，在下一步中，爬行每个类别，并从页面中提取报价。<br/> <strong class="ky ir">注意:</strong>一个类别可能包含多个页面，如第1页、第2页……</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="7534" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">我已经在Kaggle上传了这些提取的报价作为数据集。</strong></p><div class="mg mh gp gr mi mj"><a href="https://www.kaggle.com/santhalnr/quotations" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd ir gy z fp mo fr fs mp fu fw ip bi translated">英语语录</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">4000个英文报价网站被废弃</h3></div></div><div class="mr l"><div class="ms l mt mu mv mr mw jw mj"/></div></div></a></div><h1 id="8ca9" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">Glove(单词表示的全局向量)</h1><p id="c0e1" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对于语言模型来说，单词到嵌入的映射使得维数空间比一键向量编码低，因为词汇大小非常大。</p><p id="8fb8" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">glove方法捕获单词对的全局上下文和局部上下文。<br/>在<a class="ae lz" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>阅读更多关于手套的文章。</p><p id="f9ee" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">使用<a class="ae lz" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">预训练的</a>手套权重正确地将单词表示到维度空间中，但是在本文中，我们为单词实现了自己的手套嵌入。<br/>词汇中的单词被表示为嵌入，并用手套方法进行训练。因此，在从原始数据集中获取报价时，要准备好词汇。</p><pre class="ma mb mc md gt mx my mz na aw nb bi"><span id="0c78" class="nc jz iq my b gy nd ne l nf ng">Read quotes with minimum word count of 5 (User Choice) to maintain minimum length quotation.<br/>As quotes contain ',' '.', and '-' next to a word like "Ciao Bello, Howdy? ", for better tokenization add space between words and special characters.<br/>Also append ';' as end-of-quote token.</span></pre><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="6cef" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在每段引语中，使用独特的词，组成词汇。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="c0bb" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">此外，单词不能直接用于神经网络，将单词映射到整数，将整数映射到单词，以获得单词。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><pre class="ma mb mc md gt mx my mz na aw nb bi"><span id="6259" class="nc jz iq my b gy nd ne l nf ng">To train glove embeddings we need co-occurance matrix between words in vocabulary.<br/>Follow sliding window method by fixing window size (context range between words) iterating each quote, and calcuate similarity as a function of reciprocal of distance between two words with in window.</span></pre><p id="d3b9" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">共现矩阵捕捉语料库中单词的全局上下文。共现矩阵是一个字典，以词对作为关键字，以值作为相似性得分。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/48997bf8b576191f67235b8cd37b6f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Omm5m3k25JT7jOpspCtHA.png"/></div></div></figure><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="1bbc" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">观察词与词之间相似度得分的分布:</p><pre class="ma mb mc md gt mx my mz na aw nb bi"><span id="be1f" class="nc jz iq my b gy nd ne l nf ng">topk,ind=torch.topk(torch.tensor(list(co_occ_matrix.values())),200)<br/>sns.distplot(topk)</span></pre><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/4400e40fc9c1be10528275eb2d7dfa30.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*ne8jkRaqBbwTRtgnpS6x0A.png"/></div></figure><p id="4706" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">该图显示了前200个相似性得分的分布，这些得分大多低于100。这是由于语料库的规模较小。</p><pre class="ma mb mc md gt mx my mz na aw nb bi"><span id="b6a0" class="nc jz iq my b gy nd ne l nf ng">elem_den = []<br/>for i <strong class="my ir">in</strong> co_occ_matrix.values():<br/>    if i &gt; 0 <strong class="my ir">and</strong> i &lt; 10:<br/>        elem_den.append(i)<br/>sns.violinplot(x = elem_den)</span></pre><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/8ccb42cc3d315cddc700198e8ef4280d.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*mUcQK_l1QSbgUH2e08h8MQ.png"/></div></figure><p id="f432" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Violin plot显示，对于给定的上下文范围5，语料库中出现的大多数百分比的词对非常罕见，相似度小于1。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="1783" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">这些作为关键字存储在字典中的词对捕获本地上下文，并且我们使用这些关键字来训练嵌入。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="c53f" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Glove试图通过移动接近全局上下文相似性的局部上下文词对的维度来表示接近全局上下文和局部上下文的向量嵌入。</p><p id="1094" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">像所有的神经网络一样，手套嵌入通过定义如下所示的损失函数来训练。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c2b03c481b58c909c21fb407769f977f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*rQKl31brZcR4VPqONfzCEg.png"/></div></figure><p id="015b" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">想象零件乘积和偏差相加作为经典的MLP网络输出<br/> y-hat，我们训练这些向量以减少接近于零的损失，将y-hat映射到y，y在这里是相似性得分的对数。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="70ab" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">损失函数是具有加权求和的定制损失，其中权重表示当前单词对在通过减少倒数幂中的稀有单词对分数来计算损失时的影响程度。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/ba0b5142ec834cb9a6b5954f2509d4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*qdqYuuow-i3r64XAPhBsew.png"/></div></div></figure><p id="d074" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在上面的表达式中，alpha小于1，相似性得分小于阈值相似性的词对被赋予较小的权重，而相似性高于阈值的词对保持原样。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="f96e" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我们使用128个维度进行单词嵌入，并且只训练100个时期，以避免由于语料库规模小而过度拟合。</p><p id="fd3f" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">手套模型的输入是形状为<strong class="ky ir"> ( batch_size，indices _ 1，index _ 2)</strong>的批中单词对的矢量表示，其中<strong class="ky ir">索引</strong>是给定批中单词的数字整数，Pytorch嵌入层自动创建形状为<strong class="ky ir"> (vocab_len，num_dim) </strong>的这些索引的矢量维度。</p><p id="dca1" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在手套模型中训练的权重具有形状<strong class="ky ir"> (Vocab_len，num_dim) </strong>是每个单词在具有维度<strong class="ky ir"> num_dim的词汇表中的嵌入。</strong></p><p id="f9e8" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">由于我们将单词对副本存储为<strong class="ky ir"> a-b </strong>和<strong class="ky ir"> b-a </strong>类型以增加输入大小，我们简单地添加了两个权重<strong class="ky ir"> Ui </strong>和<strong class="ky ir"> Uj。</strong></p><pre class="ma mb mc md gt mx my mz na aw nb bi"><span id="07bc" class="nc jz iq my b gy nd ne l nf ng">emb_i = glove.ui.weight.cpu().data.numpy()<br/>emb_j = glove.uj.weight.cpu().data.numpy()<br/>emb = emb_i + emb_j</span></pre><p id="e063" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir"> emb </strong>是维<strong class="ky ir"> (Vocab_len，num_dim)词汇表中单词的最终单词嵌入。</strong></p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="412c" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">通过使用TSNE将高维向量空间映射到2D来可视化这些嵌入。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/1d5235727b92780e99ee17205658d65b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VWWLFvCOcB5DaKq5PNKw7A.png"/></div></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk translated">2D首次推出100个单词</figcaption></figure><div class="mg mh gp gr mi mj"><a href="https://colab.research.google.com/drive/1MlsQUliIUiyWe42gS5OkItwFpe-FuwpV" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd ir gy z fp mo fr fs mp fu fw ip bi translated">报价_世代_手套_LSTM_Pytorch</h2></div><div class="mr l"><div class="ny l mt mu mv mr mw jw mj"/></div></div></a></div><p id="873e" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">如果需要，可以在Google Colab中运行模型。</p><h1 id="8fdd" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">报价生成— LSTM模型</h1><p id="3323" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">像所有语言模型一样，可变长度的序列不能直接馈入网络，因为神经网络只接受固定维度的输入。为此，我们采用固定大小的序列长度，并为下一个输入中断额外的序列。<br/>我们遵循经典的文本生成模型，传递一个固定的输入序列，得到序列中的下一个单词作为输出。我们通过获取固定大小的输入序列并作为当前序列后的下一个单词输出来准备数据集。我们对语料库中的所有引用都这样做。</p><pre class="ma mb mc md gt mx my mz na aw nb bi"><span id="5e61" class="nc jz iq my b gy nd ne l nf ng"><strong class="my ir">Ex:</strong> Sequence "Love means never having to say you're sorry . ;"<br/>';' is end-of-sequence. If fixed input length 7 then input and next word as output are</span><span id="950f" class="nc jz iq my b gy nz ne l nf ng">              input                              next word<br/>_______________________________________    _________________________<br/>Love means never having to say you're              sorry<br/>means never having to say you're sorry               .<br/>never having to say you're sorry .                   ;         </span></pre><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="b1e0" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我们使用一个简单的模型来生成文本，在密集层之后有一个单LSTM层，以利用损失函数Cross_Entropy_Loss来获得下一个单词的概率。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="9d1c" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">对于每个时期<strong class="ky ir">，单元状态(h_c) </strong>和<strong class="ky ir">隐藏激活(h_h) </strong>必须重新初始化，否则丢失不会减少，也不会有训练。由于词汇表非常大，并且计算值可能很大，这取决于网络的大小，因此使用梯度裁剪可以防止模型发生梯度爆炸，从而导致<strong class="ky ir"> nan </strong>丢失。</p><h1 id="0256" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">生成报价</h1><p id="44eb" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在训练输出给定输入序列的下一个词的概率的模型之后，通过将初始种子馈送到网络以获得下一个词的概率来进行报价生成，使用技术<a class="ae lz" href="https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277" rel="noopener" target="_blank">温度</a>对这些概率进行采样，并将生成的下一个词添加到最后一个索引，同时移除先前输入中的第一个词以保持窗口大小以馈送到网络。重复此操作，直到输出的引号字符或generated_length小于max_len。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="85f2" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">具有很少的训练时期、小规模的语料库并且没有超参数调整模型生成的报价:</p><pre class="ma mb mc md gt mx my mz na aw nb bi"><span id="88ea" class="nc jz iq my b gy nd ne l nf ng">"there is no one love was . life being you'll or and the , is all her all heaven comes with order they to you . , you hurt get when hope that for , of the which for for no others having take being the gone free is me the by knows where for"</span></pre><p id="ca9a" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Kaggle笔记本:</p><div class="mg mh gp gr mi mj"><a href="https://www.kaggle.com/santhalnr/quote-generation-glove-training-lstm-pytorch" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd ir gy z fp mo fr fs mp fu fw ip bi translated">报价生成手套培训+LSTM pytorch</h2><div class="oa l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">www.kaggle.com</p></div></div><div class="mr l"><div class="ob l mt mu mv mr mw jw mj"/></div></div></a></div><p id="f2b0" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Github回购:</p><div class="mg mh gp gr mi mj"><a href="https://github.com/santhalakshminarayana/Quotation_Generation_Glove_LSTM_Pytorch" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd ir gy z fp mo fr fs mp fu fw ip bi translated">santhalakshminarayana/Quotation _ Generation _ Glove _ LSTM _ py torch</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">报价生成使用LSTM - Pytorch与自定义嵌入训练手套模型。</h3></div><div class="oa l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">github.com</p></div></div><div class="mr l"><div class="oc l mt mu mv mr mw jw mj"/></div></div></a></div></div></div>    
</body>
</html>