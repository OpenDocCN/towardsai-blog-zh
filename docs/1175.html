<html>
<head>
<title>Best and Worst Cases of Machine-Learning Models — Part-1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习模型的最佳和最差情况—第一部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/best-and-worst-cases-of-machine-learning-models-part-1-36cdb9296611?source=collection_archive---------1-----------------------#2020-11-24">https://pub.towardsai.net/best-and-worst-cases-of-machine-learning-models-part-1-36cdb9296611?source=collection_archive---------1-----------------------#2020-11-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f779" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="4a65" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">用什么？</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/9ba1dea6cd0993d0d001d2ed9ae0b800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AF9F9EN5x3Wn-okPkcdOKA.jpeg"/></div></div></figure><h1 id="5c9e" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak">简介:</strong></h1><p id="2b1f" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">了解我们的模型在哪里运行良好，在哪里失败，这非常重要。如果有一个低延迟的要求，KNN肯定会是一个更差的选择。类似地，如果数据是非线性的，那么选择逻辑回归是不好的，所以让我们深入讨论并找出模型的利弊。</p><h1 id="b3fc" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak"> 1) KNN: </strong></h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/fc90fc11efece467fcfb3e8be9002d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*B6ADaptU3rJWN1mXwki6eA.jpeg"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">摘自https://www . fromthegenesis . com/pros-and-cons-of-k-nearest-neighbors/</figcaption></figure><h2 id="035c" class="mt lb iq bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq iw bi translated"><strong class="ak">最佳案例:</strong></h2><ul class=""><li id="ebdf" class="ne nf iq lu b lv lw ly lz mb ng mf nh mj ni mn nj nk nl nm bi translated">当数据不是线性的时候，KNN将胜过逻辑回归和线性SVM。</li><li id="5ad3" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated">当没有延迟要求时，KNN可以很好地处理维度很少的数据。</li><li id="64f8" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated">KNN不需要训练，因此模型可以轻松处理新数据。</li></ul><h2 id="5ecf" class="mt lb iq bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq iw bi translated"><strong class="ak">限制:</strong></h2><ul class=""><li id="c658" class="ne nf iq lu b lv lw ly lz mb ng mf nh mj ni mn nj nk nl nm bi translated"><strong class="lu ja">大数据集:</strong>由于KNN是基于距离的模型，计算查询点到数据中每一点的距离的代价非常高。所以当数据很大时，Knn不是首选。</li><li id="641c" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">不同比例特征:</strong>距离测量前必须进行标准化或规范化。</li><li id="0730" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">不平衡数据:</strong>当<strong class="lu ja"> </strong>数据不平衡时，则KNN预测值偏向多数类。</li><li id="57fa" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated">由于维度效应的诅咒，KNN不能很好地管理大维度的数据。</li><li id="c7e8" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">随机数据:</strong>如果数据被严重随机化，那么KNN的表现会更差。</li></ul><h1 id="626e" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">2)逻辑回归:</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2909ea84bce9c33f5d6cf50b11e26a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*LGndP1gTFtTdFVnv.png"/></div></figure><h2 id="8721" class="mt lb iq bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq iw bi translated">最佳案例:</h2><ul class=""><li id="1d4f" class="ne nf iq lu b lv lw ly lz mb ng mf nh mj ni mn nj nk nl nm bi translated"><strong class="lu ja">低潜伏期:</strong>逻辑回归在推断阶段非常快。这只是向量乘法，很容易部署。</li></ul><blockquote class="nt"><p id="dbdf" class="nu nv iq bd nw nx ny nz oa ob oc mn dk translated">预测值=Sigmoid(W*(查询点))</p></blockquote><ul class=""><li id="2942" class="ne nf iq lu b lv od ly oe mb of mf og mj oh mn nj nk nl nm bi translated"><strong class="lu ja">特征重要性:</strong>来自逻辑回归的训练权重给出了<strong class="lu ja"> </strong>每个特征的重要性。</li><li id="9a0e" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">可解释性:</strong>逻辑回归生成的输出是概率。这比其他只给出标签作为输出的模型有优势，比如SVM。</li><li id="d988" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">基准模型:</strong>由于实现起来简单优雅，所以最好用作基准，而不是从复杂的模型开始。</li></ul><h2 id="188e" class="mt lb iq bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq iw bi translated"><strong class="ak">限制:</strong></h2><ul class=""><li id="6033" class="ne nf iq lu b lv lw ly lz mb ng mf nh mj ni mn nj nk nl nm bi translated"><strong class="lu ja">非线性数据:</strong>在现实世界中，在大多数情况下，数据会是杂乱的和非线性可分的，那么逻辑回归将表现不佳，因为模型创建的决策边界是线性的。</li></ul><div class="kp kq kr ks gt ab cb"><figure class="oi kt oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/f6300eecc3496826b955cae3de1d8bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*uQlJH4wbObbyMYCx.png"/></div></figure><figure class="oi kt oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/0a1677752b9f1bf0f8b3d8158624fd74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*_pz4wLv71yF33TWM.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk oo di op oq translated"><strong class="bd lc">非线性数据示例</strong>(摘自https://machine learning mastery . com/generate-test-datasets-python-sci kit-learn/)</figcaption></figure></div><ul class=""><li id="d82b" class="ne nf iq lu b lv or ly os mb ot mf ou mj ov mn nj nk nl nm bi translated"><strong class="lu ja">异常值的影响:</strong>异常值会转移逻辑回归创建的决策边界，因此建议将其移除以提高性能。</li><li id="2173" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated">要素中的共线性会影响要素重要性权重的可解释性。</li></ul><h1 id="24a2" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">3)决策树:</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/75edbfbbea3a5c3db24e479d6cc5304d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/0*Jqh-c2kkvJ6FFK-s.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated"><a class="ae ox" href="https://financetrain.com/decision-trees-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://finance train . com/决策树-机器学习/ </a></figcaption></figure><h2 id="0417" class="mt lb iq bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq iw bi translated">最佳案例:</h2><ul class=""><li id="2afe" class="ne nf iq lu b lv lw ly lz mb ng mf nh mj ni mn nj nk nl nm bi translated"><strong class="lu ja">低延迟:</strong>训练模型后无需计算。通过训练的if-else条件，DT预测查询点的标签。</li><li id="4a10" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">灵活性:</strong>决策树可以灵活地使用分类和数字数据执行多类分类和回归任务。</li><li id="4d84" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">特征重要度:</strong>通过对特征用于分裂节点的次数进行排序，可以得到重要特征。</li><li id="e04f" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">数据预处理:</strong>不需要对数据进行缩放、标准化或规范化，因为不需要对分割树进行距离测量。</li></ul><p id="47b8" class="pw-post-body-paragraph ls lt iq lu b lv or ka lx ly os kd ma mb oy md me mf oz mh mi mj pa ml mm mn ij bi translated"><strong class="lu ja">限制:</strong></p><ul class=""><li id="7a86" class="ne nf iq lu b lv or ly os mb ot mf ou mj ov mn nj nk nl nm bi translated"><strong class="lu ja">计算量大:</strong>决策树训练在每个节点分裂树的计算量大。必须考虑每个特征，并且必须计算信息增益。</li><li id="d574" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">决策边界:</strong>决策<strong class="lu ja">树</strong>有轴平行边界，如果你有平滑的边界，它就不能很好地工作。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/14d9691bc944612ce5507f52a3bae702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sV5MNGnrLzv3r7ed.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">摘自<a class="ae ox" href="https://www.researchgate.net/figure/An-example-of-a-decision-tree-left-with-the-decision-boundary-for-two-features-X1_fig5_313720565" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/figure/An-example-of-a-decision tree-left-with-decision-boundary-for-two-features-X1 _ fig 5 _ 313720565</a></figcaption></figure><ul class=""><li id="d7c1" class="ne nf iq lu b lv or ly os mb ot mf ou mj ov mn nj nk nl nm bi translated"><strong class="lu ja">无关变量:</strong>决策树在数据中相互影响变量。如果变量之间没有关系，DT可能不是最好的。</li></ul><h1 id="7ede" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">4) SVM:</h1><h2 id="1c69" class="mt lb iq bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq iw bi translated">最佳案例:</h2><ul class=""><li id="f789" class="ne nf iq lu b lv lw ly lz mb ng mf nh mj ni mn nj nk nl nm bi translated">非线性数据: SVMS可以解决不一定是线性的复杂问题。这可以通过使用内核化技术将<strong class="lu ja"> </strong>输入数据转换成高维数据来实现。</li></ul><blockquote class="nt"><p id="2345" class="nu nv iq bd nw nx ny nz oa ob oc mn dk translated">x1+x2)^p(多项式kenel)</p></blockquote><ul class=""><li id="9cf3" class="ne nf iq lu b lv od ly oe mb of mf og mj oh mn nj nk nl nm bi translated">x1、x2是用于将n维空间映射到m维空间的n维输入</li><li id="3d99" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">对异常值稳健:</strong>由于SVM的决策边界依赖于支持向量，异常值对边界的影响较小</li><li id="6e99" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn nj nk nl nm bi translated"><strong class="lu ja">低延迟:</strong>与其他分类器相比，推理时间非常少。</li></ul><h2 id="cda2" class="mt lb iq bd lc mu mv dn lg mw mx dp lk mb my mz lm mf na nb lo mj nc nd lq iw bi translated">局限性:</h2><ol class=""><li id="75e4" class="ne nf iq lu b lv lw ly lz mb ng mf nh mj ni mn pc nk nl nm bi translated">内核SVM的一个困难任务是为给定的问题选择正确的内核。</li><li id="e16f" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn pc nk nl nm bi translated"><strong class="lu ja">高训练时间:</strong>在大数据上训练SVM在计算上是昂贵的。</li><li id="b694" class="ne nf iq lu b lv nn ly no mb np mf nq mj nr mn pc nk nl nm bi translated"><strong class="lu ja">可解释性和特征重要性:</strong>支持向量机不能给出标签的概率输出，难以从权重中解释特征重要性。</li></ol><h1 id="c505" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">结论:</h1><p id="39fb" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">事实上，没有使用特定模型的经验法则；这完全取决于给定问题的业务需求。尝试每个模型，调整超参数，并仔细分析误差。</p><h1 id="06ed" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak">参考文献:</strong></h1><p id="e984" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">www.appliedaicourse.com</p><h1 id="4c90" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">检查:</h1><div class="pd pe gp gr pf pg"><a href="https://medium.com/towards-artificial-intelligence/time-and-space-complexity-of-machine-learning-models-df9b704e3e9c" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd ja gy z fp pl fr fs pm fu fw iz bi translated">机器学习模型的时间和空间复杂性</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">ML模型训练和测试时间复杂性的详细解释</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">medium.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu ky pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a href="https://medium.com/towards-artificial-intelligence/ace-your-machine-learning-interview-with-how-and-why-questions-a0f028a8439e" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd ja gy z fp pl fr fs pm fu fw iz bi translated">用“如何”和“为什么”的问题赢得你的机器学习面试。</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">这都是关于如何和为什么？</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">medium.com</p></div></div><div class="pp l"><div class="pv l pr ps pt pp pu ky pg"/></div></div></a></div></div></div>    
</body>
</html>