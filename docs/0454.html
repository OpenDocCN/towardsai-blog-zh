<html>
<head>
<title>K-Means Clustering from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类从零开始</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/k-means-clustering-from-scratch-4e357ac4716f?source=collection_archive---------1-----------------------#2020-05-03">https://pub.towardsai.net/k-means-clustering-from-scratch-4e357ac4716f?source=collection_archive---------1-----------------------#2020-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="22e2" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="791e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">从头开始实现，了解最强大的聚类算法之一！</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/7a39e5e097c910a3ecbe4d52206315e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*RsgSfd_S1RudtEe69gkCZQ.gif"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">K-Means算法动画</figcaption></figure><p id="0b82" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">大家好！</p><p id="67fb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">今天，让我们深入研究最著名的聚类算法:K-Means。</p><p id="f567" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">但是首先，什么是集群？聚类是一种将数据对象分成组的机器学习技术。</p><p id="906c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了实现良好的聚类，我们需要两件事情:</p><ol class=""><li id="9073" class="ma mb iq lg b lh li lk ll ln mc lr md lv me lz mf mg mh mi bi translated">来自一个集群的对象彼此非常相似/接近(高<strong class="lg ja">类内</strong>相似度)</li><li id="5265" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz mf mg mh mi bi translated">集群确实不同于/远离其他集群(低<strong class="lg ja">类间</strong>相似性)。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mo"><img src="../Images/090553ccbdd48074103a0c1801fc8e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PITguyEgzBpCR297"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">良好聚类的示例</figcaption></figure><p id="da8d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这里，聚类彼此远离(低<strong class="lg ja">类间</strong>相似性)，并且在每个聚类内，数据点接近(高<strong class="lg ja">类内</strong>相似性)。可以说是很好的聚类！</p><blockquote class="mp mq mr"><p id="d34b" class="le lf ms lg b lh li ka lj lk ll kd lm mt lo lp lq mu ls lt lu mv lw lx ly lz ij bi translated">注意:像<a class="ae mw" href="https://medium.com/k-nearest-neighbors-from-scratch-633dfbeac740" rel="noopener">K-最近邻</a>一样，K-Means需要选择它的“K”个质心作为函数的输入。</p></blockquote><p id="586b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在这篇文章中，我们将使用<em class="ms"> iris-dataset </em>，因为它对于初学者和聚类问题来说是一个众所周知的数据集。</p><p id="b3a4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ms"> iris数据集</em>呈现了3种花:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mx"><img src="../Images/5012f5a98b5cdd5bc2141de103702b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jOVlQBHtwqkIhSnW-sEo-A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">虹膜数据集的三个种类</figcaption></figure><p id="d4e0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们的目标是创建3个集群，每个物种一个。如果算法表现良好，它会将来自同一物种的花分组在一起。</p><p id="3f6c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，我们将能够通过让算法将新的数据点分配给其中一个簇来预测一种新的花的种类。</p><h1 id="596c" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated"><strong class="ak"> K均值算法</strong></h1><p id="3b5f" class="pw-post-body-paragraph le lf iq lg b lh nq ka lj lk nr kd lm ln ns lp lq lr nt lt lu lv nu lx ly lz ij bi translated">那么它是如何工作的呢？</p><p id="140f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">K均值算法(也称为劳埃德算法)由3个主要步骤组成:</p><ul class=""><li id="daa3" class="ma mb iq lg b lh li lk ll ln mc lr md lv me lz nv mg mh mi bi translated">将<em class="ms"> K </em>形心放置在随机位置(这里<em class="ms"> K </em> =3)</li><li id="a4d7" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz nv mg mh mi bi translated">将所有数据点分配到最近的质心(使用欧几里德距离)</li><li id="03c2" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz nv mg mh mi bi translated">计算新的质心作为聚类中所有点的平均值</li></ul><p id="4b8f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一旦质心停止从一次迭代移动到另一次迭代(我们说算法收敛)，我们停止算法并返回每个数据点的指定聚类。</p><p id="b6af" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，让我们编码吧！</p><p id="a6da" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 1。在随机位置放置K个质心</strong></p><p id="1f4f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">放置质心意味着创建一个新的数据点。一种简单的方法是选择数据集的一个随机点，并将质心放在同一个位置。</p><p id="33fc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，我们可以创建一个空列表，并用与数据集中的随机点相同的信息存储我们的<em class="ms"> K </em> =3个质心。</p><blockquote class="mp mq mr"><p id="570a" class="le lf ms lg b lh li ka lj lk ll kd lm mt lo lp lq mu ls lt lu mv lw lx ly lz ij bi translated">注意:这个函数是算法的第一步，只被调用一次。</p></blockquote><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="fe2d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 2。将所有数据点分配到最近的质心</strong></p><p id="1a3b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们现在要比较每个点与3个质心的距离，并将数据点分配给最近的质心。</p><blockquote class="mp mq mr"><p id="12e5" class="le lf ms lg b lh li ka lj lk ll kd lm mt lo lp lq mu ls lt lu mv lw lx ly lz ij bi translated">例如，更靠近质心<em class="iq"> K </em> =1的所有点将被分配给聚类‘1’。</p></blockquote><p id="d183" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为此，我们为每个数据点创建一个列表(<em class="ms">dist _ point _ cluster</em>)来存储到3个质心的距离。</p><p id="20b9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，使用<em class="ms"> argmin() NumPy </em>函数，我们访问最近的质心并将其存储——即其编号<em class="ms"> K - </em>到我们的<em class="ms">赋值</em>列表中。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="e883" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一旦所有数据点都被分配到它们最近的聚类，我们返回<em class="ms">分配</em>列表，并且…步骤2完成！</p><p id="15cc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3。计算新的质心，作为集群中所有点的平均值</strong></p><p id="b5d5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">要将我们的质心放在聚类的中心，我们必须计算聚类中所有数据点的平均值，并将质心放在这个位置。</p><p id="84a0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">渐渐地，这个函数会把我们的质心带到最终的聚类中间。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/3c73ba16447f636aef7437cc91d1a66f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*si68Y22qIsggFjxCU55Gaw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">K-均值中连续迭代的可视化</figcaption></figure><blockquote class="mp mq mr"><p id="e0cf" class="le lf ms lg b lh li ka lj lk ll kd lm mt lo lp lq mu ls lt lu mv lw lx ly lz ij bi translated">注意:如果您觉得某些点不属于正确的聚类，请不要担心。我们使用所有特征来计算距离，但是在2D空间中绘制结果。这样我们就失去了信息，绘图也没有显示出对真实距离的准确估计。</p></blockquote><p id="bb2c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">新的质心等于分配给它们的所有数据点的平均值。</p><p id="31cd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于每个<em class="ms"> K </em> =3个质心，我们遍历所有数据点并查看它们的聚类分配。如果它们属于我们在当前循环(I)中查看的<em class="ms"> K </em>簇，我们将它们添加到<em class="ms"> pt_cluster </em>列表中。然后，<em class="ms"> K </em> =i的新质心是<em class="ms"> pt_cluster中数据点的平均值。</em></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="a84a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们的<em class="ms"> new_centroids() </em>函数返回K=3 <em class="ms"> new_centroids的列表。我们完成了第三步。</em></p><p id="e48c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">太好了！</p><p id="2170" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在我们只需要运行算法，直到它收敛。</p><p id="2b28" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">但是我们怎么知道它已经收敛了呢？两个选项:</p><ol class=""><li id="9b13" class="ma mb iq lg b lh li lk ll ln mc lr md lv me lz mf mg mh mi bi translated">它达到最大迭代次数(定义为我们算法的输入)</li><li id="54cd" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz mf mg mh mi bi translated">两次迭代之间的<strong class="lg ja">误差</strong>小于我们的容许水平(定义为我们算法的输入)</li></ol><p id="e760" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">但是这里的错误是什么呢？在K-means中，误差是指数据点和质心之间的距离。</p><p id="9620" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于这个例子，我们选择我们的最大迭代次数，<em class="ms"> max_iter </em> = 100，以及容差水平，<em class="ms"> tol </em> = 0.001。</p><p id="13fc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">容差水平是两次连续迭代的误差平方和(SSE)之差。</p><p id="c66f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果质心移动不多，SSE的差值将接近0，如果小于0.001，那么我们的算法已经收敛。我们可以停止执行并返回最终结果。</p><blockquote class="mp mq mr"><p id="82ad" class="le lf ms lg b lh li ka lj lk ll kd lm mt lo lp lq mu ls lt lu mv lw lx ly lz ij bi translated">注:停止标准(或公差水平)定义如下:</p></blockquote><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="a136" class="oe mz iq oa b gy of og l oh oi"><strong class="oa ja">if</strong> np.absolute(all_sse[it] - all_sse[it-1])/all_sse[it-1] &lt;= tol</span></pre><p id="c436" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">所以现在，我们有一个额外的步骤要做:创建一个为每次迭代返回SSE的函数。</p><p id="fe93" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 4。计算停止准则的误差平方和。</strong></p><p id="c09a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为此，我们遍历每个数据点，并计算与其指定质心的距离(<em class="ms">即</em>误差)。我们将误差平方，并将其添加到<em class="ms">误差</em>列表中。通过对所有误差求和，我们得到SSE。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="3b51" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">就是这样！</p><p id="e5ed" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们现在可以在主K-Means函数中调用我们的4个函数。</p><p id="02f5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们可以返回函数的结果:</p><ul class=""><li id="abe0" class="ma mb iq lg b lh li lk ll ln mc lr md lv me lz nv mg mh mi bi translated">分配(每个数据点的聚类数列表- 0到2)</li><li id="b1ed" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz nv mg mh mi bi translated">每个簇的质心</li><li id="f00b" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz nv mg mh mi bi translated">每次迭代的SSE</li><li id="c751" class="ma mb iq lg b lh mj lk mk ln ml lr mm lv mn lz nv mg mh mi bi translated">迭代次数</li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nw nx l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">k-均值主函数</figcaption></figure><p id="4da5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">您可以通过绘制由分类编号着色的质心和数据点来可视化您的结果。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/c964950f74e68919ed6a24f2efb2c80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bA-_T20Gnd1zk1fN5xb9sA.png"/></div></div></figure><p id="e8eb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">搞定了。我们刚刚用4个简单的步骤实现了K-Means算法。</p><blockquote class="mp mq mr"><p id="d060" class="le lf ms lg b lh li ka lj lk ll kd lm mt lo lp lq mu ls lt lu mv lw lx ly lz ij bi translated">注意:完整的代码可以在我的GitHub上找到，<a class="ae mw" href="https://github.com/Theob0t/Medium/blob/master/K-means-implementation.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></blockquote><p id="19c1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果你喜欢钻研最著名的聚类算法背后的数学，请随意查看我在<a class="ae mw" href="https://medium.com/ai-in-plain-english/k-nearest-neighbors-from-scratch-633dfbeac740" rel="noopener">K-最近邻</a>上的帖子。</p><p id="76be" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">感谢阅读！</strong></p></div></div>    
</body>
</html>