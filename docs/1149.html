<html>
<head>
<title>Create your First Text Generator with LSTM in few minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用LSTM在几分钟内创建你的第一个文本生成器</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/create-your-first-text-generator-with-lstm-in-few-minutes-3b59ee139ca0?source=collection_archive---------3-----------------------#2020-11-16">https://pub.towardsai.net/create-your-first-text-generator-with-lstm-in-few-minutes-3b59ee139ca0?source=collection_archive---------3-----------------------#2020-11-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="650d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/6882364806d3a9c5674ad9776f166641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xo8jOf0k9BVWI48s"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated"><a class="ae kl" href="https://unsplash.com/@retrosupply?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拍照</a>在<a class="ae kl" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">上反溅</a></figcaption></figure><p id="5a29" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我告诉你，一整部简短的科幻电影是由一个建立在LSTM递归神经网络上的人工智能机器人编写的，它甚至获得了积极的评论和批评，惊讶吧？！嗯，我肯定你是，因为这是我第一次看《太阳之春》的感觉，我的意思是，我知道它不能与史蒂芬·斯皮尔伯格或亚历克斯·嘉兰的编剧质量相比，但难怪在未来几年内，人工智能机器人会在奥斯卡金像奖上与他们竞争。事实上，我们不应该再惊讶于人工智能能够颠覆我们的世界，让它成为一个更好、“更容易”和最舒适的居住地。</p><p id="506d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我看来，在所有的人工智能子领域中，NLP拥有最酷和最令人兴奋的应用。其中之一是文本生成，我们应该深入研究它。</p><p id="4abc" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这篇文章中，我将简要解释RNN和LSTM是如何工作的，以及我们如何在Python中使用LSTM生成文本。</p><p id="a21d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我们开始之前，对于那些还没看的人，我想让你们去看看《太阳之春》<a class="ae kl" href="https://www.youtube.com/watch?v=LY7x2Ihqjmc" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=LY7x2Ihqjmc</a></p><h2 id="8866" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated"><strong class="ak">目录</strong></h2><p id="eca8" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated"><a class="ae kl" href="#df05" rel="noopener ugc nofollow"> 1。递归神经网络</a> <br/> <a class="ae kl" href="#83cd" rel="noopener ugc nofollow"> 2。长短期记忆</a><br/>T22】3。正文生成t24】∘<a class="ae kl" href="#119a" rel="noopener ugc nofollow">1。文字处理</a> <br/> ∘ <a class="ae kl" href="#3587" rel="noopener ugc nofollow"> 2。创建批次</a> <br/> ∘ <a class="ae kl" href="#acc5" rel="noopener ugc nofollow"> 3。创作&amp;培训模特</a>t33】∘<a class="ae kl" href="#2274" rel="noopener ugc nofollow">4。生成文本</a></p><h1 id="df05" class="mi lm iq bd ln mj mk ml lq mm mn mo lt mp mq mr lw ms mt mu lz mv mw mx mc my bi translated">1.递归神经网络</h1><p id="afa4" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">文本、音频、视频、时序数据……它们有什么共同点？答案很清楚，它们每一个都是由有序序列组成的，其中每一个组块都依赖于前面的那些才有意义。这使得传统的前馈网络无法处理这种数据，因为它们没有任何时间依赖性或记忆效应。这就是rnn发挥作用的地方，通过引入一个额外的维度，这是它们在处理序列数据以创建预测和生成模型方面比其他类型的ANN更有效的时间。</p><p id="e094" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它是这样工作的:每个生成的输出不仅依赖于它的输入，还依赖于在以前的时间步骤中输入到网络的输入的整个历史。这可以归纳为两个主要等式:</p><p id="1794" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">内部状态更新:</strong></p><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/bea8807b3f47ffe144c4d71ebe37af24.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/1*87fWA108Icd9_yt6Nr-ibQ.gif"/></div></figure><p id="eae9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">输出更新:</strong></p><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/96c3eeeb626aa126f51ce86100e98105.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/1*d1oXrzHyl3IQtUee7t0bsA.gif"/></div></figure><figure class="na nb nc nd gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi nf"><img src="../Images/c05fb21d22e2e02195c01e7f923c30f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgEOMaeJuSnq5r-h6G8eeA.png"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">展开的RNN</figcaption></figure><ul class=""><li id="902e" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj nl nm nn no bi translated">X(t)是输入，Y(t)是输出，H(t)是当前时间步的内部状态(谁扮演内存的角色)，H(t-1)是前一个时间步的内部状态。</li><li id="a7db" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj nl nm nn no bi translated">u、W和V是网络共享的权重。事实上，在RNN中，所有步骤共享相同的参数，这主要减少了参数的数量和计算成本。</li></ul><p id="78ec" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在时间步骤t=0，<strong class="ko ja"> X0 </strong>被传递到网络，在网络中被单元分析和处理，以便计算<strong class="ko ja"> H0 </strong>并产生输出<strong class="ko ja"> Y0 </strong>，然后在下一个时间步骤t=1，<strong class="ko ja"> X1 </strong>与<strong class="ko ja"> H0 </strong>合并，以计算新的内部状态<strong class="ko ja"> H1 </strong>，然后产生<strong class="ko ja"> Y1 </strong>，并且相同的过程继续运行，直到第n个时间步骤。</p><h2 id="a21c" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated"><strong class="ak"> RNN问题</strong></h2><p id="7202" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">RNN的应用是多方面的，可以在任何地方找到，它用于股票预测，语音识别，文本摘要和生成，搜索引擎，机器翻译…所以为了给出准确的结果，它必须在“无限”的序列上进行训练，这导致了两个主要问题:</p><ul class=""><li id="8466" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj nl nm nn no bi translated"><strong class="ko ja">渐变消失</strong></li><li id="4cc6" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj nl nm nn no bi translated"><strong class="ko ja">渐变爆炸</strong></li></ul><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/29170b1f5f97348b85d06b4b606152dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/1*cTdjagMRS-lqWI_JnP60vg.gif"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">更新权重</figcaption></figure><p id="c159" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们知道，通过<a class="ae kl" href="https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c" rel="noopener"><strong class="ko ja"/></a><strong class="ko ja"/>(在这种情况下为<a class="ae kl" href="https://en.wikipedia.org/wiki/Backpropagation_through_time#:~:text=Backpropagation%20through%20time%20(BPTT)%20is,independently%20derived%20by%20numerous%20researchers." rel="noopener ugc nofollow" target="_blank"> <strong class="ko ja">通过时间</strong> </a>的反向传播)计算梯度的目标是找到将最小化每个连接的总损失的最佳权重，因此如果序列太长并且在梯度太小(小于1)的情况下，权重将保持不变， 意味着网络将无法学习和回忆第一层中发生的事情，这就是<strong class="ko ja">消失梯度的问题，</strong>但是，在梯度过大(大于1)的相反一侧，学习将会发散，因为权重将远离最佳值，这就是<strong class="ko ja">爆炸梯度的问题。 </strong></p><h1 id="83cd" class="mi lm iq bd ln mj mk ml lq mm mn mo lt mp mq mr lw ms mt mu lz mv mw mx mc my bi translated">2.<strong class="ak">长短期</strong> - <strong class="ak">期限记忆</strong></h1><p id="5990" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">正如我们所见，rnn只在小序列上工作良好，因为它们不能长时间记忆信息。这就是LSTM加入游戏的地方。由于其<strong class="ko ja">门控细胞结构</strong>，它克服了传统RNN的局限性，建立了长期依赖性，并能够在多个时间步长内回忆和记忆数据，这使得LSTM成为迄今为止最受欢迎和最有效的递归神经网络模型之一。</p><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/8e19360cadbafd86035d4d1ae4327107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*M7Iq7jXXOPt-qoB3mHxWLg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">LSTM细胞</figcaption></figure><p id="146e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">每个LSTM单元由一个简单的RNN细胞和另一个负责长期记忆的细胞以及3个门组成:输入门、输出门和遗忘门。它们就像过滤器一样，决定哪些信息会被网络记住，哪些信息会被网络遗忘。</p><p id="0ab6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">整个过程是这样的:</p><ul class=""><li id="3a72" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj nl nm nn no bi translated">在第一步中，通过将<strong class="ko ja"> sigmoid层</strong>应用于当前输入<strong class="ko ja"> x(t) </strong>和前一时间步<strong class="ko ja"> h(t-1) </strong>的隐藏状态的组合，遗忘门评估哪些数据将从单元状态中删除，它产生0和1之间的值，因为0意味着该信息将被删除，1意味着该信息将被保留。</li></ul><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/dc01283ba10fbd7e1aa9d2809d2d98d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/1*sTkhop0liKVA7fwyty_r3g.gif"/></div></figure><ul class=""><li id="ae6b" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj nl nm nn no bi translated">在第二步中，输入门通过组合<strong class="ko ja"> sigmoid层</strong>的输出和<strong class="ko ja"> tanh层</strong>的输出来控制是否更新存储单元。应用于<strong class="ko ja"> x(t) </strong>和<strong class="ko ja"> h(t-1) </strong>的sigmoid层产生范围从0到1的值，其中1表示该信息很重要，需要添加到单元状态，而0则不然。tanh层返回-1和1之间的值，以帮助调节网络，并创建一个新的候选值向量<strong class="ko ja">(t)</strong>，可以添加到状态中。</li></ul><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/76a9f9251abe9d9cf2bab42df0361fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/1*Rrr69TA0o2V0I8CbS8l7UA.gif"/></div></figure><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/5ea9a662a32d7e23580228a3d1c280ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/1*0O9xag6sBQe7qf46DPdQ_g.gif"/></div></figure><ul class=""><li id="49bf" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj nl nm nn no bi translated">然后，在下一步中，基于先前步骤的计算创建新的单元状态。</li></ul><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c7052b4d746406f1d81504c139e68745.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/1*Q_eiK5j-S9I-ZgKA99dgbQ.gif"/></div></figure><ul class=""><li id="07e7" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj nl nm nn no bi translated">最后，在最后一步，输出门选择下一个隐藏状态应该是什么。首先，将先前的隐藏状态和当前输入传递到<strong class="ko ja"> sigmoid层</strong>以产生o(t)，然后将其乘以<strong class="ko ja">双曲正切函数</strong>的输出，该函数应用于先前步骤中新计算的单元状态。</li></ul><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/14d26a95f553ee5beecc1432010aa32c.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/1*8TMwxHvFpMjCrqW9VM-Iig.gif"/></div></figure><figure class="na nb nc nd gt ka gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e04c8612c01c6e9038eecd04c0c74965.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/1*XJ4fYDoycNmDUuIgSmrZaw.gif"/></div></figure><h1 id="2c63" class="mi lm iq bd ln mj mk ml lq mm mn mo lt mp mq mr lw ms mt mu lz mv mw mx mc my bi translated">3.文本生成</h1><p id="3ee7" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">根据定义，文本生成是基于巨大的文本语料库由神经网络自动产生新文本的机制，所以让我们看看它是如何使用LSTM制作的。</p><p id="3bbc" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先需要导入<strong class="ko ja"> Numpy </strong>、<strong class="ko ja"> Pandas、</strong>和<strong class="ko ja"> Tensorflow </strong>库。对于数据集，我们将选择莎士比亚的所有作品，主要有两个原因:</p><ol class=""><li id="0023" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj oc nm nn no bi translated">这是一个很大的文本语料库。通常建议您至少拥有总计100万个字符的源，以获得逼真的文本生成。</li><li id="60fe" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj oc nm nn no bi translated">它有非常独特的风格。由于文本数据使用旧式英语，并且是以舞台剧的风格格式化的，如果模型能够再现类似的结果，这对于我们来说将是非常明显的。</li></ol><p id="4e6e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你可以下载莎翁的数据集，或者从古腾堡获取任何你想要的免费文本:<a class="ae kl" href="https://www.gutenberg.org/" rel="noopener ugc nofollow" target="_blank">https://www.gutenberg.org/</a></p><figure class="na nb nc nd gt ka"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="308d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">文本看起来是这样的。</p><pre class="na nb nc nd gt of og oh oi aw oj bi"><span id="ceb6" class="ll lm iq og b gy ok ol l om on">                          1   <br/>From fairest creatures we desire increase,   <br/>That thereby beauty's rose might never die,   <br/>But as the riper should by time decease,   <br/>His tender heir might bear his memory:   <br/>But thou contracted to thine own bright eyes,   <br/>Feed'st thy light's flame with self-substantial fuel,   <br/>Making a famine where abundance lies,   <br/>Thy self thy foe, to thy sweet self too cruel:   <br/>Thou that art now the world's fresh ornament,   <br/>And only herald to the gaudy spring,   <br/>Within thine own</span></pre><h2 id="119a" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated">1.文本处理</h2><p id="34ed" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated"><strong class="ko ja">文本矢量化</strong></p><p id="6576" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们知道神经网络不能接受原始字符串数据，所以我们需要通过给每个字符分配数字来对其进行编码。让我们创建两个字典，它们可以从数字索引到字符，从字符到数字索引。</p><figure class="na nb nc nd gt ka"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="3587" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated">2.创建批处理</h2><p id="eeab" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">总的来说，我们试图实现的是让模型预测给定历史字符序列的下一个最高概率的字符。由我们(用户)来选择这个历史序列有多长。太短的序列和我们没有足够的信息(例如，给定字母“a”，下一个是什么)，太长的序列和训练将花费太长的时间，并且很可能对与更远的字符不相关的字符进行排序。虽然没有正确的序列长度选择，但您应该考虑文本本身、正常短语的长度以及什么字符/单词彼此相关的合理想法。</p><p id="c08b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">训练序列</strong></p><p id="53dc" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实际的文本数据将是向前移动一个字符的文本序列。例如:</p><p id="ee98" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">顺序输入:“你好，我的名字”顺序输出:“你好，我的名字”</p><figure class="na nb nc nd gt ka"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="5c34" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> batch </strong>方法将这些单独的字符调用转换成序列，我们可以批量输入。我们使用seq_len+1是因为零索引。</p><p id="32ae" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">drop_remainder的意思是:drop_remainder:(可选。)一个<code class="fe oo op oq og b">tf.bool</code>标量<code class="fe oo op oq og b">tf.Tensor</code>，代表该情况下是否应该丢弃最后一批，它比<code class="fe oo op oq og b">batch_size</code>元素少；默认行为是不丢弃较小的批处理。</p><pre class="na nb nc nd gt of og oh oi aw oj bi"><span id="6593" class="ll lm iq og b gy ok ol l om on">sequences = char_dataset.batch(seq_len+1, drop_remainder=True)</span></pre><ol class=""><li id="a756" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj oc nm nn no bi translated">抓取输入文本序列</li><li id="c4f9" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj oc nm nn no bi translated">将目标文本序列指定为向前移动一步的输入文本序列</li><li id="74c6" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj oc nm nn no bi translated">将它们分组为一个元组</li></ol><figure class="na nb nc nd gt ka"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="b2d1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">生成训练批次</strong></p><p id="258e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在我们有了实际的序列，我们将创建批处理。我们希望将这些序列随机排列，这样模型就不会过度适应文本的任何部分，而是可以在给定任何种子文本的情况下生成字符。</p><figure class="na nb nc nd gt ka"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="acc5" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated">3.创建和训练模型</h2><p id="6eb7" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">我们将使用一个具有一些额外功能的基于LSTM的模型，包括一个嵌入层和两个LSTM层。我们将这个模型架构基于<a class="ae kl" href="https://deepmoji.mit.edu/" rel="noopener ugc nofollow" target="_blank"> DeepMoji </a>。</p><p id="c4c3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">嵌入层将作为输入层，它实质上创建了一个查找表，将每个字符的数字索引映射到一个具有“嵌入维数”的向量。可以想象，这个嵌入大小越大，训练就越复杂。这类似于word2vec背后的思想，将单词映射到某个n维空间。在直接馈入LSTM之前嵌入通常会产生更真实的结果。</p><figure class="na nb nc nd gt ka"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="2274" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated">4.生成文本功能</h2><p id="c1d4" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">目前，我们的模型一次只需要128个序列。我们可以创建一个新模型，它只要求batch_size=1。我们可以用这个批量创建一个新模型，然后加载我们保存的模型的权重。那就打电话。在模型上构建()，然后我们创建一个生成新文本的函数。</p><figure class="na nb nc nd gt ka"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="5254" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">瞧啊。！这是生成的文本的样子</p><figure class="na nb nc nd gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi or"><img src="../Images/3028c12ff40072e07af9a62ee65c91c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CJN3CGzt9lPpsIY4ow_tdQ.png"/></div></div></figure><p id="72da" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你可能会注意到，这些生成的场景有些不真实，但你可以压缩一些参数，还可以添加一个丢弃层以避免过度拟合，然后该模型可以更好地生成文本。</p></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><h2 id="888e" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated">参考资料和其他资源:</h2><p id="ef64" class="pw-post-body-paragraph km kn iq ko b kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">[1]<a class="ae kl" href="https://en.wikipedia.org/wiki/Sunspring" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Sunspring</a></p><p id="b550" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[2]<a class="ae kl" href="https://medium.com/analytics-vidhya/understanding-rnns-652b7d77500e" rel="noopener">https://medium . com/analytics-vid hya/understanding-rnns-652 b7d 77500 e</a></p><p id="d7a1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[3]<a class="ae kl" href="https://towardsdatascience.com/understanding-rnns-lstms-and-grus-ed62eb584d90" rel="noopener" target="_blank">https://towards data science . com/understanding-rnns-lst ms-and-grus-ed 62 EB 584d 90</a></p><p id="9277" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[4]<a class="ae kl" href="https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/text-generation-lstm-recurrent-neural-networks-python-keras/</a></p></div></div>    
</body>
</html>