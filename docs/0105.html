<html>
<head>
<title>Techniques in Self-Attention Generative Adversarial Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自我关注生成对抗网络中的技术</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/techniques-in-self-attention-generative-adversarial-networks-22f735b22dfb?source=collection_archive---------0-----------------------#2019-07-15">https://pub.towardsai.net/techniques-in-self-attention-generative-adversarial-networks-22f735b22dfb?source=collection_archive---------0-----------------------#2019-07-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9354" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">自我关注甘(SAGAN) | <a class="ae ep" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">对艾</a></h2><div class=""/><div class=""><h2 id="37ac" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">关于SAGAN的不同方法的讨论，如光谱归一化、条件批量归一化等。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/c3df660dc5d7417f59fc9c11aee52229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FryeFtwSXB5hHBNN9Tx1gQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">经过12万次迭代后，我在celebA数据集上对SAGAN 的<a class="ae le" href="https://github.com/xlnwel/cv/tree/master/algo/sagan" rel="noopener ugc nofollow" target="_blank">实现生成的图像</a></figcaption></figure><h2 id="c1a1" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">介绍</h2><p id="21f9" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">张寒等人在PMLR 2019年提出的自我注意生成对抗网络(SAGAN)结构，经实验证明，在图像合成方面明显优于现有的工作。在本文中，我们讨论了SAGAN中涉及到的几种技术，包括自注意、谱归一化、条件批量归一化、投影鉴别器等。</p><p id="aee3" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja">奖励</strong>:我们会给每个关键组件一个简单的示例代码，但是你应该知道这里提供的代码只是为了说明的目的而简化的。整个实现，你可以参考GitHub上<a class="ae le" href="https://github.com/xlnwel/cv/tree/master/algo/sagan" rel="noopener ugc nofollow" target="_blank">我的萨根回购</a>，或者来自Google Brain 的<a class="ae le" href="https://github.com/brain-research/self-attention-gan" rel="noopener ugc nofollow" target="_blank">官方实现。</a></p><h2 id="1469" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">自我关注</h2><p id="539c" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated"><strong class="mc ja">动机</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi my"><img src="../Images/0f9c66c1193256facc2a4ca329aeb792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-SICHy5Yc0IT10Usl5IKg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:自我关注生成对抗网络。</figcaption></figure><p id="d865" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">GANs在结构纹理建模方面取得了成功，但它们往往无法捕捉到在某些类别中持续出现的几何或结构模式。例如，合成的狗通常绘制有逼真的毛发纹理，但没有明确定义的单独的脚。对此的一种解释是，卷积层擅长捕捉局部结构，但在发现长程相关性方面有困难:1)。尽管深度ConvNets在理论上能够捕捉长期依赖性，但优化算法很难找到仔细协调多个层以捕捉这些依赖性的参数值，并且这些参数在统计上可能是脆弱的，并且在应用于以前未见过的数据时容易失败。2).大的卷积核增加了表示能力，但是计算效率更低。</p><p id="817a" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">另一方面，自我关注在模拟长期依赖性的能力和计算及统计效率之间表现出更好的平衡。基于这些想法，张寒等人建议萨根斯在卷积高斯模型中引入自我注意机制。</p><p id="b67b" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja">对图像的自我关注</strong></p><p id="82f3" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">我们已经在<a class="ae le" href="https://medium.com/towards-artificial-intelligence/attention-is-all-you-need-transformer-4c34aa78308f?source=friends_link&amp;sk=a259e84597d542f812a155711e9c8e97" rel="noopener">之前的文章</a>中讨论了自我注意机制，它被应用于3D序列数据以捕捉时间依赖性。为了将自我关注应用到图像中，张寒等人建议进行三大修改:</p><ol class=""><li id="9e23" class="mz na iq mc b md mt mg mu lo nb ls nc lw nd ms ne nf ng nh bi translated">用1乘1卷积层替换全连接层。</li><li id="323a" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated">在计算注意力之前，将4D张量整形为3D张量(合并高度和宽度),之后再将它们整形回来。</li><li id="f10f" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated">将注意力图层的输出乘以比例参数，然后添加回输入要素地图:</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9cebbb26793a3e21c0788755ca7da0c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*L0wp4-zjogdAMxZUnwNwAA.png"/></div></figure><ul class=""><li id="b690" class="mz na iq mc b md mt mg mu lo nb ls nc lw nd ms no nf ng nh bi translated">其中<em class="np"> o </em>是注意力层的输出，而<em class="np"> γ </em>是可学习的标量，它被初始化为0。引入可学习的<em class="np"> γ </em>允许网络首先依赖本地邻域中的线索——因为这更容易——然后逐渐学习给非本地证据分配更多权重。我们这样做的原因很简单:我们想先学习简单的任务，然后逐步增加任务的复杂性。[1]</li></ul><p id="7cac" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja"> Python代码</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/ed0bc514d005db31a6b3b1b1e7e09fc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NUSCTPPAAMWXqgHN-2DS2Q.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">用4D张量实现自我关注的Python代码</figcaption></figure><h2 id="1786" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">光谱归一化</h2><p id="00a6" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated"><strong class="mc ja">动机</strong></p><p id="7228" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在讨论频谱归一化的细节之前，我们先简要介绍一些基本概念，以确保我们在同一页上。</p><ol class=""><li id="c533" class="mz na iq mc b md mt mg mu lo nb ls nc lw nd ms ne nf ng nh bi translated">函数的平坦局部最小值对输入扰动不太敏感。</li><li id="c041" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated">Hessian矩阵描述了多变量函数<a class="ae le" href="https://math.stackexchange.com/a/492420/401382" rel="noopener ugc nofollow" target="_blank">在局部最小值</a>处的局部曲率；它测量一个函数在局部最小值时对其输入的敏感度。</li><li id="ed8d" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated">实矩阵的谱范数等于其最大奇异值。具体地，对于对称实矩阵(例如，Hessian矩阵)，其谱范数是其最大特征值。关于谱范数和K-Lipschitz连续函数的相应概念的更详细讨论，请参考[ <a class="ae le" href="https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html" rel="noopener ugc nofollow" target="_blank"> 6 </a> ]。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/62640fe795b3e92063264d142fe153f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DTgK4Vat0HPzCT1f9d13WA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">黑色曲线上平坦的局部最小值被投影到测试函数(红色虚线曲线)的局部最小值附近的某处(蓝色菱形)，而尖锐的局部最小值投影偏离测试函数的局部最小值。来源:关于深度学习的大批量训练:泛化差距和尖锐极小值</figcaption></figure><p id="d89c" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在[3]中，Yuichi Yoshida等人强调，损失函数的平坦局部最小值比尖锐局部最小值概括得更好(根据(1))，并且他们将平坦性公式化为损失函数的Hessian矩阵的特征值(根据(2))。遵循这一思想，他们证明，在某些约束下，为了实现平坦的局部最小值，在每一层限制权重矩阵的谱范数是足够的(部分地根据(3))。因此，他们建议像L2正则化一样正则化损失函数中每个权重矩阵的谱范数。</p><p id="5611" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">基于Y. Yoshida的工作，Takeru Miyato等人在[2]中开发了频谱归一化，它明确地归一化每一层中权重矩阵的频谱范数，使得它满足Lipschitz约束<em class="np"> σ(W)=1 </em>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/be598c2825d795130d16e6676f2861c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*0fx4ErOt_e0i_5DGCcLSRw.png"/></div></figure><p id="0e3d" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">其中<em class="np"> σ(W)=1 </em>是<em class="np"> W </em>的谱范数，一个常数。我们可以通过展示来验证它的谱范数</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1da5f3339a3794bf72cc3eca11d78f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*5-5uA-MTQZ7ieVqDLxJwVg.png"/></div></figure><p id="e7e3" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">Takeru Miyato等人进一步证明，谱归一化使<em class="np"> W </em>的梯度规则化，防止<em class="np"> W </em>的列空间向一个特定方向集中。这阻止了每层的转换在一个方向上变得敏感。</p><p id="f848" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja">如何计算谱范数？</strong></p><p id="582f" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">假设<em class="np"> W </em>的形状为<em class="np"> (N，M) </em>并且我们有一个随机初始化的向量<em class="np"> u. </em>幂迭代法计算<em class="np"> W </em>的谱范数如下</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/034be3999f6f91a8069fba4c388fe5cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*cAWCQpJOsx1nmiSICAfrzw.png"/></div></figure><p id="3706" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">其中<em class="np"> u </em>和<em class="np"> v </em>近似为<em class="np"> W </em>的第一个左右奇异向量。实际上，<em class="np"> T=1 </em>就足够了，因为我们也会逐渐更新<em class="np"> W </em>。</p><p id="2764" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja"> Python代码</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/a5b07d03077fd86c62c75306a7e09558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k0I6lJpFLZZzv0uNrBKuXA.png"/></div></div></figure><p id="7c59" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja">条件批量规范化</strong></p><p id="7d2d" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">条件批处理规范化首先由Harm de Vries、Florian Strub等人提出[4]。中心思想是将批规范化的<em class="np"> γ </em>和<em class="np"> β </em>条件化在某个<em class="np"> x </em>(例如语言嵌入)上，这是通过将<em class="np"> f(x) </em>和<em class="np"> h(x) </em>分别添加到<em class="np"> γ </em>和<em class="np"> β </em>上来完成的。这里，<em class="np"> f </em>和<em class="np"> h </em>可以是任何函数(例如一个隐藏层MLP)。这样，他们可以用最小的开销将一些附加信息合并到预先训练的网络中。</p><p id="6041" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">通过将类标签集成到生成器和鉴别器中，可以将SAGAN实现为条件gan(cgan)的一种形式。在生成器中，这是通过有条件的批量标准化层来实现的，其中我们给每个标签一个特定的gamma和beta。在鉴别器中，这是通过投影来完成的，我们将在下一节很快看到这种方法。在这里，我们提供了[7]中的条件批处理规范化的代码和一些注释。</p><p id="9220" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja"> Python代码</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/a0a0eaadd69da9987dd86891be34b252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXfvj64IWDqp2gWfUODVQw.png"/></div></div></figure><p id="eec1" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja">投影鉴别器</strong></p><p id="22de" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在[5]中，Takeru Miyato提出将类别标签纳入鉴别器。为了了解其工作原理，我们将条件鉴别器表示为<em class="np"> D(x，y)=σ(f(x，y)) </em>，其中<em class="np"> f(x，y) </em>是<em class="np"> x </em>和<em class="np"> y </em>的函数。我们首先通过将<em class="np"> D </em>的导数设置为零来导出最佳鉴别器</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/b7593fb796ca6481fae625ed26e393df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bNzQqZfs_i2iyeTigjDRw.png"/></div></div></figure><p id="9994" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">求解这个方程，我们得到了最佳鉴别器</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6dd8362adfe38307d0432841e4691491.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*gbDCFCaXKWXrPtBYcZe97g.png"/></div></figure><p id="41a8" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">通过用<em class="np"> σ(f(x，y)) </em>替换鉴别器，我们得到</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/57f19b6592e72bd39bcba2e8a9465f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*fiq2eAKtVIV_uph_xLngSw.png"/></div></figure><p id="5ac1" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">这给了我们逻辑</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/76a428917278041beaa8eb8a9750b018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*YuZeWblFrXKcgqLu4ivCkA.png"/></div></figure><p id="48c9" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">现在我们仔细看看<em class="np"> p(y|x) </em>，这是一个分类分布，通常表示为softmax函数。其对数线性模型为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4f1384648f44bf8abdc16c1e72b0adbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*GL7_GCUhPsyg81FZZOzxMA.png"/></div></figure><p id="06d0" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">其中<em class="np">z(ϕ(x)</em>是配分函数。因此，对数似然比将采用以下形式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/3571383dd1f2f5c2cd0a45987b73b3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yEE8hIrEQaJYt6IgTmdtng.png"/></div></div></figure><p id="106e" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">现在，如果我们把<em class="np"> v=v_p-v_q </em>，把归一化常数和<em class="np"> r(x) </em>一起放入一个表达式<em class="np">ψ(ϕ(x)】</em>，我们可以把<em class="np"> f(x，y) </em>改写为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c691ca62c61524cc7b8e0ee57bd2970d.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*Ei-gwqSO9sQFfVKXVoLhuQ.png"/></div></figure><p id="81b0" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">如果我们使用<em class="np"> y </em>来表示标签的独热向量，并使用<em class="np"> V </em>来表示由行向量<em class="np"> v </em>组成的嵌入矩阵，我们可以通过以下方式重写上述模型</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/1da55b91d767765ff6c539bfe9ca3095.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*O4sAsolTd7eGbIgqZeiT0g.png"/></div></figure><p id="9479" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">该配方通过内部产品引入标签信息，如下图所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0d7ac9bb58599260fbef220dc9d8aa42.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*eu5eOqKrjOkVO3OZQr9ozg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">投影鉴别器。来源:带投影鉴别器的cGANs</figcaption></figure><p id="4c4a" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja"> Python代码</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/c443b99060ee77ac734f29a4a143cff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yiDTQxh7K7AWCdgpw2p5qQ.png"/></div></div></figure><p id="7aa9" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><strong class="mc ja">杂记</strong></p><p id="abe7" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在这一节中，我们将简要介绍萨根斯采用的其他几种技术</p><ul class=""><li id="6962" class="mz na iq mc b md mt mg mu lo nb ls nc lw nd ms no nf ng nh bi translated">SAGANs使用铰链损失作为对抗性损失，其定义为</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/22a0bd1d4820ea34a5807573047b36f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*67mZEYslDqL9toCdVOlQ_Q.png"/></div></figure><ul class=""><li id="3216" class="mz na iq mc b md mt mg mu lo nb ls nc lw nd ms no nf ng nh bi translated">SAGANs对生成器和鉴别器使用不同的学习速率，这就是所谓的双时标更新规则(TTUR)。对于ImageNet，他们对鉴别器使用0.0004，对生成器使用0.0001。在我的实现中，对于celebA数据集，我使用0.0001作为鉴别器，0.00005作为生成器。</li></ul><h1 id="a3f2" class="oi lg iq bd lh oj ok ol lk om on oo ln kf op kg lr ki oq kj lv kl or km lz os bi translated">结束</h1><p id="129e" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">希望这篇文章能帮助你对萨根有所了解。如果你碰到一些错误或不清楚的地方，欢迎发表评论。</p><h1 id="50c8" class="oi lg iq bd lh oj ok ol lk om on oo ln kf op kg lr ki oq kj lv kl or km lz os bi translated">参考</h1><ol class=""><li id="2bc0" class="mz na iq mc b md me mg mh lo ot ls ou lw ov ms ne nf ng nh bi translated">张寒等。自我注意生成敌对网络。在2019年的ICML。</li><li id="78e4" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated">生成对抗网络的谱归一化。在2018年的ICLR</li><li id="eea9" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated">Yuichi Yoshida等人.谱范数正则化用于提高深度学习的可推广性</li><li id="55a6" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated">通过语言调节早期视觉处理</li><li id="aad8" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated">宫藤忠俊，小山正德。带投影鉴别器的cGANs</li><li id="7a2a" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated"><a class="ae le" href="https://github.com/brain-research/self-attention-gan" rel="noopener ugc nofollow" target="_blank">萨根的官方代号</a></li><li id="245b" class="mz na iq mc b md ni mg nj lo nk ls nl lw nm ms ne nf ng nh bi translated">克里斯蒂安·科斯格罗维对谱范数的详细讨论</li></ol></div></div>    
</body>
</html>