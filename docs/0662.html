<html>
<head>
<title>Regression Line with Mathematics for the Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的数学回归直线</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/regression-line-with-mathematics-for-the-linear-regression-a80cc25bcfe2?source=collection_archive---------0-----------------------#2020-07-09">https://pub.towardsai.net/regression-line-with-mathematics-for-the-linear-regression-a80cc25bcfe2?source=collection_archive---------0-----------------------#2020-07-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ffe5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/statistics" rel="noopener ugc nofollow" target="_blank">统计数据</a></h2><div class=""/><h1 id="2dd2" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">介绍</h1><p id="32bd" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归是一种预测，其中目标是连续的，其应用是多种多样的。这是最简单的参数模型。每个数据集都是成对给出的，由输入特征向量和标签值组成。主要目标是假设参数，以从训练数据集中预测训练后测试数据的目标值。</p><p id="82be" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">下表有两个变量X和Y。这里，Y被称为目标变量或自变量，X被称为解释变量。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/c6970c1304e50c35e1f881916c354e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2KJbVxXfpIOjYKqz5T_45g.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">x和Y变量</figcaption></figure><p id="1a48" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">根据孩子的年龄和体重来预测他的身高就是回归问题的一个例子。</p><p id="179b" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">设X是实数值:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mq"><img src="../Images/426d13bc18733bee02e8e81c5a1cd818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*khAPnc8sTKGiYgveGgxI3g.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">X的值</figcaption></figure><p id="d753" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">Y的真实值是:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mr"><img src="../Images/4928c2e5a2fe1879deed230b8f41876b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*D26k73BhEjzcV-GcgfHPeQ.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">Y的值</figcaption></figure><p id="69b3" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">所以，回归过程基于给定的规则:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ms"><img src="../Images/f325cf238d7c3df98854ebef36096a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2Ht5XdNr1Br1rMCtuK4CA.png"/></div></div></figure><h1 id="1c31" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">回归方法</h1><p id="5d35" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是回归的一般方法:</p><ol class=""><li id="a6f5" class="mt mu it kz b la lv le lw li mv lm mw lq mx lu my mz na nb bi translated">收集数据</li><li id="9eb3" class="mt mu it kz b la nc le nd li ne lm nf lq ng lu my mz na nb bi translated">准备数据:回归时应该有数值。如果有标称值，应该映射到二进制值。</li><li id="528a" class="mt mu it kz b la nc le nd li ne lm nf lq ng lu my mz na nb bi translated">分析:有利于2D情节的可视化。</li><li id="8fec" class="mt mu it kz b la nc le nd li ne lm nf lq ng lu my mz na nb bi translated">训练:找出回归权重。</li><li id="5595" class="mt mu it kz b la nc le nd li ne lm nf lq ng lu my mz na nb bi translated">测试:测量预测值和数据的R2或相关性。它衡量模型的准确性。</li></ol><h1 id="a85d" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">回归线</h1><p id="6abd" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归包括寻找通过点的最佳拟合直线。最佳拟合线称为<strong class="kz jd">回归线</strong>。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/18b705895feac623303f16f7f556d53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*57caHzQqpfxziONir3BPMA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">回归线</figcaption></figure><p id="8b51" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">回归线方程:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ni"><img src="../Images/0d52799b57324cefa7433e684c310557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgBAcqetiV5ed2s4Jy_jMw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">回归线方程</figcaption></figure><h2 id="443b" class="nj ka it bd kb nk nl dn kf nm nn dp kj li no np kn lm nq nr kr lq ns nt kv iz bi translated">截距a的等式:</h2><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nu"><img src="../Images/904d959a3a3ff8b2141db8d9d9961ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXGRewfXPiNA4uwJb7U3zA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">截距a的方程式</figcaption></figure><h2 id="20e9" class="nj ka it bd kb nk nl dn kf nm nn dp kj li no np kn lm nq nr kr lq ns nt kv iz bi translated">斜率b的等式:</h2><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nv"><img src="../Images/bce81b04d5c3e810634ed439705bbd4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gt0S5EKKKufgpb5JEU-Gbg.png"/></div></div></figure><h2 id="88f8" class="nj ka it bd kb nk nl dn kf nm nn dp kj li no np kn lm nq nr kr lq ns nt kv iz bi translated">回归线的性质</h2><p id="7b3f" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归线具有以下属性:</p><ol class=""><li id="d558" class="mt mu it kz b la lv le lw li mv lm mw lq mx lu my mz na nb bi translated">回归总是通过点x和y的平均值运行和上升。</li><li id="b4a4" class="mt mu it kz b la nc le nd li ne lm nf lq ng lu my mz na nb bi translated">这条线使观察值和预测值之间的平方差最小。</li><li id="822c" class="mt mu it kz b la nc le nd li ne lm nf lq ng lu my mz na nb bi translated">在回归线中，x是输入值，y是输出值。</li></ol><h2 id="cd59" class="nj ka it bd kb nk nl dn kf nm nn dp kj li no np kn lm nq nr kr lq ns nt kv iz bi translated">回归线中的剩余误差</h2><p id="62d7" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">残差是因变量的观测值和预测值之间的差值。</p><p id="5f13" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">残差=观察值-预测值</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/4ba30cc14fd5ad9cba1352f0662ba1fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/0*ZDkUT5gwwaf7sC0t.jpg"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">残留误差</figcaption></figure><h1 id="0dcc" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">求导求回归线方程</h1><p id="d1c3" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑以下变量x和y及其值:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nx"><img src="../Images/5f1502800927d1a7ff8f52290946046d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wvvBFnNySLg5bK8CSJ_zEw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">变量X和Y及其值</figcaption></figure><p id="511f" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">因此，为了计算a和b的值，让我们找到<em class="ny"> XY，X和Y的值</em></p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nz"><img src="../Images/168a728a8d32bdbb1805b2ae94992155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wfnRL_U4J6lIq5_-ryeddA.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">准备好寻找截距和斜率的值</figcaption></figure><p id="69ac" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">这里，</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/a1837eb95b00a16deec15dadeb697c31.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*KrX3WSrLLZwtoOJKh0x-bQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">项目数量</figcaption></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f922f7f4fab1fb2daf798ca9d3a5052d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Yr-iM8IJ_lpO-i4dMX7wdw.png"/></div></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/38f8576eb11c385bc38a28dd899f8cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*Ksvn57Z-zLkn5HtoLy2k4Q.png"/></div></figure><p id="3d4f" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">现在，找出截距<em class="ny"> a </em>的值:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi od"><img src="../Images/a28f30aa3c4b4a06b40729fd06bbe65e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gYhLYbjWGVSYE3sVV3fxow.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">截距值</figcaption></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/cf18befa3b8003e91583a07cbb6aeb90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*P9vn3L8y9k4cQS_BhCnMhw.png"/></div></figure><p id="5063" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">求斜率<em class="ny"> b </em>的值:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi of"><img src="../Images/53b79d117e1a8f1e0d7786ebd7778221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0dtlS2JND5qY6hi5P3i0A.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">斜率值</figcaption></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi og"><img src="../Images/81a3b2d49211d2a6ab138896b73d3285.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*dJtBzRRt3FX2SPqnxHxnrg.png"/></div></figure><p id="4d28" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">因此，<strong class="kz jd">回归线</strong>方程:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi oh"><img src="../Images/2ea61b8abac2e0adba18a1708f05861e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S14aqGOhbz9Lgl8kf91T3w.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">回归线方程</figcaption></figure><h1 id="8b72" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">线性回归</h1><p id="5234" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们举一个例子，试着预测一个朋友的汽车的马力，因此它的等式将是:</p><p id="b2e6" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated"><em class="ny">马力= 0.0018 *年薪—0.99 *小时收听_广播</em></p><p id="cf43" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">这个方程被称为<em class="ny">回归方程。</em>0.0018和0.99的值称为回归权重。并且，寻找这些回归权重的过程叫做回归。</p><p id="3c5b" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">一旦找到回归权重，就很容易预测给定输入集的新值。</p><p id="16f5" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">对于回归，线性回归的预测公式如下:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi oi"><img src="../Images/debdfa09c703f8f3e7f66fb6b8e4b5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qfa30RNN3agLP5KYu9Fdw.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">线性回归方程</figcaption></figure><pre class="mb mc md me gt oj ok ol om aw on bi"><span id="aaec" class="nj ka it ok b gy oo op l oq or">import mglearn</span><span id="f331" class="nj ka it ok b gy os op l oq or">mglearn.plots.plot_linear_regression_wave()</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/85741373525061239193dca9ec339bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*LpFkJqLN4N0PDj7dqFFVhg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">波浪数据集上的线性回归</figcaption></figure><p id="88bd" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated">回归有许多不同的线性模型。这些模型之间的区别在于如何从训练数据中学习模型参数w和b，以及如何控制模型复杂性。</p><p id="81cc" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated"><strong class="kz jd">线性回归的优点:</strong></p><ol class=""><li id="5051" class="mt mu it kz b la lv le lw li mv lm mw lq mx lu my mz na nb bi translated">它易于解释并且计算量小</li></ol><p id="6b80" class="pw-post-body-paragraph kx ky it kz b la lv lc ld le lw lg lh li lx lk ll lm ly lo lp lq lz ls lt lu im bi translated"><strong class="kz jd">线性回归的缺点:</strong></p><ol class=""><li id="b8bf" class="mt mu it kz b la lv le lw li mv lm mw lq mx lu my mz na nb bi translated">它对非线性数据的建模很差</li></ol><h1 id="ab17" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论</h1><p id="f2be" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">找到通过这些点的最佳拟合直线是线性回归的重要部分，这条线被称为<strong class="kz jd">回归线</strong>。线性回归包括寻找通过点的最佳拟合直线。最小二乘法用于在回归中寻找最佳拟合直线。</p><h2 id="fdbc" class="nj ka it bd kb nk nl dn kf nm nn dp kj li no np kn lm nq nr kr lq ns nt kv iz bi translated">参考</h2><p id="5b9c" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归简介:<a class="ae ou" href="http://onlinestatbook.com/2/regression/introC.html" rel="noopener ugc nofollow" target="_blank">http://onlinestatbook.com/2/regression/introC.html</a></p></div></div>    
</body>
</html>