<html>
<head>
<title>NLP using Deep Learning Tutorials: Understand the Activation Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习教程的NLP:了解激活功能</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/nlp-using-deep-learning-tutorials-understand-the-activation-function-8f62613e32d2?source=collection_archive---------1-----------------------#2021-02-23">https://pub.towardsai.net/nlp-using-deep-learning-tutorials-understand-the-activation-function-8f62613e32d2?source=collection_archive---------1-----------------------#2021-02-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6760" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/98e835adac456ab74c518b877dbb07b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YECeOxlko9KoOJNw8RNm3A.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">来自https://unsplash.com<a class="ae ko" href="https://unsplash.com" rel="noopener ugc nofollow" target="_blank">的免费图片</a></figcaption></figure><p id="e802" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">本文是我正在撰写的系列文章的第一篇，在这里我将尝试解决在NLP中使用深度学习的主题。首先，我正在写一篇关于使用感知器进行文本分类的例子的文章，但我认为最好先复习一些基础知识，如激活和损失函数。</p><p id="d7b4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在神经网络中引入激活函数来捕捉数据中的复杂关系。通常在网络的末端添加非线性函数，以将数据从复杂格式转换为简单格式，这使其易于根据模型的主要目的进行解释。</p><p id="1ac2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">有许多类型的激活功能。Pytorch库有20多个预定义的(<a class="ae ko" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity" rel="noopener ugc nofollow" target="_blank">https://py torch . org/docs/stable/nn . html # non-linear-activations-weighted-sum-nonlinear it</a>)。</p><p id="ae50" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在下一节中，我将展示NLP中五个最常用的激活函数。</p><h1 id="f59b" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">乙状结肠的</h1><p id="4773" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">sigmoid是最早使用的激活函数之一。这是一个可微分的函数，可以将任何实数值转换为0到1之间的数值。它的数学表达式是:</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/b5f4d3b22eb7091ae643dda37f8c9a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*qyq6ZSU9nbMsbP8iHjt3XA.png"/></div></figure><p id="3681" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以用Pytorch轻松实现它，如下所示:</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/cf677f1a955c65bf279fb885f5f37234.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Koosw-1eh1K7OJ61nhgRKA.png"/></div></figure><pre class="mr ms mt mu gt mw mx my mz aw na bi"><span id="9874" class="nb lo it mx b gy nc nd l ne nf">import torch<br/>import matplotlib.pyplot as plt<br/><br/>x = torch.range(-5., 5., 0.1)<br/><br/><em class="ng"># Sigmoid<br/></em>y = torch.sigmoid(x)<br/>plt.plot(x.numpy(), y.numpy())<br/>plt.show()</span></pre><p id="15be" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如图所示，对于大多数输入，sigmoid函数很快收敛到其最大值和最小值。这可能导致两种现象，即消失梯度问题和爆炸梯度问题。因此，这个函数很少用在神经网络的中间。sigmoid用作网络的输出，将结果解释为概率。</p><h1 id="b8fc" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">双曲正切</h1><p id="dfdf" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">Tanh和sigmoid在视觉上看起来是一样的，但是，数学公式是不同的。Tanh将任何实数值转换为-1和1之间的值。Tanh可以表示如下:</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a8dcc92401fa62e9eb27cdcc28e1b324.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*WexP-qmhkueSpR2zRp3cXQ.png"/></div></figure><p id="7b4b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">或者</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/2572640401d982257f7460a010e02346.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*_yd5TkeXLIhmC53Bpk9_pg.png"/></div></figure><p id="b167" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Tanh可以使用Pytorch实现，如下所示:</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3b8eb929f66fcf5e43ef72d66789296c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*WELBov9covgAcPHU3pbnEg.png"/></div></figure><pre class="mr ms mt mu gt mw mx my mz aw na bi"><span id="e677" class="nb lo it mx b gy nc nd l ne nf">import torch<br/>import matplotlib.pyplot as plt<br/><br/>x = torch.range(-5., 5., 0.1)<br/><br/><em class="ng"># tanh<br/></em>y = torch.tanh(x)<br/>plt.plot(x.numpy(), y.numpy())<br/>plt.show()</span></pre><p id="c74d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过更深入的分析，我们还可以将Tanh视为Sigmoid的线性变换。因此，这两个激活函数也有相同的消失和爆炸梯度的问题。</p><h1 id="c0ee" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">整流线性单位</h1><p id="03bc" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">Relu是迄今为止所有激活函数中最重要的。它被用于最近的许多深度学习工作和创新中。然而，这非常简单:</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/79e007b4a5264ea71af24235e14d66bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*o5z_ttMOSaRbzrabpYiiXQ.png"/></div></figure><p id="20a0" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">ReLu可以使用Pytorch实现，如下所示:</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/94c3a467bf093c0d54a6cc28a7b2db51.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*D8JYCd6fLFb3_stsqjthkw.png"/></div></figure><pre class="mr ms mt mu gt mw mx my mz aw na bi"><span id="4cd7" class="nb lo it mx b gy nc nd l ne nf">import torch<br/>import matplotlib.pyplot as plt<br/><br/>x = torch.range(-5., 5., 0.1)<br/><br/><em class="ng"># ReLU<br/></em>y = torch.relu(x)<br/>plt.plot(x.numpy(), y.numpy())<br/>plt.show()</span></pre><p id="e88c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如下图所示，ReLU将负值削波为零。这种效果对于解决渐变消失问题非常有用，但是如果某些输出变为零，并且永远不会达到正值，那么这种效果本身也会成为一个问题。这就是所谓的垂死的ReLU问题</p><h1 id="745c" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">参数ReLU</h1><p id="52b3" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">PReLU是ReLU的推广，纠正了垂死的ReLU问题。该函数的数学公式为:</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f55bf299d9ff21bb4ce2f42a3929f4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*rjEeQRFUwicIgF_VP2Calg.png"/></div></figure><p id="2707" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请注意，a是一个学习参数。同样，如果a等于零，我们回到ReLU的使用。</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/ce4eadb98903bc09971a52ba4ca888e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*yuxV_O-AAlZvy2EumChiTQ.png"/></div></figure><pre class="mr ms mt mu gt mw mx my mz aw na bi"><span id="0a60" class="nb lo it mx b gy nc nd l ne nf">import torch<br/>import matplotlib.pyplot as plt<br/><br/>x = torch.range(-5., 5., 0.1)<br/><br/><em class="ng"># PReLU<br/></em>prelu = torch.nn.PReLU(num_parameters=1)<br/>y = prelu(x)<br/>plt.plot(torch.detach(x).numpy(), torch.detach(y).numpy())<br/>plt.show()</span></pre><h1 id="642d" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated"><strong class="ak"> SoftMax </strong></h1><p id="df49" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">像sigmoid函数一样，SoftMax激活函数将每个输出转换为0到1之间的实数值。它还将每一个除以所有输出的总和，从而得到一个离散的概率分布。</p><p id="918f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">softmax的数学公式为:</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nj"><img src="../Images/16a92ad6625105cbc30b8147251bf66d.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*96otlaBsj3a143qjQ6fr7g.png"/></div></div></figure><p id="7f1e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">并且可以使用PyTorch实现，如下所示:</p><pre class="mr ms mt mu gt mw mx my mz aw na bi"><span id="c06e" class="nb lo it mx b gy nc nd l ne nf">import torch<em class="ng"><br/></em>import torch.nn as nn<br/><br/><em class="ng"># softmax</em><br/>softmax = nn.Softmax(dim=1)<br/>x_input = torch.randn(1, 3)<br/>y_input = softmax(x_input)<br/>print(x_input)<br/>print(y_input)<br/>print(torch.sum(y_input, dim=1))</span></pre><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b7b9ec0978b8674d1abccd6d8c4e9831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*XMFs8H3N3K97yDZV8EwIow.png"/></div></figure><p id="4e4d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SoftMax激活函数对于分类任务的解释非常有用，因为输出被认为是概率。</p><h1 id="d1dc" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">结论</h1><p id="7915" class="pw-post-body-paragraph kp kq it kr b ks ml ku kv kw mm ky kz la mn lc ld le mo lg lh li mp lk ll lm im bi translated">在本文中，我展示了五个激活函数:Sigmoid、Tanh、ReLU、PReLU和Softmax。还有许多其他的，我希望在我的下一篇文章中介绍。然而，对于如何为每个模型或神经网络选择使用哪个函数，并没有指导:这是一个实验之母。所以，简单地使用别人过去用过的方法。</p><p id="2d4c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">参考文献:</strong></p><ol class=""><li id="a3eb" class="nl nm it kr b ks kt kw kx la nn le no li np lm nq nr ns nt bi translated">《用Pytorch进行自然语言处理》一书(<a class="ae ko" href="https://www.amazon.fr/Natural-Language-Processing-Pytorch-Applications/dp/1491978236" rel="noopener ugc nofollow" target="_blank">https://www . Amazon . fr/Natural-Language-Processing-py torch-Applications/DP/1491978236</a>)</li></ol></div></div>    
</body>
</html>