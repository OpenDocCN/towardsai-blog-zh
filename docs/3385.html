<html>
<head>
<title>ChatGPT: How Does It Work Internally?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ChatGPT:它内部是如何工作的？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/chatgpt-how-does-it-work-internally-e0b3e23601a1?source=collection_archive---------0-----------------------#2022-12-10">https://pub.towardsai.net/chatgpt-how-does-it-work-internally-e0b3e23601a1?source=collection_archive---------0-----------------------#2022-12-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6a7a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">OpenAI于2022年11月30日推出了一款新的对话助手，它提供的答案质量令人难以置信，在互联网上引起了轰动。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/56dde8a25bc6aadc2f27986f2d04e40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kABKSnhczSzVHMbV"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">罗曼·维涅斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h2 id="927a" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">这个聊天机器人基于语言模型(大型语言模型的LLM)GPT 3，或者更准确地说，基于它的3.5版本。ChatGPT实际上是InstructGPT的改编版，instruct GPT于2022年1月推出，但当时并没有给人留下同样的印象。可能是因为访问它的难度，也可能是因为模型比ChatGPT小100倍。</h2></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="628d" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">ChatGPT令人难以置信的成功归功于它能够自动生成与人类相似的文本，并且能够在避免其祖先(如微软的Tay或Meta的Galactica)的缺点的同时，考虑到对话的上下文。</p><p id="758c" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">Tay在24小时内变得种族主义和排外。卡拉狄加正在制造无稽之谈和错误的信息，并能以一种非常令人信服的方式给出它对种族主义的看法。Tay在24小时内被关闭，Galactica也关闭了，但是三天后。</p><p id="658f" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">似乎OpenAI从微软和Meta的错误中吸取了教训，将系统推向了一个尚未达到的水平。在很短的时间内，它已经成为端到端对话系统的参考(即，能够使用模型直接回答，而不需要分析和响应引擎的帮助)。</p><h1 id="fb2b" class="mv la it bd lb mw mx my le mz na nb lh jz nc ka ll kc nd kd lp kf ne kg lt nf bi translated">GPT-3</h1><p id="93ae" class="pw-post-body-paragraph mc md it me b mf ng ju mh mi nh jx mk li ni mm mn lm nj mp mq lq nk ms mt mu im bi translated">GPT(生成式预训练转换器)系列模型由基于转换器技术的语言模型组成。它是由位于旧金山的OpenAI公司开发的。OpenAI于2015年12月由埃隆·马斯克(他已经不需要介绍了)和美国商人萨姆·奥特曼(Sam Altman)共同创立，后者曾是孵化器Y Combinator (Scribd、Reddit、Airbnb、Dropbox、GitLab、Women Who Code等)的总裁。)，并自2020年起担任OpenAI董事会主席。</p><p id="5919" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">2020年，GPT-3是有史以来最大的语言模型，有1750亿个参数。它太大了，需要800 GB的内存来训练它。这些天来，作为最大的模型从来没有持续很长时间，因为今年它凭借其1760亿个参数被BLOOM (BigScience大型开放科学开放访问多语言模型)从最大模型的榜首位置上赶了下来。</p><p id="860c" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">LLM通常是从大量不同语言和领域的样本文本中生成的。GPT-3也不例外，它已经接受了来自英语普通爬行、网络文本2、书籍1/2和维基百科的数千亿单词的训练。它还接受了用CSS、JSX、Python等语言编写的程序示例的训练。它的强大之处还在于它接受2048个标记作为输入，这使它能够处理大约1500个单词的非常大的句子(OpenAI认为一个标记是大约四个字符的单词的一部分，并举例说明1000个标记代表大约750个单词)。</p><p id="b454" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">GPT-3被归类为生成模型，这意味着它主要被训练来预测简单地设置在输入句子末尾的下一个单词。就像搜索引擎或Outlook now中的自动完成机制一样。</p><p id="8f66" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">GPT-3被多次引用，因为它能够生成非常接近记者或作者能力的文本。只要给它一个句子的开头，它就会逐字完成段落或文章的其余部分。通过扩展，该模型已经证明了它能够处理大量的语言处理任务，例如翻译、回答问题和填充文本中缺失的单词。</p><p id="c1c3" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">要了解语言处理的不同任务是什么，你可以参考我关于这个主题的文章:</p><div class="nl nm gp gr nn no"><a href="https://towardsdatascience.com/natural-language-processing-tasks-3278907702f3" rel="noopener follow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">自然语言处理任务</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">NLP是一组操作，主要是通过各种活动处理文本数据。在…的帮助下</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">towardsdatascience.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc ks no"/></div></div></a></div><p id="f318" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">GPT-3.5是主要的GPT-3型号的变体。在2021年第四季度之前，它已经使用选定的文本和代码的混合进行了训练。这解释了为什么ChatGPT不能提出这个日期之后的任何事实，因为我们可以测试许多人寻找2022年或之后发生的事实的答案。重要的是要考虑到ChatGPT与GPT-3略有不同，因为它主要是按照用户的指示进行训练，而不是简单地扩展用户的句子。例如，如果用户写“给我讲一个故事”，它将遵循指示，而不是“只是”扩展句子，而是继续写这个故事。考虑这种差异很重要，因为正是这种方法决定了ChatGPT的工作方式。ChatGPT模型是从一个带有会话数据的语言模型微调而来的。这个源模型也被用来生成instruct GPT(2022年1月发布)。</p><h1 id="593a" class="mv la it bd lb mw mx my le mz na nb lh jz nc ka ll kc nd kd lp kf ne kg lt nf bi translated">学习方法</h1><p id="2fbd" class="pw-post-body-paragraph mc md it me b mf ng ju mh mi nh jx mk li ni mm mn lm nj mp mq lq nk ms mt mu im bi translated">ChatGPT模型由OpenAI团队按照3步法进行培训:</p><ul class=""><li id="4b72" class="od oe it me b mf mg mi mj li of lm og lq oh mu oi oj ok ol bi translated">步骤1:收集演示数据，在监督模式下训练生成规则(策略)。这第一步对应于通过监督学习获得的GPT-3.5模型的微调。这种调整是使用问题/答案对来完成的。问题由系统从问题数据库中自动提供，人工智能训练器(贴标签者)负责回答问题。他们让培训师获得建议(来自模型)，以帮助他们撰写答案。该团队选择的方法是监督微调(或SFT)，即通过强制模型保持与输入和输出数据的拟合来改善网络。反向传播使用这些新数据调整模型的参数。</li><li id="27db" class="od oe it me b mf om mi on li oo lm op lq oq mu oi oj ok ol bi translated">第二步:收集对比数据，训练奖励模型。第二步包括自动生成一个问题/答案对，并让人工智能训练器按偏好顺序排列答案。系统生成两个答案，人工智能训练员必须按照质量降序排列。选择的方法是学习奖励模型(RM代表奖励模型),它基于生成的答案和人工智能训练器提供的命令工作。这些数据来自人工智能训练员与聊天机器人的对话。这些消息是随机选择的；他们对几种可能的完成进行了采样，并要求人工智能训练员对它们进行排序。</li><li id="722e" class="od oe it me b mf om mi on li oo lm op lq oq mu oi oj ok ol bi translated">步骤3:使用强化学习(RL)算法优化关于奖励模型的调节原则。该第三步也是最后一步在于通过小步骤使用规则和生成原则的管理模型的强化学习机制，称为近似策略优化(PPO)模型。模型的输入是来自数据库的问题。该模型生成由奖励模型评估的输出。奖励被注入到生成规则管理模型中以提高其性能。</li></ul><h1 id="9b72" class="mv la it bd lb mw mx my le mz na nb lh jz nc ka ll kc nd kd lp kf ne kg lt nf bi translated">对话的</h1><p id="f5f9" class="pw-post-body-paragraph mc md it me b mf ng ju mh mi nh jx mk li ni mm mn lm nj mp mq lq nk ms mt mu im bi translated">OpenAI没有解释机器人的对话部分是如何工作的。然而，可以假设它们使用上述相同的组件，包括PPO模型和系统所基于的优化的GPT-3.5模型。很难知道是否使用了审核API，因为聊天机器人会用正确的句子回答，即使内容无关。</p><p id="5a0b" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">我假设API仅用于训练，并且网络基于它所拥有的学习自动调节内容，这表明调节是内在的。特别难知道ChatGPT如何管理对话的上下文，在我看来，从对话的角度来看，这是这个系统中最有趣的部分。</p><p id="eb39" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">适度方面是一个相当大的进步(参见Tay和Galactica)，特别是当我们知道在仇恨言论识别系统上工作的困难时，但在我看来，上下文管理部分是最令人惊讶的。很难知道系统从前面的问题中保留了什么…</p><p id="9aca" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">ChatGPT可以使用self-ask原则，该原则包括使用用户的问题来生成self-question，并将其重新引入到模型中。该系统还可以使用“思维链”(CoT)原则，该原则对应于提问向一系列推理步骤的演变，这些推理步骤允许回答复杂的问题。</p><p id="57ff" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">重要的是要考虑到问题的形式非常重要，尤其是自我提问机制。哪怕是最初问题的一个小小的改变，都可能导致完全不同的答案。所以，如果你对答案不完全满意，不要犹豫，重新措辞和修改最初的问题，以获得更好的答案。</p><p id="4c26" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">经过几个月的测试期，产生了许多版本，第一个版本对所有人开放。OpenAI团队随后观察了系统的交互，并首次观察了大型语言模型在几百万人手中的行为。这个主动观察阶段的主要目的是确保当用户成功地智取ChatGPT的审核系统时，团队会将对策添加到训练和测试数据中。</p><h1 id="259b" class="mv la it bd lb mw mx my le mz na nb lh jz nc ka ll kc nd kd lp kf ne kg lt nf bi translated">限制</h1><p id="82b5" class="pw-post-body-paragraph mc md it me b mf ng ju mh mi nh jx mk li ni mm mn lm nj mp mq lq nk ms mt mu im bi translated">像任何文本生成系统一样，ChatGPT模型可以根据它从语言中保留的内容生成无意义的文本，但不理解其含义。正如OpenAI在描述中所述，最难解决的问题是强化学习没有真值来源，导致权重缺失。</p><p id="6ab4" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">其次，在问题没有实质问题的情况下，版主可能会出错，礼貌地拒绝回答。与Tay和Galactica不同，ChatGPT的培训是在源位置使用一个审核API进行审核的，该API允许在培训期间将不适当的请求推回。尽管如此，假阳性和假阴性仍然可能发生，并导致过度节制。节制API是由GPT模型基于以下类别执行的分类模型:暴力、自残、仇恨、骚扰和性。为此，OpenAI使用了匿名化数据和合成数据(在零拍中)，尤其是在没有足够数据的情况下。</p><p id="46a9" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">与任何训练系统相反，生成网络学习什么是好的答案(通过克隆行为)。不幸的是，模型没有学习到正确的答案，因此答案将取决于模型保留了什么。因此，它将无法呈现专家认为准确的内容。</p><p id="0512" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">另一个已知的问题与句子或句子部分的重复有关(也见于摘要中)。OpenAI将此视为与人工智能分析师制定的长答案相关的学习偏差，以及过度学习问题。</p><p id="6fc8" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">这些系统的特殊性在于，即使问题是模糊的，它们也会提供答案，并且不存在对问题的更好理解的搜索(消歧)。然后，模型对公式非常敏感，有时需要重新制定以获得更好的答案。</p><h1 id="3867" class="mv la it bd lb mw mx my le mz na nb lh jz nc ka ll kc nd kd lp kf ne kg lt nf bi translated">结论</h1><p id="203f" class="pw-post-body-paragraph mc md it me b mf ng ju mh mi nh jx mk li ni mm mn lm nj mp mq lq nk ms mt mu im bi translated">ChatGPT无可争议的成功在很大程度上归功于这样一个事实，即每个人都可以通过一个简单的对话界面而不是API来使用一个伟大的模型。它模拟真实对话的能力非同寻常。即使我们意识到它是一台机器，一种算法，我们也只能陷入向它提出许多问题的游戏中，因为它拥有超大的知识，所以这台机器看起来很神圣。</p><p id="20c5" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">但当我们仔细观察时，它仍然是一个句子生成器，没有理解，也没有像人一样的自我批评。ChatGPT代表了端到端系统中最好的，即使它有时在答案上仍然是完美的。我更想知道接下来会发生什么，他们会在这种类型的架构上成功到什么程度。我只能想象，为了能够提供一个正确的答案，有必要包含评估循环和可解释性的机制。</p><p id="e3c2" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">要访问ChatGPT，您只需点击此链接<a class="ae ky" href="https://chat.openai.com/" rel="noopener ugc nofollow" target="_blank">https://chat.openai.com/</a>并注册您的电子邮件和电话号码。</p><p id="8a58" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk li ml mm mn lm mo mp mq lq mr ms mt mu im bi translated">一周内已经有超过100万人测试过了，为什么不是你呢？OpenAI需要你的反馈来改进它的模型，他们甚至为此组织了一场比赛。同样，不要错过Dall-E，它也是GPT-3的衍生产品，允许您根据描述创建图像。</p><h1 id="3de4" class="mv la it bd lb mw mx my le mz na nb lh jz nc ka ll kc nd kd lp kf ne kg lt nf bi translated">参考</h1><ul class=""><li id="5070" class="od oe it me b mf ng mi nh li or lm os lq ot mu oi oj ok ol bi translated">车型索引:【https://beta.openai.com/docs/model-index-for-researchers T2】</li><li id="58f7" class="od oe it me b mf om mi on li oo lm op lq oq mu oi oj ok ol bi translated">https://openai.com/blog/instruction-following/<a class="ae ky" href="https://openai.com/blog/instruction-following/" rel="noopener ugc nofollow" target="_blank"/></li><li id="e091" class="od oe it me b mf om mi on li oo lm op lq oq mu oi oj ok ol bi translated">https://openai.com/blog/chatgpt/</li><li id="5894" class="od oe it me b mf om mi on li oo lm op lq oq mu oi oj ok ol bi translated">布鲁姆:<a class="ae ky" href="https://bigscience.huggingface.co/blog/bloom" rel="noopener ugc nofollow" target="_blank">https://bigscience.huggingface.co/blog/bloom</a></li><li id="4971" class="od oe it me b mf om mi on li oo lm op lq oq mu oi oj ok ol bi translated">y组合子:<a class="ae ky" href="https://fr.wikipedia.org/wiki/Y_Combinator" rel="noopener ugc nofollow" target="_blank">https://fr.wikipedia.org/wiki/Y_Combinator</a></li><li id="c179" class="od oe it me b mf om mi on li oo lm op lq oq mu oi oj ok ol bi translated">审核API细节:现实世界中检测不良内容的整体方法—<a class="ae ky" href="https://arxiv.org/abs/2208.03274" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2208.03274</a></li><li id="4a36" class="od oe it me b mf om mi on li oo lm op lq oq mu oi oj ok ol bi translated">创作者访谈:<a class="ae ky" href="https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/" rel="noopener ugc nofollow" target="_blank">https://www . technology review . com/2023/03/03/1069311/inside-story-口述-历史-how-chatgpt-build-open ai/</a></li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="2420" class="mv la it bd lb mw ou my le mz ov nb lh jz ow ka ll kc ox kd lp kf oy kg lt nf bi translated">想了解更多？</h1><div class="nl nm gp gr nn no"><a rel="noopener  ugc nofollow" target="_blank" href="/conversational-ai-7-trends-and-predictions-for-2023-9a644becb90b"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd iu gy z fp nt fr fs nu fu fw is bi translated">对话式人工智能:2023年的7个趋势和预测</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">我提出了会话助手市场发展的七个新趋势和预测(通常称为…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">pub.towardsai.net</p></div></div><div class="nx l"><div class="oz l nz oa ob nx oc ks no"/></div></div></a></div></div></div>    
</body>
</html>