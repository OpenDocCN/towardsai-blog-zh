<html>
<head>
<title>A Journey into the Fabulous Applications of Transformers — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器神奇应用之旅——第二部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-journey-into-the-fabulous-applications-of-transformers-part-2-81a349a70cf3?source=collection_archive---------2-----------------------#2022-12-14">https://pub.towardsai.net/a-journey-into-the-fabulous-applications-of-transformers-part-2-81a349a70cf3?source=collection_archive---------2-----------------------#2022-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8c89" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">演示重点是自然语言处理使用Python，拥抱脸</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ef8d7ebb3faee2beefacc147091326ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VCAxXUWGlSR9NzzR"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@samule?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Samule孙</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="91b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Transformer架构广泛应用于自然语言处理中，它极大地促进了大型语言模型的发展。随着使用LLM的NLP中<a class="ae kv" href="https://towardsdatascience.com/a-gentle-introduction-to-transfer-learning-in-nlp-b71e87241d66" rel="noopener" target="_blank">迁移学习</a>的兴起，许多模型被构建并在<a class="ae kv" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>中与研究社区共享。</p><p id="0d14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将会看到一些有趣的变形金刚在NLP中的应用，包括演示和解释。这篇文章是下面链接的上一篇文章的延续。</p><div class="ls lt gp gr lu lv"><a rel="noopener  ugc nofollow" target="_blank" href="/a-journey-into-the-fabulous-applications-of-transformers-part-1-630cc80e3ce6"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">变压器神奇应用之旅——第一部分</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">演示重点是使用Python的NLP，拥抱脸。</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">pub.towardsai.net</p></div></div><div class="me l"><div class="mf l mg mh mi me mj kp lv"/></div></div></a></div><p id="f299" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文中使用的拥抱脸模型也可以用作使用加速推理API的即插即用模型。请参考这篇文章，了解更多关于如何利用模型的API。</p><div class="ls lt gp gr lu lv"><a href="https://blog.jovian.ai/plug-and-play-ml-models-with-accelerated-inference-api-from-hugging-face-964d25d9dd65" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">即插即用的ML模型，带有来自拥抱脸的“加速推理API”</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">python实现的4步指南</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">blog.jovian.ai</p></div></div><div class="me l"><div class="mk l mg mh mi me mj kp lv"/></div></div></a></div><p id="2f69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本<a class="ae kv" href="https://github.com/dharini-projects/NLP_Applications_with_Transformers" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a>中给出了所有讨论的应用程序的演示代码以及Colab链接。作为一篇介绍性文章，关于所讨论的模型的细节在相应的章节中有链接。为了让所有的应用程序工作(除了句子相似性)，我们需要使用<code class="fe ml mm mn mo b">pip install transformers.</code>安装<code class="fe ml mm mn mo b">transformers</code>库</p><p id="c66d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在本文中看到的应用有</p><blockquote class="mp mq mr"><p id="edd5" class="kw kx ms ky b kz la jr lb lc ld ju le mt lg lh li mu lk ll lm mv lo lp lq lr ij bi translated"><a class="ae kv" href="#0148" rel="noopener ugc nofollow"> 6。翻译</a></p><p id="c802" class="kw kx ms ky b kz la jr lb lc ld ju le mt lg lh li mu lk ll lm mv lo lp lq lr ij bi translated"><a class="ae kv" href="#a8c1" rel="noopener ugc nofollow"> 7。令牌分类</a></p><p id="9878" class="kw kx ms ky b kz la jr lb lc ld ju le mt lg lh li mu lk ll lm mv lo lp lq lr ij bi translated"><a class="ae kv" href="#db4d" rel="noopener ugc nofollow"> 8。句子相似度</a></p><p id="3f27" class="kw kx ms ky b kz la jr lb lc ld ju le mt lg lh li mu lk ll lm mv lo lp lq lr ij bi translated">9。零枪击分类</p><p id="7dfe" class="kw kx ms ky b kz la jr lb lc ld ju le mt lg lh li mu lk ll lm mv lo lp lq lr ij bi translated">10。填充蒙版</p></blockquote><h2 id="0148" class="mw mx iq bd my mz na dn nb nc nd dp ne lf nf ng nh lj ni nj nk ln nl nm nn no bi translated">6.翻译</h2><p id="65d9" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">transformers的一个有趣而有用的应用是能够在不同语言之间翻译文本。根据培训阶段使用的语言，翻译任务有几种模式可供选择。</p><p id="3ba9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的例子中，我们将把英语文本翻译成意大利语。要知道什么模型可以执行翻译，我们必须探索拥抱人脸库。名为<code class="fe ml mm mn mo b"><a class="ae kv" href="https://huggingface.co/Helsinki-NLP/opus-mt-it-en" rel="noopener ugc nofollow" target="_blank">Helsinki-NLP/opus-mt-en-it</a> </code>的模型执行这种转换，我们将在演示中使用它。</p><p id="0ed8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一步是用任务<code class="fe ml mm mn mo b">translation.</code>实例化<code class="fe ml mm mn mo b">pipeline</code>。任务名称根据所处理的语言而不同。关于不同语言文本翻译、相应模型和数据集的更多信息可在此<a class="ae kv" href="https://huggingface.co/languages" rel="noopener ugc nofollow" target="_blank">链接</a>中找到。</p><p id="b042" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的例子中，任务是<code class="fe ml mm mn mo b">translation_en_to_it</code>，它和模型名称一起给出，如下所示。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="26f7" class="ny mx iq mo b be nz oa l ob oc">from transformers import pipeline<br/>text_translator = pipeline("translation_en_to_it", model="Helsinki-NLP/opus-mt-en-it")</span></pre><p id="dde4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ml mm mn mo b">input_text</code>存储有我们希望翻译的英语句子。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="9df0" class="ny mx iq mo b be nz oa l ob oc">input_text = "This text is translated from English to Italian"</span></pre><p id="5c00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的代码行中，我们将输入加载到我们的模型中，并将输出存储在<code class="fe ml mm mn mo b">italian_text.</code>中。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="b1d0" class="ny mx iq mo b be nz oa l ob oc">italian_text = text_translator(input_text, clean_up_tokenization_spaces=True)<br/>print(italian_text)<br/>Output:<br/>[{'translation_text': "Questo testo è tradotto dall'inglese all'italiano"}]</span></pre><h2 id="a8c1" class="mw mx iq bd my mz na dn nb nc nd dp ne lf nf ng nh lj ni nj nk ln nl nm nn no bi translated">7.令牌分类</h2><p id="a47d" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated"><a class="ae kv" href="https://huggingface.co/tasks/token-classification" rel="noopener ugc nofollow" target="_blank">标记分类</a>是给文本中出现的单词分配一个标记/标签的任务。句子中的一个单词叫做标记。标记分类的例子有<a class="ae kv" href="https://huggingface.co/dslim/bert-base-NER" rel="noopener ugc nofollow" target="_blank">命名实体识别</a> (NER)和<a class="ae kv" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank">词性</a>(词性)标注。</p><p id="8e3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经在文章的第一部分介绍了NER，所以现在我们来看第二个任务，词性标注。词性标注是识别一个词的相应词性并在句子中用标签如名词、代词、形容词、副词等来标记它的过程。</p><p id="51a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们首先导入<code class="fe ml mm mn mo b">pipeline</code>并用任务<code class="fe ml mm mn mo b">token-classification.</code>实例化它</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="4bea" class="ny mx iq mo b be nz oa l ob oc">from transformers import pipeline<br/><br/>pos_classifier = pipeline("token-classification", model = "vblagoje/bert-english-uncased-finetuned-pos")</span></pre><p id="827a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所述，我们使用的型号是<code class="fe ml mm mn mo b"><a class="ae kv" href="https://huggingface.co/vblagoje/bert-english-uncased-finetuned-pos" rel="noopener ugc nofollow" target="_blank">vblagoje/bert-english-uncased-finetuned-pos</a></code>。现在我们将对代币进行分类并展示它们。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="e712" class="ny mx iq mo b be nz oa l ob oc">tags = pos_classifier("Hello I am happy when I see waterfalls.")<br/>print(tags)<br/><br/>Output:<br/>[{'entity': 'INTJ', 'score': 0.9958117, 'index': 1, 'word': 'hello', 'start': 0, 'end': 5}, {'entity': 'PRON', 'score': 0.9995704, 'index': 2, 'word': 'i', 'start': 6, 'end': 7}, {'entity': 'AUX', 'score': 0.9966484, 'index': 3, 'word': 'am', 'start': 8, 'end': 10}, {'entity': 'ADJ', 'score': 0.99829704, 'index': 4, 'word': 'happy', 'start': 11, 'end': 16}, {'entity': 'ADV', 'score': 0.99861526, 'index': 5, 'word': 'when', 'start': 17, 'end': 21}, {'entity': 'PRON', 'score': 0.9995561, 'index': 6, 'word': 'i', 'start': 22, 'end': 23}, {'entity': 'VERB', 'score': 0.99941325, 'index': 7, 'word': 'see', 'start': 24, 'end': 27}, {'entity': 'NOUN', 'score': 0.9958626, 'index': 8, 'word': 'waterfalls', 'start': 28, 'end': 38}, {'entity': 'PUNCT', 'score': 0.9996631, 'index': 9, 'word': '.', 'start': 38, 'end': 39}]</span></pre><p id="587f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了清楚地看到输出，我们将导入<code class="fe ml mm mn mo b">pandas</code>并创建结果的数据帧，然后看到如下所示的输出。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="f75b" class="ny mx iq mo b be nz oa l ob oc">import pandas as pd<br/>df_tags = pd.DataFrame(tags)<br/>print(df_tags)<br/><br/>Output:<br/>  entity     score  index        word  start  end<br/>0   INTJ  0.995812      1       hello      0    5<br/>1   PRON  0.999570      2           i      6    7<br/>2    AUX  0.996648      3          am      8   10<br/>3    ADJ  0.998297      4       happy     11   16<br/>4    ADV  0.998615      5        when     17   21<br/>5   PRON  0.999556      6           i     22   23<br/>6   VERB  0.999413      7         see     24   27<br/>7   NOUN  0.995863      8  waterfalls     28   38<br/>8  PUNCT  0.999663      9           .     38   39</span></pre><h2 id="db4d" class="mw mx iq bd my mz na dn nb nc nd dp ne lf nf ng nh lj ni nj nk ln nl nm nn no bi translated">8.句子相似度</h2><p id="8cc5" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">transformers的一个有趣的应用是提取句子嵌入的能力，这种能力可以捕获语义信息。嵌入的<a class="ae kv" href="https://engineering.talkdesk.com/what-are-sentence-embeddings-and-why-are-they-useful-53ed370b3f35" rel="noopener ugc nofollow" target="_blank">句子是整个句子的数字表示，并被用作模型的输入。在我们的例子中，我们将使用句子转换器来生成句子嵌入，并使用它来识别两个句子之间的</a><a class="ae kv" href="https://huggingface.co/tasks/sentence-similarity" rel="noopener ugc nofollow" target="_blank">相似度</a>。</p><p id="e7bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个例子，我们必须从拥抱脸安装<code class="fe ml mm mn mo b"><a class="ae kv" href="https://www.sbert.net/" rel="noopener ugc nofollow" target="_blank">sentence-transformers</a></code>库，不像其他应用程序，我们将安装<code class="fe ml mm mn mo b">transformers</code>库。有许多模型可用于句子相似性的任务，我们将在我们的示例中使用模型<code class="fe ml mm mn mo b"><a class="ae kv" href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" rel="noopener ugc nofollow" target="_blank">sentence-transformers/all-MiniLM-L6-v2</a></code>。</p><p id="e6f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一步是导入<code class="fe ml mm mn mo b">SentenceTransformer </code>和<code class="fe ml mm mn mo b">util</code>，如下图所示。需要使用<code class="fe ml mm mn mo b">util</code>包来使用余弦相似性函数来测量两个嵌入之间的距离。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="a100" class="ny mx iq mo b be nz oa l ob oc">!pip install sentence-transformers<br/>from sentence_transformers import SentenceTransformer, util</span></pre><p id="8fad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一步，我们在<code class="fe ml mm mn mo b">input_sentences</code>中加载我们想用来测试模型的句子。我们给<code class="fe ml mm mn mo b">SentenceTransformer</code>指定具体的型号名称。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="478f" class="ny mx iq mo b be nz oa l ob oc">similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')<br/>input_sentences = ["I'm happy", "I'm not sad"]</span></pre><p id="cfd9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一步，我们提取两个句子的嵌入。两个句子的嵌入都存储在单独的变量中。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="03f6" class="ny mx iq mo b be nz oa l ob oc">embedding_1= similarity_model.encode(input_sentences[0], convert_to_tensor=True)<br/>embedding_2 = similarity_model.encode(input_sentences[1], convert_to_tensor=True)</span></pre><p id="3542" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在最后一步，我们利用来自<code class="fe ml mm mn mo b">util</code>的<code class="fe ml mm mn mo b">cos_sim</code>函数来计算两个向量之间的距离(嵌入)</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="e796" class="ny mx iq mo b be nz oa l ob oc">print(util.pytorch_cos_sim(embedding_1, embedding_2))<br/>Output:<br/>tensor([[0.4624]])</span></pre><p id="e503" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该值越接近1，句子之间的相似度越大。如果值更接近0，我们可以理解为句子不相似。</p><h2 id="c2a6" class="mw mx iq bd my mz na dn nb nc nd dp ne lf nf ng nh lj ni nj nk ln nl nm nn no bi translated">9.零射击分类</h2><p id="5075" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">零触发分类源于<a class="ae kv" href="https://joeddav.github.io/blog/2020/05/29/ZSL.html" rel="noopener ugc nofollow" target="_blank">零触发学习</a>，这基本上意味着用一组标签训练分类器，用另一组标签测试分类器。模型根本看不到评估标签集。</p><p id="b88b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着我们可以采用<a class="ae kv" href="https://towardsdatascience.com/zero-shot-text-classification-with-hugging-face-7f533ba83cd6" rel="noopener" target="_blank">零镜头分类</a>模型，给出输入，也给出我们自己的标签。该模型将根据我们给定的标签对文本进行分类，即使该模型没有被这些标签中的任何一个训练。现在，我们将在演示中尝试同样的方法。让我们从用任务<code class="fe ml mm mn mo b">zero-shot-classification.</code>实例化<code class="fe ml mm mn mo b">pipeline</code>开始</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="0a30" class="ny mx iq mo b be nz oa l ob oc">from transformers import pipeline<br/>zero_shot_classifier = pipeline("zero-shot-classification")<br/>Output:<br/>No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).<br/>Using a pipeline without specifying a model name and revision in production is not recommended.<br/>Downloading: 100%<br/>1.15k/1.15k [00:00&lt;00:00, 28.0kB/s]<br/>Downloading: 100%<br/>1.63G/1.63G [00:26&lt;00:00, 63.5MB/s]<br/>Downloading: 100%<br/>26.0/26.0 [00:00&lt;00:00, 467B/s]<br/>Downloading: 100%<br/>899k/899k [00:00&lt;00:00, 5.16MB/s]<br/>Downloading: 100%<br/>456k/456k [00:00&lt;00:00, 2.73MB/s]<br/>Downloading: 100%<br/>1.36M/1.36M [00:00&lt;00:00, 2.71MB/s]</span></pre><p id="c7ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们没有给出具体的模型，模型<code class="fe ml mm mn mo b"><a class="ae kv" href="https://huggingface.co/facebook/bart-large-mnli" rel="noopener ugc nofollow" target="_blank">facebook/bart-large-mnli</a></code>由库选择，模型权重下载后参考<code class="fe ml mm mn mo b"> zero_shot_classifier.</code></p><p id="0249" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一步是定义我们需要分类的输入文本和标签。使用<code class="fe ml mm mn mo b"> zero_shot_classifier, </code>我们将输入的文本和标签进行分类。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="ecfa" class="ny mx iq mo b be nz oa l ob oc">input_sentence = "I am hungry and angry. I think an ice cream will make me feel good"<br/>labels = ['food', 'travel','entertainment','sad','happy','neutral']<br/>results = zero_shot_classifier(input_sentence, labels)<br/>print(results)<br/><br/>Output:<br/>{'sequence': 'I am hungry and angry. I think an ice cream will make me feel good', 'labels': ['food', 'sad', 'entertainment', 'travel', 'neutral', 'happy'], 'scores': [0.8720405101776123, 0.07575708627700806, 0.023996146395802498, 0.010163746774196625, 0.009797507897019386, 0.00824508722871542]}</span></pre><p id="a8fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了清楚地看到输出，我们将导入<code class="fe ml mm mn mo b">pandas</code>库，并在一个数据帧中看到结果。如下面的数据框所示，标签<code class="fe ml mm mn mo b">food</code>和<code class="fe ml mm mn mo b">sad</code>的得分高于其他标签。我们可以看到，给出的句子与标签<code class="fe ml mm mn mo b">food </code>的关联超过了所有其他标签。所以我们得到了最高分。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="51d3" class="ny mx iq mo b be nz oa l ob oc">import pandas as pd<br/>df_result = pd.DataFrame(results)<br/>print(df_result.loc[:, ['labels','scores']])<br/>Output:<br/>          labels    scores<br/>0           food  0.872041<br/>1            sad  0.075757<br/>2  entertainment  0.023996<br/>3         travel  0.010164<br/>4        neutral  0.009798<br/>5          happy  0.008245</span></pre><h2 id="963c" class="mw mx iq bd my mz na dn nb nc nd dp ne lf nf ng nh lj ni nj nk ln nl nm nn no bi translated">10.填充蒙版</h2><p id="9d3b" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated"><a class="ae kv" href="https://analyticsindiamag.com/a-complete-tutorial-on-masked-language-modelling-using-bert/" rel="noopener ugc nofollow" target="_blank">屏蔽语言建模(MLM) </a>的概念处理预测单词以替换数据中有意屏蔽的单词。这有助于提高对模型训练语言的统计理解，并带来更好的文本表示。MLM的优点是自我监督的预训练，不需要标记数据进行训练。</p><p id="ced1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">任务<code class="fe ml mm mn mo b"><a class="ae kv" href="https://huggingface.co/tasks/fill-mask" rel="noopener ugc nofollow" target="_blank">Fill Mask</a>,</code>因此处理屏蔽给定句子中的随机单词，并探索模型给出的替换屏蔽的各种建议。</p><p id="a1b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从用任务<code class="fe ml mm mn mo b">fill-mask</code>实例化<code class="fe ml mm mn mo b">pipeline</code>开始，如下所示。由于我们没有给出具体的模型，所以下载默认模型(<code class="fe ml mm mn mo b"><a class="ae kv" href="https://huggingface.co/distilroberta-base" rel="noopener ugc nofollow" target="_blank">distilroberta-base</a></code>)。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="d161" class="ny mx iq mo b be nz oa l ob oc">from transformers import pipeline<br/>fill_mask_clf = pipeline("fill-mask")<br/><br/>Output:<br/>No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).<br/>Using a pipeline without specifying a model name and revision in production is not recommended.<br/>Downloading: 100%<br/>480/480 [00:00&lt;00:00, 13.5kB/s]<br/>Downloading: 100%<br/>331M/331M [00:08&lt;00:00, 43.4MB/s]<br/>Downloading: 100%<br/>899k/899k [00:00&lt;00:00, 1.56MB/s]<br/>Downloading: 100%<br/>456k/456k [00:00&lt;00:00, 1.77MB/s]<br/>Downloading: 100%<br/>1.36M/1.36M [00:00&lt;00:00, 1.77MB/s]</span></pre><p id="4401" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们通过用<code class="fe ml mm mn mo b">&lt;mask&gt;</code>标记屏蔽一个单词来给模型一个句子。在打印输出时，我们会看到屏蔽单词的五个不同的可能单词、相应的分数、标记表示和带有预测单词的完整句子。</p><pre class="kg kh ki kj gt nu mo nv bn nw nx bi"><span id="70bc" class="ny mx iq mo b be nz oa l ob oc">print(fill_mask_clf("artificial intelligence is going to be &lt;mask&gt; in the future"))<br/><br/>Output:<br/>[{'score': 0.061495572328567505,<br/>  'token': 30208,<br/>  'token_str': ' commonplace',<br/>  'sequence': 'artificial intelligence is going to be commonplace in the future'},<br/> {'score': 0.05322642996907234,<br/>  'token': 25107,<br/>  'token_str': ' ubiquitous',<br/>  'sequence': 'artificial intelligence is going to be ubiquitous in the future'},<br/> {'score': 0.0344822071492672,<br/>  'token': 5616,<br/>  'token_str': ' useful',<br/>  'sequence': 'artificial intelligence is going to be useful in the future'},<br/> {'score': 0.033160075545310974,<br/>  'token': 6128,<br/>  'token_str': ' everywhere',<br/>  'sequence': 'artificial intelligence is going to be everywhere in the future'},<br/> {'score': 0.025654001161456108,<br/>  'token': 956,<br/>  'token_str': ' needed',<br/>  'sequence': 'artificial intelligence is going to be needed in the future'}]</span></pre><h2 id="ed57" class="mw mx iq bd my mz na dn nb nc nd dp ne lf nf ng nh lj ni nj nk ln nl nm nn no bi translated">摘要</h2><p id="2365" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">在本文的这两部分中，我们讨论了变压器可以实现的最重要的10种应用。结论是，随着变形金刚和大型语言模型的出现，原本需要大量成本和时间的复杂任务现在可以轻松完成。还必须注意的是，相同的模型可以用于不同的任务。</p><p id="42e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个想法是理解一个模型的可能性，然后<a class="ae kv" href="https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e" rel="noopener" target="_blank">根据自己的目的对其进行微调</a>。我们尝试得越多，学到的就越多。前进，成功！！！</p><blockquote class="mp mq mr"><p id="9f97" class="kw kx ms ky b kz la jr lb lc ld ju le mt lg lh li mu lk ll lm mv lo lp lq lr ij bi translated">请在此<a class="ae kv" href="https://medium.com/@dharini_r" rel="noopener">页面</a>中找到更多与NLP相关的文章。</p><p id="b740" class="kw kx ms ky b kz la jr lb lc ld ju le mt lg lh li mu lk ll lm mv lo lp lq lr ij bi translated">谢谢大家！！</p></blockquote></div></div>    
</body>
</html>