<html>
<head>
<title>Why LSTM more useful than RNN in Deep Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么LSTM在深度学习方面比RNN更有用？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/deep-learning-88e218b74a14?source=collection_archive---------0-----------------------#2021-05-15">https://pub.towardsai.net/deep-learning-88e218b74a14?source=collection_archive---------0-----------------------#2021-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="eb88" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="aa91" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">深度神经网络中的记忆存储容量方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/24d028c33808908fdb5ced1691068c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YFKYPX8CI19oiO1xT_RcQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">LSTM模型。作者的照片</figcaption></figure><blockquote class="lh li lj"><p id="513d" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd">T3】简介T5】</strong></p></blockquote><p id="2140" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">本文是上一篇关于递归神经网络文章的续篇。LSTM模型克服了递归神经网络对前一状态的记忆存储的问题，并基于它来决定预测。这意味着梯度消失的问题，由此我们不能适当地训练网络，这导致递归神经网络不能在存储器中保留长期序列。</p><p id="8ced" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">LSTM的特别之处在于它实现了长期依赖，并且它处理信息以保存和丢弃设备。所有这些条件都是借助三个门完成的，即<strong class="ln jd"> <em class="lm">输出门、输入门和</em> </strong>。</p><p id="d151" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">递归神经网络和长短期记忆的区别在于，RNN有一个<strong class="ln jd"> <em class="lm">隐藏状态</em> </strong>来存储信息和进行预测。另一方面，LSTM将<strong class="ln jd"> <em class="lm">隐藏状态</em> </strong>分解成<strong class="ln jd">单元状态</strong>和<strong class="ln jd">隐藏状态</strong>。</p><pre class="ks kt ku kv gt mk ml mm mn aw mo bi"><span id="09ce" class="mp mq it ml b gy mr ms l mt mu">1. The <strong class="ml jd">cell state</strong> is an internal memory to store all the<br/>   information. <br/>2. The <strong class="ml jd">hidden state</strong> is used for operations for the output.</span></pre><p id="f47e" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">LSTM的基本架构如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/da145b09a8bc9a84018b404f7800204d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*Z5jIHNFLafKBkL9sJQRMcA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">基本LSTM。作者的照片</figcaption></figure><p id="33a3" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">基本架构由<em class="lm">前单元状态</em>、<em class="lm">单元状态</em>、<em class="lm">前隐藏状态</em>和<em class="lm">隐藏状态</em>组成。下标是<em class="lm"> t </em>和<em class="lm"> (t-1) </em>当时所处的状态。</p><p id="3246" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">f <strong class="ln jd">遗忘门、输入门和输出门</strong>正在调制输入信号。这些门正在修改存储在<strong class="ln jd"> <em class="lm">先前单元状态</em> </strong>中的信息，其中一些操作如在<strong class="ln jd"> <em class="lm">单元状态</em> </strong>中那样修改结果。输出门将存储在<strong class="ln jd"> <em class="lm">单元状态</em> </strong>中的信息传输到输出状态y(t)。</p><div class="mw mx gp gr my mz"><a rel="noopener  ugc nofollow" target="_blank" href="/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">神经网络:递归神经网络的兴起</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">深度学习中的渐进一代</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">pub.towardsai.net</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn lb mz"/></div></div></a></div><blockquote class="lh li lj"><p id="1ef6" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd"> <em class="it">这些门的工作</em> </strong></p></blockquote><h2 id="7912" class="mp mq it bd no np nq dn nr ns nt dp nu mh nv nw nx mi ny nz oa mj ob oc od iz bi translated">忘记大门</h2><p id="78fb" class="pw-post-body-paragraph lk ll it ln b lo oe kd lq lr of kg lt mh og lw lx mi oh ma mb mj oi me mf mg im bi translated">这个门负责决定应该从单元状态中移除什么信息。</p><p id="6c21" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">例如:</p><pre class="ks kt ku kv gt mk ml mm mn aw mo bi"><span id="0c17" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml jd">Indian</strong> is a beautiful country. I live in India. <br/><strong class="ml jd">Norway</strong> is also a beautiful country.</span></pre><p id="3678" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">在第一句中，印度是一个主语，但当我们谈到挪威时，这就成了主语。在LSTM，忘记门被识别，主题被改变为挪威，并删除印度作为主题。这种类型的智能行为是在遗忘门的帮助下完成的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/9ac5070852ce3747cbd1a7f7ddd88f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*zlNWguCYUcBR5K0mQdbalQ.png"/></div></figure><p id="8412" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">我们可以看到遗忘门的公式，它是输入信号和先前隐藏状态的函数，乘以权重矩阵u(f)、w(f)和b(f)。所有这些单元都由sigmoid函数sigma调制。“忽略门”中的值在“0”到“1”的范围内，即值为“0”的信息将被删除，值为“1”的信息将被保留。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/8f82eb565e307c898c98e5b4163f3e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*TCS-wE1h8qwmycXfu-OPgg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">遗忘之门的运作。作者的照片</figcaption></figure><h2 id="f0f1" class="mp mq it bd no np nq dn nr ns nt dp nu mh nv nw nx mi ny nz oa mj ob oc od iz bi translated">输入门</h2><p id="e581" class="pw-post-body-paragraph lk ll it ln b lo oe kd lq lr of kg lt mh og lw lx mi oh ma mb mj oi me mf mg im bi translated">这个门负责决定应该在单元状态中存储什么信息。</p><p id="638d" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">例如:</p><pre class="ks kt ku kv gt mk ml mm mn aw mo bi"><span id="2294" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml jd">Indian</strong> is a beautiful country. I live in India. <br/><strong class="ml jd">Norway</strong> is also a beautiful country.</span></pre><p id="bea9" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">在上面的例子中，forget被用来删除哪些信息，并选择新的主题“挪威”。现在输入门的工作是把这个新课题储存到细胞状态c(t-1)中。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/17f2e91e3630c807ba875b744b6793ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*k00hOOeDaEWLL4TN6D33tQ.png"/></div></figure><p id="261d" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">输入数据的公式如下所示，它是输入信号和先前隐藏状态与权重矩阵u(i)、w(i)和b(i)相乘的函数。所有这些单元都由sigmoid函数sigma调制。输入门的值在“0”到“1”的范围内，即不存储值为“0”的信息，而存储/更新值为“1”的信息。</p><div class="mw mx gp gr my mz"><a rel="noopener  ugc nofollow" target="_blank" href="/understand-tensorflow-basic-with-python-87281e737db9"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">使用Python了解TensorFlow Basic</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">张量流中使用的基本术语</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">pub.towardsai.net</p></div></div><div class="ni l"><div class="om l nk nl nm ni nn lb mz"/></div></div></a></div><h2 id="77a5" class="mp mq it bd no np nq dn nr ns nt dp nu mh nv nw nx mi ny nz oa mj ob oc od iz bi translated">输出门</h2><p id="aa8f" class="pw-post-body-paragraph lk ll it ln b lo oe kd lq lr of kg lt mh og lw lx mi oh ma mb mj oi me mf mg im bi translated">这个门负责从单元状态c(t-1)中选择哪些信息作为输出y。</p><p id="653f" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">例如:</p><pre class="ks kt ku kv gt mk ml mm mn aw mo bi"><span id="29de" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml jd">Indian</strong> is a beautiful country. I live in India. <br/><strong class="ml jd">Norway</strong> is also a beautiful country. You must visit the capital of .........</span></pre><p id="c51b" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">在上面的例子中，输出已经使用了处于隐藏状态的信息是否会被传递。输出门将决定填空的主题，即挪威。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/f1151322e63dbac1a848d28e005f291b.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*xyBy1IRHaeBGTo2AzfgttQ.png"/></div></figure><p id="265a" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">输出数据的公式如下所示，它是输入信号和先前隐藏状态的函数，乘以权重矩阵u(0)、w(0)和b(0)。所有这些单元都由sigmoid函数sigma调制。输入门中的值在“0”到“1”的范围内，即值为“0”的信息不会将隐藏状态传递到输出，而值为“1”的信息会将隐藏状态传递到输出。</p><h2 id="26ea" class="mp mq it bd no np nq dn nr ns nt dp nu mh nv nw nx mi ny nz oa mj ob oc od iz bi translated">候选州</h2><p id="6a4c" class="pw-post-body-paragraph lk ll it ln b lo oe kd lq lr of kg lt mh og lw lx mi oh ma mb mj oi me mf mg im bi translated">该状态负责添加新信息或更新单元状态。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/b8ee06ec924cf69142b3115068b20175.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*17NRnHR0ulIobEIgRFacdA.png"/></div></figure><p id="fa26" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">这种状态的公式如下所示:</p><p id="b82b" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">上面的公式现在有了参数化函数“<em class="lm"> tan双曲线”。</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/74a2d9bb56844d672d283cafc3f58247.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*qCqrJwHI1_fAylCDO1pWRA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">LSTM的候选州。作者的照片</figcaption></figure><p id="1e87" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">状态用于保存由i(t)和乘法运算符控制的信息。如果i(t)的值是“0 ”,则没有信息将传递到单元状态，并且i(t)的值是“1 ”,则g(t)信息将添加到单元状态线。</p><div class="mw mx gp gr my mz"><a rel="noopener  ugc nofollow" target="_blank" href="/step-by-step-basic-understanding-of-neural-networks-with-keras-in-python-94f4afd026e5"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">使用Python中的Keras逐步基本了解神经网络</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">具有定义的神经网络的学习</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">pub.towardsai.net</p></div></div><div class="ni l"><div class="oq l nk nl nm ni nn lb mz"/></div></div></a></div><blockquote class="lh li lj"><p id="a6d3" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd"> <em class="it">结论:</em> </strong></p></blockquote><p id="6310" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">这是深度学习中LSTM算法工作流程背后的基本思想。来自门和候选状态的信息流改变了单元状态输出c(t)。</p><p id="8e08" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae or" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae or" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="db11" class="os mq it bd no ot ou ov nr ow ox oy nu ki oz kj nx kl pa km oa ko pb kp od pc bi translated">推荐文章</h1><p id="a27e" class="pw-post-body-paragraph lk ll it ln b lo oe kd lq lr of kg lt mh og lw lx mi oh ma mb mj oi me mf mg im bi translated"><a class="ae or" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄用Python </a> <br/> 2。<a class="ae or" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a> <br/> 3。<a class="ae or" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae or" rel="noopener ugc nofollow" target="_blank" href="/principal-component-analysis-in-dimensionality-reduction-with-python-1a613006d531?source=friends_link&amp;sk=3ed0671fdc04ba395dd36478bcea8a55">用Python进行主成分分析降维</a> <br/> 5。<a class="ae or" href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a?source=friends_link&amp;sk=9c5c613ceb10f2d203712634f3b6fb28" rel="noopener">用Python全面讲解K-means聚类</a> <br/> 6。<a class="ae or" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python充分解释了线性回归</a> <br/> 7。<a class="ae or" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae or" rel="noopener ugc nofollow" target="_blank" href="/differences-between-concat-merge-and-join-with-python-1a6541abc08d?source=friends_link&amp;sk=3b37b694fb90db16275059ea752fc16a">concat()、merge()和join()与Python </a> <br/> 9的区别。<a class="ae or" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a>T30】10。<a class="ae or" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>