<html>
<head>
<title>Don’t Be Overwhelmed by NLP Research</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不要被NLP研究淹没</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/dont-be-overwhelmed-by-nlp-c174a8b673cb?source=collection_archive---------0-----------------------#2020-08-21">https://pub.towardsai.net/dont-be-overwhelmed-by-nlp-c174a8b673cb?source=collection_archive---------0-----------------------#2020-08-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8737" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="194b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何应对NLP中正在进行的大量研究，这些研究对你来说可能是不可行的</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/0cb950b80600a96294db0147ca8baa55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odADLNKmfYa2KmNe9Eod5Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">-由<a class="ae lh" href="http://geek-and-poke.com/" rel="noopener ugc nofollow" target="_blank">geek-and-poke.com</a>下<a class="ae lh" href="https://creativecommons.org/licenses/by/3.0/" rel="noopener ugc nofollow" target="_blank"> CC-BY-3.0 </a></figcaption></figure><h1 id="4ff4" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">这是怎么回事？</h1><blockquote class="ma mb mc"><p id="5866" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">NLP是新的计算机视觉</p></blockquote><p id="6c73" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">有大量文本数据集可用；像谷歌、微软、脸书等巨头已经将他们的注意力转向自然语言处理。</p><p id="bc02" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">使用<strong class="mg jd">成千上万</strong>超级昂贵的TPU/GPU的模型，使它们对大多数人来说不可行。</p><blockquote class="ma mb mc"><p id="2f6e" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated"><strong class="mg jd">这让我焦虑！</strong>(我们会回来的)</p></blockquote><p id="a6a4" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">让我们从这些推文的角度来看问题:</p><p id="5796" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated"><strong class="mg jd">推文1: </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/ac2ddf4568426dd98c450e52761afb90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1417qPcET9nQZsOOFV87Dg.png"/></div></div></figure><p id="b2cc" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated"><strong class="mg jd">推文2: </strong> <em class="mf">(阅读后面的推文)</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/e8106da8745d92c3c3bbfba6e36294ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UirseD4gVp5YpPotd0OJPA.png"/></div></div></figure><h1 id="cec7" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">后果？</h1><p id="e0ad" class="pw-post-body-paragraph md me it mg b mh nf kd mj mk ng kg mm na nh mp mq nb ni mt mu nc nj mx my mz im bi translated">大约在过去一年中，以下知识成为主流:</p><ul class=""><li id="1168" class="nk nl it mg b mh mi mk ml na nm nb nn nc no mz np nq nr ns bi translated">《变形金刚》之后是《改革家》、《龙成形家》、《GTrXL》、《林成形家》等。</li><li id="b71f" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">伯特之后是XLNet，罗伯塔，阿尔伯特，伊莱克特，巴特，T5，大鸟，和其他人。</li><li id="7b85" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">模型压缩由DistilBERT、TinyBERT、BERT-of-thesews、Huffman编码、运动修剪、PrunBERT、MobileBERT等扩展。</li><li id="3cde" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">甚至引入了新的标记化:字节对编码(BPE)、单词片段编码(WPE)、句子片段编码(SPE)等等。</li></ul><p id="40aa" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">这仅仅是冰山一角。</p><blockquote class="ma mb mc"><p id="68ae" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">所以当你试图理解和实现一个模型的时候，一堆新的更轻更快的模型已经出现了。</p></blockquote><h1 id="3c84" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">如何应对？</h1><p id="d8f3" class="pw-post-body-paragraph md me it mg b mh nf kd mj mk ng kg mm na nh mp mq nb ni mt mu nc nj mx my mz im bi translated">答案很简单:</p><blockquote class="ny"><p id="84ab" class="nz oa it bd ob oc od oe of og oh mz dk translated">你不需要知道一切，只知道什么是必要的，使用什么是可用的</p></blockquote><h1 id="5849" class="li lj it bd lk ll lm ln lo lp lq lr ls ki oi kj lu kl oj km lw ko ok kp ly lz bi translated">理由</h1><p id="75ce" class="pw-post-body-paragraph md me it mg b mh nf kd mj mk ng kg mm na nh mp mq nb ni mt mu nc nj mx my mz im bi translated">我把它们都读了一遍，意识到大多数研究都是类似概念的重复。</p><p id="6ab7" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">说到底(含糊地说):</p><ul class=""><li id="c364" class="nk nl it mg b mh mi mk ml na nm nb nn nc no mz np nq nr ns bi translated">重整器是变形器的散列版本，longfomer是变形器的基于卷积的对应物</li><li id="b884" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">所有的压缩技术都试图整合信息</li><li id="e7b9" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">从BERT到GPT3的一切都只是一个语言模型</li></ul><h1 id="5a5b" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">优先级-&gt;管道优先于准确性</h1><blockquote class="ny"><p id="2e42" class="nz oa it bd ob oc od oe of og oh mz dk translated">在跳到其他可以使用的东西之前，学会有效地使用可用的东西</p></blockquote><p id="3841" class="pw-post-body-paragraph md me it mg b mh ol kd mj mk om kg mm na on mp mq nb oo mt mu nc op mx my mz im bi translated">实际上，这些模型只是一个更大的管道中的一小部分。</p><blockquote class="ma mb mc"><p id="37a5" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">你的第一直觉不应该是在训练更好的模型方面与科技巨头竞争。</p><p id="d572" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">相反，你的第一直觉应该是使用可用的模型来构建一个端到端的应用程序，这个应用程序解决了一个实际问题。</p></blockquote><p id="ee1b" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">现在如果你觉得模型是你应用的性能瓶颈；重新训练该模型或切换到另一个模型。</p><p id="a758" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">请考虑以下情况:</p><ul class=""><li id="7951" class="nk nl it mg b mh mi mk ml na nm nb nn nc no mz np nq nr ns bi translated">巨大的深度学习模型通常需要数千个GPU小时来训练。</li><li id="ee89" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">当您考虑超参数调优(HP调优)时，这增加了10倍。</li><li id="ea37" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">惠普调试像伊莱克特模型这样高效的东西也需要一两周的时间。</li></ul><h1 id="934b" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">实际场景-&gt;真正的加速</h1><p id="6bf3" class="pw-post-body-paragraph md me it mg b mh nf kd mj mk ng kg mm na nh mp mq nb ni mt mu nc nj mx my mz im bi translated">以问答系统为例。鉴于有数百万份文档，对于这项任务来说，像ElasticSearch这样的工具比新的问答模式(相对而言)更重要。</p><p id="a70c" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">在生产中，你的管道的成功将(不仅)取决于你的深度学习模型有多棒，还取决于:</p><ul class=""><li id="2342" class="nk nl it mg b mh mi mk ml na nm nb nn nc no mz np nq nr ns bi translated"><strong class="mg jd">推断时间的潜伏期</strong></li><li id="56dc" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated"><strong class="mg jd">结果的可预测性和边界情况</strong></li><li id="52f2" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">微调的容易程度</li><li id="a83f" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">在相似数据集上重现模型的容易程度</li></ul><p id="4497" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">Robolox在<a class="ae lh" href="https://blog.roblox.com/2020/05/scaled-bert-serve-1-billion-daily-requests-cpus/" rel="noopener ugc nofollow" target="_blank">的博客</a>中漂亮地提到，像DistilBERT这样的东西可以扩展到处理十亿个查询。</p><blockquote class="ma mb mc"><p id="0d44" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">而新模型可以将推理时间减少<strong class="mg jd"> 2x-5x </strong>。</p><p id="ac70" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">像<a class="ae lh" href="https://www.mathworks.com/company/newsletters/articles/what-is-int8-quantization-and-why-is-it-popular-for-deep-neural-networks.html" rel="noopener ugc nofollow" target="_blank">量化</a>，剪枝和使用<a class="ae lh" href="https://medium.com/microsoftazure/accelerating-model-training-with-the-onnx-runtime-519d75a97166#:~:text=ONNX%20Runtime%20is%20a%20performance,in%20high%2Dscale%20production%20scenarios." rel="noopener"> Onnx </a>这样的技术可以减少推理时间<strong class="mg jd"> 10x-40x </strong>！</p></blockquote><h1 id="cb64" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">亲身经历</h1><p id="dbdf" class="pw-post-body-paragraph md me it mg b mh nf kd mj mk ng kg mm na nh mp mq nb ni mt mu nc nj mx my mz im bi translated">我正在开发一个事件提取管道，它使用了:</p><ul class=""><li id="a357" class="nk nl it mg b mh mi mk ml na nm nb nn nc no mz np nq nr ns bi translated">4种不同的基于变压器的模型</li><li id="e94e" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">1个基于RNN的模型</li></ul><p id="6c0f" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">酪整个管道的核心是:</p><ul class=""><li id="6068" class="nk nl it mg b mh mi mk ml na nm nb nn nc no mz np nq nr ns bi translated">WordNet</li><li id="7412" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">框架网</li><li id="29a6" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">Word2Vec</li><li id="17ff" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">正则表达式</li></ul><p id="22e6" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">还有。我的团队主要关注的是:</p><ul class=""><li id="7d60" class="nk nl it mg b mh mi mk ml na nm nb nn nc no mz np nq nr ns bi translated"><strong class="mg jd">从PPT、图像&amp;表格中提取文本</strong></li><li id="fd4b" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated"><strong class="mg jd">清洗&amp;预处理文本</strong></li><li id="9625" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated"><strong class="mg jd">结果可视化</strong></li><li id="4b68" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">弹性搜索的优化</li><li id="2a8e" class="nk nl it mg b mh nt mk nu na nv nb nw nc nx mz np nq nr ns bi translated">Neo4J的信息格式</li></ul><h1 id="eeee" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">结论</h1><p id="f97a" class="pw-post-body-paragraph md me it mg b mh nf kd mj mk ng kg mm na nh mp mq nb ni mt mu nc nj mx my mz im bi translated">拥有一个性能一般的管道比拥有一个只有几个优秀模块的非功能性管道更重要。</p><p id="6cd8" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">克里斯托弗·曼宁(Christopher Manning)和安德鲁·吴(Andrew NG)都不知道这一切。他们只知道需要什么，什么时候需要；很好。</p><p id="9f42" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">所以，对自己要有现实的期望。</p><blockquote class="ny"><p id="e76e" class="nz oa it bd ob oc od oe of og oh mz dk translated">谢谢大家！</p></blockquote></div></div>    
</body>
</html>