<html>
<head>
<title>PySyft is a Framework for Private Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySyft是一个私人深度学习的框架</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pysyft-is-a-framework-for-private-deep-learning-f5420b01332f?source=collection_archive---------0-----------------------#2022-07-14">https://pub.towardsai.net/pysyft-is-a-framework-for-private-deep-learning-f5420b01332f?source=collection_archive---------0-----------------------#2022-07-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3eaa" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">该框架在基于PyTorch和TensorFlow构建的模型中使用差分隐私。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0afbaaaed8b7c7bda11801684e492deb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gcPTA5p7qhwe7dIu"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:PySyft</figcaption></figure><blockquote class="ky kz la"><p id="888a" class="lb lc ld le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过125，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="ly lz gp gr ma mb"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mc ab fo"><div class="md ab me cl cj mf"><h2 class="bd iu gy z fp mg fr fs mh fu fw is bi translated">序列</h2><div class="mi l"><h3 class="bd b gy z fp mg fr fs mh fu fw dk translated">与机器学习、人工智能和数据发展保持同步的最佳资源…</h3></div><div class="mj l"><p class="bd b dl z fp mg fr fs mh fu fw dk translated">thesequence.substack.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp ks mb"/></div></div></a></div><p id="d2d2" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">信任是深度学习应用实现的关键因素。从训练到优化，深度学习模型的生命周期与不同方之间的可信数据交换密切相关。这种动态对于实验室环境当然是有效的，但是结果容易受到各种各样的安全攻击，这些攻击操纵模型中不同参与者之间的信任关系。让我们以信用评分模型为例，该模型使用金融交易对特定客户的信用风险进行分类。用于训练或优化模型的传统机制假设执行这些操作的实体将完全访问这些财务数据集，这为各种隐私风险打开了大门。随着深度学习的发展，对在数据集和模型的生命周期中实施隐私约束的机制的需求变得越来越重要。在试图解决这一巨大挑战的技术中，<a class="ae mt" href="https://github.com/OpenMined/PySyft" rel="noopener ugc nofollow" target="_blank"> PySyft </a>是最近在深度学习社区中稳步增长的一个框架。</p><p id="f7b3" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">隐私在深度学习应用中的重要性与分布式多方模型的出现直接相关。深度学习解决方案的传统方法依赖于控制模型整个生命周期的集中各方，即使使用大型分布式计算基础设施。这是一个组织创建预测模型来管理访问其网站的客户的偏好的情况。然而，集中式深度学习拓扑已被证明在移动或物联网(IOT)等依赖于大量设备产生数据和执行模型的场景中不切实际。在这些场景中，分布式各方不仅经常产生敏感数据集，而且还执行和评估深度学习模型的性能。这种动态需要负责创建、训练和执行深度学习模型的不同方之间的双向隐私关系。</p><p id="1cc3" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">向更分布式架构的过渡是深度学习模型中对强隐私机制的需求背后的主要力量之一。这就是PySyft要解决的挑战，但如果没有机器学习和分布式编程的几个研究领域的发展，这是不可能的。</p><h1 id="f6b6" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">促成因素</h1><p id="2044" class="pw-post-body-paragraph lb lc it le b lf nm ju lh li nn jx lk mq no ln lo mr np lr ls ms nq lv lw lx im bi translated">深度学习模型中的隐私多年来一直是一个众所周知的问题，但可以提供解决方案的技术现在才达到一定的可行性水平。在PySyft的例子中，该框架利用了过去十年中机器学习和密码学中最引人入胜的三项技术:</p><p id="5c2f" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated"><em class="ld">安全多方计算</em></p><p id="99fd" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated"><em class="ld">联邦学习</em></p><p id="331c" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated"><em class="ld">差分隐私</em></p><h1 id="87e5" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">安全多方计算</h1><p id="4ccf" class="pw-post-body-paragraph lb lc it le b lf nm ju lh li nn jx lk mq no ln lo mr np lr ls ms nq lv lw lx im bi translated">安全多方计算(sMPC)是一种加密技术，它允许不同方对输入执行计算，同时保持这些输入的私密性。在计算机科学理论中，sMPC通常被视为著名的<a class="ae mt" href="https://en.wikipedia.org/wiki/Yao%27s_Millionaires%27_Problem" rel="noopener ugc nofollow" target="_blank">姚百万富翁问题</a>的解决方案，该问题由计算机科学家<a class="ae mt" href="https://en.wikipedia.org/wiki/Andrew_Yao" rel="noopener ugc nofollow" target="_blank"> Andrew Yao </a>于20世纪80年代提出。这个问题描述了一个场景，在这个场景中，多个百万富翁想知道他们中谁更富有，而不透露他们的实际财富。百万富翁的问题存在于许多现实世界的场景中，如拍卖、选举或在线游戏。</p><p id="97f4" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">从概念上讲，sMPC提供了安全计算，取代了对可信中介的需求。在sMPC模型中，一组具有私有输入的参与方计算分布式功能，例如公平、隐私和正确性等安全属性被保留。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e6d654003bb710041da37585730800ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*0vGCFD-ZLskClhb3"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mt" href="https://www.semanticscholar.org/paper/Collaborative-network-outage-troubleshooting-with-Djatmiko-Schatzmann/e932792557c785e7084e16691512d1866a6264d5/figure/0" rel="noopener ugc nofollow" target="_blank">https://www . semantic scholar . org/paper/Collaborative-network-outage-trouble shooting-with-Djatmiko-Schatzmann/e 932792557 c 785 e 7084 e 16691512d 1866 a 6264 D5/figure/0</a></figcaption></figure><h2 id="7aa8" class="ns mv it bd mw nt nu dn na nv nw dp ne mq nx ny ng mr nz oa ni ms ob oc nk od bi translated">联合学习</h2><p id="3869" class="pw-post-body-paragraph lb lc it le b lf nm ju lh li nn jx lk mq no ln lo mr np lr ls ms nq lv lw lx im bi translated">联合学习是一种新的学习架构，用于在高度分布式拓扑结构中运行的人工智能系统，如移动或物联网(IOT)系统。最初由谷歌研究实验室提出，联合学习代表了集中式人工智能训练的一种替代方案，在集中式人工智能训练中，在来自参与设备联盟的中央服务器的协调下训练共享的全局模型。在该模型中，不同的设备可以为模型的训练和知识做出贡献，同时将大部分数据保留在设备中。</p><p id="048d" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">在联合学习模型中，一方下载深度学习模型，通过从给定设备上的数据中学习来改进它，然后将这些变化总结为小的、有重点的更新。只有这个模型的更新被发送到云，使用加密的通信，在那里它立即与其他用户更新进行平均，以改进共享模型。所有训练数据都保留在原始设备上，没有单独的更新存储在云中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/1f3db6ddb9c256465f588642fd90b248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*PxLfr7vFR2aaWZ-4.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mt" href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2017/04/federated-learning-collaborative . html</a></figcaption></figure><h2 id="80cc" class="ns mv it bd mw nt nu dn na nv nw dp ne mq nx ny ng mr nz oa ni ms ob oc nk od bi translated">差异隐私</h2><p id="8a74" class="pw-post-body-paragraph lb lc it le b lf nm ju lh li nn jx lk mq no ln lo mr np lr ls ms nq lv lw lx im bi translated">差异隐私是一种技术，用于限制统计算法对主体隐私的影响，这些主体的信息是更大数据集的一部分。粗略地说，如果一个观察者看到一个算法的输出时不能判断出计算中是否使用了某个特定个体的信息，那么这个算法就是差分私有的。差别隐私经常在识别其信息可能在数据库中的个人的上下文中被讨论。虽然它不直接涉及识别和再识别攻击，但差分私有算法可证明抵抗这种攻击。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/c500cd44833a7a742c8193c78fe2eabb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xQiVAJcv5uFU6Omi.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mt" href="https://towardsdatascience.com/understanding-differential-privacy-85ce191e198a" rel="noopener" target="_blank">https://towards data science . com/understanding-differential-privacy-85ce 191 e 198 a</a></figcaption></figure><h1 id="b301" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">PySyft</h1><p id="d324" class="pw-post-body-paragraph lb lc it le b lf nm ju lh li nn jx lk mq no ln lo mr np lr ls ms nq lv lw lx im bi translated">PySyft是一个框架，可以在深度学习模型中实现安全、私有的计算。PySyft将联邦学习、安全多方计算和差分隐私结合在一个编程模型中，该模型集成到不同的深度学习框架中，如PyTorch、Keras或TensorFlow。PySyft <a class="ae mt" href="https://arxiv.org/abs/1811.04017" rel="noopener ugc nofollow" target="_blank">的原理最初是在一篇研究论文</a>中概述的，它的<a class="ae mt" href="https://github.com/OpenMined/PySyft" rel="noopener ugc nofollow" target="_blank">首次实现是由OpenMind </a>领导的，OpenMind是领先的去中心化人工智能平台之一。</p><p id="7609" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">PySyft的核心组件是一个叫做SyftTensor的抽象。SyftTensors旨在表示数据的状态/转换，并且可以链接在一起。链式结构的头部总是有PyTorch张量，而SyftTensors所体现的状态转换使用子属性向下访问，使用父属性向上访问。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/3bcd485d86214f74d0bea7304dcd9b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ldbcwrtfWOg61s2c.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mt" href="https://arxiv.org/pdf/1811.04017.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1811.04017.pdf</a></figcaption></figure><p id="9b16" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">使用PySyft相对简单，与标准PyTorch或Keras程序没有太大区别。下面的动画演示了一个使用PySyft的简单分类模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/d7e2128af6bff9899347075e8fc269ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MeFE2n5TE5eXO4b4.gif"/></div></div></figure><p id="6879" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">PySyft代表了在深度学习程序中启用稳健隐私模型的首批尝试之一。随着空间的发展，隐私可能会成为下一代深度学习框架的基础构件之一。</p></div></div>    
</body>
</html>