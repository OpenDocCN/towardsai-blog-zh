<html>
<head>
<title>K-Nearest Neighbors (KNN) Algorithm Tutorial — Machine Learning Basics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k近邻(KNN)算法教程—机器学习基础</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/k-nearest-neighbors-knn-algorithm-tutorial-machine-learning-basics-ml-ec6756d3e0ac?source=collection_archive---------0-----------------------#2021-02-28">https://pub.towardsai.net/k-nearest-neighbors-knn-algorithm-tutorial-machine-learning-basics-ml-ec6756d3e0ac?source=collection_archive---------0-----------------------#2021-02-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/0a23213d617639c327ed14ae092020bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J_M-QOaebt1byl-2dBHy0w.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">这张图片是卡耐基梅隆大学<a class="ae jg" href="https://delphi.cmu.edu/" rel="noopener ugc nofollow" target="_blank"> Delphi研究小组</a>的<a class="ae jg" href="https://github.com/cmu-delphi/www-covidcast" rel="noopener ugc nofollow" target="_blank">开源项目</a>的<a class="ae jg" href="https://covidcast.cmu.edu/" rel="noopener ugc nofollow" target="_blank"> COVIDCAST </a>应用程序的截图。</figcaption></figure><h2 id="9988" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>、<a class="ae ep" href="https://towardsai.net/p/category/editorial" rel="noopener ugc nofollow" target="_blank">编辑</a>、<a class="ae ep" href="https://towardsai.net/p/category/tutorial" rel="noopener ugc nofollow" target="_blank">教程</a></h2><div class=""/><div class=""><h2 id="b2b1" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">潜入K-最近邻，一个基本的经典机器学习(ML)算法</h2></div><p id="830c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">作者:</strong> <a class="ae jg" href="https://www.linkedin.com/in/sujan-shirol/" rel="noopener ugc nofollow" target="_blank">苏扬·希罗</a>，<a class="ae jg" href="https://www.linkedin.com/in/husnasayedi/" rel="noopener ugc nofollow" target="_blank">胡斯娜·萨伊迪</a>，<a class="ae jg" href="https://mktg.best/vguzs" rel="noopener ugc nofollow" target="_blank">罗伯托·伊里翁多</a></p><div class="is it gp gr iu md"><a href="https://members.towardsai.net" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">加入人工智能，成为会员，你将不仅支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><blockquote class="ms mt mu"><p id="2871" class="lh li mv lj b lk ll kt lm ln lo kw lp mw lr ls lt mx lv lw lx my lz ma mb mc im bi translated">鸣谢:这项工作由苏扬·希罗尔领导，胡斯娜·萨耶迪监督，罗伯特·伊里翁多审核和编辑。我们使用自然语言优化来改善读者对本文的体验和感受。请让我们知道你是否有任何反馈，你是否希望看到更多的这种优化。<strong class="lj jt">除非另有说明，所有图片均来自作者。</strong></p></blockquote></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="759f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi ng translated"><span class="l nh ni nj bm nk nl nm nn no di">T</span>he k-nearest neighborhood algorithm，俗称KNN算法，是一种简单而有效的分类和回归监督<a class="ae jg" href="https://news.towardsai.net/mla" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">机器学习算法</strong> </a>。这篇文章将涵盖KNN算法，它的应用，优缺点，它背后的数学，以及它在Python中的实现。请务必在<a class="ae jg" href="https://colab.research.google.com/drive/1MI6EBfMRZTneiymgezTVUvz6xfOGZozy?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> Google Colab </strong> </a>或<a class="ae jg" href="https://github.com/towardsai/tutorials/tree/master/k-nearest-neighbors" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Github</strong></a><strong class="lj jt"/>上查看本教程的整个实现，以帮助您阅读。</p><p id="5202" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们来理解分类和回归的含义:分类从我们还是孩子的时候就已经形成了。而我们说的第一个词可能是“爸爸”或“妈妈”婴儿怎么知道他们的“妈妈”或“爸爸”是谁？我们可能看过视频，或者从父母那里听说过，他们是多么高兴和激动地教我们如何识别他们。他们以前每天都互相指指点点，直到我们明白其中的区别。</p><p id="15fc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，这种互相指着对方让我们学习的方法被称为监督学习。从根本上来说，<a class="ae jg" href="https://news.towardsai.net/mla#bc24" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">监督学习</strong> </a>是学习一个函数，这个函数根据例子，输入-输出对，把一个输入映射到一个输出[9]。</p><p id="06d3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">婴儿进行分类的特征最有可能是面部毛发和声音，这些被称为非独立<strong class="lj jt">变量/输入</strong>，独立<strong class="lj jt">变量/输出</strong>是二元的，几乎可以说是或不是。这整个过程可以被称为模型训练，我们将学习更复杂的方法来以机器可以理解的方式实现这一过程。类似地，当独立变量/输出是一个常量值时，它被称为回归问题。</p><h1 id="fb09" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">什么是K近邻(KNN)算法？</h1><p id="7a20" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">KNN算法是一种主要的经典机器学习算法，它专注于从新的<strong class="lj jt">未分类/未标记的</strong>数据点到现有的<strong class="lj jt">已分类/已标记的</strong>数据点的距离。不管什么原因，把它想成是在一个学年的中间进入一所学院的过程。正如我们所看到的，学生之间可能已经形成了志同道合的群体，我们找出自己适合什么样的群体只是时间问题——特别是，我们会觉得与哪个群体更有联系，或者换句话说，与哪个群体的距离更小。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/9db7ad0ef5cdfa083f4bf293ccd299c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Lg6X07T8WLJnPWNb"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:我们的示例数据集中的三种花的颜色，红色、黄色和绿色。</figcaption></figure><p id="f5ef" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们已经从数据集中标记了数据点。我们将把它们绘制在二维图上；这些数据点属于用红色、绿色和黄色表示的三个类别，如图1所示。</p><p id="97a6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们将考虑由黑色十字标记表示的新的未标记数据点。这使得需要从三种颜色中确定这个新数据点属于哪一类。首先，我们取一个随机值，即k值。k值表示要从新的未标记数据点中寻找的最近点的数量。将k值视为5。接下来，我们计算从未标记的数据点到图上每个数据点的距离，并选择前5个最短距离。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/57131adece2fc245809200dbc2717fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ksMa1Of3NFCvHo_Y"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:测量数据集类别之间的距离。</figcaption></figure><p id="e663" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在前5个最接近的数据点中，3个属于红色类别，1个属于绿色类别，1个属于黄色类别。这些被称为新数据的k(5)个最近邻。现在很明显，新的数据点属于红色类别，因为它的大多数最近的邻居来自红色类别。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/1d20740f9fc738ad23ac047e2b481e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gU-1n6umqceN100g"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3:查看新数据点如何属于示例数据集中的红色类别。</figcaption></figure><p id="af50" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">类似地，在回归问题中，目标是预测新数据点的值，而不是它所属的类别。再次举例说明，绘制一个由给定数据集中的数据点组成的二维图形。由于它是一个二维图形，每个数据点有两个特征。x轴表示特征1，y轴表示特征2。</p><p id="5b98" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们引入一个新的数据点，它只有feature-1的值是已知的，我们需要预测feature-2的值。我们取k值为5，并从新数据点得到最近的5个相邻点。新数据点的特征-2的预测值是5个最近邻点的特征-2的平均值。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/82a544781426d84bb3e490fcb5d69ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YXH-PXGg_RPpupEB"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:我们的例子引入了一个新的数据点来说明k近邻(KNN)算法。</figcaption></figure><h1 id="5bb3" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">何时使用KNN？</h1><ol class=""><li id="0f41" class="ou ov jj lj b lk oh ln oi lq ow lu ox ly oy mc oz pa pb pc bi translated">KNN算法可以与最精确的模型竞争，因为它做出高度精确的预测。因此，我们可以将KNN算法用于要求高精度但不需要人类可读模型的应用[11]。</li><li id="fcdf" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc oz pa pb pc bi translated">当为我们的任务提供的数据集很小时。</li><li id="defa" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc oz pa pb pc bi translated">当数据被正确标注时，预测值将在给定的标注中。如果存在类别1、类别2和类别3，那么预测的类别应该是其中之一，而不是任何其他类别。</li><li id="6837" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc oz pa pb pc bi translated">KNN用于解决回归、分类或搜索问题</li></ol><h1 id="9f6c" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">使用KNN的利与弊</h1><h2 id="cedd" class="pi nq jj bd nr pj pk dn nv pl pm dp nz lq pn po ob lu pp pq od ly pr ps of jp bi translated">赞成的意见</h2><ul class=""><li id="bde1" class="ou ov jj lj b lk oh ln oi lq ow lu ox ly oy mc pt pa pb pc bi translated">该算法简单易行，因为它只需要两个参数:k值和距离函数。</li><li id="be84" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc pt pa pb pc bi translated">一个好的k值将使算法对噪声具有鲁棒性。</li><li id="9b1f" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc pt pa pb pc bi translated">它学习非线性决策边界。</li><li id="aab0" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc pt pa pb pc bi translated">给定的数据几乎没有任何假设。唯一假定的是邻近/相似的实例属于同一类别。</li><li id="817e" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc pt pa pb pc bi translated">这是一种非参数方法。不需要模型装配/培训。数据不言自明。</li><li id="c7b3" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc pt pa pb pc bi translated">由于不需要模型训练，因此很容易更新数据集。</li></ul><h2 id="dc95" class="pi nq jj bd nr pj pk dn nv pl pm dp nz lq pn po ob lu pp pq od ly pr ps of jp bi translated">骗局</h2><ul class=""><li id="ee4b" class="ou ov jj lj b lk oh ln oi lq ow lu ox ly oy mc pt pa pb pc bi translated">对于大型数据集效率低下，因为必须计算每个点的距离，每次算法遇到新的数据点时都会循环。</li><li id="98ab" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc pt pa pb pc bi translated">KNN假设相似的数据点彼此接近。因此，模型容易受到异常值的影响。即使在新数据属于不同类别的情况下，来自特定类别的一些异常值也可以将新数据吸引到该类别。</li><li id="1abb" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc pt pa pb pc bi translated">它不能处理不平衡的数据。当数据不平衡时，属于一个特定类别的数据比其他类别的数据多得多；算法会有偏差。所以需要明确处理。</li><li id="a3fe" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc pt pa pb pc bi translated">如果我们的数据集需要一个很大的K，这将增加算法的计算开销。</li></ul><h1 id="29e1" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">深入KNN背后的数学</h1><p id="5875" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">如上所述，该算法计算从新数据点到每个现有数据点的距离。问题是，距离是如何测量的？有三种方法，我们将在本书中讨论。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/0659682ea4dffe201d7af1809ff6fb1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/0*KyTT3RiknPQX4qVK"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图5:闵可夫斯基距离的表示。</figcaption></figure><h2 id="4673" class="pi nq jj bd nr pj pk dn nv pl pm dp nz lq pn po ob lu pp pq od ly pr ps of jp bi translated">闵可夫斯基距离:</h2><p id="3ae5" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">a)闵可夫斯基距离是赋范向量空间中的广义距离函数。向量空间必须满足以下要求:</p><ol class=""><li id="785f" class="ou ov jj lj b lk ll ln lo lq pv lu pw ly px mc oz pa pb pc bi translated"><strong class="lj jt">零矢量:</strong>零矢量长度为0</li><li id="b530" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc oz pa pb pc bi translated"><strong class="lj jt">标量因子:</strong>一个矢量乘以一个标量只改变长度，不改变方向</li><li id="6d5b" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc oz pa pb pc bi translated"><strong class="lj jt">三角形不等式:</strong>任意两点间的最短距离是一条直线。</li></ol><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/2d4be3b6fef40c742d9a30c7aca64535.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ll7kHBDPpK7pvXywlNaqWg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图6:闵可夫斯基距离的公式。</figcaption></figure><p id="6000" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">b)在某些情况下，</p><ol class=""><li id="12aa" class="ou ov jj lj b lk ll ln lo lq pv lu pw ly px mc oz pa pb pc bi translated">当p=1时——这是曼哈顿距离</li><li id="50e5" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc oz pa pb pc bi translated">当p=2时，这是欧几里德距离</li><li id="52c3" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc oz pa pb pc bi translated">当p =无穷大时，这是切比雪夫距离</li></ol><h2 id="a5b3" class="pi nq jj bd nr pj pk dn nv pl pm dp nz lq pn po ob lu pp pq od ly pr ps of jp bi translated">欧几里得距离</h2><p id="662e" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">在数学中，欧几里得空间中两点之间的欧几里得距离是两点之间线段的长度。它可以使用勾股定理从点的笛卡尔坐标中计算出来，因此有时被称为勾股距离[10]。</p><p id="7ba7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了计算二维平面上两点(x1，y1)和(x2，y2)之间的距离，我们使用以下公式:</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/6b63ff3764b3e423c1a9b2653a52cdac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3SHCUs7gn73FYUy5aFlVKA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7:欧几里德距离公式。</figcaption></figure><h2 id="e744" class="pi nq jj bd nr pj pk dn nv pl pm dp nz lq pn po ob lu pp pq od ly pr ps of jp bi translated">曼哈顿距离</h2><p id="d615" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">曼哈顿距离的计算类似于欧几里德距离的计算，唯一的区别是我们取绝对值，而不是取平方差之和的平方根。通过取绝对值，我们不是像欧几里得距离那样计算两点之间的最短距离。</p><p id="b4ed" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从根本上来说，<strong class="lj jt">欧几里德距离</strong>代表<strong class="lj jt">“从一点飞到另一点”，</strong>，而<strong class="lj jt">曼哈顿距离</strong>是<strong class="lj jt">“在一个城市中沿着小路或公路从一点行进到另一点”</strong>。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/7f3fa86f883a284a56992162b1342e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ouilSM53ltNzO2KwZBkkg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:曼哈顿距离公式。</figcaption></figure><p id="35cb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，最有可能的是，用于计算距离的方法是欧几里德距离公式。其中一个原因是，欧几里德距离可以计算任何维度上的距离，而曼哈顿可以找到垂直平面上的元素。</p><h1 id="15c7" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">如何为K选择合适的值？</h1><p id="caf3" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">选择K对于每个数据集都是唯一的。没有计算最佳K值的标准统计方法。我们想选择一个能减少误差的K值。随着K的增加，由于平均或多数投票，我们的预测变得更加稳定。然而，如果K太大，那么错误率将再次增加，因为它将使模型欠拟合。换句话说，小K产生低偏差和高方差(更高的复杂性)，而大K产生高偏差和低方差。也就是说，有几种不同的方法可以尝试:</p><ol class=""><li id="801f" class="ou ov jj lj b lk ll ln lo lq pv lu pw ly px mc oz pa pb pc bi translated"><strong class="lj jt">领域知识:</strong>如前所述，K高度依赖于数据。例如，如果分析一个独特的花卉物种数据集，很容易看出K应该是花卉物种的特定数量。</li><li id="e1b3" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc oz pa pb pc bi translated"><strong class="lj jt">交叉验证:</strong>这种众所周知的技术在比较K值范围的精度测量中是有用的，例如，K是从1到10的值。这种技术需要将训练集分成测试/验证集来调整K以找到最佳值。</li><li id="b719" class="ou ov jj lj b lk pd ln pe lq pf lu pg ly ph mc oz pa pb pc bi translated"><strong class="lj jt">平方根:</strong>当对数据缺乏领域知识时，可以尝试的一个简单方法是对训练集中的数据点数求平方根。</li></ol><h1 id="2f83" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">KNN在Python中的实现</h1><p id="9122" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">为了实现KNN算法，我们将使用虹膜数据集。鸢尾数据集是三个相关物种的鸢尾花的形态学变化的集合:刚毛鸢尾、杂色鸢尾和海滨鸢尾。观察到的形态变异是萼片长度、萼片宽度、花瓣长度和花瓣宽度。</p><p id="6440" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Sklearn是一个Python库，具有各种分类、回归和聚类算法。它还保存虹膜数据集作为样本数据。我们将导入必要的库，如NumPy、Pandas和matplotlib。</p><pre class="on oo op oq gt pz qa qb qc aw qd bi"><span id="2a8b" class="pi nq jj qa b gy qe qf l qg qh">import numpy as np</span><span id="95a9" class="pi nq jj qa b gy qi qf l qg qh">import pandas as pd</span><span id="35dc" class="pi nq jj qa b gy qi qf l qg qh">import matplotlib.pyplot as plt</span><span id="1722" class="pi nq jj qa b gy qi qf l qg qh">import seaborn as sns</span><span id="afa0" class="pi nq jj qa b gy qi qf l qg qh">from sklearn.model_selection import train_test_split</span><span id="5956" class="pi nq jj qa b gy qi qf l qg qh">from sklearn.neighbors import KNeighborsClassifier</span><span id="0ab4" class="pi nq jj qa b gy qi qf l qg qh">from sklearn.model_selection import cross_val_score</span><span id="c6a9" class="pi nq jj qa b gy qi qf l qg qh">from sklearn.datasets import load_iris</span><span id="ce84" class="pi nq jj qa b gy qi qf l qg qh">iris = load_iris()</span></pre><p id="3cff" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下表显示了我们在Pandas数据帧中获得前5行数据后的样子。目标值为0.0、1.0和2.0，分别代表Setosa、Versicolor和Virginica。</p><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qj"><img src="../Images/2e325dd4342b1e4ba504b1bf4b48c525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cqwym43F0VcFxt52"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:显示熊猫数据帧中前5行数据的表格。</figcaption></figure><p id="637a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于KNN对异常值和不平衡数据非常敏感，因此检查和处理异常值和不平衡数据非常重要。通过绘制目标变量的计数图来检查不平衡数据，每种花类型有50个样本。因此，数据完全平衡。</p><p id="1e4a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe qk ql qm qa b">sns.countplot(x=’target’, data=iris)</code></p><figure class="on oo op oq gt iv gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/1def11960e2c9522a8d2f5b9de91326c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/0*oNWeu_nZUEu7xb4Y"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:</figcaption></figure><p id="2dec" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过使用箱线图检查异常值，似乎没有多少异常值需要处理。</p><pre class="on oo op oq gt pz qa qb qc aw qd bi"><span id="7e33" class="pi nq jj qa b gy qe qf l qg qh">for feature in [‘sepal length (cm)’, ‘sepal width (cm)’, ‘petal length (cm)’, ‘petal width (cm)’]:</span><span id="8791" class="pi nq jj qa b gy qi qf l qg qh">sns.boxplot(x=’target’, y=feature, data=iris)</span><span id="55b1" class="pi nq jj qa b gy qi qf l qg qh">plt.show()</span></pre><figure class="on oo op oq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qo"><img src="../Images/d2d976b3f4a55940adc0f832e9212390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ahJLLRBHhx7ApTgu"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:</figcaption></figure><p id="50e1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们将数据分为训练集和测试集，以衡量模型的准确性。该模型将在训练集上进行训练，该训练集是随机选择的60%的原始数据，然后用剩余40%的原始数据的测试集进行评估。在将其分成训练集和测试集之前，有必要将<strong class="lj jt">特征/从属</strong>和<strong class="lj jt">目标/独立</strong>变量分开。</p><pre class="on oo op oq gt pz qa qb qc aw qd bi"><span id="36fb" class="pi nq jj qa b gy qe qf l qg qh">X = iris.drop([‘target’], axis=1)</span><span id="e143" class="pi nq jj qa b gy qi qf l qg qh">y = iris[‘target’]</span><span id="d480" class="pi nq jj qa b gy qi qf l qg qh">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)</span></pre><p id="c894" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">构建k值为1的初始模型，这意味着在对新数据点进行分类时将只考虑1个最近邻。在内部，将计算从新数据点到所有数据点的距离，然后按照从最小到最大的顺序以及它们各自的类别进行排序。由于k值为1，排序数组中第一个实例的类(目标值)将决定新的数据类。正如我们所看到的，我们获得了91.6%的正确率。但是，需要选择最佳k值。</p><pre class="on oo op oq gt pz qa qb qc aw qd bi"><span id="7dea" class="pi nq jj qa b gy qe qf l qg qh">knn = KNeighborsClassifier(n_neighbors=1)</span><span id="3fd6" class="pi nq jj qa b gy qi qf l qg qh">knn.fit(X_train, y_train)</span><span id="4de7" class="pi nq jj qa b gy qi qf l qg qh">print(knn.score(X_test, y_test))</span><span id="c95b" class="pi nq jj qa b gy qi qf l qg qh">Output: 0.9166666666666666</span></pre><p id="9468" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了通过交叉验证方法找到最佳k值，我们在这种情况下计算范围从1到26的k值的准确度，并选择最佳k值。准确度分数大约在86%到96%之间。正如所观察到的，准确度分数从低值开始，在某个点达到峰值，在一段时间内保持大致恒定，然后再次下降。分数在一段时间内保持恒定的范围可被视为给定数据集的最佳k值。</p><pre class="on oo op oq gt pz qa qb qc aw qd bi"><span id="d7f9" class="pi nq jj qa b gy qe qf l qg qh">k_range = list(range(1,26))</span><span id="c083" class="pi nq jj qa b gy qi qf l qg qh">scores = []</span><span id="602f" class="pi nq jj qa b gy qi qf l qg qh">for k in k_range:</span><span id="92e9" class="pi nq jj qa b gy qi qf l qg qh">knn = KNeighborsClassifier(n_neighbors=k)</span><span id="b07e" class="pi nq jj qa b gy qi qf l qg qh">knn.fit(X_train, y_train)</span><span id="9e78" class="pi nq jj qa b gy qi qf l qg qh">y_pred = knn.predict(X_test)</span><span id="5de0" class="pi nq jj qa b gy qi qf l qg qh">scores.append(metrics.accuracy_score(y_test, y_pred))</span><span id="7fab" class="pi nq jj qa b gy qi qf l qg qh">plt.plot(k_range, scores)</span><span id="8b49" class="pi nq jj qa b gy qi qf l qg qh">plt.xlabel(‘Value of k’)</span><span id="71ab" class="pi nq jj qa b gy qi qf l qg qh">plt.ylabel(‘Accuracy Score’)</span><span id="221e" class="pi nq jj qa b gy qi qf l qg qh">plt.title(‘Accuracy Scores for different values of k’)</span><span id="b74f" class="pi nq jj qa b gy qi qf l qg qh">plt.show()</span></pre><figure class="on oo op oq gt iv gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/ed4eaee39e0558a7f38d07a58ebc6327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/0*gjuFrH8XVotq1z3x"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12:</figcaption></figure><p id="9b62" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">引入一个新的未标记数据点，我们需要预测的类是花类型，它属于基于其形态学特征的类别。我们将构建k值为11的模型。</p><pre class="on oo op oq gt pz qa qb qc aw qd bi"><span id="a9ea" class="pi nq jj qa b gy qe qf l qg qh">knn = KNeighborsClassifier(n_neighbors=11)<br/>knn.fit(iris.drop([‘target’], axis=1), iris[‘target’])</span><span id="8066" class="pi nq jj qa b gy qi qf l qg qh">X_new = np.array([[1, 2.9, 10, 0.2]])</span><span id="7401" class="pi nq jj qa b gy qi qf l qg qh">prediction = knn.predict(X_new)</span><span id="ba9c" class="pi nq jj qa b gy qi qf l qg qh">print(prediction)</span><span id="1b19" class="pi nq jj qa b gy qi qf l qg qh">if prediction[0] == 0.0:</span><span id="b836" class="pi nq jj qa b gy qi qf l qg qh">print(‘Setosa’)</span><span id="aea4" class="pi nq jj qa b gy qi qf l qg qh">elif prediction[0] == 1.0:</span><span id="d72e" class="pi nq jj qa b gy qi qf l qg qh">print(‘Versicolor’)</span><span id="d08b" class="pi nq jj qa b gy qi qf l qg qh">else:</span><span id="cb65" class="pi nq jj qa b gy qi qf l qg qh">print(‘Virginica’)</span><span id="bec6" class="pi nq jj qa b gy qi qf l qg qh">Output: [2.]</span><span id="0d42" class="pi nq jj qa b gy qi qf l qg qh">Virginica</span></pre><h1 id="bcb4" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">KNN应用</h1><p id="c468" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">从预测流行病[2]和经济到信息检索[4] [5]，推荐系统[3]，数据压缩和医疗保健[1]，k-最近邻(KNN)算法已经成为这些应用的基础。众所周知，KNN是最简单的监督机器学习算法之一，正如我们已经讨论过的，它的实现主要用于回归和分类任务。</p><p id="328b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">k-最近邻算法最重要的一个用例是推荐系统[3] [6]。一个简单的谷歌搜索为我们提供了几篇有前途的文章来实现使用KNN的推荐系统[7]，这主要是由于KNN对一组特定项目传播相似推荐的能力。</p><p id="f60b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如，想象一下，我们让一组用户对电影有不同的伪随机兴趣。推荐系统比较用户的简档以发现一组用户是否具有相似的品味。之后，假设两个用户在比较期间对两个或更多的项目有相似的品味。在这种情况下，第一用户喜欢的项目可能对第二用户来说是令人愉快的。</p><p id="c342" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">类似地，KNN可以在分类任务中使用[8]。</p><h1 id="190f" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">结论</h1><p id="c63b" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">KNN是一种高效、简单、易于实现的监督机器学习算法，可用于分类和回归问题。该模型通过计算离预测点最近的选定数量的样本K的距离来起作用。</p><p id="e2a0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于一个分类问题，标签变成了最近的K个点的多数票。对于回归问题，标签成为最近的K个点的平均值。每当进行预测时，模型都会搜索整个训练集，以找到K个最相似的示例来标记原始预测点。</p><p id="96a0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这种算法的主要缺点是，随着数据数量的增加，计算费用和时间也会增加。然而，如果我们正在处理的数据集是一个适当大小的数据集(如虹膜数据集)，KNN是一个简单而直接的算法，因为不需要建立模型，调整参数，或对模型做任何额外的假设。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="44e1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">免责声明:</strong>本文所表达的观点均为作者个人观点，不代表与作者(直接或间接)相关的任何公司的观点。这项工作并不打算成为最终产品，而是当前思想的反映，同时也是讨论和改进的催化剂。</p><p id="9515" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">除非另有说明，所有图片均来自作者。</strong></p><p id="af19" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">经由<a class="ae jg" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">发布<strong class="lj jt">走向AI </strong>发布</a></p><h1 id="823b" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">资源</h1><div class="is it gp gr iu md"><a href="https://github.com/towardsai/tutorials/tree/master/k-nearest-neighbors" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">k-最近邻教程</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">AI相关教程。免费访问其中任何一个→https://towardsai.net/editorial-toward sai/教程</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">github.com</p></div></div><div class="mm l"><div class="qq l mo mp mq mm mr ja md"/></div></div></a></div><div class="is it gp gr iu md"><a href="https://colab.research.google.com/drive/1MI6EBfMRZTneiymgezTVUvz6xfOGZozy?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">k近邻—谷歌联合实验室</h2><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">colab.research.google.com</p></div></div><div class="mm l"><div class="qr l mo mp mq mm mr ja md"/></div></div></a></div><p id="79c9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://www.youtube.com/watch?v=otolSnbanQk" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> Youtube教程</strong> </a>。</p><h1 id="fb46" class="np nq jj bd nr ns nt nu nv nw nx ny nz ky oa kz ob lb oc lc od le oe lf of og bi translated">参考</h1><p id="30ba" class="pw-post-body-paragraph lh li jj lj b lk oh kt lm ln oi kw lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">[1]使用KNN分类算法预测新冠肺炎可能性，(2021)。检索于2021年1月14日，来自<a class="ae jg" href="https://assets.researchsquare.com/files/rs-70985/v2_stamped.pdf" rel="noopener ugc nofollow" target="_blank">https://assets . research square . com/files/RS-70985/v2 _ stamped . pdf</a></p><p id="614b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]基于推特数据分类的流感KNN算法预测模型，Kavitha等人，(2021)。检索于2021年1月14日，来自<a class="ae jg" href="http://ijseas.com/volume1/v1i7/ijseas20150712.pdf" rel="noopener ugc nofollow" target="_blank">http://ijseas.com/volume1/v1i7/ijseas20150712.pdf</a></p><p id="8207" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3] Subramaniyaswamy，v .，&amp; Logesh，R. (2017年)。基于用户偏好挖掘的自适应KNN推荐系统。<em class="mv">无线个人通信</em>，<em class="mv"> 97 </em> (2)，2229–2247。DOI:10.1007/s 11277–017–4605–5</p><p id="25c8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4]《信息检索导论》，克里斯·曼宁和潘杜·纳亚克，斯坦福大学，(2021)。检索于2021年1月14日，来自<a class="ae jg" href="https://web.stanford.edu/class/cs276/handouts/lecture12-textcat.pdf" rel="noopener ugc nofollow" target="_blank">https://web . Stanford . edu/class/cs 276/讲义/讲师12-textcat.pdf </a></p><p id="7e48" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]最近邻搜索。(2021).检索于2021年1月14日，来自<a class="ae jg" href="https://en.wikipedia.org/wiki/Nearest_neighbor_search" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Nearest_neighbor_search</a></p><p id="cc2f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6]基于协同过滤的推荐系统，郑文，斯坦福大学，(2021)。检索于2021年1月14日，来自<a class="ae jg" href="http://cs229.stanford.edu/proj2008/Wen-RecommendationSystemBasedOnCollaborativeFiltering.pdf" rel="noopener ugc nofollow" target="_blank">http://cs 229 . Stanford . edu/proj 2008/Wen-recommendationsystembasedoncollaborative filtering . pdf</a></p><p id="8b46" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[7]使用KNN的推荐系统——谷歌搜索。(2021).检索于2021年1月14日，来自<a class="ae jg" href="https://mktg.best/a1t-9" rel="noopener ugc nofollow" target="_blank">https://mktg.best/a1t-9</a></p><p id="4e34" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[8]K-最近邻算法。(2021).2021年1月14日检索，来自<a class="ae jg" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a></p><p id="9998" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[9]“监督学习”。2021.En.Wikipedia.Org。<a class="ae jg" href="https://en.wikipedia.org/wiki/Supervised_learning#cite_note-1." rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Supervised _ learning # cite _ note-1。</a></p><p id="79e1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[10]“欧几里得距离”。2021.En.Wikipedia.Org。https://en . Wikipedia . org/wiki/Euclidean _ distance #:~:text = In % 20数学% 2C % 20欧几里得% 20距离，被称为% 20毕达哥拉斯% 20距离。</p><p id="80c8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[11]“IBM知识中心”。2021.Ibm.Com。<a class="ae jg" href="https://www.ibm.com/support/knowledgecenter/SSCJDQ/com.ibm.swg.im.dashdb.analytics.doc/doc/r_knn_usage.html." rel="noopener ugc nofollow" target="_blank">https://www . IBM . com/support/knowledge center/SSCJDQ/com . IBM . swg . im . dash db . analytics . doc/doc/r _ KNN _ usage . html</a></p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="a944" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">元数据:</strong> {knn算法} {knn分类器} {knn in r } {knn in python } { sklearn KNN } {knn回归} {knn聚类} {knn分类器sklearn} {knn监督} { KNN分类} {knn vs k均值} { KNN算法示例} { KNN过拟合}</p></div></div>    
</body>
</html>