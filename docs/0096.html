<html>
<head>
<title>Demystifying the Architecture of Long Short Term Memory (LSTM) Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭开长短期记忆(LSTM)网络结构的神秘面纱</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/demystifying-the-architecture-of-long-short-term-memory-lstm-networks-38163ade5aa2?source=collection_archive---------1-----------------------#2019-07-02">https://pub.towardsai.net/demystifying-the-architecture-of-long-short-term-memory-lstm-networks-38163ade5aa2?source=collection_archive---------1-----------------------#2019-07-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="46c8" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">面向人工智能的LSTMs | <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">架构</a></h2><div class=""/><h1 id="a265" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">介绍</h1><p id="41ce" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我之前的文章中，我解释了<a class="ae ls" href="https://medium.com/towards-artificial-intelligence/introduction-to-the-architecture-of-recurrent-neural-networks-rnns-a277007984b7" rel="noopener"><strong class="kw ja"/>架构</a>。<strong class="kw ja">rnn</strong>并不完美，主要有两大问题<strong class="kw ja">爆炸渐变</strong>和<strong class="kw ja">消失渐变</strong>。<strong class="kw ja">爆炸渐变</strong>更容易发现，但是<strong class="kw ja">消失渐变</strong>更难解决。我们使用<strong class="kw ja">长短期记忆(LSTM) </strong>和<strong class="kw ja">门控递归单元(GRU) </strong>，它们是解决<strong class="kw ja">消失梯度</strong>问题的非常有效的解决方案，并且它们允许神经网络捕获更长的范围相关性。</p><h2 id="23be" class="lt jx iq bd jy lu lv dn kc lw lx dp kg lf ly lz kk lj ma mb ko ln mc md ks iw bi translated"><strong class="ak">爆炸渐变</strong></h2><p id="c26d" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当通过时间反向传播(<strong class="kw ja"> BPTT </strong>)算法赋予权重极大的重要性，权重的值变得非常大。这可能导致权重值溢出和NaN值。这导致网络不稳定。</p><p id="70d2" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">在网络训练期间，我们可以通过观察以下迹象来检测<strong class="kw ja">爆炸梯度</strong>。</p><ul class=""><li id="dc4f" class="mj mk iq kw b kx me lb mf lf ml lj mm ln mn lr mo mp mq mr bi translated">在训练期间，模型的权重值很快变得非常大。</li><li id="7c71" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">模型的权重值在训练期间变为NaN。</li><li id="060d" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">在训练期间，每个节点和层的误差梯度值始终高于1.0。</li></ul><p id="b57d" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">我们可以用几种方法处理<strong class="kw ja">爆炸渐变</strong>问题。以下是流行的技术。</p><ul class=""><li id="ef00" class="mj mk iq kw b kx me lb mf lf ml lj mm ln mn lr mo mp mq mr bi translated">这个问题很容易解决。看看你的梯度向量，如果它大于某个阈值，重新缩放你的梯度向量，使它不要太大。这被称为<strong class="kw ja">渐变剪辑</strong>。</li><li id="ce11" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">我们可以通过检查网络的权重值并对大权重值的网络损失函数应用惩罚来使用权重正则化。</li><li id="8f0b" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">我们可以用<strong class="kw ja"> LSTMs </strong>或者<strong class="kw ja"> GRUs </strong>代替<strong class="kw ja"> RNNs </strong>。</li><li id="9f96" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">我们可以使用<strong class="kw ja"> Xavier </strong>初始化或<strong class="kw ja"> He </strong>初始化进行权重初始化。</li></ul><h2 id="dfe6" class="lt jx iq bd jy lu lv dn kc lw lx dp kg lf ly lz kk lj ma mb ko ln mc md ks iw bi translated"><strong class="ak">消失渐变</strong></h2><p id="5e2d" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="kw ja">消失梯度</strong>一般出现在激活函数的梯度很小时。在反向传播算法中，当权重与低梯度相乘时，它们变得非常小，并且随着它们进一步进入网络而消失。这使得神经网络忘记了<strong class="kw ja">的长期依赖性</strong>。</p><p id="2c23" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">我们知道<strong class="kw ja">长期依赖</strong>对于rnn的正常运行非常重要。为了理解<strong class="kw ja">长期依赖的重要性，</strong>考虑下面两个语句，它们将一个字一个字地馈入RNN以预测接下来的单词。</p><blockquote class="mx my mz"><p id="3eee" class="ku kv na kw b kx me kz la lb mf ld le nb mg lh li nc mh ll lm nd mi lp lq lr ij bi translated">猫喜欢吃鱼，鱼很好吃，T9渴望吃更多。</p><p id="4c15" class="ku kv na kw b kx me kz la lb mf ld le nb mg lh li nc mh ll lm nd mi lp lq lr ij bi translated">这些猫喜欢吃鱼，鱼很好吃，而且渴望吃更多。</p></blockquote><ul class=""><li id="5bf4" class="mj mk iq kw b kx me lb mf lf ml lj mm ln mn lr mo mp mq mr bi translated">网络必须在时间步骤2记住句子(cat)的主语(单数或复数),以便在时间步骤12预测两个语句的单词。</li><li id="6b7e" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">在训练中，误差随着时间反向传播。与较早时间步长的层权重相比，更接近当前时间步长的层权重受到的影响更大。</li><li id="2c7e" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">通常在每个时间步与这些偏导数成比例更新的递归层中的权重没有在正确的方向上被充分推动，这导致网络进一步学习。</li><li id="2fd5" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">该模型不能更新层权重以反映来自较早时间步骤的长期语法依赖性。</li></ul><p id="b2a2" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">我们可以用几种方法处理渐变消失问题。以下是流行的技术。</p><ul class=""><li id="f801" class="mj mk iq kw b kx me lb mf lf ml lj mm ln mn lr mo mp mq mr bi translated">初始化单位矩阵的网络权重，以便最小化消失梯度的可能性。</li><li id="ddb5" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">我们可以使用<strong class="kw ja"> ReLU </strong>激活功能代替<strong class="kw ja">s形</strong>或<strong class="kw ja"> tanh </strong>。</li><li id="7578" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">我们可以用<strong class="kw ja">lstm</strong>或者<strong class="kw ja"> GRUs </strong>代替<strong class="kw ja"> RNNs </strong>。lstm或<strong class="kw ja"> GRUs </strong>被设计成捕获顺序数据中长期依赖关系。</li></ul><h1 id="6065" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">LSTM建筑</h1><p id="482b" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="kw ja"> LSTMs </strong>是<strong class="kw ja"> RNNs </strong>的变体，能够学习长期依赖性。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ne"><img src="../Images/342d38b9acd7cc7701a45eb82feabdf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2DR__KRTAUOd6DSC52AWCg.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk translated"><strong class="bd jy"> LSTM建筑</strong></figcaption></figure><ul class=""><li id="3152" class="mj mk iq kw b kx me lb mf lf ml lj mm ln mn lr mo mp mq mr bi translated"><strong class="kw ja"> LSTMs </strong>的关键是<strong class="kw ja">单元状态(C) </strong>存储信息。</li><li id="7d2b" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated"><strong class="kw ja"> LSTMs </strong>确实有能力删除或添加信息到<strong class="kw ja">单元状态</strong>。</li><li id="2017" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">移除或添加信息到由称为<strong class="kw ja">门</strong>的结构调节的<strong class="kw ja">单元状态</strong>的能力。</li><li id="71ed" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">门由一个sigmoid神经网络层和一个逐点乘法运算组成。</li></ul><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nu"><img src="../Images/5cc4aae93b3b8bc2c64f11a4b70afdd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IwOArrA5sKMd3Uf-qXTx5g.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk translated"><strong class="bd jy">门</strong></figcaption></figure><ul class=""><li id="01ca" class="mj mk iq kw b kx me lb mf lf ml lj mm ln mn lr mo mp mq mr bi translated">sigmoid层输出0到1之间的数字。</li><li id="2a62" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">1代表“完全保留这个”，0代表“完全摆脱这个。”</li><li id="0062" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated"><strong class="kw ja"> LSTMs </strong>单元由三个关键部件<strong class="kw ja">忘记</strong>、<strong class="kw ja">更新</strong>和<strong class="kw ja">输出</strong>门组成。</li></ul><p id="02e7" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated"><strong class="kw ja">忘记门架构</strong></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ne"><img src="../Images/cf53ff5f5d9e45c982c8823eb8461921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hwWppCgUjseERF9K_BS6sA.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk translated"><strong class="bd jy"> <em class="nv">忘门建筑</em> </strong></figcaption></figure><p id="6e79" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated"><strong class="kw ja"> <em class="na">遗忘门</em> </strong>是存储信息的<strong class="kw ja"> LSTMs </strong>中的<strong class="kw ja">单元状态</strong>。它从<strong class="kw ja">单元状态</strong>中决定哪些信息需要保留，哪些信息需要丢弃。这个决定是由称为“<strong class="kw ja">忘记门</strong>”层的s形层做出的。</p><ul class=""><li id="6780" class="mj mk iq kw b kx me lb mf lf ml lj mm ln mn lr mo mp mq mr bi translated">它采用先前隐藏的层激活h <t>和当前输入x <t>，并在其上应用sigmoid层，并输出值在0和1之间的张量</t></t></li><li id="26f5" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">将这个张量与之前的<strong class="kw ja">细胞状态</strong>C&lt;t1&gt;相乘，以保留相关信息，丢弃不相关信息。</li></ul><p id="7769" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">让我们回到之前的例子</p><blockquote class="mx my mz"><p id="88d4" class="ku kv na kw b kx me kz la lb mf ld le nb mg lh li nc mh ll lm nd mi lp lq lr ij bi translated">这只猫很喜欢吃鱼，鱼很好吃，T33渴望吃更多。</p><p id="62ac" class="ku kv na kw b kx me kz la lb mf ld le nb mg lh li nc mh ll lm nd mi lp lq lr ij bi translated">猫喜欢吃鱼，鱼很好吃，T37渴望吃更多。</p></blockquote><p id="14f8" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">为了预测每个序列中第12个时间步的单词，网络必须记住在时间步2看到的句子的主语(猫)是单数还是复数。</p><p id="d8a1" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">该模型将基于所有先前的输入，尝试预测两个语句在时间步骤12的下一个单词。<strong class="kw ja">单元格状态</strong>必须包括在时间步骤2中看到的句子的主语(cats ),它是单数或复数。</p><p id="cfb9" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">当它看到一个主题(猫)时，它希望保留关于该主题的信息，无论它是单数还是复数。</p><p id="a8dc" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated"><strong class="kw ja">更新星门架构</strong></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ne"><img src="../Images/f1c4bbfdba8680c45d2d044c9f8ef12a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IbsT0JBnvEp--6A262-tsw.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk translated"><strong class="bd jy"> <em class="nv">更新门架构</em> </strong></figcaption></figure><p id="ab8e" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated"><strong class="kw ja"> <em class="na">更新门</em> </strong>决定单元状态中需要存储什么新信息。</p><ul class=""><li id="720e" class="mj mk iq kw b kx me lb mf lf ml lj mm ln mn lr mo mp mq mr bi translated">名为“<strong class="kw ja">输入门</strong>”层的sigmoid函数决定哪些值需要更新。它给输出一个值在0和1之间的张量</li><li id="0422" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">一个<strong class="kw ja"> tanh </strong>函数创建一个新的候选值向量，C~ &lt; t &gt;，它可以被添加到状态中。它也是一个张量。</li><li id="61d2" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">将这两个张量相乘，并创建状态更新。</li><li id="3ae3" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">此更新将被添加到<strong class="kw ja">单元状态</strong>。</li></ul><p id="baa9" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">在我们之前的语言模型示例中，它想要将句子的主题添加到<strong class="kw ja">单元格状态</strong>。</p><p id="5d5f" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated"><strong class="kw ja">输出门架构</strong></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi ne"><img src="../Images/03dcf202d0c47fbe43373dd68bfc8cca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ozW8tQHzAQxaFfISiSZeQQ.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk translated"><strong class="bd jy"> <em class="nv">输出门架构</em> </strong></figcaption></figure><p id="7288" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated"><strong class="kw ja"> <em class="na">输出门</em> </strong>决定要输出什么。该输出将基于<strong class="kw ja">单元状态</strong>，但将是过滤后的版本。</p><ul class=""><li id="75ea" class="mj mk iq kw b kx me lb mf lf ml lj mm ln mn lr mo mp mq mr bi translated">决定<strong class="kw ja">单元状态</strong>将要输出的部分的sigmoid层。</li><li id="e3aa" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">通过<strong class="kw ja"> tanh </strong>层传递<strong class="kw ja">单元状态</strong>(将值推到1和1之间)。</li><li id="5b29" class="mj mk iq kw b kx ms lb mt lf mu lj mv ln mw lr mo mp mq mr bi translated">将它乘以sigmoid门的输出，以便它只输出它决定的部分。</li></ul><p id="1fe3" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated">在我们前面的语言模型的例子中，由于它只看到一个主题，它可能想要输出与一个动词相关的信息。例如，它可能输出主语是单数还是复数，以便知道接下来应该预测动词的什么形式。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nw"><img src="../Images/382f01c5103c6a8b8330bddd632b883d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SauNvBp7uwVCIlb7TDLhJA.png"/></div></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk translated"><strong class="bd jy">展开图<em class="nv">一个LSTM单位穿越时间</em>T21</strong></figcaption></figure><p id="bb56" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated"><strong class="kw ja">伴随jupyter本帖的笔记本可以在</strong><a class="ae ls" href="https://github.com/nitwmanish/Demystifying-Architecture-Of-Long-Short-Term-Memory-LSTM" rel="noopener ugc nofollow" target="_blank"><strong class="kw ja">Github</strong></a><strong class="kw ja">上找到。</strong></p><h1 id="33c4" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">结论</h1><p id="75ad" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="kw ja">rnn</strong>用于处理顺序数据。但是<strong class="kw ja"> RNNs </strong>遭遇两大问题<strong class="kw ja">爆炸渐变</strong>和<strong class="kw ja">消失渐变</strong>。<strong class="kw ja"> RNNs </strong>忘记<strong class="kw ja">长期依赖</strong>。<strong class="kw ja"> LSTMs </strong>是<strong class="kw ja"> RNNs </strong>的变体，能够学习<strong class="kw ja">长期依赖性</strong>。</p><p id="b1a4" class="pw-post-body-paragraph ku kv iq kw b kx me kz la lb mf ld le lf mg lh li lj mh ll lm ln mi lp lq lr ij bi translated"><strong class="kw ja"> <em class="na">我希望这篇文章能帮助你理解</em> LSTMs，以及它如何能够学习长期依赖关系。它也很好地<em class="na">解释了lstm的关键组件</em>，以及为什么我们使用lstm来处理爆炸渐变和消失渐变问题。</strong></p><h1 id="17c9" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">参考</h1><p id="1591" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">克里斯多夫·奥拉赫，了解LSTM网络公司。<a class="ae ls" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p></div></div>    
</body>
</html>