<html>
<head>
<title>Fully Explained K-Nearest Neighbors with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python完整解释K近邻</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fully-explained-k-nearest-neighbors-with-python-ebbe27f93ba9?source=collection_archive---------2-----------------------#2021-02-19">https://pub.towardsai.net/fully-explained-k-nearest-neighbors-with-python-ebbe27f93ba9?source=collection_archive---------2-----------------------#2021-02-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="5226" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="4057" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">数据科学中解决真实案例的机器学习分类算法研究。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ccce26c4db7d7df9b2768ad72236b21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tZs4MsP4hog0rGzGMfyb7w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">KNN的示例图。作者的照片</figcaption></figure><p id="1497" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">大家好，该系列的另一篇文章全面解释了机器学习算法。在本文中，我们将讨论k近邻分类问题。一篇好的文章就像故事的流程，读者可以在很短的时间内获得尽可能多的信息。</p><p id="5eef" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们澄清一些观点</p><ul class=""><li id="d2fe" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">在无监督的情况下，最近邻是许多学习技术如聚类的基础。</li><li id="0f15" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">在监督学习的情况下，它分为两类:分类和回归。</li></ul><p id="ddca" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，我们将讨论监督分类问题学习技术。</p><p id="d3a2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">主要目标是根据数据点附近的样本预测新的数据点。这些采样点可以基于k值或半径。k值由用户定义，半径基于数据点的密度。</p><p id="790a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">测量新点与其附近样本点之间的距离是基于欧几里德距离或曼哈顿距离，前者是最常用的。嗯，有很多距离度量，用来度量距离，像切比雪夫，闵可夫斯基在向量空间里用实值。</p><p id="250b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">还有许多其他距离度量，如下所示:</p><ul class=""><li id="e037" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated"><strong class="lj jd">哈弗辛:</strong>输入输出上有弧度值时使用。</li><li id="7648" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><strong class="lj jd">汉明:</strong>用于计算二进制字符串中的等长。</li><li id="cabf" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><strong class="lj jd"> Jaccard: </strong>基于两组之间的相似点。</li></ul><p id="d924" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">谈论不同的距离，因为它们在分类和聚类的许多学习算法中起着重要作用。一个好的距离度量总是可以加快性能优化的速度。</p><div class="mr ms gp gr mt mu"><a rel="noopener  ugc nofollow" target="_blank" href="/become-a-data-scientist-in-2021-with-these-following-steps-5bf70a0fe0a1"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jd gy z fp mz fr fs na fu fw jc bi translated">按照以下步骤，在2021年成为一名数据科学家</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">走上数据科学家之路需要具备的基本点</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">pub.towardsai.net</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni lb mu"/></div></div></a></div><p id="0972" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该算法在机器学习中采用某种类型的搜索，如下所示:</p><ul class=""><li id="c914" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated"><strong class="lj jd">强力搜索:</strong>用于快速搜索最近邻。这是一个很好的小数据集，当数据集增长时不可行。</li><li id="ac4b" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><strong class="lj jd"> K-D树:</strong>说到克服蛮力问题。这是基于树形结构的，距离搜索是基于关联的。如果X离Y远，Z离Y近，那么Z离X也远，所以，在这个树中进行快速搜索优化。但是当相邻点在极限值之后增长时，也是不可行的。</li><li id="2ced" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><strong class="lj jd">球树:</strong>它的开发是为了克服KD树中的更多邻居维度问题。它构建了一个嵌套的超球面几何，在高维空间中非常有用。</li></ul><p id="d6f9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在k近邻回归的情况下，数据应该是连续的形式。权重参数被用来分配为均匀或距离，以形成预测线。</p><p id="558c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">选择正确的K值也很重要，它可以通过误差率图来选择，误差较小的值将是优化的K值。</p><div class="mr ms gp gr mt mu"><a rel="noopener  ugc nofollow" target="_blank" href="/regression-and-classification-metrics-in-machine-learning-with-python-6d9fcd8b73aa"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jd gy z fp mz fr fs na fu fw jc bi translated">Python机器学习中的回归和分类度量</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">使用度量API进行回归和分类的模型评估</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">pub.towardsai.net</p></div></div><div class="nd l"><div class="nj l nf ng nh nd ni lb mu"/></div></div></a></div><p id="cabd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用python实现KNN算法。</p><p id="6d5f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">导入库</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="0a16" class="np nq it nl b gy nr ns l nt nu">import numpy as np <br/>import pandas as pd <br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline</span></pre><p id="ebcf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，在熊猫的read方法的帮助下，读取数据集的CSV文件。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="0bb9" class="np nq it nl b gy nr ns l nt nu">dataset = pd.read_csv('classified_data.csv')</span></pre><p id="bb52" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">查看前五行</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="efd0" class="np nq it nl b gy nr ns l nt nu">dataset.head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/c4d44ff7e774fc5cfe0f5c56d10db7ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSEtfG29OvAfHQSVxXh6AA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">数据集的视图。作者的照片</figcaption></figure><p id="664c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">删除建模中无用的未命名列。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="c73d" class="np nq it nl b gy nr ns l nt nu">dataset = dataset.drop("Unnamed: 0", axis=  1)</span></pre><p id="9e13" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过描述方法了解数据集的统计信息。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="fbe5" class="np nq it nl b gy nr ns l nt nu">dataset.describe()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/eadd44ba6ea8376d101138536bcb1fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HY3Wjm3hAC66dnJqYpDMag.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">数据集的统计信息。作者的照片</figcaption></figure><p id="9de9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在对模型建模之前，由于低值和高值的变化，我们需要对数据进行缩放。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="d7d0" class="np nq it nl b gy nr ns l nt nu">from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()</span></pre><p id="2da7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">删除asix=1的目标变量作为列和独立特征。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="c0f5" class="np nq it nl b gy nr ns l nt nu">#fitting the features<br/>scaler.fit(dataset.drop('TARGET CLASS',axis=1))</span><span id="b2bb" class="np nq it nl b gy nx ns l nt nu">#tranform the data<br/>scaled_features = scaler.transform(dataset.drop('TARGET CLASS',axis=1))</span></pre><p id="35a8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，制作一个变量来保存其中所有的独立特性。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="507c" class="np nq it nl b gy nr ns l nt nu">df_feat = pd.DataFrame(scaled_features,columns=dataset.columns[:-1])</span></pre><p id="6820" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们将数据分为训练集和测试集。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="1501" class="np nq it nl b gy nr ns l nt nu">from sklearn.model_selection import train_test_split</span><span id="6f47" class="np nq it nl b gy nx ns l nt nu">#drop trget class from frature<br/>X = df_feat  <br/>y = df['TARGET CLASS']</span><span id="ded3" class="np nq it nl b gy nx ns l nt nu">#splitting in train and test set<br/>X_train,X_test,y_train,y_test =<br/>            train_test_split(X,y,test_size=0.3,random_state=42)</span></pre><p id="364c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在通过检查误差率图来选择K的合适值。</p><p id="d458" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">肘法得到k的最佳值。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="2a2a" class="np nq it nl b gy nr ns l nt nu">error_rate = []</span><span id="8a07" class="np nq it nl b gy nx ns l nt nu">for i in range(1,20):<br/>    knn = KNeighborsClassifier(n_neighbors=i)<br/>    knn.fit(X_train, y_train)<br/>    y_pred_i = knn.predict(X_test)<br/>    error_rate.append(np.mean(y_pred_i != y_test))</span></pre><p id="fa4e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在绘制图表</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="b6ea" class="np nq it nl b gy nr ns l nt nu">plt.figure(figsize=(10,5))<br/>plt.plot(range(1,20),error_rate,color='blue',ls='--' <br/>              ,marker='o',markerfacecolor='red', markersize=10)<br/>plt.title('Error Rate vs K Value')<br/>plt.xlabel('K')<br/>plt.ylabel('Error Rate')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/9be6c957930f1a543f7d5a72bd2168be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*Ce081wsR5049K0uVcs_GVg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">错误率图表。作者的照片</figcaption></figure><p id="d3d8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">分析图表后，最佳K值似乎是11。因此，对于我们的n_neighbors参数，我们将使用这个值。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="321b" class="np nq it nl b gy nr ns l nt nu">#import knn library<br/>from sklearn.neighbors import KNeighborsClassifier</span><span id="eedb" class="np nq it nl b gy nx ns l nt nu">knn = KNeighborsClassifier(n_neighbors=11)</span><span id="40c8" class="np nq it nl b gy nx ns l nt nu">#fit the model<br/>knn.fit(X_train, y_train)</span><span id="bbcf" class="np nq it nl b gy nx ns l nt nu">#output:<br/>KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',metric_params=None, n_jobs=None, n_neighbors=11, p=2,weights='uniform')</span><span id="2d29" class="np nq it nl b gy nx ns l nt nu">#predict the model<br/>y_pred = knn.predict(X_test)</span></pre><p id="d0e7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们观察KNN分类器的参数，使用的度量是Minkowski，P值是2，这意味着用于测量点的距离是欧几里得距离。</p><p id="2131" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在打印混淆度量和分类报告。</p><pre class="ks kt ku kv gt nk nl nm nn aw no bi"><span id="16ec" class="np nq it nl b gy nr ns l nt nu">print(classification_report(y_test,y_pred))<br/>print(confusion_matrix(y_test,y_pred))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/af1339b0303b2dcc589884c9da06867c.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*DDUWwozRYDspxC3ioJOG-w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">分类报告和混淆矩阵。作者的照片</figcaption></figure><p id="f4de" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，对正确的目标列进行分类的准确率接近95%。</p><blockquote class="oa ob oc"><p id="fd5b" class="lh li od lj b lk ll kd lm ln lo kg lp oe lr ls lt of lv lw lx og lz ma mb mc im bi translated"><strong class="lj jd"> <em class="it">结论:</em> </strong></p></blockquote><p id="7e4f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在用误差图计算k值之后，我们实现了该模型。要了解使用和不使用图表的区别，我们可以将k值设为“1 ”,也可以在不缩放要素的情况下检查精度。</p><p id="01ef" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">KNN适用于小数据集，并且总是尝试使用不同的度量和算法参数来观察准确性，以获得更好的性能。</p><p id="548d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae oh" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae oh" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="115c" class="oi nq it bd oj ok ol om on oo op oq or ki os kj ot kl ou km ov ko ow kp ox oy bi translated">推荐文章</h1><ol class=""><li id="8478" class="md me it lj b lk oz ln pa lq pb lu pc ly pd mc pe mj mk ml bi translated"><a class="ae oh" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> NLP —用Python从零到英雄</a></li></ol><p id="ce80" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">2.<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a></p><p id="f4aa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">3.<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/python-zero-to-hero-with-examples-c7a5dedb968b?source=friends_link&amp;sk=186aff630c2241aca16522241333e3e0" rel="noopener"> Python:零到英雄附实例</a></p><p id="28a1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">4.<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/fully-explained-svm-classification-with-python-eda124997bcd?source=friends_link&amp;sk=da300d557992d67808746ee706269b2f" rel="noopener">用Python全面讲解SVM分类</a></p><p id="193e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">5.<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a?source=friends_link&amp;sk=9c5c613ceb10f2d203712634f3b6fb28" rel="noopener">用Python全面解释K-means聚类</a></p><p id="9d76" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">6.<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python全面解释线性回归</a></p><p id="b393" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">7.<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python全面解释逻辑回归</a></p><p id="2a8e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">8.<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/basic-of-time-series-with-python-a2f7cb451a76?source=friends_link&amp;sk=09d77be2d6b8779973e41ab54ebcf6c5" rel="noopener">Python时间序列基础</a></p><p id="1268" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">9.<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/numpy-zero-to-hero-with-python-d135f57d6082?source=friends_link&amp;sk=45c0921423cdcca2f5772f5a5c1568f1" rel="noopener"> NumPy:用Python零到英雄</a></p><p id="04b9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">10.<a class="ae oh" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>