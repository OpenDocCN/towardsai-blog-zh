<html>
<head>
<title>How To Use Rasa To Build A Bot That Understands Bahasa Melayu</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用Rasa来构建一个理解巴哈萨梅拉尤语的机器人</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-use-rasa-to-build-a-bot-that-understands-bahasa-melayu-ab97bcb6f546?source=collection_archive---------1-----------------------#2021-08-31">https://pub.towardsai.net/how-to-use-rasa-to-build-a-bot-that-understands-bahasa-melayu-ab97bcb6f546?source=collection_archive---------1-----------------------#2021-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2087" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="a9b0" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">关于扩展LanguageModelFeaturizer组件以使用其他预训练模型的教程</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a946da2466dfb35726e90cc7cd073fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0cyxhSK6NCjKNPkr94rsw.png"/></div></div></figure><h1 id="0aa7" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">介绍</h1><p id="7b63" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">在本文中，我将解释如何扩展Rasa的<a class="ae mr" href="https://rasa.com/docs/rasa/components#languagemodelfeaturizer" rel="noopener ugc nofollow" target="_blank">语言模型特征器</a>组件，以使用HuggingFace的<a class="ae mr" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">模型库</a>上的其他模型。</p><p id="5864" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">我假设读者熟悉使用Rasa构建聊天机器人。如果不是这样，那么<a class="ae mr" href="https://www.youtube.com/playlist?list=PL75e0qA87dlEjGAc9j9v3a5h1mxI2Z9fi" rel="noopener ugc nofollow" target="_blank">在YouTube上观看这些系列视频</a>来快速跟上进度。</p><p id="5521" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">重现本文中描述的结果的代码可以在<a class="ae mr" href="https://github.com/hsm207/rasa_moodbot/tree/melayu" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="0204" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">问题陈述</h1><p id="5414" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">假设你的任务是构建一个聊天机器人，它将用马来语与用户互动。</p><p id="35b2" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">因为你刚刚开始，你没有很多现实世界的对话来训练<a class="ae mr" href="https://rasa.com/docs/rasa/glossary#nlu" rel="noopener ugc nofollow" target="_blank"> NLU模型</a>。然而，有很多用马来语进行的对话(如社交媒体)。我们如何利用这些对话来改进NLU模式？</p><h1 id="1b35" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">解决办法</h1><p id="6f57" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">一个解决方案是在马来语语料库上训练一个语言模型，该语言模型近似于你期望你的用户对你的机器人说出的话语。然后，您可以将语言模型视为一个特征化器，并将这些特征输入到<a class="ae mr" href="https://rasa.com/docs/rasa/components#dietclassifier" rel="noopener ugc nofollow" target="_blank">饮食分类器</a>中进行进一步的“微调”。</p><p id="da36" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">如果语言模型是使用Hugging Face的<a class="ae mr" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> transformers </a>库构建的，那么它可以使用<a class="ae mr" href="https://rasa.com/docs/rasa/components#languagemodelfeaturizer" rel="noopener ugc nofollow" target="_blank">LanguageModelFeaturizer</a>组件集成到NLU模型的预测管道中。</p><p id="5346" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">本文的其余部分假设我们有兴趣使用<a class="ae mr" href="https://huggingface.co/StevenLimcorn/MelayuBERT" rel="noopener ugc nofollow" target="_blank"> Melayu BERT </a>模型作为特征。</p><h1 id="8943" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">扩展或重用LanguageModelFeaturizer组件？</h1><p id="fa9a" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">因为Melayu BERT基于BERT模型，所以使用LanguageModelFeaturizer组件的合理方法是这样配置它:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/9fb7d4ea9d3bda9931e4c07f1b89a996.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*rCQpUbiTQgd7Zf1VLEtU3A.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk translated">图1:配置LanguageModelFeaturizer以使用Melayu BERT</figcaption></figure><p id="0d70" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">尽管培训运行成功，但日志显示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/603b28bc46d6a530f9086a991c0f5c28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-vcv3K0YM0IP5zVjgHMnw.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk translated">图2: Melayu BERT与BERT不“兼容”</figcaption></figure><p id="c76a" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">这是因为<code class="fe nd ne nf ng b">model_name: "bert"</code>参数对应于<a class="ae mr" href="https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel" rel="noopener ugc nofollow" target="_blank"> TFBertModel </a>架构，而Melayu BERT模型是使用<a class="ae mr" href="https://huggingface.co/transformers/model_doc/bert.html#transformers.TFBertForMaskedLM" rel="noopener ugc nofollow" target="_blank"> TFBertForMaskedLM </a>架构实现的。</p><p id="6e7c" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">更谨慎的方法是子类化<code class="fe nd ne nf ng b">LanguageModelFeaturizer</code>来定义如何从基于TFBertForMaskedLM架构的模型中提取特征。这有助于避免由于模型缺少关键层而意外引入错误。</p><h1 id="e0da" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">实施细节</h1><p id="edea" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">TFBertForMaskedLM模型与TFBertModel有许多相似之处。这意味着我们可以重用<code class="fe nd ne nf ng b">LanguageModelFeaturizer</code>类中的大多数现有方法。</p><p id="110c" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">这些是我们需要覆盖的唯一方法:</p><ol class=""><li id="d7b9" class="nh ni it lx b ly ms mb mt me nj mi nk mm nl mq nm nn no np bi translated"><code class="fe nd ne nf ng b">_load_model_metadata</code></li><li id="7e87" class="nh ni it lx b ly nq mb nr me ns mi nt mm nu mq nm nn no np bi translated"><code class="fe nd ne nf ng b">_load_model_instance</code></li><li id="c4a3" class="nh ni it lx b ly nq mb nr me ns mi nt mm nu mq nm nn no np bi translated"><code class="fe nd ne nf ng b">_add_lm_specifc_special_tokens</code></li><li id="c999" class="nh ni it lx b ly nq mb nr me ns mi nt mm nu mq nm nn no np bi translated"><code class="fe nd ne nf ng b">_lm_specific_token_cleanup</code></li><li id="f3b7" class="nh ni it lx b ly nq mb nr me ns mi nt mm nu mq nm nn no np bi translated"><code class="fe nd ne nf ng b">_compute_batch_sequence_features</code></li><li id="1675" class="nh ni it lx b ly nq mb nr me ns mi nt mm nu mq nm nn no np bi translated"><code class="fe nd ne nf ng b">_post_process_sequence_embeddings</code></li></ol><p id="c3b3" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">我们将创建一个名为<code class="fe nd ne nf ng b">CustomLanguageModelFeaturizer</code>的类来覆盖上面的方法。这个类将在名为<code class="fe nd ne nf ng b">addons</code>的文件夹中名为<code class="fe nd ne nf ng b">custom_lm_featurizer.py</code>的模块中定义。</p><h2 id="8a36" class="nv le it bd lf nw nx dn lj ny nz dp ln me oa ob lp mi oc od lr mm oe of lt iz bi translated">步骤1:如何加载模型的元数据</h2><p id="8f2e" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated"><code class="fe nd ne nf ng b">_load_model_metadata</code>控制如何处理<code class="fe nd ne nf ng b">LanguageModelFeaturizer</code>中的配置。</p><p id="9948" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">假设我们希望组件失败，如果<code class="fe nd ne nf ng b">model_name</code>不是<code class="fe nd ne nf ng b">StevenLimcorn/MelayuBERT</code>并且模型的权重没有被下载。这是该方法的实现方式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/47533cc65fed737a4feac0f758cd22f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*ji_RVRBf8pSncFPiVuwKCw.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk translated">图3:如何处理组件的配置参数</figcaption></figure><h2 id="3ef0" class="nv le it bd lf nw nx dn lj ny nz dp ln me oa ob lp mi oc od lr mm oe of lt iz bi translated">步骤2:如何加载模型</h2><p id="05bc" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated"><code class="fe nd ne nf ng b">_load_model_instance</code>定义组件记号赋予器、模型和填充记号。这很容易实现:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3b6985e7ec16b04be0dcff4e1a93988d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*e1qY-ykWl4DSbS9YU9fv4w.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk translated">图4:定义组件的标记器、模型和填充标记</figcaption></figure><h2 id="7676" class="nv le it bd lf nw nx dn lj ny nz dp ln me oa ob lp mi oc od lr mm oe of lt iz bi translated">步骤3:添加语言模型特定的标记</h2><p id="ec33" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们知道，伯特模型需要添加[CLS]和[SEP]标记作为其输入的一部分。这部分由<code class="fe nd ne nf ng b">_add_lm_specific_special_tokens</code>方法处理:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/712a397453cc285fb9813fdefbe53c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*E475lXSN2To3xlYq57yb1Q.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk translated">图5:添加[CLS]和[SEP]标记</figcaption></figure><h2 id="da65" class="nv le it bd lf nw nx dn lj ny nz dp ln me oa ob lp mi oc od lr mm oe of lt iz bi translated">步骤4:清理令牌</h2><p id="05b9" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们还知道，BERT的tokenize有时会将一个单词分解成多个单词。例如，在<a class="ae mr" href="https://huggingface.co/bert-base-uncased" rel="noopener ugc nofollow" target="_blank"> bert-base-uncased </a>模型中，单词“草莓”将被标记为“稻草”和“# #浆果”。</p><p id="05a4" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated"><code class="fe nd ne nf ng b">_lm_specific_token_cleanup</code>该方法将删除“##”，这样我们就有了更具可读性的内容，以防我们想要将令牌缝合在一起，以便在下游进行进一步处理:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/9fd23a409efe1250c8a83d3c280aeb58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*-YYFOX8pse1KsRNexiIBnw.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk translated">图6:清理单词块标记化的令牌</figcaption></figure><h2 id="df7b" class="nv le it bd lf nw nx dn lj ny nz dp ln me oa ob lp mi oc od lr mm oe of lt iz bi translated">第五步:如何提取特征</h2><p id="d6e8" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">我们希望DIET分类器使用Melayu BERT的最后隐藏状态作为一个特征来执行意图分类和/或实体提取。我们在<code class="fe nd ne nf ng b">_compute_batch_sequence_features</code>方法中定义了这种行为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/278472d2835136ec72f52f9d25820bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rzxa67KnjCsZjxtdghbMmw.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk translated">图7:定义使用Melayu BERT中的哪些特性</figcaption></figure><h2 id="6ed7" class="nv le it bd lf nw nx dn lj ny nz dp ln me oa ob lp mi oc od lr mm oe of lt iz bi translated">步骤6:如何使用这些功能</h2><p id="8b63" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">一旦我们从Melayu BERT中提取了特征，我们需要告诉DIET Classifier如何使用它进行意图分类和实体提取。这是在<code class="fe nd ne nf ng b">post_process_sequence_embeddings</code>中完成的:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi gj"><img src="../Images/7ef3078e638a9f565426762785d5a4ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4n8Tt6BbGZtTzDP8CkVnOA.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk translated">图8:如何使用特征进行意图分类(句子嵌入)和实体提取(后期处理序列嵌入)</figcaption></figure><p id="8dc5" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">给定来自Melayu BERT的最后一个隐藏状态，用于意图分类和实体提取的特征是[CLS]令牌的矢量表示。用于实体提取的特性是除了[CLS]和[SEP]标记之外的所有特性。这个逻辑与TFBertModel 的预定义<a class="ae mr" href="https://github.com/RasaHQ/rasa/blob/0d97d427fd342b3f0fab4eaabc3d5169249dbf61/rasa/nlu/utils/hugging_face/transformers_pre_post_processors.py#L116-L134" rel="noopener ugc nofollow" target="_blank">后处理器相同，这也是我们决定重用它的原因(见第85行)。</a></p><h1 id="74c1" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">使用</h1><p id="1ff4" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">下面的代码片段显示了如何将<code class="fe nd ne nf ng b">CustomLanguageModelFeaturizer</code>组件作为NLU管道的一部分:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/76934f23e1c6f4a51a956b4e67eb28e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*ZgjHA3m3ff0NiWDhXrwdhg.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk translated">图9:配置<code class="fe nd ne nf ng b">CustomLanguageModelFeaturizer component</code></figcaption></figure><p id="2bb2" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">图9假设模型工件已经下载到一个名为<code class="fe nd ne nf ng b">.cache</code>的文件夹中。</p><h1 id="988e" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">结论</h1><p id="4499" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">本文描述了一种使用自定义LanguageModelFeaturizer组件的方法，以将TFBertForMaskedLM模型用作Rasa中的特征。类似的方法可以适用于使用transformers库创建的任何模型。</p><p id="63fd" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">如果你有任何问题，请在评论中告诉我。</p></div></div>    
</body>
</html>