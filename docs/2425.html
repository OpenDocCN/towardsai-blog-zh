<html>
<head>
<title>Basic Intuition And Guide to Neural Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经类型转移的基本直觉和指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/basic-intuition-on-neural-style-transfer-idea-c5ac179d1530?source=collection_archive---------0-----------------------#2021-12-16">https://pub.towardsai.net/basic-intuition-on-neural-style-transfer-idea-c5ac179d1530?source=collection_archive---------0-----------------------#2021-12-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="db9e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><p id="8136" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">用PyTorch简单解释神经类型转换的思想和实现。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ku"><img src="../Images/5af1496e13c42207463ed0e8a4412907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NboieStzDxzE7v6v"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">图片来自<a class="ae lk" href="https://unsplash.com/photos/wKlHsooRVbg" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><h1 id="33ac" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">介绍</h1><p id="86be" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">神经风格转移，简称NST，是一个有趣的想法，其中神经网络学习转移风格，即它学习如何绘画并生成具有独特绘画的新图像。</p><p id="11e8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">风格转移的概念是将一种风格或纹理转移到输入图像上，用深度神经网络实现风格转移图像的方法称为神经风格转移。这意味着NST是一种技术，它采用两个图像—一个内容图像和一个样式参考图像(可以是著名画家的作品或新纹理)—并将它们混合在一起，以便输出看起来像内容图像，但以参考图像的样式“绘制”。下图描述了NST的输出。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/de7e984006a09804b1b7e7a625785354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/0*z2s9phaCWbMmCfKF"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">神经类型转移结果</figcaption></figure><h2 id="7b6b" class="mp lm iq bd ln mq mr dn lr ms mt dp lv kh mu mv lz kl mw mx md kp my mz mh iw bi translated">让我们深入研究一下！</h2><p id="fad9" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">实际上，它与神经网络的工作方式略有不同。我们都知道，在神经网络中，当图像作为输入时，权重被更新并反向传播，例如，在CNN进行图像识别的情况下，网络试图学习如何识别它。</p><p id="87a3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">然而，在NST中，我们更新图像的像素(结果图像的待更新权重)和预训练网络的冻结权重。我们根据损失不断更新最终图像像素，直到它是期望的输出，即最小损失。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi na"><img src="../Images/6905d7ae46ce54a1add64013272f6b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/0*OwXjMw5_jlj3r5JA"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">更新图像像素并冻结预先训练的网络权重</figcaption></figure><p id="24cc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">预训练网络(VGG，雷斯网)用于从内容图像和风格图像中提取特征，并且某种机制用于更新风格转移图像的像素，即最终输出。</p><p id="f4a9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这就是为什么只有单一风格的图像被转移到相应的单一风格的图像，而不是数据集使用。其实风格和内容形象是一一对应的。</p><h1 id="fd2c" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">组件:</h1><ul class=""><li id="fa29" class="nb nc iq jy b jz mj kd mk kh nd kl ne kp nf kt ng nh ni nj bi translated">预先训练美国有线电视新闻网像VGG，雷斯内特。</li></ul><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi nk"><img src="../Images/4596e907daebadd4012255619364e1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HLcs6fvwLFTnCtg3"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">VGG建筑</figcaption></figure><ul class=""><li id="ff93" class="nb nc iq jy b jz ka kd ke kh nl kl nm kp nn kt ng nh ni nj bi translated"><strong class="jy ja">内容图片</strong>:我们要传递风格的图片。</li></ul><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi no"><img src="../Images/13a9317188597e5d7120c42b4ead8fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/0*pkIBD4dpWVRp6ItQ"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">内容图像</figcaption></figure><ul class=""><li id="094e" class="nb nc iq jy b jz ka kd ke kh nl kl nm kp nn kt ng nh ni nj bi translated"><strong class="jy ja">样式图像</strong>:要转移样式的图像。</li></ul><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ku"><img src="../Images/17c313c0877dcb45f6caad87c8a2cefb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YJFEiyXLutNDFS1x"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">风格图像</figcaption></figure><ul class=""><li id="83e6" class="nb nc iq jy b jz ka kd ke kh nl kl nm kp nn kt ng nh ni nj bi translated"><strong class="jy ja">生成的图像</strong>:包含最终结果(像素即权重)的图像</li></ul><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2c1b194e1e35823cc03a694af30bd41c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*rYwChLLhDloZPJ3T"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">生成的图像</figcaption></figure><h1 id="ef8f" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">如何运作？</strong></h1><p id="30d8" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">我们有三个预先训练好的比如说VGG(固定权重)网络，分别用于每个内容图像、风格图像和生成的图像。主要目标是在生成的图像中保持内容和样式图像的细节。</p><ol class=""><li id="f1c2" class="nb nc iq jy b jz ka kd ke kh nl kl nm kp nn kt nq nh ni nj bi translated">将内容图像详细信息复制到新图像中，以最小化内容图像的内容距离</li><li id="2787" class="nb nc iq jy b jz nr kd ns kh nt kl nu kp nv kt nq nh ni nj bi translated">将样式图像详细信息复制到新图像中，以便最大限度地缩短样式与样式图像之间的距离。</li><li id="6fdd" class="nb nc iq jy b jz nr kd ns kh nt kl nu kp nv kt nq nh ni nj bi translated">最小化距离意味着最小化图像到生成图像之间的损失。</li></ol><h1 id="7165" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">涉及的重要步骤:</h1><ol class=""><li id="3539" class="nb nc iq jy b jz mj kd mk kh nd kl ne kp nf kt nq nh ni nj bi translated">针对每种内容、样式和生成的图像，使用预先训练好的模型(如VGG、ResNet)来计算特征。</li><li id="4705" class="nb nc iq jy b jz nr kd ns kh nt kl nu kp nv kt nq nh ni nj bi translated">计算内容损失和风格损失。</li><li id="f83a" class="nb nc iq jy b jz nr kd ns kh nt kl nu kp nv kt nq nh ni nj bi translated">计算总综合损失。</li><li id="feb1" class="nb nc iq jy b jz nr kd ns kh nt kl nu kp nv kt nq nh ni nj bi translated">反向传播梯度以更新生成的图像权重像素，而预先训练的模型权重被冻结。</li></ol><p id="c429" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">第一步:</strong></p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="5fd5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">第二步:计算内容和风格损失</strong></p><p id="2a60" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">内容丢失:</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ny"><img src="../Images/c4b719091e465bdc4d2e9419029ce2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*anKKgof7NIjsnYPo"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">内容损失</figcaption></figure><p id="f831" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们可以通过称为内容图像和生成图像的最后特征图之间的均方误差(MSE)的损失函数将内容图像的细节复制到生成图像，该损失函数最小化了内容图像和生成图像之间的距离，即，它指示内容图像的细节正在被转移到生成图像。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/dd646e0e4bc4795761683105c4953cc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/0*bVCrPw33--8tJnqz"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">MSE:内容丢失</figcaption></figure><p id="221a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi oa translated">为什么我们只比较内容图像的最后一层特征图？</p><p id="b4bc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在深度卷积层中，不同的层学习不同的特征。第一卷积层学习诸如边缘和简单纹理的特征。后来的卷积层学习更复杂的纹理和图案等特征。最后一个卷积层学习诸如部分对象的对象的特征。因此，我们需要重要对象的内容特征来生成图像。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oj"><img src="../Images/e51769bd5af791a083b99eb2b4f67cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RAjJW-BU_z9wss2H"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">每层的五层卷积输出<a class="ae lk" href="https://christophm.github.io/interpretable-ml-book/cnn-features.html" rel="noopener ugc nofollow" target="_blank">:来源</a></figcaption></figure><p id="3ac9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">风格缺失:</strong></p><p id="ebdd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">它的主要目标是在生成的图像中加强样式图像的细节。为此，在风格图像和生成的图像之间应该有类似的激活相关性，即通过称为Gram矩阵的相关性矩阵来测量。</p><p id="6cb1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">克拉姆矩阵(风格矩阵)概念</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ok"><img src="../Images/f6e4ee8a9eb78a41bdddad977a819f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LYSlulwDzQmTuRqb"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">过程:Gram矩阵计算</figcaption></figure><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="nw nx l"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">计算Gram矩阵</figcaption></figure><p id="7b8c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">gram矩阵用于捕获称为风格矩阵的一组特征图的“特征分布”。上图描绘了那个概念。</p><p id="1259" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">gram矩阵是一种相关运算，即总结同时发生的激活的层上的特征图的点积。由于纹理(风格)具有很强的局部性，当我们捕获大量同时发生的激活时，我们捕获局部性。所测量的是，在特定的像素位置，特征#F1是否倾向于与特征#F2同时出现。</p><p id="3979" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">通过找到这个矩阵，我们得到更接近目标的编码的相关激活(我们想要捕获的样式),检索样式。此外，Gram matrix是位置不变的，它基于特征图中单个点的统计数据。</p><p id="19ae" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">样式丢失中使用的步骤</strong>:</p><ol class=""><li id="2701" class="nb nc iq jy b jz ka kd ke kh nl kl nm kp nn kt nq nh ni nj bi translated">首先，计算风格图像的层I的特征图的克矩阵和生成图像的层I的特征图的克矩阵。</li><li id="23c8" class="nb nc iq jy b jz nr kd ns kh nt kl nu kp nv kt nq nh ni nj bi translated">求风格图像的gram矩阵和内容图像的gram矩阵之间的均方误差，称为风格损失。</li><li id="823d" class="nb nc iq jy b jz nr kd ns kh nt kl nu kp nv kt nq nh ni nj bi translated">计算所有图层的风格影像特征图和生成的影像特征图的风格损失，如下图所示。</li></ol><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ol"><img src="../Images/6941b6f396f7a7f8c47af8899e1968a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SAO7SXOdrc8mjwIl"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">生成的和样式图像gram矩阵之间的MSE。</figcaption></figure><p id="1454" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">类似地，计算所有层中的样式损失，以在生成的图像中保留如上所述的每个细节(样式)。</p><p id="b437" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">为什么？</strong></p><p id="c738" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们需要将样式图像的每个细节转移到生成的图像，并需要从编码器的每个层(预训练的CNN模型)计算样式损失，其中发现不同的细节。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi om"><img src="../Images/9a3c52576fba302cb9553d68cb0ccda2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-xC9fO__YOydqhwH"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">所有图层中的样式丢失</figcaption></figure><p id="9a7a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">代号:</strong></p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="008c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">第三步:计算总的综合损失。</strong></p><p id="a934" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">最后，总损失计算为加权损失= 𝜶 x内容损失+ 𝝱 x风格损失。<strong class="jy ja"> <em class="on"> α </em> </strong>和<strong class="jy ja"> <em class="on"> β </em> </strong>用于控制生成的图像中呈现的内容和样式的数量。您还可以在<a class="ae lk" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">论文</a>中看到不同<strong class="jy ja"> <em class="on"> α </em> </strong>和<strong class="jy ja"> <em class="on"> β </em> </strong>值的不同效果的可视化效果。我们的主要工作是使用Adam这样的优化器来生成包含内容图像和样式图像细节的图像，从而最大限度地减少这种总损失。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oo"><img src="../Images/577542056ca45611d21dd8f6478165d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RCNz-O5Lu83EXH-l"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">综合全损</figcaption></figure><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="8c65" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">步骤5:反向传播梯度以更新生成的图像权重像素，而预先训练的模型权重被冻结。</strong></p><p id="1be3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">主要的事情是有一个梯度的反向传播来更新生成的图像的权重，即像素值。下图描述了该机制。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi op"><img src="../Images/13bd45437e0773d7fc30234c60791f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*esY2rPhXTl-tXIvt"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">反向传播的总损失</figcaption></figure><p id="8c89" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">代码:</strong></p><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="78f9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在对比如说10000个时代的训练中，网络学会了很好地将风格转移到生成的图像中。当您对单个图像运行上述代码时，结果如下所示:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/83ef6de2b5067afaa028a27621c58185.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*xUzQAIiuoTVjP5WrrtwyqA.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk translated">NST训练后的输出</figcaption></figure><p id="c923" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">神经风格转移就是这么来的。</p><h1 id="034d" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">结论</h1><p id="fc03" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">在本教程中，您将学习神经风格转换是如何工作的，以及如何用PyTorch实现它。神经风格转移将参考图像的内容和风格特征整合到新的艺术中。它不同于神经网络机制，在神经网络机制中，生成的图像的像素被更新，而预训练网络的权重被冻结。从每个内容、样式和生成的图像中提取第一特征。最初，生成的图像是内容图像的克隆，并且在训练时，网络试图更新生成的图像像素，包括样式图像细节。你也知道两个损失内容损失和风格损失和组合损失帮助我们实现我们想要的。组合损失被计算为内容和风格损失的权重，并且在利用梯度反向传播的训练期间，生成的图像的像素被更新。</p><p id="8973" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">本教程的代码可从<a class="ae lk" href="https://github.com/sushant097/Neural-Style-Transfer-Implementation" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><h1 id="d479" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">进一步阅读</h1><p id="2dba" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">[1] <a class="ae lk" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">原创论文:艺术风格的新算法</a></p><p id="4af4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[2] <a class="ae lk" href="https://www.tensorflow.org/tutorials/generative/style_transfer" rel="noopener ugc nofollow" target="_blank"> Tensorflow神经风格转移教程</a></p><p id="75d2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[3] <a class="ae lk" href="https://arxiv.org/pdf/1603.08155.pdf" rel="noopener ugc nofollow" target="_blank">快速神经风格转移</a></p></div></div>    
</body>
</html>