<html>
<head>
<title>Review: G-RMI — 1st Runner Up in COCO KeyPoint Detection Challenge 2016 (Human Pose Estimation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">点评:G-RMI——2016年COCO关键点检测挑战赛亚军(人体姿态估计)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/review-g-rmi-1st-runner-up-in-coco-keypoint-detection-challenge-2016-human-pose-estimation-6c8d250f62a0?source=collection_archive---------0-----------------------#2020-03-15">https://pub.towardsai.net/review-g-rmi-1st-runner-up-in-coco-keypoint-detection-challenge-2016-human-pose-estimation-6c8d250f62a0?source=collection_archive---------0-----------------------#2020-03-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7711" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">两阶段自顶向下的方法:首先，人检测。然后，关键点检测。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/a14766a4185255879e3c2787d2ef2d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*qI_DUCLOLkdIsTot6FpbyQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">两阶段方法</strong></figcaption></figure><p id="46c1" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi lo translated"><span class="l lp lq lr bm ls lt lu lv lw di">在</span>这个故事中，回顾了<strong class="ku ir"> Google Inc. </strong>的<strong class="ku ir"> G-RMI </strong>用于人物关键点检测或人体姿态估计。G-RMI、<strong class="ku ir"> Google Research和Machine Intelligence </strong>应该是团队名称而不是方法名称。</p><p id="6b34" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为了检测多人并估计姿态，提出了一种两阶段方法。</p><ul class=""><li id="f65e" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">首先，使用<a class="ae mg" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快的R-CNN </a>检测多人。</li><li id="28c5" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">然后用全卷积<a class="ae mg" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a>检测每个人的关键点。</li></ul><p id="c39d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这是<strong class="ku ir"> 2017年CVPR </strong>论文，引用超过<strong class="ku ir"> 200次</strong>。(<a class="mm mn ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----6c8d250f62a0--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="240c" class="mv mw iq bd kr mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">概述</h1><ol class=""><li id="c752" class="lx ly iq ku b kv nm ky nn lb no lf np lj nq ln nr md me mf bi translated"><strong class="ku ir">人箱检测</strong></li><li id="f762" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln nr md me mf bi translated"><strong class="ku ir">人物姿势估计</strong></li><li id="867b" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln nr md me mf bi translated"><strong class="ku ir">消融研究</strong></li><li id="0faa" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln nr md me mf bi translated"><strong class="ku ir">与SOTA方法的比较</strong></li></ol></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="bfe6" class="mv mw iq bd kr mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">1.人物箱检测</h1><ul class=""><li id="28e0" class="lx ly iq ku b kv nm ky nn lb no lf np lj nq ln mc md me mf bi translated"><a class="ae mg" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="ku ir">更快R-CNN </strong> </a> <strong class="ku ir"> </strong>被使用。</li><li id="fe67" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated"><a class="ae mg" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="ku ir"> ResNet-101 </strong> </a>用作主干。</li><li id="cd87" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">阿特鲁卷积(请参考<a class="ae mg" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank">deeplab v1&amp;deeplab v2</a>)用于具有8的步幅，而不是默认的32。</li><li id="9bd1" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">由ImageNet预先训练。</li><li id="1412" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">然后，<strong class="ku ir">仅使用COCO数据集中的人类别来训练网络，</strong>并且忽略剩余79个COCO类别的框注释。</li><li id="818e" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">不使用多尺度评估或模型集合。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d2bf87323f0b4542abcadce81bc970f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*az3ga21NyPHCIS87oe3d6Q.png"/></div></figure><ul class=""><li id="eaf5" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">在人检测之后，检测到的框将被输入到CNN用于人姿态估计，这将在下面提到。</li></ul></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="a7ce" class="mv mw iq bd kr mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">2.人姿态估计</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/641562ac12e0667e16674e8eebfc1e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*8E_W-DYo9AhYT1Z5PUPkjQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">网络目标输出(3个通道)。<strong class="bd kr">左&amp;中</strong>:左肘关键点的热图目标。<strong class="bd kr">右侧</strong>:偏移场L2量级(以灰度显示)和二维偏移矢量(红色)</figcaption></figure><ul class=""><li id="e241" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">使用组合的分类和回归方法。</li><li id="1830" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">网络<strong class="ku ir">首先分类它是否在K个关键点的每一个附近</strong>(我们称之为“热图”)。</li><li id="19e3" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated"><strong class="ku ir">然后预测一个二维局部偏移向量，以获得相应关键点位置的更精确估计</strong>。</li></ul><h2 id="c765" class="nu mw iq bd kr nv nw dn na nx ny dp ne lb nz oa ng lf ob oc ni lj od oe nk of bi translated">2.1.图像裁剪</h2><ul class=""><li id="191b" class="lx ly iq ku b kv nm ky nn lb no lf np lj nq ln mc md me mf bi translated">首先，让所有的盒子都有相同的固定纵横比。</li><li id="9a0d" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">然后，在评估期间使用等于1.25的重新标度因子，在训练期间使用介于1.0和1.5之间的随机重新标度因子(用于数据扩充)。</li><li id="34f3" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">接下来，从结果框中裁剪图像，并调整为固定的高度353和宽度257像素。(长宽比为1.37)</li></ul><h2 id="6768" class="nu mw iq bd kr nv nw dn na nx ny dp ne lb nz oa ng lf ob oc ni lj od oe nk of bi translated">2.2.热图和偏移预测</h2><ul class=""><li id="7bda" class="lx ly iq ku b kv nm ky nn lb no lf np lj nq ln mc md me mf bi translated">全卷积<a class="ae mg" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet-101 </a>用于为<strong class="ku ir">总共3个<em class="og"> K </em>输出通道</strong>产生热图(每个关键点一个通道)和偏移(每个关键点两个通道，用于x和y方向)，其中<em class="og"> K </em> = 17是关键点的数量。</li><li id="6574" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">使用ImageNet预训练的<a class="ae mg" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet-101 </a>模型，用1×1卷积替换其最后一层，得到3个<em class="og"> K </em>输出。</li><li id="5ac3" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">阿特鲁卷积(请参考<a class="ae mg" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank">deeplabv 1&amp;deeplabv 2</a>)用于8步。</li><li id="ae17" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">双线性上采样用于将网络输出放大回353×257的作物尺寸。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/dace62e11294ac4f9a5fb6e0aefd8e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*CJMw1ZKik_kNfNJq950fGQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">CNN的两个输出头</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/c393c971cd84a713de434ba0a6231ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*GHWtZW4Y9Xq2hYs213oP4Q.png"/></div></figure><ul class=""><li id="dc90" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated"><strong class="ku ir">第一个头</strong>:一个sigmoid函数，产生<strong class="ku ir">每个位置<em class="og"> xi </em>和每个关键点<em class="og"> k </em>的热图概率<em class="og"> hk </em> ( <em class="og"> xi </em> ) </strong>。</li><li id="978b" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated"><em class="og"> hk </em> ( <em class="og"> xi </em>)是点<em class="og"> xi </em>在从第<em class="og"> k </em>个关键点的位置<em class="og"> lk </em>开始半径为<em class="og"> R </em>的圆盘内的概率。如果在<em class="og"> R </em>之外，<em class="og"> hk </em> ( <em class="og"> xi </em> )=0。</li><li id="6c3e" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">相应的损失函数<em class="og"> Lh </em>分别是每个位置和关键点的逻辑损失之和。</li><li id="dadb" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">此外，在<a class="ae mg" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a>的中间层50增加了一个额外的热图预测层作为辅助损失项来加速训练过程。</li><li id="40fe" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated"><strong class="ku ir">第二个头</strong>:这是一个补偿回归头，补偿预测和地面真实补偿之间的差异。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/5fea2fa1b2bb6f074a792af3a9dcc029.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*I03J9hes4nLrwuPO5sJhlg.png"/></div></figure><ul class=""><li id="dd05" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">休伯鲁棒损失<em class="og"> H </em> ( <em class="og"> u </em>)被使用，其中<em class="og"> Fk </em> ( <em class="og"> xi </em>)是预测的2-D偏移向量。</li><li id="b625" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">最后，最终的损失函数是:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0107ed551c0cdb99165f4d6df293224e.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*BdTTXi-VJAAuCjw_cX2j_g.png"/></div></figure><ul class=""><li id="f172" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">其中<em class="og"> λh </em>和<em class="og"> λo </em>分别为4和1。</li></ul><h2 id="dd68" class="nu mw iq bd kr nv nw dn na nx ny dp ne lb nz oa ng lf ob oc ni lj od oe nk of bi translated">2.3.基于OKS的非最大抑制</h2><ul class=""><li id="97be" class="lx ly iq ku b kv nm ky nn lb no lf np lj nq ln mc md me mf bi translated">标准方法使用盒的交集(IoU)来测量重叠，并移除冗余盒。</li><li id="f884" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">现在，<strong class="ku ir"> G-RMI使用两个候选姿态检测的对象关键点相似性(OKS)来测量重叠</strong>。</li><li id="d6f6" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated"><strong class="ku ir">相对较高的IOU-NMS阈值(0.6) </strong>被用在人物框检测器的输出端，以<strong class="ku ir">过滤高度重叠的框</strong>。</li><li id="f833" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">这更适合于确定两个候选检测是对应于假阳性(同一个人的双重检测)还是真阳性(两个人彼此非常接近)。</li></ul></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="9899" class="mv mw iq bd kr mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">3.消融研究</h1><h2 id="d980" class="nu mw iq bd kr nv nw dn na nx ny dp ne lb nz oa ng lf ob oc ni lj od oe nk of bi translated">3.1.盒子检测模块</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi ol"><img src="../Images/8fdcebe541ae748dd8b15701eaebb9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SH2aAOIJsovySHgSrFvcNg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">可可关键点迷你瓦尔</strong></figcaption></figure><ul class=""><li id="bbba" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated"><strong class="ku ir"> COCO-only </strong>:仅使用COCO进行训练。</li><li id="90d8" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated"><strong class="ku ir"> COCO+Int </strong>:带有附加Flickr图片的COCO，用于训练。</li><li id="c4fb" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">测试了一种快速600x900变体，它使用小边600像素和大边900像素的输入图像</li><li id="dfde" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">还测试了使用小边800像素和大边1200像素的输入图像的精确800x1200变体。</li><li id="7ce0" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated"><strong class="ku ir">使用准确</strong> <a class="ae mg" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="ku ir">快速R-CNN </strong> </a> <strong class="ku ir"> (800x1200)盒式探测器</strong>。</li></ul><h2 id="5f0d" class="nu mw iq bd kr nv nw dn na nx ny dp ne lb nz oa ng lf ob oc ni lj od oe nk of bi translated">3.2.姿态估计模块</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi oq"><img src="../Images/b06b7966ba7be6a281752e1727a1d0a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQ4Q8-yj1GFlQgmatDcrgw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr"> COCO关键点测试开发</strong></figcaption></figure><ul class=""><li id="f062" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">较小(257x185)可获得更快的推断速度，较大(353x257)可获得更高的准确性。</li><li id="7ce1" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated"><strong class="ku ir">使用精确的</strong><a class="ae mg" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="ku ir">ResNet-101</strong></a><strong class="ku ir">(353 x257)姿态估计器</strong>，盘半径R = 25像素。</li></ul><h2 id="6209" class="nu mw iq bd kr nv nw dn na nx ny dp ne lb nz oa ng lf ob oc ni lj od oe nk of bi translated">3.3.基于OKS的非最大抑制</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e937a56a029bfb135b2aa7e1149eb688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*F1urqr2qu1IPQT9gEnOurA.png"/></div></figure><ul class=""><li id="6c6c" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">在后来的所有实验中，人箱检测器输出端的IOU-NMS阈值保持固定在0.6。</li><li id="7160" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">对于总部位于OKS的NMS来说，0.5的数值也不错。</li></ul></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="8e48" class="mv mw iq bd kr mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">4.与SOTA方法的比较</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi os"><img src="../Images/f592438134b7d09a52a3319a800e23a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gGE0q_NjPXgYusNFHpR9mA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr"> COCO关键点测试开发</strong></figcaption></figure><ul class=""><li id="374e" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">G-RMI(仅限可可)胜过CMU-Pose和面具R-CNN。</li><li id="32f5" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">G-RMI (COCO-Int)获得了更高的AP值0.685。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi ot"><img src="../Images/af1c49409173598d350ebb8f55eb732b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qf8oFCgp01Ocqp4hFWpiAg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr"> COCO关键点测试-标准</strong></figcaption></figure><ul class=""><li id="aa3d" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">再次，G-RMI(可可唯一)优于CMU姿势。</li><li id="459e" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">而G-RMI (COCO-Int)获得了0.673的甚至更高的AP。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi ou"><img src="../Images/b7e4a4bf1edf75af45e3107799a5acf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WePnQhGr11QY9KduKuIMpw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated"><strong class="bd kr">定性结果</strong></figcaption></figure><ul class=""><li id="6ff1" class="lx ly iq ku b kv kw ky kz lb lz lf ma lj mb ln mc md me mf bi translated">非常杂乱的场景(第三排，最右边和最后一排，右边)</li><li id="d37b" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">闭塞(最后一排，左)和幻觉闭塞的关节。</li><li id="244d" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">一些假阳性检测实际上是正确的，因为它们代表人(第一排，中间)或玩具(第四排，中间)的照片。</li><li id="f3d2" class="lx ly iq ku b kv mh ky mi lb mj lf mk lj ml ln mc md me mf bi translated">我希望将来我能回顾CMU-Pose/OpenPose和面具R-CNN。</li></ul></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h2 id="9483" class="nu mw iq bd kr nv nw dn na nx ny dp ne lb nz oa ng lf ob oc ni lj od oe nk of bi translated">参考</h2><p id="e5b5" class="pw-post-body-paragraph ks kt iq ku b kv nm jr kx ky nn ju la lb ov ld le lf ow lh li lj ox ll lm ln ij bi translated">【2017 CVPR】【G-RMI】<br/><a class="ae mg" href="https://arxiv.org/abs/1701.01779" rel="noopener ugc nofollow" target="_blank">走向野外精确多人姿态估计</a></p><h1 id="0a2d" class="mv mw iq bd kr mx oy mz na nb oz nd ne jw pa jx ng jz pb ka ni kc pc kd nk nl bi translated">我以前对人体姿态估计的评论</h1><p id="473b" class="pw-post-body-paragraph ks kt iq ku b kv nm jr kx ky nn ju la lb ov ld le lf ow lh li lj ox ll lm ln ij bi translated"><strong class="ku ir">人体姿态估计</strong><a class="ae mg" href="https://towardsdatascience.com/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------" rel="noopener" target="_blank">deep Pose</a>】<a class="ae mg" href="https://towardsdatascience.com/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=post_page---------------------------" rel="noopener" target="_blank">汤普逊NIPS ' 14</a>】<a class="ae mg" href="https://towardsdatascience.com/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------" rel="noopener" target="_blank">汤普逊CVPR ' 15</a><a class="ae mg" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener">CPM</a><a class="ae mg" href="https://medium.com/@sh.tsang/review-fcgn-fully-convolutional-google-net-human-pose-estimation-52022a359cb3" rel="noopener">FCGN</a><a class="ae mg" href="https://medium.com/towards-artificial-intelligence/review-ief-iterative-error-feedback-human-pose-estimation-a56add160fa5" rel="noopener">IEF</a>]<a class="ae mg" href="https://medium.com/@sh.tsang/review-deepcut-deepercut-multi-person-pose-estimation-human-pose-estimation-da5b469cbbc3" rel="noopener">deep cut&amp;DeeperCut</a><a class="ae mg" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank">纽维尔ECCV'16 &amp;纽维尔POCV ' 16</a><a class="ae mg" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank"/></p><h2 id="0353" class="nu mw iq bd kr nv nw dn na nx ny dp ne lb nz oa ng lf ob oc ni lj od oe nk of bi translated"><a class="ae mg" href="https://medium.com/@sh.tsang/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我之前的其他评论</a></h2></div></div>    
</body>
</html>