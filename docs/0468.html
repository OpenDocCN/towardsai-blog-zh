<html>
<head>
<title>A Comparative Study of Linear and KNN Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归和KNN回归的比较研究</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-comparative-study-of-linear-and-knn-regression-a31955e6263d?source=collection_archive---------1-----------------------#2020-05-07">https://pub.towardsai.net/a-comparative-study-of-linear-and-knn-regression-a31955e6263d?source=collection_archive---------1-----------------------#2020-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b499e41045dab313458c63ce37986d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Fqej14RxTPp9zzQ5gKm_g.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">Benjamin O. Tayo的图片</figcaption></figure><h2 id="189e" class="jg jh ji bd b dl jj jk jl jm jn jo dk jp translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="31f3" class="pw-subtitle-paragraph ko jr ji bd b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf dk translated">使用游轮数据集比较监督学习(连续目标)的两种最流行算法的预测能力</h2></div><h2 id="3ba5" class="lg lh ji bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma jo bi translated"><a class="mb mc ep" href="https://medium.com/u/27ca1e213060?source=post_page-----a31955e6263d--------------------------------" rel="noopener" target="_blank"> Vivek Chaudhary </a>和<a class="mb mc ep" href="https://medium.com/u/3a025d440e6b?source=post_page-----a31955e6263d--------------------------------" rel="noopener" target="_blank"> Benjamin Obi Tayo博士</a></h2><h1 id="8c87" class="md lh ji bd li me mf mg ll mh mi mj lo kx mk ky ls la ml lb lw ld mm le ma mn bi translated">一.导言</h1><p id="c2ce" class="pw-post-body-paragraph mo mp ji mq b mr ms ks mt mu mv kv mw lp mx my mz lt na nb nc lx nd ne nf ng im bi translated">在本文中，我们报告了两种最流行的监督学习(连续目标)算法的预测能力的比较研究结果，即线性回归和k-最近邻(KNN)回归。线性回归是一种参数模型，即数据集用于计算权重因子(回归参数)，权重因子必须应用于要素以预测目标变量(见上图)。KNN回归是一种插值算法，它使用k近邻来估计目标变量。</p><p id="8853" class="pw-post-body-paragraph mo mp ji mq b mr nh ks mt mu ni kv mw lp nj my mz lt nk nb nc lx nl ne nf ng im bi translated">与KNN回归相比，线性回归的优势在于线性回归易于解释，因为权重因子可以告诉我们哪些特征是主要的预测因素。KNN回归的一个优点是它不需要特征和目标变量之间的任何相关性(线性关系),这是线性回归的一个要求。KNN回归实现起来很慢，因为它依赖于计算所有矢量实例之间的距离，对于包含数百个要素和数千个观测值的大型数据集来说，这可能非常耗时。</p><h1 id="2ef5" class="md lh ji bd li me mf mg ll mh mi mj lo kx mk ky ls la ml lb lw ld mm le ma mn bi translated">二。数据集和模型选择</h1><p id="8f24" class="pw-post-body-paragraph mo mp ji mq b mr ms ks mt mu mv kv mw lp mx my mz lt na nb nc lx nd ne nf ng im bi translated">在我们的研究中，我们使用了游轮数据集<a class="ae nm" href="https://github.com/bot13956/ML_Model_for_Predicting_Ships_Crew_Size" rel="noopener ugc nofollow" target="_blank"><strong class="mq js">cruise _ ship _ info . CSV</strong></a><strong class="mq js">。</strong>在之前的文章中(<a class="ae nm" href="https://medium.com/towards-artificial-intelligence/feature-selection-and-dimensionality-reduction-using-covariance-matrix-plot-b4c7498abd07" rel="noopener"> <strong class="mq js">特征选择和使用协方差矩阵图</strong> </a>的维数减少)，显示了在数据集中的6个预测特征[' <strong class="mq js">年龄</strong>'、<strong class="mq js">吨位</strong>'、<strong class="mq js">乘客</strong>'、<strong class="mq js">长度</strong>'、<strong class="mq js">客舱</strong>'、<strong class="mq js">乘客密度</strong> ]中，如果我们假设重要特征具有相关性 那么目标变量“<strong class="mq js">乘员</strong>”与4个预测变量:“<strong class="mq js">吨位</strong>”、“<strong class="mq js">乘客</strong>”、“<strong class="mq js">长度</strong>、“<strong class="mq js">车厢</strong>”强相关。</p><h1 id="8a62" class="md lh ji bd li me mf mg ll mh mi mj lo kx mk ky ls la ml lb lw ld mm le ma mn bi translated">三。线性和K近邻回归的Sklearn实现</h1><p id="ebd1" class="pw-post-body-paragraph mo mp ji mq b mr ms ks mt mu mv kv mw lp mx my mz lt na nb nc lx nd ne nf ng im bi translated">我们将使用sklearn进行建模。对于KNN回归，我们对参数<strong class="mq js"> <em class="nn"> n_neighbors </em> </strong>的不同值进行了几次计算。我们发现R2分数相当稳定，不会随着n_neighbors的增加而波动太大。我们发现n_neighbors = 3比其他值略高，因此我们采用n_neighbors = 3进行计算。</p><p id="c490" class="pw-post-body-paragraph mo mp ji mq b mr nh ks mt mu ni kv mw lp nj my mz lt nk nb nc lx nl ne nf ng im bi translated"><strong class="mq js"> III A .导入必要的库</strong></p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="2abc" class="lg lh ji nt b gy nx ny l nz oa">import numpy as np</span><span id="4c26" class="lg lh ji nt b gy ob ny l nz oa">import pandas as pd</span><span id="e57e" class="lg lh ji nt b gy ob ny l nz oa">import matplotlib.pyplot as plt</span><span id="234d" class="lg lh ji nt b gy ob ny l nz oa">from sklearn.metrics import r2_score</span><span id="899e" class="lg lh ji nt b gy ob ny l nz oa">from sklearn.model_selection import cross_val_score</span><span id="331e" class="lg lh ji nt b gy ob ny l nz oa">from sklearn.model_selection import train_test_split</span><span id="e818" class="lg lh ji nt b gy ob ny l nz oa">from sklearn.preprocessing import StandardScaler<br/><br/>from sklearn.linear_model import LinearRegression</span><span id="21f3" class="lg lh ji nt b gy ob ny l nz oa">from sklearn.neighbors import KNeighborsRegressor</span><span id="2bdc" class="lg lh ji nt b gy ob ny l nz oa">from sklearn.pipeline import Pipeline</span><span id="ebff" class="lg lh ji nt b gy ob ny l nz oa">pipe_lr = Pipeline([('scl', StandardScaler()),<br/>                    ('slr', LinearRegression())])</span><span id="7b54" class="lg lh ji nt b gy ob ny l nz oa">knn_lr = KNeighborsRegressor(n_neighbors = 3)</span></pre><h2 id="3b5d" class="lg lh ji bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma jo bi translated">读取数据集并选择预测值和目标变量</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="1d55" class="lg lh ji nt b gy nx ny l nz oa">df=pd.read_csv("cruise_ship_info.csv")</span><span id="3933" class="lg lh ji nt b gy ob ny l nz oa">cols_selected = ['Tonnage', 'passengers', 'length', 'cabins','crew']</span><span id="f07b" class="lg lh ji nt b gy ob ny l nz oa">X = df[cols_selected].iloc[:,0:4].values  <br/>   <br/>y = df[cols_selected]['crew']  </span><span id="2218" class="lg lh ji nt b gy ob ny l nz oa">sc_y = StandardScaler()</span><span id="6d74" class="lg lh ji nt b gy ob ny l nz oa">train_score_lr = []</span><span id="00f5" class="lg lh ji nt b gy ob ny l nz oa">train_score_knn =  []</span></pre><h2 id="09aa" class="lg lh ji bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma jo bi translated">三. c .模型建立和评估</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="1231" class="lg lh ji nt b gy nx ny l nz oa">n = 15</span><span id="041b" class="lg lh ji nt b gy ob ny l nz oa">for i in range(n):</span><span id="e136" class="lg lh ji nt b gy ob ny l nz oa">    X_train, X_test, y_train, y_test = train_test_split( X, y,  <br/>                                      test_size=0.3, random_state=i)</span><span id="ea65" class="lg lh ji nt b gy ob ny l nz oa">    y_train_std = sc_y.fit_transform(y_train[:, <br/>                                      np.newaxis]).flatten()</span><span id="96c5" class="lg lh ji nt b gy ob ny l nz oa">    train_score_lr = np.append(train_score_lr, <br/>                               np.mean(cross_val_score(pipe_lr, <br/>                               X_train, y_train_std, <br/>                               scoring ='r2' , cv = 10)))</span><span id="0bd4" class="lg lh ji nt b gy ob ny l nz oa">    train_score_knn = np.append(train_score_knn, <br/>                                np.mean(cross_val_score(knn_lr, <br/>                                X_train, y_train_std, <br/>                                scoring ='r2' , cv = 10)))</span><span id="76cc" class="lg lh ji nt b gy ob ny l nz oa">train_mean_lr = np.mean(train_score_lr)</span><span id="4cc6" class="lg lh ji nt b gy ob ny l nz oa">train_std_lr = np.std(train_score_lr)</span><span id="85f5" class="lg lh ji nt b gy ob ny l nz oa">train_mean_knn = np.mean(train_score_knn)</span><span id="8c7a" class="lg lh ji nt b gy ob ny l nz oa">train_std_knn = np.std(train_score_knn)</span><span id="3d9d" class="lg lh ji nt b gy ob ny l nz oa">print('R2 train for lr: %.3f +/- %.3f' %<br/>                             (train_mean_lr,train_std_lr))</span><span id="f192" class="lg lh ji nt b gy ob ny l nz oa">print('R2 train for knn_lr: %.3f +/- %.3f' % <br/>                             (train_mean_knn,train_std_knn))</span></pre><p id="4b15" class="pw-post-body-paragraph mo mp ji mq b mr nh ks mt mu ni kv mw lp nj my mz lt nk nb nc lx nl ne nf ng im bi translated">这段代码的输出是:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="3d82" class="lg lh ji nt b gy nx ny l nz oa">R2 train for lr: 0.914 +/- 0.013</span><span id="1087" class="lg lh ji nt b gy ob ny l nz oa">R2 train for knn_lr: 0.843 +/- 0.033</span></pre><p id="2b0c" class="pw-post-body-paragraph mo mp ji mq b mr nh ks mt mu ni kv mw lp nj my mz lt nk nb nc lx nl ne nf ng im bi translated">我们清楚地看到，线性回归的总体R2分数优于KNN回归。</p><h2 id="5133" class="lg lh ji bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma jo bi translated">III D .不同随机状态下平均交叉验证R2评分变化的可视化</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="80f2" class="lg lh ji nt b gy nx ny l nz oa">plt.figure(figsize=(15,11))</span><span id="10af" class="lg lh ji nt b gy ob ny l nz oa">plt.plot(range(n),train_score_lr,color='blue', linestyle='dashed', <br/>         marker='o',markerfacecolor='blue', markersize=10, <br/>         label='linear regression')</span><span id="d392" class="lg lh ji nt b gy ob ny l nz oa">plt.fill_between(range(n),<br/>                 train_score_lr + train_std_lr,<br/>                 train_score_lr - train_std_lr,<br/>                 alpha=0.15, color='blue')</span><span id="fc66" class="lg lh ji nt b gy ob ny l nz oa">plt.plot(range(n),train_score_knn,color='green', linestyle='dashed', <br/>         marker='s',markerfacecolor='green', markersize=10, <br/>         label = 'Kneighbors regression')</span><span id="8269" class="lg lh ji nt b gy ob ny l nz oa">plt.fill_between(range(n),<br/>                 train_score_knn + train_std_knn,<br/>                 train_score_knn - train_std_knn,<br/>                 alpha=0.15, color='green')</span><span id="65d5" class="lg lh ji nt b gy ob ny l nz oa">plt.grid()</span><span id="4384" class="lg lh ji nt b gy ob ny l nz oa">plt.ylim(0.7,1)</span><span id="66c0" class="lg lh ji nt b gy ob ny l nz oa">plt.title ('Mean cross-validation R2 score vs. random state <br/>            parameter', size = 14)</span><span id="42b6" class="lg lh ji nt b gy ob ny l nz oa">plt.xlabel('Random state parameter', size = 14)</span><span id="6ab8" class="lg lh ji nt b gy ob ny l nz oa">plt.ylabel('Mean cross-validation R2 score', size = 14)</span><span id="1218" class="lg lh ji nt b gy ob ny l nz oa">plt.legend()</span><span id="d967" class="lg lh ji nt b gy ob ny l nz oa">plt.show()</span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/3630a8ec9ff3f70f3ec609a33c44674c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*js8mrRnhCAe1nkUTE8QtPw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="mb mc ep" href="https://medium.com/u/27ca1e213060?source=post_page-----a31955e6263d--------------------------------" rel="noopener" target="_blank"> Vivek Chaudhary </a>和<a class="mb mc ep" href="https://medium.com/u/3a025d440e6b?source=post_page-----a31955e6263d--------------------------------" rel="noopener" target="_blank"> Benjamin Obi Tayo博士</a>拍摄</figcaption></figure><p id="f83d" class="pw-post-body-paragraph mo mp ji mq b mr nh ks mt mu ni kv mw lp nj my mz lt nk nb nc lx nl ne nf ng im bi translated">该图显示了作为随机状态参数的函数绘制的平均交叉验证R2评分。浅蓝色和浅绿色阴影区域显示了计算的R2分数的68%置信区间。我们看到，平均而言，线性回归的R2得分高于KNN回归，且方差较小。</p><h1 id="bd60" class="md lh ji bd li me mf mg ll mh mi mj lo kx mk ky ls la ml lb lw ld mm le ma mn bi translated">四。总结和结论</h1><p id="2d95" class="pw-post-body-paragraph mo mp ji mq b mr ms ks mt mu mv kv mw lp mx my mz lt na nb nc lx nd ne nf ng im bi translated">总之，我们报告了使用游轮数据集对线性回归和KNN回归进行比较研究的结果。我们表明，对于给定的数据集，线性回归比KNN回归表现更好。在机器学习中，在选择最终模型之前比较几个模型总是好的。</p><p id="896e" class="pw-post-body-paragraph mo mp ji mq b mr nh ks mt mu ni kv mw lp nj my mz lt nk nb nc lx nl ne nf ng im bi translated">本文的数据集和Jupyter笔记本可以从下面的Github资源库下载:<a class="ae nm" href="https://github.com/bot13956/Linear_vs_KNN_Regression" rel="noopener ugc nofollow" target="_blank"><strong class="mq js">https://github.com/bot13956/Linear_vs_KNN_Regression</strong></a><strong class="mq js">。</strong></p><h1 id="32fe" class="md lh ji bd li me mf mg ll mh mi mj lo kx mk ky ls la ml lb lw ld mm le ma mn bi translated">参考</h1><ol class=""><li id="14ad" class="od oe ji mq b mr ms mu mv lp of lt og lx oh ng oi oj ok ol bi translated"><a class="ae nm" href="https://medium.com/towards-artificial-intelligence/feature-selection-and-dimensionality-reduction-using-covariance-matrix-plot-b4c7498abd07" rel="noopener">使用协方差矩阵图进行特征选择和降维</a>。</li><li id="291a" class="od oe ji mq b mr om mu on lp oo lt op lx oq ng oi oj ok ol bi translated">拉什卡、塞巴斯蒂安和瓦希德·米尔贾利利<strong class="mq js">。</strong> <em class="nn"> Python机器学习，第二版</em>。帕克特出版社，2017年。</li><li id="f96e" class="od oe ji mq b mr om mu on lp oo lt op lx oq ng oi oj ok ol bi translated">Benjamin O. Tayo，<em class="nn">预测船只船员规模的机器学习模型</em>，<a class="ae nm" href="https://github.com/bot13956/ML_Model_for_Predicting_Ships_Crew_Size" rel="noopener ugc nofollow" target="_blank">https://github . com/bot 13956/ML _ Model _ for _ Predicting _ Ships _ Crew _ Size</a>。</li></ol></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><p id="8eaa" class="pw-post-body-paragraph mo mp ji mq b mr nh ks mt mu ni kv mw lp nj my mz lt nk nb nc lx nl ne nf ng im bi translated">由<a class="ae nm" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向艾</a>发布</p></div></div>    
</body>
</html>