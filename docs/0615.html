<html>
<head>
<title>🍖🍲 Recibrew! Predicting Food Ingredients with Deep Learning!🍲🍖</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">🍖🍲Recibrew！用深度学习预测食物成分！🍲🍖</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/recibrew-find-out-the-foods-ingredients-dbc2a4e37383?source=collection_archive---------2-----------------------#2020-06-23">https://pub.towardsai.net/recibrew-find-out-the-foods-ingredients-dbc2a4e37383?source=collection_archive---------2-----------------------#2020-06-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2afa" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/tutorial" rel="noopener ugc nofollow" target="_blank">教程</a></h2><div class=""/><div class=""><h2 id="5e0c" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">在Pytorch Lightning中实现的深度学习中使用seq2seq预测食品成分的▶️▶️循序渐进教程。◀️◀️</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/58629bc3ee4da7bceb675ed7a47c88ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*foGYVpmcioZrRb4-"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@danielcgold?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">丹金</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="d169" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">嗨，伙计们，欢迎来到我的新文章。距离上次写东西已经很久了。今天，我想写一写我的简单有趣的项目，叫做<strong class="lh ja">配方酿酒师</strong> (Recibrew)。Recipe Brewer是我在空闲时间做的有趣项目之一，以满足我的好奇心。因此，我想尽可能诚实地分享我是如何完成这个有趣的小项目的😄。所以你可以复制和学习我做的东西。</p><h1 id="e89d" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">概述</h1><ol class=""><li id="5270" class="mt mu iq lh b li mv ll mw lo mx ls my lw mz ma na nb nc nd bi translated">介绍</li><li id="bedb" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">贮藏室ˌ仓库</li><li id="d230" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">模型</li><li id="9009" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">数据集、EDA和数据预处理</li><li id="7f1b" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">评估指标</li><li id="156b" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">解码策略</li><li id="0315" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">履行</li><li id="9459" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">实验</li><li id="a47b" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">结果</li><li id="d073" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">建议</li><li id="e8a3" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">趣味配料生成器</li><li id="a571" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">结论</li><li id="e1de" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">编后记</li></ol><h1 id="60e7" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">介绍</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nj"><img src="../Images/b455dee19148fb3c845e071ec3d6a953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q8oQ3SDdbmNY-eLO"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@loija?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Loija Nguyen </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="e08c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">该项目是关于建立一个配料生成器应用程序。它是基于配方标题作为输入的配料预测器。我使用<strong class="lh ja">深度学习</strong>作为创建生成食谱名称的模型的方法。</p><p id="c981" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当我妈妈做饭时，想知道用什么配料来做一些新的食物，这个想法就来了。然后我想，‘嗯，不如我建一个程序，可以从食物名称中提取配料。’既然我有开发一些深度学习(DL)应用的经验，为什么不从中获得乐趣呢，✌️.</p><p id="99c1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">总之，我想建立一个程序，它接收食物名称作为输入，并输出配料。下面是我想开发的一个例子:</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="c4c8" class="np mc iq nl b gy nq nr l ns nt">Input : 'fried rice'<br/>Model Output : '1 gr rice || 1 gr salt'<br/></span></pre><p id="8d33" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这篇文章将主要是关于在自然语言处理(NLP)中试验一些深度学习<strong class="lh ja"> Seq2Seq </strong>架构。分别是<strong class="lh ja">门控循环单元(GRU)+巴丹瑙注意</strong>和<strong class="lh ja">变压器</strong>。因为我对它们在低参数下的性能很好奇，所以我把这些架构做得很小。它们每个都有几乎相似的参数(1M左右)。稍后，我们将比较它们中的每一个，看看哪个架构是最好的。我还想介绍一些在<strong class="lh ja"> Pytorch </strong>中开发深度学习应用的伟大工具，这就是<strong class="lh ja"> Pytorch Lightning </strong>。我会告诉你让我的代码库更加结构化是多么有帮助。</p><p id="03cd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文的主要焦点将主要是一步一步地讲述如何实现和试验<strong class="lh ja">配方酿造器</strong>。这篇文章不会过多地讨论这个概念的细节。尽管如此，我还是会提供一些好的读物来帮助你更好地理解这个概念。如果你是新来的，不要担心！我将对这些概念进行温和的描述。如果你还不明白。不要担心！</p><p id="6cc7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是我从一位医学大师那里得到的终极名言，它可能会让你精神振奋😆。</p><blockquote class="nu"><p id="5a08" class="nv nw iq bd nx ny nz oa ob oc od ma dk translated">如果你不明白，不要担心</p><p id="eb63" class="nv nw iq bd nx ny nz oa ob oc od ma dk translated">~吴恩达</p></blockquote><h1 id="fb50" class="mb mc iq bd md me mf mg mh mi mj mk ml kf oe kg mn ki of kj mp kl og km mr ms bi translated">贮藏室ˌ仓库</h1><p id="f885" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">对于任何想深入研究代码的人来说，这里是存储库链接:</p><div class="ok ol gp gr om on"><a href="https://github.com/haryoa/ingredbrew.git" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">haryoa/ingredbrew</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">在GitHub上创建一个帐户，为haryoa/ingredbrew开发做贡献。</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">github.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ky on"/></div></div></a></div><p id="b3b5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">几个注意事项:</p><ul class=""><li id="afae" class="mt mu iq lh b li lj ll lm lo pc ls pd lw pe ma pf nb nc nd bi translated">源代码在'<strong class="lh ja"> recibrew </strong>'中源代码主要用Pytorch写，用Pytorch Lightning包装。</li><li id="4167" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma pf nb nc nd bi translated">存储库中的笔记本是构建代码库的“草稿本”。对于任何对我如何沮丧地实现我的想法感兴趣的人，请看笔记本。(EDA除外)</li><li id="4920" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma pf nb nc nd bi translated">要运行GRU +巴赫达瑙注意力实验，请访问这个<a class="ae le" href="https://colab.research.google.com/drive/1T_Bp_RSsNpp16wu001YF3REwAYkkD00d?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab链接</a></li><li id="30f0" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma pf nb nc nd bi translated">要运行变压器实验，使用这个<a class="ae le" href="https://colab.research.google.com/drive/1S91goRUhoSEu6JcflPuhhm55BUjBTC6g?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab链接</a></li></ul><h1 id="9424" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">模型</h1><p id="a4a6" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">我在简介里说过，我用了2个<strong class="lh ja"> Seq2Seq架构</strong>，分别是<strong class="lh ja"> GRU +巴赫达瑙注意</strong>和<strong class="lh ja">变压器</strong>。我将简要描述它们中的每一个。</p><h2 id="f1a7" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">Seq2Seq</h2><p id="dbbe" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">顾名思义，seq2seq是深度学习中的一种架构“模式”，它接收顺序输入(文本、音频、视频)，也产生顺序输出(文本、音频、视频)。该架构主要包含<strong class="lh ja">编码器</strong>和<strong class="lh ja">解码器</strong>。<strong class="lh ja">编码器</strong>是一个深度学习模型，它对输入seq2seq进行编码，编码后的输入将由<strong class="lh ja">解码器解码，以帮助产生输出序列。</strong>解码器也是深度学习模型。请参见图像0以查看可视化效果。</p><p id="acc9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">实现seq2seq的一些示例:</p><ol class=""><li id="d7fc" class="mt mu iq lh b li lj ll lm lo pc ls pd lw pe ma na nb nc nd bi translated"><strong class="lh ja">机器翻译</strong>:输入源语言，输出目标语言</li><li id="413d" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><strong class="lh ja">摘要</strong>:输入文章，输出文章摘要</li><li id="02b7" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><strong class="lh ja">食品配料</strong>:(本项目)</li></ol><p id="1de2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文中，我们将只探讨<strong class="lh ja"> GRU </strong>和<strong class="lh ja">变压器seq2seq </strong>架构。</p><p id="92fe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于seq2seq的更多信息，可以看这个<a class="ae le" href="https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pq"><img src="../Images/18c0827f96fea11c893503c35108553a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-pm2SxEBcqg-jsZCMokaw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图0 : Seq2Seq可视化</figcaption></figure><h2 id="9657" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">GRU +注意seq2seq</h2><p id="952e" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">我建造的一个<strong class="lh ja"> seq2seq </strong>由<strong class="lh ja"> GRU </strong>编码器和<strong class="lh ja"> GRU </strong>解码器组成，带有<strong class="lh ja"> Bahdanau注意</strong>。我将在下面逐一介绍。</p><p id="ceaa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">门控循环单元(GRU) </strong>是RNN的变体，处理顺序(例如文本)输入。输入序列由<strong class="lh ja">复位门</strong>和<strong class="lh ja">更新门</strong>进行“门控”。<strong class="lh ja">更新门</strong>增加或删除序列中每一步的信息。同时，<strong class="lh ja">复位门</strong>决定在过去步骤中忘记多少信息。</p><p id="00d6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你想了解更多关于GRU的细节，我建议你阅读这篇文章(感谢迈克尔·皮):</p><div class="ok ol gp gr om on"><a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noopener follow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">LSTM和GRU的图解指南:一步一步的解释</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">嗨，欢迎来到长短期记忆(LSTM)和门控循环单位(GRU)的图解指南。我是迈克尔…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pr l oy oz pa ow pb ky on"/></div></div></a></div><p id="ec0a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注意力，</strong>简单来说，学习变量之间什么重要什么不重要来做一些任务。在情感分析任务中，当使用注意力时，它可能会学习使用什么令牌来使模型更容易学习。图1告诉我们一些例子，当看到一个单词对其他单词进行“自我关注”时。</p><p id="2e23" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">直觉上，注意力是一种记住输入顺序的机制。在给定一个输出的情况下，它学习什么样的输入是最好的。给定单词，注意力选择性地决定需要什么输入。它可能会提高模型的性能。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/ad9156e3c4d7d57a4ea70efaa000da18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*6EXvW6qtX8-7SkGD_BQFnQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图1:其他注意直观例子。</figcaption></figure><p id="a249" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们的例子中，模型想要了解输入(源)和输出标记(目标)的重要性。这里有一些例子</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="e5a7" class="np mc iq nl b gy nq nr l ns nt">Source : <strong class="nl ja">fried </strong>rice<br/>Current token in target : <strong class="nl ja">oil</strong><br/>The model should learn that <strong class="nl ja">oil </strong>should become attentive<strong class="nl ja"> </strong>to the <strong class="nl ja">fried </strong>token in the<strong class="nl ja"> source. </strong></span></pre><blockquote class="pt pu pv"><p id="0c41" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">注</strong>:</p><p id="59f1" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">你用油来做油炸食品，对吗？告诉我，如果我错了，因为我在这方面一无所知<strong class="lh ja"> XD。</strong></p></blockquote><p id="a247" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意力公式如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qa"><img src="../Images/9b9eb090e51d222e63b50bca5a07ed4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3M9N3mn3wF57UaKg.jpg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图二:注意力公式<a class="ae le" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">来源:https://www . tensor flow . org/tutorials/text/NMT _ with _ attention</a></figcaption></figure><p id="1e3c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们使用Bahdanau的注意力公式。注意力的输入是作为<strong class="lh ja">值(h_t) </strong>的源编码输出(编码器的输出)和作为<strong class="lh ja">查询(h_s)的解码器GRU层的隐藏状态。</strong> v，W是学习到的参数。</p><p id="26f4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要了解更多信息，我鼓励你阅读这篇关于注意力的技术细节的文章。</p><div class="ok ol gp gr om on"><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">注意？立正！</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">近年来，注意力已经成为深度学习社区中一个相当流行的概念和有用的工具。在这个…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">lilianweng.github.io</p></div></div><div class="ow l"><div class="qb l oy oz pa ow pb ky on"/></div></div></a></div><h2 id="5d62" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">变压器序列2序列</h2><p id="075a" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">变压器是<strong class="lh ja">当前元炒作基础</strong>深度学习架构。现有的NLP架构主要使用变压器作为基础架构。这种架构催生了很多芝麻街命名的深度学习架构，像伯特和GPT。</p><p id="b1fa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">转换器</strong>本身是一个<strong class="lh ja"> seq2seq </strong>架构，由编码器和解码器端的几个<strong class="lh ja">自关注层</strong>组成(GRU seq2seq在编码器和解码器端使用GRU)。该变压器缓解了GRU <strong class="lh ja">需要等待每个顺序步骤的进程</strong>的弱点。转换器同时处理它们中的每一个，使转换器能够并行化。</p><p id="e78b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">架构可视化如图3所示。关于Transformer如何工作更多信息，我强烈建议您访问这篇优秀的文章。</p><div class="ok ol gp gr om on"><a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">图示的变压器</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">讨论:黑客新闻(65分，4条评论)，Reddit r/MachineLearning (29分，3条评论)翻译…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">jalammar.github.io</p></div></div><div class="ow l"><div class="qc l oy oz pa ow pb ky on"/></div></div></a></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/c18e6a094b65ce871476c8929da2ed33.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*UCaH1jpzSTz1gF22Gww2gQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图3:变形金刚在<a class="ae le" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">注意你所需要的一切</a></figcaption></figure><h1 id="67a8" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">数据集、EDA和数据预处理</h1><h2 id="a4b7" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">资料组</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qe"><img src="../Images/21384d494f10f636e07bbbb3c1a4ad4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*at-596DB079BzwXN.jpg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">再见阿萨姆。来源:<a class="ae le" href="https://id.wikipedia.org/wiki/Berkas:Sayur_asem_vegetable_soup.jpg" rel="noopener ugc nofollow" target="_blank">维基百科</a> ( CC BY-SA 2.0)</figcaption></figure><p id="0e39" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个项目中，我使用了一个食物食谱数据集。我试着在谷歌搜索食物食谱，在<strong class="lh ja"> Kaggle </strong>找到了一个印尼食物食谱数据集。因为这是我想要的数据，所以我决定用它作为我项目的数据集。</p><p id="829f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">数据集使用印度尼西亚语言。我试图用英语搜索数据集食物食谱，但我没有找到。所以，我放弃了，决定用印尼的。</p><p id="1480" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不要担心语言。我将提供一些关于数据含义的解释(如有必要)。我想强调的是我做这个项目的过程。你可以把数据转换成你想要的其他食物食谱数据集。</p><p id="6087" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是数据集:</p><div class="ok ol gp gr om on"><a href="https://www.kaggle.com/canggih/indonesian-food-recipes" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">印度尼西亚食物食谱</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">14000份鸡肉、羊肉、牛肉、鸡蛋、豆腐、豆豉和鱼的食谱</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">www.kaggle.com</p></div></div><div class="ow l"><div class="qf l oy oz pa ow pb ky on"/></div></div></a></div><p id="cabb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个项目中，我探索和分析数据，以确定我处理什么样的数据。之后，我根据seq2seq模型的需要对数据进行了预处理。</p><h2 id="316c" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">探索性数据分析</h2><p id="90f5" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">我做的第一件事就是看数据的格式。数据在里面。csv '格式。所以我做了这个:</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="01a7" class="np mc iq nl b gy nq nr l ns nt">df = pd.read_csv("../data/raw/indonesia_food_recipe.csv")</span></pre><p id="3a3c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以下是一些例子:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ca"><img src="../Images/a0c7b42f9170e4552e7e8791cdb74c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R65xTyWKiI04OCleBcFwjw.png"/></div></div></figure><p id="f932" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">该数据包含'<strong class="lh ja">标题</strong>'、<strong class="lh ja">配料</strong>'、<strong class="lh ja">步骤</strong>'、<strong class="lh ja">爱</strong>'、<strong class="lh ja">网址</strong>'、【基本_配料】。<strong class="lh ja">Title’</strong>是食物名称，<strong class="lh ja"> Steps </strong>是如何烹饪食物，Loves是用户喜欢在网站中，“basic_ingredient”是制作食物所需的核心配料</p><blockquote class="pt pu pv"><p id="9782" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">注</strong></p><p id="4346" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">(' ayam' ==鸡)。</p></blockquote><p id="8839" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因为这个项目是<strong class="lh ja">‘预测配料给出食物名称’</strong>此项目需要的列是“标题”、“成分”和“基本成分”。分割数据集需要“basic_ingredient”。</p><p id="b89f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，我决定提取这些统计数据来回答这个描述性的问题:</p><ol class=""><li id="a01c" class="mt mu iq lh b li lj ll lm lo pc ls pd lw pe ma na nb nc nd bi translated">令牌数量</li><li id="3e9f" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">唯一令牌的数量</li><li id="8953" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">前20个频率唯一令牌</li></ol><p id="804f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为此，我使用我的python包来提取它们中的每一个。我把它命名为<strong class="lh ja"> Exploda(探索数据)。</strong>该包提取数据的基本统计数据。</p><blockquote class="pt pu pv"><p id="c97e" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">我的故事/观点</p><p id="1a58" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">目前这个包是私有的。然而，如果有人对它感兴趣，我可能会想开源它。</p></blockquote><p id="104c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> EDA成分</strong></p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="5eb6" class="np mc iq nl b gy nq nr l ns nt">from exploda.explore import get_stat<br/>result = get_stat(df, 'Ingredients', ['get_total_instances', 'get_value_counts_column_and_unique_token', 'get_sum_count_token',<br/>                                      'get_stat_token'])<br/>print('total_instance : ', result['get_total_instances'])<br/>print('total token : ', result['get_sum_count_token'])<br/>print('unique token : ', result['get_value_counts_column_and_unique_token']['total_unique'])</span></pre><p id="989d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">输出:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/6a14b08b86165364bc2723c7297abb2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*-KpOWlBRDGXw8zS2ss0fug.png"/></div></figure><p id="c617" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">数据集中前20个令牌</strong></p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="e8dd" class="np mc iq nl b gy nq nr l ns nt">[('1', 48903),<br/> ('2', 30534),<br/> ('bawang', 27729),<br/> ('secukupnya', 20039),<br/> ('merah', 16548),<br/> ('buah', 15676),<br/> ('siung', 14954),<br/> ('putih', 14403),<br/> ('3', 13590),<br/> ('garam', 12228),<br/> ('sdm', 11257),<br/> ('daun', 10905),<br/> ('4', 10013),<br/> ('sdt', 9542),<br/> ('cabe', 9451),<br/> ('5', 9141),<br/> ('gula', 7777),<br/> ('bumbu', 6922),<br/> ('air', 6892),<br/> ('iris', 6244)]</span></pre><p id="3da3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所以在数据集中，‘1’和‘2’出现的频率最高。它们可能是一种成分的数量(例如，1件，2公斤)。“王霸”或<strong class="lh ja">洋葱是印度尼西亚一种相当常见的食材，其出现频率位居第三。</strong></p><p id="cd82" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">总而言之，大多数前20个令牌是关于如何应用“secukupnya”(充分)、“buah”(水果)、1，2，3，4，5、sdm、SDT“iris”(切片)等成分的度量。</p><p id="bc70" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它还含有受欢迎的成分，如“王霸普提”(大蒜)、“王霸梅拉”(香葱)、“道恩”(叶子)、“卡布”(辣椒)、“古拉”(糖)、“空气”(水)。</p><p id="6e89" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> EDA食品名称</strong></p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="3642" class="np mc iq nl b gy nq nr l ns nt">result2 = get_stat(df, 'Title', ['get_total_instances', 'get_value_counts_column_and_unique_token', 'get_sum_count_token'])<br/>print('total_instance : ', result2['get_total_instances'])<br/>print('total token : ', result2['get_sum_count_token'])<br/>print('unique token : ', result2['get_value_counts_column_and_unique_token']['total_unique'])</span></pre><p id="203d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">输出:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/baeb2b1cce5f5c19a7b6311e4bd645c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*41Ic-OSeGHnIx45p030_hw.png"/></div></figure><p id="ae3d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如所料，食品名称的总的和唯一的令牌低于配料。</p><p id="aa35" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">数据集中前20个令牌</strong></p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="bb38" class="np mc iq nl b gy nq nr l ns nt">[('tahu', 2557),<br/> ('tempe', 2269),<br/> ('ayam', 1999),<br/> ('telur', 1768),<br/> ('sapi', 1611),<br/> ('kambing', 1442),<br/> ('goreng', 1000),<br/> ('daging', 928),<br/> ('ikan', 842),<br/> ('tumis', 748),<br/> ('pedas', 733),<br/> ('bumbu', 607),<br/> ('ala', 572),<br/> ('kecap', 485),<br/> ('telor', 453),<br/> ('balado', 434),<br/> ('sambal', 428),<br/> ('dan', 426),<br/> ('sate', 417),<br/> ('beef', 406)]</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qi"><img src="../Images/a3ffee3054429cb3f9bbd2dfc74f54fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ggjOPcr65q5AZLOI"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@ryanquintal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">瑞安·昆塔尔</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="2fe2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">食品名称中前20个最高频率标记中的大多数是主要成分，如“tahu”(豆腐)、tempe、“ayam”(鸡肉)、“telur”(鸡蛋)、“sapi”(牛肉)、“kambing”(山羊)、“daging”(肉)、“ikan”(鱼)、“telor”(也是鸡蛋)、“sate”(沙爹)。除此之外，它们是食物的味道(“辣”)</p><p id="2261" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过这种探索，我相信该模型可以识别给定食物名称的成分。几个成分名称标记成为食品名称的一部分。让我们稍后测试。我相信像‘pedas’(辣)这样的食物名称一定会输出‘cabe’(辣椒)</p><blockquote class="pt pu pv"><p id="3c15" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">我的故事/观点</strong></p><p id="b6fd" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">我希望这些数据不是很糟糕，例如，包含“炸鸡”的食品名称，以及用鱼代替鸡肉作为配料😅。</p></blockquote><p id="4ff1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi">—</p><blockquote class="pt pu pv"><p id="b94c" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">我的故事/观点</strong></p><p id="6e97" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">出现频率最高的两个食物名称是“tahu”(豆腐)和“tempe”(谷歌搜索)。供你参考,‘tahu’和tempe是我们在印度尼西亚吃的最常见的基本食物。</p></blockquote><h2 id="bc1d" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">数据预处理</h2><p id="263e" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">以下是我对数据进行的预处理:</p><ol class=""><li id="d0a7" class="mt mu iq lh b li lj ll lm lo pc ls pd lw pe ma na nb nc nd bi translated">将“/n”改为“||”作为配料分隔符</li><li id="4347" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">小写字母</li><li id="54ca" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">删除重复项</li><li id="4f31" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">删除非字母数字字符</li><li id="7ce7" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">分开配料及其尺寸(“15克”-&gt;“15克”)</li><li id="0a53" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">删除一些超过120个令牌的实例。我选择这个超参数是因为它可以被4除(对于变压器中的多头)并且大于Q3 (84) +标准差(31)。</li></ol><p id="de09" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以下是基于<strong class="lh ja">基本成分</strong>的最终统计数据:</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="ae18" class="np mc iq nl b gy nq nr l ns nt">tempe      1958 <br/>telur      1931  # egg<br/>tahu       1931  # tofu<br/>ikan       1858  # fish<br/>ayam       1772  # chicken<br/>sapi       1760  # beef<br/>kambing    1590  # goat meat<br/>Name: basic_ingredient, dtype: int64</span></pre><p id="e52a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，我使用<strong class="lh ja">sk learn . model _ selection . train _ test _ split到</strong>确保平衡分割<strong class="lh ja">将数据集分割成训练集、开发集和测试集。</strong>我根据‘基本_配料’列进行了拆分。比例是80% 10% 10%。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="fb89" class="np mc iq nl b gy nq nr l ns nt">train_df, remainder_df, _, _ = train_test_split(df, df['basic_ingredient'], test_size=0.2, random_state=234)<br/>dev_df, test_df, _, _ = train_test_split(remainder_df, remainder_df['basic_ingredient'], test_size=0.5, random_state=345)</span></pre><p id="59f9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，我将它们输出到“已处理”文件夹中</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="f3e1" class="np mc iq nl b gy nq nr l ns nt">train_df.to_csv('../data/processed/train.csv',index_label='no')<br/>dev_df.to_csv('../data/processed/dev.csv', index_label='no')<br/>test_df.to_csv('../data/processed/test.csv', index_label='no')</span></pre><p id="9f00" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">数据操作和探索已经完成:)</p><p id="007f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有关EDA的实施，请参见本笔记:</p><div class="ok ol gp gr om on"><a href="https://github.com/haryoa/ingredbrew/blob/master/notebooks/00001-eda-data-and-split-train-dev-test.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">haryoa/ingredbrew</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">笔记本</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">github.com</p></div></div><div class="ow l"><div class="qj l oy oz pa ow pb ky on"/></div></div></a></div><h1 id="ea56" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">评估指标</h1><p id="b5d8" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">在这个项目中，我们使用<strong class="lh ja"> BLEU </strong>作为评估指标来比较GRU和Transformer的性能。</p><h2 id="1fbe" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">蓝色</h2><p id="4632" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">BLEU ( <strong class="lh ja">双语评估替补</strong>)是一个流行的指标，广泛用于seq2seq任务，如机器翻译。BLEU的主要思想是检查在预测输出和地面真实中是否有许多记号重叠。使用的n-克是1克、2克、3克和4克。</p><p id="8781" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要了解更多关于BLEU及其问题的信息，我推荐阅读这篇文章(谢谢你，<a class="qk ql ep" href="https://medium.com/u/703b09baff4e?source=post_page-----dbc2a4e37383--------------------------------" rel="noopener" target="_blank"> Rachael Tatman </a>关于BLEU的精彩文章:)</p><div class="ok ol gp gr om on"><a href="https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213" rel="noopener follow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">评估NLP: BLEU中的文本输出风险自担</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">刚进入NLP的人经常问我的一个问题是，当系统的输出…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="qm l oy oz pa ow pb ky on"/></div></div></a></div><blockquote class="pt pu pv"><p id="b368" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">我的故事/看法</strong></p><p id="6e39" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">TIL(今天我才知道)关于BLEU的缩写来源于(<strong class="lh ja">双语评估候补</strong>)。在我写这篇文章之前，我以为BLEU是随机命名的。</p></blockquote><h1 id="d618" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">解码策略</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/876e078f8ca0ee19a46a95015bd0f42e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*AJJ2TY8tdbBonbEH9KgZlw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图4:贪婪解码策略</figcaption></figure><p id="80c4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当我们实现一个生成机器学习系统时，我们必须决定如何生成输出序列。在我们的seq2seq系统中，<strong class="lh ja">解码器</strong>将为每一步输出<strong class="lh ja">置信度得分</strong> (softmax概率分布)。问题是，我们如何使用每一步的置信度？</p><p id="d55b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一种通用的方法是<strong class="lh ja">贪婪搜索解码器。</strong>策略足够简单；只需选择每一步的<strong class="lh ja">最高</strong>置信度得分，直到您找到停止标记(&lt;EOS&gt;’)。就是这样。</p><blockquote class="pt pu pv"><p id="093c" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">注</strong></p><p id="bea2" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">记住解码策略是做推理的时候用的，不是训练。</p></blockquote><p id="4867" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">提醒一下，seq2seq由解码器和编码器组成。输入将被编码成某种东西(实际上是一个矢量)，由解码器对每一步进行解码。然后，它将生成词汇表中每个单词的置信度得分。然后，我们选择最高分。</p><p id="125a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们来看一些例子:</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="b99e" class="np mc iq nl b gy nq nr l ns nt">Encoder : Encoder of seq2seq</span><span id="17d7" class="np mc iq nl b gy qo nr l ns nt">Decoder : Decoder of seq2seq<br/>&lt;s&gt; and &lt;/s&gt; are start and end special token respectively</span><span id="d4cb" class="np mc iq nl b gy qo nr l ns nt">input = "&lt;s&gt; fried chicken &lt;/s&gt;"<br/>encoder_output, hidden_variable = Encoder(input)</span><span id="476d" class="np mc iq nl b gy qo nr l ns nt"><strong class="nl ja">STEP 1:</strong></span><span id="fcd0" class="np mc iq nl b gy qo nr l ns nt">target_input = "&lt;s&gt;"<br/>decoded, hidden_variable : Decoder(target_input, encoder_output, hidden_variable)</span><span id="0972" class="np mc iq nl b gy qo nr l ns nt">Output decoded example:</span><span id="5591" class="np mc iq nl b gy qo nr l ns nt">.<br/>.<br/>chili 0.01<br/><strong class="nl ja">chicken 0.95 &lt;- we choose the highest (greedy)</strong><br/>fish 0.03<br/>egg 0.01<br/>.<br/>.</span><span id="ce31" class="np mc iq nl b gy qo nr l ns nt">target_input = "&lt;s&gt; chicken"</span><span id="2890" class="np mc iq nl b gy qo nr l ns nt"><strong class="nl ja">STEP 2:<br/>target_input = "&lt;s&gt; chicken"<br/></strong>decoded, hidden_variable : Decoder(target_input, encoder_output, hidden_variable)</span><span id="552b" class="np mc iq nl b gy qo nr l ns nt">Output decoded example:</span><span id="dcbb" class="np mc iq nl b gy qo nr l ns nt">.<br/>.<br/>|| 1.00 <strong class="nl ja">&lt;- we choose the highest (greedy)</strong><br/>chicken 0.00<strong class="nl ja"> </strong><br/>fish 0.00<br/>egg 0.00<br/>.<br/>.</span><span id="66fa" class="np mc iq nl b gy qo nr l ns nt">target_input = "&lt;s&gt; chicken || "</span><span id="8ef0" class="np mc iq nl b gy qo nr l ns nt"><strong class="nl ja">STEP 3:<br/>target_input = "&lt;s&gt; chicken ||"<br/></strong>decoded, hidden_variable : Decoder(target_input, encoder_output, hidden_variable)</span><span id="a92e" class="np mc iq nl b gy qo nr l ns nt">Output decoded example:</span><span id="c4cf" class="np mc iq nl b gy qo nr l ns nt">.<br/>.<br/>|| 0.00 <br/>chicken 0.00<strong class="nl ja"> </strong><br/>fish 0.00<br/>egg 0.00<br/><strong class="nl ja">oil 1.00  &lt;- we choose the highest (greedy)</strong><br/>.<br/>.</span><span id="0686" class="np mc iq nl b gy qo nr l ns nt">target_input = "&lt;s&gt; chicken || oil "</span><span id="124f" class="np mc iq nl b gy qo nr l ns nt"><strong class="nl ja">STEP 4:<br/>target_input = "&lt;s&gt; chicken || oil"<br/></strong>decoded, hidden_variable : Decoder(target_input, encoder_output, hidden_variable)</span><span id="f378" class="np mc iq nl b gy qo nr l ns nt">Output decoded example:</span><span id="a565" class="np mc iq nl b gy qo nr l ns nt">.<br/>.<br/>|| 0.00 <br/>chicken 0.00<strong class="nl ja"> </strong><br/>fish 0.05<br/>egg 0.30<br/>oil 0.20<strong class="nl ja"> <br/>&lt;/s&gt;</strong>  <strong class="nl ja">0.55 &lt;- we choose the highest (greedy)</strong><br/>.<br/>.</span><span id="deb4" class="np mc iq nl b gy qo nr l ns nt">target_input = "&lt;s&gt; chicken || oil &lt;/s&gt;"<br/><strong class="nl ja">END</strong></span></pre><p id="d4d1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">上面的例子基本上就是seq2seq贪婪解码的工作原理。</p><p id="471b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">还有其他解码策略，如<strong class="lh ja">波束搜索、Top-k采样和top-p采样。</strong>阻止我创作60分钟阅读文章😜，我决定只用贪心搜索解码。</p><blockquote class="pt pu pv"><p id="b73a" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">我的故事/观点</strong></p><p id="8a87" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">作为一个引子，我计划将来发布一篇关于采样策略的新文章，并对它们进行比较:)</p></blockquote><h1 id="ab7c" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">履行</h1><h2 id="5837" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">使用的库</h2><ol class=""><li id="285e" class="mt mu iq lh b li mv ll mw lo mx ls my lw mz ma na nb nc nd bi translated">Pytorch 1.5.1 :一个流行的深度学习框架是用Python写的</li><li id="33c4" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">Torchtext 0.6.0 :一个专注于在Pytorch上处理文本的库</li><li id="d09d" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">Sacrebleu 1.4.10 :计算bleu的工具包</li><li id="1499" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><strong class="lh ja">py torch Lightning 0 . 8 . 1:</strong>py torch新的强大扩展。如果你发现Pytorch中需要类似<strong class="lh ja"> Keras的</strong>框架(回调！)和一个使你的代码更整洁和更高的可重用性的伟大的节省者，我强烈<strong class="lh ja">鼓励</strong>你使用这个包！谢谢你，威廉·法尔肯和他的团队，感谢你们令人敬畏的包裹😄</li><li id="cd15" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated"><strong class="lh ja"> Pandas 1.0.5: Python常用数据表格操作包. 3 </strong></li></ol><p id="c13d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">来自<code class="fe qp qq qr nl b">pytorch-lightning</code> GitHub包:</p><blockquote class="pt pu pv"><p id="8df2" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">注</strong></p><p id="9298" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">Lightning是组织PyTorch代码的一种方式，可以将科学代码从工程中分离出来。它更像是PyTorch风格指南，而不是框架。</p><p id="aa7d" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">在Lightning中，您将代码组织成3个不同的类别:</p><p id="0581" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">研究代码(在照明模块中)。</p><p id="4e4c" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">工程代码(你删除，并由培训师处理)。</p><p id="c478" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">非必要的研究代码(日志等…这在回调中)。</p></blockquote><p id="d778" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi">—</p><blockquote class="pt pu pv"><p id="5cf1" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">我的故事/观点</strong></p><p id="b2e3" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">只有我在用纯Pytorch编写代码时，在解决代码可读性和结构化方面有困难吗？这就是为什么我以前的文章都是用Keras写的。</p></blockquote><h2 id="3a21" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">令牌表示</h2><p id="4234" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">我用的是词索引顺序表示，会被模型转化为词嵌入。使用<code class="fe qp qq qr nl b">nltk.word_tokenizer</code>从句子中对标记进行标记化(拆分)。我用torchtext库对它进行了处理，它将把每个标记顺序编码成数组中的索引表示。我基于训练集构建了令牌词汇表。我将init和end token分别设置为&lt; s &gt;和&lt; /s &gt;。</p><p id="ab3a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在生成模型使用的迭代器时，我使用了torchtext.data.BucketIterator，它将根据食品名称标记的长度对批次进行排序。下面是关于如何处理和构造迭代器的代码(摘自我的源代码)。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="9196" class="np mc iq nl b gy nq nr l ns nt">Example:<br/>'fried rice' -&gt; '&lt;s&gt; fried rice &lt;/s&gt;' -&gt; [2, 4342, 42432, 3]</span></pre><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="qs qt l"/></div></figure><h2 id="eb2f" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">建模</h2><p id="3574" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">我用PyTorch lightning来构建PyTorch代码。</p><h2 id="13c3" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated"><strong class="ak"> GRU +关注</strong></h2><p id="5500" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">我使用了来自<a class="ae le" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank"> Tensorflow NMT注意力教程</a>的代码，并将它们改写成Pytorch。我还添加了一些小功能，比如在解码器端启用双向和多层GRU(虽然我没有使用它😛).你可以在这个链接里看到代码(我的回购):</p><div class="ok ol gp gr om on"><a href="https://github.com/haryoa/ingredbrew/blob/master/recibrew/nn/gru_bahdanau.py" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">haryoa/ingredbrew</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">在GitHub上创建一个帐户，为haryoa/ingredbrew开发做贡献。</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">github.com</p></div></div><div class="ow l"><div class="qu l oy oz pa ow pb ky on"/></div></div></a></div><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="qs qt l"/></div></figure><p id="20c2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我将它们组合在Pytorch Lightning模型界面中。参见上面的代码片段。</p><p id="61c0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在forward函数中，我把forward分为<code class="fe qp qq qr nl b">forward_encoder </code>和<code class="fe qp qq qr nl b">forward_decoder_train </code>。<code class="fe qp qq qr nl b">forward_encoder </code>将接收信号源输入，并将其编码成<code class="fe qp qq qr nl b">enc_out </code>和<code class="fe qp qq qr nl b">hidden</code>。它们将作为解码器的输入，解码器将输出每一步的置信度得分。解码器和编码器使用共享嵌入(意味着共享词汇)。</p><p id="438b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在训练中，我使用了全<strong class="lh ja">老师强制</strong>(下一步的输入是地面真相)使其类似于<strong class="lh ja">变压器设置</strong>。使用AdamW optimizer优化培训。</p><blockquote class="pt pu pv"><p id="a946" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja"> Noob建议:</strong></p><p id="2dec" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">请确保根据您的标准(交叉熵)和嵌入来设置padding_idx，以确保不会对填充进行任何操作。在过去，当我开发一些NER系统时，我忽略了这一点，并得到了糟糕的模型结果。</p></blockquote><h2 id="9eb7" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">变压器</h2><p id="f41c" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">我通过使用内置的Pytorch transformer包来实现转换器。这个包不包括位置嵌入和单词嵌入。所以我从Pytorch教程(见<a class="ae le" href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/beginner/transformer _ tutorial . html</a>)中拿了位置嵌入的实现。</p><p id="b58d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这一节中，我将重点描述Transformer中mask实现的实现，尤其是如何实现mask。我们开始吧:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="qs qt l"/></div></figure><p id="e36e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">编码器和解码器正向传递实现是直接的。我想强调的是掩码变量。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="45ae" class="np mc iq nl b gy nq nr l ns nt">out_trf = self.trfm(out_emb_enc, out_emb_dec, <strong class="nl ja">src_mask</strong>=None, <strong class="nl ja">tgt_mask</strong>=tgt_mask, <strong class="nl ja">memory_mask</strong>=None,                                                   <strong class="nl ja">src_key_padding_mask</strong>=src_pad_mask, <strong class="nl ja">tgt_key_padding_mask</strong>=tgt_pad_mask,                                                   <strong class="nl ja">memory_key_padding_mask</strong>=src_pad_mask</span></pre><p id="04e3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> src_mask:在源输入(编码器的输入)</strong>中，一个位置，如果在<strong class="lh ja">布尔张量</strong>中提供了一个“<strong class="lh ja">真</strong>标志(意味着被屏蔽)，将不会关注任何其他序列(记住变压器有自我关注)。该变量将用于变压器的编码器。转换器的默认实现会将此设置为<strong class="lh ja">无</strong>。</p><p id="884e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> tgt_mask: </strong>类似于src_mask，<strong class="lh ja">但是在目标输入(解码器的输入)</strong>。该变量将用于变压器的解码器。正如我上面所说，转换器可以同时处理，所以它需要一个掩码来防止在注意力过程上进行计算。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="a258" class="np mc iq nl b gy nq nr l ns nt">Easy way to visualize tgt_mask<br/>Remember: decoder output sequence of text in sequential order.<br/>For example, If the model want to output 'Fried Rice is delicious'. </span><span id="99d2" class="np mc iq nl b gy qo nr l ns nt">In the first step (<strong class="nl ja">Fried</strong>), it will not attend 'Rice is delicious'<br/>In the second step (<strong class="nl ja">Rice), It will attend 'Fried' and will not attend 'is delicious'<br/></strong>In the third step (<strong class="nl ja">is</strong>), <strong class="nl ja">It will attend 'Fried' and 'Rice and will not attend 'delicious'</strong><br/>In the forth step (<strong class="nl ja">delicious), It will attend 'Fried, Rice, and is'</strong></span></pre><p id="95b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> memory_mask:在内存(编码器变压器的输出)中，</strong>在第二次注意上使用时(记住，解码器层的注意过程包含两个步骤，自我注意和编码器-解码器注意)，不会参加任何其他序列。变压器默认实现将此设置为<strong class="lh ja">无</strong>。</p><p id="0d03" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> src_key_padding_mask </strong>:基本上是在信源上填充掩码。这个过程将作为掩蔽注意过程中的关键</p><p id="a6ba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> tgt_key_padding_mask </strong>:基本上是在目标上填充遮罩。这个过程将作为掩蔽注意过程中的关键</p><p id="ef9f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> memory_key_padding_mask </strong>:基本上是编码器输出的存储器上的填充掩码。这个过程会在注意力过程中起到掩蔽关键的作用。编码的记忆是信源的填充。</p><p id="526c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">实现padding_mask很容易(见<code class="fe qp qq qr nl b">mask_pad_mask</code>)，对于tgt_mask，实现应该使用<code class="fe qp qq qr nl b">pytorch.nn.Transformer</code>中的<code class="fe qp qq qr nl b">transformer.generate_square_subsequent_mask</code>。</p><p id="e0d3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在Pytorch Lightning实现中，只需像这样正常操作即可:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="qs qt l"/></div></figure><p id="8ea7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">简单明了，易于设置！请记住下面我给你的建议。一些傻傻的小问题由此而来。</p><blockquote class="pt pu pv"><p id="3230" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja"> Noob建议:</strong></p><p id="3dab" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">看到这个了吗？</p><p id="3256" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja"> logits = self.forward(src，TGT[:-1])<br/>loss = loss _ criterion(logits . view(-1，output_dim)，tgt[1:]。视图(-1)) </strong></p><p id="cd36" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">不要忘记将<strong class="lh ja">接地</strong>真值设置为tgt[1:]并将<strong class="lh ja">编码器输入</strong>设置为tgt[:-1]</p><p id="c336" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">我们希望模型学会预测解码器中的下一个单词，不是吗？</p><p id="6370" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">例如:当前令牌是“”我们要确保模型学习下一个单词“鸡”</p></blockquote><p id="24fb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi">—</p><blockquote class="pt pu pv"><p id="68ab" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">我的故事:</strong></p><p id="04eb" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">让我来讲述一下我实现转换器的故事。我搜索了一个内置的easy copy pasta代码(懒得调试，从头实现)。我偶然发现Pytorch内置了transformer包，并有相关教程。我看到了教程，并得出结论，该教程不能适用于我的情况。它不处理<strong class="lh ja">填充遮罩。我到底该怎么做？</strong>我看了文档，找到一个变量叫做[src/tgt/memory]_mask和[src/tgt/memory]_padded _mask。是..仍然困惑，因为描述太短。我搜索所有的堆栈溢出和Github问题。最后，我发现了一些启示，有人用Colab编写了一个实现。此外，我不知道还有关于这些掩码变量附加文档(我应该在文档中向下滚动一点)。</p><p id="1669" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">嗯……问题还是解决了。如果有人有这样的困扰，就看看我实现Pytorch转换器的实现吧。</p></blockquote><h2 id="6142" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">贪婪解码器</h2><p id="f146" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">贪婪搜索解码实现很简单。见下文:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="qs qt l"/></div></figure><p id="af4a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以上是GRU的推理源代码。在推理步骤，模型逐个循环每个实例。对于解码器中的每一步，它都会进行贪婪搜索，直到找到句尾令牌<code class="fe qp qq qr nl b">‘&lt;/s&gt;’</code>。</p><p id="00b5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对变压器的推论也与GRU的相似。您可以在这里看到我的实现:</p><div class="ok ol gp gr om on"><a href="https://github.com/haryoa/ingredbrew/blob/master/recibrew/core_nn.py" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">haryoa/ingredbrew</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">github.com</p></div></div><div class="ow l"><div class="qv l oy oz pa ow pb ky on"/></div></div></a></div><h1 id="a51e" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">实验</h1><p id="5648" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">我不做超参数调整，以避免在训练模型上花费大量时间。</p><h2 id="62f5" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">变压器</h2><p id="bfd3" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">这是我为变压器模型选择的超参数:</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="58be" class="np mc iq nl b gy nq nr l ns nt">"dim_feedforward":   100<br/>"dropout":           0.1<br/>"lr":                0.001<br/>"nhead":             2<br/>"num_decoder_layer": 4</span><span id="335c" class="np mc iq nl b gy qo nr l ns nt">"num_embedding":     100  # Also act as hidden unit in decoder and encoder layer</span><span id="ce04" class="np mc iq nl b gy qo nr l ns nt">"num_encoder_layer": 4<br/>"padding_idx":       1</span></pre><p id="9fc8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">总参数:</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="5d74" class="np mc iq nl b gy nq nr l ns nt">| Name             | Type            | Params<br/>-----------------------------------------------------<br/>0 | full_transformer | FullTransformer | 1 M</span></pre><h2 id="2695" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">苏军总参谋部情报总局</h2><p id="58cb" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">以下是GRU seq2seq模型的超参数:</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="485a" class="np mc iq nl b gy nq nr l ns nt">embedding_dim=100</span><span id="9183" class="np mc iq nl b gy qo nr l ns nt">hidden_dim=100</span><span id="5d9c" class="np mc iq nl b gy qo nr l ns nt">enc_bidirectional=True</span><span id="4afc" class="np mc iq nl b gy qo nr l ns nt">enc_gru_layers=1</span><span id="9cc2" class="np mc iq nl b gy qo nr l ns nt">dropout=0.1</span></pre><p id="59d1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">总参数:</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="4fd3" class="np mc iq nl b gy nq nr l ns nt">| Name             | Type             | Params<br/>------------------------------------------------------<br/>0 | encoder          | Encoder          | 121 K <br/>1 | decoder          | Decoder          | 945 K <br/>2 | shared_embedding | Embedding        | 300 K <br/>3 | criterion        | CrossEntropyLoss | 0</span></pre><p id="bee9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">他们使用Colab GPU进行训练，并使用<strong class="lh ja">贪婪搜索解码器进行推理。</strong></p><blockquote class="pt pu pv"><p id="2c8e" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">注</strong></p><p id="acec" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">Colab笔记本将作为调用Pytorch Lightning实现的主程序。</p></blockquote><p id="14ea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在做实验之前，我将种子设置为888，以确保重现性</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="a25c" class="np mc iq nl b gy nq nr l ns nt">from pytorch_lightning import Trainer, seed_everything</span><span id="5551" class="np mc iq nl b gy qo nr l ns nt">seed_everything(888)</span></pre><p id="24c6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我用批量64和<code class="fe qp qq qr nl b">accumulated_grad_batches=2</code>。这意味着用于计算损失的批量将是128。</p><h1 id="12c0" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">结果</h1><p id="f524" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">关键时刻到了。🎃</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qw"><img src="../Images/5c4765c378effb0112e3ba517c771307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JOCFDHLrUis9SFP3"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@tetrakiss?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Arseny Togulev </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="ea94" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">训练之后，我加载用<code class="fe qp qq qr nl b">pytorch_lightning</code> ModelCheckpoint产生的最佳模型。我预测测试数据集，并使用<code class="fe qp qq qr nl b">sacrebleu</code>计算它的BLEU。表0显示了GRU和变压器的结果。从表中，我们可以得出结论，变压器真正主导GRU +注意Seq2Seq。由于两者具有几乎相似的低参数，Transformer击败了gru。BLEU差异幅度约为13。</p><blockquote class="pt pu pv"><p id="ccb5" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">我的故事/观点</strong></p><p id="ac1d" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">我认为Transformer输给了GRU，因为根据我的经验，Transformer倾向于过度适应数据，并且经常产生错误的输出。</p></blockquote><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="qs qt l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">表0:结果</figcaption></figure><p id="fde1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们采集一些数据样本，了解更多信息！</p><blockquote class="pt pu pv"><p id="9ddb" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated"><strong class="lh ja">注</strong></p><p id="6a31" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">我运行存储库中笔记本内<code class="fe qp qq qr nl b">00007_eda_result.ipynb</code>中的代码。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qx"><img src="../Images/5b5c45bce6192c11cf1a46eee7eca859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rs8OQrhp41joUv0_eBgJw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图5</figcaption></figure><p id="ae4a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">图5显示了3个样本作为我们训练模型的输出。从上面的例子中，我们可以看出GRU经常重复成分而不停顿。这可能是由GRU引起的，它对某些成分摄入过多。当使用贪婪解码器时，每个步骤的最高置信水平总是产生相同的成分。</p><p id="6452" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">变形金刚和GRU倾向于生产“1/2”令牌。从我们在EDA中看到的情况来看，“1”和“2”出现的频率最高，我认为我们可以推断出模型可能会过度拟合这些数字。</p><p id="b2b1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然《变形金刚》比《GRU》有更好的品种输出，但还是重复了一些成分。从图5中，我们可以看到，在总结中，它重复了第4次“kikil sapi”(牛肉)。在那之后，它最终把输出变成别的东西。总而言之，这个变形金刚和GRU模型不知道一个成分不能出现两次或更多次。</p><p id="e18d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">无论如何，Transformer的输出可能代表原始配方。我们知道烹饪食物有很多种方法。因此，即使它有不同于地面真相的输出，我们也不能判定它是错误的。</p><h1 id="cbbc" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">建议</h1><ol class=""><li id="09ba" class="mt mu iq lh b li mv ll mw lo mx ls my lw mz ma na nb nc nd bi translated">我应该做过超参数调优(中小型号)。有了它，也许它能生产得更好。这也是我要写的待办事项文章。</li><li id="3a23" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">使用其他解码策略。正如我前面所说，我将很快就此写一篇新文章。让我们看到贪婪与他人的区别。</li><li id="ce9e" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">避免重复。为了避免重复，也许我可以加一个解码的限制？</li><li id="ae31" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">用其他(大)数据对模型进行预训练。我认为有了一个预先训练好的模型，可以进一步提高结果。</li><li id="1e98" class="mt mu iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">用半监督学习如<strong class="lh ja">反向翻译</strong>添加数据。这是目前的热点，也可能提高模型的质量</li></ol><h1 id="1e54" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">有趣的配方啤酒生成器</h1><p id="5753" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">😆让我们一起享受模型的乐趣吧！😆。</p><p id="267e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你也可以试试下面的Colab链接。注意，配料是生成的！</p><h2 id="854d" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated"><strong class="ak">随机输入1: </strong></h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qy"><img src="../Images/a40fbb9152b988223168f5d0521f00fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SSZ3gBeZ1rkXMHyz"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@ambientpictures?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">莫哈末·阿拉姆</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="1780" class="np mc iq nl b gy nq nr l ns nt">Food : nasi goreng pedas (spicy fried rice)<br/>Ingredients:<br/> 1 piring nasi putih (rice)<br/> 1 butir telur (egg)<br/> 1 buah wortel (carrot)<br/> 1 buah timun  (cucumber)<br/> 1 buah bawang putih  (garlic)<br/> 1 buah bawang merah  (onion)<br/> 1 buah cabe merah  (red chili)<br/> 1 buah cabe rawit  (cayenne pepper)<br/> 1 buah tomat  (tomato)<br/> 1 / 2 sdt terasi  (shrimp paste)<br/> 1 / 2 sdt garam  (salt)<br/> 1 / 2 sdt merica (pepper)</span></pre><p id="5de6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们试试一般的吧，‘香辣炒饭’。如您所见，即使食品名称中没有辣椒，它也会输出辣椒。</p><h2 id="f320" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">随机输入2:</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qe"><img src="../Images/13f43e821fea6bd12c325c16f4f0938c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Wph8bKg-HU4JVNZT.jpg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">Indom <a class="ae le" href="https://commons.wikimedia.org/wiki/File:2016_Indomie_Mi_Goreng_Rendang_NL_02.jpg" rel="noopener ugc nofollow" target="_blank"> ie源</a> ( CC BY-SA 2.0)</figcaption></figure><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="8448" class="np mc iq nl b gy nq nr l ns nt">Food : indomie lodeh <br/>Ingredients :<br/> 1 bungkus indomie goreng <br/> 1 / 2 kg toge (sprouts)<br/> 1 / 2 kg kacang tanah  (peanuts)<br/> 1 / 2 kg bawang bombay  (onion)<br/> 1 / 2 kg cabe merah   (red chili)<br/> 1 / 2 ons cabe rawit  (cayenne pepper)<br/> 1 / 2 ons cabe merah  (red chili)<br/> 1 / 2 sdt garam  (salt)<br/> 1 / 2 sdt gula   (sugar)<br/> 1 / 2 sdt merica bubuk  (pepper powder)<br/> 1 / 2 sdt penyedap rasa  (flavoring)<br/> 1 / 2 sdt gula pasir   (granulated sugar)</span></pre><p id="ae46" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于任何想知道lodeh是什么的人来说:</p><div class="ok ol gp gr om on"><a href="https://en.wikipedia.org/wiki/Sayur_lodeh" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">萨尤尔·洛德</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">Sayur lodeh是一种印度尼西亚蔬菜汤，由椰奶中的蔬菜制成，在印度尼西亚很受欢迎，但大多数…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">en.wikipedia.org</p></div></div><div class="ow l"><div class="qz l oy oz pa ow pb ky on"/></div></div></a></div><p id="e51b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">营多面是一种在印尼很受欢迎的方便面。你应该已经知道了。</p><p id="99cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">仅供参考，我认为没有人结合洛德与营多面，也许有人想尝试这些成分？😜</p><p id="7838" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">随机输入3 </strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ra"><img src="../Images/f9ca7d9669377e66a835d27397c2701d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PvL0OXtuATLExzj4"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@gaspanik?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Masaaki Komori </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="61b3" class="np mc iq nl b gy nq nr l ns nt">Food : sop nasi goreng  (FRIED RICE SOUP)<br/>Ingredients:<br/> 1 piring nasi putih  (rice)<br/> 1 buah wortel    (carrot)<br/> 1 buah kentang   (potato)<br/> 1 buah tomat    (tomato)<br/> 1 buah bawang bombay   (onion)<br/> 2 siung bawang putih   (garlic)<br/> 1 / 2 sdt lada bubuk   (ground pepper)<br/> 1 / 2 sdt garam     (salt)<br/> 1 / 2 sdt kaldu bubuk  (broth powder)<br/> 1 / 2 sdt bubuk   (any powder? LOL)<br/> 1 / 2 sdt minyak goreng    (cooking oil)</span></pre><p id="d234" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">汤炒饭！😰</strong></p><p id="9e62" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在印尼，它是一种反主流的食物(还是只有我？).有人想试试吗？</p><h2 id="b5dd" class="np mc iq bd md pg ph dn mh pi pj dp ml lo pk pl mn ls pm pn mp lw po pp mr iw bi translated">Colab链接:</h2><p id="5b51" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">这里是任何想尝试生产原料的人的合作链接。它将使用我自己训练过的最好的变形金刚模型。如果你愿意，你可以在这里评论你生产的食物成分😃。</p><div class="ok ol gp gr om on"><a href="https://colab.research.google.com/drive/1yBHIbBLd2Um6CWpxZfEXvNkNHGDNuH_1?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">谷歌联合实验室</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">编辑描述</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">colab.research.google.com</p></div></div><div class="ow l"><div class="rb l oy oz pa ow pb ky on"/></div></div></a></div><h1 id="3386" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">结论</h1><p id="4413" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">这篇文章分享了如何制作配方(成分)啤酒，这是我在业余时间做的一个项目。它还告诉您它背后的技术以及如何实现它。文章还告诉你，在几乎相同的参数下，GRU和Transformer之间最好的深度学习架构是什么。实验表明，变形金刚战胜了格鲁。之后，本文还尝试生成一些无意义的食物成分。</p><p id="1bde" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我希望它可以让你更好地理解如何实现seq2seq深度学习系统，并激励你做一些有趣的项目并与大家分享。:)</p><h1 id="b8bb" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">编后记</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qi"><img src="../Images/03fd6f78fb9485f619dc0f8d822f811c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JfEQaejNtJXDRlmG"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">Zhdana Iyuleva 在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="00d3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">终于！我又能写文章了。已经过了很长时间了。仅供参考，我几年前就想写这篇文章了。但随着我的生活继续，它被转移了，我忘记了它。不久前，我做自我评价。我评价我最近没有写任何文章，这是我分享一些东西的‘媒介’之一。爱是关心，你知道。瞧，这就是文章！</p><p id="1c0b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所以，我希望我能继续写更多的文章。我希望分享我的知识再次成为我的习惯。我希望我的知识不仅留在我的大脑里，还能和其他人一起交流，帮助他们取得成就。</p><p id="482a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你想知道为什么我的文章这么长。嗯…我没想到我的文章会这么长，当我意识到这一点时，没关系…我写的信息越多，就越好。事实上，我可以把我的文章缩短到5-10分钟。我想给你关于我的有趣项目的最详细的信息，以便更好地理解它。</p><p id="bfba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">无论如何，如果你发现我的文章需要改进，你可以通过评论或者发邮件给我来反馈。我已经在我的存储库中共享了我的电子邮件。我喜欢这些反馈！</p><p id="2952" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你对我的文章感兴趣并想关注，仅供参考我的文章将主要是关于自我发展和人工智能(特别是深度学习)。对这些领域感兴趣的可以关注我:)。</p><p id="a5f4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，</p><blockquote class="pt pu pv"><p id="b7b0" class="lf lg pw lh b li lj ka lk ll lm kd ln px lp lq lr py lt lu lv pz lx ly lz ma ij bi translated">我欢迎任何可以提高我自己和这篇文章的反馈。我正在学习写作，学习变得更好。我感激能让我变得更好的反馈。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi rc"><img src="../Images/07cbac6d32f913b49f2b2f4b8dd63f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SaSU4hhuPMuHoFLs.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:<a class="ae le" href="https://pixabay.com/illustrations/thank-you-polaroid-letters-2490552/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>作者:<a class="ae le" href="https://pixabay.com/illustrations/thank-you-polaroid-letters-2490552/" rel="noopener ugc nofollow" target="_blank">杰洛特</a></figcaption></figure><h1 id="afff" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">来源</h1><div class="ok ol gp gr om on"><a href="https://en.wikipedia.org/wiki/BLEU" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">蓝色</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">BLEU(双语评估替补)是一种算法，用于评估文本的质量已经…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div><div class="ok ol gp gr om on"><a href="https://github.com/PyTorchLightning/pytorch-lightning" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">火炬之光/火炬之光</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">阅读此快速启动页面系统/ PyTorch版本。1.3(最小。请求。)1.4 1.5(最新)Linux py 3.6[CPU]Linux py 3.7[GPU]…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">github.com</p></div></div><div class="ow l"><div class="rd l oy oz pa ow pb ky on"/></div></div></a></div></div></div>    
</body>
</html>