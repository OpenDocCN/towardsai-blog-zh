<html>
<head>
<title>Building a Recommender System Using TFRS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用TFRS构建推荐系统</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/building-a-recommender-system-using-tfrs-93da18f7a955?source=collection_archive---------3-----------------------#2022-11-22">https://pub.towardsai.net/building-a-recommender-system-using-tfrs-93da18f7a955?source=collection_archive---------3-----------------------#2022-11-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d0fe" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第2部分:建模和评估</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e9510f0f09ff42e4a78b5917d3f78b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SGwXUUqt_0aTINDe"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">马丁·亚当斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="1503" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程的<a class="ae ky" href="https://medium.com/towards-artificial-intelligence/building-a-recommender-system-using-tfrs-4043db00ab79" rel="noopener">第一部分</a>是关于导入和清理数据集的。在这一部分中，我们将更多地关注特征工程、训练和评估模型。</p><h1 id="fcb8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">步骤1:清理和过滤数据集</h1><p id="75f4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在接下来的部分中，我们将运行<strong class="lb iu"> remove_repeating_subs() </strong>和<strong class="lb iu">build _ training _ sequences()</strong>函数。注意，为了简洁起见，我们不会包括这两个函数的代码。这两个函数的代码可以在本教程末尾的链接中找到。</p><p id="861b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顾名思义，使用<strong class="lb iu">remove _ repeating _ subs()</strong>函数将检查并删除数据集中所有重复的subs，从重复数据中过滤掉数据集。</p><p id="f87f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">build _ training _ sequences()</strong>函数将获取一个给定的数据集，并将用户子序列分割成定义大小的块。接下来，对于每个块，我们将筛选出潜在的标签来选择一个训练标签，然后通过顶部的子过滤器列表进行过滤。然后，我们将从过滤的子序列中随机选择标签，使用vocab概率分布来平滑它。最后，我们将通过确保用户的子序列存在于模型的词汇表中并过滤掉所选择的子序列来构建一个序列。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="152f" class="mx lw it mt b be my mz l na nb">pp_user_data = remove_repeating_subs(df)<br/>train_data = build_training_sequences(pp_user_data)<br/>seqs,lbls,lngths = zip(*train_data)</span></pre><p id="33e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一部分中，我们将在执行remove_repeating_subs和build_training_sequences函数后获取数据集的新值，并将它们存储到一个新的数据集中，新的列名为{sub_seqs，sub _ label。序列长度}。从现在开始，将使用这个新数据集。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="5f26" class="mx lw it mt b be my mz l na nb">train_df = pd.DataFrame({‘sub_seqs’:seqs,<br/>'sub_label':lbls,<br/>'seq_length':lngths})</span></pre><h2 id="bbba" class="nc lw it bd lx nd ne dn mb nf ng dp mf li nh ni mh lm nj nk mj lq nl nm ml nn bi translated">步骤2:导入TensorFlow和tflearn</h2><p id="ad87" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">然后，我们将TensorFlow和tflearn库导入到我们的模型中。这两个库都将在模型的后续部分中使用。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="0a5e" class="mx lw it mt b be my mz l na nb">import tensorflow as tf<br/>import tflearn<br/>from tflearn.data_utils import to_categorical, pad_sequences</span></pre><h2 id="936e" class="nc lw it bd lx nd ne dn mb nf ng dp mf li nh ni mh lm nj nk mj lq nl nm ml nn bi translated">步骤3:设置模型的参数</h2><p id="ac5c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们将从向我们的训练模型传递所需的参数开始。一些train_model函数参数包括:</p><p id="8698" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">训练</strong>:过滤后的训练数据集。</p><p id="7764" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">测试:</strong>过滤后的测试数据集。</p><p id="6042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> vocab_size: </strong>要创建的词汇表的大致大小。</p><p id="e2ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> n_epoch: </strong>表示机器学习算法已经完成的整个训练数据集的通过次数。</p><p id="8bfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> n_units: </strong>神经网络中选择的节点数。</p><p id="e1ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">丢失:</strong>是训练一层中给定节点的概率，其中1表示没有丢失，0表示该层没有输出。</p><p id="c3c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> learning_rate: </strong>是模型在每次迭代中为达到最小损失函数所采取的实际步长。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="acdf" class="mx lw it mt b be my mz l na nb">def train_model(train,test,vocab_size,n_epoch=2,n_units=128,dropout=0.6,learning_rate=0.0001):</span></pre><p id="ab82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">定义新的培训和测试数据。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="972e" class="mx lw it mt b be my mz l na nb">trainX = train[‘sub_seqs’]<br/>trainY = train['sub_label']<br/>testX = test['sub_seqs']<br/>testY = test['sub_label']</span></pre><h2 id="3dd6" class="nc lw it bd lx nd ne dn mb nf ng dp mf li nh ni mh lm nj nk mj lq nl nm ml nn bi translated">步骤4:序列填充</h2><p id="11b9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">使用序列填充，我们将确保列表中的所有序列具有相同的长度。这是通过在每个序列的开头添加额外的0，直到它们达到可用的最高序列的长度来实现的。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="6a5a" class="mx lw it mt b be my mz l na nb">trainX = pad_sequences(trainX, maxlen=sequence_chunk_size, value=0.,padding=’post’)</span></pre><pre class="no ms mt mu bn mv mw bi"><span id="7a2f" class="mx lw it mt b be my mz l na nb">testX = pad_sequences(testX, maxlen=sequence_chunk_size, value=0.,padding=’post’)</span></pre><p id="7334" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们将把标签转换成二进制向量</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="9fc5" class="mx lw it mt b be my mz l na nb">trainY = to_categorical(trainY, nb_classes=vocab_size)</span></pre><pre class="no ms mt mu bn mv mw bi"><span id="275e" class="mx lw it mt b be my mz l na nb">testY = to_categorical(testY, nb_classes=vocab_size)</span></pre><h1 id="598a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">第五步:网络建设</h1><p id="5bbc" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">对于这个模型的主要部分，我们将使用tflearn库来构建我们的神经网络。这里使用的神经网络总共有4层。然后将每一层的输出作为输入插入到下一层。</p><p id="1163" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的第一隐藏层是<strong class="lb iu">嵌入层</strong>。嵌入层负责<a class="ae ky" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/#:~:text=The%20Embedding%20layer%20is%20defined,vocabulary%20would%20be%2011%20words." rel="noopener ugc nofollow" target="_blank">字嵌入</a>。这个模型中嵌入层的大小等于vocab_size的大小。</p><p id="c571" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二层是<a class="ae ky" href="https://towardsdatascience.com/lstm-networks-a-detailed-explanation-8fae6aefc7f9" rel="noopener" target="_blank"> <strong class="lb iu"> LSTM层</strong> </a> <strong class="lb iu"> </strong>这是<strong class="lb iu">长短期记忆</strong>的简称。这一层使用128个神经元(单元)。第三层是<a class="ae ky" href="https://indiantechwarrior.com/fully-connected-layers-in-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">全连通层</a>。在这一层中，输入乘以一个权重矩阵，然后将一个偏置向量添加到最终结果中。</p><p id="b036" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第四层也是最后一层是<a class="ae ky" href="https://www.mathworks.com/help/deeplearning/ref/regressionlayer.html;jsessionid=38604f8c811874db641c9ab60e05" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a>回归层。回归层返回模型的最终输出，也就是subreddit post推荐的权重。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="8fe1" class="mx lw it mt b be my mz l na nb">net = tflearn.input_data([None, 15])<br/>net = tflearn.embedding(net, input_dim=vocab_size, output_dim=128,trainable=True)<br/>net = tflearn.lstm(net, n_units=n_units, dropout=dropout,weights_init=tflearn.initializations.xavier(),return_seq=False)<br/>net = tflearn.fully_connected(net, vocab_size, activation='softmax',weights_init=tflearn.initializations.xavier())<br/>net = tflearn.regression(net, optimizer='adam', learning_rate=learning_rate,<br/>loss='categorical_crossentropy')</span></pre><h2 id="52de" class="nc lw it bd lx nd ne dn mb nf ng dp mf li nh ni mh lm nj nk mj lq nl nm ml nn bi translated">步骤6:训练我们的模型</h2><p id="ca69" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们将把训练和测试数据集传递到最终模型中，并确定批量大小为512。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="6a9b" class="mx lw it mt b be my mz l na nb">model = tflearn.DNN(net, tensorboard_verbose=2)<br/>model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=False,batch_size=512,n_epoch=n_epoch)<br/>return model</span></pre><h2 id="0011" class="nc lw it bd lx nd ne dn mb nf ng dp mf li nh ni mh lm nj nk mj lq nl nm ml nn bi translated">步骤7:拆分数据集</h2><p id="c118" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们将把数据集分成80%的训练数据集和20%的测试数据集。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="f796" class="mx lw it mt b be my mz l na nb">split_perc=0.8<br/>train_len, test_len = np.floor(len(train_df)*split_perc), np.floor(len(train_df)*(1-split_perc))<br/>train, test = train_df.ix[:train_len-1], train_df.ix[train_len:train_len + test_len]</span></pre><h2 id="e7f0" class="nc lw it bd lx nd ne dn mb nf ng dp mf li nh ni mh lm nj nk mj lq nl nm ml nn bi translated">步骤8:运行最终模型</h2><p id="4a0f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了运行我们的最终模型，我们将把<strong class="lb iu">训练</strong>、<strong class="lb iu">测试</strong>和<strong class="lb iu">词汇长度</strong>传递给train_model函数。</p><pre class="kj kk kl km gt ms mt mu bn mv mw bi"><span id="8f09" class="mx lw it mt b be my mz l na nb">model = train_model(train,test,len(vocab))</span></pre><h1 id="4f1b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">步骤9:模型评估</h1><p id="7853" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了检查我们模型的性能，我们提供了<strong class="lb iu">损失</strong>和<strong class="lb iu">验证</strong>函数的图表。这两个图表是一个模型如何评估的可靠表示。</p><h1 id="1082" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">损失图</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/d18bd215d2b137a6bd3fd18b41dbe1b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FRzo2mu5kamNwjAo"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">损失图图片(<a class="ae ky" href="https://www.kaggle.com/code/gauthierhaas/tensorflow-subreddit-recommender-system/notebook" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="faaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个<a class="ae ky" href="https://www.datarobot.com/blog/introduction-to-loss-functions/" rel="noopener ugc nofollow" target="_blank">损失图/函数</a>是检查你的模型性能的最简单的方法之一。它们给我们一个模型学习方向的快照。如果模型的结果完全偏离(不准确)，损失函数将返回高数值。另一方面，如果模型运行良好，将返回较低的值。</p><h1 id="828b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">验证图</h1><p id="8621" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">损失图显示模型如何拟合训练数据，而<a class="ae ky" href="https://www.baeldung.com/cs/training-validation-loss-deep-learning" rel="noopener ugc nofollow" target="_blank">损失/验证</a>图显示模型如何拟合新数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/28dd72d2de48fd958176a0b03030003f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UdguOo2KSr3Tazr6"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">损失/验证图的图像(<a class="ae ky" href="https://www.kaggle.com/code/gauthierhaas/tensorflow-subreddit-recommender-system/notebook" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="1073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要找上面教程的原代码，查看下面链接<a class="ae ky" href="https://www.kaggle.com/code/gauthierhaas/tensorflow-subreddit-recommender-system/data" rel="noopener ugc nofollow" target="_blank">原代码</a>。</p><h1 id="f163" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="439b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">随着公司为他们的客户提供个性化的定制销售人员，客户倾向于在新产品上花费越来越多的钱，使得今天的销售额比以往任何时候都多。无论是在流媒体、零售还是就业领域，公司的推荐系统都是举足轻重的。随着推荐系统日益流行，越来越多的公司将这种系统集成到他们自己的系统中，更多的公司肯定会效仿。</p></div></div>    
</body>
</html>