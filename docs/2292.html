<html>
<head>
<title>Self-Supervised Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自我监督学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/self-supervised-learning-b65fc6d560ad?source=collection_archive---------2-----------------------#2021-11-03">https://pub.towardsai.net/self-supervised-learning-b65fc6d560ad?source=collection_archive---------2-----------------------#2021-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8834" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="07b8" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">从其他事物中预测所有事物</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/1ad674c3e0ef21dbea4e2bbcfd6a0b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FQQQVZwDeHU_twIj"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@itshoobastank?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">的照片由<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的</a>代罗田友</figcaption></figure><p id="749b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">机器学习大致分为监督、非监督、半监督和强化学习问题。机器学习通过解决监督学习问题获得了大部分成功。监督学习任务中的数据被标记，因此为最先进的模型提供了更多的性能增强机会。</p><p id="3236" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最近，通过监督学习的深度学习也取得了巨大的成功。从图像分类到语言翻译，它们的性能一直在提高。然而，在一些领域，如罕见疾病的医学数据集，收集大的标记数据集是昂贵且不可能的。这些类型的数据集为自我监督算法提供了充分的机会，以进一步提高预测模型的性能。</p><p id="69f4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">自监督学习旨在从未标记数据中学习信息表示。通常，在这种情况下，标记数据集比未标记数据集相对较小。自我监督学习使用这些未标记的数据，并执行托词任务和对比学习。</p><p id="13d6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在一篇关于自我监督学习的精彩文章中，Jeremey Howard将监督学习定义为两个阶段:“<em class="mk">我们用于预训练的任务被称为借口任务。我们随后用于微调的任务被称为下游任务</em>。自我监督学习的例子包括未来单词预测、屏蔽单词预测修补、彩色化和超分辨率。</p><h1 id="d2ee" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">计算机视觉的自我监督学习</h1><p id="125f" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo nf lq lr ls ng lu lv lw nh ly lz ma ij bi translated">自监督学习方法依赖于数据的空间和语义结构。对于图像来说，空间结构学习是极其重要的。包括<a class="ae le" href="https://arxiv.org/pdf/1803.07728.pdf?ref=hackernoon.com" rel="noopener ugc nofollow" target="_blank">旋转</a>、<a class="ae le" href="https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5" rel="noopener ugc nofollow" target="_blank">拼图</a>和<a class="ae le" href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40" rel="noopener ugc nofollow" target="_blank">着色</a>在内的不同技术被用作从图像中学习表征的借口任务。对于彩色化，灰度照片作为输入给出，并且生成照片的彩色版本。张等人[1]的论文解释了产生鲜明和逼真的彩色化的彩色化过程。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ni"><img src="../Images/e796091cf79fa23fc405ba502cd6e14e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a-WS4C9FaovRZNhTh6SfHQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图1:该图摘自张等人的论文[1]</figcaption></figure><p id="5bea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一种广泛用于计算机视觉的自我监督学习的方法是放置图像补片。这方面的一个例子包括Doersch等人的论文。在这项工作中，提供了一个大的未标记的图像数据集，并从中提取随机对的补丁。在初始步骤之后，卷积神经网络预测第二个面片相对于第一个面片的位置。图2展示了这个过程。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nj"><img src="../Images/ef950ac55da3e3c9300883a0f0bd5695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzysrd-lXQoadeCnWbGhqQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图2:图片取自Doersch等人的论文[2]</figcaption></figure><p id="e460" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有不同的其他方法用于自我监督学习，包括<a class="ae le" href="https://arxiv.org/abs/1604.07379" rel="noopener ugc nofollow" target="_blank">修补</a>和<a class="ae le" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Jenni_Self-Supervised_Feature_Learning_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">分类损坏的图像</a>。如果你对这个话题感兴趣，请查阅参考文献[3]。它提供了一个关于上述主题的文献综述。</p><h1 id="3ae2" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">自然语言处理的自我监督学习</h1><p id="723c" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo nf lq lr ls ng lu lv lw nh ly lz ma ij bi translated">在自然语言处理任务的情况下，自我监督学习方法是最常见的。<a class="ae le" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>论文的“<strong class="lh ja">连续单词袋</strong>”方法是自我监督学习最著名的例子。</p><p id="ff3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">类似地，存在用于自我监督学习的其他不同方法，包括相邻单词预测、相邻句子预测、自回归语言建模和屏蔽语言建模。在<a class="ae le" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae le" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">罗伯塔</a>和<a class="ae le" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">艾伯特</a>的论文中已经使用了屏蔽语言建模公式。在这项任务中，预测了一小组屏蔽词。</p><p id="100b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">文本自我监督学习的最新例子包括张等人的论文。作者提出了一种间断句生成机制。这种机制用于摘要的下游任务。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/0dd0bdd8b743cd1a320fcef6c3f4710d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yftjlevBMEi95bRaFAYcEA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图3:该图摘自张等人的论文[4]</figcaption></figure><p id="63d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">刘写了一篇关于这个话题的非常有趣的博客。更多见解请阅读。</p><h1 id="7ad6" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">表格数据的自我监督学习</h1><p id="2161" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo nf lq lr ls ng lu lv lw nh ly lz ma ij bi translated">对图像和文本的自我监督学习已经取得了进展。然而，现有的自监督方法对于表格数据并不有效。表格数据没有下划线空间或语义结构，因此依赖于空间和语义结构的现有技术是无用的。</p><p id="e5f4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">大多数表格数据涉及分类特征，这些特征不具有有意义的凸组合。即使对于连续变量，也不能保证数据流形是凸的。这一挑战为研究人员提供了一个新的研究方向。我将简要说明在这方面所做的一些工作。</p><p id="9e13" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">由Vincent等人[5]完成的工作T11提出了一种去噪自动编码器的机制。借口任务是从损坏的样本中恢复原始样本。在另一篇<a class="ae le" href="https://arxiv.org/pdf/1604.07379.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中，Pathak等人【6】提出了一种上下文编码器，其借口任务是从被破坏的样本和掩码向量中重建原始样本。</p><p id="7f39" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对<a class="ae le" href="https://arxiv.org/pdf/1908.07442.pdf" rel="noopener ugc nofollow" target="_blank">Tabnet</a>【7】和<a class="ae le" href="https://arxiv.org/abs/2005.08314" rel="noopener ugc nofollow" target="_blank">TaBERT</a>【8】的研究也是一项朝着自我监督学习的进步工作。在这两项研究中，借口任务是恢复损坏的表格数据。TABERT net侧重于注意机制，并在每一步选择特征进行推理，另一方面，TABERT学习自然语言句子和半结构化表格的表示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nl"><img src="../Images/d7310858625579428157df93d6882115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lHcU6vyIQ3sz0CLzwBvKA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图4:该图摘自TabNet论文[7]</figcaption></figure><p id="825c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最近的一项工作(<a class="ae le" href="https://vanderschaar-lab.com/papers/NeurIPS2020_VIME.pdf" rel="noopener ugc nofollow" target="_blank">VIME</a>)【9】提出了一种新颖的托辞任务，利用新颖的损坏样本生成技术来恢复掩码向量和原始样本。作者还提出了一种新的表格数据扩充机制，该机制可以与对比学习相结合，以扩展表格数据的监督学习。正如我在之前的<a class="ae le" href="https://towardsdatascience.com/data-transformation-methods-deep-neural-networks-for-tabular-data-8d9ebdeacc16" rel="noopener" target="_blank">博客</a>::<em class="mk">中所解释的，一个被破坏的样本是使用生成二进制掩码向量和输入样本的掩码生成器创建的。注意，输入样本是从未标记的数据集</em>中生成的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/ad534bad79db1677687ae34d201978fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_QQpG1lJcTa9_EkUK21qrg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图5:该图摘自Yoon等人的论文[9]</figcaption></figure><p id="74ba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">自我监督学习是深度学习的新规范。图像和文本数据的自我监督学习技术是惊人的，因为它们分别依赖于空间和序列相关性。但是，表格数据中没有通用的相关结构。这使得表格数据的自我监督学习更具挑战性。</p><p id="c889" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">感谢阅读我的文章。直到下一次…</p><p id="83ca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">快乐阅读！</p><h1 id="05b9" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">参考资料:</h1><p id="60e1" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo nf lq lr ls ng lu lv lw nh ly lz ma ij bi translated">[1]张曦轲、菲利普·伊索拉和阿列克谢·a·埃夫罗斯，<a class="ae le" href="https://arxiv.org/pdf/1603.08511.pdf" rel="noopener ugc nofollow" target="_blank">彩色图像着色</a> (2016)，欧洲计算机视觉会议</p><p id="605c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2] Carl Doersch、Abhinav Gupta和Alexei A. Efros，<a class="ae le" href="https://arxiv.org/pdf/1505.05192.pdf" rel="noopener ugc nofollow" target="_blank">通过上下文预测进行无监督视觉表示学习</a> (2015)，IEEE计算机视觉国际会议论文集</p><p id="67db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3]景龙龙，田英丽，<a class="ae le" href="https://arxiv.org/pdf/1902.06162.pdf" rel="noopener ugc nofollow" target="_blank">深度神经网络的自监督视觉特征学习:综述</a> (2020)，IEEE模式分析与机器智能汇刊</p><p id="a3a9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4]张，，默罕默德萨勒，<a class="ae le" href="https://arxiv.org/pdf/1912.08777.pdf" rel="noopener ugc nofollow" target="_blank">帕伽索斯:用提取的间隔句进行抽象概括的预训练</a> (2020)，机器学习国际会议</p><p id="ba22" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[5] Pascal Vincent、Hugo Larochelle、Yoshua Bengio和Pierre-Antoine Manzagol，<a class="ae le" href="https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf" rel="noopener ugc nofollow" target="_blank">用去噪自动编码器提取和合成鲁棒特征</a> (2008)，载于第25届机器学习国际会议论文集</p><p id="0ca1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[6] Deepak Pathak，Philipp Krahenbuhl，Jeff Donahue，Trevor Darrell和Alexei A. Efros，<a class="ae le" href="https://arxiv.org/pdf/1604.07379.pdf" rel="noopener ugc nofollow" target="_blank">上下文编码器:通过修补进行特征学习</a> (2016)，IEEE计算机视觉和模式识别会议论文集</p><p id="4ad5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[7]塞尔詹。Arik和Tomas Pfister，Tabnet: <a class="ae le" href="https://arxiv.org/pdf/1908.07442.pdf" rel="noopener ugc nofollow" target="_blank">专注的可解释表格学习</a> (2021)，AAAI人工智能会议论文集</p><p id="37ee" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[8]尹鹏程，Graham Neubig，Wen-tau Yih，Sebastian Riedel，<a class="ae le" href="https://arxiv.org/abs/2005.08314" rel="noopener ugc nofollow" target="_blank"> TaBERT:联合理解文本和表格数据的预处理</a> (2020)，计算语言学协会第58届年会会议录</p><p id="24ac" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[9]尹金松、张尧、詹姆斯·乔登和米哈埃拉·范德沙尔、维梅:<a class="ae le" href="https://vanderschaar-lab.com/papers/NeurIPS2020_VIME.pdf" rel="noopener ugc nofollow" target="_blank">将自我和半监督学习的成功扩展到表格域</a> <em class="mk"> </em> (2020)，神经信息处理系统进展</p></div></div>    
</body>
</html>