<html>
<head>
<title>Gradient Descent v/s Normal Equation For Regression Problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归问题的梯度下降v/s正规方程</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/gradient-descent-v-s-normal-equation-for-regression-problems-e6c3cdd705f?source=collection_archive---------0-----------------------#2020-06-27">https://pub.towardsai.net/gradient-descent-v-s-normal-equation-for-regression-problems-e6c3cdd705f?source=collection_archive---------0-----------------------#2020-06-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="fee1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="ee27" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">选择正确的算法以找到最小化成本函数的参数</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/256aecd756a9aeb225c35146d447d338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XmGwOm_Yxqqtb1_f-UXbuA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">梯度下降v/s正常方程</figcaption></figure><blockquote class="le lf lg"><p id="95ba" class="lh li lj lk b ll lm ka ln lo lp kd lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在本文中，我们将在实际方法中看到<strong class="lk ja">梯度下降</strong>和<strong class="lk ja">法线方程</strong>之间的实际差异。大多数新手机器学习爱好者在线性回归期间了解梯度下降，并进一步前进，甚至不知道最被低估的<strong class="lk ja">正态方程</strong>，它远没有那么复杂，并为<strong class="lk ja">小</strong>到<strong class="lk ja">中等</strong>大小的数据集提供非常好的结果。</p></blockquote><p id="1912" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">如果你是机器学习的新手，或者不熟悉一个<strong class="lk ja">法线</strong>方程式<strong class="lk ja">方程式</strong>或<strong class="lk ja">梯度</strong> <strong class="lk ja">下降</strong>，不要担心我会尽力用<strong class="lk ja"> <em class="lj">俗人的</em> </strong>术语解释这些。所以，我将从解释回归问题开始。</p><h2 id="01c5" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">什么是线性回归？</h2><p id="f79e" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">就是入门级的<strong class="lk ja"> <em class="lj">有监督的</em> </strong>(特征和目标变量都给定)机器学习算法。假设，我们在空间中绘制所有这些变量，那么这里的主要任务是用这样一种方式将<strong class="lk ja"><em class="lj"/></strong>拟合<strong class="lk ja"> <em class="lj">最小化</em></strong><strong class="lk ja"><em class="lj">代价函数</em> </strong>或<strong class="lk ja"><em class="lj"/></strong>(不用担心我也会解释这一点)。有各种类型的线性回归，如简单(一个特征)、多元和逻辑(用于分类)。本文考虑了<strong class="lk ja"> <em class="lj">多元线性回归</em> </strong>。实际的回归公式是:-</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/01f6e9f16a6e9082e91c1e9f514f7f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*VUROsVB0Fjd_wnWnKv0Fjw.png"/></div></figure><p id="5992" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">其中<strong class="lk ja"> θ </strong> ₀和<strong class="lk ja"> θ </strong> ₁是我们必须以最小化<strong class="lk ja"> <em class="lj">损失</em> </strong>的方式找到的参数。在多元回归中，公式扩展得像<strong class="lk ja">θ</strong>₀+<strong class="lk ja">θ</strong>₁x₁+<strong class="lk ja">θ</strong>₂x₂.<strong class="lk ja">成本函数</strong>找出我们算法的<strong class="lk ja"> <em class="lj">实际值</em> </strong>和<strong class="lk ja"> <em class="lj">预测值</em> </strong>之间的误差。应该尽可能少。同样的公式是:-</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/3b297fa9bc6d233cd93bae9cc84d7fd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*4Vl6z2EolRUPcAzGZwE0vA.png"/></div></figure><p id="3282" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">其中m =数据集中的示例数或行数，iᵗʰ示例的xᶦ=feature值，iᵗʰ示例的yᶦ=actual结果。</p><h2 id="575b" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">梯度下降</h2><p id="c32d" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">它是一种<strong class="lk ja"> <em class="lj">优化</em> </strong>技术，寻找<strong class="lk ja"> <em class="lj">最小化</em></strong><strong class="lk ja"><em class="lj">代价函数</em> </strong>的最佳参数组合。在这种情况下，我们从参数的随机值开始(在大多数情况下<strong class="lk ja"><em class="lj"/></strong>)然后不断改变参数以减少J( <strong class="lk ja"> θ </strong> ₀，<strong class="lk ja"> θ </strong> ₁)或成本函数，直到最后达到最小值。同样的公式是:-</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a72d247fb700237cd510abefd860e4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQ0iGTXb_h1_7P2rKtS3lQ.png"/></div></div></figure><p id="8f97" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">其中<strong class="lk ja"> j </strong>表示参数的编号，<strong class="lk ja"> α </strong>表示学习率。我就不深入讨论了。你可以在这里找到这些<a class="ae ng" href="https://github.com/PushkaraSharma/medium_articles_code/tree/master/Gradientdescent_VS_NormalEquation" rel="noopener ugc nofollow" target="_blank">的手写笔记</a>。</p><h2 id="b7db" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">正态方程</h2><p id="8d42" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">这是一种不用梯度下降就能直接找到最佳参数值的方法。这是一个非常有效的算法，或者说公式(因为它只有一行😆)当您处理较小的数据集时。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e558d824ef2268f39bc9a38bf279bc13.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*w6M0avkhKBzm15aH65ZL-w.png"/></div></figure><p id="2395" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated"><strong class="lk ja"> <em class="lj">法方程</em> </strong>的唯一问题是，在大型数据集中，寻找矩阵的<strong class="lk ja"> <em class="lj">逆</em> </strong>的计算非常昂贵。</p><p id="2425" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">我知道这需要很多理论，但这是理解下面的代码片段所必需的。而我只触及了表面，所以请谷歌一下以上的主题，获取更深入的知识。</p><h1 id="df39" class="ni mi iq bd mj nj nk nl mm nm nn no mp kf np kg ms ki nq kj mv kl nr km my ns bi translated">先决条件:</h1><p id="37ac" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">我假设你熟悉<strong class="lk ja"> <em class="lj"> python </em> </strong>并且已经在你的系统中安装了<strong class="lk ja"> <em class="lj"> python 3 </em> </strong>。这个教程我用了一个<strong class="lk ja"> <em class="lj"> jupyter笔记本</em> </strong>。你可以使用你喜欢的<strong class="lk ja"> IDE </strong>。所有需要的库都内置在<strong class="lk ja"> <em class="lj"> anaconda </em> </strong>套件中。</p><h1 id="de16" class="ni mi iq bd mj nj nk nl mm nm nn no mp kf np kg ms ki nq kj mv kl nr km my ns bi translated">让我们编码</h1><p id="9367" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">我使用的数据集由<strong class="lk ja"> 3 </strong>列组成，其中两列被认为是<strong class="lk ja">特征</strong>，另一列是<strong class="lk ja">目标</strong>变量。<a class="ae ng" href="https://github.com/PushkaraSharma/medium_articles_code/tree/master/Gradientdescent_VS_NormalEquation" rel="noopener ugc nofollow" target="_blank"> <strong class="lk ja"> GithHub </strong> </a>中可用的数据集。</p><p id="e0dd" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">首先，我们需要导入我们将在本研究中使用的库。这里，<code class="fe nt nu nv nw b">numpy</code>用于创建NumPy数组，用于训练和测试数据。<code class="fe nt nu nv nw b">pandas</code>用于制作数据集的数据框架，方便取值。<code class="fe nt nu nv nw b">matplotlib.pyplot</code>绘制数据，如整体股票价格和预测价格。<code class="fe nt nu nv nw b">mpl_toolkits</code>用于绘制3d数据，<code class="fe nt nu nv nw b">sklearn</code>用于分割数据集和计算精度。我们还导入了<code class="fe nt nu nv nw b">time</code>来计算每个算法花费的时间。</p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="95be" class="mh mi iq nw b gy ob oc l od oe"><strong class="nw ja">import</strong> pandas as <strong class="nw ja">pd</strong><br/><strong class="nw ja">import</strong> numpy as <strong class="nw ja">np</strong><br/><strong class="nw ja">from</strong> mpl_toolkits <strong class="nw ja">import</strong> mplot3d<br/><strong class="nw ja">import</strong> matplotlib.<strong class="nw ja">pyplot</strong> as plt<br/><strong class="nw ja">from</strong> sklearn.metrics <strong class="nw ja">import</strong> mean_squared_error<br/><strong class="nw ja">from</strong> sklearn.model_selection <strong class="nw ja">import</strong> train_test_split<br/><strong class="nw ja">import</strong> time</span></pre><p id="100e" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">我们已经将数据集加载到<strong class="lk ja"> <em class="lj"> pandas </em> </strong> <code class="fe nt nu nv nw b">dataframe</code>中，数据集的形状显示为<strong class="lk ja"> (1000，3) </strong>，然后我们简单地打印带有<code class="fe nt nu nv nw b">head()</code>的前<strong class="lk ja"> 5 </strong>行。</p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="a865" class="mh mi iq nw b gy ob oc l od oe"><strong class="nw ja">df</strong> = <strong class="nw ja">pd</strong>.read_csv('student.csv')<br/>print(<strong class="nw ja">df</strong>.shape)<br/><strong class="nw ja">df</strong>.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6060d920a0239dcad12b4c25b0303c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*oeFuqtzGhdHpGy6-l_CIxw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">所用数据集的预览</figcaption></figure><p id="8696" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">在这里，<strong class="lk ja"> <em class="lj">特性</em> </strong>值即<strong class="lk ja"> <em class="lj">数学</em> </strong>和<strong class="lk ja"> <em class="lj">读数</em> </strong>被保存在变量<strong class="lk ja"><em class="lj">【X1】</em></strong>和<strong class="lk ja"><em class="lj">【X2】</em></strong>中作为NumPy数组而<strong class="lk ja"> <em class="lj">写作</em> </strong>列则被认为是<strong class="lk ja"> <em class="lj">目标</em> </strong>变量然后我们绘制了这个3D数据。</p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="5d7d" class="mh mi iq nw b gy ob oc l od oe"><strong class="nw ja">X1</strong> = <strong class="nw ja">df</strong>['Math'].values<br/><strong class="nw ja">X2</strong> = <strong class="nw ja">df</strong>['Reading'].values<br/><strong class="nw ja">Y</strong> = <strong class="nw ja">df</strong>['Writing'].values</span><span id="484a" class="mh mi iq nw b gy og oc l od oe">ax = plt.axes(projection='3d')<br/>ax.scatter(X1, X2, Y, c=Y, cmap='viridis', linewidth=0.5);</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/508fd343f8e50d8f963b75b2b4ec98a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*pXZHDFD_TTse17eizppYPA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">数据的三维可视化</figcaption></figure><p id="4da8" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">现在，<strong class="lk ja"> <em class="lj"> X₀ </em> </strong>被初始化为一个numpy数组，该数组由与其他特征具有相同维数的元素组成(其作用类似于bias)。之后，我们将所有的特性分组到一个变量中，并将它们转换成正确的格式。然后借助<code class="fe nt nu nv nw b">train_test_split</code>和<strong class="lk ja"> <em class="lj">将数据拆分为训练和测试，测试大小</em> </strong>为<strong class="lk ja">±5%</strong>即<strong class="lk ja"> <em class="lj"> 50行</em> </strong>进行测试。下面的截图给出了这些形状。</p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="fd1b" class="mh mi iq nw b gy ob oc l od oe"><strong class="nw ja">X0</strong> = <strong class="nw ja">np</strong>.ones(len(<strong class="nw ja">X1</strong>))<br/><strong class="nw ja">X</strong> = <strong class="nw ja">np</strong>.array([<strong class="nw ja">X0</strong>,<strong class="nw ja">X1</strong>,<strong class="nw ja">X2</strong>]).T<br/><strong class="nw ja">x_train</strong>,<strong class="nw ja">x_test</strong>,<strong class="nw ja">y_train</strong>,<strong class="nw ja">y_test</strong> = train_test_split(<strong class="nw ja">X</strong>,<strong class="nw ja">Y</strong>,test_size=0.05)<br/>print("X_train shape:",<strong class="nw ja">x_train</strong>.shape,"\nY_train shape:",<strong class="nw ja">y_train</strong>.shape)<br/>print("X_test shape:",<strong class="nw ja">x_test</strong>.shape,"\nY_test shape:",<strong class="nw ja">y_test</strong>.shape)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/0b46d0b32162785f01a58865b82ceb1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*bkh5Cvgl2hN9onprl010gw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">训练、测试集的形状</figcaption></figure><p id="00f3" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">这里，<strong class="lk ja"> <em class="lj"> Q </em> </strong>表示参数列表，在我们的例子中是3个(X₀，X₁，X₂)，它们被初始化为(0，0，0)。<strong class="lk ja"> <em class="lj"> n </em> </strong>只是一个值等于训练样本数的整数。然后我们定义了我们的<strong class="lk ja">成本函数</strong>，它将在<strong class="lk ja"> <em class="lj">梯度下降</em> </strong>函数中使用，以计算每个参数组合的<em class="lj">成本</em>。</p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="e1db" class="mh mi iq nw b gy ob oc l od oe"><strong class="nw ja">Q</strong> = <strong class="nw ja">np</strong>.zeros(3)<br/><strong class="nw ja">n</strong> = len(<strong class="nw ja">X1</strong>)<br/>def cost_function(<strong class="nw ja">X</strong>,<strong class="nw ja">Y</strong>,<strong class="nw ja">Q</strong>):<br/>    return <strong class="nw ja">np</strong>.sum(((<strong class="nw ja">X</strong>.dot(<strong class="nw ja">Q</strong>)-<strong class="nw ja">Y</strong>)**2)/(2*<strong class="nw ja">n</strong>))</span></pre><p id="e0e6" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">这是<strong class="lk ja"> <em class="lj">梯度下降</em> </strong>函数，以<strong class="lk ja">特征</strong>、<strong class="lk ja">目标</strong>变量、<strong class="lk ja">参数</strong>、<strong class="lk ja">历元</strong>(迭代次数)、以及<strong class="lk ja"> alpha </strong>(学习速率)为自变量。在该函数中，<code class="fe nt nu nv nw b">cost_history</code>被初始化以追加每个参数组合的成本。之后，我们开始一个循环，重复寻找参数的过程。然后，我们计算<strong class="lk ja"> <em class="lj">损失</em> </strong>和梯度项，并更新参数集。在这里，你看不到<em class="lj">偏导数</em>项，因为这里的公式是在计算偏导数后使用的(作为参考，见上述公式中的平方项被分母中的2抵消)。最后，我们调用成本函数来计算成本，并将其添加到<code class="fe nt nu nv nw b">cost_history</code>中。</p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="2616" class="mh mi iq nw b gy ob oc l od oe">def <strong class="nw ja">gradient_descent</strong>(<strong class="nw ja">X</strong>,<strong class="nw ja">Y</strong>,<strong class="nw ja">Q</strong>,<strong class="nw ja">epochs</strong>,<strong class="nw ja">alpha</strong>):<br/>    cost_history = <strong class="nw ja">np</strong>.zeros(epochs)<br/>    for <strong class="nw ja">i</strong> in range(<strong class="nw ja">epochs</strong>):<br/>        <strong class="nw ja">pred</strong> = X.dot(Q)<br/>        <strong class="nw ja">loss</strong> = pred-Y<br/>        <strong class="nw ja">gradient</strong> = X.T.dot(loss)/n<br/>        <strong class="nw ja">Q</strong> = Q-gradient*alpha<br/>        cost_history[i] = <strong class="nw ja">cost_function</strong>(X,Y,Q)<br/>    return cost_history,Q</span></pre><p id="581d" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">这里，在调用上述函数之前，我们已经启动了<strong class="lk ja"> <em class="lj">定时器</em> </strong>，并设置<strong class="lk ja"> <em class="lj"> epochs =1000 </em> </strong>和<strong class="lk ja"> <em class="lj"> alpha =0.0001 </em> </strong>(应尽可能低)，定时器在函数执行后立即停止。所以我们的<strong class="lk ja">梯度下降</strong>大约需要<strong class="lk ja"> 82毫秒</strong>来执行(1000个历元)。</p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="f5a7" class="mh mi iq nw b gy ob oc l od oe">start = time.time()<br/><strong class="nw ja">cost_his</strong>,<strong class="nw ja">parameters</strong> = <strong class="nw ja">gradient_descent</strong>(x_train,y_train.flatten(),Q,1000,0.0001)<br/>end = time.time()<br/>print(end - start)</span></pre><p id="489c" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">在这里，我们绘制了<code class="fe nt nu nv nw b">cost_history</code>的图表。正如我们可以看到的，图形在大约400个时期收敛，所以我运行梯度下降函数，时期=400，这一次花费了大约25.3毫秒<strong class="lk ja">。这个你可以用我的<a class="ae ng" href="https://github.com/PushkaraSharma/medium_articles_code/tree/master/Gradientdescent_VS_NormalEquation" rel="noopener ugc nofollow" target="_blank"><strong class="lk ja"><em class="lj">GitHub</em></strong></a>的笔记本自己测试。</strong></p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="8e69" class="mh mi iq nw b gy ob oc l od oe"><strong class="nw ja">x</strong> = [i for i in range(1,1001)]<br/>plt.plot(x,cost_his)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/31a7c62ece89fdc350e759137e7ed4fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*-t99JKn4Q3Med6mMk88-FA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">迭代V/S成本</figcaption></figure><p id="7714" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">考验的时候到了。<code class="fe nt nu nv nw b">mean squared error</code>出来是在<strong class="lk ja"> 3.86 </strong>左右，这是非常可以接受的。</p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="151b" class="mh mi iq nw b gy ob oc l od oe"><strong class="nw ja">y_pred</strong> = <strong class="nw ja">x_test</strong>.dot(parameters)<br/><strong class="nw ja">np</strong>.sqrt(mean_squared_error(<strong class="nw ja">y_pred</strong>,<strong class="nw ja">y_test</strong>))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/917e5e6fa5e4e0d4ae287c4ccbbb12c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*-vQTNFRAvMhtU_ctBtKZaw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">使用梯度下降的精度</figcaption></figure><p id="9b24" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated"><strong class="lk ja">法线方程</strong></p><p id="261a" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated"><strong class="lk ja"> <em class="lj">正规方程</em> </strong>非常简单，你可以在代码本身中看到这一点(只有<strong class="lk ja"> <em class="lj">单行</em> </strong>行)。如上所述，我们已经测量了公式计算参数所花费的时间。不要担心不可逆矩阵，NumPy在这里涵盖了它们。大约需要<strong class="lk ja"> 3毫秒</strong>(平均)。是不是很棒！</p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="a4e6" class="mh mi iq nw b gy ob oc l od oe">start = time.time()<br/><strong class="nw ja">Q1</strong> = <strong class="nw ja">np</strong>.linalg.inv(<strong class="nw ja">x_train</strong>.T.dot(<strong class="nw ja">x_train</strong>)).dot(<strong class="nw ja">x_train</strong>.T).dot(<strong class="nw ja">y_train</strong>)<br/>end = time.time()<br/>print(end - start)</span></pre><p id="cf2e" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">最后，我们计算了法方程的<code class="fe nt nu nv nw b">mean squared error</code>，结果是<strong class="lk ja"> 3.68 </strong></p><pre class="kp kq kr ks gt nx nw ny nz aw oa bi"><span id="44b8" class="mh mi iq nw b gy ob oc l od oe"><strong class="nw ja">pred_y</strong> = <strong class="nw ja">x_test</strong>.dot(<strong class="nw ja">Q1</strong>)<br/><strong class="nw ja">np</strong>.sqrt(mean_squared_error(<strong class="nw ja">pred_y</strong>,<strong class="nw ja">y_test</strong>))</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/3c8fe653490419c6b2583062e088917d.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*0bk98vFDl7RGJfUfj8OOeQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">使用正规方程的精度</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/990b2475341792f34a00fb9c1103e547.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*71BJNVb2MYiwdlWkw3Xx6g.gif"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">考虑梯度下降前的正常方程</figcaption></figure><h1 id="3d1d" class="ni mi iq bd mj nj nk nl mm nm nn no mp kf np kg ms ki nq kj mv kl nr km my ns bi translated">结论</h1><p id="40d4" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">事实证明，<strong class="lk ja"> <em class="lj">法方程</em> </strong>计算参数花费的时间更少，给出的结果在精度方面几乎相同，并且非常容易使用。在我看来，在数据集规模不太大<strong class="lk ja"> (~2万)的情况下，<strong class="lk ja"> <em class="lj">法方程</em> </strong>比<strong class="lk ja"> <em class="lj">梯度下降</em> </strong>。</strong>由于当今现代系统良好的计算能力，法线方程是回归情况下首先要考虑的算法。</p><p id="e704" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">源代码在<a class="ae ng" href="https://github.com/PushkaraSharma/medium_articles_code/tree/master/Gradientdescent_VS_NormalEquation" rel="noopener ugc nofollow" target="_blank"> <strong class="lk ja"> GitHub </strong> </a>上有。请随意改进。</p><p id="0ebe" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">谢谢你宝贵的时间。😊我希望你喜欢这个教程。</p><p id="4297" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">还有，查一下我的教程上的一个<a class="ae ng" href="https://towardsdatascience.com/simple-text-summarizer-using-extractive-method-849b65c2dc5a" rel="noopener" target="_blank"> <strong class="lk ja"> <em class="lj">简单文字总结器</em> </strong> </a>。</p><div class="om on gp gr oo op"><a href="https://towardsdatascience.com/simple-text-summarizer-using-extractive-method-849b65c2dc5a" rel="noopener follow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd ja gy z fp ou fr fs ov fu fw iz bi translated">使用提取方法的简单文本摘要</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">自动对包含最重要句子的文章进行简短总结。</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ky op"/></div></div></a></div><div class="om on gp gr oo op"><a href="https://medium.com/towards-artificial-intelligence/predict-the-stock-trend-using-deep-learning-5a4b7df1d152" rel="noopener follow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd ja gy z fp ou fr fs ov fu fw iz bi translated">使用深度学习预测股票趋势</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">使用深度学习模型(递归神经网络)预测股票的未来趋势</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">medium.com</p></div></div><div class="oy l"><div class="pe l pa pb pc oy pd ky op"/></div></div></a></div></div></div>    
</body>
</html>