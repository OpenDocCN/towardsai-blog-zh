<html>
<head>
<title>Making Video Conferencing more Accessible with Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过机器学习让视频会议变得更加容易</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/making-video-conferencing-more-accessible-with-machine-learning-3cfd66febd8c?source=collection_archive---------1-----------------------#2020-12-01">https://pub.towardsai.net/making-video-conferencing-more-accessible-with-machine-learning-3cfd66febd8c?source=collection_archive---------1-----------------------#2020-12-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="dd76" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="81f1" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">学会为视力受损或有其他障碍的人建造Azure供电的管道</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/bb9ddbbf6c42fba9158cf3237cda6bdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Mqha5muQ2-YIalwj"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@joshcala?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔希·卡拉布雷斯</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="4024" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">新冠肺炎疫情把组织推到了边缘，大多数雇主求助于在家工作(WFH)安排。除了工作，随着严格的全球旅行限制，人们正在慢慢适应视频会议模式，以保持与朋友和家人的联系。这个疫情孕育了像微软团队和Zoom这样的平台的崛起，在过去的六个月里，这些平台已经出现了巨大的增长模式[1]。</p><p id="3403" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文中，我们将学习构建一个基于Microsoft Azure的管道，实时检测Microsoft Teams ( <em class="mb"> Teams </em>)呼叫中参与者的情绪和面部表情。我们还将增加一项功能，帮助参与者在通话开始前正确对准摄像机。</p><blockquote class="mc md me"><p id="dcba" class="lf lg mb lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">d<!-- -->isclaimer——该项目最初是为<a class="ae le" href="https://www.microsoftaccessibility.ai/" rel="noopener ugc nofollow" target="_blank">微软人工智能无障碍黑客马拉松</a>而构建的，后来被评为新加坡五大提交作品之一，并被微软&amp;合作伙伴评委评为亚太地区的顶级提交作品。我要感谢我的队友们<a class="ae le" href="https://au.linkedin.com/in/aditya-bansal-43b211195" rel="noopener ugc nofollow" target="_blank"> Aditya </a>，<a class="ae le" href="https://sg.linkedin.com/in/ritwikkanodia" rel="noopener ugc nofollow" target="_blank">rit wik</a>&amp;<a class="ae le" href="https://my.linkedin.com/in/vidurveer-duggal-1b941b132" rel="noopener ugc nofollow" target="_blank">Vidurveer</a>，没有他们，这一节选是不可能的。</p></blockquote></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><h1 id="b079" class="mp mq iq bd mr ms mt mu mv mw mx my mz kf na kg nb ki nc kj nd kl ne km nf ng bi translated"><strong class="ak">背景</strong></h1><p id="cc1a" class="pw-post-body-paragraph lf lg iq lh b li nh ka lk ll ni kd ln lo nj lq lr ls nk lu lv lw nl ly lz ma ij bi translated">虽然视频会议有其好处，但对于那些在各个方面有视觉障碍和视觉障碍的人来说，它严重缺乏可访问性。</p><p id="da68" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于无数的疾病，不仅限于社交情绪失认症、自闭症、阅读障碍和焦虑症，这给这些人理解他人的情绪和面部表情，并在视频会议上做出适当反应带来了挑战。在这种情况下，他们对视频电话感到不舒服，而在虚拟工作面试中，解读面试官的面部表情对他们来说至关重要。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/1425728143b6999473feb81a34fd3e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uW19qeX4WlTsNOt8OaeCbA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">为什么向视频会议平台添加更多辅助功能非常重要</figcaption></figure><p id="6ca3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第二个问题是，由于这些残疾，人们很难让自己意识到这样一个事实，即使他们被其他人看到，如果他们的相机对准正确。</p><h1 id="cd32" class="mp mq iq bd mr ms nn mu mv mw no my mz kf np kg nb ki nq kj nd kl nr km nf ng bi translated">管道</h1><p id="eed8" class="pw-post-body-paragraph lf lg iq lh b li nh ka lk ll ni kd ln lo nj lq lr ls nk lu lv lw nl ly lz ma ij bi translated">构建我们的管道有三个步骤，首先，我们在设定的时间间隔后，比如说每两秒钟，从视频会议中捕获帧(截图)。之后，我们将这些帧发送到Microsoft Azure APIs，以获取通话中个人的情绪(情感)，然后，最终在屏幕上呈现检测到的情绪。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/5431518ce1ae173e14b6b910cdc01cd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rC87p_0Uldj9eRwDwofxlA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">三步Azure驱动的管道</figcaption></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nt nu l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">样板Python类为我们管道中的三个步骤提供了不同的方法</figcaption></figure><h2 id="991f" class="nv mq iq bd mr nw nx dn mv ny nz dp mz lo oa ob nb ls oc od nd lw oe of nf iw bi translated">捕捉帧</h2><p id="f794" class="pw-post-body-paragraph lf lg iq lh b li nh ka lk ll ni kd ln lo nj lq lr ls nk lu lv lw nl ly lz ma ij bi translated">该过程的第一步是在设定的持续时间后，从正在进行的视频通话中捕获帧。我们直接截取了电脑屏幕的截图，因为目前还没有直接从团队电话中获取视频的选项。</p><p id="3f6d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在Python的跨平台'<em class="mb"> pyautogui' </em>库的帮助下，截图的过程很简单。截图后，在我们的本地目录中以<em class="mb">‘screen . png’</em>保存。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nt nu l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">方法在Python中截图</figcaption></figure><h2 id="84f4" class="nv mq iq bd mr nw nx dn mv ny nz dp mz lo oa ob nb ls oc od nd lw oe of nf iw bi translated">使用Azure Face API</h2><p id="d501" class="pw-post-body-paragraph lf lg iq lh b li nh ka lk ll ni kd ln lo nj lq lr ls nk lu lv lw nl ly lz ma ij bi translated">其次，在一帧被捕获后，我们调用<a class="ae le" href="https://azure.microsoft.com/en-us/services/cognitive-services/face/" rel="noopener ugc nofollow" target="_blank"> Azure Face API </a>来检测帧中人的情绪。为简单起见，我假设视频通话中只有一个人。</p><p id="cf8d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Face API是认知服务的一部分，由Azure提供，具有一种称为“感知情感识别”的功能，可以检测感知到的面部表情，如愤怒、快乐和惊讶。Face API是免费的，每分钟最多20次API调用，这对于我们的用例来说应该足够了。</p><p id="3374" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面提到的代码是来自Azure Face API的示例JSON响应。我们注意到API给出的无数情绪以及它们的置信区间。例如，我们可以在下面的JSON响应中看到，对于给定的样本图像，60%的置信度下“快乐”情绪最高。API给出了更多的数据，如位置和面部特征的存在，如头发颜色、微笑和头部姿势，但我选择在这里省略这些。</p><pre class="kp kq kr ks gt og oh oi oj aw ok bi"><span id="676c" class="nv mq iq oh b gy ol om l on oo">[<br/>   {<br/>      "faceId":"92e6d028-188a-40bc-9696-6ccc54831ead",<br/>      "faceRectangle":{<br/>         "top":636,<br/>         "left":1249,<br/>         "width":403,<br/>         "height":403<br/>      },<br/>      "faceAttributes":{<br/>         "emotion":{<br/>            "anger":0.0,<br/>            "contempt":0.087,<br/>            "disgust":0.0,<br/>            "fear":0.0,<br/>            "happiness":0.606,<br/>            "neutral":0.307,<br/>            "sadness":0.0,<br/>            "surprise":0.0<br/>         }<br/>      }<br/>   }<br/>]</span></pre><p id="947f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在Python中调用API的过程非常简单。首先，我们配置通用的API端点和唯一的API订阅密钥。这项服务在Azure中是免费的，按照<a class="ae le" href="https://docs.microsoft.com/en-us/azure/cognitive-services/Face/Quickstarts/client-libraries?pivots=programming-language-python&amp;tabs=visual-studio" rel="noopener ugc nofollow" target="_blank"> Azure文档</a>中的“<em class="mb">先决条件</em>标签下的三个快速步骤就可以轻松获得密钥。</p><p id="e4b1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">配置完成后，我们只需读取第一步中获取的帧，为API配置参数，以便只获取所需的信息并调用API。之后，如果响应为空，则没有检测到人脸，否则，我们从JSON响应中返回最自信的情绪。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nt nu l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">方法来调用Python中的Azure Face API</figcaption></figure><h2 id="f462" class="nv mq iq bd mr nw nx dn mv ny nz dp mz lo oa ob nb ls oc od nd lw oe of nf iw bi translated">在屏幕上渲染情绪</h2><p id="b1aa" class="pw-post-body-paragraph lf lg iq lh b li nh ka lk ll ni kd ln lo nj lq lr ls nk lu lv lw nl ly lz ma ij bi translated">最后，在团队呼叫中检测到个体的情绪后，我们将它显示在屏幕上。在这个过程中，我使用了<em class="mb"> Tkinter </em> Python库，但是也可以随意使用自己喜欢的库。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi op"><img src="../Images/97ab83259f8d492394669bfdf95d404f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*86SHpap-sD7dGcNzOs1t1g.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">所有表情符号都是从<a class="ae le" href="https://openmoji.org/" rel="noopener ugc nofollow" target="_blank"> OpenMoji </a>获得的，这是一个<a class="ae le" href="https://creativecommons.org/licenses/by-sa/4.0/#" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a>表情符号库</figcaption></figure><p id="e688" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，我创建了一个本地目录，其中包含不同情绪的表情图像，如上所示。该方法只是获取表情名称作为参数，并将其显示在屏幕上的正确位置。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nt nu l"/></div></figure><h2 id="9709" class="nv mq iq bd mr nw nx dn mv ny nz dp mz lo oa ob nb ls oc od nd lw oe of nf iw bi translated">驱动程序代码</h2><p id="a3f7" class="pw-post-body-paragraph lf lg iq lh b li nh ka lk ll ni kd ln lo nj lq lr ls nk lu lv lw nl ly lz ma ij bi translated">最后一部分是驱动程序代码，通过它，我们结合了上面写的所有方法来运行我们的程序。您可以更改“<em class="mb">模式</em>变量，在视频会议期间检测情绪和通话开始前检测面部是否正确对齐之间切换。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/17e0fbd042f4aa5c9122d3945204a9c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qxLI0upav0foyN2JAfJ8IQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">团队通话中的实时面部表情检测</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi or"><img src="../Images/d7cd0d3acc736006599998e484e41403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBhYSn7wVOBL5ugDS5eSqA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">(从视障人士的视点故意模糊)左:脸部可见(“勾号”图标)，右:脸部不清晰(“十字”图标)</figcaption></figure><h1 id="cacb" class="mp mq iq bd mr ms nn mu mv mw no my mz kf np kg nb ki nq kj nd kl nr km nf ng bi translated">包裹</h1><p id="f754" class="pw-post-body-paragraph lf lg iq lh b li nh ka lk ll ni kd ln lo nj lq lr ls nk lu lv lw nl ly lz ma ij bi translated">我们已经创建的Azure pipeline是一个原型，它可以潜在地集成到视频会议平台中，作为视觉障碍者和受损者的辅助功能。</p><p id="1a23" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们的方法有各种各样的缺点，最突出的是缺乏这个程序与微软团队平台本身的集成。在撰写本文时，由于开发人员的限制，不可能在Teams或Zoom中构建端到端的集成解决方案。然而，正如我们所知，这些应用程序正在快速发展，在不久的将来紧密集成是可能的。第二，假设视频会议中有一个人，这通常不是事实。</p><p id="919e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请随意试验，让代码变得更好。如果你觉得还有其他改进的方法，请留下评论。</p><h2 id="dbec" class="nv mq iq bd mr nw nx dn mv ny nz dp mz lo oa ob nb ls oc od nd lw oe of nf iw bi translated">参考</h2><p id="276f" class="pw-post-body-paragraph lf lg iq lh b li nh ka lk ll ni kd ln lo nj lq lr ls nk lu lv lw nl ly lz ma ij bi translated">[1]斯帕塔罗，J. (2020年，10月30日)。微软团队达到1.15亿DAU以上，这是微软365新的每日协作分钟数指标。</p><p id="9461" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]赵，y，吴，s，雷诺兹，l .，&amp;阿岑科特，S. (2018)。一个面向视觉障碍人士的人脸识别应用程序。<em class="mb">2018年中国计算机学会计算系统中人的因素会议论文集——中国计算机学会</em> '18。土井指数:10.1145/317538363637</p></div></div>    
</body>
</html>