<html>
<head>
<title>Hand-Segmentation Application(Ego-Hands Dataset) using Monk AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Monk AI的手部分割应用程序(Ego-Hands数据集)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/hand-segmentation-application-ego-hands-dataset-using-monk-ai-d0f67c0b776c?source=collection_archive---------3-----------------------#2020-08-20">https://pub.towardsai.net/hand-segmentation-application-ego-hands-dataset-using-monk-ai-d0f67c0b776c?source=collection_archive---------3-----------------------#2020-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="45a2" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><blockquote class="jz ka kb"><p id="153a" class="kc kd ke kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la im bi translated">使用Monk，低代码深度学习工具和计算机视觉的统一包装器，使计算机视觉变得简单。</p></blockquote><h1 id="0383" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">介绍</h1><p id="dd6a" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">在本教程中，我们将制作一个从EgoHands数据集中分割手部图像的应用程序。这种手分割系统可以用于手势识别应用。这些手势识别应用程序帮助残疾人和老年人进行日常活动，如传达简单信息或在简单手势的帮助下控制机器。Monk toolkit允许我们使用Monk的低级代码语法部署模型，从而帮助我们创建真实世界的计算机视觉应用程序。Monk的不同深度学习管道的单线安装使我们的工作没有错误。</p><h1 id="ae9c" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">使用Monk创建真实世界的图像分割应用程序</h1><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/3ec1693cc159388d3ff1ca4690ae210f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*IAvS0Q9U4CijKZ0l_gWjyw.gif"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">卫星图像中的道路分割</figcaption></figure><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/dafe712cf057fa1eef44ed794eea4f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*b3xN04eTMgSavhiVGNxQWA.gif"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">超声神经分割</figcaption></figure><h1 id="bb15" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">关于数据集</h1><p id="a8a7" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">EgoHands数据集包含48个谷歌眼镜视频，这些视频是两个人之间复杂的第一人称互动。从上面提供的链接下载<strong class="kf jd">标签为data.zip </strong>的文件夹。这个zip文件夹包含所有标记为JPEG文件的框架。48个视频中的每一个都有100个标记帧，总共4800个帧。该文件夹包含不同对的四个参与者的图像，这些参与者彼此面对，从事不同的活动。<br/>活动包括:<br/> —打牌<br/> —下棋<br/> —解24或48块拼图<br/> —玩叠人偶</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/5d86b9ab4387a70c4c4439a86397e1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*qt_gr0kfe77AGWGvmOF_dA.gif"/></div></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="aab2" class="lb lc it bd ld le nb lg lh li nc lk ll lm nd lo lp lq ne ls lt lu nf lw lx ly bi translated">目录</h1><h2 id="0855" class="ng lc it bd ld nh ni dn lh nj nk dp ll mb nl nm lp md nn no lt mf np nq lx iz bi translated">1.安装说明</h2><h2 id="852a" class="ng lc it bd ld nh ni dn lh nj nk dp ll mb nl nm lp md nn no lt mf np nq lx iz bi translated">2.使用已经训练好的模型</h2><h2 id="72f7" class="ng lc it bd ld nh ni dn lh nj nk dp ll mb nl nm lp md nn no lt mf np nq lx iz bi translated">3.训练自定义分割器</h2><p id="f19e" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated"><strong class="kf jd"> —创建数据集的步骤<br/> —将数据集分为训练和验证数据集<br/> —训练</strong></p><h2 id="b3c6" class="ng lc it bd ld nh ni dn lh nj nk dp ll mb nl nm lp md nn no lt mf np nq lx iz bi translated">4.推理模型</h2></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="b2e0" class="lb lc it bd ld le nb lg lh li nc lk ll lm nd lo lp lq ne ls lt lu nf lw lx ly bi translated">安装说明</h1><p id="d203" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">在进入细分部分之前，我们将在我们正在工作的平台上设置Monk AI toolkit及其依赖项，我使用Google Colab作为我的环境。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="a730" class="ng lc it ns b gy nw nx l ny nz">! git clone <a class="ae oa" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection.git" rel="noopener ugc nofollow" target="_blank">https://github.com/Tessellate-Imaging/Monk_Object_Detection.git</a></span><span id="ca12" class="ng lc it ns b gy ob nx l ny nz"># For colab use the command below</span><span id="7f12" class="ng lc it ns b gy ob nx l ny nz">! cd Monk_Object_Detection/9_segmentation_models/installation &amp;&amp; cat requirements_colab.txt | xargs -n 1 -L 1 pip install</span><span id="6f36" class="ng lc it ns b gy ob nx l ny nz"># For Local systems and cloud select the right CUDA version</span><span id="e28c" class="ng lc it ns b gy ob nx l ny nz">#! cd Monk_Object_Detection/9_segmentation_models/installation &amp;&amp; cat requirements_cuda10.0.txt | xargs -n 1 -L 1 pip install</span></pre><h1 id="bec0" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">使用已经训练好的模型。</h1><p id="b212" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">Monk允许我们使用预先训练的模型来演示我们的应用程序。我们可以使用预先训练的模型并推断一些EgoHands图像的分割。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="df7b" class="ng lc it ns b gy nw nx l ny nz">import os</span><span id="b519" class="ng lc it ns b gy ob nx l ny nz">import sys</span><span id="b945" class="ng lc it ns b gy ob nx l ny nz">sys.path.append("Monk_Object_Detection/9_segmentation_models/lib/");</span><span id="0b9b" class="ng lc it ns b gy ob nx l ny nz">from infer_segmentation import Infer</span><span id="6bf4" class="ng lc it ns b gy ob nx l ny nz">gtf = Infer();</span></pre><p id="bea9" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">在每幅图像中，我们必须区分手和背景的细节。因此，字典class_dict将有两个键值对。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="34b9" class="ng lc it ns b gy nw nx l ny nz">classes_dict = {</span><span id="c63b" class="ng lc it ns b gy ob nx l ny nz">'background': 0,</span><span id="3995" class="ng lc it ns b gy ob nx l ny nz">'hand': 1</span><span id="3594" class="ng lc it ns b gy ob nx l ny nz">};</span></pre><p id="e5d5" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">该模型将只针对<strong class="kf jd">手</strong>类进行训练。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="4ea8" class="ng lc it ns b gy nw nx l ny nz">classes_to_train = ['hand'];</span></pre><p id="563b" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">下一步将是指定数据参数。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="415a" class="ng lc it ns b gy nw nx l ny nz">gtf.Data_Params(classes_dict, classes_to_train, image_shape=[716,1024])</span></pre><p id="f54d" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">现在我们将下载训练好的模型。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="bd19" class="ng lc it ns b gy nw nx l ny nz">! wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&amp;confirm=$(wget --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=1c97ms04BVQKS3KH6TZ87KSEJkXCl-Qma' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&amp;id=1c97ms04BVQKS3KH6TZ87KSEJkXCl-Qma" -O seg_hand_trained.zip &amp;&amp; rm -rf /tmp/cookies.txt</span></pre><p id="aedc" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">seg_hand_trained.zip文件夹将包含已训练的模型文件和一个包含待测试图像的测试图像文件夹，下一步是解压缩该文件夹。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="c80a" class="ng lc it ns b gy nw nx l ny nz">! unzip -qq seg_hand_trained.zip</span></pre><p id="4af8" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">在下一步中，我们指定模型参数。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="1da9" class="ng lc it ns b gy nw nx l ny nz">gtf.Model_Params(model="Unet", backbone="efficientnetb3", path_to_model='seg_hand_trained/best_model.h5')</span></pre><p id="1f16" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">现在，我们将建立模型并预测细分。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="3349" class="ng lc it ns b gy nw nx l ny nz">gtf.Setup();</span></pre><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/3e8339955e90745095ee80e1b15a3709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*lFfwL3mkLbujlI2I8wX9vg.gif"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">预测分割</figcaption></figure><h1 id="529e" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">训练自定义分割器</h1><p id="d501" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">现在，我们将创建一个自定义分段器，为此，第一步也是最重要的一步是数据集创建。从上面给出的链接下载的数据集将有EgoHands图像和MATLAB代码文件。使用这些文件，我们将为每个EgoHands图像生成蒙版图像。</p><h2 id="fe54" class="ng lc it bd ld nh ni dn lh nj nk dp ll mb nl nm lp md nn no lt mf np nq lx iz bi translated">创建数据集的步骤</h2><p id="3da4" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">—我们将从上面给出的链接中获取EgoHand图像。<br/> —从上面链接下载的文件夹也会有Matlab代码文件。<br/> —按照下载文件夹中自述文件的说明运行Matlab代码文件，将获得屏蔽图像。<br/> —所有最初保存在不同类别文件夹中的EgoHand图像将保存在一个名为Hand_Img的单独文件夹中。<br/> —运行Matlab代码后获得的屏蔽图像将存储在一个名为Hand_Annot的单独文件夹中。<br/> —我们应该确保图像列表和屏蔽标签的顺序相同，否则标签映射将不会正确。<br/> —在获得单独的文件夹Hand_Img和Hand_Annot后，我们可以将它们上传到我们的笔记本并相应地使用它们。</p><div class="mi mj mk ml gt ab cb"><figure class="oc mm od oe of og oh paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><img src="../Images/3874c55b236e1f39e7c21e62fef77afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bh9S3vVVwmoPGkk8y4oTHQ.jpeg"/></div></figure><figure class="oc mm od oe of og oh paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><img src="../Images/77c1eaf29897929b10d39c35da224d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*PPKUyjtzSMlFjUnfsGOD6g.jpeg"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk om di on oo translated">EgoHands图像及其对应的遮罩图像</figcaption></figure></div><h2 id="f2f6" class="ng lc it bd ld nh ni dn lh nj nk dp ll mb nl nm lp md nn no lt mf np nq lx iz bi translated">将数据集拆分为测试和验证数据</h2><p id="0305" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">我们现在将把图像数据集分成训练数据和验证数据。第一步是创建原始图像和屏蔽图像的排序列表。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="ac65" class="ng lc it ns b gy nw nx l ny nz">#The path given below will be the path of the two folders Hand_Annot and Hand_Img which are obtained after following steps to create the dataset.</span><span id="8864" class="ng lc it ns b gy ob nx l ny nz">import os</span><span id="c518" class="ng lc it ns b gy ob nx l ny nz">img_list = sorted(os.listdir("vision.soic.indiana.edu/projects/egohands/Hand_Img"));</span><span id="3785" class="ng lc it ns b gy ob nx l ny nz">mask_list = sorted(os.listdir("vision.soic.indiana.edu/projects/egohands/Hand_Annot"));</span></pre><p id="126b" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">下一步是为训练和验证创建一个单独的文件夹，并在其中创建两个子文件夹，分别用于EgoHand图像和相应的遮罩图像。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="a691" class="ng lc it ns b gy nw nx l ny nz">import os<br/>os.mkdir("/content/train");<br/>os.mkdir("/content/train/img31");<br/>os.mkdir("/content/train/mask31");</span><span id="a0a2" class="ng lc it ns b gy ob nx l ny nz">os.mkdir("/content/val");<br/>os.mkdir("/content/img31");<br/>os.mkdir("/content/val/mask31");</span></pre><p id="b80b" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">现在，将EgoHands图像数据分成训练和验证数据。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="594a" class="ng lc it ns b gy nw nx l ny nz">import cv2<br/>import numpy as np<br/>from tqdm.notebook import tqdm</span><span id="7a0a" class="ng lc it ns b gy ob nx l ny nz"><br/>for i in tqdm(range(len(img_list))):</span><span id="cefe" class="ng lc it ns b gy ob nx l ny nz">img_path = "vision.soic.indiana.edu/projects/egohands/Hand_Img/"+img_list[i];</span><span id="74f6" class="ng lc it ns b gy ob nx l ny nz">img = cv2.imread(img_path, 1);</span><span id="739e" class="ng lc it ns b gy ob nx l ny nz">cv2.imwrite("/content/train/img31/img"+str(i+1)+".png" ,img);</span><span id="cda7" class="ng lc it ns b gy ob nx l ny nz">for i in tqdm(range(100)):</span><span id="368a" class="ng lc it ns b gy ob nx l ny nz">img_path = "vision.soic.indiana.edu/projects/egohands/Hand_Img/"+img_list[i];</span><span id="ffad" class="ng lc it ns b gy ob nx l ny nz">img = cv2.imread(img_path, 1);</span><span id="ff66" class="ng lc it ns b gy ob nx l ny nz">cv2.imwrite("/content/val/img31/img"+str(i+1)+".png", img);</span></pre><p id="c4ad" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">现在，我们将把蒙版图像保存在training and validation文件夹的蒙版文件夹中。被屏蔽的图像在分成训练和验证之前被修改。在每个屏蔽图像中，大于0的像素值被改变为1，像素值1表示手，而0表示背景。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="df73" class="ng lc it ns b gy nw nx l ny nz">import cv2<br/>import numpy as np<br/>from tqdm.notebook import tqdm</span><span id="2217" class="ng lc it ns b gy ob nx l ny nz">for i in tqdm(range(len(mask_list))):</span><span id="8a48" class="ng lc it ns b gy ob nx l ny nz">  img_path =   "vision.soic.indiana.edu/projects/egohands/Hand_Annot/"+mask_list[i];</span><span id="c58e" class="ng lc it ns b gy ob nx l ny nz">  img = cv2.imread(img_path,0)</span><span id="a226" class="ng lc it ns b gy ob nx l ny nz">  img[img &gt; 0 ] = 1;</span><span id="a78c" class="ng lc it ns b gy ob nx l ny nz">  cv2.imwrite("/content/train/mask31/img" + str(i+1)+".png", img);</span><span id="385d" class="ng lc it ns b gy ob nx l ny nz">for i in tqdm(range(100)):</span><span id="014f" class="ng lc it ns b gy ob nx l ny nz">  img_path = "vision.soic.indiana.edu/projects/egohands/Hand_Annot/" + mask_list[i];</span><span id="7a69" class="ng lc it ns b gy ob nx l ny nz">  img = cv2.imread(img_path,0)</span><span id="cc6b" class="ng lc it ns b gy ob nx l ny nz">  img[img&gt;0] = 1;</span><span id="bd03" class="ng lc it ns b gy ob nx l ny nz">  cv2.imwrite("/content/val/mask31/img" +str(i+1)+".png", img);</span></pre><p id="047f" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated"><strong class="kf jd">最终数据集目录结构</strong></p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="33cc" class="ng lc it ns b gy nw nx l ny nz">root_dir<br/>      |<br/>      | <br/>      |         <br/>      |----train<br/>      |       |----img31<br/>      |              |<br/>      |              |---------img1.jpg<br/>      |              |---------img2.jpg<br/>      |                   |---------.........(and so on) <br/>      |<br/>      |----train<br/>      |       |----mask31<br/>      |              |<br/>      |              |---------img1.jpg<br/>      |              |---------img2.jpg<br/>      |                   |---------..........(and so on)<br/>      |<br/>      |----val (optional)<br/>      |       |----img31<br/>      |              |<br/>      |              |---------img1.jpg<br/>      |              |---------img2.jpg<br/>      |                   |---------..........(and so on)<br/>      |<br/>      |----val<br/>      |       |----mask31<br/>      |              |<br/>      |              |---------img1.jpg<br/>      |              |---------img2.jpg<br/>      |                   |---------..........(and so on)</span></pre><h2 id="e970" class="ng lc it bd ld nh ni dn lh nj nk dp ll mb nl nm lp md nn no lt mf np nq lx iz bi translated">培养</h2><p id="6539" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">这是创建自定义分段器的最后一步。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="1871" class="ng lc it ns b gy nw nx l ny nz">import os<br/>import sys</span><span id="52bb" class="ng lc it ns b gy ob nx l ny nz">sys.path.append("Monk_Object_Detection/9_segmentation_models/lib/");</span><span id="ce6e" class="ng lc it ns b gy ob nx l ny nz">from train_segmentation import Segmenter</span><span id="3f9b" class="ng lc it ns b gy ob nx l ny nz">gtf = Segmenter();</span></pre><p id="4870" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">保存EgoHands和masked图像目录的路径。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="3511" class="ng lc it ns b gy nw nx l ny nz">img_dir = "/content/train/img31";</span><span id="6204" class="ng lc it ns b gy ob nx l ny nz">mask_dir = "/content/train/mask31";</span></pre><p id="d8b0" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">两个类别‘手’和‘背景’分别由像素值1和0表示，并且我们将仅为‘手’类别训练分割模型。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="20ae" class="ng lc it ns b gy nw nx l ny nz">classes_dict = {</span><span id="ba48" class="ng lc it ns b gy ob nx l ny nz">'background': 0,</span><span id="d942" class="ng lc it ns b gy ob nx l ny nz">'hand': 1</span><span id="5652" class="ng lc it ns b gy ob nx l ny nz">};</span><span id="6dc7" class="ng lc it ns b gy ob nx l ny nz">classes_to_train = [ 'hand'];</span></pre><p id="f104" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">指定用于训练和验证的数据集参数。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="fd9c" class="ng lc it ns b gy nw nx l ny nz">gtf.Train_Dataset(img_dir, mask_dir, classes_dict, classes_to_train)</span><span id="faf2" class="ng lc it ns b gy ob nx l ny nz">img_dir = "/content/drive/My Drive/val/img31";<br/>mask_dir = "/content/drive/My Drive/val/mask31";</span><span id="a41c" class="ng lc it ns b gy ob nx l ny nz">gtf.Val_Dataset(img_dir, mask_dir)</span></pre><p id="e24f" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">获得可用于分段网络的主干网列表并选择适当的数据参数。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="93ea" class="ng lc it ns b gy nw nx l ny nz">gtf.List_Backbones();</span><span id="4e32" class="ng lc it ns b gy ob nx l ny nz">gtf.Data_Params(batch_size=10, backbone="efficientnetb3",image_shape = [720,1280])</span></pre><p id="4f10" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">获取可用模型列表并选择适当的模型参数。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="07e1" class="ng lc it ns b gy nw nx l ny nz">gtf.List_Models();</span><span id="33e9" class="ng lc it ns b gy ob nx l ny nz">gtf.Model_Params(model="Unet")</span></pre><p id="f6e2" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">建立细分模型。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="5c6a" class="ng lc it ns b gy nw nx l ny nz">gtf.Setup();</span></pre><p id="7f99" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">开始训练。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="4003" class="ng lc it ns b gy nw nx l ny nz">gtf.Train(num_epochs=5);</span></pre><p id="3bc5" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">将训练结果可视化。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="2837" class="ng lc it ns b gy nw nx l ny nz">gtf.Visualize_Training_History();</span></pre><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi op"><img src="../Images/8a19b5b359f168ee307efa12fa6c1ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aoGJ_iKbT7WkfC1fyhGy1w.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">已训练模型的Iou_score曲线和损耗曲线</figcaption></figure><h1 id="cc9d" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">推理模型</h1><p id="5902" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">它将类似于预先训练的模型。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="08bd" class="ng lc it ns b gy nw nx l ny nz">import os<br/>import sys<br/>sys.path.append("Monk_Object_Detection/9_segmentation_models/lib/");</span><span id="65e6" class="ng lc it ns b gy ob nx l ny nz">from infer_segmentation import Infer<br/>gtf = Infer();</span><span id="5b9d" class="ng lc it ns b gy ob nx l ny nz">classes_dict = {<br/>'background': 0,<br/>'hand': 1<br/>};<br/>classes_to_train = ['hand'];</span></pre><p id="fce3" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">定义数据和模型参数，并给出通过训练获得的最佳模型的路径</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="7252" class="ng lc it ns b gy nw nx l ny nz">gtf.Data_Params(classes_dict, classes_to_train, image_shape=[716,1024])</span><span id="bfc8" class="ng lc it ns b gy ob nx l ny nz">gtf.Model_Params(model="Unet", backbone="efficientnetb3", path_to_model='best_model.h5')</span></pre><p id="9300" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">现在，建立模型。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="b367" class="ng lc it ns b gy nw nx l ny nz">gtf.Setup();</span></pre><p id="9c83" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">推断图像。</p><pre class="mi mj mk ml gt nr ns nt nu aw nv bi"><span id="7023" class="ng lc it ns b gy nw nx l ny nz">from PIL import Image<br/>img = Image.open("/content/train/img31/img100.png")<br/>img = img.resize((1024,1024))<br/>img.save('tmp_img.png')</span><span id="8546" class="ng lc it ns b gy ob nx l ny nz">gtf.Predict("/content/tmp_img.png", vis=True);</span></pre><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi oq"><img src="../Images/83247325363d724c46542cbc8f567ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a9LzMxA9yxZpYC9f4BUq6w.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">预测分割</figcaption></figure><h1 id="1758" class="lb lc it bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="35b5" class="pw-post-body-paragraph kc kd it kf b kg lz ki kj kk ma km kn mb mc kq kr md me ku kv mf mg ky kz la im bi translated">我们通过使用Monk的低代码语法成功创建了手部分割应用程序，该分割系统可用于创建手势识别应用程序，有关更多此类应用程序，请参考<a class="ae oa" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection/tree/master/application_model_zoo" rel="noopener ugc nofollow" target="_blank">应用程序模型Zoo </a>。</p><p id="5e2d" class="pw-post-body-paragraph kc kd it kf b kg kh ki kj kk kl km kn mb kp kq kr md kt ku kv mf kx ky kz la im bi translated">在<a class="ae oa" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection/blob/master/application_model_zoo/Example%20-%20Hand%20segmentation%20(Ego-Hands%20Dataset).ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上有教程。</p></div></div>    
</body>
</html>