<html>
<head>
<title>Fully Explained Gradient Boosting Technique in Supervised Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督学习中的梯度推进技术</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fully-explained-gradient-boosting-technique-in-supervised-learning-d3e293ca70e1?source=collection_archive---------1-----------------------#2021-02-25">https://pub.towardsai.net/fully-explained-gradient-boosting-technique-in-supervised-learning-d3e293ca70e1?source=collection_archive---------1-----------------------#2021-02-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="3558" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="7a44" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">机器学习中的回归和分类方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/d8446c223359b16ad9023cd1862dc892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*REUu0LJytEt6kIOFQeB0Tw.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">机器学习中的梯度推进。作者的照片</figcaption></figure><p id="0497" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">大家好，在这篇文章中，我们将讨论集成提升技术，即梯度提升。在关于ensemble的早期文章中，我们讨论了随机森林，这是一种打包技术。在增强弱学习者对训练集的预测时，将误差/残差与其权重一起转发给下一个弱学习者。</p><div class="lz ma gp gr mb mc"><a rel="noopener  ugc nofollow" target="_blank" href="/fully-explained-ensemble-techniques-example-with-python-b83e50310841"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd jd gy z fp mh fr fs mi fu fw jc bi translated">用Python完整解释了整体技术示例</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">基于几种决策树的机器学习方法</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">pub.towardsai.net</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq kx mc"/></div></div></a></div><p id="7af8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们在bagging技术中看到了基尼系数和熵，但是在boosting的情况下，我们将处理损失函数，因为高权重的损失属于下一个基础学习者。因此，最后一个基学习器给出基于较小误差的最佳预测，然后组合预测非常适合该模型。</p><p id="6425" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">n <strong class="lf jd">推进方法</strong>，我们试图减少偏差，而预测是基于顺序的。</p><p id="492a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">梯度增强是一种顺序模型，其中借助于梯度下降来减小误差，并且以基本模型的形式来制作模型。</p><p id="e583" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">梯度推进的优点如下所示:</p><ul class=""><li id="4744" class="mr ms it lf b lg lh lj lk lm mt lq mu lu mv ly mw mx my mz bi translated">它减少了偏见。</li><li id="45c9" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated">处理数据集中缺失的值。</li><li id="ea5e" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated">灵活处理超参数调谐。</li><li id="a368" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated">通过梯度下降处理作为损失函数一部分的误差。</li><li id="ae2f" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated">提升分类处理二元和多类问题。</li></ul><blockquote class="nf ng nh"><p id="debd" class="ld le ni lf b lg lh kd li lj lk kg ll nj ln lo lp nk lr ls lt nl lv lw lx ly im bi translated"><strong class="lf jd"> <em class="it">一些在分类和回归中使用的损失函数借助损失参数如下所示:</em> </strong></p></blockquote><p id="cfe0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">用于回归:</strong></p><ul class=""><li id="5bfc" class="mr ms it lf b lg lh lj lk lm mt lq mu lu mv ly mw mx my mz bi translated"><em class="ni"> LAD -最小绝对偏差</em>:处理回归中目标值的中值。</li><li id="cb71" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated"><em class="ni"> LS -最小二乘法</em>:处理回归中目标值的平均值。</li><li id="cd24" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated"><em class="ni"> Huber </em>:它结合了LAD和LS进行计算。为了控制灵敏度，我们在异常值的情况下使用α参数。</li></ul><p id="c70f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">分类:</strong></p><ul class=""><li id="2d62" class="mr ms it lf b lg lh lj lk lm mt lq mu lu mv ly mw mx my mz bi translated"><em class="ni">二项式偏差</em>:这个损失函数基于对数奇比对二元分类起作用。</li><li id="337b" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated"><em class="ni">多项式偏差</em>:多类分类中使用的损失函数的似然性。当类别增加时，按照n_class参数中给定的回归树变得不能有效地生成概率。</li><li id="fe97" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated"><em class="ni">指数损失</em>:仅用于二元分类。这也用于Adaboost分类。</li></ul><blockquote class="nf ng nh"><p id="3234" class="ld le ni lf b lg lh kd li lj lk kg ll nj ln lo lp nk lr ls lt nl lv lw lx ly im bi translated"><strong class="lf jd"> <em class="it">重要点要记住:</em> </strong></p></blockquote><ul class=""><li id="e4df" class="mr ms it lf b lg lh lj lk lm mt lq mu lu mv ly mw mx my mz bi translated">该模型在随机梯度提升中的子样本上训练，该随机梯度提升将梯度提升与bagging平均(自举)相结合。</li><li id="2f31" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated">学习率通过收缩用于正则化策略。收缩用于减少迭代次数。</li><li id="87e2" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated">梯度推进中的分类类似于回归，因此来自树的值不是类的，而是连续值。因此，对于二分类，我们使用sigmoid函数建模，对于多分类，我们使用softmax。</li><li id="82af" class="mr ms it lf b lg na lj nb lm nc lq nd lu ne ly mw mx my mz bi translated">标准参数分类中使用的默认度量是“friedman_mse”。它衡量分割的质量。这个默认度量对于给出比MSE和MAE更好的近似值非常有用。</li></ul><div class="lz ma gp gr mb mc"><a rel="noopener  ugc nofollow" target="_blank" href="/fully-explained-k-nearest-neighbors-with-python-ebbe27f93ba9"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd jd gy z fp mh fr fs mi fu fw jc bi translated">用Python完整解释K近邻</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">数据科学中解决真实案例的机器学习分类算法研究。</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">pub.towardsai.net</p></div></div><div class="ml l"><div class="nm l mn mo mp ml mq kx mc"/></div></div></a></div><blockquote class="nf ng nh"><p id="cbce" class="ld le ni lf b lg lh kd li lj lk kg ll nj ln lo lp nk lr ls lt nl lv lw lx ly im bi translated"><strong class="lf jd"> <em class="it">用python制作泰坦尼克号数据集的例子。</em> </strong></p></blockquote><p id="d308" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">导入库</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="22be" class="ns nt it no b gy nu nv l nw nx">import pandas as pd<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import classification_report, confusion_matrix<br/>from sklearn.ensemble import GradientBoostingClassifier</span></pre><p id="dea0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">MinMaxScaler用于缩放值，这里我们将使用boosting分类器。</p><p id="a648" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">数据集由两个文件组成。训练和测试CSV文件。为了阅读这些文件，我们使用了熊猫。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="8b14" class="ns nt it no b gy nu nv l nw nx">train_data = pd.read_csv("train.csv")<br/>test_data = pd.read_csv("test.csv")</span></pre><p id="a2d3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">训练数据集的视图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="nz oa di ob bf oc"><div class="gh gi ny"><img src="../Images/f9c181bfdafbcef8da228203cd0a46e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J1EC98MHD8J01I-JJCDhdw.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">列车组。作者的照片</figcaption></figure><p id="ccf4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，我们将进行一些预处理，并选择保留在y_train变量中的输出列，并将其从features数据集中移除。这里，inplace是真的，因为我们也希望原始数据集中的变化。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="4194" class="ns nt it no b gy nu nv l nw nx">y_train = train_data["Survived"]<br/>train_data.drop(labels="Survived", axis=1, inplace=True)</span></pre><p id="e069" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">将测试集添加到训练集。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="5e33" class="ns nt it no b gy nu nv l nw nx">full_data = train_data.append(test_data)</span></pre><p id="1537" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">作为功能工程的一部分，我们可以删除一些不太有用的列。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="5b72" class="ns nt it no b gy nu nv l nw nx">drop_columns = ["Name", "Age", "SibSp", "Ticket", "Cabin", "Parch", "Embarked"]<br/>full_data.drop(labels=drop_columns, axis=1, inplace=True)</span></pre><p id="1768" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们将对分类列进行编码。pandas提供get_dummies方法来将分类值标记为数值。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="1e45" class="ns nt it no b gy nu nv l nw nx">full_data = pd.get_dummies(full_data, columns=["Sex"])<br/>full_data.fillna(value=0.0, inplace=True)</span></pre><p id="a669" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">预处理后，现在在训练和测试中分离。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="983e" class="ns nt it no b gy nu nv l nw nx">X_train = full_data.values[0:891]<br/>X_test = full_data.values[891:]</span></pre><p id="cdcf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在建模之前，缩放是标准化训练集和测试集中的值和划分数据的重要步骤..</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="9f14" class="ns nt it no b gy nu nv l nw nx">scaler = MinMaxScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span><span id="d2a6" class="ns nt it no b gy od nv l nw nx">state = 12  <br/>test_size = 0.30  <br/>  <br/>X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,  <br/>    test_size=test_size, random_state=state)</span></pre><p id="0097" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，我们将尝试拟合模型，并评估不同学习率的准确性。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="c455" class="ns nt it no b gy nu nv l nw nx">lrate_list = [0.05, 0.075, 0.5, 0.75, 1]<br/><br/>for learning_rate in lrate_list:<br/>    gb_clf = GradientBoostingClassifier(n_estimators=20<br/>    , learning_rate=learning_rate, max_features=2, max_depth=2,<br/>     random_state=0)</span><span id="5fa7" class="ns nt it no b gy od nv l nw nx">    gb_clf.fit(X_train, y_train)<br/><br/>    print("Learning rate: ", learning_rate)<br/>    print("Accuracy score (training):<br/>               {0:.3f}".format(gb_clf.score(X_train, y_train)))<br/>    print("Accuracy score (validation):<br/>               {0:.3f}".format(gb_clf.score(X_val, y_val)))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7a6d0a46abe29e253a7f63fed034958b.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*4ss0SiVwLEhFU1qvZo2rAw.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">不同学习率的表现。作者的照片</figcaption></figure><p id="f165" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从上面的表现我们可以看到，学习率为“0.5”的验证分数是很好的。现在，我们将用这个值来预测模型。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="13ac" class="ns nt it no b gy nu nv l nw nx">gb_clf2 = GradientBoostingClassifier(n_estimators=20,<br/>     learning_rate=0.5, max_features=2, max_depth=2, random_state=0)</span><span id="c46d" class="ns nt it no b gy od nv l nw nx">gb_clf2.fit(X_train, y_train)<br/>predictions = gb_clf2.predict(X_val)<br/><br/>print("Confusion Matrix:")<br/>print(confusion_matrix(y_val, predictions))<br/><br/>print("Classification Report")<br/>print(classification_report(y_val, predictions))</span><span id="da0b" class="ns nt it no b gy od nv l nw nx">#output:</span><span id="d604" class="ns nt it no b gy od nv l nw nx">Confusion Matrix:<br/>[[142  19]<br/> [ 42  65]]<br/>Classification Report<br/>              precision    recall  f1-score   support<br/><br/>           0       0.77      0.88      0.82       161<br/>           1       0.77      0.61      0.68       107<br/><br/>    accuracy                           0.77       268<br/>   macro avg       0.77      0.74      0.75       268<br/>weighted avg       0.77      0.77      0.77       268</span></pre><blockquote class="nf ng nh"><p id="31bb" class="ld le ni lf b lg lh kd li lj lk kg ll nj ln lo lp nk lr ls lt nl lv lw lx ly im bi translated"><strong class="lf jd">T9】结论:T11】</strong></p></blockquote><p id="dce6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">梯度推进在分类和回归中给出非常好的结果。</p><div class="lz ma gp gr mb mc"><a rel="noopener  ugc nofollow" target="_blank" href="/become-a-data-scientist-in-2021-with-these-following-steps-5bf70a0fe0a1"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd jd gy z fp mh fr fs mi fu fw jc bi translated">按照以下步骤，在2021年成为一名数据科学家</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">走上数据科学家之路需要具备的基本点</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">pub.towardsai.net</p></div></div><div class="ml l"><div class="of l mn mo mp ml mq kx mc"/></div></div></a></div><p id="d80f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae og" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae og" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="5c8d" class="oh nt it bd oi oj ok ol om on oo op oq ki or kj os kl ot km ou ko ov kp ow ox bi translated">推荐文章</h1><ol class=""><li id="7049" class="mr ms it lf b lg oy lj oz lm pa lq pb lu pc ly pd mx my mz bi translated"><a class="ae og" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> NLP —用Python从零到英雄</a></li></ol><p id="04f4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">2.<a class="ae og" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a></p><p id="c4f4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">3.<a class="ae og" href="https://medium.com/towards-artificial-intelligence/python-zero-to-hero-with-examples-c7a5dedb968b?source=friends_link&amp;sk=186aff630c2241aca16522241333e3e0" rel="noopener"> Python:零到英雄附实例</a></p><p id="7aa8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">4.<a class="ae og" href="https://medium.com/towards-artificial-intelligence/fully-explained-svm-classification-with-python-eda124997bcd?source=friends_link&amp;sk=da300d557992d67808746ee706269b2f" rel="noopener">用Python全面讲解SVM分类</a></p><p id="2ae3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">5.<a class="ae og" href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a?source=friends_link&amp;sk=9c5c613ceb10f2d203712634f3b6fb28" rel="noopener">用Python全面解释K-means聚类</a></p><p id="0e2e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">6.<a class="ae og" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python全面解释线性回归</a></p><p id="1480" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">7.<a class="ae og" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python全面解释逻辑回归</a></p><p id="4f2e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">8.<a class="ae og" href="https://medium.com/towards-artificial-intelligence/basic-of-time-series-with-python-a2f7cb451a76?source=friends_link&amp;sk=09d77be2d6b8779973e41ab54ebcf6c5" rel="noopener">Python时间序列基础</a></p><p id="9ebc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">9.<a class="ae og" href="https://medium.com/towards-artificial-intelligence/numpy-zero-to-hero-with-python-d135f57d6082?source=friends_link&amp;sk=45c0921423cdcca2f5772f5a5c1568f1" rel="noopener"> NumPy:用Python零到英雄</a></p><p id="42d5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">10.<a class="ae og" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>