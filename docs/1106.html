<html>
<head>
<title>10 Game-changing AI Breakthroughs Worth Knowing About</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">值得了解的10个改变游戏规则的人工智能突破</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/10-game-changing-ai-breakthroughs-worth-knowing-about-b2076afc4930?source=collection_archive---------0-----------------------#2020-11-02">https://pub.towardsai.net/10-game-changing-ai-breakthroughs-worth-knowing-about-b2076afc4930?source=collection_archive---------0-----------------------#2020-11-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b5f8" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="a126" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">过去几十年中引人入胜的想法和概念</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/987b5c06ad0a34d5bc984e34a8a08fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*3g4be3-tpusxn773yaxV0Q.gif"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者图片(GIF)</figcaption></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="lo lp l"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">阅读和聆听——为了更好的体验</figcaption></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><p id="c05e" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di"> F </span>从我的AI之旅开始，我就发现了几个很有前途的想法和概念<strong class="ls jd">无与伦比的潜力</strong>；绝对令人兴奋的研发项目<strong class="ls jd"/>；和突破，推动了这一领域的发展，在其辉煌的历史上留下了自己的印记。</p><p id="28a2" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">此外，在过去的几年里，指向“<em class="mv">天网-终结者</em>”场景的人数呈指数级增长……^_^！</p><p id="7b7b" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">所以今天，我决定整理出一些最有趣的想法和概念(来自我自己的经历)，这些想法和概念让我坚持了这么多年。</p><p id="86ac" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我希望它们也能激励你，就像它们激励我一样。</p><p id="bb94" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">一些目标、可能性和“新的思维方式”已经从这些想法中浮现出来，所以不要掉以轻心。我们永远不知道接下来会发生什么。</p><blockquote class="mw"><p id="f6e0" class="mx my it bd mz na nb nc nd ne nf ml dk translated">在人工智能领域度过的一年足以让人相信上帝——艾伦·珀利斯</p></blockquote><p id="d210" class="pw-post-body-paragraph lq lr it ls b lt ng kd lv lw nh kg ly lz ni mb mc md nj mf mg mh nk mj mk ml im bi translated">所以让我们从一个人工智能爱好者的初恋开始。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="8197" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#1.神经网络——“生物灵感”</h1><p id="d504" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">每一个新的ML开发人员，当他/她第一次学习人工神经网络(ANN)时，都会体验到肾上腺素激增的感觉，他/她曾经使用过统计模型(如回归和所有的模型);在深度学习的门口。</p><p id="c4d1" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这里的基本思想是<strong class="ls jd"> <em class="mv">通过编程模仿生物神经元的工作，制作通用函数逼近器。</em> </strong></p><p id="0603" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">神经科学和计算机科学这两个领域的融合本身就是一个令人兴奋的想法。我们将在未来进一步探索它的细节。</p><blockquote class="oi oj ok"><p id="bec0" class="lq lr mv ls b lt lu kd lv lw lx kg ly ol ma mb mc om me mf mg on mi mj mk ml im bi translated">数学上，突触和连接如何归结为巨大的矩阵乘法；神经元的放电如何类似于激活功能，像<a class="ae oo" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">s形</a>；大脑中的高级认知抽象和暗箱操作的人工神经网络，听起来既神秘又酷；所有这些给了新的ML开发者希望，这是不可思议的。</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/008b02154bd7ab3ba5539b27c9749d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CAHpGCjN9gekmZf5A2F9dQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:图片由作者提供(使用Canva制作)</figcaption></figure><p id="9e26" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在这一点上，新手们认为，<em class="mv">字面上的</em> <strong class="ls jd"> <em class="mv">一切</em> </strong> <em class="mv">都有可能借助这种仿生技术实现。毕竟，大自然会选择最好、最有效的方式来执行这些过程，不是吗？”</em></p><p id="d38c" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">只是在后来的一堂课中，他们才知道人工神经网络是如何部分地受到T2的启发，因此有很大的局限性。</p><p id="ad11" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><em class="mv">理论上，一切听起来都很好，但是，他们不切实际的雄心勃勃的梦想就像错误配置的神经网络训练课程中的梯度一样消失了</em>(你明白了吗？哈哈)。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="35e0" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#2.遗传算法——“向达尔文问好”</h1><p id="49f4" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">另一套进入计算机科学领域的受自然启发的算法是<a class="ae oo" href="https://en.wikipedia.org/wiki/Genetic_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="ls jd"> <em class="mv">遗传算法(GAs) </em> </strong> </a>。在这里，你会发现所有与进化相关的达尔文术语，如突变、繁殖、种群、交叉、适者生存等。</p><p id="4ab3" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这些进化算法背后的想法是遵循自然选择的过程，其中最适合的个体被选择来繁殖。为了给种群增加一些多样性，每隔一段时间，最适合的个体的染色体会随机变异。</p><blockquote class="oi oj ok"><p id="09cb" class="lq lr mv ls b lt lu kd lv lw lx kg ly ol ma mb mc om me mf mg on mi mj mk ml im bi translated">在这里，“个人”意味着给定问题的潜在答案。</p></blockquote><p id="e554" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">看看工作流程—</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/1218ffb54b0ae4aaabd3dc71d04f27a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hnf6QAk1fYLG64O2BF7JmQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:图片由作者提供(使用Canva设计)</figcaption></figure><p id="8309" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">很整洁，不是吗？</p><p id="53f0" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这种看起来简单的算法在现实世界中有着巨大的应用，例如在优化、递归神经网络(RNN)的训练、某些问题解决任务的并行化、图像处理等领域。</p><p id="ba47" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在兑现了这么多承诺后，天然气不会很快消失。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="4190" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#3.自我修改程序——“当心编码者”</h1><p id="8966" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">在遗传算法应用的延续中，这一个无疑是最令人兴奋的；它应该有自己独立的部分。</p><p id="53d9" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">想象一个<strong class="ls jd"> <em class="mv">人工智能程序访问自己的源代码。它一次又一次地、递归地改进自己，直到达到它的目标。许多人相信，</em></strong></p><blockquote class="mw"><p id="bbd5" class="mx my it bd mz na nb nc nd ne nf ml dk translated">自我完善/修改代码+ AGI = AI超级智能</p></blockquote><p id="c1c3" class="pw-post-body-paragraph lq lr it ls b lt ng kd lv lw nh kg ly lz ni mb mc md nj mf mg mh nk mj mk ml im bi translated">显然，这一条的道路上有许多障碍，但考虑到2013年的这个<a class="ae oo" href="http://www.primaryobjects.com/2013/01/27/using-artificial-intelligence-to-write-self-modifying-improving-programs/" rel="noopener ugc nofollow" target="_blank">实验</a>:<strong class="ls jd"><em class="mv">一种遗传算法被设计成在</em></strong><a class="ae oo" href="https://en.wikipedia.org/wiki/Brainfuck" rel="noopener ugc nofollow" target="_blank"><strong class="ls jd"><em class="mv">brain fuck</em></strong></a><strong class="ls jd"><em class="mv"/></strong><em class="mv">(一种编程语言)</em> <strong class="ls jd"> <em class="mv">中编写一个可以打印“你好”的程序</em> </strong></p><p id="8c1f" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">GA的源代码中没有任何编程规则，只有简单的自然选择算法。在<strong class="ls jd"> <em class="mv"> 29分钟</em> </strong>内，它生成了这个——</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="a857" class="ow nm it os b gy ox oy l oz pa">+-+-+&gt;-&lt;[++++&gt;+++++&lt;+&lt;&gt;++]&gt;[-[---.--[[-.++++[+++..].]]]]</span></pre><p id="8399" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">当你在Brainfuck解释器中运行这个程序时，它会打印<em class="mv">‘你好。’</em>实验成功！</p><p id="0c26" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这证明了遗传算法的力量，如果有足够的时间和计算资源，它可以实现什么。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="424f" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#4.神经常微分方程——“螺旋层”</h1><p id="f636" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">几年前，在neur IPS<a class="ae oo" href="https://nips.cc/About" rel="noopener ugc nofollow" target="_blank">提交的4854篇研究论文中，</a><a class="ae oo" href="https://arxiv.org/pdf/1806.07366.pdf" rel="noopener ugc nofollow" target="_blank">这篇名为“神经常微分方程”的</a>论文名列前四。</p><p id="e4f9" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为什么这么棒？因为它确实改变了我们对神经网络的看法。</p><p id="a219" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">传统上，神经网络具有谨慎的层数，并且依赖梯度下降和反向传播来进行优化(搜索全局最小值)。随着层数的增加，我们的内存消耗也会增加，但从理论上讲，我们不再需要这样做了。</p><p id="ad69" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们可以从<strong class="ls jd">离散层模型</strong>转移到<strong class="ls jd">连续层模型——无限层。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/a2b161782bf07b0ddc1b2d0d085328c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*4WjkZwunXVCRQGvGWJZTyw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:上述研究论文</figcaption></figure><p id="5c12" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">不需要预先指定层数。取而代之的是，只需输入你想要的精确度，就可以看到在不变的内存成本下奇迹的发生。</p><p id="7367" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">根据研究论文，这种方法在时间序列数据上(特别是在<strong class="ls jd">不规则</strong>时间序列数据上)比传统的递归神经网络(RNN)和残差网络表现得更好。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/33085ae845549b688cd9ea5187ae9252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*i42arVTSS9T4pN3Izc4gsg.gif"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:上述研究论文</figcaption></figure><p id="a4fb" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在这种新技术中，我们可以使用任何常微分方程求解器(ODE Solver)，像<a class="ae oo" href="https://en.wikipedia.org/wiki/Euler_method" rel="noopener ugc nofollow" target="_blank">欧拉方法</a>而不是梯度下降，这样效率要高得多。</p><p id="9ed7" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">正如您所知，时间序列数据无处不在，从股票市场的金融数据到医疗保健行业。一旦这项技术成熟，它的应用将是巨大的。还不发达。</p><p id="2254" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">让我们抱最好的希望吧！</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="7766" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#5.神经进化——“再次模仿自然”</h1><p id="e277" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">神经进化是一个<a class="ae oo" href="http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf" rel="noopener ugc nofollow" target="_blank">古老的想法</a>，可以追溯到21世纪初，与强化学习(RL)领域中著名的反向传播算法相比，它最近显示出了希望和进步。像在神经结构搜索、自动建模、超参数优化等中。</p><p id="1341" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">一行中，<strong class="ls jd"><em class="mv">“</em></strong><a class="ae oo" href="https://en.wikipedia.org/wiki/Neuroevolution" rel="noopener ugc nofollow" target="_blank"><strong class="ls jd"><em class="mv">神经进化</em> </strong> </a> <strong class="ls jd"> <em class="mv">是一种利用遗传算法进化神经网络的技术”；</em> </strong>而且不仅仅是权重和参数，还有网络的架构(拓扑结构)(更新的研究)。</p><p id="a9cb" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><em class="mv">这是一场</em> <strong class="ls jd"> <em class="mv">梯度下降优化和</em> </strong> <em class="mv">进化优化之间的网络模型训练之战。</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/e189c0ca5c5347bfc5da0eff32c5f59d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bvfNLLLPoKc7vl0xBptU_A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:https://blog.otoro.net</figcaption></figure><p id="9909" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls jd"> <em class="mv">但是为什么要用这个呢？</em>T25】</strong></p><p id="c52a" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">因为，在优步(深度神经进化)最近的<a class="ae oo" href="https://eng.uber.com/deep-neuroevolution/" rel="noopener ugc nofollow" target="_blank">研究中，他们发现这种技术比反向传播导致的模型收敛更快。在低端电脑上，数天的计算时间可以缩短到数小时。</a></p><p id="ea6d" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果你正在使用梯度下降训练一个神经网络，并且陷入了困境，比如在一些局部最小值或由于饱和，那么神经进化可以帮助你获得更好的结果。</p><p id="92f0" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在这种选择中没有权衡取舍。在每一个使用神经网络的地方，这种技术都可以应用于它们的优化和训练。</p><blockquote class="mw"><p id="a51c" class="mx my it bd mz na nb nc nd ne nf ml dk translated">"三个臭皮匠胜过一个诸葛亮，不是因为两个人都不会犯错，而是因为他们不太可能在同一个方向上出错。"刘易斯</p></blockquote></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="0111" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#6.谷歌的人工智能孩子——“人工智能创造人工智能”</h1><p id="d8b3" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">超参数调优是一项非常繁琐的工作，每个数据科学家都讨厌这项工作。由于神经网络的黑盒性质，我们实际上不知道我们的变化如何影响网络的学习。</p><p id="b07d" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">2018年，谷歌通过建立一个名为NASNet的模型，在自动机器学习(AutoML)领域取得了突破。该物体识别模型具有最高的准确度，<strong class="ls jd"><em class="mv"/></strong>(即，比计算机视觉领域中的任何其他模型高1.2%)82.7%，同时效率高4%。</p><p id="640d" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">而最棒的是，<strong class="ls jd">是另一个AI，</strong>用强化学习开发的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/654126708310b8095b383069436a9790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nepBNPIrF7messsn0UDrWg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:谷歌研究</figcaption></figure><p id="41d4" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">“神经网络来设计神经网络”……多么不可思议的概念，你不觉得吗？</p><p id="7d97" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这里，父AI被称为<strong class="ls jd"> <em class="mv">控制器网络，</em> </strong>，其通过数千次迭代开发子AI。在每次迭代中，它计算it的性能，并在下一次迭代中使用该信息来构建更好的模型。</p><p id="9fae" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这种类似于盗梦空间的方法促进了“学会学习”(或元学习)的概念，它在准确性和效率方面都优于地球上所有人类设计的神经网络。</p><p id="1e0b" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">想象一下它在计算机视觉领域之外能做什么。难怪这引发了AI超智能霸主们的最大恐惧。LOL！</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="93a2" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#7.GANs —“神经网络与神经网络”</h1><p id="8d54" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">GAN代表生成性对抗网络，它们可以学习模仿任何数据分布。</p><p id="561d" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><em class="mv">这是什么意思？</em></p><p id="7bbe" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在GANs之前，机器学习算法专注于寻找输入和输出之间的<strong class="ls jd">相关性</strong>。这些被称为鉴别算法。比如区分苹果和橘子的图像分类器。</p><blockquote class="oi oj ok"><p id="e0dc" class="lq lr mv ls b lt lu kd lv lw lx kg ly ol ma mb mc om me mf mg on mi mj mk ml im bi translated">当你给这个网络输入一个图像时，它要么返回0(比如苹果)，要么返回1(橘子)。你可以把它想象成分配标签。它在内部创建了一个什么特征对应苹果，什么特征对应橙子的模型。然后以一定的概率吐出标签..</p><p id="f806" class="lq lr mv ls b lt lu kd lv lw lx kg ly ol ma mb mc om me mf mg on mi mj mk ml im bi translated">但是…它们不能生成苹果或橘子的图像。尽管它们可能有一些内部的表示来进行比较。</p></blockquote><p id="10ec" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这就是甘斯介入的原因。GAN由两个网络组成——一个发生器和一个鉴别器。</p><p id="a5b6" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">继续前面的例子，如果我想生成一个苹果的图像，那么我将使用一个解卷积网络作为生成器，一个卷积网络作为鉴别器。</p><p id="130f" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">生成器将从一个随机噪声图像开始，并试图使它看起来像一个苹果。然而，鉴别器将试图识别输入图像是真的还是假的(由生成器生成)。</p><p id="9025" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果鉴别器正确地识别出图像，那么发生器就试图改进自己以产生更真实的图像。否则鉴别器试图改进其预测。这意味着，无论如何，改进是不可避免的，无论是鉴别器还是发生器。</p><p id="8b3e" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这有点像双反馈回路。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/eba54f0f75240202457a7908a1439b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fg5RmooNfQNAZQmGMvBu3A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:https://developers.google.com</figcaption></figure><p id="26ee" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">gan的复杂性使得它们很难训练，最近，在Nvidia发表的一篇研究论文<a class="ae oo" href="https://research.nvidia.com/publication/2017-10_Progressive-Growing-of" rel="noopener ugc nofollow" target="_blank">中，他们描述了我们如何通过逐步增加生成器和鉴别器来训练gan(有趣的阅读！).</a></p><p id="d0a6" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><em class="mv">甘的力学讲的够多了。我们如何使用它？</em></p><p id="5f36" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">氮化镓的一些很酷的应用:</p><ul class=""><li id="8cca" class="pf pg it ls b lt lu lw lx lz ph md pi mh pj ml pk pl pm pn bi translated">面部老化</li><li id="6634" class="pf pg it ls b lt po lw pp lz pq md pr mh ps ml pk pl pm pn bi translated">超分辨率</li><li id="217d" class="pf pg it ls b lt po lw pp lz pq md pr mh ps ml pk pl pm pn bi translated">照片混合</li><li id="dfdb" class="pf pg it ls b lt po lw pp lz pq md pr mh ps ml pk pl pm pn bi translated">服装翻译</li><li id="ee4c" class="pf pg it ls b lt po lw pp lz pq md pr mh ps ml pk pl pm pn bi translated">3D对象生成等。</li></ul><p id="cc83" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">请看这篇<a class="ae oo" href="https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank">文章</a>了解更多信息。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="f13f" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#8.迁移学习——“使用预训练网络”</h1><p id="0c55" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">从头开始训练一个神经网络在计算上极其昂贵，有时会变得超级混乱。但是，如果我们能够<strong class="ls jd"> <em class="mv">获得一个网络</em> </strong>的知识，这些知识是通过先前在一些其他数据集上训练获得的，并且能够<strong class="ls jd"> <em class="mv">重用它来在我们新的目标数据集上训练</em> </strong>会怎么样呢？</p><p id="6cd2" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这样，我们可以加快新领域的学习过程，并减少大量的计算能力和资源。想象一下，在其他人之前开始比赛。</p><blockquote class="mw"><p id="0df8" class="mx my it bd mz na nb nc nd ne nf ml dk translated">不要浪费你的时间去重新发明轮子</p></blockquote><p id="3dff" class="pw-post-body-paragraph lq lr it ls b lt ng kd lv lw nh kg ly lz ni mb mc md nj mf mg mh nk mj mk ml im bi translated">显然，你<strong class="ls jd">不能</strong>在两个完全不相关的领域使用这种技术，但是，有几个领域使用预训练网络是新的规范——<strong class="ls jd"><em class="mv"/></strong><em class="mv">(NLP)</em><strong class="ls jd"><em class="mv"/></strong><em class="mv">和</em> <strong class="ls jd"> <em class="mv">计算机视觉</em> </strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/f01fbb7e794ca223578751342cbec586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUQTeWgjrB6pgmqRti9y9w.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:https://www.topbots.com/</figcaption></figure><p id="ceca" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在计算机视觉、物体检测、物体识别、图像分类等领域，人们使用预先训练好的网络如<a class="ae oo" href="https://en.everybodywiki.com/VGG_Net" rel="noopener ugc nofollow" target="_blank"><strong class="ls jd">【VGG】</strong></a><strong class="ls jd">conv net、</strong><a class="ae oo" href="https://en.wikipedia.org/wiki/AlexNet" rel="noopener ugc nofollow" target="_blank"><strong class="ls jd">Alex net</strong></a><strong class="ls jd">、ResNet-50、InceptionV3、EfficientNet </strong>等。为了抢先一步。</p><p id="23b7" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">即使是像<a class="ae oo" href="https://en.wikipedia.org/wiki/Neural_Style_Transfer" rel="noopener ugc nofollow" target="_blank">神经风格转移</a> (NST)这样的任务，为了快速获得内部表示，也可以使用VGG19来节省时间。</p><p id="b878" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在情感分类和语言翻译等自然语言处理任务中，不同类型的单词嵌入，如<strong class="ls jd">斯坦福的GloVe </strong>(单词表示的全局向量)或<strong class="ls jd">谷歌的Word2Vec </strong>都是非常标准的。</p><p id="9d5d" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">而且不要让我从最新的语言模型(大男孩)开始，如<a class="ae oo" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> <strong class="ls jd">谷歌的BERT</strong></a><strong class="ls jd"/><a class="ae oo" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"><strong class="ls jd">open ai的GPT-2 </strong> </a> <strong class="ls jd"> </strong>(生成式预训练变形金刚)<strong class="ls jd"/>和全能的<strong class="ls jd"/><a class="ae oo" href="https://en.wikipedia.org/wiki/OpenAI#GPT-3" rel="noopener ugc nofollow" target="_blank"><strong class="ls jd">GPT-3</strong></a>。这些都是在如此大量的数据上训练出来的，我们这些普通人连想都不敢想。他们实际上放弃了大部分互联网作为他们的输入数据集，并花费了数百万美元来训练它们。</p><p id="d04a" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在这一点上，预训练网络似乎是未来。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="5420" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#9.神经形态建筑——“下一代的东西”</h1><p id="020d" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">看完了软件世界的所有进步，我们再来看看硬件部门。</p><p id="b12e" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">但在此之前，先看看这个可笑的对比…</p><p id="9470" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">人类大脑平均由860亿个神经元和大约1000亿个突触组成。换个角度来看，<strong class="ls jd"> <em class="mv">你可以毫不夸张地解开你的大脑到达月球(用40万公里长的神经纤维)。</em> </strong></p><p id="6abd" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果你想模仿你的大脑，你将需要巨大的计算能力(一万亿次)，这在我们目前的技术下是不可能的。</p><p id="7629" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">即使是最强大的超级计算机也比不上这么大的功率，然而我们的大脑仅用20瓦的电就能做到这一点(比点亮一个灯泡还少)。</p><blockquote class="mw"><p id="c743" class="mx my it bd mz na nb nc nd ne nf ml dk translated"><strong class="ak"> <em class="pu">为什么？</em> </strong> <em class="pu">因有</em> <strong class="ak"> <em class="pu">【建筑】</em> </strong> <em class="pu">。</em></p></blockquote><figure class="pw px py pz qa kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pv"><img src="../Images/c78c6e007a30a4c253381f021488b04e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZC2zdjvZj9dHKcc9f_ZHrw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:https://randommathgenerator.com/</figcaption></figure><p id="6d6f" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">你知道吗，你今天使用的每一台计算机都是基于一个有75年历史的架构，叫做<a class="ae oo" href="https://en.wikipedia.org/wiki/Von_Neumann_architecture" rel="noopener ugc nofollow" target="_blank"> <strong class="ls jd">冯诺依曼架构</strong> </a>。在这里，内存和处理器是分开的，这在执行大型计算任务(如大型矩阵乘法)时会产生瓶颈。</p><p id="bbef" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这种冯诺依曼瓶颈是由于一组指令发送输入和接收输出的顺序性质造成的。但是在我们的生物大脑中，<strong class="ls jd"> <em class="mv">内存&amp;处理单元本质上是相同的</em> </strong>，这使得它能够以闪电般的速度在无限小的功率上处理大量的数据。在这里，连接本身就是记忆。</p><p id="f5b5" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">像IBM和Intel这样的公司正试图模仿与我们的生物大脑完全相同的架构(在硬件层面上)。而且它最终会催生一种新的计算，<strong class="ls jd"> <em class="mv">神经形态计算。</em>T3】</strong></p><p id="c509" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><em class="mv">多GPU和TPU的时代已经一去不复返了。我真的等不及了！！！</em></p><p id="b202" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">发展:<a class="ae oo" href="https://www.research.ibm.com/articles/brain-chip.shtml" rel="noopener ugc nofollow" target="_blank"> IBM的TrueNorth芯片</a>和<a class="ae oo" href="https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html" rel="noopener ugc nofollow" target="_blank">英特尔的神经形态芯片</a>。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="5ddd" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">#10.人工通用智能(AGI)——“我们的最终目标”</h1><p id="685d" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">当你听到有人尖叫<em class="mv">“有一天，AI会杀了我们所有人”</em>那么人工通用智能(AGI)将是最有可能的原因。</p><p id="e000" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><em class="mv">为什么？</em></p><p id="42ed" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">因为目前，我们正在玩<strong class="ls jd">“人工狭义智能”，</strong>意味着我们当前的模型不能在它们特定的领域之外执行太多。但是，全球各地的科学家和研究人员正在研究一种能够执行各种任务的智能；或者可以学习完成几乎任何给定的任务。</p><p id="9365" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果他们成功，预言是，这将导致<em class="mv">智能爆炸</em>，远远超过人类的智能，从而诞生<strong class="ls jd">超级智能</strong>。</p><p id="3739" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">当它发生时，那种超级智慧将成为一种<em class="mv">【存在】</em>，具有意识、自我意识和更高的认知能力。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qb"><img src="../Images/bb1e44d88a9c025c8d877a87d5e61595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s4SKv4TRYWxucUemzMEcnQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:https://www.theverge.com(忍不住放了这张图片)</figcaption></figure><p id="a4c9" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">接下来会发生什么？天知道。</p><p id="903c" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">但是这有一个术语。<em class="mv">一个</em> <em class="mv">假设的时间点，在这个时间点上，技术增长将变得不可控制和不可逆转，导致人类文明发生不可预见的变化，称为</em> <strong class="ls jd"> <em class="mv">奇点</em> </strong> <em class="mv"> —维基百科</em></p><p id="e52a" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">难道我们不能阻止它的发展吗？没有。</p><p id="43a0" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">人工智能就是今天的电。我们严重依赖它，停止它的发展就像回到黑暗时代。除此之外，没有一个国家会因为同样的心态——“如果我们不愿意，他们就会愿意”——而停止发展，就像我们制造大规模杀伤性核武器一样。</p><p id="cc31" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">埃隆不会杞人忧天。他是认真的。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="523e" class="nl nm it bd nn no np nq nr ns nt nu nv ki nw kj nx kl ny km nz ko oa kp ob oc bi translated">结束语—</h1><p id="64ce" class="pw-post-body-paragraph lq lr it ls b lt od kd lv lw oe kg ly lz of mb mc md og mf mg mh oh mj mk ml im bi translated">我有一种强烈的感觉，我已经泄露了制作一个完美的人工智能霸主的配方。但是谁在乎呢？只要我们能和他们共生共存，我就很好。</p><p id="4827" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我希望在<a class="ae oo" href="https://neuralink.com/" rel="noopener ugc nofollow" target="_blank"> Neuralink </a>的人在世界末日之前完成他们的脑机接口。埃隆成功地实现了这一点，而没有将我们的大脑暴露给新时代的黑客。(我不太擅长闲聊)</p><p id="9763" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为了一个乌托邦式的未来，我们可以就此打住。最后，我想说，我非常喜欢写这篇文章，希望你也喜欢。</p><p id="84b9" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">感谢您的阅读&amp;祝您度过美好而富有成效的一天！</p><blockquote class="mw"><p id="3bad" class="mx my it bd mz na nb nc nd ne nf ml dk translated">机器智能是人类需要创造的最后一项发明——尼克·博斯特罗姆</p></blockquote></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><p id="5d1d" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><em class="mv">如果你喜欢阅读这些故事，那么我敢肯定你会喜欢成为</em> <a class="ae oo" href="https://nishu-jain.medium.com/membership" rel="noopener"> <strong class="ls jd"> <em class="mv">中等付费会员</em> </strong> </a> <strong class="ls jd"> <em class="mv">。每月只需5美元，你就可以无限制地接触成千上万的故事和作家。你可以通过</em> <a class="ae oo" href="https://nishu-jain.medium.com/membership" rel="noopener"> <strong class="ls jd"> <em class="mv">使用此链接</em> </strong> </a>、<em class="mv">注册来支持我，我将赚取一点佣金，这将帮助我成长并出版更多这样的故事。</em></strong></p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><p id="e5ac" class="pw-post-body-paragraph lq lr it ls b lt lu kd lv lw lx kg ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls jd">您可能喜欢的其他文章— </strong></p><div class="qc qd gp gr qe qf"><a href="https://medium.com/geekculture/medium-api-documentation-90a01549d8db" rel="noopener follow" target="_blank"><div class="qg ab fo"><div class="qh ab qi cl cj qj"><h2 class="bd jd gy z fp qk fr fs ql fu fw jc bi translated">中等API —文档</h2><div class="qm l"><h3 class="bd b gy z fp qk fr fs ql fu fw dk translated">中型API入门</h3></div><div class="qn l"><p class="bd b dl z fp qk fr fs ql fu fw dk translated">medium.com</p></div></div><div class="qo l"><div class="qp l qq qr qs qo qt lb qf"/></div></div></a></div><div class="qc qd gp gr qe qf"><a href="https://towardsdatascience.com/how-i-won-a-national-level-ml-competition-with-my-unique-informal-approach-e86fd95532fd" rel="noopener follow" target="_blank"><div class="qg ab fo"><div class="qh ab qi cl cj qj"><h2 class="bd jd gy z fp qk fr fs ql fu fw jc bi translated">我是如何用我独特的“非正式方法”赢得国家级ML比赛的</h2><div class="qm l"><h3 class="bd b gy z fp qk fr fs ql fu fw dk translated">像数据科学黑客一样思考——你不需要遵守规则就能获胜</h3></div><div class="qn l"><p class="bd b dl z fp qk fr fs ql fu fw dk translated">towardsdatascience.com</p></div></div><div class="qo l"><div class="qu l qq qr qs qo qt lb qf"/></div></div></a></div><div class="qc qd gp gr qe qf"><a href="https://medium.com/towards-artificial-intelligence/why-its-super-hard-to-be-an-ml-researcher-or-developer-67fa62fc1971" rel="noopener follow" target="_blank"><div class="qg ab fo"><div class="qh ab qi cl cj qj"><h2 class="bd jd gy z fp qk fr fs ql fu fw jc bi translated">为什么做一个ML研究员或者开发者超级难？</h2><div class="qm l"><h3 class="bd b gy z fp qk fr fs ql fu fw dk translated">这一认识彻底改变了我的生活</h3></div><div class="qn l"><p class="bd b dl z fp qk fr fs ql fu fw dk translated">medium.com</p></div></div><div class="qo l"><div class="qv l qq qr qs qo qt lb qf"/></div></div></a></div></div></div>    
</body>
</html>