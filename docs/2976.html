<html>
<head>
<title>Large Language Models (LLM): Top 3 of the Most Important Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大型语言模型(LLM):最重要方法的前三名</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/large-language-models-llms-top-3-of-the-most-important-methods-f7d92a2aa05a?source=collection_archive---------1-----------------------#2022-07-22">https://pub.towardsai.net/large-language-models-llms-top-3-of-the-most-important-methods-f7d92a2aa05a?source=collection_archive---------1-----------------------#2022-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e7dd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">确保您正在训练或使用的大型语言模型(LLM)提供最佳输出和结果的3个要求</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/94711ecf75f6336d6af0d9096bccec8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zbw4d1__ye9KPgT0A1HYDw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由Pexels的<a class="ae ky" href="https://www.pexels.com/@thisisengineering/" rel="noopener ugc nofollow" target="_blank">this is工程</a></figcaption></figure><p id="e4f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大型语言模型(LLM)目前适用于自然语言特定的实现，如机器翻译、语音识别和文本生成。LLM基于大量数据进行训练，可以由许多层组成。</p><p id="11e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例1: Google Translate是一个大型语言模型，使用人工智能将一种语言翻译成另一种语言。它支持100多种语言，可以处理每种语言的多种方言。基本上，Google Translate对LLM的使用为语言之间的翻译提供了信息。此外，因为许多用户每天都与它进行交互，所以模型会不断更新。</p><p id="94c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更详细地说:当有人使用谷歌翻译时，谷歌服务器将使用其内部算法调用的LLM来找到在含义和语法规则方面与输入的句子最可能匹配的句子。因此，该模型会将这个结果实时返回给用户，以便他们可以在说完或写完之后立即看到他们的翻译文本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/6ddf4be2fc9a8ea7993d9c2e47045935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TXzJPjVyMtXD-Jq3oICYtw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的<a class="ae ky" href="https://www.pexels.com/@thisisengineering/" rel="noopener ugc nofollow" target="_blank"> ThisIsEngineering </a></figcaption></figure><p id="7fc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例子2:生成性对抗网络已经被用来创建与真实图像无法区分的虚假图像——即使是人类也无法区分。</p><p id="1a99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更详细地说:生成性对抗网络采用两个神经网络，一个生成器和一个鉴别器[1]，并使它们相互对抗，直到生成器创建令人信服的输出，或者直到鉴别器不再能够以高置信度辨别哪个输出是假的或不是。</p><p id="4e3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">示例3:考虑OpenAI的GPT，作为行业LLM的一种转换能力，允许用户针对特定语言的用例与其进行交互。</p><p id="0acc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更详细地说:GPT代表“生成性预训练转换器”，能够在对任何数据集进行训练之前，根据给定的上下文为单词确定某些概率函数(与LSTMs或长短期记忆等其他方法相比，它在学习速度方面非常有效)。</p><p id="fd93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无需解释，众所周知，Siri和Alexa也是LLM。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/c38802bf2d46a39a37ac2615c47c9e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GQjFtvbIdbNrtKpS6IScA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的<a class="ae ky" href="https://www.pexels.com/@thisisengineering/" rel="noopener ugc nofollow" target="_blank"> ThisIsEngineering </a></figcaption></figure><h1 id="f7cd" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">LLM要成功必须具备哪些条件？</h1><p id="58a2" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">实现LLM需要满足三个一般条件:</p><p id="ef9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.大量数据的存在来训练他们的模型。</p><p id="8f4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.LLM有许多层神经元，可以学习数据中的复杂模式。</p><p id="eada" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.LLM针对特定任务进行了优化，如机器翻译、图像字幕和语音识别。因此，准确的结果需要正确的特性和用例。</p><h1 id="715d" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak">存在大量数据来训练他们的模型</strong></h1><p id="09a4" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">LLM使用大量数据来训练它们的模型，通过使用特定的方法来最有效地利用这些数据。一种方法被称为迁移学习，它允许模型从其他相关任务中学习，并更好地概括新数据。另一种方法是自我监督的预训练[2]，它使用未标记的数据来学习一般的表示，这些表示可以针对带有标记数据的特定任务进行微调。最后，集成多个模型通过减少对训练集的过度拟合和增加对噪声或非分布输入的鲁棒性来参与提高性能。</p><p id="0f45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LLM需要能够在不牺牲太多准确性的情况下进行计算缩放的算法，以便有效地使用大量训练数据。可扩展机器学习算法的一些示例包括随机梯度下降(SGD)、并行化SGD (PSGD)和分布式PSGD。举例来说，用这些可扩展算法训练的每个时期在数据集大小方面具有线性运行时间，通过录音来控制训练时间(即使数据集变得更大)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e4865ddab7f1d2797f5609ed9e4f4aa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DC5PCOlIctXTH85tEBHpXA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Pexels的<a class="ae ky" href="https://www.pexels.com/@thisisengineering/" rel="noopener ugc nofollow" target="_blank"> ThisIsEngineering </a></figcaption></figure><h1 id="2889" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak">LLM在给出提示学习数据中的复杂模式之前，首先需要多层神经元。</strong></h1><p id="e55c" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">大型语言模型由于其设计的功能而具有各种层次的神经元，经过专门训练以理解数据中的复杂模式。较深层的神经元比较浅层的神经元能学习多边复杂模式。</p><p id="5a00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个典型的例子，一个大的语言模型可以知道某些单词经常一起使用，并且这些单词的顺序很重要。它还可以学习哪些主题显示出一起出现的趋势，以及不同的词类如何相互关联。这些关系可以通过在大量文本数据上训练神经网络来学习。网络训练的数据越多，它就能更好地学习这些复杂的模式。</p><h1 id="c4a8" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak">LLM针对特定任务进行了优化。</strong></h1><p id="507b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">通过调整层数、每层神经元数和所用激活函数的类型，LLM可以针对特定任务进行优化。一个恰当的例子是，设计用来预测句子中下一个单词的语言模型可能比设计用来从头生成新句子的语言模型使用更少的层和每层神经元。此外，所使用的激活功能的类型可以根据期望的任务进行调整；sigmoid或softmax激活[3]在许多情况下用于分类任务，而校正线性单位(ReLU)激活[4]可用于生成任务。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/d57d6574bb07531cbc40b43a1c566f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8GpIWumG71Fah_S8ln_HXA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由Pexels的<a class="ae ky" href="https://www.pexels.com/@thisisengineering/" rel="noopener ugc nofollow" target="_blank">this is工程</a></figcaption></figure><p id="5b5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">离别的思念</strong></p><p id="20c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LLM通过考虑单词的上下文允许更准确的预测。一个恰当的例子，一个语言模型，会预测单词“the”更可能出现在名词之前而不是动词之后。此外，LLM可以帮助消除同音异义词和多义词(具有多个含义的单词)的歧义。也就是说，如果我们在“钱”的上下文中看到“银行”这个词，我们可以合理地肯定它指的是金融机构，但如果我们在“河”的上下文中看到它，我们可以合理地肯定它指的是自然特征。同时，LLM通过利用语言中的冗余来提高数据压缩率。给定一个句子，比如“我看见戴夫和他的狗在外面玩捡东西”，一个语言模型可能会选择简单地表示为“我看见约翰了。”</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="a688" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有任何编辑/修改建议或关于进一步扩展此主题的建议，请考虑与我分享您的想法。</p><h1 id="77a2" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak">另外，请考虑订阅我的每周简讯:</strong></h1><div class="nc nd gp gr ne nf"><a href="https://pventures.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">周日报告#1</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">设计思维与AI的共生关系设计思维能向AI揭示什么，AI又能如何拥抱…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">pventures.substack.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt ks nf"/></div></div></a></div></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="154b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">我写了以下与这篇文章相关的内容:他们可能和你有相似的兴趣:</strong></p><h1 id="66e9" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak">新的NLP模型Minerva的性能如何优于GPT-3 </strong></h1><div class="nc nd gp gr ne nf"><a rel="noopener  ugc nofollow" target="_blank" href="/how-minerva-a-new-nlp-model-performs-better-than-gpt-3-f7713458b59b"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">一个新的NLP模型Minerva如何比GPT-3表现得更好</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">自然语言处理的进步正在解决数学和许多其他定量推理问题。</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">pub.towardsai.net</p></div></div><div class="no l"><div class="nu l nq nr ns no nt ks nf"/></div></div></a></div><h1 id="c12e" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak"> 16个开源的用于情感分析的NLP模型；一个上升到顶部</strong></h1><div class="nc nd gp gr ne nf"><a rel="noopener  ugc nofollow" target="_blank" href="/16-open-source-nlp-models-for-sentiment-analysis-one-rises-on-top-b5867e247116"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">16个用于情感分析的开源NLP模型；一个在顶端升起</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">介绍16款车型，深入了解风格。</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">pub.towardsai.net</p></div></div><div class="no l"><div class="nv l nq nr ns no nt ks nf"/></div></div></a></div><h1 id="94a9" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak">NLP的未来是量子物理学</strong></h1><div class="nc nd gp gr ne nf"><a rel="noopener  ugc nofollow" target="_blank" href="/the-future-of-nlp-is-quantum-physics-37e3673e82bc"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">NLP的未来是量子物理学</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">但是，它已经在这里了。</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">pub.towardsai.net</p></div></div><div class="no l"><div class="nw l nq nr ns no nt ks nf"/></div></div></a></div><h1 id="5a0d" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated"><strong class="ak">设计思维与人工智能的共生关系</strong></h1><div class="nc nd gp gr ne nf"><a href="https://uxdesign.cc/the-symbiotic-relationship-between-design-thinking-and-ai-ac38db258209" rel="noopener follow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">设计思维与人工智能的共生关系</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">设计思维能向AI揭示什么，AI如何拥抱设计思维。</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">uxdesign.cc</p></div></div><div class="no l"><div class="nx l nq nr ns no nt ks nf"/></div></div></a></div><p id="f8cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ny">参考文献。</em></p><p id="069b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ny"> 1。生成对抗网络:综述。(未注明)。IEEE Xplore。检索到2022年7月22日，来自</em><a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/8253599" rel="noopener ugc nofollow" target="_blank"><em class="ny">【https://ieeexplore.ieee.org/abstract/document/8253599】</em></a></p><p id="0677" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ny"> 2。Reed，C. J .，Yue，x .，Nrusimha，a .，s .，Vijaykumar，v .，Mao，r .，Li，b .，Zhang，s .，Guillory，d .，Metzger，s .，Keutzer，k .，&amp;# 38；t .达雷尔(2021年3月23日)。自我监督预训练改进了自我监督预训练。ArXiv.Org。</em><a class="ae ky" href="https://arxiv.org/abs/2103.12718" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://arxiv.org/abs/2103.12718</em></a></p><p id="cae4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ny"> 3。陈，丁，王，陈，&amp;# 38；陈(2021年9月14日)。探索知识提炼与逻辑匹配的关系。ArXiv.Org。</em><a class="ae ky" href="https://arxiv.org/abs/2109.06458" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://arxiv.org/abs/2109.06458</em></a></p><p id="9bc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ny"> 4。Sudjianto，a .，Knauth，w .，Singh，r .，Yang，z .，&amp;# 38；张(2020年11月8日)。揭开深层relu网络的黑盒:可解释性、诊断性和简化。ArXiv.Org。</em><a class="ae ky" href="https://arxiv.org/abs/2011.04041" rel="noopener ugc nofollow" target="_blank">【https://arxiv.org/abs/2011.04041】T21</a></p></div></div>    
</body>
</html>