<html>
<head>
<title>Feature Selection With Practical Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实用的特征选择方法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/feature-selection-with-practical-approach-13ea32b7a46f?source=collection_archive---------2-----------------------#2021-02-12">https://pub.towardsai.net/feature-selection-with-practical-approach-13ea32b7a46f?source=collection_archive---------2-----------------------#2021-02-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9053" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="b5fe" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">了解用于以最实用的方式从数据集中选择重要要素的要素选择技术。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/c8886711e1cd020de4e2754a2e2f1ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0xDq9kf7SNC3lYW7xSr7A.png"/></div></div></figure><blockquote class="la lb lc"><p id="fa0b" class="ld le lf lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在本文中，我们将使用最实用的方法中的各种技术对数据集进行特征选择。</p></blockquote><p id="5ba6" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated"><strong class="lg ja">特性选择</strong>是任何数据科学项目生命周期的关键步骤之一。有许多文章从理论上描述了可用于特征选择的技术，但是缺乏实用的方法。在这里，我们将直接处理代码，看看它实际上是如何工作的。我将尝试提供初学者友好的代码片段的高级描述。</p><h2 id="8aa4" class="md me iq bd mf mg mh dn mi mj mk dp ml ma mm mn mo mb mp mq mr mc ms mt mu iw bi translated">什么是特征选择？</h2><p id="0483" class="pw-post-body-paragraph ld le iq lg b lh mv ka lj lk mw kd lm ma mx lp lq mb my lt lu mc mz lx ly lz ij bi translated">当我们训练我们的机器学习模型时，并不是所有的特征都有同等的贡献。有些功能非常重要，有些甚至对训练毫无帮助。因此，我们应该从数据集中移除这些类型的要素。因此，<strong class="lg ja">特征选择</strong>是移除或减少输入特征/变量的过程，这反过来降低了<strong class="lg ja">复杂性</strong>，使得训练过程<strong class="lg ja">更快</strong>，并且还增加了模型的<strong class="lg ja">准确性</strong>。</p><p id="65ae" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">有许多技术可用于特征选择，但是我们将只讨论著名的和重要的技术。这些技术是:-</p><ul class=""><li id="2d9b" class="na nb iq lg b lh li lk ll ma nc mb nd mc ne lz nf ng nh ni bi translated">方差阈值</li><li id="52d5" class="na nb iq lg b lh nj lk nk ma nl mb nm mc nn lz nf ng nh ni bi translated">相互关系</li><li id="2da6" class="na nb iq lg b lh nj lk nk ma nl mb nm mc nn lz nf ng nh ni bi translated">卡方检验</li><li id="17cc" class="na nb iq lg b lh nj lk nk ma nl mb nm mc nn lz nf ng nh ni bi translated">信息增益</li></ul><p id="cead" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated"><strong class="lg ja">注意:-在本文中，我们将只讨论前两个，其余两个将在第二部分讨论。</strong></p><h2 id="1b72" class="md me iq bd mf mg mh dn mi mj mk dp ml ma mm mn mo mb mp mq mr mc ms mt mu iw bi translated">资料组</h2><p id="a88f" class="pw-post-body-paragraph ld le iq lg b lh mv ka lj lk mw kd lm ma mx lp lq mb my lt lu mc mz lx ly lz ij bi translated">在本教程中，我们使用了来自<strong class="lg ja"> Kaggle </strong>的"<em class="lf">桑坦德银行客户满意度</em>"数据集，该数据集也可在<a class="ae no" href="https://github.com/PushkaraSharma/articles_codes/tree/master/Feature_Selection_with_Practical_Approach" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja"> GitHub </strong> </a>上获得。与此同时，使用的其他数据集可以通过描述的代码片段直接获得。</p><h2 id="6252" class="md me iq bd mf mg mh dn mi mj mk dp ml ma mm mn mo mb mp mq mr mc ms mt mu iw bi translated">先决条件:</h2><p id="2f75" class="pw-post-body-paragraph ld le iq lg b lh mv ka lj lk mw kd lm ma mx lp lq mb my lt lu mc mz lx ly lz ij bi translated">我假设你熟悉<strong class="lg ja"> <em class="lf"> python </em> </strong>并且已经在你的系统中安装了<strong class="lg ja"> <em class="lf"> python 3 </em> </strong>。这个教程我用了一个<strong class="lg ja"> <em class="lf"> jupyter笔记本</em> </strong>。你可以使用你喜欢的<strong class="lg ja"> IDE </strong>。所有需要的库都内置在<strong class="lg ja"> <em class="lf"> anaconda </em> </strong>套件中。</p><h1 id="da7c" class="np me iq bd mf nq nr ns mi nt nu nv ml kf nw kg mo ki nx kj mr kl ny km mu nz bi translated">让我们编码</h1><p id="6ef5" class="pw-post-body-paragraph ld le iq lg b lh mv ka lj lk mw kd lm ma mx lp lq mb my lt lu mc mz lx ly lz ij bi translated">这里，我们将讨论两种技术，即<strong class="lg ja">方差阈值</strong>和<strong class="lg ja">相关性</strong>。</p></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><h1 id="48ae" class="np me iq bd mf nq oh ns mi nt oi nv ml kf oj kg mo ki ok kj mr kl ol km mu nz bi translated">使用方差阈值移除恒定要素</h1><p id="c0a7" class="pw-post-body-paragraph ld le iq lg b lh mv ka lj lk mw kd lm ma mx lp lq mb my lt lu mc mz lx ly lz ij bi translated">我们应该从数据集中移除的第一个要素是常量要素。手动执行似乎很容易，但假设您有<strong class="lg ja">200</strong>–<strong class="lg ja">300</strong>特征。在这种情况下，使用一些技巧是有意义的。</p><p id="97c5" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">首先，让我们导入所有需要的库。<code class="fe om on oo op b">Pandas</code>用于创建和操作数据集。<code class="fe om on oo op b">VarianceThreshold</code>用于去除方差较小的特征。<strong class="lg ja">方差</strong>只是对<strong class="lg ja">可变性</strong>的一种度量。<strong class="lg ja"> 0 </strong>方差意味着所有值都相同/不变。<code class="fe om on oo op b">Train_test_split</code>用于为训练和测试目的拆分数据。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="c378" class="md me iq op b gy ou ov l ow ox">import pandas as pd<br/>from sklearn.feature_selection import VarianceThreshold<br/>from sklearn.model_selection import train_test_split</span></pre><p id="5d36" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">这里，我们刚刚创建了一个虚拟的<strong class="lg ja">数据框</strong>，以便于理解该特征选择。我们可以看到列<strong class="lg ja"> C </strong>和<strong class="lg ja"> D </strong>具有常量值，因此它们应该被删除。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="8783" class="md me iq op b gy ou ov l ow ox">df = pd.DataFrame({"A":[2,3,5,2,6],<br/>                  "B":[4,6,4,2,8],<br/>                  "C":[5,5,5,5,5],<br/>                  "D":[0,0,0,0,0]})<br/>df</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f53aafd98576860825948823eed6b5a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*NhIREblXOPcw2yEOxL2-LQ.png"/></div></figure><p id="5620" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">现在，我们已经初始化了VarianceThreshold的对象<code class="fe om on oo op b">var</code>，阈值为<strong class="lg ja"> 0 </strong>(常量)。我们可以根据需要改变这一点。然后，我们打印了布尔数组，其中<code class="fe om on oo op b">true</code>和<code class="fe om on oo op b">false </code>分别代表非常数和常数特性。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="1935" class="md me iq op b gy ou ov l ow ox">var = VarianceThreshold(threshold=0.0)<br/>var.fit(df)<br/>var.get_support()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/d8b85f6a7fc52bc91269c46b1894991e.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*0c2F4gGIUS6ZOhyEsIfbJA.png"/></div></figure><p id="3cff" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">我们使用了列表理解，循环遍历所有列，并将常量特性插入到名为<code class="fe om on oo op b">constant_features</code>的列表中。最后，我们从数据集中删除了常量要素。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="ced2" class="md me iq op b gy ou ov l ow ox">constant_features = [i for i in df.columns<br/>                    if i not in df.columns[var.get_support()]]<br/>print(constant_features)<br/>df.drop(constant_features,axis=1)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/3b37b0d59aa43cd9ce6405a48878a2c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*Y-dbPVB334ZkWVCEUedVIw.png"/></div></figure><p id="d100" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">前面的例子可能看起来太简单了，很容易被发现。但是在现实世界的场景中，将会有<strong class="lg ja"> 100 </strong>或<strong class="lg ja"> 1000 </strong>的特性。这种技术在这些情况下会有所帮助。让我们用更大更真实的数据集来看看同样的情况。</p><p id="ff88" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">这里，我们已经加载了GitHub上的数据集。数据集由<strong class="lg ja"> 371 </strong>个特征组成。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="8844" class="md me iq op b gy ou ov l ow ox">df = pd.read_csv("train.csv")<br/>print(df.shape)</span></pre><p id="7249" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">这里，<strong class="lg ja"> X </strong>代表输入变量，<strong class="lg ja"> Y </strong>代表目标变量。我们应该始终将任何特征选择技术应用于训练集，以避免任何类型的<strong class="lg ja">过度拟合</strong>。这就是我们将数据集分为训练和测试的原因。之后的过程和上面解释的一样。在这里，371–332 =<strong class="lg ja">39</strong>特征是不变的。因此，我们从训练集和测试集中删除了这些特性。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="e009" class="md me iq op b gy ou ov l ow ox">X = df.drop(labels=['TARGET'],axis=1)<br/>Y = df['TARGET']<br/>x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2)<br/>var = VarianceThreshold(threshold=0.0)<br/>var.fit(x_train)<br/>sum(var.get_support())</span><span id="ebc6" class="md me iq op b gy pb ov l ow ox">constant_features = [i for i in x_train<br/>                    if i not in x_train.columns[var.get_support()]]<br/>x_train.drop(constant_features,axis=1)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/422dbaebd882770f8f5a4cfb73c7f4b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9iaHE7jdJt_G-P47GQxM0g.png"/></div></div></figure></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><h1 id="8729" class="np me iq bd mf nq oh ns mi nt oi nv ml kf oj kg mo ki ok kj mr kl ol km mu nz bi translated">使用皮尔逊相关移除相似特征</h1><p id="3e02" class="pw-post-body-paragraph ld le iq lg b lh mv ka lj lk mw kd lm ma mx lp lq mb my lt lu mc mz lx ly lz ij bi translated">一些特征彼此之间呈线性关系，即当变量2增加<strong class="lg ja"> 3 </strong>倍时，变量1增加<strong class="lg ja"> 2 </strong>倍。所以这两个特征是高度相关的。因此，我们可以删除其中一个，我们的模型仍然不会丢失任何重要信息。</p><p id="4812" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">这里，我们为示例导入了<code class="fe om on oo op b">boston</code>数据集。<code class="fe om on oo op b">seaborn</code>将用于绘制交互图形。其他库已经讨论过了。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="480a" class="md me iq op b gy ou ov l ow ox">from sklearn.datasets import load_boston<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>import seaborn as sns</span></pre><p id="18cd" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">现在，我们已经创建了加载数据集的<code class="fe om on oo op b">pandas</code>数据帧，并添加了目标变量作为<code class="fe om on oo op b">TARGET</code>列。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="35a3" class="md me iq op b gy ou ov l ow ox">dataset = load_boston()<br/>df = pd.DataFrame(dataset.data,columns = dataset.feature_names)<br/>df['TARGET'] = dataset.target<br/>df.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pd"><img src="../Images/946fdd1df9054189450f9a4d6c7f629c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqlUOMbglj6_fWm5XWEAOA.png"/></div></div></figure><p id="d53a" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">这与前面的技术步骤相似，我们将输入变量声明为<strong class="lg ja"> X </strong>，将目标变量声明为<strong class="lg ja"> Y </strong>。需要分割数据，因为我们只对训练集执行特征选择部分，以避免任何类型的过拟合。<code class="fe om on oo op b">x_train</code>的形状是(404，13)<code class="fe om on oo op b">x_test</code>的形状是(102，13)。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="3603" class="md me iq op b gy ou ov l ow ox">X = df.drop('TARGET',axis=1)<br/>Y = df['TARGET']<br/>x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2)<br/>x_train.shape , x_test.shape</span></pre><p id="d282" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">这里，我们给出了一个固定大小的图，然后在<code class="fe om on oo op b">x_train</code>上调用<code class="fe om on oo op b">corr()</code>函数，但是如果我们只打印<code class="fe om on oo op b">correlation</code>变量，那么将很难解释。因此，在<code class="fe om on oo op b">seaborn</code>的帮助下，我们绘制了<strong class="lg ja">热图</strong>，清楚地显示了特征之间的相关性。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="4795" class="md me iq op b gy ou ov l ow ox">plt.figure(figsize=(12,10))<br/>correlation = x_train.corr()<br/>sns.heatmap(correlation,annot=True,cmap=plt.cm.CMRmap)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/34f998aa451ff3c288b3b88a596a3dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*LIpA5bNyEpRVf2rVoTsdHw.png"/></div></figure><p id="62c0" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">之后，我们声明了以<strong class="lg ja">数据集</strong>和<strong class="lg ja">阈值</strong>为参数的函数<code class="fe om on oo op b">find_correlated_features</code>。在函数中，我们迭代了相关矩阵的每个值，如果相关值高于给定的阈值，则将列名添加到名为<code class="fe om on oo op b">col_corr</code>的集合中。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="a146" class="md me iq op b gy ou ov l ow ox">def find_correlated_features(data,threshold):<br/>    col_corr = set()<br/>    corr_metrix = data.corr()<br/>    for i in range(len(corr_metrix.columns)):<br/>        for j in range(i):<br/>            if((corr_metrix.iloc[i,j])&gt;threshold):<br/>                column_name = corr_metrix.columns[i]<br/>                col_corr.add(column_name)<br/>    return col_corr</span></pre><p id="2961" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">现在，我们刚刚使用<code class="fe om on oo op b">x_train</code>作为数据和threshold = <strong class="lg ja"> 0.7 </strong>调用了上面的函数，以获得我们可以丢弃的相关特征。在我们的例子中，这些特征是{ <strong class="lg ja">年龄</strong>，<strong class="lg ja">氮氧化物</strong>，<strong class="lg ja">税收</strong> }。删除这些特征后，我们的数据集特征从<strong class="lg ja"> 13 </strong>减少到<strong class="lg ja"> 10 </strong>。</p><p id="12e5" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">当要素数量过多而导致热图无助于解释时，定义上述函数将有助于我们。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="30da" class="md me iq op b gy ou ov l ow ox">corelated_features = find_correlated_features(x_train,0.7)<br/>print(corelated_features)<br/>x_train = x_train.drop(corelated_features,axis=1)<br/>x_train.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/8e70fee7cd225ba391ae08e69cdcbb92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*Kz0gwtG1HchHu_zgDWpNxA.png"/></div></figure></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><p id="c9d4" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">我希望你理解这两个<strong class="lg ja">特征选择</strong>的技巧。如果你有任何疑问，请随时评论，我会尽快回复。</p><p id="55b2" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">其他两种技术将很快介绍。</p><p id="ca5b" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">源代码在<a class="ae no" href="https://github.com/PushkaraSharma/articles_codes/tree/master/Feature_Selection_with_Practical_Approach" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja"> GitHub </strong> </a>上有。请随意改进。</p><p id="039f" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">谢谢你宝贵的时间。😊我希望你喜欢这个教程。</p><p id="c05b" class="pw-post-body-paragraph ld le iq lg b lh li ka lj lk ll kd lm ma lo lp lq mb ls lt lu mc lw lx ly lz ij bi translated">另外，查看我关于<strong class="lg ja"> </strong> <a class="ae no" href="https://medium.com/towards-artificial-intelligence/classify-plant-leaf-diseases-using-machine-learning-4747dc1eb43d" rel="noopener"> <strong class="lg ja">利用机器学习</strong> </a>对植物叶部病害进行分类的文章</p><div class="pg ph gp gr pi pj"><a href="https://medium.com/towards-artificial-intelligence/logistic-regression-from-scratch-with-only-python-code-9d3ae607e739" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd ja gy z fp po fr fs pp fu fw iz bi translated">仅使用Python代码从零开始进行逻辑回归</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">仅使用Python对多要素数据集应用逻辑回归。分步实现编码示例…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">medium.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px ky pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a href="https://medium.com/towards-artificial-intelligence/gradient-descent-v-s-normal-equation-for-regression-problems-e6c3cdd705f" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd ja gy z fp po fr fs pp fu fw iz bi translated">回归问题的梯度下降v/s正规方程</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">选择正确的算法来找到使成本函数最小的参数。</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">medium.com</p></div></div><div class="ps l"><div class="py l pu pv pw ps px ky pj"/></div></div></a></div></div></div>    
</body>
</html>