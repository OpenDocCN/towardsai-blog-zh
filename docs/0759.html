<html>
<head>
<title>The Math behind Gradient Descent and the Normal Equation for Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降背后的数学和线性回归的标准方程</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/gradient-descent-and-the-normal-equation-for-linear-regression-with-practical-implemetation-in-2e7fc99cb80d?source=collection_archive---------0-----------------------#2020-08-04">https://pub.towardsai.net/gradient-descent-and-the-normal-equation-for-linear-regression-with-practical-implemetation-in-2e7fc99cb80d?source=collection_archive---------0-----------------------#2020-08-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/786a931819340a611b0d63acd452a853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UXCinQ6M4V--DBdS"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae jg" href="https://unsplash.com/@zbigniew?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">zbysu Rodak</a>拍摄的照片</figcaption></figure><h2 id="607d" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="d0ca" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated"><em class="lh">梯度下降和线性回归的标准方程，用Python实现</em></h2></div><figure class="li lj lk ll gt iv"><div class="bz fp l di"><div class="lm ln l"/></div></figure><p id="c210" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">当我开始机器学习之旅时，数学一直让我着迷，现在依然如此。<br/>就我个人而言，我相信scikit learn这样的库确实在实现算法方面为我们创造了奇迹，但如果不理解制作算法所需的数学知识，我们肯定会在复杂的问题上出错。<br/>在这篇文章中，我将回顾梯度下降背后的数学和正常线性方程背后的推导，然后在数据集上实现它们以获得我的系数。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mk"><img src="../Images/fcb35f9f45006a86b0ee854442afcf46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XCtgr2tTSRtJR__TQGzgBw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">线性回归模型。红点是真正的y值，蓝线是模型线</figcaption></figure><h1 id="8087" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated"><strong class="ak">法线方程</strong></h1><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/21a7438c3fdfef302e91f133ecef9fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*7ZiWm6xAF4oWiYfWklUMEw.jpeg"/></div></figure><p id="5a5e" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">看起来很直截了当，不是吗？当我开始学习线性回归并试图理解计算系数的不同方法时，正规方程是我最喜欢的寻找系数的方法，但是这个方程从何而来呢？好吧，让我们来看看。<br/>我们首先要明白的是，均方差(简称MSE)是衡量我们的模型表现如何的一个指标。MSE越低，我们的预测就越接近y的实际值。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/b98f8152a04d556a03e1a3e59bd21470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*m-ZzU5oK4CIFSqRARURWLg.jpeg"/></div></div></figure><p id="0d1d" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">从等式中可以看出，y的实际值和预测值之间的差异越小，MSE就越低。<strong class="lq jt">这就是我们的目标，尽可能降低MSE。</strong></p><p id="5223" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">但是我们能使用MSE来找到我们的系数吗？是的，我们可以。</p><h1 id="4e9f" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated"><strong class="ak">进入成本函数</strong></h1><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/33f534ee6d63aa7aae693f45fe1cdda8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*HzkGyfYwGps6n2HVZZqXoA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">线性回归成本函数</figcaption></figure><p id="37dc" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">你可能已经注意到了，MSE方程和成本函数是相同的，但是我们用θ来代替ŷ的预测值。<br/>在这里，ŷ只不过是θ的转置和x的值的点积</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/68858429d523cfc52d471e6f81a4a4cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/format:webp/1*j-4_2yvpalHYMwtzKj5HTA.jpeg"/></div></figure><p id="76f3" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">将h(θ)的值代入我们的成本函数并简化方程，我们得到:</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/15267d9af0f716de0c558e2c59a5ab5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*LCwPASyG7EE_zwSnIK2a3A.jpeg"/></div></figure><p id="b05d" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">注意，我用矩阵乘法代替了求和符号，得到了相同的结果。<br/>现在使用矩阵转置恒等式，我们进一步将其简化为:</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/bee58e4543d57be0003422b9ec15e414.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*E4G8waalms-3R48TU-MBUQ.jpeg"/></div></figure><p id="4e7d" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">现在，取J(θ)关于θ的导数并将其设置为零，<br/>我们找到成本函数的最小点(微积分101):</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1ebad68f114d563659472d1c2e1b6136.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*5KDxwTirq3gV7FIwUqvbsg.jpeg"/></div></figure><p id="2dd2" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">进一步简化后，我们得到:</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3cafb40592f7458f018a938231d1762b.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*IngYxRsPNGRQ4_8X05aJcA.jpeg"/></div></figure><h1 id="94c9" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">梯度下降:</h1><p id="f2f8" class="pw-post-body-paragraph lo lp jj lq b lr nl kt lt lu nm kw lw lx nn lz ma mb no md me mf np mh mi mj im bi translated">顾名思义，梯度下降就是沿着梯度下降，直到我们到达可能的最低点</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/872223e5860c66d609b7f1a146b4fde2.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*f_f4oa-DECpP4X73tEmcUA.jpeg"/></div></figure><p id="e33f" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">代价函数只不过是一个二次方程，所有的二次方程都有一个“全局最小值”，即函数值最低，梯度为零。</p><p id="b940" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated"><strong class="lq jt">那么梯度下降是如何工作的呢？</strong></p><p id="6649" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">简而言之，我们对我们的θ值进行初始猜测，以找到<strong class="lq jt">梯度</strong>，然后从初始θ<strong class="lq jt">值中减去<strong class="lq jt">梯度</strong>，以获得新的θ值，并重复该过程，直到我们收敛到<strong class="lq jt">全局最小点。</strong></strong></p><p id="44c3" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">数学上，这个等式看起来有点像:</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e634d9ee93bef304decba5cb6f9883f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*5JIS0oheAUt-aSyOu4v1MA.jpeg"/></div></figure><p id="4628" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">这里，<strong class="lq jt"> α </strong>表示学习率，如果梯度太大，它基本上防止我们的θ值“过冲”。</p><p id="9294" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">学习率的高值导致函数错过其目标，而小的学习率有助于容易地收敛到全局最小值。</p><p id="379a" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">注意:学习率不应该太小，因为这会不必要地增加计算时间。一个好的估计是从0.01到0.001，但这取决于手头的情况。</p><h1 id="6971" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">Python中的实际实现</h1><p id="09a4" class="pw-post-body-paragraph lo lp jj lq b lr nl kt lt lu nm kw lw lx nn lz ma mb no md me mf np mh mi mj im bi translated">在这一部分中，我将在Andrew ng的“机器学习入门课程”中的回归数据集上实现梯度下降法和法方程。<br/>数据集是基于房屋总面积和卧室数量的房屋价格列表。<br/>这里价格是我们的因变量y，X是我们的一组自变量。<br/>注意:我添加了一个额外的列“intercept_term ”,顾名思义，它将用于查找我们的截距。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/facb4c4fc271251c2dc0ed7d62dee83f.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*jaXwFQxrXDnkkacQEYePXQ.jpeg"/></div></figure><p id="8461" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">在分离变量和进行计算之前，我们必须意识到我们的值有不同的范围。</p><p id="18a5" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我的意思是卧室栏在1到5的范围内，而平方英尺栏有数百个不同的值。</p><p id="a713" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">仅仅因为一个变量有一个更高的值并不意味着它更重要，但是这个高值可能会对我们的模型产生一些负面影响。</p><p id="da62" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">所以在线性回归中，标准化我们的变量总是一个好的做法，这样它们就在同一个范围内。我们希望变量的均值为零，标准差为一。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/88747356a1289594024e214602dd693f.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*EtKemSBHeAVuyj89qvJxGQ.jpeg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">标准化公式</figcaption></figure><p id="1740" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">使用Scikit学习内置的StandardScaler模块，我们将标准化我们的整个变量，使其达到相同的规模。</p><pre class="li lj lk ll gt nu nv nw nx aw ny bi"><span id="6130" class="nz mm jj nv b gy oa ob l oc od">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>data_new = sc.fit_transform(data)<br/>S = pd.DataFrame(data_new)<br/>S.drop(3,axis=1,inplace=True)<br/>S.columns = ['sq_ft','bedrooms','price']<br/>S['ones'] = S['bedrooms']/S['bedrooms']<br/>S</span></pre><p id="a189" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">标准化后，我们得到</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f3e5d1b737c630f53202b5289fc0a0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*KWGGrTDrWdLN8b3bmQcWCA.jpeg"/></div></figure><p id="989c" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">注意:我们之前的“intercept_term”标准化为零，所以我们再次添加了它。</p><p id="66bf" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我们将把<strong class="lq jt">平方英尺、卧室和个人列</strong>分离出来作为我们的<strong class="lq jt"> X </strong>数据框架，而<strong class="lq jt">价格</strong>列将表示为<strong class="lq jt"> y. </strong></p><pre class="li lj lk ll gt nu nv nw nx aw ny bi"><span id="5497" class="nz mm jj nv b gy oa ob l oc od">X_array = S.drop('price',axis=1)<br/>X_array<br/>y = S['price']<br/>X_array = np.array(X_array).reshape(46,3)<br/>y_array = np.array(y).reshape(46,1)</span></pre><p id="5fe7" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">注意:我们将把列转换成numpy数组，这样我们就可以对变量执行numpy矩阵运算。</p><h1 id="935d" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated"><strong class="ak">首先:正规方程</strong></h1><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e558d824ef2268f39bc9a38bf279bc13.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*w6M0avkhKBzm15aH65ZL-w.png"/></div></figure><p id="1c13" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我将法线方程分成两部分，第一部分将是<strong class="lq jt"> X_transpose与X </strong>的点积的倒数，第二部分将是<strong class="lq jt"> X_transpose与y </strong>的点积</p><p id="bf99" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">然后，我将得到两个结果的点积。</p><pre class="li lj lk ll gt nu nv nw nx aw ny bi"><span id="4880" class="nz mm jj nv b gy oa ob l oc od">first_term = np.linalg.inv((np.dot(x_transpose,X_array)))<br/>second_term = np.dot(x_transpose,y_array)<br/>Theta = np.dot(first_term,second_term)<br/>Theta<br/></span></pre><p id="0f21" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我得到的结果是:</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/0f7c80be04185b2eabdbba1f7fc6e3eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*LUZSyBdDmXP9VQx_I2dzLQ.jpeg"/></div></div></figure><p id="a945" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我有一个3x1矩阵，两个是系数，另一个是截距项。</p><h1 id="c3d7" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated"><strong class="ak">梯度下降</strong></h1><pre class="li lj lk ll gt nu nv nw nx aw ny bi"><span id="e1ad" class="nz mm jj nv b gy oa ob l oc od">m = 96 # number of samples<br/>alpha=0.01<br/>x_transpose = X_array.transpose()<br/>for iter in range(0, 100000):<br/>        hypothesis = np.dot(x, theta)<br/>        loss = hypothesis - y_array<br/>        <br/>             <br/>        gradient = np.dot(x_transpose, loss) / m         <br/>        theta = theta - alpha * gradient  # update<br/>print(theta)</span></pre><p id="6f8a" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我将样本变量的数量定义为m，将学习率定义为α，并将迭代次数设置为100000。<br/>随着for循环的运行，我们的θ值在每次迭代中都会更新，直到我们到达θ不再收敛的点，结果为。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/60b8d350b2bf970fab7dd509f5c2b703.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*WX_lYktjccU_QU8zU4hz_w.jpeg"/></div></figure><p id="6c64" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">所以我们的结果匹配，还有什么？<br/>好吧，让我们试试scikit learns的线性回归模块，看看它能给我们带来什么。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/7ccb275de8da76c74e3606f196abf111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3r58vaYZQuk4sjlUylbrEA.jpeg"/></div></div></figure><p id="a922" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">让我们想象一下我们的结果。尽管这是一个多元回归问题，让我们将价格变量与sq_ft变量进行对比。</p><figure class="li lj lk ll gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/6ca65735026c887282927bdd3afc5784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1NDX0PQk_7TrkWt-Dx6-w.jpeg"/></div></div></figure><p id="23a2" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">注意:U变量不过是一个线性方程，其中一个系数作为梯度。(0.883) <br/>这条线很好地捕捉了数据。</p><p id="ae8e" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">希望对梯度下降法和法方程的介绍将证明是富有成果和有益的。</p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="2c2d" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">[1]:the green place(2014年12月22日)。<em class="or">线性回归正规方程的推导</em><a class="ae jg" href="https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression" rel="noopener ugc nofollow" target="_blank">https://Eli . the green place . net/2014/线性回归正规方程的推导</a></p><p id="f9d6" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">[2]:千里眼软件. com(<a class="ae jg" href="https://blog.clairvoyantsoft.com/the-ascent-of-gradient-descent-23356390836f?source=post_page-----23356390836f----------------------" rel="noopener ugc nofollow" target="_blank">2019年9月30日</a> ) <em class="or">梯度下降的上升<br/> </em> <a class="ae jg" href="https://blog.clairvoyantsoft.com/the-ascent-of-gradient-descent-23356390836f" rel="noopener ugc nofollow" target="_blank"> https://blog .千里眼软件. com/The-Ascent-of-Gradient-Descent-23356390836 f</a></p></div></div>    
</body>
</html>