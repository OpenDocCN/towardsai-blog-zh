<html>
<head>
<title>Jordan Peterson NLP Word Frequency Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">乔丹·彼得森自然语言处理词频分析</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/jordan-peterson-nlp-word-frequency-analysis-b18642205311?source=collection_archive---------0-----------------------#2020-12-01">https://pub.towardsai.net/jordan-peterson-nlp-word-frequency-analysis-b18642205311?source=collection_archive---------0-----------------------#2020-12-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e7c0" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="eaa6" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在同一课程中，他的语言使用是否有所不同？完整的代码使用空间可用<a class="ae kr" href="https://github.com/arditoibryan/Projects/tree/master/20200730_NLP_word_frequency" rel="noopener ugc nofollow" target="_blank">在我的Github回购</a></h2></div><p id="cad5" class="pw-post-body-paragraph ks kt it ku b kv kw kd kx ky kz kg la lb lc ld le lf lg lh li lj lk ll lm ln im bi translated">在这篇文章中，我将使用多伦多大学30小时的乔丹·彼得森人格课程来进行NLP分析。我的目标是找出哪些是他用得最多的词:</p><ul class=""><li id="988c" class="lo lp it ku b kv kw ky kz lb lq lf lr lj ls ln lt lu lv lw bi translated">在30小时内</li><li id="772e" class="lo lp it ku b kv lx ky ly lb lz lf ma lj mb ln lt lu lv lw bi translated">每节课</li></ul><p id="bb12" class="pw-post-body-paragraph ks kt it ku b kv kw kd kx ky kz kg la lb lc ld le lf lg lh li lj lk ll lm ln im bi translated">结果会有所不同吗？从理论上讲，乔丹·彼得森可能会在一节课中使用一些术语，然后在不同的课上换成一套不同的术语，也许是受不同情绪的驱使。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/d435e57984f63cc336531cb4069f6f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BdVWcDYqGKdMy5c4VgyQdQ.jpeg"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">图片:Rene Johnston /多伦多星报</figcaption></figure><p id="af5d" class="pw-post-body-paragraph ks kt it ku b kv kw kd kx ky kz kg la lb lc ld le lf lg lh li lj lk ll lm ln im bi translated">为了进行这个实验，我需要他整个课程的文本内容。因为我需要使用Youtube API来访问字幕(这需要一篇文章来解释如何使用python连接到Youtube)，所以我将使用第三方软件:JDownloader。如果你想在类似的视频上复制这个实验，你可以在。srt格式。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ms"><img src="../Images/6193f1687c4b690bbb3a5cfe924e73da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OWdi-dNwuPssWpT9beyC1w.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">JDownloader的屏幕截图</figcaption></figure><p id="4ed0" class="pw-post-body-paragraph ks kt it ku b kv kw kd kx ky kz kg la lb lc ld le lf lg lh li lj lk ll lm ln im bi translated">我将只选择字幕，并将所有内容下载到同一文件夹下:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mt"><img src="../Images/49b8cd4408d4ed97237ba54b6d7d1a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFPLr4veX_jS4sTTQ5w8GA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">列表。srt下载</figcaption></figure><h1 id="905e" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">创建函数</h1><p id="e8d0" class="pw-post-body-paragraph ks kt it ku b kv nm kd kx ky nn kg la lb no ld le lf np lh li lj nq ll lm ln im bi translated">为了执行这个实验，我需要预处理文本数据:</p><ul class=""><li id="7e86" class="lo lp it ku b kv kw ky kz lb lq lf lr lj ls ln lt lu lv lw bi translated">清除标点符号、符号和停用词中的数据</li><li id="cfd8" class="lo lp it ku b kv lx ky ly lb lz lf ma lj mb ln lt lu lv lw bi translated">将每个单词词条化:将它们转换为原始词根</li></ul><pre class="md me mf mg gt nr ns nt nu aw nv bi"><span id="e3db" class="nw mv it ns b gy nx ny l nz oa">!pip install spacy<br/>!pip install pysrt #per sottotitoli</span><span id="ddf6" class="nw mv it ns b gy ob ny l nz oa">#find top words<br/>def top_frequent(text, num_words):<br/>  #frequency of most common words<br/>  import spacy<br/>  from collections import Counter</span><span id="f05e" class="nw mv it ns b gy ob ny l nz oa">nlp = spacy.load("en")<br/>  text = text</span><span id="cd54" class="nw mv it ns b gy ob ny l nz oa">#lemmatization<br/>  doc = nlp(text)<br/>  token_list = list()<br/>  for token in doc:<br/>    #print(token, token.lemma_)<br/>    token_list.append(token.lemma_)<br/>  token_list</span><span id="ded1" class="nw mv it ns b gy ob ny l nz oa">  lemmatized = ''<br/>  for _ in token_list:<br/>    lemmatized = lemmatized + ' ' + _<br/>  lemmatized</span><span id="79b6" class="nw mv it ns b gy ob ny l nz oa">#remove stopwords and punctuations<br/>  doc = nlp(lemmatized)<br/>  words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]<br/>  word_freq = Counter(words)<br/>  common_words = word_freq.most_common(num_words)<br/>  return common_words</span></pre><p id="df79" class="pw-post-body-paragraph ks kt it ku b kv kw kd kx ky kz kg la lb lc ld le lf lg lh li lj lk ll lm ln im bi translated">使用函数<strong class="ku jd">top _ frequency(text，num_words)，</strong>我可以输入任何一串文本(如果我愿意，甚至可以输入整本书)，我将得到按升序排列的词汇数。使用参数<strong class="ku jd"> num_words </strong>，我可以指定我想要返回的单词数(例如，返回2000个单词的整个列表没有多大用处)。</p><h1 id="0102" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">整个课程的分析</h1><p id="3882" class="pw-post-body-paragraph ks kt it ku b kv nm kd kx ky nn kg la lb no ld le lf np lh li lj nq ll lm ln im bi translated">一个有趣的实验是对整个课程进行分析，然后将结果与单独的课程进行比较。</p><p id="df7e" class="pw-post-body-paragraph ks kt it ku b kv kw kd kx ky kz kg la lb lc ld le lf lg lh li lj lk ll lm ln im bi translated">为什么要对全文和不同分区进行实验？最重要的是，为什么我有足够的信心声称彼得森使用的最频繁的词汇可能在所有课程中都不相同？通过分析整个课程，我不知道单词分布的概念，因为它代表所有的单个课程。通过从所有单独的课程中获取数据，并分别分析它们，我可以建立一个词频分布，它可以告诉我结果是否一致。</p><pre class="md me mf mg gt nr ns nt nu aw nv bi"><span id="5c92" class="nw mv it ns b gy nx ny l nz oa">#   parse sub<br/>#total words of JP in the course<br/>import pysrt</span><span id="8358" class="nw mv it ns b gy ob ny l nz oa">total_string = ''<br/>for text_n in range(1, 10):<br/>  subs = pysrt.open('/content/drive/My Drive/Colab Notebooks/Projects/20200729_NLP_frequency/JP_'+str(text_n)+'.srt')<br/>  for sub in subs:<br/>    #print(sub.text)<br/>    total_string += sub.text<br/>total_string</span><span id="11eb" class="nw mv it ns b gy ob ny l nz oa">len(total_string)</span></pre><h2 id="85c1" class="nw mv it bd mw oc od dn na oe of dp ne lb og oh ng lf oi oj ni lj ok ol nk iz bi translated">输出</h2><p id="3a58" class="pw-post-body-paragraph ks kt it ku b kv nm kd kx ky nn kg la lb no ld le lf np lh li lj nq ll lm ln im bi translated">我让软件将所有字幕组合在一起，并对彼得森最常用的1000个单词进行排序:</p><pre class="md me mf mg gt nr ns nt nu aw nv bi"><span id="74a3" class="nw mv it ns b gy nx ny l nz oa">common_words = top_frequent(total_string, 1000)<br/>common_words<br/>[('-PRON-', 13496),  <br/>('know', 1133),  <br/>('like', 924),  <br/>('thing', 592),  <br/>('think', 495),  <br/>('people', 397),  <br/>('right', 361),  <br/>('way', 294),  <br/>('world', 272),  <br/>('happen', 235),  <br/>('mean', 225),  <br/>('good', 223),  <br/>('look', 219),  <br/>('time', 192),  <br/>('want', 185),  <br/>('maybe', 185),  <br/>('tell', 183),  <br/>('idea', 181),  <br/>('sort', 168),  <br/>('little', 156),  <br/>('come', 145),  <br/>('story', 143),  <br/>('actually', 142),  <br/>('kind', 134), <br/>('problem', 133),<br/>...</span></pre><h1 id="5409" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">个别课程的分析</h1><p id="8649" class="pw-post-body-paragraph ks kt it ku b kv nm kd kx ky nn kg la lb no ld le lf np lh li lj nq ll lm ln im bi translated">有了这个代码分区，我将构建一个值的分布，看看它是否遵循均匀分布。如果是这样的话，我可能会说，独立于演讲的内容，乔丹·彼得森将保持一致的词频。</p><pre class="md me mf mg gt nr ns nt nu aw nv bi"><span id="dd62" class="nw mv it ns b gy nx ny l nz oa">def find_common_factors(X):<br/>  #flat everything in one list<br/>  one_row = list()<br/>  for k in X.values:<br/>    for n in k:<br/>      one_row.append(n)<br/>  one_row = pd.DataFrame(one_row)<br/>  one_row</span><span id="e99a" class="nw mv it ns b gy ob ny l nz oa">#labeled list<br/>  from sklearn.preprocessing import LabelEncoder<br/>  le = LabelEncoder()<br/>  le.fit(one_row[0])<br/>  one_row_labeled = le.transform(one_row[0])<br/>  one_row_labeled</span><span id="2c45" class="nw mv it ns b gy ob ny l nz oa">#rebuild the original dataset and convert it to DataFrame<br/>  import numpy<br/>  X_labeled = numpy.array_split(one_row_labeled, (X.shape[0]))<br/>  X_labeled = pd.DataFrame(X_labeled)<br/>  X_labeled</span><span id="cf0b" class="nw mv it ns b gy ob ny l nz oa">m = [[0 for x in range(max(one_row_labeled)+1)] for x in range(len(X_labeled))]</span><span id="7749" class="nw mv it ns b gy ob ny l nz oa">#turn each corresponding label to 1<br/>  for row in range(len(X_labeled.values)):<br/>    for num in range(len(X_labeled.values[1])):<br/>      m[row][X_labeled.values[row][num]] = 1<br/>  m = pd.DataFrame(m)<br/>  m</span><span id="0e25" class="nw mv it ns b gy ob ny l nz oa">original_shape = m.shape<br/>  original_shape</span><span id="e215" class="nw mv it ns b gy ob ny l nz oa">#convert column num in corresponding text<br/>  m.loc[original_shape[0]] = [0 for x in range(0, original_shape[1])]<br/>  m</span><span id="b657" class="nw mv it ns b gy ob ny l nz oa">for _ in range(original_shape[0]):<br/>    m.loc[original_shape[0]] += m.loc[_] <br/>  m</span><span id="2837" class="nw mv it ns b gy ob ny l nz oa">#rename columns with label values<br/>  m.columns = a = le.inverse_transform(m.columns)<br/>  m</span><span id="927f" class="nw mv it ns b gy ob ny l nz oa">#isolate count of columns<br/>  m.loc[m.shape[0]] = m.columns #we add last col with names<br/>  f = m.transpose().drop([x for x in range(m.shape[0]-2)], axis=1).transpose() #we only save the last 2 cols<br/>  f = f.reset_index()<br/>  f.pop('index')</span><span id="8f12" class="nw mv it ns b gy ob ny l nz oa">#sorting values<br/>  f = f.transpose().reset_index().drop(['index'], axis=1)<br/>  return f.sort_values(0, axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')</span><span id="074d" class="nw mv it ns b gy ob ny l nz oa">#parse individual lesson<br/>#total words of JP in the course<br/>import pysrt<br/>import pandas as pd</span></pre><p id="4197" class="pw-post-body-paragraph ks kt it ku b kv kw kd kx ky kz kg la lb lc ld le lf lg lh li lj lk ll lm ln im bi translated">现在算法已经准备好了，我可以访问所有的字幕，并将实验结果收集到熊猫的数据帧中。<strong class="ku jd">记得更改路径文件夹。</strong>我将使用我的Google Drive来存储我的代码。您将把数据存储在不同的位置。</p><pre class="md me mf mg gt nr ns nt nu aw nv bi"><span id="44f0" class="nw mv it ns b gy nx ny l nz oa">total_subs = list()<br/>for text_n in range(1, 15):<br/>  subs = pysrt.open('/content/drive/My Drive/Colab Notebooks/Projects/20200729_NLP_frequency/JP_'+str(text_n)+'.srt')<br/>  top_words = top_frequent(subs.text, 12) #n_words qui, metto 12 perchè le prime 2 si eliminano<br/>  df = pd.DataFrame([top_words[x][0] for x in range(10)])<br/>  total_subs.append(df)</span><span id="fc0c" class="nw mv it ns b gy ob ny l nz oa">X = pd.concat(total_subs, axis=1)<br/>X = X.transpose()<br/>X<br/>X[[x for x in range(2, 12)]]</span></pre><h2 id="ac9c" class="nw mv it bd mw oc od dn na oe of dp ne lb og oh ng lf oi oj ni lj ok ol nk iz bi translated">每个单独课程的输出:</h2><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi om"><img src="../Images/a78f09cb7813fa983954b0f8af2c3a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DWb7QtxHBK5rDX31CSHCIA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">我去掉了前两列，因为它们计入了文本中的-PRON-和\ an，这可能是一个spacy bug。</figcaption></figure><h1 id="ab59" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">实验的结论</h1><p id="8bec" class="pw-post-body-paragraph ks kt it ku b kv nm kd kx ky nn kg la lb no ld le lf np lh li lj nq ll lm ln im bi translated">结果简直不可思议！所用词的分布似乎是均匀的。我们对词汇的选择在未来保持一致。</p><pre class="md me mf mg gt nr ns nt nu aw nv bi"><span id="4499" class="nw mv it ns b gy nx ny l nz oa">find_common_factors(X).reset_index().drop(['index'], axis=1).iloc[2:20]</span></pre><p id="84b6" class="pw-post-body-paragraph ks kt it ku b kv kw kd kx ky kz kg la lb lc ld le lf lg lh li lj lk ll lm ln im bi translated">通过计算数据集中的单词，结果如下:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7a2e2334ef21af81ad3193255455e822.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*QlVwCE1mKvbsUdAtgtJKwg.png"/></div></figure></div></div>    
</body>
</html>