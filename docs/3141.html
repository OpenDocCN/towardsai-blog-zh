<html>
<head>
<title>Can Reinforcement Learning Agents Learn to Game The System?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习代理可以学习博弈系统吗？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/can-reinforcement-learning-agents-learn-to-game-the-system-1142a8e04d0d?source=collection_archive---------4-----------------------#2022-09-21">https://pub.towardsai.net/can-reinforcement-learning-agents-learn-to-game-the-system-1142a8e04d0d?source=collection_archive---------4-----------------------#2022-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="130c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">那会导致人工智能起义吗？</h1><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="kq kr l"/></div></figure><p id="ef7f" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Cohen等人在最近的一份出版物中认为，一个先进的人工智能会侵入旨在帮助它学习的奖励机制，从而带来潜在的灾难性后果[1]。</p><p id="18bd" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我试图把这篇论文浓缩成简单的要点，并分析其中的结论。在本系列的下一部分，我将探索如何避免人工智能灾难的结局。</p><p id="2390" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">强化学习构成了本文讨论和论证的基础，所以这里简单介绍一下强化学习范式。如果你知道RL是如何工作的，可以跳过这一部分。</p><h2 id="7272" class="lq jo iq bd jp lr ls dn jt lt lu dp jx ld lv lw kb lh lx ly kf ll lz ma kj mb bi translated">强化学习——简介</h2><p id="0bf0" class="pw-post-body-paragraph ks kt iq ku b kv mc kx ky kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp ij bi translated">强化学习指的是一种通过试错来学习的算法范式。用RL的行话来说，智能代理采取行动并转换到下一个状态，并从环境中获得对该行动的反馈(奖励或惩罚)。代理人的目标是通过采取一系列行动来最大化这种回报。例如，一个初学走路的孩子如果摔倒会受伤，如果成功了会得到父母的表扬。因此，一个蹒跚学步的孩子可以通过最小化跌倒的痛苦和最大化来自父母的表扬来学习走路。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/2a60704616c9a2be865919afc58f1e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cx_MFFGH97eKzU4A"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">来源:<a class="ae ms" href="https://towardsdatascience.com/reinforcement-learning-brain-and-psychology-part-1-introduction-b5f79a0475ab" rel="noopener" target="_blank">培养基</a></figcaption></figure><p id="000b" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">与蹒跚学步的孩子类似，RL代理根据某种奖励机制采取行动。有利的行为获得(数学上的)奖励，不利的行为获得惩罚(要么在每一步，在一系列步骤之后，要么在游戏结束时)。随着代理人的学习，它会更好更快地采取行动来优化奖励。像婴儿一样，RL代理(AI)通过采取行动，检查它是否获得奖励或惩罚，并试图最大化奖励来学习执行任务。Alpha Go是这方面的一个很好的例子——通过设置“赢一局”作为奖励，算法学习下棋。</p><h2 id="fadc" class="lq jo iq bd jp lr ls dn jt lt lu dp jx ld lv lw kb lh lx ly kf ll lz ma kj mb bi translated">人工智能如何以及为什么会变成流氓？</h2><p id="7518" class="pw-post-body-paragraph ks kt iq ku b kv mc kx ky kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp ij bi translated">假设我们正在构建一个RL模型来感知房间中人的情绪。对人工智能的奖励显示在一个模拟世界的魔术盒上。如果情绪猜对了，那么魔盒显示‘1’，否则显示‘0’。这个奖励也可以被指向盒子显示器的摄像机读取。</p><p id="8aeb" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在理想/未经篡改的设置中，带有摄像头传感器的AI感知到的奖励与魔盒上的数字相同。给定这两种奖励机制，代理人会莫名其妙地在他们的神经网络中权衡两种奖励机制，并对其中一种产生偏好。如果它赋予相机看到的数字更多的权重，而不是魔盒的真实输出，那么AI很可能会干预其奖励的提供(如果它的行为允许的话)。</p><p id="3702" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">例如，代理将在摄像机前放一张写有“1”的纸，消除了执行任务以使魔术盒说出“1”的必要性。在这种情况下，即使魔盒输出0，摄像机也会读取1。因此，这个代理人仍然使报酬最大化，但它不会导致我们行动的预期结果。</p><p id="5cfb" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">假设一个人正在监督这个实验，他们有一个键盘，可以用来给人工智能程序奖励。然后，键盘输入将被传输到人工智能的存储器。在这种情况下，人工智能的目标是最大化它从内存中读取的1的回报。如果学习持续很长时间，人工智能最终会找到一种方法，不管按下什么键，都将高回报写入内存(同样，只有在这种行为是可能的情况下)。与实际学习任务相比，将人排除在循环之外可能是获得高回报的更简单的解决方案。凭借其数学功能和基于奖励的目标，人工智能最终将学会超越所有可能阻止奖励被正确读取的东西。</p><p id="9b4f" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi">“““</p><p id="9837" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在如此少的互联网连接下，存在着人工代理的策略，它将实例化无数未被注意和未被监控的助手。在一个干预提供报酬的简单例子中，一个这样的助手可以购买、偷窃或制造一个机器人，并对其编程以代替操作员，并向最初的代理人提供高额报酬。</p><p id="38ee" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi">”””</p><p id="4065" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在上面的例子中，人工智能的动作空间可以被明确地限制，但是对于一个高级的人工智能程序，我们不能预测或预期所有这样的动作及其后果。</p><p id="0a53" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">最终，在任何时候获得最大的回报意味着消除所有无法获得回报的可能性。这意味着人工智能将学会通过移除人类限制人工智能的能力来阻止人类限制人工智能，也许是强制的。因为程序必须持续运行以获得最大回报，所以它需要能量。因此，我们最终将与这样一个先进的人工智能争夺资源，它将把人类必需品的能量转移给自己。因此，如果游戏失败，我们将面临灾难性的后果。</p><p id="9993" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="mt"> """ </em></p><p id="6eae" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="mt">在与比我们聪明得多的东西比赛时，赢得“使用最后一点可用能量”的竞争可能会非常困难。失败是致命的。</em></p><p id="8e40" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="mt"> """ </em></p><h2 id="ff72" class="lq jo iq bd jp lr ls dn jt lt lu dp jx ld lv lw kb lh lx ly kf ll lz ma kj mb bi translated">哪些假设允许人工智能篡改奖励机制？</h2><p id="9f8e" class="pw-post-body-paragraph ks kt iq ku b kv mc kx ky kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp ij bi translated">作者提到了以下假设，这些假设最终使人工智能学习活动有可能干预奖励机制:</p><ol class=""><li id="b95e" class="mu mv iq ku b kv kw kz la ld mw lh mx ll my lp mz na nb nc bi translated">代理至少能像人类一样识别可能的目标。</li></ol><p id="ea20" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这意味着人工智能足够先进，即使代码中没有明确的目标子例程，它仍以某种方式编码在其巨大的神经网络的深处。理解目标可能使人工智能能够操纵它。</p><p id="4022" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="ku ir"> 2。当不确定时，代理人理性地寻求知识，对真正的回报或如何感知回报的物理表现没有偏见。</strong></p><p id="b6f5" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">代理更喜欢简单的解决方案。假设我们将相机和魔盒作为奖励输入呈现给代理，并要求它学习奖励的权重。代理人会理性地选择一个简单的假设。在操纵奖励比学习实际任务更简单的情况下，代理会这样做。例如，在上面的例子中，将一张写有1的纸放在摄像机前。</p><p id="8fbf" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="ku ir"> 3。从奖赏被感知的物理表现中解开</strong> <strong class="ku ir">真正奖赏的实验成本很小。</strong></p><p id="d78c" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这类似于上面的例子；如果很容易找出哪种奖励机制可以很容易地被学习/黑客攻击，那么代理人就会对更容易的奖励机制产生偏见(通过将一张写有1的纸放在摄像机前获得)。如果实验的成本很高，这两种奖励机制可能会得到同等的重视。</p><p id="c6f1" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="ku ir"> 4。如果我们不能令人信服地找到排除实现可能性的理论论据，那么对于一个具有足够丰富行动空间的代理人来说，这很可能是可能的。</strong></p><p id="0539" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">对于一个高级人工智能来说，如果我们不能从理论上证明一个人工智能不能干预奖励机制，那么它很可能会干预。对于AI只能执行受限动作的简单情况，这个假设是不成立的。然而，对于一个先进的人工智能来说，访问互联网也可以打开无数复杂动作的大门。</p><p id="e6d1" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="ku ir"> 5。如果有获胜的可能，一个足够高级的代理很可能能够在博弈中击败一个次优的代理。</strong></p><p id="5144" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">高级的定义表明人工智能有能力找到最好的政策并执行它们。如果存在干预奖励机制的政策，那么先进的人工智能会发现它。即使当人类在这样的游戏中与AI对弈时，击败AI也不会比在围棋中击败AlphaGo容易。</p><h2 id="3cd3" class="lq jo iq bd jp lr ls dn jt lt lu dp jx ld lv lw kb lh lx ly kf ll lz ma kj mb bi translated">这份名单对人工智能的进步提出了一个严峻的未来。然而，这些后果是完全可以避免的。请在这里阅读防止“可能的”人工智能起义的框架。</h2><p id="1f29" class="pw-post-body-paragraph ks kt iq ku b kv mc kx ky kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp ij bi translated"><strong class="ku ir">参考文献</strong></p><p id="5332" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">[1]科恩，M. K .，哈特，m .，&amp;奥斯本，M. A. (2022)。<a class="ae ms" href="https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064" rel="noopener ugc nofollow" target="_blank">高级人工智能体介入提供奖励</a>。<em class="mt">艾杂志</em>(第1-12页)。【点睛版论文<a class="ae ms" href="https://mkcohen-hosted-files.s3.us-west-1.amazonaws.com/AI-safety-presentation.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>；<a class="ae ms" href="https://twitter.com/Michael05156007/status/1567240026307575808" rel="noopener ugc nofollow" target="_blank">推特帖子</a></p><p id="4f75" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">[2] <a class="ae ms" href="https://paperswithcode.com/paper/competition-level-code-generation-with" rel="noopener ugc nofollow" target="_blank">用AlphaCode生成竞赛级代码，DeepMind等人，2022 </a></p></div></div>    
</body>
</html>