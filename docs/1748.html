<html>
<head>
<title>Support Vector Machine (SVM) Introduction — Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)简介—机器学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/support-vector-machine-svm-introduction-machine-learning-8c56b7da63f1?source=collection_archive---------0-----------------------#2021-04-09">https://pub.towardsai.net/support-vector-machine-svm-introduction-machine-learning-8c56b7da63f1?source=collection_archive---------0-----------------------#2021-04-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9a3d05c02f10c903b2e5a14653fb93fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XH5xCDGEFwj2g8X-kMg-w.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">支持向量机(SVM)图用于我们在葡萄酒数据集上的实现，如下所示|来源:图片由作者提供。</figcaption></figure><h2 id="3b9a" class="jg jh ji bd b dl jj jk jl jm jn jo dk jp translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/editorial" rel="noopener ugc nofollow" target="_blank">社论</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="3d65" class="pw-subtitle-paragraph ko jr ji bd b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf dk translated">介绍机器学习中的支持向量机(SVM ),以及它们在有监督的最大似然学习中的作用，并举例说明</h2></div><p id="f78e" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li js">作者:</strong> <a class="ae mc" href="https://www.linkedin.com/in/sujan-shirol/" rel="noopener ugc nofollow" target="_blank">苏扬·希罗</a>，<a class="ae mc" href="https://mktg.best/vguzs" rel="noopener ugc nofollow" target="_blank">罗伯托·伊里翁多</a></p><div class="is it gp gr iu md"><a href="https://members.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd js gy z fp mi fr fs mj fu fw jr bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">向着AI加入。通过成为会员，你不仅将支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><p id="0c86" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi ms translated"><span class="l mt mu mv bm mw mx my mz na di"> S </span> VM代表支持向量机(support vector machine)，虽然可以同时解决分类和回归问题，但主要用于<a class="ae mc" href="https://mld.ai/mldcmu" rel="noopener ugc nofollow" target="_blank"> <strong class="li js">机器学习</strong> </a> <strong class="li js"> (ML) </strong>中的分类问题。SVM模型帮助我们根据以前分类的类似数据对新数据点进行分类，使其成为一种受监督的机器学习技术。本文的参考资料可以在<a class="ae mc" href="https://colab.research.google.com/drive/151vvpQelTep3ftrvpK3LzqTeyF_HwFGW?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="li js"> Google Colab </strong> </a>或<a class="ae mc" href="https://github.com/towardsai/tutorials/tree/master/support-vector-machine-svm" rel="noopener ugc nofollow" target="_blank"> <strong class="li js"> Github </strong> </a>上找到。</p><h2 id="a64b" class="nb nc ji bd nd ne nf dn ng nh ni dp nj lp nk nl nm lt nn no np lx nq nr ns jo bi translated">机器学习中的分类是什么？</h2><p id="98d0" class="pw-post-body-paragraph lg lh ji li b lj nt ks ll lm nu kv lo lp nv lr ls lt nw lv lw lx nx lz ma mb im bi translated"><strong class="li js">分类</strong>是一项受监督的ML任务，它需要机器学习算法来学习如何为来自问题领域的示例分配类别标签。一个容易理解的例子是将电子邮件分类为“<em class="ny">垃圾邮件</em>或“<em class="ny">非垃圾邮件</em>”</p><p id="e683" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在机器学习中，我们可能会遇到几种不同类型的分类任务，以及可用于每个<a class="ae mc" href="https://machinelearningmastery.com/types-of-classification-in-machine-learning/#:~:text=In%20machine%20learning%2C%20classification%20refers,one%20of%20the%20known%20characters." rel="noopener ugc nofollow" target="_blank">【1】</a>的专门建模方法。这种模型将需要输入数据和标签来学习和理解，然后估计分类或映射新输入数据的最佳方法。</p><p id="1455" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有许多不同类型的算法，SVM是其中之一，是最广泛使用的。分类任务有四种类型:</p><ol class=""><li id="39a0" class="nz oa ji li b lj lk lm ln lp ob lt oc lx od mb oe of og oh bi translated">二元分类:两个类别标签，垃圾邮件/非垃圾邮件，0/1。</li><li id="da05" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">多类分类:两个以上的类标签面临分类。</li><li id="fe11" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">多标签分类:每个实例可以被分配多个类别。</li><li id="cc22" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">不平衡分类:每个类中的实例数量不相等。</li></ol><blockquote class="on oo op"><p id="a3b7" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated">如果你还没有退房，推荐我们的<a class="ae mc" href="https://news.towardsai.net/knn" rel="noopener ugc nofollow" target="_blank"><strong class="li js">K-近邻教程</strong> </a>，因为它是有监督机器学习中学习分类的基本机器学习算法之一。</p></blockquote><div class="is it gp gr iu md"><a rel="noopener  ugc nofollow" target="_blank" href="/k-nearest-neighbors-knn-algorithm-tutorial-machine-learning-basics-ml-ec6756d3e0ac"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd js gy z fp mi fr fs mj fu fw jr bi translated">k近邻(KNN)算法教程—机器学习基础</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">潜入K-最近邻，一个基本的经典机器学习(ML)算法</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">pub.towardsai.net</p></div></div><div class="mm l"><div class="ot l mo mp mq mm mr ja md"/></div></div></a></div><p id="1cee" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">支持向量机负责在高维或无限维空间中构建一个超平面或一组超平面，进而可用于分类、回归或其他任务，如离群值检测。</p><p id="762c" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">直观地，通过具有到任何类别的最近训练数据点的最显著距离的超平面(所谓的函数裕度)来实现良好的分离，因为一般来说，裕度越大，分类器的泛化误差越低[<a class="ae mc" href="https://en.wikipedia.org/wiki/Support-vector_machine#Definition" rel="noopener ugc nofollow" target="_blank">2]</a>。</p><blockquote class="on oo op"><p id="6ac7" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated"><strong class="li js">术语警告</strong></p><p id="5849" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated">1.支持向量</p><p id="dd7c" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated">2.超平面</p><p id="7045" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated">3.功能保证金(从现在起称为保证金)</p><p id="1c97" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated">4.线性可分的</p><p id="ca5e" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated">5.非线性可分的</p></blockquote><p id="0047" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae mc" href="https://news.towardsai.net/mla" rel="noopener ugc nofollow" target="_blank"> <strong class="li js">机器学习算法</strong> </a>因为使用的术语，听起来很花哨，很吓人。然而，它们很容易理解，并且来源于日常生活事件。</p><p id="db73" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们用分类例子来理解SVM的每个术语。</p><h2 id="8a88" class="nb nc ji bd nd ne nf dn ng nh ni dp nj lp nk nl nm lt nn no np lx nq nr ns jo bi translated">SVM的功能</h2><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/45059b65133a7f1f1c35a17a41ffaf08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l5yQxBwZJlwQN3jeGFG5ug.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:SVM的功能</figcaption></figure><p id="2f42" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">该图像代表了理想的SVM分类器在二维平面上的样子。有两类:阴性和阳性，分别用红色和蓝色标记。将它们标记为消极和积极的确切原因将会更加明显。SVM在二维特征中创建一个平面，在多维特征中创建一个超平面，称为<em class="ny">决策边界</em>。</p><p id="875f" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">超平面分离特征，并帮助根据其值对新的输入数据点进行分类。可以生成n个可能的超平面，可以把类分开，找到最优的？这时<em class="ny">裕度</em>和<em class="ny">支持向量</em>开始发挥作用。</p><p id="93d9" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当生成决策边界的超平面时，它还生成两个被称为<em class="ny">边缘平面、</em>的超平面，这两个超平面精确地平行于决策边界，使得这两个边缘平面都通过各自类别的至少一个最近点。这两个边缘平面通过的最近点称为支持向量。</p><p id="ac24" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最佳决策边界是两个平行边缘超平面之间距离最大的边界，它们之间的距离称为<em class="ny">边缘</em>。因此，SVM算法致力于最大限度地提高利润。</p><p id="7b21" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">关于数据分布有两类分类问题:<em class="ny">线性可分</em>和<em class="ny">非线性可分</em>。如果平面上至少有一条线，所有的蓝点在线的一边，所有的红点在线的另一边，那么两组是线性可分的<a class="ae mc" href="https://en.wikipedia.org/wiki/Linear_separability" rel="noopener ugc nofollow" target="_blank">【3】</a>。</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/7460c76f1df72139bde3f6a1811ac492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2sN3CGigCOy0cTWchfwneQ.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:非线性可分离点。</figcaption></figure><p id="1ab7" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当数据点不能用直线或直超平面分开时，这些类型被称为非线性可分的。在这种情况下，SVM核起作用并将低维转换成高维，使得数据点是线性可分的。如需了解更多信息，请查看下面的视频。</p><figure class="ov ow ox oy gt iv"><div class="bz fp l di"><div class="pa pb l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">来源:视频由<a class="ae mc" href="https://www.youtube.com/channel/UCcf4LQogGFtYzPhq05uHE4g" rel="noopener ugc nofollow" target="_blank"> udiprod </a></figcaption></figure><h2 id="f2d3" class="nb nc ji bd nd ne nf dn ng nh ni dp nj lp nk nl nm lt nn no np lx nq nr ns jo bi translated">SVM用例</h2><ol class=""><li id="db11" class="nz oa ji li b lj nt lm nu lp pc lt pd lx pe mb oe of og oh bi translated">人脸检测</li><li id="240e" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">文本和超文本分类</li><li id="f1a9" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">图像分类</li><li id="9a14" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">生物信息学</li><li id="1065" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">远程同源性检测</li><li id="c233" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">笔迹检测</li><li id="81a1" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">广义预测控制</li></ol><h2 id="5f90" class="nb nc ji bd nd ne nf dn ng nh ni dp nj lp nk nl nm lt nn no np lx nq nr ns jo bi translated">SVM背后的数学</h2><p id="1500" class="pw-post-body-paragraph lg lh ji li b lj nt ks ll lm nu kv lo lp nv lr ls lt nw lv lw lx nx lz ma mb im bi translated">超平面的方程由下式给出:</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/10bb32e09adaef7af09bf1ea2b855a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*85fibvyP5MBRsAlh1fq-bg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3:超平面的方程。</figcaption></figure><p id="09cf" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">其中输出<em class="ny"> y </em>指示它是在正类还是负类中。<em class="ny"> w是表示飞机参数的矩阵</em>，也是<em class="ny"> x </em>的系数，其中<em class="ny"> x </em>是输入数据。<em class="ny"> b </em>代表超平面的截距。</p><p id="afb1" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当我们有多维特征时，比如说有<em class="ny"> p </em>个特征，等式扩展为:</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/e956fae4968f83208168e4c1cf9ac16b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*QjWAuc2KmS4NgPt6jLMQjg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:超平面的扩展方程。</figcaption></figure><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/bef6847947d8de620f267087dc874dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GS1qDNi9PDVGmIBTyOySsA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图5:超平面中的支持向量机(SVM)。</figcaption></figure><p id="80e7" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在我们考虑的这个特定例子中，我们期望分类器方程的输出是正数，表示数据点属于正类，或者是负数，表示数据点属于负类。如果任意点正好在决策边界上，那么分类器的输出将为零，因此，决策边界的等式为:</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/150184761bab0219a5ed7d9ba58080c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*_qzuauZNDUl-Vubfaly5WA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图6:决策边界的等式。</figcaption></figure><p id="a6db" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">类似地，边缘超平面的分类器方程:</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/8cfce0990a13ed89c3ebbda7075cd8b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*B2hiTgcBQCyPwiiMl13whw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7:边缘超平面的方程。</figcaption></figure><p id="adf6" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们不要盲目相信它。我们将继续推导为什么这个方程的输出不是正的就是负的。</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pj"><img src="../Images/9761e16cdc586559b3a5d3824297e754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EkCZXPsI4G4HhnqLYZnFIA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:等式表示的推导。</figcaption></figure><p id="b1b7" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑上述问题，其中判定边界穿过原点，因此截距为零，斜率为+1。超平面每一侧的单个数据点代表正类和负类。代入超平面方程中的值:</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/b4767d70592d3bdfc443d3ed7eb34177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vHOI3ok1WMTMendeH2g0g.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:超平面方程中的值替换。</figcaption></figure><p id="9733" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">超平面下面的任何点总是正的，超平面上面的点总是负的。</p><p id="9ec9" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">接下来，必须最大化裕量，以找到最佳决策边界。考虑负支持向量为点<em class="ny"> x1，正支持向量为<em class="ny">点x2。</em>余量就是<em class="ny"> x1 </em>和<em class="ny"> x2 </em>之差。让我们借助解线性方程组来得出这个方程。数学上，我们有两个带有两个未知数的方程(<em class="ny"> x1 </em>和<em class="ny"> x2 </em>)。要找出未知数，从一个方程中减去另一个方程。</em></p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/a5cb8357751b6d085d56055e9ea6f377.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*2sXEASOt5qn2WyC6bbEGXg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:寻找未知数。</figcaption></figure><p id="dede" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了找到<em class="ny"> x1 </em> - <em class="ny"> x2 </em>，<em class="ny"> w </em>必须被送到等式的左侧，这给出了2除以<em class="ny"> w </em>。已经知道<em class="ny"> w </em>是向量，向量不能像标量值一样直接分割。相当于将两边除以<em class="ny"> w、</em>的长度，即<em class="ny"> w </em>的范数的大小。</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/68229ef930c8882bda9340f8aa661bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*8hCxbTea-gEfPEAuAouAHw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:<em class="pn">w</em>的范数的大小。</figcaption></figure><p id="352f" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当<em class="ny"> w </em>除以它的范式时，它仍然指向相同的方向，但是幅度将是1个单位，相当于标量1。因此</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi po"><img src="../Images/f1f91297ace2d4e0ad82f6ab894e1b1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*E8E23WYNvZIQ912wchnuDw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12:量级1相当于标量1。</figcaption></figure><p id="1f16" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">既然我们已经得到了余量的等式，那么就认为需要使用优化算法(如梯度下降)来最大化优化函数。优化算法在寻找局部最小值时工作得最好，因此为了缓解问题，最小化<em class="ny"> x1 </em> - <em class="ny"> x2 </em>的倒数可以用作优化函数，这是<em class="ny"> w </em>超过2的规范。</p><p id="fcf7" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">SVM模型也可能具有一定百分比的误差，即新数据的错误分类，并且这必须被集成到我们的优化函数中，其中<em class="ny"> Ci </em>表示误差点的数量，换句话说，错误分类的数据点的数量以及边缘超平面和错误分类的数据点之间的距离的总和。</p><blockquote class="on oo op"><p id="48a2" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated"><strong class="li js">术语警告</strong></p><p id="ac18" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated">1.<strong class="li js">硬裕度</strong>:不能容忍任何误差的裕度称为硬裕度。它对异常值很敏感，单个异常值就能影响决策边界。</p><p id="dc66" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated">2.<strong class="li js">软余量</strong>:可以考虑一定百分比误差的余量称为软余量。对异常值不敏感。</p></blockquote><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/79c2fdd054c582f308a8e935cf80bd7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*ontfM4cjsXLZWhoG7rWNVQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图13:硬边界和软边界。</figcaption></figure><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/bce67586e10a32cdfd68623d28862af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*_dln-UjKtTAMZovYbYmXLg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图14:最终函数。</figcaption></figure><blockquote class="on oo op"><p id="1bd8" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated"><strong class="li js">总结:</strong></p><p id="3587" class="lg lh ny li b lj lk ks ll lm ln kv lo oq lq lr ls or lu lv lw os ly lz ma mb im bi translated">完全理解数学并不是强制性的。如果你不能理解推导过程，也没关系。基本上，必须知道SVM创建了一个超平面，作为分类数据点的决策边界，为了找到正确的边界，需要考虑两个超平面，它们之间的距离必须最大。</p></blockquote><h2 id="e274" class="nb nc ji bd nd ne nf dn ng nh ni dp nj lp nk nl nm lt nn no np lx nq nr ns jo bi translated">SVM核</h2><p id="61df" class="pw-post-body-paragraph lg lh ji li b lj nt ks ll lm nu kv lo lp nv lr ls lt nw lv lw lx nx lz ma mb im bi translated">为线性可分的数据点构建超平面时，SVM很容易。然而，当数据是非线性可分的时，就更具挑战性了。如上所述，SVM核有助于将低维非线性可分离数据点转换成高维线性可分离数据点。有三种广为人知的SVM核:</p><ol class=""><li id="5ab5" class="nz oa ji li b lj lk lm ln lp ob lt oc lx od mb oe of og oh bi translated">多项式</li><li id="5ab3" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">径向基函数</li><li id="0437" class="nz oa ji li b lj oi lm oj lp ok lt ol lx om mb oe of og oh bi translated">乙状结肠的</li></ol><p id="4b97" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们通过将1-d非线性可分数据转换成2-d线性可分数据来建立内核如何工作的直觉。</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/98097ca945d6fb7c384d85efc9c9c2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*42cjAR_Nzukt7FR5X0OV4g.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图15:内核转换。</figcaption></figure><p id="9968" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑上述数据点，它是二元分类(1和0)的一个例子。不可能画出一条直线来区分这两个阶级。对每个点应用变换会将一维数据点转换为二维数据点，有助于建立决策边界。这里，转换函数是:</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/bd29698e76c9b596468f78053bba80ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*waThERF7m8ebsU9OFZalXw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图16:运输功能。</figcaption></figure><p id="882a" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">也就是说，将每个数据点平方，并将结果绘制为第二维(<em class="ny"> y轴</em>)。</p><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/3cc7c03a1ea09a08348c5370a76ab088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*63EFSnK_p1edaDQ9xBt_OA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图17:第二维度的结果图(y轴)。</figcaption></figure><p id="e053" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">原始数据点绘制在<em class="ny"> x轴</em>上，变换后的数据点(x-square)绘制在<em class="ny"> y轴</em>上。现在，数据点很容易通过绘制一个线性超平面来分离。可视化更高维度的数据点将是具有挑战性的。然而，SVM核也类似地处理高维数据点，将低维数据转换成高维数据，以便它们转换成线性可分离的数据点。</p><h2 id="b456" class="nb nc ji bd nd ne nf dn ng nh ni dp nj lp nk nl nm lt nn no np lx nq nr ns jo bi translated">履行</h2><p id="26bb" class="pw-post-body-paragraph lg lh ji li b lj nt ks ll lm nu kv lo lp nv lr ls lt nw lv lw lx nx lz ma mb im bi translated">python的实现非常简单。Python开源为我们提供了现成的包来实现几种机器学习算法，SVM也不例外。使用sklearn的SVC包提供了几个优点和灵活性。SVC代表支持向量分类，实现基于<a class="ae mc" href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/" rel="noopener ugc nofollow" target="_blank"> <strong class="li js"> libsvm </strong> </a> <strong class="li js"> </strong>和一个围绕svm的包装器。让我们导入必要的包。</p><pre class="ov ow ox oy gt pu pv pw px aw py bi"><span id="e5cd" class="nb nc ji pv b gy pz qa l qb qc">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="a051" class="nb nc ji pv b gy qd qa l qb qc">#classic datasets from sklearn library<br/>from sklearn import datasets</span><span id="484c" class="nb nc ji pv b gy qd qa l qb qc">from sklearn.model_selection import train_test_split</span><span id="d671" class="nb nc ji pv b gy qd qa l qb qc">#Support Vector Classification-wrapper around SVM<br/>from sklearn.svm import SVC</span><span id="d865" class="nb nc ji pv b gy qd qa l qb qc">#different matrices to score model performance<br/>from sklearn import metrics<br/>from sklearn.metrics import classification_report,confusion_matrix</span></pre><p id="e2f0" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">出于实现的目的，我们将使用一个葡萄酒数据集。这是一个多类数据集，包含3个类、178个样本和13个要素。这一数据来自于对意大利同一地区种植的三种不同品种的葡萄酒的化学分析。分析确定了三种葡萄酒中13种成分的含量。</p><p id="f268" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们从sklearn库中加载葡萄酒数据。由于数据是从sklearn加载的，如果我们将它存储到一个<strong class="li js">数据帧</strong>中，使用起来会更容易。</p><pre class="ov ow ox oy gt pu pv pw px aw py bi"><span id="f51e" class="nb nc ji pv b gy pz qa l qb qc">#loading WINE dataset<br/>cancer_data = datasets.load_wine()</span><span id="99cb" class="nb nc ji pv b gy qd qa l qb qc">#storing into DataFrame<br/>df = pd.DataFrame(cancer_data.data, columns = cancer_data.feature_names)<br/>df['target'] = cancer_data.target<br/>df.head()</span></pre><p id="595d" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通过绘制图表来执行分析将有助于更好地理解数据集。总是建议检查数据是否平衡，也就是说，是否所有的目标类都有相同数量的记录。</p><pre class="ov ow ox oy gt pu pv pw px aw py bi"><span id="1229" class="nb nc ji pv b gy pz qa l qb qc">#analysing target variable<br/>sns.countplot(df.target)<br/>plt.show()</span></pre><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/762d125687600fc50d4a1453c8abf4eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*YFVwEtKX3Su9E6olQsj0Iw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图18:目标类的图表。</figcaption></figure><p id="96fc" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">作为一个多类分类问题，它也代表了一个不平衡的分类问题。接下来，让我们检查我们的数据是否是线性可分的。这将给出一个初步的推断，以决定哪种类型的内核最适合对数据进行分类。为了简单起见，让我们根据酒精绘制每个特征，并根据它们的类别给数据点着色。</p><pre class="ov ow ox oy gt pu pv pw px aw py bi"><span id="3c7a" class="nb nc ji pv b gy pz qa l qb qc">#visualizing datapoints separability<br/>fig, axes = plt.subplots(6, 2, figsize=(22,14))<br/>axes = [ax for axes_rows in axes for ax in axes_rows]<br/>columns = list(df.columns)<br/>columns.remove('target')<br/>columns.remove('alcohol')</span><span id="26ff" class="nb nc ji pv b gy qd qa l qb qc">#looping through every columns of data<br/>#and plotting against alcohol<br/>for i, col in enumerate(columns):<br/>   sns.scatterplot(data=df, x='alcohol', y=col, hue='target',         palette="deep", ax=axes[i])</span></pre><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qf"><img src="../Images/b9c990acaf3f2bad16be54a461606021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c0te3TO_ypDFDZchwvbbPQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图19:葡萄酒数据集的数据类特性。</figcaption></figure><p id="1eab" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">显而易见，数据点是线性可分的，当然有一些重叠百分比，这是现实世界数据中的理想情况。现在构建线性SVM更有意义。在构建任何模型之前，建议将数据集分为训练数据和测试数据。训练数据用于训练我们的模型，测试用于评估已训练模型的性能。理想情况下，80%的数据用于训练，20%用于测试。</p><pre class="ov ow ox oy gt pu pv pw px aw py bi"><span id="3fea" class="nb nc ji pv b gy pz qa l qb qc">#splitting data into 80:20 train test ratio<br/>X = df.drop('target', axis=1)<br/>y = df.target</span><span id="a10d" class="nb nc ji pv b gy qd qa l qb qc">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)</span></pre><p id="30b2" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="ny"> X_train </em>和<em class="ny"> X_test </em>包含所有特征，随机选择80%的行，而<em class="ny"> y_train </em>和<em class="ny"> y_test </em>只包含相应行的目标值。让我们使用sklearn库中的SVC包来构建线性SVM模型。函数<em class="ny"> predict() </em>用于预测特征<em class="ny"> x </em>数据的目标<em class="ny"> y </em>值。</p><pre class="ov ow ox oy gt pu pv pw px aw py bi"><span id="992a" class="nb nc ji pv b gy pz qa l qb qc">#training SVM model with linear kernel<br/>model = SVC(kernel='linear', random_state = 10)<br/>model.fit(X_train, y_train)</span><span id="7f94" class="nb nc ji pv b gy qd qa l qb qc">#predicting output for test data<br/>pred = model.predict(X_test)</span></pre><p id="a2cf" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">混淆矩阵是评价分类模型性能的首选方法之一。它通过绘制预测值与实际值的图表，对正确分类和错误分类的几个测试数据点进行可视化分析。</p><pre class="ov ow ox oy gt pu pv pw px aw py bi"><span id="be40" class="nb nc ji pv b gy pz qa l qb qc">#building confusion matrix<br/>cm = confusion_matrix(y_test, pred)</span><span id="1633" class="nb nc ji pv b gy qd qa l qb qc">#defining the size of the canvas<br/>plt.rcParams['figure.figsize'] = [15,8]</span><span id="dc6d" class="nb nc ji pv b gy qd qa l qb qc">#confusion matrix to DataFrame<br/>conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1', 'Predicted:2'], index = ['Actual:0','Actual:1', 'Actual:2'])</span><span id="202d" class="nb nc ji pv b gy qd qa l qb qc">#plotting the confusion matrix<br/>sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Paired', cbar = False,linewidths = 0.1, annot_kws = {'size':25})<br/>plt.xticks(fontsize = 20)<br/>plt.yticks(fontsize = 20)<br/>plt.show()</span></pre><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qg"><img src="../Images/0f34a7e017a45a11e131705c6a4e54fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fEkLxe3lEtfn9GmTBS5Y_A.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图20:预测值与实际值的对比。</figcaption></figure><p id="eb4b" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">0、1和2是类别，该图给出了属于相应块的测试数据点的数量。<em class="ny">类0 </em>和<em class="ny"> 2 </em>的所有10个数据点都被正确预测/分类。也就是说，预测等于实际。而<em class="ny">类别1 </em>的2个数据点被错误预测为<em class="ny">类别2 </em>。更多的推论，如精确度、召回率、准确度分数、f1分数等等。混淆矩阵可以用<em class="ny">分类_报告</em>包提取。</p><pre class="ov ow ox oy gt pu pv pw px aw py bi"><span id="d765" class="nb nc ji pv b gy pz qa l qb qc">print(classification_report(y_test,pred))</span></pre><figure class="ov ow ox oy gt iv gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/6d840dd5a394bbf3c602c245cd7e2f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*CsBOPRg_qmxw7sbyZX7o6w.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图21:结果，突出显示我们的SVM具有94%的显著准确性。</figcaption></figure><p id="ae9f" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">准确率得分显著94%。来自类别1的两个错误分类数据解释了6%准确度的损失，这是合理可接受的和预期的。</p></div><div class="ab cl qi qj hx qk" role="separator"><span class="ql bw bk qm qn qo"/><span class="ql bw bk qm qn qo"/><span class="ql bw bk qm qn"/></div><div class="im in io ip iq"><p id="faa8" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li js">免责声明:</strong>本文所表达的观点均为作者个人观点，不代表与作者(直接或间接)相关的任何公司的观点。这项工作并不打算成为最终产品，而是当前思想的反映，同时也是讨论和改进的催化剂。</p><p id="2944" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li js">除非另有说明，所有图片均来自作者。</strong></p><p id="2581" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">经由<a class="ae mc" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">发布<strong class="li js">走向AI </strong>发布</a></p><h2 id="82fa" class="nb nc ji bd nd ne nf dn ng nh ni dp nj lp nk nl nm lt nn no np lx nq nr ns jo bi translated">资源:</h2><div class="is it gp gr iu md"><a href="https://github.com/towardsai/tutorials/tree/master/support-vector-machine-svm" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd js gy z fp mi fr fs mj fu fw jr bi translated">支持向量机知识库| Github</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">AI相关教程。免费访问其中任何一个→https://towardsai.net/editorial-toward sai/教程</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">github.com</p></div></div><div class="mm l"><div class="qp l mo mp mq mm mr ja md"/></div></div></a></div><div class="is it gp gr iu md"><a href="https://colab.research.google.com/drive/151vvpQelTep3ftrvpK3LzqTeyF_HwFGW?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd js gy z fp mi fr fs mj fu fw jr bi translated">SVM介绍——谷歌合作实验室</h2><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">colab.research.google.com</p></div></div><div class="mm l"><div class="qq l mo mp mq mm mr ja md"/></div></div></a></div><h2 id="7927" class="nb nc ji bd nd ne nf dn ng nh ni dp nj lp nk nl nm lt nn no np lx nq nr ns jo bi translated">参考资料:</h2><p id="bea0" class="pw-post-body-paragraph lg lh ji li b lj nt ks ll lm nu kv lo lp nv lr ls lt nw lv lw lx nx lz ma mb im bi translated">[1]詹森·布朗利。2020.“机器学习中的4类分类任务”。机器学习精通。<a class="ae mc" href="https://machinelearningmastery.com/types-of-classification-in-machine-learning/." rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/types-of-class ification-in-machine-learning/。</a></p><p id="0e0e" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[2]“支持向量机——维基百科”。2021.En.Wikipedia.Org。<a class="ae mc" href="https://en.wikipedia.org/wiki/Support-vector_machine#Definition." rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Support-vector _ machine #定义。</a></p><p id="5428" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[3]“线性可分性——维基百科”。2021.En.Wikipedia.Org。<a class="ae mc" href="https://en.wikipedia.org/wiki/Linear_separability." rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Linear_separability.</a></p><p id="015c" class="pw-post-body-paragraph lg lh ji li b lj lk ks ll lm ln kv lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[4]《UCI机器学习知识库:葡萄酒数据集》。2021.Archive.Ics.Uci.Edu。https://archive.ics.uci.edu/ml/datasets/wine.<a class="ae mc" href="https://archive.ics.uci.edu/ml/datasets/wine." rel="noopener ugc nofollow" target="_blank"/></p></div><div class="ab cl qi qj hx qk" role="separator"><span class="ql bw bk qm qn qo"/><span class="ql bw bk qm qn qo"/><span class="ql bw bk qm qn"/></div><div class="im in io ip iq"><div class="ov ow ox oy gt md"><a href="https://ws.towardsai.net/shop" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd js gy z fp mi fr fs mj fu fw jr bi translated">店铺↓ |走向AI</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">发布最好的技术、科学和工程|社论→https://towardsai.net/p/editorial |订阅→…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">ws.towardsai.net</p></div></div><div class="mm l"><div class="qr l mo mp mq mm mr ja md"/></div></div></a></div><div class="is it gp gr iu md"><a href="https://members.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd js gy z fp mi fr fs mj fu fw jr bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">向着AI加入。通过成为会员，你不仅将支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="qs l mo mp mq mm mr ja md"/></div></div></a></div><div class="is it gp gr iu md"><a href="https://sponsors.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd js gy z fp mi fr fs mj fu fw jr bi translated">赞助商|了解如何成为《走向人工智能》的赞助商</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">无论你是想以一种吸引读者的方式突出你的产品，吸引高度相关的小众受众，还是…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">sponsors.towardsai.net</p></div></div><div class="mm l"><div class="qt l mo mp mq mm mr ja md"/></div></div></a></div></div></div>    
</body>
</html>