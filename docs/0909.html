<html>
<head>
<title>5 Ways To Improve The Performance Of Your Task-Oriented Conversational AI Agent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">提高面向任务的对话式人工智能代理性能的5种方法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/5-ways-to-improve-the-performance-of-your-task-based-conversational-ai-agent-1b3ae7db5f94?source=collection_archive---------2-----------------------#2020-09-09">https://pub.towardsai.net/5-ways-to-improve-the-performance-of-your-task-based-conversational-ai-agent-1b3ae7db5f94?source=collection_archive---------2-----------------------#2020-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1d03" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="6035" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">根据我在ConvAI第二次NLP研讨会上学到的知识</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/56f632517ea02ec7c643b578d75995c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ad-40qKlDR2QLUcw2r430w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:https://sites.google.com/view/2ndnlp4convai/home<a class="ae lh" href="https://sites.google.com/view/2ndnlp4convai/home" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><h1 id="a8ee" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="5b71" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在本文中，我将总结5个想法来改进构成面向任务的对话代理的各种组件。我是在参加与ACL 2020一起举办的第二届对话式人工智能NLP研讨会时了解到这些想法的。</p><p id="0dc6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果你对我在主要的ACL 2020会议上学到的想法感兴趣，请查看我在ACL 2020 文章中学到的<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/5-machine-learning-ideas-i-learned-at-acl-2020-781750387c53" rel="noopener"> 5个机器学习想法。</a></p><h1 id="043c" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">概观</h1><p id="935d" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">以下是本文将涉及的观点:</p><ol class=""><li id="3b4a" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated">构建对自动语音识别(ASR)错误具有鲁棒性的嵌入</li><li id="8e73" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">一种构建对ASR错误具有鲁棒性的代理的数据扩充技术</li><li id="5aac" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">使用上下文信息来改进槽填充</li><li id="8c4d" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">将元学习应用于意图分类和槽填充</li><li id="9d7c" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">为意图分类构建计算高效的文本表示</li></ol><h1 id="b036" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">主意</h1><h2 id="5569" class="np lj it bd lk nq nr dn lo ns nt dp ls mj nu nv lu mn nw nx lw mr ny nz ly iz bi translated">构建对自动语音识别(ASR)错误具有鲁棒性的嵌入</h2><p id="b08a" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">接受语音作为输入的代理通常具有ASR模块，该ASR模块将用户的话语从音频转录为文本，然后将其传递给自然语言理解(NLU)模块进行进一步处理，例如意图分类、槽填充、攻击性过滤等。</p><p id="4dc9" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这种设置的挑战是ASR模块中的错误会传播到下游模块。一类特别令人烦恼的ASR错误是对声学上相似但语义上不同的单词的错误识别，例如“fare”被转录为“fair”。</p><p id="ae84" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[1]提出了一种新颖的微调方法来解决这个问题。他们的方法包括微调语境化的单词嵌入，例如带有附加损失项的ELMo。该损失项被设计成使得发音相似的单词的余弦相似性最大化，从而ASR模块容易混淆的单词对将具有相似的向量表示。</p><p id="d6f7" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在ATIS数据集(既有音频记录又有用户话语的手动转录文本)上的实验表明，当针对音频记录进行评估时，他们的方法优于不使用他们的微调方法的基线。当针对转录文本进行评估时，他们的方法产生了与基线模型相同的结果。</p><p id="5251" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">他们实现的代码在<a class="ae lh" href="https://github.com/MiuLab/SpokenVec" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> Github </strong> </a> <strong class="mc jd">上。</strong></p><h2 id="1ae8" class="np lj it bd lk nq nr dn lo ns nt dp ls mj nu nv lu mn nw nx lw mr ny nz ly iz bi translated">一种构建对ASR错误具有鲁棒性的代理的数据扩充技术</h2><p id="e796" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">当话语的音频记录可用时，在[1]中描述的用于减轻ASR错误的影响的方法工作良好。当情况并非如此时，可以考虑[2]提出的思路。</p><p id="f6fb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[2]基本上建立了一个模拟器来模拟ASR假设。然后将这些假设添加到训练数据中。ASR假设模拟器的细节在[3]中描述。</p><p id="7ac0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这种方法的主要优点是不需要对代理的任何组件进行任何更改。因此，这种方法可以被视为其他方法的补充，以构建对ASR错误具有鲁棒性的代理。</p><p id="0666" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">此外，作者还表明，他们的方法将更有利于简单的模型架构，而不是复杂的架构，例如手套嵌入与BERT。因此，当在受模型大小、计算能力或延迟限制的环境中构建代理时，这是一种很有前途的技术。</p><h2 id="dd97" class="np lj it bd lk nq nr dn lo ns nt dp ls mj nu nv lu mn nw nx lw mr ny nz ly iz bi translated">使用上下文信息来改进槽填充</h2><p id="5e92" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">槽填充任务通常被视为序列标记任务。如今，组成话语的文本使用语境化嵌入(如ELMo、BERT)转换为向量。根据[4],在将上下文结合到嵌入中方面可以做得更多:</p><blockquote class="oa ob oc"><p id="b56f" class="ma mb od mc b md mw kd mf mg mx kg mi oe my ml mm of mz mp mq og na mt mu mv im bi translated">…现有模型以受限的<br/>方式使用上下文信息，例如使用自我关注。这些方法不能区分上下文对单词表示和单词标签的影响。</p></blockquote><p id="3642" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">作者使用BiLSTM架构来生成用于执行槽填充任务的特征序列。为了将更多的上下文结合到生成的特征中，他们建议在多任务设置中使用以下辅助任务来训练模型:</p><ol class=""><li id="8415" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated">最大化话语中每个单词与其上下文的互信息</li><li id="2daf" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">仅使用其上下文来预测话语中每个单词的标签</li><li id="51b5" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">对于话语中的每个句子，使用BiLSTM层的输出来预测出现在句子中的标签。</li></ol><p id="d286" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在这种情况下，作者报告了在SNIPS、ATIS和EditMe数据集上的最新研究结果。他们的消融研究表明，辅助任务2对他们的模型性能的贡献最大。</p><h2 id="1a5b" class="np lj it bd lk nq nr dn lo ns nt dp ls mj nu nv lu mn nw nx lw mr ny nz ly iz bi translated">将元学习应用于意图分类和槽填充</h2><p id="f8b0" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">[5]提出了一种称为少量拍摄意图分类/槽填充的任务。他们基于ATIS、TOPS和SNIPS数据集为这项任务建立了一个基准。</p><p id="387a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">他们研究的元学习方法是原型网络和模型不可知元学习(MAML)。被评估的模型是BiLSTM模型的变体，其中输入嵌入来自GloVe、ELMo + GloVe或BERT嵌入。</p><p id="322c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这篇论文的一个令人惊讶的结果是，作者试图微调BERT导致了较差的结果。</p><h2 id="9d33" class="np lj it bd lk nq nr dn lo ns nt dp ls mj nu nv lu mn nw nx lw mr ny nz ly iz bi translated">构建计算高效的文本表示</h2><p id="651d" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">[6]是我在研讨会上最喜欢的论文，因为它的实用性和可重复性。</p><p id="2e18" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在本文中，作者将从通用句子编码器和转换模型获得的话语表示串联起来，冻结它们，并在其上训练一个多层感知器来执行意图分类。这种方法在CLINC150、HWU64和BANKING 77数据集上产生了最先进的结果。</p><p id="cc1d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">BANKING 77数据集是作者贡献的一个新数据集，它只覆盖了一个领域(银行业)，但具有非常细粒度的意图。它包括13，083个客户服务查询，标记有77个意向。</p><p id="5e4a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">除了简单之外，这种方法还有以下好处:</p><ul class=""><li id="6450" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv oh nh ni nj bi translated">少量拍摄设置中的卓越性能</li><li id="44e1" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv oh nh ni nj bi translated">对超参数的选择不变</li><li id="3016" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv oh nh ni nj bi translated">能够在不使用GPU或TPU的情况下进行训练和推理</li></ul><p id="765e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">复制论文中描述的结果的代码是<a class="ae lh" href="https://github.com/PolyAI-LDN/polyai-models/tree/master/intent_detection" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><h1 id="103a" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">结论</h1><p id="52d1" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">本文总结了5个有助于提高面向任务的对话代理性能的想法。</p><p id="c650" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果你想了解更多，请查阅参考资料。</p><h1 id="c08c" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">参考</h1><p id="b80f" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">[1] <a class="ae lh" href="https://arxiv.org/abs/1909.10861" rel="noopener ugc nofollow" target="_blank">学习ASR——健壮的语境化嵌入用于口语理解</a>。黄和陈。2019</p><p id="29fa" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[2] <a class="ae lh" href="https://arxiv.org/abs/2006.05635" rel="noopener ugc nofollow" target="_blank">对语音识别错误稳健的训练对话模型的数据增强</a>。王等人。艾尔。2020</p><p id="72fb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[3] <a class="ae lh" href="https://arxiv.org/abs/1911.03378" rel="noopener ugc nofollow" target="_blank">研究用于学习会话错误恢复的对话策略的错误模拟技术</a>。法泽尔-扎兰迪等人。艾尔。2019</p><p id="4b31" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[4] <a class="ae lh" href="https://arxiv.org/abs/1911.01680" rel="noopener ugc nofollow" target="_blank">利用上下文信息改进槽填充</a>。Veyseh等人。艾尔。2019</p><p id="099b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[5] <a class="ae lh" href="https://arxiv.org/abs/2004.10793" rel="noopener ugc nofollow" target="_blank">学习对意图和槽标签进行分类，给出一些例子</a>。克朗等人。艾尔。2020</p><p id="9ebb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[6] <a class="ae lh" href="https://arxiv.org/abs/2003.04807" rel="noopener ugc nofollow" target="_blank">使用双语句编码器进行有效的意图检测</a>。卡萨努埃瓦等人。艾尔。2020</p></div></div>    
</body>
</html>