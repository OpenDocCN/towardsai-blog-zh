<html>
<head>
<title>How To Train a Seq2Seq Summarization Model Using “BERT” as Both Encoder and Decoder!! (BERT2BERT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练一个Seq2Seq摘要模型，使用“BERT”作为编码器和解码器！！(伯特2伯特)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-train-a-seq2seq-summarization-model-using-bert-as-both-encoder-and-decoder-bert2bert-2a5fb36559b8?source=collection_archive---------1-----------------------#2022-06-20">https://pub.towardsai.net/how-to-train-a-seq2seq-summarization-model-using-bert-as-both-encoder-and-decoder-bert2bert-2a5fb36559b8?source=collection_archive---------1-----------------------#2022-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="b352" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko"> BERT是一个众所周知的强大的预训练“编码器”模型。让我们看看如何将它用作“解码器”来形成编码器-解码器架构。</em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/4533d2896af9f96b8f7fae5dc4489a5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NuzM-evy2ghHEWix"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">照片由<a class="ae lf" href="https://unsplash.com/@aaronburden?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚伦·伯顿</a>在<a class="ae lf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="e121" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Transformer架构由两个主要构建模块组成，即编码器和解码器组件，我们将它们堆叠在一起，形成seq2seq模型。(你可以在我的<a class="ae lf" rel="noopener ugc nofollow" target="_blank" href="/how-to-train-a-seq2seq-text-summarization-model-with-sample-code-ft-huggingface-pytorch-8ba97492f885">上一篇文章</a>中了解更多)从头开始训练一个基于transformer的模型通常很难，因为它需要大型数据集和高GPU内存。因此，有<a class="ae lf" href="https://medium.com/mlearning-ai/what-are-the-differences-in-pre-trained-transformer-base-models-like-bert-distilbert-xlnet-gpt-4b3ea30ef3d7" rel="noopener">许多预先训练好的模型</a>有着不同的目标。</p><p id="7b4e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，<strong class="js iu">编码器</strong>模型(例如，BERT、RoBERTa、FNet、…)学习如何从它们读取的文本创建固定大小的表示。这种表示可用于训练网络进行分类、翻译、总结等。其次，基于<strong class="js iu">解码器</strong>的型号(如GPT家族)具有发电能力。通过在顶部添加一个线性层(也称为“语言模型头”)，使他们能够预测下一个令牌，这是可能的。最后，<strong class="js iu">编码器-解码器</strong>型号(BART、Pegasus、MASS……)能够根据编码器的表示调节解码器的输出。它可用于摘要和翻译等任务。这是通过从编码器到解码器的交叉注意连接来实现的。</p><p id="a5c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个故事中，我想展示如何使用仅编码器模型的预训练权重来给我们一个微调的开端。在这个例子中，我们将训练一个以BERT作为编码器和解码器的摘要模型。</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><p id="4b34" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Huggingface库前段时间引入了一个新的API，使得混合和匹配不同的预训练模型成为可能。它非常灵活，让我们的工作变得超级简单！但是，在开始编写代码之前，让我们先看看这个概念。要使BERT(一个编码器模型)在seq2seq设置下工作，应该做些什么？</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ln"><img src="../Images/9c4d0b1ba91300fd42791a7e24aa2c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t60pBpi5Q88bJ24XA9SQiw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">图二。(左)BERT等纯编码器模型与(右)BART等同时具有编码器和解码器的网络的高级方案。</figcaption></figure><p id="04d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请记住，为了简单起见，图2中演示的网络中的其他元素已被删除！为了做一个简单的比较，<strong class="js iu">编码器专用</strong>模型(左)的每个块(层)由一个自我关注和一个线性层组成。同时，<strong class="js iu">编解码</strong>网络(右)在每层也有交叉注意连接。交叉关注层使模型能够根据输入来调节预测。</p><p id="703c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">显而易见，直接将BERT模型用作解码器是不可能的，因为构建模块并不相同！理论上，很容易添加额外的连接，并使用BERT权重设置解码器的适用部分。然后，我们需要微调模型来训练这些连接和语言模型头部权重。(注意:语言模型头部位置在输出和最后一个线性层之间——它不包括在图2中)</p><p id="72bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用Huggingface的<code class="fe lo lp lq lr b">EncoderDecoderModel</code>对象来混合和匹配不同的预训练模型。它将通过调用<code class="fe lo lp lq lr b">.from_encoder_decoder_pretrained()</code>方法并指定编码器/解码器模型来添加所需的连接和权重。在下面的例子中，我们使用BERT-base作为编码器和解码器。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ls lt l"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">代码1。加载预训练模型的代码。</figcaption></figure><p id="298e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于BERT模型不是为文本生成而设计的，我们需要做一些配置。因此，下一步是设置标记器，并指定句首和句尾标记，以正确指导训练和推理过程。它应该在模型的配置和它的tokenizer对象中定义。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ls lt l"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">代码2。将bos和eos令牌添加到model和tokenizer中。</figcaption></figure><p id="bf61" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以使用拥抱脸的<a class="ae lf" href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainer" rel="noopener ugc nofollow" target="_blank"> Seq2Seq Trainer </a>对象，使用<code class="fe lo lp lq lr b">Seq2SeqTrainingArguments()</code>参数来微调模型。您可以更改和试验许多配置，以获得适合您模型的完美组合。请注意，以下值不是最佳选择，仅用于测试！如果你没有GPU内存的话，<code class="fe lo lp lq lr b">fp16</code>值是很重要的一个。它将使用半精度数字，这减少了内存的使用。要研究的另一个有用变量是<code class="fe lo lp lq lr b">learning_rate</code>、<code class="fe lo lp lq lr b">batch_size</code>。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="ls lt l"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">代码3。Hugingface的seq2seq Trainer对象。</figcaption></figure><p id="ac03" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我不打算经历整个微调过程，因为我已经提到了如何使用<code class="fe lo lp lq lr b">datasets</code>库来加载数据。这里有一个到<a class="ae lf" href="https://github.com/NLPiation/tutorial_notebooks/blob/main/summarization/hf_BERT-BERT_training.ipynb" rel="noopener ugc nofollow" target="_blank"> Google Colab笔记本</a>的链接(我已经将代码复制/粘贴到我的GitHub帐户，以确保链接保持有效)，这是我从<code class="fe lo lp lq lr b"><a class="ae lf" href="https://huggingface.co/patrickvonplaten/bert2bert_cnn_daily_mail" rel="noopener ugc nofollow" target="_blank">patrickvonplaten/bert2bert_cnn_daily_mail</a></code> hub检查点获取的。笔记本将检查整个微调过程，以训练模型进行总结。</p><h1 id="6f4b" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">结果</h1><p id="9983" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">我们可以看到在CNN/DM数据集上微调的BERT-to-BERT模型性能。我使用了数据集中枢上的可用检查点，使用了波束搜索解码方法。使用ROUGE评分标准计算结果。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mx"><img src="../Images/e9d067f1e8801c416b0e8e5adc4dc186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wTAszuX96PjN1b-it4eVbg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk translated">图3。比较BERT-base和BART-base的结果。</figcaption></figure><p id="2ebd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">BART模型是文本摘要中的SOTA模型，并且BERT seq2seq网络保持得相当好！只有1%的差异通常不会转化为句子质量的巨大变化。</p><h1 id="0554" class="lu lv it bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">最后的话，</h1><p id="0755" class="pw-post-body-paragraph jq jr it js b jt ms jv jw jx mt jz ka kb mu kd ke kf mv kh ki kj mw kl km kn im bi translated">混合搭配的方法可以产生令人兴奋的实验。例如，可以将BERT连接到GPT-2，以使用BERT的能力创建文本的强大表示，并使用GPT的能力生成高质量的句子。在为所有问题选择SOTA模型之前，最好对自定义数据集使用不同的网络。使用BERT(与BART相比)的主要区别是512个令牌输入序列长度限制(与1024个相比)。因此，如果数据集的输入序列较小，则BERT-to-BERT模型是一个不错的选择。训练更小的模型会更有效，并且需要更少的资源，例如数据和GPU内存。</p><blockquote class="my"><p id="c1c4" class="mz na it bd nb nc nd ne nf ng nh kn dk translated">我每周给NLP的书呆子发一份时事通讯。如果您想了解自然语言处理的最新发展，可以考虑订阅。<br/> <a class="ae lf" href="https://nlpiation.github.io/" rel="noopener ugc nofollow" target="_blank">阅读更多，订阅</a> —加入酷孩子俱乐部，立即报名！</p></blockquote></div></div>    
</body>
</html>