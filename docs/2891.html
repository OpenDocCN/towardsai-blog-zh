<html>
<head>
<title>Modern Spam Detection with DistilBERT on NVIDIA Triton</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在NVIDIA Triton上使用DistilBERT检测现代垃圾邮件</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/nvidia-triton-spam-detection-engine-of-c-suite-labs-cc77db2c2cfe?source=collection_archive---------4-----------------------#2022-07-01">https://pub.towardsai.net/nvidia-triton-spam-detection-engine-of-c-suite-labs-cc77db2c2cfe?source=collection_archive---------4-----------------------#2022-07-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a200ff263a3f853639abc67b27260a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_KENY7xpidTTBTlagclTA.jpeg"/></div></div></figure></div><div class="ab cl jy jz hu ka" role="separator"><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd"/></div><div class="ij ik il im in"><p id="c33d" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir">作者:</strong> <a class="ae ld" href="https://www.linkedin.com/in/jiripik" rel="noopener ugc nofollow" target="_blank"> Jiri Pik </a>，<a class="ae ld" href="https://www.linkedin.com/in/saumyahuja/" rel="noopener ugc nofollow" target="_blank"> Saumya Ahuja </a>，<a class="ae ld" href="https://www.linkedin.com/in/arulkumarasan/" rel="noopener ugc nofollow" target="_blank"> Janakan Arulkumarasan </a>，<a class="ae ld" href="https://www.linkedin.com/in/ebjattardo/" rel="noopener ugc nofollow" target="_blank">ermano Attardo</a>，<a class="ae ld" href="https://www.linkedin.com/in/kshitiz-gupta/" rel="noopener ugc nofollow" target="_blank"> Kshitiz Gupta </a>，<a class="ae ld" href="https://www.linkedin.com/in/jiahong-liu514/" rel="noopener ugc nofollow" target="_blank">刘家宏</a></p></div><div class="ab cl jy jz hu ka" role="separator"><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd"/></div><div class="ij ik il im in"><h1 id="0549" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">1.介绍</h1><p id="060d" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">每年，不需要的和未经请求的大量数字通信(“垃圾邮件”)造成了大量直接和间接的经济损失。</p><p id="840e" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">通过检测某些关键字、手动检查文本记录、甚至运行自然语言处理(“NLP”)管道来识别垃圾邮件的传统方法已经不够用了。</p><p id="4a6e" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir">本文描述了一种最先进的垃圾邮件检测引擎的架构，该引擎针对带有URL链接的社交网络帖子，由多个相互依赖的不同分类器组成，可提供实时、高性能、卓越的准确性，所需的人工审查最少。</strong></p><p id="1244" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">本文介绍了一个由三部分组成的垃圾邮件检测系统，该系统结合了现有的最佳技术方法，并侧重于NLP分类器的DistilBERT模型。</p><p id="de9f" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir">NVIDIA Triton推理服务器在该用例中的应用提供了比AWS TorchScript推理服务器高2.4倍的推理吞吐量，而模型延迟低52.9倍。</strong></p><p id="6432" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">文章的GitHub资源库是<a class="ae ld" href="https://github.com/jiripik/nvidia-triton-c-suite-labs-spam-detection-engine" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/jiri pik/NVIDIA-triton-c-suite-labs-spam-detection-engine</a>。</p><h1 id="d169" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">2.单一因素方法不起作用</h1><p id="81bc" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">随着时间的推移，我们意识到<strong class="kh ir">没有可靠的公共API考虑多种因素来生成总体垃圾邮件分数</strong>。</p><p id="66ad" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">部分的、单一因素的解决方案根本不起作用:</p><ul class=""><li id="ab42" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated"><em class="mv">阻止列表不起作用</em> —阻止已知滥用域名、关键字、IP地址或用户名的解决方案非常容易被绕过</li><li id="5fd5" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated"><em class="mv">文本分析方法不起作用</em> —分析评论中文本的解决方案，如OOPSpam，无法解决隐藏在预览图像或目标页面中的文本</li><li id="6bb5" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated"><em class="mv">基于域的方法不起作用</em> —旨在通过研究托管域的声誉来确定URL链接是否安全的解决方案，如谷歌Web Risk API，不起作用——任何人都可以将包含垃圾文本的视频上传到YouTube，这些服务会认为它是安全的，因为它托管在受信任的域(YouTube)上。</li></ul><h1 id="cb2f" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">3.现代垃圾邮件检测引擎的蓝图</h1><p id="a447" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">我们从三个不同的角度来处理这个问题:用户、消息和链接。每个社交网络的记录都来自一个来源，即用户，来自一个具有<em class="mv"> IP地址</em>的设备，遵循<em class="mv">特定的UI步骤序列</em>。记录的内容有一条消息，可能包含一个链接。</p><p id="f58d" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir">我们的联合设计由一系列模型组成:</strong></p><ol class=""><li id="ae0f" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc nb ms mt mu bi translated"><strong class="kh ir"> IP &amp; URL模型</strong></li></ol><ul class=""><li id="32b0" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">发帖者的IP地址—发帖者的IP地址是有效的垃圾邮件指示器，随着时间的推移，阻止它们可以显著减少垃圾邮件帖子。我们的引擎识别持续发布垃圾邮件的IP地址。</li><li id="cf5d" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">链接URL域—垃圾邮件URL遵循类似的模式。因此，如果URL模式与以前报告的URL匹配，则预示着垃圾邮件。关键特性是:<br/> + url_age:链接在web上处于活动状态的时间<br/>+URL _ wot _ score:URL的信任分数<br/>+URL _ Google _ score:URL的Google Web风险分数<br/>+URL _ domain:URL的域<br/>+URL _ TLD:URL的网络位置顶级域<br/>+URL _ subdomain _ count:URL中的子域数量<br/> + url_token:</li><li id="441d" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">链接url上下文——我们在平台的历史数据上运行数据辩论技术，并生成与垃圾邮件相关的特征:<br/>+URL _ post _ count:URL被发布的次数<br/> + post_share_count:带有该URL的消息被共享的次数<br/> + post_comment_count:帖子得到的响应/吸引的次数<br/>+domain _ report _ count:URL的域名被报告的次数<br/> + text_content:的内容</li></ul><p id="4415" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir"> 2。社会行为模型</strong></p><ul class=""><li id="e683" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">海报简介——我们研究的特征包括:( 1)朋友数量;( 2)在我们平台上的停留时间;( 3)用户过去在社交网站上的活动。如果用户配置文件与之前报告为垃圾邮件的其他帐户相似，则很有可能是垃圾邮件发送者用户配置文件。此外，用户的历史也可能指示垃圾邮件帐户。结合这些， 与垃圾邮件相关的简档特征有:<br/> + user_age:用户在平台上拥有帐户的时间<br/> + user_email_domain:用户注册的电子邮件地址的域<br/> + user_friends:用户的朋友数量<br/> + user_followers:用户的关注者数量<br/> + user_verified:一个布尔值，用于确定用户是否有影响力并被平台验证<br/> + user_posts:数量 用户在平台上发布的帖子数<br/> + user_spam_posts:用户帖子被举报为垃圾信息的次数<br/> + user_spam_report:一个布尔值，用于确定用户是否被举报为垃圾信息<br/> + user_link_count:用户共享此链接的次数<br/> + user_post_count:用户共享此帖子的次数</li><li id="52b4" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">海报之旅——我们分析用户发布社交网络新记录的步骤顺序。例如，垃圾邮件发送者或机器人倾向于利用高效的工作流。</li><li id="325f" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">用户反馈——我们分析其他用户对每个社交网络记录的投诉模式</li></ul><p id="ee0e" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir"> 3。内容模型</strong></p><ul class=""><li id="698f" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">NLP分析—我们分析用户帖子和链接网页的内容，包括对页面上任何图像的OCR分析以及所有视频摘录的文字记录。该模型基于监督或半监督学习模型，如伯特和GPT-3。</li></ul><p id="fc22" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">我们应用基于神经网络和机器学习的方法为每个子系统开发一系列模型。它们的输出然后被输入到主要的机器学习模型，一个决策树，输出最终的分类结果:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/7c019840b1d724ec2e9f44c4d694c15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHMwJVmq1rrlAg_AscVSZw.png"/></div></div></figure><p id="cc3f" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">该架构带来了以下见解:</p><ol class=""><li id="fa05" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc nb ms mt mu bi translated">如果URL域已经被安全禁止，我们可以立即得出结论，新社交网络的记录是垃圾邮件，并停止。</li><li id="4aa5" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc nb ms mt mu bi translated">然后，社交行为分析提供用户的新帖子是否是垃圾邮件的指示——例如，过去的投诉(如果存在)和发帖人的旅程是新帖子是垃圾邮件的非常强的指标(大约90%的准确率)。</li><li id="bdd1" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc nb ms mt mu bi translated">最后，我们对用户帖子的文本内容和相关图像或视频的文本表示进行NLP分析。</li></ol><h1 id="d701" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">4.垃圾邮件引擎的DistilBERT内容模型简介</h1><h1 id="fe04" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">4.1伯特模型</h1><p id="8690" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated"><a class="ae ld" href="https://arxiv.org/abs/2202.03480" rel="noopener ugc nofollow" target="_blank">谷歌的基于变形金刚(BERT)的双向编码器表示无外壳模型</a>是当前最先进的文本分类模型，根据训练集的质量，准确率超过97%。</p><p id="4afa" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><a class="ae ld" href="https://towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766" rel="noopener" target="_blank"> <strong class="kh ir">伯特架构基于</strong> </a></p><ul class=""><li id="422f" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated"><em class="mv">一个Transformer编码器模型</em>(一个神经网络，它获取一个输入句子并标记每个单词，输出是每个标记的矢量数字表示)。它提供了比LSTM或RNN更好的语境理解，因为它同时处理句子。它使用单词前后的输入来构建单词的上下文，而LSTM或RNN只考虑单词之前的输入→ BERT具有更好的性能。例如，“我需要苹果”和“我需要苹果产品”中的单词“苹果”在LSTM或RNN具有相同的向量值，但在伯特中却不同。</li><li id="cd87" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated"><em class="mv">转移学习</em>(为一般任务训练一个模型，并重用它来微调BERT以适应新任务)。伯特接受了图书语料库(8亿字，11038本书——与GPT-1相同)和英语维基百科(25亿字)的训练。在64个TPU上用了4天。预训练较慢，但微调较快。有时候可以在单个GPU上完成。</li></ul><p id="08f3" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir">伯特有两个任务:</strong></p><p id="8609" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><em class="mv">任务#1:掩蔽语言模型(MLM) </em>最初由Devlin等人于2018年在谷歌发布。</p><ol class=""><li id="3eb8" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc nb ms mt mu bi translated">预测随机15%的(子)单词标记。为了正则化，它:在80%的时间里用掩码替换输入单词。在10%的情况下，用随机令牌替换输入单词。它在10%的时间里保持不变(但仍然预测它)</li><li id="92bf" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc nb ms mt mu bi translated">它使用一个变压器编码器来做到这一点</li><li id="eddc" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc nb ms mt mu bi translated">BERT的预训练输入是两个独立的连续序列</li></ol><p id="0523" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><em class="mv">任务2:下一句话预测(NSP) </em>。MLM不支持句子之间关系的概念。在这个任务中，模型被教导句子之间的关系。</p><p id="9e3d" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">塞尔吉奥·罗哈斯-加莱亚诺的一项研究强调了</p><ul class=""><li id="51da" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">BoW(单词袋)、TFIDF(词频逆文档频率)和BERT编码器可以使用广泛使用的分类算法提取有效的函数来识别垃圾邮件，但BERT的性能略好。这证实了他们之前引用的文献。</li><li id="0b04" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">经验证据表明，BERT可以抵抗Mad-lib攻击，而BoW或TFiDF则易受攻击。它使用对抗性的自动程序来产生攻击。</li></ul><p id="892b" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">Andrew McCarren和Jennifer Foster的研究表明，BERT优于基于FFNN、CNN和LSTM的神经模型。</p><h1 id="5848" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">4.2蒸馏模型</h1><p id="8306" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">对于我们的垃圾邮件检测引擎，我们选择了<a class="ae ld" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank">的DistilBERT变体</a>，根据其作者的说法，它是“BERT模型大小的60%，同时保留了97%的语言理解能力，速度快了60%”，这意味着它的应用也更具成本效益。</p><p id="b38f" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">我们使用了<a class="ae ld" href="https://huggingface.co/docs/transformers/model_doc/distilbert" rel="noopener ugc nofollow" target="_blank">的HuggingFace实现</a>。</p><h1 id="36e4" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">4.3训练集</h1><p id="1b7e" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">我们的训练集由以下数据集组合而成:</p><ul class=""><li id="0c3a" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">专业标记的垃圾邮件数据集</li><li id="e3cc" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">手动管理的垃圾邮件数据集</li><li id="71c4" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">基于规则的标记数据集。</li></ul><h1 id="4b03" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">4.4培训代码</h1><p id="9315" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">下面是我们的DistilBERT代码的简化实现，在本文的GitHub库中:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="a563" class="nm lf iq ni b gy nn no l np nq">logging.info('Loading the pretrained tokenizer and model')<br/>tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')<br/>model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")<br/><br/>logging.info('Preparing the training and evaluation dataset')<br/>train_data, val_data, train_labels, val_labels = train_test_split(dataset[COL_DATA].values, dataset[LABEL].values)<br/>train_tokens = tokenizer(list(train_data), return_tensors="pt", padding=True, truncation=True, max_length=BATCH_SIZE)<br/>val_tokens = tokenizer(list(val_data), return_tensors="pt", padding=True, truncation=True, max_length=BATCH_SIZE)<br/><br/>train_dataset = ClassificationDataset(train_tokens, train_labels)<br/>val_dataset = ClassificationDataset(val_tokens, val_labels)<br/><br/>logging.info('Training Started')<br/>trainer = Trainer(<br/>    model=model,<br/>    args=TrainingArguments(output_dir=TRAIN_DIR, num_train_epochs=NUM_EPOCHS),<br/>    train_dataset=train_dataset,<br/>    eval_dataset=val_dataset,<br/>    compute_metrics=compute_metrics,<br/>)<br/><br/>trainer.train()<br/><br/>model.save_pretrained(FINAL_DIR)<br/>tokenizer.save_pretrained(FINAL_DIR)<br/>logging.info('Training Completed')<br/><br/>print("**************** Evaluation ************")<br/>metrics = trainer.evaluate()<br/>metrics["eval_samples"] = len(val_dataset)<br/>trainer.log_metrics("eval", metrics)<br/>trainer.save_metrics("eval", metrics)</span></pre><h1 id="cc4c" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">5.英伟达TensorRT和英伟达Triton推理服务器简介</h1><p id="a901" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated"><a class="ae ld" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">NVIDIA TensorRT</strong></a>是一款用于高性能深度学习推理的SDK，包括</p><ul class=""><li id="0674" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">提供低延迟的深度学习推理优化器和运行时</li><li id="ffdb" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">推理应用的高吞吐量。</li></ul><p id="bf07" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">TensorRT可以理解为深度学习编译器，产生一个NVIDIA GPU优化的二进制可执行文件(<strong class="kh ir"> TensorRT engine </strong>)。</p><p id="9289" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><a class="ae ld" href="https://developer.nvidia.com/nvidia-triton-inference-server" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> NVIDIA Triton推理服务器</strong> </a>是一款开源推理服务器，有助于标准化模型部署和执行，并在生产中提供快速和可扩展的AI，支持大多数机器学习框架，以及自定义C++和Python代码。</p><p id="36c2" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">Triton的后端:</p><ul class=""><li id="b674" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">TensorRT是推荐使用Triton进行GPU优化推理的后端。</li><li id="39c6" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">将TensorFlow或PyTorch模型转换为tensort的最佳方式是将其转换为ONNX模型，然后将ONNX模型转换为tensort。在ONNX到TensorRT的转换步骤中，TensorRT优化器步骤运行若干优化，例如层融合，这产生了高度GPU优化的推理模型。</li><li id="274d" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">可以选择另一个Triton支持的后端，比如TensorFlow、Torchscript、ONNX等。</li></ul><p id="c497" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir">您应该使用NVIDIA Triton推理服务器来部署机器学习模型的主要原因是，与其他模型服务解决方案相比，它可以提高吞吐量和硬件利用率。</strong></p><h1 id="c437" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">6.AWS设置</h1><p id="183e" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">相对于其他公共云，我们更喜欢AWS的可靠性、成本效益和易用性。我们更喜欢英伟达GPU来训练深度学习模型的性能。</p><h1 id="aef0" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">6.1 AWS推理环境</h1><p id="bfec" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">AWS为训练和部署机器学习模型提供了两种解决方案:</p><ol class=""><li id="90d6" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc nb ms mt mu bi translated"><a class="ae ld" href="https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html" rel="noopener ugc nofollow" target="_blank"> AWS SageMaker笔记本实例</a>用于训练，<a class="ae ld" href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html" rel="noopener ugc nofollow" target="_blank"> AWS SageMaker推理端点</a>用于推理</li><li id="96b9" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc nb ms mt mu bi translated"><a class="ae ld" href="https://aws.amazon.com/ec2/" rel="noopener ugc nofollow" target="_blank"> AWS EC2实例</a>用于训练，而<a class="ae ld" href="https://aws.amazon.com/ecs/" rel="noopener ugc nofollow" target="_blank"> AWS ECS </a>用于推理</li></ol><p id="a9b3" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">每种解决方案都有其优点和缺点:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/9cbfd7e99cdacb35b35eaffaf3da7720.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dza3ltsT0ED6JIf6BngsrQ.jpeg"/></div></div></figure><p id="9b05" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">此外，我们强烈建议不要将不同版本的PyTorch / TensorFlow的输出作为另一个版本的PyTorch / TensorFlow的输入，例如，尝试将一个版本从EC2实例导入SageMaker笔记本。在大多数情况下，您只会在ML管道的末端收到一条错误消息——NVIDIA Triton的SageMaker端点将无法初始化。</p><h1 id="1913" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">6.2将模型部署到NVIDIA Triton推理服务器</h1><p id="26d5" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">下面我们介绍使用AWS SageMaker Notebook实例将所述模型部署到运行NVIDIA Triton推理服务器的Docker映像<a class="ae ld" href="https://aws.amazon.com/marketplace/pp/prodview-mzfjpok66eclw" rel="noopener ugc nofollow" target="_blank">的AWS SageMaker推理端点。</a></p><p id="909b" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">我们描述了<a class="ae ld" href="https://github.com/jiripik/nvidia-triton-c-suite-labs-spam-detection-engine/blob/main/SpamDetection-Triton.ipynb" rel="noopener ugc nofollow" target="_blank">解决方案的Jupyter笔记本</a>中概述的步骤以及预期输出。</p><h2 id="7b8d" class="nm lf iq bd lg ns nt dn lk nu nv dp lo kq nw nx ls ku ny nz lw ky oa ob ma oc bi translated">6.2.1步骤0 —设置AWS SageMaker笔记本实例</h2><p id="da96" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">AWS提供了多种适合深度学习的计算实例类型。查看本指南<a class="ae ld" href="https://cdn.jiripik.com/2022/06/08122920/AL-ML-for-Startups-Select-the-Right-ML-Instance.pdf" rel="noopener ugc nofollow" target="_blank">以决定哪一个最适合您的使用案例。</a></p><p id="7e82" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">我们使用了这个实例:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi od"><img src="../Images/09f4ef9447fa1126210d878b3c8e7260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PbyRPwczT2Vie3JO.png"/></div></div></figure><p id="e167" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">注意:</p><ol class=""><li id="7264" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc nb ms mt mu bi translated">我们将所需的磁盘空间增加到1 TB的安全值。</li><li id="b958" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc nb ms mt mu bi translated">我们不需要使用<a class="ae ld" href="https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html" rel="noopener ugc nofollow" target="_blank">弹性推理</a>——它是用来添加一个分数GPU，以更低的成本加速推理。在我们的例子中，我们已经在使用一个完整的V100 GPU附带的ml.p3.2xlarge的GPU实例→我们不需要弹性推理。</li><li id="8aff" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc nb ms mt mu bi translated">我们用的是亚马逊Linux 2的平台标识符，Jupyter Lab 1。使用不同的值可能会导致版本冲突。</li><li id="58be" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc nb ms mt mu bi translated">Jupyter实验室用于解决方案笔记本的内核是“conda _ amazonei _ py torch _ latest _ p37”。使用不同的内核可能会导致版本冲突。</li></ol><h2 id="3295" class="nm lf iq bd lg ns nt dn lk nu nv dp lo kq nw nx ls ku ny nz lw ky oa ob ma oc bi translated">6.2.2步骤1 —安装模型所需的库</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="76e6" class="nm lf iq ni b gy nn no l np nq">!pip install torch -U<br/>!pip install -qU pip awscli boto3 sagemaker transformers<br/>!pip install nvidia-pyindex<br/>!pip install tritonclient[http]<br/>!pip3 install pickle5<br/>!pip install datasets<br/>!pip install nltk</span></pre><p id="c783" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">注意:我们已经将PyTorch库升级到了最新版本。如果没有这个步骤，我们将无法保存ONNX文件并将其部署到NVIDIA Triton推理服务器。此升级会导致较小的库版本冲突—我们不依赖于它们:</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="99f9" class="nm lf iq ni b gy nn no l np nq">ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.<br/>torchvision 0.6.1 requires torch==1.5.1, but you have torch 1.11.0 which is incompatible.<br/>torcheia 1.0.0 requires torch==1.5.1, but you have torch 1.11.0 which is incompatible.</span></pre><h2 id="233a" class="nm lf iq bd lg ns nt dn lk nu nv dp lo kq nw nx ls ku ny nz lw ky oa ob ma oc bi translated">6.2.3步骤2-训练模型</h2><p id="1a12" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">该代码已在第4.4节中描述。</p><p id="72a3" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">通常，我们会添加一个在更大的测试数据集上评估模型的步骤。我们不与解决方案共享数据集，因此我们省略了。</p><h2 id="b3b2" class="nm lf iq bd lg ns nt dn lk nu nv dp lo kq nw nx ls ku ny nz lw ky oa ob ma oc bi translated">6.2.4步骤3 —生成ONNX文件</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="9a32" class="nm lf iq ni b gy nn no l np nq">!docker run --gpus=all --rm -it -v `pwd`/workspace-trt:/workspace nvcr.io/nvidia/pytorch:21.08-py3 /bin/bash generate_models.sh</span></pre><p id="c505" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">ONNX文件由<a class="ae ld" href="https://github.com/jiripik/nvidia-triton-c-suite-labs-spam-detection-engine/blob/main/workspace-trt/generate_models.sh" rel="noopener ugc nofollow" target="_blank">workspace-TRT/generate _ models</a>中的脚本生成，需要出现在解决方案中。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="f699" class="nm lf iq ni b gy nn no l np nq">#!/bin/bash<br/>pip install transformers[onnx]<br/>python -m transformers.onnx --model=./ --feature=sequence-classification ./<br/>trtexec --onnx=model.onnx --saveEngine=model_bs16.plan --minShapes=input_ids:1x128,attention_mask:1x128 --optShapes=input_ids:1x128,attention_mask:1x128 --maxShapes=input_ids:1x128,attention_mask:1x128 --fp16 --verbose --workspace=14000 | tee conversion_bs16_dy.txt</span></pre><p id="e5fe" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir"> trtexec </strong>是TensorRT的命令行工具，用于从onnx文件构建. plan优化TensorRT模型文件。其参数<strong class="kh ir">–save engine</strong>(此处<em class="mv"> model_bs16.plan </em>)用于指定输出引擎的名称。</p><p id="dcd4" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">您可以通过在PyTorch NGC容器中做<strong class="kh ir">TRT exec-–help</strong>来了解更多信息。</p><p id="9fc4" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">注意TensorRT的trtexec命令的参数指定了<strong class="kh ir">输入和输出的形状</strong>:</p><ol class=""><li id="3193" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc nb ms mt mu bi translated"><strong class="kh ir">批量大小</strong></li></ol><ul class=""><li id="837d" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">当我们将minShapes指定为1×128，optShapes指定为1×128，maxShapes指定为1×128时，我们被定义为固定的批处理大小，这意味着模型只能接受批处理大小1。</li><li id="907d" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">然而，TensorRT支持构建具有动态形状的优化引擎，因此我们也可以，例如，将minShapes指定为1×128，将optShapes指定为16×128，将maxShapes指定为128×128，这意味着该模型可以接受1到128之间的批量大小，并且针对批量大小16进行了优化。</li></ul><p id="b61e" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir"> 2。序列长度</strong></p><ul class=""><li id="6957" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">它由这些形状轮廓中的第二维表示。在本例中，它固定为128(但是您可以将其更改为最适合您的模型的序列长度)。</li></ul><p id="93fe" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ir">警告:只有ONNX到TensorRT的转换步骤需要在与部署GPU相同的GPU上进行<br/>。</strong></p><p id="a03c" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">这意味着，如果您在具有g4dn.xlarge(具有T4 GPU)的端点上进行部署，那么您需要确保ONNX到TensorRT转换步骤(我们也称之为构建TensorRT引擎)需要在T4 GPU (g4dn实例)上。</p><p id="7e8a" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">从训练模型到将模型导出到ONNX的所有其他事情都可以在训练GPU上发生，训练GPU可以是p3实例、p4实例甚至g4dn实例。</p><p id="022b" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">trtexec命令支持的参数取决于其版本:</p><ul class=""><li id="cfb1" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">PyTorch 21.08 NGC容器具有tensort 8.0.1.6，它不支持DistilBERT模型的动态形状。在PyTorch 22.04 NGC容器中，此限制在最近的TensorRT版本8.2.4.2中得到修复。</li><li id="08d3" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">然而，在当前的SageMaker推断端点版本中使用最新的PyTorch NGC容器是不可能的。为了在Triton中成功运行TensorRT模型，我们还需要更新到较新的Triton版本，如22.05，它具有TensorRT版本8.2.4.2或更高版本。目前SageMaker支持的最新Triton容器版本是21.08，其中有旧的tensort版本8.0.1.6，但我们需要tensort v 8 . 2 . 4 . 2或更高版本，最近有<a class="ae ld" href="https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcatalog.ngc.nvidia.com%2Forgs%2Fnvidia%2Fcontainers%2Ftritonserver&amp;data=05%7C01%7Cjiri%40jiripik.com%7C2eba4243907e4a9c2e0e08da43918c02%7Cc16662ac71064a71b92a97c86d48549a%7C0%7C0%7C637896590958586963%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=T4I%2FHI%2BpH4dou6AgGq8N3BJvp6IvQ5mGu4p4skeJic0%3D&amp;reserved=0" rel="noopener ugc nofollow" target="_blank"> NGC Triton 22.5容器</a>。不幸的是，SageMaker团队还没有一个具体的发布日期，他们将在SageMaker中正式添加新的Triton 22.05容器。</li></ul><h2 id="6af9" class="nm lf iq bd lg ns nt dn lk nu nv dp lo kq nw nx ls ku ny nz lw ky oa ob ma oc bi translated">6.2.5步骤4 —创建SageMaker模型包，并上传至SageMaker</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="bcc3" class="nm lf iq ni b gy nn no l np nq">!mkdir -p triton-serve-trt/bert/1/ <br/>!cp workspace-trt/model_bs16.plan triton-serve-trt/bert/1/model.plan <br/>!tar -C triton-serve-trt/ -czf model.tar.gz bert </span><span id="346a" class="nm lf iq ni b gy oe no l np nq">import boto3, json, sagemaker, time<br/>from sagemaker import get_execution_role</span><span id="1caf" class="nm lf iq ni b gy oe no l np nq">sess = boto3.Session()<br/>sm = sess.client("sagemaker")<br/>sagemaker_session = sagemaker.Session(boto_session=sess)<br/>role = get_execution_role()<br/>client = boto3.client("sagemaker-runtime")</span><span id="d36a" class="nm lf iq ni b gy oe no l np nq">model_uri = sagemaker_session.upload_data(path="model.tar.gz", key_prefix="triton-serve-trt")</span></pre><p id="adbe" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">该脚本依赖于定义NVIDIA Triton推理服务器模型的<a class="ae ld" href="https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">模型配置文件</strong></a><strong class="kh ir"/><a class="ae ld" href="https://github.com/jiripik/nvidia-triton-c-suite-labs-spam-detection-engine/blob/main/triton-serve-trt/bert/config.pbtxt" rel="noopener ugc nofollow" target="_blank">Triton-serve-TRT/Bert/config . Pb txt</a>的存在，该文件需要存在于解决方案中。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="e8a5" class="nm lf iq ni b gy nn no l np nq">name: "bert"<br/>platform: "tensorrt_plan"<br/>max_batch_size: 128<br/>input [<br/>  {<br/>    name: "input_ids"<br/>    data_type: TYPE_INT32<br/>    dims: [128]<br/>  },<br/>  {<br/>    name: "attention_mask"<br/>    data_type: TYPE_INT32<br/>    dims: [128]<br/>  }<br/>]<br/>output [<br/>  {<br/>    name: "logits"<br/>    data_type: TYPE_FP32<br/>    dims: [2]<br/>  }<br/>]<br/>instance_group {<br/>  count: 1<br/>  kind: KIND_GPU<br/>}</span></pre><h2 id="bf98" class="nm lf iq bd lg ns nt dn lk nu nv dp lo kq nw nx ls ku ny nz lw ky oa ob ma oc bi translated">6.2.6步骤5-创建SageMaker推理端点</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="19fc" class="nm lf iq ni b gy nn no l np nq">sm_model_name = "triton-nlp-bert-trt-" + time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</span><span id="03a5" class="nm lf iq ni b gy oe no l np nq">container = {<br/>    "Image": triton_image_uri,<br/>    "ModelDataUrl": model_uri,<br/>    "Environment": {"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME": "bert"},<br/>}</span><span id="a87d" class="nm lf iq ni b gy oe no l np nq">create_model_response = sm.create_model(<br/>    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container<br/>)</span><span id="4c61" class="nm lf iq ni b gy oe no l np nq">print("Model Arn: " + create_model_response["ModelArn"])</span><span id="42ec" class="nm lf iq ni b gy oe no l np nq">endpoint_config_name = "triton-nlp-bert-trt-" + time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</span><span id="d338" class="nm lf iq ni b gy oe no l np nq">create_endpoint_config_response = sm.create_endpoint_config(<br/>    EndpointConfigName=endpoint_config_name,<br/>    ProductionVariants=[<br/>        {<br/>            "InstanceType": "ml.p3.2xlarge",<br/>            "InitialVariantWeight": 1,<br/>            "InitialInstanceCount": 1,<br/>            "ModelName": sm_model_name,<br/>            "VariantName": "AllTraffic",<br/>        }<br/>    ],<br/>)</span><span id="f401" class="nm lf iq ni b gy oe no l np nq">print("Endpoint Config Arn: " + create_endpoint_config_response["EndpointConfigArn"])</span><span id="f04a" class="nm lf iq ni b gy oe no l np nq">endpoint_name = "triton-nlp-bert-trt-" + time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</span><span id="b5a6" class="nm lf iq ni b gy oe no l np nq">create_endpoint_response = sm.create_endpoint(<br/>    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name<br/>)</span><span id="7382" class="nm lf iq ni b gy oe no l np nq">print("Endpoint Arn: " + create_endpoint_response["EndpointArn"])</span></pre><p id="1d0b" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">该代码在AWS ECR中找到NVIDIA Triton推理服务器的最近位置，并将其部署到AWS SageMaker推理端点。部署应该不到10分钟。</p><p id="3530" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">注意:</p><ul class=""><li id="0e05" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">在端点配置中，我们定义端点实例类型——它必须与用于创建TensorRT模型的实例相同。从ONNX模型创建TensorRT模型(引擎)，即ONNX到TensorRT的转换步骤，需要在与目标部署GPU相同的GPU上进行。因此，如果您想在T4 GPU上部署tensort模型，这是在g4dn实例中，然后您在g4dn实例上构建tensort引擎。类似地，如果您在p3实例中的V100 GPU上部署模型，那么您需要在p3实例上从onnx模型构建TensorRT引擎。目前，我们不能放松TensorRT的这一硬性要求</li><li id="5887" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">部署日志可从AWS SageMaker推断端点获得(用于<a class="ae ld" href="https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints" rel="noopener ugc nofollow" target="_blank"> us-east-1 </a>)。如果创建时间超过10分钟，请检查它们。如果它包含错误，部署将在30分钟后失败，然后您可以删除该端点。</li></ul><h2 id="09a6" class="nm lf iq bd lg ns nt dn lk nu nv dp lo kq nw nx ls ku ny nz lw ky oa ob ma oc bi translated">6.2.7步骤6-测试Triton SageMaker推断终点</h2><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="1c86" class="nm lf iq ni b gy nn no l np nq">import tritonclient.http as httpclient<br/>from transformers import DistilBertTokenizer<br/>import torch.nn.functional as F <br/>import numpy as np<br/>from retry import retry<br/>import botocore<br/>import concurrent<br/>import time<br/></span><span id="22f2" class="nm lf iq ni b gy oe no l np nq">enc = DistilBertTokenizer.from_pretrained("./workspace-trt/")<br/>    <br/>def tokenize_text(text):<br/>    encoded_text = enc(clean_text(text), padding="max_length", max_length=128, truncation=True)<br/>    return encoded_text["input_ids"], encoded_text["attention_mask"]<br/></span><span id="cd1a" class="nm lf iq ni b gy oe no l np nq">def get_sample_tokenized_text_binary(text):<br/>    inputs = []<br/>    outputs = []<br/>    input_names =  ["input_ids", "attention_mask"]<br/>    output_names = ["logits"]<br/>    <br/>    inputs.append(httpclient.InferInput(input_names[0], [1, 128], "INT32"))<br/>    inputs.append(httpclient.InferInput(input_names[1], [1, 128], "INT32"))<br/>    indexed_tokens, attention_mask = tokenize_text(text)</span><span id="0669" class="nm lf iq ni b gy oe no l np nq">    indexed_tokens = np.array(indexed_tokens, dtype=np.int32)<br/>    indexed_tokens = np.expand_dims(indexed_tokens, axis=0)<br/>    inputs[0].set_data_from_numpy(indexed_tokens, binary_data=True)</span><span id="02a9" class="nm lf iq ni b gy oe no l np nq">    attention_mask = np.array(attention_mask, dtype=np.int32)<br/>    attention_mask = np.expand_dims(attention_mask, axis=0)<br/>    inputs[1].set_data_from_numpy(attention_mask, binary_data=True)</span><span id="4368" class="nm lf iq ni b gy oe no l np nq">    outputs.append(httpclient.InferRequestedOutput(output_names[0], binary_data=True))<br/>    outputs.append(httpclient.InferRequestedOutput(output_names[1], binary_data=True))<br/>    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(inputs, outputs=outputs)<br/>    return request_body, header_length<br/></span><span id="9e49" class="nm lf iq ni b gy oe no l np nq">@retry(botocore.exceptions.ClientError, tries=5, delay=1)<br/>def get_prediction(text):<br/>    input_ids, attention_mask = tokenize_text(text)</span><span id="65ac" class="nm lf iq ni b gy oe no l np nq">    payload = {<br/>        "inputs": [<br/>            {"name": "input_ids", "shape": [1, 128], "datatype": "INT32", "data": input_ids},<br/>            {"name": "attention_mask", "shape": [1, 128], "datatype": "INT32", "data": attention_mask},<br/>        ]<br/>    }</span><span id="b828" class="nm lf iq ni b gy oe no l np nq">    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType="application/octet-stream", Body=json.dumps(payload))</span><span id="ade1" class="nm lf iq ni b gy oe no l np nq">    result = json.loads(response["Body"].read().decode("utf8"))<br/>    predictions = F.softmax(torch.tensor(result['outputs'][0]['data']),dim=-1)<br/>    return torch.argmax(predictions, dim=-1).numpy()<br/>    <br/>test_texts = [<br/>                "Oh k...i'''m watching here:)",<br/>                "As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a £1500 Bonus Prize, call 09066364589",<br/>                "I HAVE A DATE ON SUNDAY WITH WILL!!",<br/>                "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+"<br/>]<br/></span><span id="3d24" class="nm lf iq ni b gy oe no l np nq">num_inferences = 1000<br/>start = time.time() <br/>with concurrent.futures.ThreadPoolExecutor() as exe: <br/>    fut_list = []<br/>    for _ in range (num_inferences):<br/>        for test_text in test_texts:<br/>            fut = exe.submit(get_prediction, test_text)         <br/>            fut_list.append(fut)     <br/>    for fut in fut_list:         <br/>        rslt = fut.result() <br/>        <br/>elapsed_time = time.time() - start <br/>print('num_inferences:{:&gt;6}[texts], elapsed_time:{:6.2f}[sec], Throughput:{:8.2f}[texts/sec]'.format(num_inferences * len(test_texts), elapsed_time, num_inferences * len(test_texts)/ elapsed_time))</span></pre><p id="f6a1" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">虽然Triton支持所有受支持的后端的批处理推断，但是对于这个模型，我们被限制为一个批处理大小，因为当前SageMaker Triton容器的TensorRT可用版本不支持DistilBERT模型的动态形状。</p><p id="ac84" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">TensorRT的这一限制已在其较新的版本中得到修复，应该很快就可以在SageMaker Triton容器中使用。</p><h2 id="072d" class="nm lf iq bd lg ns nt dn lk nu nv dp lo kq nw nx ls ku ny nz lw ky oa ob ma oc bi translated">6.2.8步骤7-删除SageMaker推理端点</h2><p id="d2a0" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">在我们完成解决方案的测试之后，我们可以删除SageMaker推理端点、端点配置和模型。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="db93" class="nm lf iq ni b gy nn no l np nq">sm.delete_endpoint(EndpointName=endpoint_name)<br/>sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)<br/>sm.delete_model(ModelName=sm_model_name)</span></pre><h1 id="1a60" class="le lf iq bd lg lh mh lj lk ll mi ln lo lp mj lr ls lt mk lv lw lx ml lz ma mb bi translated">7.基于NVIDIA Triton和TorchScript的AWS SageMaker推理的性能比较</h1><p id="b8f0" class="pw-post-body-paragraph kf kg iq kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc ij bi translated">为了便于说明，我们使用<a class="ae ld" href="https://github.com/jiripik/nvidia-triton-c-suite-labs-spam-detection-engine/blob/main/SpamDetection-Triton.ipynb" rel="noopener ugc nofollow" target="_blank"> AWS SageMaker Triton推断端点</a>和<a class="ae ld" href="https://github.com/jiripik/nvidia-triton-c-suite-labs-spam-detection-engine/blob/main/SpamDetection-TorchScript.ipynb" rel="noopener ugc nofollow" target="_blank"> AWS SageMaker TorchScript推断端点</a>实现了相同的代码:</p><ul class=""><li id="66e0" class="mm mn iq kh b ki kj km kn kq mo ku mp ky mq lc mr ms mt mu bi translated">我们已经使用<a class="ae ld" href="https://docs.python.org/3/library/concurrent.futures.html" rel="noopener ugc nofollow" target="_blank"> ThreadPoolExecutor </a>对四个测试文本重复了一千次推理</li><li id="3b34" class="mm mn iq kh b ki mw km mx kq my ku mz ky na lc mr ms mt mu bi translated">虽然结果只是近似值，但应该足够稳定:<strong class="kh ir">与TorchScript推理服务器相比，NVIDIA Triton推理服务器的吞吐量高2.4倍，模型延迟低52.9倍。</strong></li></ul><p id="9897" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">推理类型吞吐量模型延迟NVIDIA Triton推理服务器339.35 texts/sec 1.48 msTorchScript推理服务器140.4 texts/sec78.3 ms</p><p id="6811" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">比较两种推理服务器的AWS CloudWatch模型延迟:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/8e81efbf3c8c0e4b3012bff58e56fc94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SPQtZM-C1gbN_i-Y.png"/></div></div></figure><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/11745807b608becbab32d3ed6e22a499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2ZYf73_NclFwQIcV.png"/></div></div></figure><p id="7548" class="pw-post-body-paragraph kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">与Triton中的框架性能以及高效的<a class="ae ld" href="https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md" rel="noopener ugc nofollow" target="_blank"> kserve v2协议</a>相比，主要的速度提升来自于<a class="ae ld" href="https://developer.nvidia.com/tensorrt#performance" rel="noopener ugc nofollow" target="_blank"> TensorTRT加速</a>。通过尝试Triton本机提供的这些<a class="ae ld" href="https://github.com/triton-inference-server/server/blob/main/docs/optimization.md#optimization" rel="noopener ugc nofollow" target="_blank">优化</a>，有可能进一步提高性能。</p></div></div>    
</body>
</html>