<html>
<head>
<title>Generating Cool Storylines Using a T5 Transformer and Having Fun</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用T5变压器生成酷炫的故事情节，享受乐趣</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/generating-cool-storylines-using-a-t5-transformer-and-having-fun-4a79f6ab8adb?source=collection_archive---------0-----------------------#2021-04-26">https://pub.towardsai.net/generating-cool-storylines-using-a-t5-transformer-and-having-fun-4a79f6ab8adb?source=collection_archive---------0-----------------------#2021-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ed51" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="425c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">t5——一个穷人的GPT 3</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/be447db577aea8c75da2477617463199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKKpveyt8vg4mxzWeJVn2Q.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@techdailyca?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">科技日报</a>在<a class="ae lh" href="https://unsplash.com/s/photos/streaming?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="dd4e" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">T5变压器简介</h1><p id="ef42" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">Google AI的人发表了一篇论文“探索使用统一的文本到文本转换器进行迁移学习的限制”，并介绍了一项关于什么类型的预训练方法或迁移学习技术最有效的实证研究，然后使用该研究创建了一个新的模型，即文本到文本转换器(T5)。这个transformer模型是在一个更干净的普通爬行语料库上进行预训练的，谷歌将其命名为巨大的干净爬行语料库(C4)。听起来很酷吧😎。当您检查该模型的能力和灵活性，以便用很少到中等的数据对大量的下游NLP问题进行微调时，它的效果也很好。</p><h1 id="336e" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">现在你可能会想，T5和其他型号的变形金刚有什么不同？</h1><p id="877c" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">要回答这个问题，我们需要看看其他转换器，如伯特、GPT等。所有这些转换器都针对大量数据进行了预训练，但为了对分类等下游任务进行微调，会添加一个分类层并输出一个分类标签，或者为NER输出一个输入范围。但是在T5，一切都是有序的，就像他们说的“文本到文本”。因此，对于分类，它将是字符串输出，对于NER，它将是字符串，甚至对于回归，它也是字符串。不信我看报纸。在某种程度上，该模型不受一定数量的类的限制，您可以轻松地处理多个文本分类数据并训练单个模型，而无需担心标签的处理。因为你给它的一切都只是一个序列。这确实为模型提供了很多表达能力，但在某些情况下，它可能会预测标签空间之外的一些东西。我没有看到任何人报告这一点或有这个问题，所以它应该工作良好。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/783a9f227591d7fa91226e177255914f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*Ps6f66o2rr-sDEAM.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:谷歌人工智能博客</figcaption></figure><p id="a35d" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">这是很多关于变形金刚及其背后的想法的讨论，如果你想阅读更多，请查看论文和谷歌官方人工智能博客。</p><ul class=""><li id="f899" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><a class="ae lh" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">用统一的文本到文本转换器探索迁移学习的极限</a></li><li id="fed4" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated"><a class="ae lh" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a></li></ul><p id="5197" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">现在让我们深入代码，构建一些非常有趣的东西。</p><h1 id="7557" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">我们今天在建造什么？</h1><p id="4c28" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">正如博客的标题所暗示的，我们将建立一个故事情节生成器。但是对于我们将用来输出故事情节/情节的输入有一点小小的变化。由于transformer是Sequence2Sequence或谷歌所说的“文本到文本”,我们将通过输入类型、演员、导演、种族和故事线/情节的训练作为输出来微调这个模型。这样做有助于我们看到建议的Sequence2Sequence(“文本到文本”)转换器的表达能力。</p><p id="b267" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">要了解本教程的效果如何，你可以点击这个<a class="ae lh" href="https://movie-plot-generator.vercel.app/" rel="noopener ugc nofollow" target="_blank">链接</a>去未来看看这个模型如何运行，然后回到这里为你自己建造它😂。</p><div class="nq nr gp gr ns nt"><a href="https://movie-plot-generator.vercel.app/" rel="noopener  ugc nofollow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jd gy z fp ny fr fs nz fu fw jc bi translated">电影情节生成器</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">我在网上生成模糊的电影情节(但有时很好)。但我可以向你保证，它将永远是…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">电影-情节-生成器. vercel.app</p></div></div></div></a></div><h1 id="6a91" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">我们将使用哪些数据？</h1><p id="f22e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们将使用的数据集是<a class="ae lh" href="https://www.kaggle.com/jrobischon/wikipedia-movie-plots" rel="noopener ugc nofollow" target="_blank">维基百科电影情节</a>数据集。它有关于发行年份、标题、种族、导演、演员、情节、流派、维基页面和情节的数据。我们将只使用类型，导演，演员，种族和情节来建立我们的模型。您可以随意尝试任何其他额外的列或任何其他数据集。</p><h1 id="5230" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">如何准备资料？</h1><p id="c0b7" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">这是建模最重要的步骤之一。因此，请从上面的链接下载数据，并按照下面的步骤操作。</p><ul class=""><li id="7298" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">一些正规进口</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="732a" class="oi lj it oe b gy oj ok l ol om">import os<br/>import re<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from tqdm import tqdm_notebook, tnrange<br/>from sklearn.utils import shuffle<br/>import pickle</span></pre><ul class=""><li id="b4ba" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">一些保存数据的功能</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="cde0" class="oi lj it oe b gy oj ok l ol om">def save_pickle(path, obj):<br/>  with open(path, 'wb') as fp:<br/>    pickle.dump(obj, fp)</span><span id="be8a" class="oi lj it oe b gy on ok l ol om">def load_pickle(path):<br/>  with open(path, 'rb') as fp:<br/>    return pickle.load(fp)</span></pre><ul class=""><li id="c4bb" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">解压数据，加载熊猫</em>🐼 🐼 🐼</li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="1ccb" class="oi lj it oe b gy oj ok l ol om">movieDf = pd.read_csv('drive/MyDrive/MoviePlotsModels/data/wiki_movie_plots_deduped.csv')</span></pre><ul class=""><li id="e983" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">收集训练我们的模型所需的任何东西</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="17e7" class="oi lj it oe b gy oj ok l ol om">outputs = []</span><span id="413d" class="oi lj it oe b gy on ok l ol om">with tqdm_notebook(total = len(movieDf)) as pbar:<br/>  for ix, row in movieDf.iterrows():<br/>    </span><span id="7e4d" class="oi lj it oe b gy on ok l ol om">    genre_to_plot_input = f'generate plot for genre: {row.Genre}'<br/>    genre_to_plot_output = row.Plot<br/>    outputs.append({<br/>        "source": genre_to_plot_input,<br/>        "target": genre_to_plot_output<br/>    })</span><span id="519a" class="oi lj it oe b gy on ok l ol om">    genre_director_to_plot_input = f'generate plot for: genre {row.Genre} and director: {row.Director}'<br/>    genre_director_to_plot_output = row.Plot<br/>    outputs.append({<br/>        "source": genre_director_to_plot_input,<br/>        "target": genre_director_to_plot_output<br/>    })</span><span id="8511" class="oi lj it oe b gy on ok l ol om">    genre_director_ethinicity_to_plot_input = f'generate plot for: genre {row.Genre} director: {row.Director} and ethinicity {row["Origin/Ethnicity"]}'<br/>    genre_director_ethinicity_to_plot_output = row.Plot<br/>    outputs.append({<br/>        "source": genre_director_ethinicity_to_plot_input,<br/>        "target": genre_director_ethinicity_to_plot_output<br/>    })</span><span id="bcc4" class="oi lj it oe b gy on ok l ol om">    if not pd.isna(row.Cast):<br/>      <br/>      genre_director_cast_to_plot_input = f'generate plot for: genre {row.Genre} director: {row.Director} and cast: {row.Cast}'<br/>      genre_director_cast_to_plot_output = row.Plot</span><span id="6e85" class="oi lj it oe b gy on ok l ol om">      outputs.append({<br/>          "source": genre_director_cast_to_plot_input,<br/>          "target": genre_director_cast_to_plot_output<br/>      })</span><span id="46db" class="oi lj it oe b gy on ok l ol om">    <br/>    pbar.update(1)</span></pre><ul class=""><li id="5b55" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">让我们保存数据</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="9a41" class="oi lj it oe b gy oj ok l ol om">save_pickle('t5-source-target-data.pkl', outputs)</span></pre><h1 id="ab66" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">让我们训练(微调😅)我们的“文本到文本”转换器</h1><p id="62f2" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">从上面的标题你可能已经明白，我们将使用一个来自<strong class="mc jd"> HuggingFace的预训练模型。是的，我们确实要使用<code class="fe oo op oq oe b"><strong class="mc jd">transformers</strong></code>库。所以让我们开始训练代码。在开始编码之前，安装下面的包，重启你的Colab并开始训练。如果你不重启，你可能会在初始化模型和令牌化器的时候遇到一些错误，当你在Stackoverflow和Github上搜索了一个小时之后，你可能会找到解决方法，那就是在安装完包之后重启Colab笔记本。因此，只需按照安装后重新启动Colab笔记本的步骤操作，它将为您节省时间。</strong></p><ul class=""><li id="bfb3" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">安装这些包(你会看到它们的重要性)</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="d38e" class="oi lj it oe b gy oj ok l ol om">!pip install sentencepiece<br/>!pip install transformers<br/>!pip install torch<br/>!pip install rich[jupyter]</span></pre><p id="2211" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated"><strong class="mc jd"> <em class="oc">顺便说一句，我们将使用PyTorch，所以如果你一直认为这是TensorFlow，我很抱歉让你失望了。我猜</em> </strong>你可能想改变你的框架🤣</p><ul class=""><li id="dd13" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">再次常规进口</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="5ae5" class="oi lj it oe b gy oj ok l ol om">import os<br/>import re<br/>import random<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from tqdm import tqdm_notebook, tnrange<br/>from sklearn.utils import shuffle<br/>import pickle<br/>import math<br/>## use if working on jupyter notebook or colab<br/>from IPython.display import clear_output</span></pre><ul class=""><li id="c322" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">一些保存和加载数据的功能</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="b3de" class="oi lj it oe b gy oj ok l ol om">def save_pickle(path, obj):<br/>  with open(path, 'wb') as fp:<br/>    pickle.dump(obj, fp)</span><span id="8756" class="oi lj it oe b gy on ok l ol om">def load_pickle(path):<br/>  with open(path, 'rb') as fp:<br/>    return pickle.load(fp)</span></pre><p id="3233" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">D <em class="oc">如果你用的是同一个Colab笔记本</em>就不要再复制粘贴了</p><ul class=""><li id="31df" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">型号相关进口</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="a919" class="oi lj it oe b gy oj ok l ol om">import numpy as np<br/>import torch<br/>import torch.nn.functional as F<br/>from torch.utils.data import Dataset, DataLoader<br/>from transformers import T5Tokenizer, T5ForConditionalGeneration<br/>from rich.table import Column, Table<br/>from rich import box<br/>from rich.console import Console</span><span id="d0b9" class="oi lj it oe b gy on ok l ol om">from torch import cuda<br/>device = 'cuda' if cuda.is_available() else 'cpu'</span></pre><ul class=""><li id="351d" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">设置丰富的控制台，以更好的方式显示分数</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="c5be" class="oi lj it oe b gy oj ok l ol om">console = Console(record=True)</span><span id="d1f5" class="oi lj it oe b gy on ok l ol om">training_logger = Table(<br/>    Column("Random Selection", justify = "center"),<br/>    Column("Epoch", justify="center"),<br/>    Column("Loss", justify="center"),<br/>    title="Training Status",<br/>    pad_edge=False,<br/>    box=box.ASCII,<br/>)</span><span id="b202" class="oi lj it oe b gy on ok l ol om">valid_loggger = Table(<br/>    Column("Random Selection", justify = "center"),<br/>    Column("Loss", justify = "center"),<br/>    title="Validation Status",<br/>    pad_edge=False,<br/>    box=box.ASCII,<br/>)</span></pre><ul class=""><li id="caef" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">加载预处理数据</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="dd54" class="oi lj it oe b gy oj ok l ol om">data = load_pickle('/content/drive/MyDrive/T5MovieWikiTraining2_0/t5-source-target-data-2.pkl')</span></pre><ul class=""><li id="0787" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc"> PyTorch </em> <code class="fe oo op oq oe b"><em class="oc">Dataset</em></code> <em class="oc">物体</em></li></ul><p id="ada0" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">初始化此对象时传递标记化器实例</p><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="583e" class="oi lj it oe b gy oj ok l ol om">class T5Dataset(Dataset):</span><span id="8829" class="oi lj it oe b gy on ok l ol om">  def __init__(self, tokenizer, data, source_len, target_len):</span><span id="005a" class="oi lj it oe b gy on ok l ol om">    super(T5Dataset, self).__init__()<br/>    self.tokenizer = tokenizer<br/>    self.source_len = source_len<br/>    self.target_len = target_len<br/>    self.data = data</span><span id="fc95" class="oi lj it oe b gy on ok l ol om">  def __len__(self):</span><span id="bdc2" class="oi lj it oe b gy on ok l ol om">    return len(self.data)</span><span id="ce57" class="oi lj it oe b gy on ok l ol om">  def __getitem__(self, index):</span><span id="08a8" class="oi lj it oe b gy on ok l ol om">    source_seq = self.data[index]['source']<br/>    target_seq = self.data[index]['target']</span><span id="20cb" class="oi lj it oe b gy on ok l ol om">    source = self.tokenizer.batch_encode_plus(<br/>        [source_seq],<br/>        max_length = self.source_len,<br/>        pad_to_max_length = True,<br/>        truncation = True,<br/>        padding = "max_length",<br/>        return_tensors = "pt"<br/>    )</span><span id="80a2" class="oi lj it oe b gy on ok l ol om">    target = self.tokenizer.batch_encode_plus(<br/>        [target_seq],<br/>        max_length = self.target_len,<br/>        pad_to_max_length = True,<br/>        truncation = True,<br/>        padding = "max_length",<br/>        return_tensors = "pt"<br/>    )</span><span id="3aea" class="oi lj it oe b gy on ok l ol om">    source_ids = source["input_ids"].squeeze()<br/>    source_mask = source["attention_mask"].squeeze()<br/>    target_ids = target["input_ids"].squeeze()<br/>    target_mask = target["attention_mask"].squeeze()</span><span id="8e2a" class="oi lj it oe b gy on ok l ol om">    return {<br/>        "source_ids": source_ids,<br/>        "source_mask": source_mask,<br/>        "target_ids": target_ids,<br/>        "target_mask": target_mask<br/>    }</span></pre><ul class=""><li id="999e" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">训练功能</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="ee96" class="oi lj it oe b gy oj ok l ol om">def train(epoch, tokenizer, model, device, loader, optimizer):</span><span id="483a" class="oi lj it oe b gy on ok l ol om">    model.train()</span><span id="5630" class="oi lj it oe b gy on ok l ol om">    total_loss = 0<br/>    total_counts = 0</span><span id="0875" class="oi lj it oe b gy on ok l ol om">    for _, data in enumerate(tqdm_notebook(loader, desc = "Train DL")):</span><span id="44a3" class="oi lj it oe b gy on ok l ol om">        y = data["target_ids"].to(device, dtype = torch.long)<br/>        y_ids = y[:, :-1].contiguous()<br/>        lm_labels = y[:, 1:].clone().detach()<br/>        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100<br/>        ids = data["source_ids"].to(device, dtype = torch.long)<br/>        mask = data["source_mask"].to(device, dtype = torch.long)</span><span id="9ed0" class="oi lj it oe b gy on ok l ol om">        optimizer.zero_grad()</span><span id="e701" class="oi lj it oe b gy on ok l ol om">        outputs = model(<br/>            input_ids = ids, attention_mask = mask, decoder_input_ids = y_ids, labels = lm_labels<br/>        )</span><span id="bc08" class="oi lj it oe b gy on ok l ol om">        loss = outputs[0]</span><span id="7788" class="oi lj it oe b gy on ok l ol om">        total_counts += 1<br/>        total_loss += loss.item()</span><span id="217b" class="oi lj it oe b gy on ok l ol om">        <br/>        loss.backward()<br/>        optimizer.step()</span><span id="a0dc" class="oi lj it oe b gy on ok l ol om">    <br/>    return total_loss/total_counts</span></pre><ul class=""><li id="da31" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">验证功能</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="cb77" class="oi lj it oe b gy oj ok l ol om">def validate(epoch, tokenizer, model, device, loader):</span><span id="2d91" class="oi lj it oe b gy on ok l ol om">    model.eval()<br/>    total_loss = 0<br/>    total_counts = 0</span><span id="0c23" class="oi lj it oe b gy on ok l ol om">    with torch.no_grad():</span><span id="968b" class="oi lj it oe b gy on ok l ol om">        for _, data in enumerate(tqdm_notebook(loader, desc = "Valid DL")):</span><span id="e4ae" class="oi lj it oe b gy on ok l ol om">            y = data["target_ids"].to(device, dtype = torch.long)<br/>            y_ids = y[:, :-1].contiguous()<br/>            lm_labels = y[:, 1:].clone().detach()<br/>            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100<br/>            ids = data["source_ids"].to(device, dtype = torch.long)<br/>            mask = data["source_mask"].to(device, dtype = torch.long)</span><span id="6dfd" class="oi lj it oe b gy on ok l ol om">            outputs = model(<br/>            input_ids = ids, attention_mask = mask, decoder_input_ids = y_ids, labels = lm_labels<br/>            )</span><span id="8e3b" class="oi lj it oe b gy on ok l ol om">            loss = outputs[0]</span><span id="b992" class="oi lj it oe b gy on ok l ol om">            total_loss += loss.item()<br/>            total_counts += 1</span><span id="5fdf" class="oi lj it oe b gy on ok l ol om">    return total_loss / total_counts</span></pre><ul class=""><li id="2004" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated">让我们把它们放在一起</li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="4b6b" class="oi lj it oe b gy oj ok l ol om">def trainer(<br/>    data, model_params, output_dir = "./outputs/"<br/>):</span><span id="6923" class="oi lj it oe b gy on ok l ol om">    torch.manual_seed(model_params["SEED"])<br/>    torch.cuda.manual_seed(model_params["SEED"])<br/>    np.random.seed(model_params["SEED"])<br/>    torch.backends.cudnn.deterministic = True<br/>    </span><span id="eb4c" class="oi lj it oe b gy on ok l ol om">    console.log(f'''Model: Loading {model_params['MODEL']}.....''')</span><span id="b2a5" class="oi lj it oe b gy on ok l ol om">    tokenizer = T5Tokenizer.from_pretrained(model_params["MODEL"])</span><span id="ea84" class="oi lj it oe b gy on ok l ol om">    model = T5ForConditionalGeneration.from_pretrained(model_params["MODEL"])</span><span id="04b2" class="oi lj it oe b gy on ok l ol om">    model = model.to(device)</span><span id="23c6" class="oi lj it oe b gy on ok l ol om">    console.log(f"[DATA]: READING DATA.......")</span><span id="c353" class="oi lj it oe b gy on ok l ol om">    optimizer = torch.optim.Adam(<br/>        params=model.parameters(), lr=model_params["LEARNING_RATE"]<br/>    )</span><span id="bc39" class="oi lj it oe b gy on ok l ol om">    # Training loop<br/>    console.log(f"[Initiating Fine Tuning]...\\n")</span><span id="355b" class="oi lj it oe b gy on ok l ol om">    ## model save path</span><span id="5adc" class="oi lj it oe b gy on ok l ol om">    path = os.path.join(output_dir, "model_files")</span><span id="0ed4" class="oi lj it oe b gy on ok l ol om">    <br/>    console.log("Starting with Random Selection")<br/>    ## random selection<br/>    prev_loss = []<br/>    for randomSelection in tnrange(model_params["RANDOM_TRAIN_STEPS"], desc = 'Random Selection'):<br/>        <br/>        copyData = data.copy()<br/>        copyData = shuffle(copyData)<br/>      </span><span id="6a45" class="oi lj it oe b gy on ok l ol om">        train_size = 0.75<br/>        random_permuts = np.random.permutation(len(copyData))<br/>        train_nums = round(len(random_permuts) * train_size)<br/>        train_dataset = [copyData[i] for i in random_permuts[:train_nums]]<br/>        valid_dataset = [copyData[i] for i in random_permuts[train_nums:]] </span><span id="e490" class="oi lj it oe b gy on ok l ol om">        training_set = T5Dataset(<br/>            tokenizer, train_dataset, model_params["MAX_SOURCE_TEXT_LENGTH"], model_params["MAX_TARGET_TEXT_LENGTH"]<br/>        )</span><span id="4161" class="oi lj it oe b gy on ok l ol om">        val_set = T5Dataset(<br/>            tokenizer, valid_dataset, model_params["MAX_SOURCE_TEXT_LENGTH"], model_params["MAX_TARGET_TEXT_LENGTH"]<br/>        )</span><span id="73a7" class="oi lj it oe b gy on ok l ol om">        train_params = {<br/>            "batch_size": model_params["TRAIN_BATCH_SIZE"],<br/>            "shuffle": True,<br/>            "num_workers": 0<br/>        }</span><span id="de20" class="oi lj it oe b gy on ok l ol om">        val_params = {<br/>            "batch_size": model_params["VALID_BATCH_SIZE"],<br/>            "shuffle": False,<br/>            "num_workers": 0<br/>        }<br/>        <br/>        train_dl = DataLoader(training_set, **train_params)<br/>        </span><span id="5218" class="oi lj it oe b gy on ok l ol om">        ## training <br/>        console.log(f'[MODEL TRAINING]')<br/>        clear_output(wait = True)<br/>        for epoch in tnrange(model_params["TRAIN_EPOCHS"], desc = "Training"):<br/>            <br/>            total_loss = train(epoch, tokenizer, model, device, train_dl, optimizer)</span><span id="b090" class="oi lj it oe b gy on ok l ol om">            training_logger.add_row(str(randomSelection), str(epoch), str(total_loss))<br/>            console.log(training_logger)<br/>            if epoch == 0:<br/>              console.log(f"Saving Model at epoch: {epoch} with total loss: {total_loss}")<br/>              model.save_pretrained(os.path.join(output_dir, "model_files_initial"))<br/>              tokenizer.save_pretrained(os.path.join(output_dir, "model_files_initial"))</span><span id="c4d8" class="oi lj it oe b gy on ok l ol om">            if epoch &gt; 0:</span><span id="2b9e" class="oi lj it oe b gy on ok l ol om">                if min(prev_loss) &gt; total_loss:<br/>                    console.log(f"Saving Model at epoch: {epoch} with total loss: {total_loss}")<br/>                    model.save_pretrained(path)<br/>                    tokenizer.save_pretrained(path)</span><span id="e47a" class="oi lj it oe b gy on ok l ol om">            prev_loss.append(total_loss)<br/>        del train_dl, training_set<br/>        ## validation<br/>        valid_dl = DataLoader(val_set, **val_params)<br/>        console.log(f'[MODEL VALIDATION]')<br/>        for epoch in tnrange(model_params["VAL_EPOCHS"], desc = "Validation"):</span><span id="94dc" class="oi lj it oe b gy on ok l ol om">            val_loss = validate(epoch, tokenizer, model, device, valid_dl)<br/>            <br/>            valid_loggger.add_row(str(randomSelection), str(val_loss))<br/>            console.log(valid_loggger)<br/>        console.save_text(os.path.join(output_dir, f"logs-random-{randomSelection}.txt"))</span><span id="fd48" class="oi lj it oe b gy on ok l ol om">        console.log(f"[VALIDATAION DONE]")     <br/>        del valid_dl, val_set</span></pre><p id="776c" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">您可以使用随机选择策略对每N个时期的小数据块进行训练，然后在N个时期后随机采样另一个数据块，并再次对新采样的数据块进行训练。这种方法可以帮助您更快地进行微调，并且模型也可以看到所有数据。但是建议仅在使用预先训练的模型进行微调时使用。</p><ul class=""><li id="875c" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><em class="oc">模型参数</em></li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="c21b" class="oi lj it oe b gy oj ok l ol om">model_params = {  <br/>    "MODEL": "t5-base", # if you have a bigger machine you can use t5-large<br/>    "TRAIN_BATCH_SIZE": 2,<br/>    "VALID_BATCH_SIZE": 2,<br/>    "TRAIN_EPOCHS": 10,<br/>    "VAL_EPOCHS": 1,<br/>    "LEARNING_RATE": 1e-4,<br/>    "MAX_SOURCE_TEXT_LENGTH": 128,<br/>    "MAX_TARGET_TEXT_LENGTH": 786,<br/>    "SEED": 3007,<br/>    "RANDOM_TRAIN_STEPS": 50<br/>}</span></pre><ul class=""><li id="55ab" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated">让我们训练这只野兽🐻</li></ul><pre class="ks kt ku kv gt od oe of og aw oh bi"><span id="9f8a" class="oi lj it oe b gy oj ok l ol om">trainer(<br/>    data = data,<br/>    model_params = model_params,<br/>    output_dir = "./outputs"<br/>)</span></pre><p id="cc4b" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated">这将开始训练您的模型，并且在每N个训练时期后，您也将收到一个验证分数。</p><h1 id="cdc0" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">概括起来</h1><p id="9496" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在这篇博客中，我们看到了Sequence2Sequence Transformer背后的方法，或者谷歌喜欢称之为“文本到文本”转换器(T5)。我们还快速运行了数据准备流程和培训流程，以便轻松快速地对预培训的T5转换器进行文本生成下游任务的微调。希望你在建造这个的过程中得到一些乐趣。稍后我们将看到如何进行推理和部署这个模型。</p><p id="c625" class="pw-post-body-paragraph ma mb it mc b md mx kd mf mg my kg mi mj mz ml mm mn na mp mq mr nb mt mu mv im bi translated"><strong class="mc jd"> <em class="oc"> Colab笔记本:</em> </strong></p><ul class=""><li id="d82b" class="nc nd it mc b md mx mg my mj ne mn nf mr ng mv nh ni nj nk bi translated"><a class="ae lh" href="https://colab.research.google.com/drive/1Wgcqn_hFDyUIi5-IDUiJz3feeidjzI60?usp=sharing" rel="noopener ugc nofollow" target="_blank">数据准备</a></li><li id="7fd8" class="nc nd it mc b md nl mg nm mj nn mn no mr np mv nh ni nj nk bi translated"><a class="ae lh" href="https://colab.research.google.com/drive/1tVYHhqYbcNnF12309p2vvF2bDMvmWVa6?usp=sharing" rel="noopener ugc nofollow" target="_blank">培训</a></li></ul></div></div>    
</body>
</html>