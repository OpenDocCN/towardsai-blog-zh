<html>
<head>
<title>Will The Next AI Be Superintelligent?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">下一个AI会是超智能的吗？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/will-the-next-ai-be-superintelligent-32e8ece7ec0b?source=collection_archive---------4-----------------------#2020-08-12">https://pub.towardsai.net/will-the-next-ai-be-superintelligent-32e8ece7ec0b?source=collection_archive---------4-----------------------#2020-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4c9e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>，<a class="ae ep" href="https://towardsai.net/p/category/opinion" rel="noopener ugc nofollow" target="_blank">观点</a></h2><div class=""/><div class=""><h2 id="4f9c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">人工智能已经可以编码了，下一个大的人工智能将会更好<em class="kr">。</em></h2></div><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ks"><img src="../Images/3c4b22af729e3f34975b2695b5fb5f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SK6X4oeWfYv4AP0B"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated">亚历山大·安德鲁斯在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="180f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">2005年，雷·库兹韦尔说，“奇点临近了。”现在，人工智能可以用任何语言编写代码，我们正在向更好的人工智能迈进。超级智能就快到了。</p><p id="f1c7" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">GPT-3通过对大量数据进行训练获得了“令人难以置信”的结果:基本上是整个互联网。它不需要针对您的特定用例进行训练(零射击学习)。它能骗过88%的人，我们还在婴儿阶段。👶</p><p id="9094" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我将阐述为什么我们不会达到超智能的三个常见论点:<strong class="ll jd"> (1) </strong>我们将触及数据和处理极限，<strong class="ll jd"> (2) </strong>我们不理解意识，<strong class="ll jd"> (3) </strong>人工智能现在是愚蠢的，所以它将永远是。</p><h1 id="e5f5" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">“我们将达到数据和处理极限”</h1><p id="c591" class="pw-post-body-paragraph lj lk it ll b lm my kd lo lp mz kg lr ls na lu lv lw nb ly lz ma nc mc md me im bi translated">拥有1750亿个参数的GPT 3号是GPT 2号的117倍，是GPT的10倍。每个版本都比上一个版本使用更多的计算。</p><p id="a284" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">最先进的语言模型每年至少增加10倍。大脑有超过100万亿个突触。GPT 3号需要增长三个数量级才能达到这个尺寸。我们可以估计达到大脑大小的人工智能模型大约需要3年时间。</p><p id="69a8" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">批评家们说我们正在接近摩尔定律的极限。然而，在这种压力下，新的计算模式正在出现。特别是，TPU实现大大提高了GPU的效率，而<a class="ae li" href="https://medium.com/towards-artificial-intelligence/the-limits-of-deep-learning-96511b36087b" rel="noopener">量子</a>计算范式提供了持续指数增长的潜力。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi nd"><img src="../Images/b6239d1b1f6afb4329e25dd5fceb1d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aD7I7m2DtTRnIKm4X_VfUw.png"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated">按作者。</figcaption></figure><p id="1eb3" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">一种范式的死亡导致一种新的范式已经发生了很多次:“从电磁计算器到基于继电器的计算机到真空管到分立晶体管到集成电路，”正如库兹韦尔在2005年写道。</p><p id="9f1f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">此外，6G有望将数据速率提高50倍，将延迟减少10倍，并将能效提高一倍——使用太赫兹频率和新型天线等技术——从而实现更大的人工智能。</p><div class="ne nf gp gr ng nh"><a href="https://medium.com/towards-artificial-intelligence/6g-and-its-implications-for-ai-288d403cec0c" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd jd gy z fp nm fr fs nn fu fw jc bi translated">6G及其对人工智能的影响</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">探索三星的6G白皮书。</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">medium.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv lc nh"/></div></div></a></div><p id="b0df" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">随着计算和网络速度的指数级增长，我们也将看到数据爆炸。光是亚洲就有<a class="ae li" href="https://news.microsoft.com/en-hk/2020/01/02/2020-2030-the-data-decade/" rel="noopener ugc nofollow" target="_blank"> 33 </a> ZB的数据。到2025年，将会有175个ZB，到2030年，亚洲可能会达到万亿字节。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi nw"><img src="../Images/60fe140e74f88a12f507d07a42f9d878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WyeCSCDQBE9oWKkUK_WdVw.png"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated">按作者。</figcaption></figure><h1 id="79db" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">“我们不了解意识”</h1><p id="dac7" class="pw-post-body-paragraph lj lk it ll b lm my kd lo lp mz kg lr ls na lu lv lw nb ly lz ma nc mc md me im bi translated">这个论点的另一个版本是，我们简单地说<em class="mf">不能</em>数字化复制意识，因此我们不能拥有超级智慧。</p><p id="85ba" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">部分原因是拒绝接受威胁我们世界观的观点。任何认为<em class="mf">智人</em>是特殊的，比其他任何人都优越的人，都会感到被超级智慧的想法所威胁。同样，哥白尼和达尔文的观点在当时也遭到了抵制。</p><p id="d639" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">此外，我们不需要理解意识的“规则”来制造人类级别的智能，因为它不是关于编写数十亿行代码，而是关于创建一个学习、自组织和受生物启发的神经网络。神经网络通过从数据中学习变得聪明，而不是通过一些知识的“注入”。</p><p id="450d" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">还出现了新的神经网络架构，以更接近地模拟大脑。</p><div class="ne nf gp gr ng nh"><a href="https://towardsdatascience.com/brainos-the-most-brain-like-ai-61b334c7658b" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd jd gy z fp nm fr fs nn fu fw jc bi translated">BrainOS——最像大脑的人工智能</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">应用神经科学实现更高效、更智能的人工智能。</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nx l ns nt nu nq nv lc nh"/></div></div></a></div><p id="e703" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在很多方面，机器已经很优越了。</p><p id="ab40" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">计算机硬件和软件可以很容易地升级——不像我们自己的大脑大多是固定的架构。脑细胞需要<a class="ae li" href="https://www.livescience.com/14735-neuron-synapse-speed-clocked.html" rel="noopener ugc nofollow" target="_blank">秒</a>才能相互交流，而顶级超级计算机可以运行<a class="ae li" href="https://www.bbc.com/news/world-asia-53147684#:~:text=The%20newly%20crowned%20world's%20fastest,IBM%20machine%20in%20the%20US." rel="noopener ugc nofollow" target="_blank">415.53</a>petaflops——或每秒4155.3亿次运算。此外，我们大脑中可以容纳的神经元连接数量有限，而神经网络的规模将在未来几年中增长得更大。</p><h1 id="ba63" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">“人工智能现在是愚蠢的，所以它将永远是”</h1><p id="c5c8" class="pw-post-body-paragraph lj lk it ll b lm my kd lo lp mz kg lr ls na lu lv lw nb ly lz ma nc mc md me im bi translated">外行人提出了一个非常普遍的论点，大意是“我从这个人工智能得到了一个看起来很傻的结果，这意味着人工智能是愚蠢的。”</p><p id="49ee" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">这类似于批评最初的iPhone、早期的区块链或任何其他新兴技术。正如我们所探索的，数据、计算和速度正在变得越来越好。</p><p id="68fd" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">此外，许多批评者只是错误地使用了这项技术。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ny"><img src="../Images/eab8feae79930ef88df5ec4e4ebe0b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j86gybDEq1Gz62BMOOltgg.jpeg"/></div></div><figcaption class="le lf gj gh gi lg lh bd b be z dk translated">来自<a class="ae li" href="https://www.pexels.com/photo/cheerful-senior-mother-and-adult-daughter-using-smartphone-together-3791664/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的Andrea Piacquadio 的照片</figcaption></figure><p id="3883" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">为了获得更好的效果，你需要按照说明去做。例如，GPT-3 <em class="mf">不需要任何额外的训练就可以工作</em>，所以你可以告诉它写Python、诗歌、说唱或者其他任何东西。</p><p id="9597" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">然而，通过“少量学习”，或者用少量训练数据将模型微调到您的用例，它的准确性会大大提高。如果你要判断GPT-3，你应该使用高质量数据的少量学习。</p><p id="6c92" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">最后，有太多我们看不到和不知道的东西。</p><p id="397e" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">批评者看到了“冰山一角”——关于GPT-3的病毒式推特，也许是TechCrunch或Wired的文章，然后就收工了。极少数人会费心去测试它，只有非常非常少的人使用少量的学习来看看它真正的能力。这只是GPT 3号。每天都有AI研究论文出来，连一小部分都没办法过。</p><p id="41d2" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">他们正在寻找一个微小的、有偏见的人工智能子集，以便在该领域进行广泛的研究。</p><h1 id="9913" class="mg mh it bd mi mj mk ml mm mn mo mp mq ki mr kj ms kl mt km mu ko mv kp mw mx bi translated">结论</h1><p id="a115" class="pw-post-body-paragraph lj lk it ll b lm my kd lo lp mz kg lr ls na lu lv lw nb ly lz ma nc mc md me im bi translated">人工智能不断取得突破，我们将继续看到数据爆炸、量子计算和6G等技术以及架构创新带来的指数级改进。批评家倾向于忽略这些事情，嘲笑人工智能的现状，忽视未来。</p><p id="5cac" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">关于人工智能未来的更多信息，请了解<a class="ae li" href="https://towardsdatascience.com/towards-no-code-analytics-making-everyone-a-data-scientist-f7693bd0abfd" rel="noopener" target="_blank">无代码分析</a>和<a class="ae li" href="https://towardsdatascience.com/will-automl-be-the-end-of-data-scientists-9af3e63990e0" rel="noopener" target="_blank"> AutoML </a>。像<a class="ae li" href="http://apteo.co/" rel="noopener ugc nofollow" target="_blank"> Apteo </a>这样的工具结合了这些趋势，所以任何人都可以不用编码就能构建AI。</p></div></div>    
</body>
</html>