<html>
<head>
<title>PyTorch Starter Pack</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch入门包</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pytorch-starter-pack-97cc5b72a4f2?source=collection_archive---------1-----------------------#2022-11-24">https://pub.towardsai.net/pytorch-starter-pack-97cc5b72a4f2?source=collection_archive---------1-----------------------#2022-11-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="5016" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">动手深度学习</h2><div class=""/><div class=""><h2 id="b15e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">PyTorch中的数据操作</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/144877386de1bacc6f3b3bfd0744bfb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0KLkNMwnJc6w-H6lGJqDg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片由DALL E 2生成图片来源:<a class="ae lh" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a></figcaption></figure><blockquote class="li"><p id="5945" class="lj lk it bd ll lm ln lo lp lq lr ls dk translated">PyTorch是一个开源的机器学习和深度学习框架，最初由<em class="lt"> M </em> eta AI开发，现在是Linux基金会的一部分。</p></blockquote><p id="2010" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo ls im bi translated">PyTorch正在迅速成为所有其他深度学习框架中的绝对赢家。它是开源的，非常容易学习和调试，支持CPU和GPU以及数据并行，旨在为深度神经网络实现提供良好的灵活性和高速度。对于任何对计算机视觉、自然语言处理或强化学习感兴趣的人来说，这是一个必不可少的工具。</p><p id="8873" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">本文将向您介绍PyTorch的基本概念，重点是如何使用它来操作数据。</p><p id="7b4e" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">如果你是初学者，从这里开始将为你未来的工作打下基础。</p><p id="768e" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">如果你已经熟悉神经网络，这篇文章将帮助你温习基本原理和底层数据交互，以便更好地理解处理它们时发生的自动化。</p><h2 id="9b86" class="mu mv it bd mw mx my dn mz na nb dp nc md nd ne nf mh ng nh ni ml nj nk nl iz bi translated">概述</h2><ul class=""><li id="66b2" class="nm nn it lw b lx no ma np md nq mh nr ml ns ls nt nu nv nw bi translated"><a class="ae lh" href="#3bfb" rel="noopener ugc nofollow">张量</a></li><li id="da47" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><a class="ae lh" href="#f32c" rel="noopener ugc nofollow">创建张量</a></li><li id="95c9" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><a class="ae lh" href="#04e7" rel="noopener ugc nofollow">张量元数据</a></li><li id="4d51" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><a class="ae lh" href="#44cf" rel="noopener ugc nofollow">张量运算</a></li><li id="e438" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><a class="ae lh" href="#a55a" rel="noopener ugc nofollow">处理张量维度</a></li><li id="be71" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><a class="ae lh" href="#719b" rel="noopener ugc nofollow">分度</a></li><li id="170e" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><a class="ae lh" href="#f300" rel="noopener ugc nofollow">广播</a></li><li id="3a41" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated">去NumPy旅游然后回来</li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="3bfb" class="oj mv it bd mw ok ol om mz on oo op nc ki oq kj nf kl or km ni ko os kp nl ot bi translated">张量</h1><blockquote class="li"><p id="8c7b" class="lj lk it bd ll lm ou ov ow ox oy ls dk translated">张量，到处都是张量！</p></blockquote><p id="16ea" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo ls im bi translated">张量是PyTorch和机器以及一般深度学习中的中心数据结构。</p><p id="83ec" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">我们可以天真地说张量是多维矩阵，就像矩阵可以被认为是多维向量一样。事实上，向量是一阶张量(一维)，而矩阵是二阶张量(二维)。</p><p id="1ad3" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">张量用于以数字形式表示图像、视频等数据。例如，我们可以用一个形状为<code class="fe oz pa pb pc b">[3, 224, 224]</code>的张量来表示一幅图像:3个颜色通道，高度和宽度为224像素。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/14731f38820005612c3c60c1431a04bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*Bsosn-S5NmdLganz_tFbbA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图像1矢量、矩阵、张量</figcaption></figure><h1 id="f32c" class="oj mv it bd mw ok pe om mz on pf op nc ki pg kj nf kl ph km ni ko pi kp nl ot bi translated">创建张量</h1><p id="6a8a" class="pw-post-body-paragraph lu lv it lw b lx no kd lz ma np kg mc md pj mf mg mh pk mj mk ml pl mn mo ls im bi translated">一个简单的<code class="fe oz pa pb pc b">torch.Tensor</code>对象可以通过多种方式创建。最直接的选择是定义它将包含的确切数据。我们必须首先导入PyTorch:我们现在就这样做，把它留到下一个代码块中。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="d341" class="pq mv it pc b be pr ps l pt pu">import torch<br/><br/>scalar = torch.tensor(3) # 0 dimensions<br/>vector = torch.tensor([1, 2]) # 1 dimension<br/>matrix = torch.tensor([[1, 2], [3, 4]]) # 2 dimensions<br/>tensor = torch.tensor([[[1, 2], [3, 4], [5, 6]]]) # 3 dimensions</span></pre><p id="7e77" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">在上面的例子中，我们定义了一个三维张量，尽管它可以是任何n维数组。</p><p id="cf2b" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">深度学习经常需要创建大型张量，决定每个元素的确切值可能不是最佳选择。相反，深度学习模型通常从随机值张量开始，并在训练过程中调整这些值。因此，PyTorch包含了一个有用的特性:</p><p id="7e85" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">随机张量</strong></p><p id="be21" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">定义一个随机张量的时候，只需要关注大小就可以了。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="6b03" class="pq mv it pc b be pr ps l pt pu">random = torch.rand(size=(3, 224, 224))</span></pre><p id="317c" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">你刚刚创建了一个形状为<code class="fe oz pa pb pc b">[3, 224, 224]</code>的张量。</p><p id="d3b6" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><code class="fe oz pa pb pc b">torch.rand</code>从<em class="pv">均匀分布</em>、<em class="pv">、</em>返回一个带有随机数的张量，其中每个结果的概率是相同的，<code class="fe oz pa pb pc b">torch.randn</code>非常相似，但使用<em class="pv">标准正态分布</em>作为采样间隔:这在现实世界的应用中非常有用，因为这种分布代表了各种各样的现象。</p><p id="7a7e" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">零、一张量</strong></p><p id="a5b7" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">除了创建随机张量之外，你可以用类似的方式创建<em class="pv">零</em>或<em class="pv">一</em>唯一张量。这可能是特别有用的，而做一些掩蔽任务。语法与随机张量的语法相同。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="9929" class="pq mv it pc b be pr ps l pt pu">zeros = torch.zeros(size=(3, 224, 224))<br/>ones = torch.ones(size=(3, 224, 224))</span></pre><p id="61a3" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">距离张量</strong></p><p id="098a" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">你也可以创建一个包含一系列数字的张量，就像在Python中一样。基本语法是<code class="fe oz pa pb pc b">torch.arange(start=0, end, step=1)</code>，唯一需要的参数是<code class="fe oz pa pb pc b">end</code>。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="c393" class="pq mv it pc b be pr ps l pt pu">zero_to_ten = torch.arange(start=0, end=10, step=1)<br/>zero_to_ten = torch.arange(10) # same result, exploiting default keyword args</span></pre><p id="4c3b" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">张量像</strong></p><p id="e9cd" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">最后但同样重要的是，可以创建一个与另一个张量形状相同的张量。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="219c" class="pq mv it pc b be pr ps l pt pu">random_like = torch.rand_like(input=zero_to_ten)<br/>zeros_like = torch.zeros_like(input=zero_to_ten)<br/>ones_like = torch.ones_like(input=zero_to_ten)</span></pre><p id="d569" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">这将创建一个随机张量，一个充满了<em class="pv">个0</em>，另一个充满了<em class="pv">个1</em>，它们都具有形状<code class="fe oz pa pb pc b">[10]</code>。</p><h1 id="04e7" class="oj mv it bd mw ok pe om mz on pf op nc ki pg kj nf kl ph km ni ko pi kp nl ot bi translated">张量元数据</h1><p id="cbd6" class="pw-post-body-paragraph lu lv it lw b lx no kd lz ma np kg mc md pj mf mg mh pk mj mk ml pl mn mo ls im bi translated">除了存储在张量中的数据，它还保存额外的属性。以下是数据操作中最重要和最广泛使用的:</p><ul class=""><li id="fc8c" class="nm nn it lw b lx mp ma mq md pw mh px ml py ls nt nu nv nw bi translated"><code class="fe oz pa pb pc b">dtype</code></li><li id="42f8" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><code class="fe oz pa pb pc b">device</code></li><li id="7ad6" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><code class="fe oz pa pb pc b">shape</code></li></ul><p id="897c" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">数据类型</strong></p><p id="0e38" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">张量的数据类型由<code class="fe oz pa pb pc b">torch.dtype</code>属性表示。PyTorch中有许多不同的数据类型，每种类型在特定的上下文和最终目的中都更好。</p><p id="12a3" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">最流行和默认的类型是<code class="fe oz pa pb pc b">torch.float32</code>或<code class="fe oz pa pb pc b">torch.float</code>，也称为32位浮点。还有16位浮点<code class="fe oz pa pb pc b">torch.float16</code>或<code class="fe oz pa pb pc b">torch.half</code>和64位浮点<code class="fe oz pa pb pc b">torch.float64</code>或<code class="fe oz pa pb pc b">torch.double</code>。还有许多表示整数的类型等等。</p><p id="6d36" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">这种多样性有助于根据您的性能和精度要求确定正确的精度水平。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="4fe9" class="pq mv it pc b be pr ps l pt pu"># Default dtype<br/>some_tensor = torch.rand(size=(3, 4))<br/>some_tensor.dtype<br/><br/>&gt;&gt;&gt; torch.float32<br/><br/># Explicit dtype<br/>some_tensor = torch.rand(size=(3, 4), dtype=torch.half)<br/>some_tensor.dtype<br/><br/>&gt;&gt;&gt; torch.float16</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/fefd2cc0c8601e59243ab717f54db98f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*cyZhd6C1EEGPj5s7mZ5NkA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图2数字精度权衡</figcaption></figure><p id="82fe" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">装置</strong></p><p id="37e3" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><code class="fe oz pa pb pc b">torch.device</code>指定张量被分配到的设备。在PyTorch中，数据可以存储在CPU或CUDA内存(GPU)中。只有当GPU在您的系统上可用时，您才能在GPU上移动张量。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="a9aa" class="pq mv it pc b be pr ps l pt pu"># Default device<br/>some_tensor = torch.zeros(size=(5, 5))<br/>some_tensor.device<br/><br/>&gt;&gt;&gt; device(type='cpu')<br/><br/># Explicit device<br/>some_tensor = torch.zeros(size=(5, 5), device=torch.device('cuda:0'))<br/>some_tensor.device<br/><br/>&gt;&gt;&gt; device(type='cuda', index=0)</span></pre><p id="0d59" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">重要的是要记住，不同设备上的张量之间的运算是不可能的。否则，您会看到类似这样的内容:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qa"><img src="../Images/987387fedcc213b770d213a2eff3ece8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h0ySNjd6-Jv2IQf2gaYxpg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图3 py torch运行时错误:张量位于不同设备上时的操作</figcaption></figure><p id="b80e" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">形状</strong></p><p id="f8b7" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><code class="fe oz pa pb pc b">torch.shape</code>表示张量的形状。例如，这是我们之前创建随机张量时作为<code class="fe oz pa pb pc b">size</code>传递的值。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="c043" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.zeros(size=(2, 3))<br/>some_tensor.shape<br/><br/>&gt;&gt;&gt; torch.Size([2, 3])</span></pre><h1 id="44cf" class="oj mv it bd mw ok pe om mz on pf op nc ki pg kj nf kl ph km ni ko pi kp nl ot bi translated">张量运算</h1><p id="a427" class="pw-post-body-paragraph lu lv it lw b lx no kd lz ma np kg mc md pj mf mg mh pk mj mk ml pl mn mo ls im bi translated">在深度学习应用中，从基本观点来看，模型通过对张量执行一系列操作来学习。其中包括:</p><ul class=""><li id="f3be" class="nm nn it lw b lx mp ma mq md pw mh px ml py ls nt nu nv nw bi translated">基本运算:加、减、积、除</li><li id="f6fd" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated">矩阵积</li><li id="abe0" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated">点积</li><li id="d328" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated">聚合运算:min，max，argmin，argmax，mean，sum</li></ul><p id="1998" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">基本操作</strong></p><p id="0c80" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">基本操作是基于元素的。您可以直接使用运算符或内置方法；结果是一样的。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="5ca3" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.tensor([1, 2, 3])<br/><br/># Addition<br/>some_tensor + 10<br/>torch.add(some_tensor, 10)<br/><br/>&gt;&gt;&gt; tensor([11, 12, 13])<br/><br/># Subtraction<br/>some_tensor - 10<br/>torch.sub(some_tensor, 10)<br/><br/>&gt;&gt;&gt; tensor([-9, -8, -7])<br/><br/># Product<br/>some_tensor * 10<br/>torch.mul(some_tensor, 10)<br/><br/>&gt;&gt;&gt; tensor([10, 20, 30])<br/><br/># Division<br/>some_tensor / 2<br/>torch.div(some_tensor, 2)<br/><br/>&gt;&gt;&gt; tensor([0.5000, 1.0000, 1.5000])</span></pre><p id="60bd" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">值得注意的是，张量值不会原地改变。要更新张量值，需要将结果重新分配给张量本身，如下所示:</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="6cb2" class="pq mv it pc b be pr ps l pt pu">some_tensor = some_tensor + 10</span></pre><p id="f071" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">这些操作也可以在两个张量之间进行；在这种情况下，它们必须具有相同的形状。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="e1ce" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.tensor([1, 2, 3])<br/>another_tensor = torch.tensor([1, 2, 3])<br/>some_tensor + another_tensor<br/><br/>&gt;&gt;&gt; tensor([2, 4, 6])</span></pre><p id="0b97" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">两个张量的元素乘积，也称为<em class="pv"> Hadamard乘积</em>，总是保持张量的形状。所有其他基本操作也保持张量形状。</p><p id="1413" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">矩阵乘积</strong></p><p id="7904" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">矩阵乘积是线性代数以及深度学习中众所周知的特定运算，因为它是最常见的运算之一。</p><p id="c515" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">它可以由专门的操作员或使用内置方法来执行。</p><p id="84a9" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">执行矩阵乘积时的主要规则是内部尺寸必须始终匹配:<code class="fe oz pa pb pc b">torch.Size([1, 2]) @ torch.Size([2, 1])</code>将起作用，因为在两种情况下内部尺寸都是2。是的，<code class="fe oz pa pb pc b">@</code>是矩阵乘积算子。结果张量形状将是最外面的维度:在上面的场景中，它将是<code class="fe oz pa pb pc b">torch.Size([1, 1])</code>。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="961a" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.tensor([[1, 2], [3, 4]])<br/>another_tensor = torch.tensor([[1, 2], [3, 4]])<br/><br/># 3 ways to write the same operation<br/>torch.matmul(some_tensor, another_tensor)<br/>torch.mm(some_tensor, another_tensor)<br/>some_tensor @ another_tensor<br/><br/>&gt;&gt;&gt; tensor([[ 7, 10],<br/>            [15, 22]])</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qb"><img src="../Images/91063642c0ca2b8d2cc87ccc54b7eb03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mpcbMQW_RWfo-vyEwXNQew.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图4矩阵产品</figcaption></figure><p id="f91e" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">点积</strong></p><p id="f8e8" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">点积只适用于1D张量。它只是两个向量的元素乘积之和。</p><p id="9446" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">方法是<code class="fe oz pa pb pc b">torch.dot(input, other)</code>。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="f201" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.tensor([2, 3])<br/>another_tensor = torch.tensor([2, 1])<br/><br/>torch.dot(some_tensor, another_tensor)<br/><br/>&gt;&gt;&gt; tensor(7)</span></pre><p id="7db4" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">点积的结果总是标量。当在两个一维张量上调用矩阵乘积时，也执行点积:如果你试图在两个1D向量上单独执行矩阵乘积，你应该注意到你也在写点积。</p><p id="838e" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">聚合操作</strong></p><p id="912a" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">聚合操作是一种特殊类型的操作，在这种操作中，可以从更多项到更少项，有时是标量。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="eff8" class="pq mv it pc b be pr ps l pt pu"># Defining the tensor<br/>some_tensor = torch.arange(0, 20, 2)<br/>some_tensor<br/><br/>&gt;&gt;&gt; tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])<br/><br/># Min<br/>some_tensor.min()<br/>&gt;&gt;&gt; tensor(0)<br/><br/># Max<br/>some_tensor.max()<br/>&gt;&gt;&gt; tensor(18)<br/><br/># Argmin (position of the smaller value)<br/>some_tensor.argmin()<br/>&gt;&gt;&gt; tensor(0)<br/><br/># Argmax (position of the bigger value)<br/>some_tensor.argmax()<br/>&gt;&gt;&gt; tensor(9)<br/><br/># Mean<br/>some_tensor.type(torch.float32).mean() # The float datatype is required<br/>&gt;&gt;&gt; tensor(9.)<br/><br/># Sum<br/>some_tensor.sum()<br/>&gt;&gt;&gt; tensor(90)</span></pre><p id="7a5c" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">还可以定义执行聚合的轴。以sum为例:</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="b5de" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.tensor([[[1, 2], [3, 4]], [[1, 2], [3, 4]]])<br/>some_tensor<br/><br/><br/>&gt;&gt;&gt; tensor([[[1, 2],<br/>            [3, 4]],<br/><br/>           [[1, 2],<br/>            [3, 4]]])<br/><br/>some_tensor.sum(axis=0)<br/>&gt;&gt;&gt; tensor([[2, 4],<br/>            [6, 8]])<br/><br/>some_tensor.sum(axis=1)<br/>&gt;&gt;&gt; tensor([[4, 6],<br/>            [4, 6]])<br/><br/>some_tensor.sum(axis=2)<br/>&gt;&gt;&gt; tensor([[3, 7],<br/>            [3, 7]])</span></pre><p id="f314" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">我们也可以直观地看到:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qc"><img src="../Images/f2808965e998201866966de613f712f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OU3emfU-zpsqhDli_jV68w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图5一个轴上的总和</figcaption></figure><p id="f960" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">作为一个容易记住哪个维度将被减少的技巧，你可以看一下<code class="fe oz pa pb pc b">torch.Size()</code>:在3D张量的情况下，第一个数组值(<code class="fe oz pa pb pc b">[0]</code>)与深度相关，因此<code class="fe oz pa pb pc b">axis=0</code>将在这个维度上减少，其他维度也是如此。</p><p id="96cc" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">值得注意的是轴的选择如何总是让输出失去这个轴。这种行为可以通过在函数调用中指定关键字<code class="fe oz pa pb pc b">keepdim=True</code>来避免。</p><p id="eff1" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">您甚至可以使用相同的逻辑提供多个轴。</p><h1 id="a55a" class="oj mv it bd mw ok pe om mz on pf op nc ki pg kj nf kl ph km ni ko pi kp nl ot bi translated">研究张量维度</h1><p id="d248" class="pw-post-body-paragraph lu lv it lw b lx no kd lz ma np kg mc md pj mf mg mh pk mj mk ml pl mn mo ls im bi translated">那么，还有另一套张量运算。这些影响张量维数，而不是张量值本身。其中包括:</p><ul class=""><li id="ffaa" class="nm nn it lw b lx mp ma mq md pw mh px ml py ls nt nu nv nw bi translated">重塑</li><li id="19dc" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated">堆叠和连接</li><li id="6e7f" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated">挤压和解除挤压</li></ul><p id="78e9" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">整形</strong></p><p id="6fd7" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><code class="fe oz pa pb pc b">torch.reshape(input, shape)</code>允许改变张量的形状，而不改变元素的数量和它们的值。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="2de9" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.tensor([[1, 2], [3, 4]])<br/>some_tensor.shape<br/><br/>&gt;&gt;&gt; torch.Size([2, 2])<br/><br/>some_tensor = some_tensor.reshape(4, 1)<br/>some_tensor.shape<br/><br/>&gt;&gt;&gt; torch.Size([4, 1])</span></pre><p id="79e8" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">整形时，唯一的规则是元素的数量必须始终相同(参数的乘积必须与大小元素的乘积相匹配)。<code class="fe oz pa pb pc b">numel()</code>方法是检查张量中元素数量的一种更快的方法。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="892f" class="pq mv it pc b be pr ps l pt pu">some_tensor.numel()<br/>&gt;&gt;&gt; 4</span></pre><p id="fa2d" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">拼接和堆叠</strong></p><p id="7776" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><code class="fe oz pa pb pc b">torch.cat(tensors, dim=0)</code>和<code class="fe oz pa pb pc b">torch.stack(tensors, dim=0)</code>方法非常相似，区分它们可能很有挑战性。</p><p id="6d12" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">简而言之:</p><ul class=""><li id="5e81" class="nm nn it lw b lx mp ma mq md pw mh px ml py ls nt nu nv nw bi translated"><code class="fe oz pa pb pc b">cat</code>连接给定维度上的张量序列。</li><li id="20cc" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><code class="fe oz pa pb pc b">stack</code>沿新维度连接一个<em class="pv"> </em>张量序列。它<strong class="lw jd">堆叠</strong>它们。</li></ul><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="089e" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.tensor([[1, 2], [3, 4]])<br/>another_tensor = torch.tensor([[5, 6], [7, 8]])</span></pre><p id="c788" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">我们刚刚创建了两个<code class="fe oz pa pb pc b">2x2</code>矩阵。让我们看看它们是如何堆叠和连接在一起的。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="908f" class="pq mv it pc b be pr ps l pt pu"># Concatenating along rows (dim=0)<br/>torch.cat([some_tensor, another_tensor], dim=0)<br/><br/>&gt;&gt;&gt; tensor([[1, 2],<br/>            [3, 4],<br/>            [5, 6],<br/>            [7, 8]])</span></pre><p id="de38" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">自己检查一下:第一个(0)形状值增加了。张量从2行变成了4行。</p><p id="e4a0" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">跨行串联可以视为在第一个矩阵下添加第二个矩阵中的行。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/c77f9f72820a7627c1e10926b5a26ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*9fOSDCLFkBCJN3Z4RK5syA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图6沿行串联</figcaption></figure><p id="a29c" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">对于列也是如此:</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="6468" class="pq mv it pc b be pr ps l pt pu"># Concatenating along columns (dim=1)<br/>torch.cat([some_tensor, another_tensor], dim=1)<br/><br/>&gt;&gt;&gt; tensor([[1, 2, 5, 6],<br/>            [3, 4, 7, 8]])</span></pre><p id="057b" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">我们刚刚向第一个矩阵添加了列，这些列取自旁边的第二个矩阵。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/659b426ca1929c4a945ccb12f118147c.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*xbmWP4762I2webkblWZGZQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图7沿列串联</figcaption></figure><p id="3238" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">这同样适用于任何N维张量序列。</p><p id="dd75" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">堆叠机制有点复杂。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="bd2e" class="pq mv it pc b be pr ps l pt pu"># Stacking along rows (dim=0)<br/>torch.stack([some_tensor, another_tensor], dim=0)<br/><br/>&gt;&gt;&gt; tensor([[[1, 2],<br/>             [3, 4]],<br/><br/>            [[5, 6],<br/>             [7, 8]]])</span></pre><p id="70ea" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">沿行堆叠相当于将第二个矩阵堆叠在第一个矩阵之后，增加了一个深度维度。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="8c5a" class="pq mv it pc b be pr ps l pt pu"># Stacking along columns (dim=1)<br/>torch.stack([some_tensor, another_tensor], dim=1)<br/><br/>&gt;&gt;&gt; tensor([[[1, 2],<br/>         [5, 6]],<br/><br/>        [[3, 4],<br/>         [7, 8]]])</span></pre><p id="fd1f" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">当<code class="fe oz pa pb pc b">dim=1</code>过去后，事情变得一团糟。在这种情况下，原始矩阵列创建深度维度，然后新的张量堆叠在一起。</p><p id="a545" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><strong class="lw jd">挤压和解除挤压</strong></p><p id="9d5f" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">PyTorch中的挤压用于<em class="pv">挤压</em>张量，移除尺寸为1的所有维度。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="ed2f" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.tensor([[[1, 2, 3]]])<br/>some_tensor.shape<br/><br/>&gt;&gt;&gt; torch.Size([1, 1, 3])</span></pre><p id="00c8" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">张量<code class="fe oz pa pb pc b">some_tensor</code>有两个<em class="pv">空</em>维度。让我们挤压它。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="7687" class="pq mv it pc b be pr ps l pt pu">some_tensor = some_tensor.squeeze()<br/>some_tensor.shape<br/><br/>&gt;&gt;&gt; torch.Size([3])</span></pre><p id="73ee" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">也可以提供施加挤压的特定尺寸。实验一下，看看会发生什么。</p><p id="5763" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">取消挤压与挤压相反:它在指定位置插入一个大小为1的新尺寸。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="ede9" class="pq mv it pc b be pr ps l pt pu">some_tensor = some_tensor.unsqueeze(0)<br/>some_tensor.shape<br/><br/>&gt;&gt;&gt; torch.Size([1, 3])</span></pre><h1 id="719b" class="oj mv it bd mw ok pe om mz on pf op nc ki pg kj nf kl ph km ni ko pi kp nl ot bi translated">索引</h1><p id="8119" class="pw-post-body-paragraph lu lv it lw b lx no kd lz ma np kg mc md pj mf mg mh pk mj mk ml pl mn mo ls im bi translated">张量元素可以通过索引直接访问，就像任何其他Python列表一样。</p><p id="f9ff" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">不同维度的索引可以使用额外的方括号或逗号分隔。您可以使用特殊字符<code class="fe oz pa pb pc b">:</code>来选择某个尺寸的所有值。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="2975" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.tensor([[[1, 2], [3, 4]], [[11, 22], [33, 44]]])<br/>some_tensor.shape<br/><br/>&gt;&gt;&gt; torch.Size([2, 2, 2])<br/><br/># Only the first element in depth, all rows, and all columns<br/>some_tensor[0, :, :]<br/><br/>&gt;&gt;&gt; tensor([[1, 2],<br/>            [3, 4]])<br/><br/># First row and first column, complete depth<br/>some_tensor[:, 0, 0]<br/><br/>&gt;&gt;&gt; tensor([ 1, 11])</span></pre><p id="087e" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">您还可以像在Python列表中一样，使用语法<code class="fe oz pa pb pc b">from:to</code>给出一系列索引。</p><h1 id="f300" class="oj mv it bd mw ok pe om mz on pf op nc ki pg kj nf kl ph km ni ko pi kp nl ot bi translated">广播</h1><p id="ce6b" class="pw-post-body-paragraph lu lv it lw b lx no kd lz ma np kg mc md pj mf mg mh pk mj mk ml pl mn mo ls im bi translated">广播是一种强大的机制，在执行元素操作时应该加以利用。</p><p id="6394" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">在对不同形状的张量执行元素运算时，广播开始发挥作用。</p><blockquote class="li"><p id="1b9e" class="lj lk it bd ll lm ou ov ow ox oy ls dk translated">简而言之，如果PyTorch操作支持广播，那么它的张量参数可以自动扩展为相同的大小(无需复制数据)。</p></blockquote><p id="4048" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo ls im bi translated">那么广播实际上是如何工作的呢？</p><ol class=""><li id="d6f6" class="nm nn it lw b lx mp ma mq md pw mh px ml py ls qf nu nv nw bi translated">如果张量不共享相同的<em class="pv">秩</em>(轴的数量，如<code class="fe oz pa pb pc b">torch.Tensor.ndim</code>所示)，则在较低秩张量的形状前加上1，使得它们长度相同(输出<code class="fe oz pa pb pc b">tensor.ndim</code>返回相同的数字)。</li><li id="40be" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls qf nu nv nw bi translated">检查张量是否在所有维度上都与<strong class="lw jd">兼容。如果两个张量在一个维度上具有<strong class="lw jd"> a) </strong>相同的大小，或者<strong class="lw jd"> b) </strong>其中一个在该维度上的大小为1，则称这两个张量在该维度上<em class="pv">相容。如果它们在所有维度上都兼容，那么它们可以一起播放。</em></strong></li></ol><p id="e430" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">如果遵循这些规则，两个张量被认为是可广播的，那么对于每个维度，结果大小是两者之间的最大值。</p><p id="3461" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">如果一开始看起来有点困惑，不要担心；这是典型的。我们来看几个例子。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="5ecd" class="pq mv it pc b be pr ps l pt pu">a = torch.rand(size=(3, 6, 2, 1))<br/>b = torch.rand(size=(   6, 1, 1))</span></pre><p id="f1b5" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated"><code class="fe oz pa pb pc b">a</code>和<code class="fe oz pa pb pc b">b</code> <em class="pv">是可广播的</em>吗？</p><ol class=""><li id="cbf7" class="nm nn it lw b lx mp ma mq md pw mh px ml py ls qf nu nv nw bi translated">他们不属于同一级别。<code class="fe oz pa pb pc b">a</code>有4个维度，而<code class="fe oz pa pb pc b">b</code>有3个维度。因此，我们必须在<code class="fe oz pa pb pc b">b</code>形状前加上<em class="pv">和</em>，直到尺寸匹配。<code class="fe oz pa pb pc b">b</code>形状变成了<code class="fe oz pa pb pc b">([1, 6, 1, 1)]</code>。</li><li id="0270" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls qf nu nv nw bi translated">它们在所有维度上都兼容吗？</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/66b7e9e8f36535203ac21187a8291d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*Qyphbx0XGywQuRWLacpeZw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图8检查张量是否兼容</figcaption></figure><p id="0072" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">好吧！这两个张量是相容的。每个维度的最终大小将是两者之间的较大值:</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="6ac2" class="pq mv it pc b be pr ps l pt pu">(a * b).shape<br/><br/>&gt;&gt;&gt; torch.Size([3, 6, 2, 1])</span></pre><p id="ddff" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">当一个轴上的一个张量的大小为1，而另一个张量的大小大于1时，第一个张量的行为就好像它是沿着那个维度被复制的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qh"><img src="../Images/b4836d190ee78ef1a5184e354166b59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UoAnKO_x55RagL9cKMl9MA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图9广播行为</figcaption></figure><p id="4c42" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">复制第一行，这样得到的张量有3行，并且形状匹配。</p><p id="8321" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">正如我大学的教授曾经告诉我的那样，如果你是第一次研究这个概念，我建议你休息24小时，然后回来再读一遍。事情会变得更清楚。</p><h1 id="110b" class="oj mv it bd mw ok pe om mz on pf op nc ki pg kj nf kl ph km ni ko pi kp nl ot bi translated">往返于NumPy</h1><p id="5c61" class="pw-post-body-paragraph lu lv it lw b lx no kd lz ma np kg mc md pj mf mg mh pk mj mk ml pl mn mo ls im bi translated">为了方便与NumPy交互，torch提供了两种主要方法:</p><ul class=""><li id="977b" class="nm nn it lw b lx mp ma mq md pw mh px ml py ls nt nu nv nw bi translated"><code class="fe oz pa pb pc b">torch.Tensor.numpy()</code></li><li id="2665" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><code class="fe oz pa pb pc b">torch.from_numpy(ndarray)</code></li></ul><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="1c58" class="pq mv it pc b be pr ps l pt pu">some_tensor = torch.ones(size=(2, 2))<br/><br/># Going from PyTorch to NumPy<br/>numpy_tensor = some_tensor.numpy()<br/>numpy_tensor += 1<br/><br/>numpy_tensor<br/>&gt;&gt;&gt; [[2. 2.]<br/>     [2. 2.]]<br/><br/>some_tensor<br/>&gt;&gt;&gt; tensor([[2., 2.],<br/>            [2., 2.]])</span></pre><p id="1f85" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">因为张量和数组共享底层内存位置，所以对其中一个的更改会影响另一个。</p><pre class="ks kt ku kv gt pm pc pn bn po pp bi"><span id="c92c" class="pq mv it pc b be pr ps l pt pu"># Going from NumPy to PyTorch<br/>tensor_again = torch.from_numpy(numpy_tensor)<br/>tensor_again += 1<br/><br/>tensor_again<br/>&gt;&gt;&gt; tensor([[3., 3.],<br/>            [3., 3.]])<br/><br/>numpy_tensor<br/>&gt;&gt;&gt; [[3. 3.]<br/>     [3. 3.]]</span></pre></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="645e" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">我真诚地希望你喜欢这篇文章。如果是这样，随时让我鼓掌或更多。</p><p id="0597" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">如果你有任何反馈，疑问，或建议，或者你只是想分享你的观点，请给我留下评论。</p><p id="b4f3" class="pw-post-body-paragraph lu lv it lw b lx mp kd lz ma mq kg mc md mr mf mg mh ms mj mk ml mt mn mo ls im bi translated">再见！</p><h2 id="ae47" class="mu mv it bd mw mx my dn mz na nb dp nc md nd ne nf mh ng nh ni ml nj nk nl iz bi translated">参考</h2><ul class=""><li id="d1bf" class="nm nn it lw b lx no ma np md nq mh nr ml ns ls nt nu nv nw bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/PyTorch" rel="noopener ugc nofollow" target="_blank"> PyTorch维基百科</a></li><li id="f50b" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><a class="ae lh" href="https://pytorch.org/docs/stable/index.html" rel="noopener ugc nofollow" target="_blank"> PyTorch文档</a></li><li id="f82f" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><a class="ae lh" href="https://www.learnpytorch.io/" rel="noopener ugc nofollow" target="_blank">学习PyTorch </a></li><li id="4740" class="nm nn it lw b lx nx ma ny md nz mh oa ml ob ls nt nu nv nw bi translated"><a class="ae lh" href="https://numpy.org/doc/stable/user/index.html" rel="noopener ugc nofollow" target="_blank">数量单据</a></li></ul><div class="qi qj gp gr qk ql"><a href="https://medium.com/@gabrielemattioli98/membership" rel="noopener follow" target="_blank"><div class="qm ab fo"><div class="qn ab qo cl cj qp"><h2 class="bd jd gy z fp qq fr fs qr fu fw jc bi translated">通过我的推荐链接加入Medium-Gabriele matti oli</h2><div class="qs l"><h3 class="bd b gy z fp qq fr fs qr fu fw dk translated">阅读Gabriele Mattioli的每一个故事(以及媒体上成千上万的其他作家)。您的会员费直接…</h3></div><div class="qt l"><p class="bd b dl z fp qq fr fs qr fu fw dk translated">medium.com</p></div></div><div class="qu l"><div class="qv l qw qx qy qu qz lb ql"/></div></div></a></div></div></div>    
</body>
</html>