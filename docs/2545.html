<html>
<head>
<title>Create Realistic 3D Renderings with AI!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用人工智能创建逼真的3D效果图！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/create-realistic-3d-renderings-with-ai-e92a063381d9?source=collection_archive---------0-----------------------#2022-02-15">https://pub.towardsai.net/create-realistic-3d-renderings-with-ai-e92a063381d9?source=collection_archive---------0-----------------------#2022-02-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="52c8" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="d4fd" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从几张图片到使用AI的3D模型！</h2></div><blockquote class="kr ks kt"><p id="bb3f" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/neroic/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/neroic/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><h2 id="0478" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">观看视频</h2><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="44e0" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">神经渲染。神经渲染是从感兴趣的物体、人或场景的图片中生成一个像这样的空间真实感模型的能力。在这种情况下，你会有一些这个雕塑的照片，并要求机器理解这些照片中的物体在太空中应该是什么样子。你基本上是在要求一台机器从图像中理解物理和形状。这对我们来说很容易，因为我们只知道真实世界和深度，但对于一台只能看到像素的机器来说，这是一个完全不同的挑战。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/dcfcacb014600c167018cd848157b76d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*d-OrDF_a10xI49qBsCc1Jg.gif"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">使用模型(NeROIC)的新颖视图合成。来自<a class="ae lr" href="https://formyfamily.github.io/NeROIC/" rel="noopener ugc nofollow" target="_blank">项目页面</a>的Gif。</figcaption></figure><p id="1d95" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">那么，你可能会问，我们为什么要这么做？我想说答案很明显。对我来说，有很多很酷的应用程序，可以简单地拍摄一些物体的照片，并完美地合成3D模型，将其放在图像中，3D场景中，甚至是视频游戏中。<br/>这确实很有前景，但要让这些模型逼真，照明是这些应用带来的另一个挑战。</p><p id="d296" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">生成的模型看起来很准确，形状逼真，这很好，但是它如何融入新的场景呢？如果拍摄的照片中的光照条件不同，生成的模型看起来也不同，这取决于您查看它的角度，该怎么办？对我们来说，这自然会显得怪异和不切实际。这些是Snapchat和南加州大学在这项新研究中应对的挑战。</p><p id="729d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">现在，让我们来看看他们是如何解决从图像中创建虚拟物体所带来的照明和真实感挑战的。该技术建立在神经辐射场的基础上，神经辐射场主要用于许多模型的重建，如我们在<a class="ae lr" href="https://medium.com/what-is-artificial-intelligence/generate-a-complete-3d-scene-under-arbitrary-lighting-conditions-from-a-set-of-input-images-9d2fbce63243" rel="noopener">以前的文章</a>中已经介绍过的NeRF。通常，神经辐射场需要在相同的理想条件下拍摄的图像，但这不是我们在这里想要的。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nf"><img src="../Images/e053f9209cdb0617a7890a7b30567cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K3MTTsjayip7_X0--zx1lg.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">NeRF方法。图片来自<a class="ae lr" href="https://arxiv.org/pdf/2003.08934.pdf" rel="noopener ugc nofollow" target="_blank">纸张</a>。</figcaption></figure><p id="ae98" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">他们的方法从NeRF开始，正如我所说的，我已经在我的频道上介绍过了，所以我不会再介绍了，但是可以休息一下，阅读我的文章来更好地了解NeRF是如何工作的。简而言之，NeRF是一个神经网络，它被训练成使用图像作为输入来推断每个像素的颜色、不透明度和亮度，并猜测图像中不存在的物体小部分的缺失像素。但是这种方法不适用于大量缺失的部分或不同的光照条件，因为它只能从输入图像中进行插值。在这里，我们需要更多的东西来进行推断，并对这里和那里应该出现什么或者这些像素在这种照明下应该看起来如何等做出假设。</p><p id="8bb9" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">许多方法基于NeRF来解决这个问题，但总是需要用户更多的输入，这不是我们想要的，并且在许多情况下很难实现，特别是当我们想要建立一个好的数据集来训练我们的模型时。简而言之，这些模型并没有真正理解物体或物体所处的环境。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ng"><img src="../Images/5af64063a363c29c572e0f0965c508f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Tujh7Gln98ajupz5epGIQ.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">摄像机参数和NeRF模型生成。图片来自<a class="ae lr" href="https://arxiv.org/pdf/2003.08934.pdf" rel="noopener ugc nofollow" target="_blank">报</a>。</figcaption></figure><p id="e448" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">所以我们总是回到照明问题…在这里，他们的目标是在在线图像中使用这种架构。或者，换句话说，不同的灯光、摄像机、环境和姿势的图像。一些NeRF很难做到的现实主义。</p><p id="ba64" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">除了对象本身的图像之外，他们唯一需要的是粗略的前景分割和相机参数的估计，这两者都可以通过其他可用的模型来获得。前景估计基本上只是一个遮罩，告诉您感兴趣的对象在图像上的位置，就像这样:</p><div class="mn mo mp mq gt ab cb"><figure class="nh mr ni nj nk nl nm paragraph-image"><img src="../Images/e5de4f13b0977d8801d330043ed48901.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*EyC4L7Y1LiPZOAZSMMFVCA.png"/></figure><figure class="nh mr ni nj nk nl nm paragraph-image"><img src="../Images/fe6b04645bb2df21363d55789d59d390.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*Eks8yHhBebjCbHliwBfeMw.png"/><figcaption class="nb nc gj gh gi nd ne bd b be z dk nn di no np translated">电视及其分段遮罩的图像。</figcaption></figure></div><p id="fa91" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">他们所做的不同之处在于，他们将物体的渲染与输入图像中的环境照明分开。他们关注两件事，分两个阶段完成。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nq"><img src="../Images/37a509ee2ced0d8fe79291745610c9b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yMfmZpmfHiHzX0T3XxUCag.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">模型的概述，我们将在下面看到。图片来自<a class="ae lr" href="https://arxiv.org/pdf/2201.02533.pdf" rel="noopener ugc nofollow" target="_blank">报</a>。</figcaption></figure><p id="7657" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">首先(a)是物体的形状，或者说它的几何形状，这是与NeRF最相似的部分，这里称为几何网络。它将采用我们讨论过的输入图像、分割蒙版和相机参数估计，建立一个辐射场，并找到每个像素的密度和颜色的第一个猜测，就像在NeRF中一样，但适用于输入图像中变化的照明条件。这种差异依赖于您在这里看到的两个分支，将静态内容从可变参数(如相机或阴影)中分离出来。这将允许我们教导我们的模型如何正确地将静态内容与其他不需要的参数(如照明)隔离开来。但是我们还没有结束。</p><p id="03d2" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">在(b)中，我们将从这个学习到的密度场估计表面法线，这将是我们的纹理。或者，换句话说，它将利用我们刚刚产生的结果，找到我们的对象对光的反应。使用具有Sobel核的3D卷积，它将在这个阶段找到物体的无偏材料属性，或者至少是它的估计。它基本上是一个过滤器，我们在三维空间中应用它来找到所有的边缘以及它们的锐度，在二维图像上看起来像这样(下图，左)，在三维渲染上看起来像这样(下图，右)，给我们关于物体的不同纹理和形状的重要信息。</p><div class="mn mo mp mq gt ab cb"><figure class="nh mr nr nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/a24646834f01a7ad49c193477a33aa3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*5ADtU2mvH3TICuo6avwoDA.png"/></div></figure><figure class="nh mr ns nj nk nl nm paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><img src="../Images/ca0dd7f80d079b1b6a3f269917a97cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*bPdq0I-Y4XqRNzFUfvJ4XQ.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk nt di nu np translated">2D应用的Sobel过滤器(左)和使用3D Sobel过滤器产生的表面法线(右)。</figcaption></figure></div><p id="9ad6" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">下一个阶段(c)是他们将修正学习到的几何图形，并优化我们刚刚使用这个渲染网络生成的法线，这与第一个几何图形网络非常相似。这里也有两个分支，一个用于材质，另一个用于照明。他们将使用球面谐波来表示照明模型，并在训练期间优化其系数。正如他们在论文中解释的，如果你感兴趣的话，还有更多信息，这里使用球谐函数来表示定义在球面上的一组基函数。我们可以在维基百科上查到“定义在球面上的每一个函数都可以写成这些球谐函数的和”。该技术通常用于计算3D模型上的光照。它以相对较少的开销产生高度真实的着色和阴影。简而言之，它将简单地减少要估计的参数数量，但保持相同的信息量。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi nv"><img src="../Images/39d77aac9d9fb069975dce8ad558dfe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3EpFlORWMXjTFFZF.png"/></div></a></figure><p id="36cf" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">因此，该模型不是从头开始学习如何为整个对象渲染适当的照明，而是学习正确的系数以用于球面谐波，该系数将估计每个像素表面发出的照明，从而将问题简化为几个参数。另一个分支将被训练来改进物体的表面法线，使用标准的Phong BRDF遵循相同的技巧，它将基于一些要找到的参数来模拟物体的材质属性。最后，两个分支的输出，因此最终的渲染和照明将合并，以找到每个像素的最终颜色。</p><p id="fe28" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">这种光线和材质的分离是他们能够将任何光线应用到物体上并让物体做出真实反应的原因。请记住，这仅仅是用互联网上的几张图片完成的，并且可能有不同的照明条件。这太酷了。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><a href="https://www.louisbouchard.ai/learnai/"><div class="gh gi nw"><img src="../Images/39961eb04fd5c1d78ab8262f14ef01ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*FG5cR8ppcW9OlHI2.png"/></div></a></figure><p id="241b" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">瞧！这就是匡和Snapchat合作者的这篇新论文如何创建NeROIC，一种用于在线图像对象的神经渲染模型！</p><p id="df52" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">我希望您喜欢这篇论文的简短概述。所有的参考链接如下，以及官方项目的链接，和他们的代码。让我知道你对这个解释、这个技术的看法，以及你将如何在现实世界中使用它！</p><p id="5dc8" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">感谢您的阅读，观看<a class="ae lr" href="https://www.youtube.com/watch?v=88Pl9zD1Z78" rel="noopener ugc nofollow" target="_blank">视频</a>了解更多示例！</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="0010" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)，并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">简讯</strong> </a>！</p><h2 id="5861" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">支持我:</h2><ul class=""><li id="26c5" class="oe of it kx b ky og lb oh mb oi mf oj mj ok lq ol om on oo bi translated">在<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a>跟我来</li><li id="eaed" class="oe of it kx b ky op lb oq mb or mf os mj ot lq ol om on oo bi translated">想进入AI或者提升技能，<a class="ae lr" href="https://www.louisbouchard.ai/learnai/" rel="noopener ugc nofollow" target="_blank">看这个</a>！</li></ul><h2 id="7f77" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">参考</h2><ul class=""><li id="1056" class="oe of it kx b ky og lb oh mb oi mf oj mj ok lq ol om on oo bi translated">匡，z .，Olszewski，k .，柴，m .，黄，z .，Achlioptas，p .和Tulyakov，s .，2022。NeROIC:来自在线图像集合的对象的神经渲染。<a class="ae lr" href="https://arxiv.org/pdf/2201.02533.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2201.02533.pdf</a></li><li id="fd0f" class="oe of it kx b ky op lb oq mb or mf os mj ot lq ol om on oo bi translated">项目链接与伟大的视频演示:<a class="ae lr" href="https://formyfamily.github.io/NeROIC/" rel="noopener ugc nofollow" target="_blank">https://formyfamily.github.io/NeROIC/</a></li><li id="bc53" class="oe of it kx b ky op lb oq mb or mf os mj ot lq ol om on oo bi translated">代号:<a class="ae lr" href="https://github.com/snap-research/NeROIC" rel="noopener ugc nofollow" target="_blank">https://github.com/snap-research/NeROIC</a></li></ul></div></div>    
</body>
</html>