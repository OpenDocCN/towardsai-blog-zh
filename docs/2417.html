<html>
<head>
<title>How to Train a Seq2Seq Text Summarization Model With Sample Code (Ft. Huggingface/PyTorch)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用示例代码训练Seq2Seq文本摘要模型(Ft。拥抱脸/PyTorch)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-train-a-seq2seq-text-summarization-model-with-sample-code-ft-huggingface-pytorch-8ba97492f885?source=collection_archive---------0-----------------------#2021-12-14">https://pub.towardsai.net/how-to-train-a-seq2seq-text-summarization-model-with-sample-code-ft-huggingface-pytorch-8ba97492f885?source=collection_archive---------0-----------------------#2021-12-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2a98" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><p id="871e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><em class="kx">介绍系列的第2部分，使用HuggingFace用示例代码训练一个文本摘要模型(或任何seq 2 seq/编码器-解码器架构)。</em></p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi ky"><img src="../Images/bc46b32c74fb343520a29e3595533183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z286Z3B-7__9noyC"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">照片由<a class="ae lo" href="https://unsplash.com/@aaronburden?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚伦·伯顿</a>在<a class="ae lo" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="a7ee" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">这篇文章旨在让你更深入地理解序列对序列(seq2seq)网络，以及如何训练它们用于自动文本摘要。我将使用Transformer体系结构，但不详细介绍它的实现细节，以使本文专注于培训部分。(已经有许多<a class="ae lo" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">资源</a> <a class="ae lo" href="https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04" rel="noopener">可用</a><a class="ae lo" href="https://machinelearningmastery.com/the-transformer-model/" rel="noopener ugc nofollow" target="_blank"/>)我们将利用<a class="ae lo" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>库，它托管主要的最先进的NLP模型。</p><blockquote class="lp lq lr"><p id="2474" class="jz ka kx kb b kc kd ke kf kg kh ki kj ls kl km kn lt kp kq kr lu kt ku kv kw im bi translated">⚠️我推荐阅读本系列的“<a class="ae lo" href="https://nlpiation.medium.com/how-to-use-huggingfaces-transformers-pre-trained-tokenizers-e029e8d6d1fa" rel="noopener">记号化简介</a>”故事和第一部分“<a class="ae lo" rel="noopener ugc nofollow" target="_blank" href="/a-full-introduction-on-text-summarization-using-deep-learning-with-sample-code-ft-huggingface-d21e0336f50c?source=your_stories_page----------------------------------------">文本摘要简介</a>”(尤其是如果你是NLP、编码器-解码器架构或文本摘要的新手)，在那里我写了编码器-解码器架构的基础知识。</p></blockquote><h1 id="2757" class="lv lw it bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">什么是培训？</h1><p id="3d97" class="pw-post-body-paragraph jz ka it kb b kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw im bi translated">这意味着通过展示几个例子来调整模型的权重，以做出准确的预测。根据任务的不同，有不同的算法来训练神经网络。我们将进行监督学习，这意味着通过提供源(文章)和目标(摘要)样本进行学习来训练模型。</p><p id="68d7" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">这个过程从向模型提供输入(文章)并生成摘要开始。然后，我们可以使用损失函数(这里是交叉熵)将生成的摘要与目标摘要进行比较，以计算它们有多接近，以及我们需要改变多少权重来使用优化器(Adam with Weight Decay)生成更好的摘要。这是一个迭代过程，将根据历元数重复进行。</p><p id="2053" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">一个时期意味着对数据集中的每个样本进行一次上述过程。因此，10个时期意味着模型将在训练期间看到每个样本10次。此外，还有批处理的概念，这意味着将数据以大于1的批量传递给模型，并对它们的损失进行平均，而不是一个样本一个样本地输入。这将加快训练过程。</p><h1 id="cbd1" class="lv lw it bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">它是如何工作的？</h1><p id="3db3" class="pw-post-body-paragraph jz ka it kb b kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw im bi translated">在深入研究代码并假设batch_size为1之前，让我们快速浏览一下图1。如果你读过第一部分的故事，你应该熟悉这个图的大部分。(<em class="kx">)用什么建筑？</em> " subsection)主要区别(与推断相比)在于，在训练过程中，整个目标摘要将被提供给解码器，因此模型可以了解好的摘要是什么样的。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi my"><img src="../Images/6283d1c5d2e74cab019eb9a3bdc10bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bKTpJYwew-IkTO7vnPXVkg.jpeg"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">图一。编解码模型体系结构综述。</figcaption></figure><p id="c17a" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">输出是大小为[generated _ sequence _ length×vocabulary _ size]的张量，其中第一维的每个索引表示来自所生成的摘要的一个标记，第二维的每个索引表示来自标记器的词汇表的一个单词及其成为下一个标记的概率。图2中示出了具有五个记号的生成摘要的样本输出张量。每个标记的可能性数量与词汇大小(9)相同。为了找到生成的摘要标记，我们将选择拥有最高概率的索引，并且可以使用该索引从词汇表中找到单词。</p><figure class="kz la lb lc gt ld gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi mz"><img src="../Images/6efb99578e3f9b691a6f757eeef352c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQ5YZD0BMOaajDs-IebGJQ.png"/></div></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">图二。对于长度为5个标记、词汇量为9的样本序列“快速的棕色狐狸跳跃”,模型的输出看起来像什么。</figcaption></figure><h1 id="a518" class="lv lw it bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">让我们看看代码！</h1><p id="8909" class="pw-post-body-paragraph jz ka it kb b kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw im bi translated"><strong class="kb jd">数据集—</strong>hugging face公司提供了一个名为“数据集”的伟大工具，让你可以快速加载和操作你的数据。我们将使用CNN/DailyMail [2]数据集，它是摘要任务的标准基准。让我们从加载数据集开始。我只用了1%的数据集来加速这个过程。(如果您想要训练您的模型，您应该通过移除<em class="kx"> [0:1%] </em>部分来使用整个数据集)</p><blockquote class="lp lq lr"><p id="e149" class="jz ka kx kb b kc kd ke kf kg kh ki kj ls kl km kn lt kp kq kr lu kt ku kv kw im bi translated">如果您对基于变压器的预训练模型(如BERT、BART、XLNet、MASS等)的差异感到困惑，请阅读故事“<a class="ae lo" href="https://medium.com/mlearning-ai/what-are-the-differences-in-pre-trained-transformer-base-models-like-bert-distilbert-xlnet-gpt-4b3ea30ef3d7" rel="noopener"> <em class="it">基于变压器的预训练模型</em> </a>有何不同”，指出它们的差异。</p></blockquote><figure class="kz la lb lc gt ld"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">代码1。正在加载CNN/DM数据集。</figcaption></figure><p id="8532" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">下一步是准备基于模型的数据集，除了查看。如图1所示，标记化的<em class="kx">输入</em>(文章)和<em class="kx">解码器输入</em>(目标摘要)以及它们的注意掩码(掩码可以使用它来忽略一些标记)加上标签参数(与目标摘要相同)。<em class="kx">process _ data _ to _ model _ inputs()</em>函数将使用BART的[1] tokenizer对象将数据集转换为所需的格式。它还将填充标记更改为-100，以确保不相关的标记不会影响丢失值。然后，我们将把提到的函数映射到数据集变量，设置正确的格式，并使用PyTorch的DataLoader函数创建一个迭代对象来批处理数据集。</p><figure class="kz la lb lc gt ld"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">代码2。正在准备数据集。</figcaption></figure><p id="b66e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">请记住，上面的代码只演示了训练集的映射和set_fromat过程。如果可能的话，也应该对其他装置如<em class="kx">验证</em>和<em class="kx">测试</em>进行测试。文章和摘要长度变量依赖于模型的设计选择。BART最多可以接受1024个令牌作为输入，但是我们将序列长度限制设置为512，以减少内存消耗。同样的想法也适用于选择批量大小，如果您有足够的资源，应该使用更大的数字。</p><p id="e7ba" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">模型— </strong>我们使用Huggingface的BART实现，这是一个预先训练好的基于transformer的seq2seq模型。让我们从加载模型及其预训练权重开始。</p><figure class="kz la lb lc gt ld"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">代码3。加载模型，并根据图1进行分割。</figcaption></figure><p id="6b50" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">BART的文本摘要微调模型使用<em class="kx">BartForConditionalGeneration</em>模块加载，并将使用<em class="kx"> from_pretrained() </em>调用下载权重。该模型将被发送到GPU(如果有的话),我们将根据图1中突出显示的组件对其进行分割。</p><blockquote class="lp lq lr"><p id="a753" class="jz ka kx kb b kc kd ke kf kg kh ki kj ls kl km kn lt kp kq kr lu kt ku kv kw im bi translated"><strong class="kb jd"> ⚠️关于分割模型的注释:</strong>我这样做是为了让连接和数据流更加清晰。这只是为了教学目的，向您展示该模型如何在引擎盖下工作。在现实中，你可以很容易地调用模型对象，它会照顾一切本身，而不是分裂它。(我们将在实现验证循环时看到它)</p></blockquote><p id="4dc9" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">损失函数和优化器— </strong>训练需要的另外两个模块是交叉熵损失和AdamW优化器，它们可以分别从PyTorch和Huggingface加载。还选择线性调度器来辅助优化器在训练期间改变学习速率。在预热期间，调度程序会将学习率从零增加到指定值，然后再降低到零。</p><figure class="kz la lb lc gt ld"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">代码4。定义损失函数和优化器。</figcaption></figure><p id="aa05" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">训练循环— </strong>现在，是时候编写主循环了。它只是负责向模型提供一批数据，并使用损失函数将其生成的摘要与所需的目标摘要进行比较。并且重复这个过程！</p><figure class="kz la lb lc gt ld"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">代码5。主训练循环。</figcaption></figure><p id="c493" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">主循环从将批处理放到GPU(如果可用)开始，然后绕过标记化的文章，通过编码器获得表示。编码器的输出将与目标摘要一起提供给解码器，并使用最后一个线性图层进行处理，以获得模型的最终预测。</p><blockquote class="lp lq lr"><p id="9646" class="jz ka kx kb b kc kd ke kf kg kh ki kj ls kl km kn lt kp kq kr lu kt ku kv kw im bi translated"><strong class="kb jd">提醒</strong>:这个过程可以(也应该！)通过调用<code class="fe nc nd ne nf b">model(**batch)</code>直接得到<strong class="kb jd"> <em class="it"> lm_head_output </em> </strong>来完成。</p></blockquote><p id="9418" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">循环的其余部分将计算当前批次的损失值并调整模型的参数。最后，向优化器和调度器发送一个信号，表明当前步骤已经完成，并通过重置梯度使优化器为下一步做好准备。</p><p id="be7c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">验证循环— </strong>难题的最后一块是在每个时期后进行验证循环，以确保模型不会过度拟合，并评估其在看不见的数据上的性能。此外，您可以在下面的代码中看到使用模型对象本身来计算损失是多么容易。</p><figure class="kz la lb lc gt ld"><div class="bz fp l di"><div class="na nb l"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk translated">代码6。验证循环。</figcaption></figure><blockquote class="lp lq lr"><p id="a065" class="jz ka kx kb b kc kd ke kf kg kh ki kj ls kl km kn lt kp kq kr lu kt ku kv kw im bi translated"><em class="it"> ⚠️的完整实现可在Github上的</em> <a class="ae lo" href="https://github.com/NLPiation/tutorial_notebooks/blob/main/summarization/hf_BART_train_breakdown.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="it">笔记本</em> </a> <em class="it">中获得。</em></p></blockquote><h1 id="3961" class="lv lw it bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">最后的话，</h1><p id="85e1" class="pw-post-body-paragraph jz ka it kb b kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw im bi translated">这是涵盖摘要任务的系列文章的第2部分，我在其中解释了在训练seq2seq(编码器-解码器)模型期间发生的事情。我建议您浏览一下笔记本内容，因为有些部分在这里没有提到，您可以在Google Colab实例上运行代码并查看输出。</p><blockquote class="ng"><p id="7d07" class="nh ni it bd nj nk nl nm nn no np kw dk translated">我每周给NLP的书呆子发一份时事通讯。如果您想了解自然语言处理的最新发展，可以考虑订阅。<br/> <a class="ae lo" href="https://nlpiation.github.io/" rel="noopener ugc nofollow" target="_blank">阅读更多，订阅</a> —加入酷孩子俱乐部，立即报名！</p></blockquote><h2 id="e03b" class="nq lw it bd lx nr ns dn mb nt nu dp mf kk nv nw mj ko nx ny mn ks nz oa mr iz bi translated">参考</h2><p id="d558" class="pw-post-body-paragraph jz ka it kb b kc mt ke kf kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw im bi translated">[1] Lewis，Mike，等人，“Bart:自然语言生成、翻译和理解的去噪序列间预训练”arXiv预印本arXiv:1910.13461 (2019)。<br/> [2]纳拉帕蒂，拉梅什，等，“使用序列到序列的rnns和超越序列的抽象文本摘要”<em class="kx"> arXiv预印本arXiv:1602.06023 </em> (2016)。</p></div></div>    
</body>
</html>