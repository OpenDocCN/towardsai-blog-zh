<html>
<head>
<title>Stitch it in Time: Facial Editing of Real Videos</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">及时缝合:真实视频的面部编辑</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/stitch-it-in-time-facial-editing-of-real-videos-741f0f653c16?source=collection_archive---------2-----------------------#2022-02-01">https://pub.towardsai.net/stitch-it-in-time-facial-editing-of-real-videos-741f0f653c16?source=collection_archive---------2-----------------------#2022-02-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b152" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="19b4" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">基于人工智能的高质量头部视频人脸操作</h2></div><blockquote class="kr ks kt"><p id="3738" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/stitch-it-in-time/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/stitch-it-in-time/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/9e1284a38fa317bf307dc42228b73d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_6i9ernIDuUDf9ggbq-mQ.png"/></div></div></figure><h2 id="09f5" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">观看视频</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="d623" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">你肯定看过像最近的《惊奇队长》或《双子杀手》这样的电影，在这些电影中，塞谬尔·杰克森和威尔·史密斯看起来要年轻得多。这需要专业人员花费数百甚至数千个小时来手工编辑他出现的场景。相反，你可以用一个简单的人工智能在几分钟内完成。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/cdc419672df8d9ef75189f23cb42dc59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*M6bEtioyw_RQHvYW6jbRTA.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">“老套”的例子。图片来自<a class="ae lr" href="https://arxiv.org/abs/2201.08361" rel="noopener ugc nofollow" target="_blank">纸</a>。</figcaption></figure><p id="882a" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">事实上，许多技术允许你添加微笑，让你看起来更年轻或更老，所有这些都是自动使用基于人工智能的算法。它们大多应用于图像，因为这要容易得多，但是同样的技术稍加调整也可以应用于视频，正如你可能怀疑的那样，这对于电影业来说是很有前途的。顺便说一下，您看到的结果都是使用我将在本文中讨论的技术制作的。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2d9ddbca6d391bd9547159aed32e7a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*qGLUMdOz8NJOdPX1brYI0A.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">结果示例。图片来自<a class="ae lr" href="https://arxiv.org/abs/2201.08361" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><p id="463e" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">主要的问题是，目前，这些生成的“旧版本”编辑图像不仅看起来很奇怪，而且当用于视频时，会有瑕疵和伪像，这肯定不是你在一部百万美元的电影中想要的。这是因为获得人的视频比图片要困难得多，这使得训练这样的人工智能模型更加困难，这些模型需要如此多不同的例子来理解要做什么。这种强烈的数据依赖性是当前AI与人类智能相差甚远的原因之一。这就是为什么像Rotem Tzaban和来自特拉维夫大学的合作者这样的研究人员努力提高自动AI视频编辑的质量，而不需要那么多视频示例。或者，更准确地说，使用经过图像训练的模型，在高质量的谈话头部视频中改进基于人工智能的面部操作。除了你要编辑的单个视频之外，它不需要任何东西，你可以添加一个微笑，让你看起来更年轻或更老。它甚至适用于动画视频！</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5153bc5d612f8ab4bdf321b6058b012b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*9GUzgT_sKqmerLB8fPVQ1Q.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">动画视频中的人脸编辑。图片来自<a class="ae lr" href="https://arxiv.org/abs/2201.08361" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><p id="f350" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">这太酷了，但更棒的是他们是如何做到的…</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi ng"><img src="../Images/b80496250d8004ac206a7a7829f5fe5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QDsnLCWNyAoYBoQy.png"/></div></a></figure><p id="bb45" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">当然，它使用了GANs或生成性对抗网络。我不会深入研究GAN的内部工作原理，因为我已经在<a class="ae lr" rel="noopener ugc nofollow" target="_blank" href="/how-ai-generates-new-images-gans-put-simply-674e413bc22a">一篇文章</a>中介绍过了，你可以在这里阅读<a class="ae lr" rel="noopener ugc nofollow" target="_blank" href="/how-ai-generates-new-images-gans-put-simply-674e413bc22a"/>，但是我们会看到它与基本的GAN架构有什么不同。如果你不熟悉GANs，就花一分钟读一下<a class="ae lr" rel="noopener ugc nofollow" target="_blank" href="/how-ai-generates-new-images-gans-put-simply-674e413bc22a">的文章</a>，然后回来，我仍然会在那里等你，我没有夸张。这篇文章花了一分钟的时间来概述什么是gan！</p><p id="12ef" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">如果可能，我们将只刷新具有生成模型的零件，该模型获取图像，或者图像的编码版本，并更改此代码以生成修改特定方面的图像的新版本。控制生成是具有挑战性的部分，因为它有如此多的参数，很难找到哪个参数负责什么，也很难理清一切，只编辑你想要的。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5eac79b689bd3fc1c827fcf5ae6c0c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*cis786j4TaEi0d9omHznsw.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">GAN网络的发生器如何工作。</figcaption></figure><p id="2d90" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">所以它使用任何基于生成的架构，比如本例中的<a class="ae lr" href="https://github.com/NVlabs/stylegan" rel="noopener ugc nofollow" target="_blank"> StyleGAN </a>。这只是一个强大的GAN架构，用于NVIDIA几年前发布的人脸图像，结果仍然非常令人印象深刻，而且版本更新。但是生成模型本身并不重要，因为它可以与你能找到的任何强大的GAN架构一起工作。</p><p id="ee3b" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">而且没错，就算这些模型都是为了图像而训练的，他们也会用它们来进行视频剪辑！假设您要发送的视频是真实的并且已经是一致的，他们将只专注于保持真实性，而不是像我们在视频合成工作中所做的那样创建一个真正一致的视频，<br/>我们在视频合成工作中创建新的视频。</p><p id="01de" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">因此，每张图像将被单独处理，而不是发送一个完整的视频，并期待一个新的视频作为回报。这种假设使任务变得更简单，但需要面对更多的挑战，比如保持这样一个逼真的视频，每一帧都流畅地进入下一帧，没有明显的故障。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nh"><img src="../Images/87d0cfb4cd8edab4006b6bdef5cc2b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kfut5_Ar7zcZJ7tXfNZ3BA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">模型概述。图片来自<a class="ae lr" href="https://arxiv.org/abs/2201.08361" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><p id="bccc" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">在这里，他们将视频的每一帧作为输入图像，仅提取面部并对齐(1)以保持一致性，这是我们将看到的重要步骤，使用他们预先训练的编码器(2)和生成器(3)对帧进行编码，并为每个帧生成新版本。不幸的是，这并不能解决现实主义的问题，当从一帧到另一帧时，新的面孔可能看起来很奇怪或不合适，还有奇怪的灯光错误和背景差异。</p><p id="90aa" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">为了解决这个问题，他们将进一步训练初始生成器(3 ),并使用它来帮助所有帧的生成更加相似和全局一致。他们还引入了另外两个步骤，一个编辑步骤和一个他们称之为“拼接-调谐”的新操作。</p><p id="bd90" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">编辑步骤(4)将简单地获取图像的编码版本，并对其进行一些更改。在这种情况下，这是它将学会改变它的部分，使这个人看起来更老。因此，该模型将被训练以理解移动哪些参数，以及对图像的正确特征进行多大程度的修改，以使该人看起来更老。比如加一些白发，加皱纹等等。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ni"><img src="../Images/66f76fd0281241c8c08782f349f457a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0hxcX7o27XVO3T8pPAKlvw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">详细步骤5:拼接-调音法。图片来自<a class="ae lr" href="https://arxiv.org/abs/2201.08361" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><p id="ec36" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">然后，这种拼接-调整方法(5)将采用您在此处看到的编码图像，并经过训练，从最适合背景和其他帧的编辑代码中生成图像。它将通过获取新生成的图像，将其与原始图像进行比较，并找到使用遮罩仅替换面部并保持裁剪图像的其余部分不变的最佳方式来实现这一点。</p><p id="5afa" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">最后，我们将修改后的面粘贴回框架上(6)。这个过程非常聪明，可以制作出真正高质量的视频，因为你只需要模型中经过裁剪和对齐的面部，这极大地降低了计算需求和任务的复杂性。因此，即使人脸需要很小，比如说200像素的平方，正如你在这里看到的，如果它只是图像的五分之一，你就可以保持相当高分辨率的视频。</p><p id="339d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">瞧！这些伟大的研究人员就是这样在视频中进行高质量的人脸操纵的！</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="https://www.louisbouchard.ai/learnai/"><div class="gh gi nj"><img src="../Images/590094e4b89505bd2e5b1e7d8e45d208.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*1JU1VMFDcxZi6otv.png"/></div></a></figure><p id="8004" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">感谢您的阅读，观看<a class="ae lr" href="https://youtu.be/mqItu9XoUgk" rel="noopener ugc nofollow" target="_blank">视频</a>获取更多示例！</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><p id="fdb4" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">如果你喜欢我的工作，并想了解人工智能的最新动态，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)，并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">简讯</strong> </a>！</p><h2 id="3817" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">支持我:</h2><ul class=""><li id="ad02" class="nr ns it kx b ky nt lb nu mn nv mr nw mv nx lq ny nz oa ob bi translated">支持我的最好方式是成为这个网站<strong class="kx jd"> </strong>的会员，或者如果你喜欢视频格式，在<strong class="kx jd"> YouTube </strong>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="0be5" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">跟着我上<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a></li><li id="070f" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">想进入AI或者提升技能，<a class="ae lr" href="https://www.louisbouchard.ai/learnai/" rel="noopener ugc nofollow" target="_blank">看这个</a>！</li></ul><h2 id="5e9f" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">参考</h2><ul class=""><li id="b512" class="nr ns it kx b ky nt lb nu mn nv mr nw mv nx lq ny nz oa ob bi translated">什么是甘？短视频介绍:<a class="ae lr" href="https://youtu.be/rt-J9YJVvv4" rel="noopener ugc nofollow" target="_blank">https://youtu.be/rt-J9YJVvv4</a></li><li id="b27f" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">Tzaban，r .，Mokady，r .，Gal，r .，Bermano，A.H .和Cohen-Or，d .，2022。实时拼接:基于GAN的真实视频面部编辑。<a class="ae lr" href="https://arxiv.org/abs/2201.08361" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2201.08361</a></li><li id="34ec" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">项目链接:<a class="ae lr" href="https://stitch-time.github.io/" rel="noopener ugc nofollow" target="_blank">https://stitch-time.github.io/</a></li><li id="8e55" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">代号:<a class="ae lr" href="https://github.com/rotemtzaban/STIT" rel="noopener ugc nofollow" target="_blank">https://github.com/rotemtzaban/STIT</a></li></ul></div></div>    
</body>
</html>