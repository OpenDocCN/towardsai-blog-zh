<html>
<head>
<title>How to Spot a Deep Fake in 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年如何识破深度假</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-spot-a-deep-fake-in-2021-3067ebb218fe?source=collection_archive---------3-----------------------#2021-06-11">https://pub.towardsai.net/how-to-spot-a-deep-fake-in-2021-3067ebb218fe?source=collection_archive---------3-----------------------#2021-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6016" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="ef42" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">突破性的美国陆军技术使用人工智能寻找deepfakes。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c3a506012f6086ab5d783ff0bc369946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dto1iLIXDMWzUcxDKnpi4A.png"/></div></div></figure><blockquote class="ld le lf"><p id="5cac" class="lg lh li lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="it">原载于</em><a class="ae md" href="https://www.louisbouchard.ai/spot-deepfakes/" rel="noopener ugc nofollow" target="_blank"><em class="it">louisbouchard . ai</em></a><em class="it">，前两天看了我的博客</em><a class="ae md" href="https://www.louisbouchard.ai/spot-deepfakes/" rel="noopener ugc nofollow" target="_blank"><em class="it"/></a><em class="it">！</em></p></blockquote><p id="1991" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">虽然它们似乎一直都在那里，但第一个现实的deepfake直到2017年才出现。它从有史以来第一个自动生成的相似的假图像发展到今天的带有声音的视频中的一模一样的某人的复制品。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/eb19a9180b05fb8f6864ddef884b320d.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/0*yWnoLNQSg_scPZBg.gif"/></div></figure><p id="3773" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">事实是，我们再也看不出真正的视频或图片与假的之间的区别了。我们如何区分什么是真实的，什么不是？如果一个人工智能可以完全生成音频文件或视频文件，它们如何在法庭上用作证据？那么，这篇新论文可能会提供这些问题的答案。这里的答案可能再次是人工智能的使用。“当我看到它时，我会相信它”这句话可能很快就会变成“当人工智能告诉我相信它时，我会相信它……”我会假设你们都看过deepfakes，并对它们有所了解。这对于本文来说已经足够了。</p><p id="0435" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">关于它们是如何产生的更多信息，我邀请你观看我制作的解释deepfakes的视频，因为这个视频将重点介绍如何识别它们。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="188e" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">更准确地说，我将报道美国DEVCOM陆军研究实验室的一篇新论文，题为“DEFAKEHOP:一种轻型高性能DEEPFAKE探测器”</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mk"><img src="../Images/91aa30dfcc550dbe42bec3a1043803f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BX0CCFORbcm3XqaO.PNG"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk translated">来自DeepFakeHop模型的结果。<a class="ae md" href="https://arxiv.org/abs/2103.06929" rel="noopener ugc nofollow" target="_blank">陈，洪硕等，(2021) </a></figcaption></figure><p id="240d" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">的确，它们可以在所有数据集中以超过90%的准确率检测deepfakes，甚至在一些基准数据集上达到100%的准确率。更不可思议的是他们探测模型的规模。正如您所看到的，这个DeFakeHop模型只有4万个参数，而其他产生更差精度的技术有大约2000万个参数！这意味着他们的模型要小500倍，同时性能优于之前的最先进技术。这使得模型可以在你的手机上快速运行，并允许你在任何地方检测深度假货。</p><p id="cfca" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">你可能认为你可以区分一张真实的照片或一张伪造的照片，但是如果你记得几周前我分享的研究，它清楚地表明大约50%的参与者失败了。这基本上是对一张照片是不是假的随机猜测。</p><p id="7a95" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">麻省理工学院有一个网站可以测试你识别假货的能力。我自己试过，我可以说做起来很有趣。有音频文件，视频，图片等。链接在下面的描述中。如果你尝试它，请让我知道你做得有多好！如果你知道任何其他有趣的应用程序来测试自己或帮助我们尽最大努力发现deepfakes的研究，请在评论中链接它们。我很想试试它们！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi mp"><img src="../Images/49c13f6358413257df7469df3558f2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YlUl1ws59hoyrpUq.png"/></div></a></figure><p id="f446" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">现在，如果我们回到纸上能够比我们更好地检测它们，问题是:这个微小的机器学习模型如何能够实现这一点，而人类却不能？</p><p id="405f" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">DeepFakeHop分四步工作。</p><h1 id="e262" class="mq mr it bd ms mt mu mv mw mx my mz na ki nb kj nc kl nd km ne ko nf kp ng nh bi translated">第一步:</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ni"><img src="../Images/2ee8b3b44f1c496b3eaa3699c340f3a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LaK3ZwcaY2cvWdJR.PNG"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk translated">DeepFakeHop模型的第一步。【陈，洪硕等，(2021) </figcaption></figure><p id="c452" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">首先，他们使用另一个模型从每个视频帧中提取68个不同的面部标志。提取这68个点是为了了解人脸的位置，对其进行重新居中、定向和调整大小，使它们更加一致，然后从图像中提取人脸的特定部分。这些是我们将发送给网络的图像的“补丁”,包含特定的个人面部特征，如眼睛、嘴、鼻子。这是使用另一个名为OpenFace 2.0的模型完成的。它可以实时准确地执行面部标志检测、头部姿态估计、面部动作单元识别和眼睛注视估计。这些都是32乘32的小块，将被一个接一个地发送到实际网络中。这使得该模型超级有效，因为它只处理少数微小的图像，而不是完整的图像。如果你对OpenFace2.0感兴趣，可以在下面的参考资料中找到更多细节。</p><h1 id="9299" class="mq mr it bd ms mt mu mv mw mx my mz na ki nb kj nc kl nd km ne ko nf kp ng nh bi translated">步骤2到4(从左到右，蓝色、绿色、橙色):</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nj"><img src="../Images/4a12cc57f71d465800dc4eeae6864f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jheWXnqz8mP0gn4b.PNG"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk translated">DeepFakeHop模型。【陈，洪硕等，(2021) </figcaption></figure><p id="4bd3" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">更准确地说，补丁被发送到第一个名为Hop-1的PixelhHop++单元，如您所见。蓝色代表第一步。这是一种叫做萨博变换的算法来降维。它将获取32乘32的图像，并将其缩小为图像的缩小版本，但使用多个通道来表示从Saab变换中学习的不同滤波器的响应。您可以将Saab变换视为一个卷积过程，其中使用PCA降维算法找到核，而不需要反向传播来学习这些权重。我将在一分钟后回到PCA降维算法，因为它将在下一阶段重复。这些滤波器经过优化，以代表图像中的不同频率，基本上由不同程度的细节激活。与用反向传播训练的基本卷积相比，Saab变换被证明能很好地抵抗敌对攻击。您还可以在下面的参考资料中找到有关北京汽车股份有限公司转型的更多信息。如果你不习惯卷积的工作方式，我强烈建议你观看我制作的介绍卷积的视频:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="5c16" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">我说过萨博变形对对抗性攻击很有效。当我们通过改变几个像素或添加人类看不到的噪声来改变机器学习模型处理图像的结果，从而“攻击”一幅图像时，这些对抗性攻击就会发生。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nk"><img src="../Images/f4f31ae250cdcab88fc424810cdbd4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gEkZCXWujDT_NssD.jpg"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk translated">对抗性攻击示例。</figcaption></figure><p id="e3b2" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">为了简单起见，我们基本上可以将这个PixelHop++单元视为一个典型的3乘3卷积，因为我们不看训练过程。当然，它的工作方式有点不同，但是它会使解释更加简单，因为这个过程是可比较的。然后，重复“跳跃”步骤三次，以获得具有集中的一般信息和更多通道的越来越小的图像版本。正如我前面说过的，这些通道只是输入图像通过滤镜的输出或响应，滤镜根据图像的细节程度做出不同的反应。每个滤波器使用一个新通道。</p><p id="f41c" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">因此，我们获得了各种结果，这些结果给出了关于图像包含什么的精确信息，但是这些结果越来越小，包含网络中发送的精确图像所特有的更少的空间细节，因此具有关于图像实际包含什么的更一般和有用的信息。最初的几个图像仍然相对较大，从32乘32开始，这是补丁的初始大小，因此包含了所有的细节。然后，它下降到15乘15，最后到7乘7的图像，这意味着我们最终几乎没有空间信息。15乘15的图像看起来就像初始图像的模糊版本，但仍然包含一些空间信息，而7乘7的图像基本上是非常通用和宽泛的图像版本，几乎没有任何空间信息。就像卷积神经网络一样，我们越深入，我们就有越多的通道，这意味着我们有更多的过滤器对不同的刺激做出反应，但它们每个都越小，以5x5大小的图像结束。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/6fb4132f3a748bdd10babd06d10cd375.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UKlma7LMGUSIN5te.gif"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk translated">输入图像的低频和高频版本示例。<a class="ae md" href="https://arxiv.org/abs/2103.06929" rel="noopener ugc nofollow" target="_blank">陈，洪硕等(2021) </a></figcaption></figure><p id="06db" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">允许我们在许多方面有更广阔的视野，即使是较小版本的图像也能保留许多独特的有价值的信息。</p><p id="516c" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">图像变得更小，因为每个PixelHop单元后面都有一个max-pooling步骤。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi gj"><img src="../Images/f6f6c082be9f31c2fc1df4584a2a7227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*58qjP9uAh1_wkWFJ.gif"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk translated">最大池可视化。</figcaption></figure><p id="9088" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">他们只是简单地取每个2乘2像素的正方形的最大值，每一步将图像尺寸缩小四分之一。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/bec1d1f35eac8ad2633802807ef49689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rIHs0pELGm3hFYIt.gif"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk translated">较高与较低的图像尺寸示例，在Hop-2之前和Hop-3之前。【陈，洪硕，等(2021) </figcaption></figure><p id="84f3" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">然后，正如您在上面显示的完整模型中所看到的，来自每个最大池层的输出被发送，以便使用PCA算法进一步降维。绿色的是第三步。PCA算法主要采用当前尺寸，例如，在第一步中为15乘15，并且最小化该尺寸，同时保持输入图像的至少90%的强度。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c2f0c33ff7753724103b913bd13e5066.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G34NuGVtbzPQF7RA.gif"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk translated">PCA算法的一个简单例子。</figcaption></figure><p id="68f5" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">这里有一个非常简单的PCA如何降维的例子，猫和狗的二维点在一条线上降维为一维，让我们可以添加一个阈值，轻松构建分类器。每一跳分别为我们提供每通道45、30和5个参数，而不是具有15×15、7×7和3×3大小的图像，这将为我们提供同样顺序的225、49和9个参数。这是一种更紧凑的表示，同时最大限度地提高了它所包含的信息的质量。所有这些步骤都被用来压缩信息，使网络超快。</p><p id="b2c2" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">您可以将此视为在裁剪图像的不同细节级别上挤压所有有用的汁液，以最终决定它是否是伪造的，在决定过程中使用详细和一般信息(橙色中的步骤4)。</p><p id="d66c" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">我很高兴看到对抗这些深度假货的研究也在进步，我很兴奋地看到未来会发生什么。请在评论中告诉我你认为deepfakes的主要后果和关注点。</p><p id="bcbb" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">它会影响法律、政治、公司、名人、普通人吗？嗯，几乎每个人…</p><p id="ff2d" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">让我们进行一次讨论，以分享意识，并传播小心谨慎的信息，不幸的是，我们不能再相信我们所看到的。这是一项不可思议又危险的新技术。</p><p id="767a" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">请不要滥用这项技术，保持道德上的正确。这里的目标是帮助改进这项技术，而不是出于错误的原因使用它。</p><h1 id="3967" class="mq mr it bd ms mt mu mv mw mx my mz na ki nb kj nc kl nd km ne ko nf kp ng nh bi translated">观看视频</h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="c9cf" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">来我们的<a class="ae md" href="https://discord.gg/learnaitogether" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd"> Discord社区与我们聊天:</strong> <strong class="lj jd">一起学习AI</strong></a>和<em class="li">分享你的项目、论文、最佳课程、寻找Kaggle队友等等！</em></p><p id="d748" class="pw-post-body-paragraph lg lh it lj b lk ll kd lm ln lo kg lp me lr ls lt mf lv lw lx mg lz ma mb mc im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae md" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae md" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)，并订阅我的每周人工智能<a class="ae md" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">简讯</strong> </a>！</p><h1 id="5b78" class="mq mr it bd ms mt mu mv mw mx my mz na ki nb kj nc kl nd km ne ko nf kp ng nh bi translated">支持我:</h1><ul class=""><li id="25e4" class="nl nm it lj b lk nn ln no me np mf nq mg nr mc ns nt nu nv bi translated">支持我的最好方式是成为这个网站<strong class="lj jd"> </strong>的成员，或者如果你喜欢视频格式，在<a class="ae md" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"><strong class="lj jd">YouTube</strong></a><strong class="lj jd"/>上订阅我的频道<strong class="lj jd"> </strong>。</li><li id="af89" class="nl nm it lj b lk nw ln nx me ny mf nz mg oa mc ns nt nu nv bi translated">在经济上支持我在<a class="ae md" href="https://www.patreon.com/whatsai" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd"> Patreon </strong> </a> <strong class="lj jd">的工作。</strong></li><li id="56ec" class="nl nm it lj b lk nw ln nx me ny mf nz mg oa mc ns nt nu nv bi translated">跟着我到这里。</li></ul><h1 id="4755" class="mq mr it bd ms mt mu mv mw mx my mz na ki nb kj nc kl nd km ne ko nf kp ng nh bi translated">参考</h1><ul class=""><li id="b392" class="nl nm it lj b lk nn ln no me np mf nq mg nr mc ns nt nu nv bi translated">测试你的deepfake检测能力:<a class="ae md" href="https://detectfakes.media.mit.edu/" rel="noopener ugc nofollow" target="_blank">https://detectfakes.media.mit.edu/</a></li><li id="fa67" class="nl nm it lj b lk nw ln nx me ny mf nz mg oa mc ns nt nu nv bi translated">DeepFakeHop:陈，洪硕，等，(2021)，“DeepFakeHop:一种轻量级高性能Deepfake检测器”ArXiv abs/2103.06929</li><li id="87f7" class="nl nm it lj b lk nw ln nx me ny mf nz mg oa mc ns nt nu nv bi translated">萨博变换:Kuo，C.-C. Jay等人(2019)，“通过前馈设计的可解释卷积神经网络。”j .维斯。Commun。图像表示。</li><li id="3db9" class="nl nm it lj b lk nw ln nx me ny mf nz mg oa mc ns nt nu nv bi translated">OpenFace 2.0: T. Baltrusaitis，a .扎德，Y. C. Lim和L. Morency，“OpenFace 2.0:面部行为分析工具包”，2018年第13届IEEE自动人脸与手势识别国际会议(FG 2018)，2018年，第59–66页，doi: 10.1109/FG.2018.00019</li></ul></div></div>    
</body>
</html>