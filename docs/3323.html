<html>
<head>
<title>Transformers for Multi-Regression — [PART1]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多元回归的变形金刚—[第一部分]</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9?source=collection_archive---------1-----------------------#2022-11-18">https://pub.towardsai.net/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9?source=collection_archive---------1-----------------------#2022-11-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="292b" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">💎作为特征提取器的变压器💎</h1><p id="0335" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我在Kaggle参加的<a class="ae lj" href="https://www.kaggle.com/competitions/feedback-prize-english-language-learning" rel="noopener ugc nofollow" target="_blank"> FB3竞赛</a>激励我写一篇关于我测试过的方法的帖子。再加上我没有找到任何明确的关于如何使用变压器解决多元回归问题的教程，所以我认为分享一下我的工作会很有用。</p><p id="c1d8" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">所有这些工作都在<a class="ae lj" href="https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners" rel="noopener ugc nofollow" target="_blank">我的卡格尔笔记本</a>中继续</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/526717509a83f7e1031b6c35ee3f2eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FJFU2zLC42kdoV6lrLJ2uA.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated"><a class="ae lj" href="https://unsplash.com/photos/AVYo3X6XZYg" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/AVYo3X6XZYg</a></figcaption></figure><h1 id="86f1" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</h1><p id="9c50" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们并不都有福楼拜🧡的才华，柏格森🤎的清晰，普鲁斯特的天才💙，也没有<strong class="kn ir">茨威格</strong>的风格和手腕💜，也不是伏尔泰<strong class="kn ir">的本事</strong>💚，也不是叔本华<strong class="kn ir">的先知先觉</strong>💖…</p><p id="5038" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">这份清单远非详尽无遗，感谢上帝，有天才的作家和哲学家允许我们暂时逃离这个唯物主义的世界。</p><p id="e9f7" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">但就我们普通人而言，不管是不是文学，我们至少可以希望尽力尊重语言的规则，写得“正确”。老师们帮助我们学习语言的基础知识，那么为什么不呢？反过来，我们也会帮助他们，利用我们的知识，节省时间来修改学生的作文，帮助他们区分每个学生的优缺点，并更好地调整他们的教学方法，以适应每个学生的水平。</p><p id="867a" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在这场比赛中，我们被要求使用8-12年级英语学习者写的预先评分的议论文，根据六个分析指标创建一个有效的模型:<strong class="kn ir">衔接、句法、词汇、短语、语法</strong>和<strong class="kn ir">惯例</strong>。分数范围从1.0到5.0，增量为0.5。</p><p id="650e" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我们将描述如何使用拥抱脸模型来解决这种类型的问题，我选择了<code class="fe mf mg mh mi b">deberta-v3-base</code>模型(这里的<a class="ae lj" href="https://huggingface.co/microsoft/deberta-v3-base?text=The+goal+of+life+is+%5BMASK%5D." rel="noopener ugc nofollow" target="_blank">是相应的模型卡)，我将展示我们如何以两种有效的方式使用它:</a></p><p id="7040" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">🤙<strong class="kn ir">特征提取</strong>:我们使用隐藏状态作为特征，只在其上训练一个分类器，而不修改预先训练好的模型。对于本节<a class="ae lj" href="https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/351577" rel="noopener ugc nofollow" target="_blank">,@ cdeotte</a>提出了这种方法的一个绝妙用法，即使用多个非微调的变压器嵌入，然后将它们连接起来并训练一个独立的分类器:我强烈邀请您查看相关的<a class="ae lj" href="https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/351577" rel="noopener ugc nofollow" target="_blank">讨论</a>和<a class="ae lj" href="https://www.kaggle.com/code/cdeotte/rapids-svr-cv-0-450-lb-0-44x" rel="noopener ugc nofollow" target="_blank">笔记本</a></p><p id="5d91" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">🤙<strong class="kn ir">微调</strong>:我们对整个模型进行端到端的训练，这也意味着对预训练模型的参数进行更新。这种方法将在<a class="ae lj" href="https://zghrib.medium.com/transformers-for-multi-regression-task-part2-fine-tuning-2683ef134d1c" rel="noopener">后面的帖子</a>中讨论</p><p id="0f53" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在这一部分，我们将一步一步地介绍第一种方法:👐基于编码器的变压器简介</p><p id="1151" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">这个想法是使用基于BERT的模型，这些模型经过预先训练，可以预测文本的屏蔽元素，以及一个自定义的分类器:工作流程如下:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mj"><img src="../Images/7e9228951e01da8e430e43524ab9a16a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4pACupZc_5f12t7LqFvPuQ.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">带有基于编码器的<br/>变压器的分类器/回归器头</figcaption></figure><p id="6c63" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated"><strong class="kn ir"> 1。生成令牌编码:</strong> <br/>首先，令牌化器生成一个名为<strong class="kn ir">令牌编码</strong>的热编码:每个向量的维数等于令牌化器词汇表<code class="fe mf mg mh mi b">[batch_size, vocab_size]</code>。HuggingFace的<code class="fe mf mg mh mi b">AutoTokenizer</code>类将自动加载对应于检查点名称的记号赋予器(对于这个笔记本，我们将使用<code class="fe mf mg mh mi b">deberta-v3-base</code>)，记号赋予器将生成一个字典，包括:</p><ul class=""><li id="df0a" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">input_ids</code>:句子中每个标记对应的索引</li><li id="d823" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">token_type_ids</code>:当有多个序列时，标识一个令牌属于哪个序列</li><li id="c7ef" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">attention_mask</code>:从真实记号中识别填充元素</li></ul><p id="656e" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">然后，模型将采用令牌编码，并如下进行:<br/> <br/> <strong class="kn ir"> 2 .生成嵌入:</strong> <br/>该模型将令牌编码转换为<strong class="kn ir">密集嵌入</strong>。与令牌编码不同，嵌入是<strong class="kn ir">密集的</strong> =非零值。令牌编码是由令牌化器生成的。<br/> - &gt;我们用模型的最大上下文尺寸得到一个维度为<code class="fe mf mg mh mi b">[batch_size, max_len]</code>的张量</p><p id="4f3a" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated"><strong class="kn ir"> 3。生成隐藏状态:</strong> <br/>模型通过编码器堆栈提供嵌入，为每个令牌输入返回隐藏状态。我们获得一个最终张量<code class="fe mf mg mh mi b">[batch_size, max_len, hidden_states_dim]</code> <br/>我们加载一个<code class="fe mf mg mh mi b">AutoModel</code>对象来初始化一个具有所有检查点权重的模型(在我们的例子中是<code class="fe mf mg mh mi b">microsoft/deberta-v3-base</code></p><p id="100f" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">现在让我们看看如何准备我们的数据集，以便由转换器进行处理:</p><h1 id="eab9" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🪵准备数据集</h1><p id="0099" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们将通过小批量进行，为此，我们将使用<code class="fe mf mg mh mi b">torch.utils.data.DataLoader</code>和<code class="fe mf mg mh mi b">torch.utils.data.Dataset</code>(您可以在此处查看参考<a class="ae lj" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="noopener ugc nofollow" target="_blank"/>)。<code class="fe mf mg mh mi b">Dataset</code>允许返回带有相应标签的数据集样本。<code class="fe mf mg mh mi b">DataLoader</code>在数据集周围包装了一个iterable以方便访问样本，提供了许多实用程序，例如在每个时期重新排列数据以减少模型过度拟合，或者允许使用多重处理来加速数据检索。</p><p id="c6a3" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">为了开发我们的定制<code class="fe mf mg mh mi b">Dataset</code>，我们必须覆盖<code class="fe mf mg mh mi b">__init__</code>、<code class="fe mf mg mh mi b">__len__</code>和<code class="fe mf mg mh mi b">__getitem__</code>函数。<br/>最重要的函数是<code class="fe mf mg mh mi b">__getitem__</code>:它从数据集中给定索引idx处返回一个样本。函数的输出格式必须符合<strong class="kn ir">模型的预期格式</strong>:</p><ul class=""><li id="869c" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">input_ids</code>:提供给模型的令牌id列表。</li><li id="cba5" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">attention_mask</code>:指定模型应该关注哪些令牌的索引列表</li><li id="8d57" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><code class="fe mf mg mh mi b">labels</code>:在我们的例子中，我们处理一个<strong class="kn ir">多类回归</strong>问题，标签是<em class="my">六个分析分数</em>的向量。</li></ul><p id="992a" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">对于训练方法，我们将参考不可否认的经典交叉验证方案</p><p id="bfea" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">🍕<strong class="kn ir">多标签数据分层:</strong></p><p id="8034" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在我们的数据科学家社区中，有证据表明如何分割交叉验证折叠对模型性能有直接影响。<br/>通常，对于单类问题，褶皱与单个目标一起分层(离散目标的<em class="my">类分布或连续目标的面元分布</em>)。</p><blockquote class="mz na nb"><p id="ee58" class="kl km my kn b ko lk kq kr ks ll ku kv nc lm ky kz nd ln lc ld ne lo lg lh li ij bi translated">但是，在多类问题的情况下，我们该怎么做呢🤔？</p></blockquote><p id="0014" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">嗯，已经开展了许多工作来处理这个问题，例如:</p><ul class=""><li id="ff85" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated">2011 : <a class="ae lj" href="http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf" rel="noopener ugc nofollow" target="_blank"> Sechidis —关于多标签数据的分层</a></li><li id="b51e" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated">2017:<a class="ae lj" href="http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html" rel="noopener ugc nofollow" target="_blank">szymński——第一届不平衡领域学习国际研讨会论文集</a></li></ul><p id="aa39" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated"><a class="ae lj" href="https://www.slideshare.net/tsoumakas/on-the-stratification-of-multilabel-data" rel="noopener ugc nofollow" target="_blank">这里</a>是一个温和的演示，解释算法。<br/>我们将使用采用<a class="ae lj" href="https://github.com/trent-b/iterative-stratification" rel="noopener ugc nofollow" target="_blank">迭代分层</a>实施的迭代方法算法:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="06d2" class="nj jo iq mi b be nk nl l nm nn">import pandas as pd<br/>from iterstrat.ml_stratifiers import MultilabelStratifiedKFoldtrain = pd.read_csv(PATH_TO_TRAIN)<br/>print("TRAIN SHAPE", train.shape)<br/>test = pd.read_csv(PATH_TO_TEST)<br/>print("TEST SHAPE", test.shape)<br/>label_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']<br/>cv = MultilabelStratifiedKFold(<br/>          n_splits=N_FOLDS, <br/>          shuffle=True, <br/>          random_state=SEED<br/>          )<br/>train = train.reset_index(drop=True)<br/>for fold, ( _, val_idx) in enumerate(cv.split(X=train, y=train[label_cols])):<br/>    train.loc[val_idx , "fold"] = int(fold)<br/>    <br/>train["fold"] = train["fold"].astype(int)</span></pre><p id="76df" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">现在让我们像前面描述的那样实现数据集迭代器:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="23d9" class="nj jo iq mi b be nk nl l nm nn"># lets define the batch genetator<br/>class CustomIterator(torch.utils.data.Dataset):<br/>    def __init__(self, df, tokenizer, labels=CONFIG['label_cols'], is_train=True):<br/>        self.df = df<br/>        self.tokenizer = tokenizer<br/>        self.max_seq_length = CONFIG["max_length"]# tokenizer.model_max_length<br/>        self.labels = labels<br/>        self.is_train = is_train<br/>        <br/>    def __getitem__(self,idx):<br/>        tokens = self.tokenizer(<br/>                    self.df.loc[idx, 'full_text'],#.to_list(),<br/>                    add_special_tokens=True,<br/>                    padding='max_length',<br/>                    max_length=self.max_seq_length,<br/>                    truncation=True,<br/>                    return_tensors='pt',<br/>                    return_attention_mask=True<br/>                )     <br/>        res = {<br/>            'input_ids': tokens['input_ids'].to(CONFIG.get('device')).squeeze(),<br/>            'attention_mask': tokens['attention_mask'].to(CONFIG.get('device')).squeeze()<br/>        }<br/>        <br/>        if self.is_train:<br/>            res["labels"] = torch.tensor(<br/>                self.df.loc[idx, self.labels].to_list(), <br/>            ).to(CONFIG.get('device')) <br/>            <br/>        return res<br/>    <br/>    def __len__(self):<br/>        return len(self.df)</span></pre><p id="9609" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">这个自定义的<code class="fe mf mg mh mi b">Dataset</code>将在以后被用于微调转换器或者仅仅作为一个特征提取器。</p><p id="c514" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">PS:我添加了<code class="fe mf mg mh mi b">is_train</code>参数来决定是否返回“标签”字段(只有训练数据集包含标签字段)</p><h1 id="91a5" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">⛏Transformers是Extractors⛏的特色</h1><p id="ed14" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">通过这种方法，编码器权重被<strong class="kn ir">冻结</strong>，隐藏状态被多元回归器用作<strong class="kn ir">独立特征</strong>。<br/>由于隐藏状态只计算一次，如果我们<strong class="kn ir">不处理GPU</strong>，这种方法是最好的选择:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mj"><img src="../Images/2c107b051a73f5501363a3daef0e9022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XvFWjL7q6_eet1-ReGVgiw.jpeg"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">编码器变压器作为特征提取器:只有头部是可训练的，所有的变压器层被冻结</figcaption></figure><p id="5ad4" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">留给我们的唯一一把自由之斧是如何将隐藏状态张量<code class="fe mf mg mh mi b">[batch_size, max_len, hidden_states_dim]</code>简化为一个单一的向量表示:我强烈建议你参考无价的<a class="ae lj" href="https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently" rel="noopener ugc nofollow" target="_blank"> @rhtsingh notebook </a>，它“详尽地”列举了隐藏状态编码的不同方式<strong class="kn ir">、</strong>。我测试了以下技巧:</p><p id="575d" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">🤙<strong class="kn ir"> CLS嵌入:</strong> BERT引入一个<strong class="kn ir">【CLS】</strong>token标签，站在每句话的第一个位置，捕捉整个句子的上下文。cls嵌入简单地包括选择每个隐藏状态向量的第一个元素，从而得到<code class="fe mf mg mh mi b">[batch_size, 1, hidden_states_dim]</code>向量</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="dfb2" class="nj jo iq mi b be nk nl l nm nn">import torch<br/>import torch.nn as nn<br/>import transformers<br/>from transformers import (<br/>    AutoModel, AutoConfig, <br/>    AutoTokenizer, logging,<br/>    AdamW, get_linear_schedule_with_warmup,<br/>    DataCollatorWithPadding,<br/>    Trainer, TrainingArguments<br/>)<br/>from transformers.modeling_outputs import SequenceClassifierOutput# https://github.com/UKPLab/sentence-transformers/blob/0422a5e07a5a998948721dea435235b342a9f610/sentence_transformers/models/Pooling.py<br/># https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently<br/>def cls_embedding(outputs):<br/>    """Since Transformers are contextual model, <br/>    the idea is [CLS] token would have captured the entire context <br/>    and would be sufficient for simple downstream tasks such as classification<br/>    Select the first token for each hidden state<br/>    <br/>    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]<br/>    @return:  tensor of dimensions = [batch_size, hidden_states_dim]<br/>    """<br/>    return outputs.last_hidden_state[:, 0, :].to(CONFIG.get('device'))</span></pre><p id="e4b7" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">🤙<strong class="kn ir">意味着汇集</strong>:我们将考虑每个隐藏状态维度的<code class="fe mf mg mh mi b">max_len </code>维度嵌入的平均值，而不是选择第一个元素:我们获得一个<code class="fe mf mg mh mi b">[batch_size, 1, hidden_states_dim]</code>的张量，或者仅仅是未排序的形式:<code class="fe mf mg mh mi b">[batch_size, hidden_states_dim]</code></p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="2caf" class="nj jo iq mi b be nk nl l nm nn">def mean_pooling(inputs, outputs):<br/>    """<br/>    For each hidden_state, average along with max_len embeddings, <br/>    but we will condider only the highlighted tokens by the attention mask<br/>    <br/>    @param inputs: = the tokenizer output = the model input : a dict must contain at least the attention_mask field<br/>    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]<br/>    @return:  tensor of dimensions = [batch_size, hidden_states_dim]<br/>    """<br/>    input_mask_expanded = inputs['attention_mask'].squeeze().unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()<br/>    sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)<br/>    sum_mask = input_mask_expanded.sum(1)<br/>    sum_mask = torch.clamp(sum_mask, min=1e-9)<br/>    mean_embeddings = sum_embeddings / sum_mask<br/>    return mean_embeddings</span></pre><p id="1118" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">🤙<strong class="kn ir">最大汇集</strong>:为了得到最大汇集，我们将在每个隐藏状态维度上的<code class="fe mf mg mh mi b">max_len</code>嵌入中取最大值，结果是一个<code class="fe mf mg mh mi b">[batch_size, hidden_states_dim]</code>维度的张量</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="25c1" class="nj jo iq mi b be nk nl l nm nn">def max_pooling(inputs, outputs):<br/>    """<br/>    For each hidden_state, get the max element along with max_len embeddings,<br/>    considering only the non masked element difined by the attention mask computed by the tokenizer<br/>    <br/>    @param inputs: = the tokenizer output = the model input : a dict must contain at least the attention_mask field<br/>    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]<br/>    @return:  tensor of dimensions = [batch_size, hidden_states_dim]<br/>    <br/>    """<br/>    last_hidden_state = outputs.last_hidden_state<br/>    input_mask_expanded = inputs['attention_mask'].squeeze().unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()<br/>    last_hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value<br/>    max_embeddings = torch.max(last_hidden_state, 1).values<br/>    return max_embeddings</span></pre><p id="9ffc" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">🤙<strong class="kn ir">平均最大池化:</strong>我们应用平均池化和最大池化，然后连接两者以获得<code class="fe mf mg mh mi b">[batch_size, 2*hidden_states_dim]</code>维度张量</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="269b" class="nj jo iq mi b be nk nl l nm nn">def mean_max_pooling(inputs, outputs):<br/>    """<br/>    Apply mean and max-pooling embeddings, then we concatenate the two onto a single final representation<br/>    <br/>    @param outputs: the model output dim = [batch_size, max_len, hidden_states_dim]<br/>    @return:  tensor of dimensions = [batch_size, 2*hidden_states_dim]<br/>    """<br/>    mean_pooling_embeddings = mean_pooling(inputs, outputs)<br/>    max_pooling_embeddings = max_pooling(inputs, outputs)<br/>    mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)<br/>    return mean_max_embeddings</span></pre><p id="8d0d" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">下面是获取所有嵌入的代码示例:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="8b06" class="nj jo iq mi b be nk nl l nm nn"><br/><br/>def get_embedding(dataloader, model, n):<br/>    """<br/>    Run the model to predict hidden states then apply all the transformations implemented above<br/>    <br/>    @param dataloader: a torch.utils.data.DataLoader the iterator along with the custom torch.utils.data.Dataset<br/>    @param model : the huggingface AutoModel that generates the hidden states<br/>    """<br/>    embeddings = {}<br/>    model = model.to(CONFIG.get('device'))<br/>    for batch in tqdm_notebook(dataloader):<br/>        with torch.no_grad():<br/>            # please note here that the labels fileds is not necessary <br/>            # since we are not going to fine tune the model but just get the vectors output<br/>            outputs = model(<br/>                input_ids=batch['input_ids'].squeeze(),<br/>                attention_mask=batch['attention_mask'].squeeze()<br/>            )<br/>        for embed_name, embed_func in zip(['cls_embeddings', "mean_pooling", "max_pooling", "mean_max_pooling"], <br/>                                          [cls_embedding, mean_pooling, max_pooling, mean_max_pooling]):<br/>            if embed_name == 'cls_embeddings':<br/>                embed = embed_func(outputs)<br/>            else:<br/>                embed = embed_func(batch, outputs)<br/>            embeddings[embed_name] = torch.cat(<br/>                (<br/>                    embeddings.get(embed_name, torch.empty(embed.size()).to(CONFIG.get('device'))), <br/>                    embed<br/>                ),<br/>                0<br/>            )<br/>    threshold = min(n,CONFIG.get('train_batch_size'))<br/>    for key in embeddings:<br/>        embeddings[key] = embeddings[key][threshold:,:]<br/>    return embeddings</span></pre><p id="6a4b" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">现在让我们看看如何为训练和测试数据集生成嵌入:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="969d" class="nj jo iq mi b be nk nl l nm nn">model = AutoModel.from_pretrained(CONFIG["model_name"], config=config)<br/># TRAIN #<br/>df_iter = CustomIterator(train, tokenizer)<br/>train_dataloader = torch.utils.data.DataLoader(<br/>    df_iter, <br/>    batch_size=CONFIG["train_batch_size"],<br/>    shuffle=False<br/>)<br/>embeddings = get_embedding(train_dataloader, model, n=len(train))<br/># TEST #<br/>df_iter = CustomIterator(test, tokenizer, is_train=False)<br/>test_dataloader = torch.utils.data.DataLoader(<br/>    df_iter, <br/>    batch_size=CONFIG["train_batch_size"],<br/>    shuffle=False<br/>)<br/>test_embeddings = get_embedding(test_dataloader, model, n=len(test))</span></pre><h1 id="edc3" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🔍隐藏状态可视化:</h1><p id="959e" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在训练分类器以获得2D可视化之前，让我们看一下嵌入。为了简单起见，我们将只对<code class="fe mf mg mh mi b">cls_embeddings</code>应用它，对于其他类型的嵌入也是一样的。</p><p id="49a1" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我们必须将隐藏状态减少到2D，许多有效的模型可以用来减少嵌入的维数:<a class="ae lj" href="https://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> UMAP </a>，<a class="ae lj" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">主成分分析</a>，<a class="ae lj" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank"> T-SNE </a></p><p id="4f80" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我们将使用PCA算法:<br/> 1。<strong class="kn ir">数据标准化</strong>:使用scikit learn <br/> 2的<code class="fe mf mg mh mi b">StandardScaler</code>标准化嵌入。<strong class="kn ir"> 2D降维</strong>:拟合嵌入的PCA模型，提取前两个分量<br/> 3。<strong class="kn ir"> Hexbin可视化</strong>:对于每个目标类，我们将可视化每个分数的bin(从1到5，步长= 0.5)</p><p id="5736" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">我们来看看词汇课剧情:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi no"><img src="../Images/a1aa560d2f3a93459232c0de2a595dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPabZvtKdMa6l42L1lz3kA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">词汇得分分布的Hexbin图，以及cls _ embeddings的前两个组成部分</figcaption></figure><p id="c730" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">PS:这个可视化部分技术很受t <a class="ae lj" href="https://github.com/nlp-with-transformers/notebooks/blob/5dce9357463435c7208bf5e1a4cc5be6e49e0a40/02_classification.ipynb" rel="noopener ugc nofollow" target="_blank">的HuggingFace NLP GitHub例子</a>的启发</p><p id="8729" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">从这个图中可以看出一些模式:对大多数人来说，极端分数是分开的，2.5分似乎分散在所有地方，而对其他人来说，有明显的重叠。</p><p id="7082" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">PS:不要忘记，这些嵌入是由一个模型<strong class="kn ir">生成的，该模型预先训练用于预测句子中的屏蔽词</strong>，而不是对分数进行分类。</p><h1 id="73c4" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">⚙多元回归头培训</h1><p id="dd8a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们在我们的嵌入上训练一个多元回归模型:我选择了一个基于梯度推进的模型:<strong class="kn ir"> Xgboost </strong></p><p id="9553" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在我们的例子中:多类回归，我们将使用scikit-learn的<code class="fe mf mg mh mi b">MultiOutputRegressor</code>估计器。我会让你从<a class="ae lj" href="https://www.kaggle.com/code/swimmy/stacking-xgboost-lgbm-ridge-catboost" rel="noopener ugc nofollow" target="_blank"> @SWIMMY优秀笔记本</a>中查看不同的基于树的模型+用一个元模型堆叠的原始和进一步的实现。</p><p id="7af0" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">为了查看哪个池具有最佳的分离表示，我们将对每个池嵌入使用交叉验证评估。全局度量包括平均6个目标列的RMSE:这个度量称为<strong class="kn ir"> MCRMSE </strong>(平均列均方根误差)</p><p id="1415" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">让我们定义评估指标:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="2dda" class="nj jo iq mi b be nk nl l nm nn">def comp_score(y_true,y_pred):<br/>    rmse_scores = []<br/>    for i in range(len(CONFIG['label_cols'])):<br/>        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))<br/>    return np.mean(rmse_scores)</span></pre><p id="cc64" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">现在开始简历培训:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="0fba" class="nj jo iq mi b be nk nl l nm nn">import joblib<br/>y_true = train[CONFIG['label_cols']].values<br/>cv_rmse = pd.DataFrame(0, index=range(N_FOLDS), columns=embeddings.keys())<br/><br/>oof_pred = {<br/>        emb_type : np.zeros((len(train), len(label_cols)))<br/>        for emb_type in embeddings<br/>    }<br/><br/>for emb_type, emb in embeddings.items(): <br/>    print(f"CV for {emb_type}")<br/>    emb = normalize(<br/>        emb, <br/>        p=1.0, <br/>        dim = 1<br/>    ).cpu()<br/><br/>    for fold, val_fold in train.groupby('fold'):<br/>        print(f"*** FOLD == {fold} **")<br/>        x_train, x_val = np.delete(emb, val_fold.index, axis=0), emb[val_fold.index]<br/>        y_train, y_val = np.delete(y_true, val_fold.index, axis=0), y_true[val_fold.index]<br/>        xgb_estimator = xgb.XGBRegressor(<br/>                n_estimators=500, random_state=0, <br/>                objective='reg:squarederror')<br/>        # create MultiOutputClassifier instance with XGBoost model inside<br/>        xgb_model = MultiOutputRegressor(xgb_estimator, n_jobs=2)<br/>        # model4 = XGBClassifier(early_stopping_rounds=10)<br/>        xgb_model.fit(x_train, y_train)<br/>        oof_pred[emb_type][val_fold.index] = xgb_model.predict(x_val)<br/>        for i, col in enumerate(CONFIG['label_cols']):<br/>            rmse_fold = np.sqrt(mean_squared_error(y_val[:,i], oof_pred[emb_type][val_fold.index,i]))<br/>            print(f'{col} RMSE = {rmse_fold:.3f}')<br/>            <br/>        cv_rmse.loc[fold, emb_type] = comp_score(y_val, oof_pred[emb_type][val_fold.index])<br/>        print(f'COMP METRIC = {cv_rmse.loc[fold, emb_type]:.3f}')        <br/>        joblib.dump(xgb_model, f'xgb_{emb_type}_{fold}.pkl')</span></pre><p id="1d8d" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在CV训练期间，对于每种嵌入类型，在每次迭代中保持一个折叠分开，并且在剩余的折叠上训练模型。然后我们预测看不见的褶皱。因此，我们获得了非折叠预测(OOF预测)，这意味着每个预测都是在看不见的数据上完成的。</p><p id="ee2f" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">然后，我们评估每个类上的RMSE和每个嵌入类型的OOF预测上的全局MCRMSE:我们获得以下性能:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5763a3b61da5c26e39161d4cdfe2984c.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*FsapaGs_qN3p7WhVpdW26g.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk translated">按嵌入类型划分的OOF XGBoost性能</figcaption></figure><ul class=""><li id="a9ef" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated">看起来<strong class="kn ir">平均最大池</strong>提供了最好的性能，但是<strong class="kn ir">平均池</strong>非常接近。由于平均最大池的容量是平均池的两倍，我们将选择平均编码来微调转换器</li><li id="340a" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated">我们语料库中效率最低的池方法是<strong class="kn ir">最大池</strong></li><li id="77a7" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated">对于分析度量的所有表示，词汇是最容易估计的目标(最低的RMSE)，而内聚是最难估计的目标(最高的RMSE)</li></ul><p id="1f5f" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">为了推断新的预测，我的一个朋友<a class="ae lj" href="https://www.kaggle.com/mathurinach" rel="noopener ugc nofollow" target="_blank"> Mathurin Aché </a>，他也是一个伟大的数据科学家和Kaggle大师，教了我两个方法:</p><ol class=""><li id="996a" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li nq mq mr ms bi translated">使用CV模型来预测新的数据:对预测进行平均可以<strong class="kn ir">减少方差</strong>但是会增加每个样本时间的预测，如果我们必须部署这种方法，我们必须保存所有的CV模型</li><li id="dc00" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li nq mq mr ms bi translated">对整个训练数据训练一次保留的模型，然后预测新的预测，这可能会稍微增加泛化误差a，但在部署的情况下(预测时间和要监视的单个模型)建议这样做</li></ol><p id="945b" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">只要我们处于Kaggle竞争环境中，我们就选择第一种方法，使用均值-最大值池:</p><pre class="lq lr ls lt gt nf mi ng bn nh ni bi"><span id="c7b0" class="nj jo iq mi b be nk nl l nm nn">import glob<br/># init output with zeros<br/>xgb_infer = np.zeros((len(test), len(label_cols)))<br/>for model_path in glob.glob("./xgb_mean_max_pooling_*.pkl"):<br/>    print(f"load {model_path} model")<br/>    xgb_model = joblib.load(model_path)<br/>    emb = normalize(<br/>        test_embeddings["mean_max_pooling"], <br/>        p=1.0, <br/>        dim = 1<br/>    ).cpu()<br/>    # add fold model prediction<br/>    xgb_infer = np.add(xgb_infer, xgb_model.predict(emb))<br/># devide by the number of folds<br/>xgb_infer = xgb_infer*(1/N_FOLDS)</span></pre><h1 id="4ac2" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🙏学分:</h1><p id="a4ba" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我在这一部分的工作受到了这些优秀资源的启发，请不要犹豫去查阅它们:</p><ul class=""><li id="cb5f" class="mk ml iq kn b ko lk ks ll kw mm la mn le mo li mp mq mr ms bi translated"><a class="ae lj" href="https://www.kaggle.com/code/swimmy/stacking-xgboost-lgbm-ridge-catboost" rel="noopener ugc nofollow" target="_blank"/></li><li id="5952" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><a class="ae lj" href="https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @rhtsingh </strong>笔记本</a>:探索变压器表象的不同方式</li><li id="1105" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><a class="ae lj" href="https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @Y.NAKAMA </strong>笔记本</a>:损失函数与多标签分层交叉验证</li><li id="d5ca" class="mk ml iq kn b ko mt ks mu kw mv la mw le mx li mp mq mr ms bi translated"><a class="ae lj" href="https://github.com/nlp-with-transformers/notebooks/blob/5dce9357463435c7208bf5e1a4cc5be6e49e0a40/02_classification.ipynb" rel="noopener ugc nofollow" target="_blank"> Huggingface NLP GitHub </a>:降维及变压器嵌入可视化</li></ul><h1 id="1d5d" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论:</h1><p id="2927" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">感谢您阅读我的帖子🤗希望有用！提醒一下，我所有的作品都可以在这里找到<a class="ae lj" href="https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners" rel="noopener ugc nofollow" target="_blank">🎁。</a></p><p id="4c56" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在这篇文章中，我们看到了如何使用一个预先训练的转换器来提取上下文捕获嵌入，并使用它来训练一个多元回归器(在我们的例子中是Xgboost)来对学生论文的分析指标进行建模。</p><p id="7c31" class="pw-post-body-paragraph kl km iq kn b ko lk kq kr ks ll ku kv kw lm ky kz la ln lc ld le lo lg lh li ij bi translated">在下一部分，我将解决同样的问题，但这次是通过<strong class="kn ir">微调</strong>转换器，并更新其所有的编码器堆栈。此外，我将向您展示如何使用<a class="ae lj" href="https://wandb.ai/site" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir">权重&amp;偏差</strong> </a>大平台来跟踪模型性能并创建模型工件。多元回归任务的转换器</p></div></div>    
</body>
</html>