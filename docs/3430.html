<html>
<head>
<title>Run Very Large Language Models on Your Computer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在您的计算机上运行非常大的语言模型</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=collection_archive---------0-----------------------#2022-12-22">https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=collection_archive---------0-----------------------#2022-12-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2ffa" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用PyTorch和拥抱脸的设备_地图</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b73e27989f976147d922aa99307c6eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ty9tiLcWTcM1FGXLzKJXHQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com/photos/nvidia-graphic-card-bitcoin-gpu-5264921/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><p id="f3e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几乎每个月都会公开发布新的大型语言模型。它们变得越来越好，越来越大。</p><p id="31fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能认为这些模型只能在大型集群或云中运行。</p><p id="50ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，事实并非如此。PyTorch的最新版本提出了几个机制，由于拥抱脸加速包，使得在标准计算机上使用大型语言模型相对容易，并且不需要太多工程。</p><p id="af21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将介绍一种在您自己的计算机或Google Colab的免费实例上使用大型语言模型的简单方法，使用拥抱面部变形器和加速包。</p><p id="73a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于本文的目的，我将使用META AI 发布的OPT模型的<a class="ae ky" href="https://arxiv.org/abs/2205.01068" rel="noopener ugc nofollow" target="_blank"> 6.7B版本。如果您的硬盘上有足够的可用空间，您可以尝试更大的版本。</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="5ac2" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">要求和环境</h1><p id="a6a5" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">如果你想在你自己的电脑上重现我的实验，我推荐你用Python 3.9创建一个conda ( <a class="ae ky" href="https://www.anaconda.com/" rel="noopener ugc nofollow" target="_blank"> Anaconda </a>)环境。</p><p id="91da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下命令创建一个名为“device_map”的环境并激活它。</p><pre class="kj kk kl km gt mz na nb bn nc nd bi"><span id="f6fe" class="ne md it na b be nf ng l nh ni">conda create -n device_map python=3.9<br/>conda activate device_map</span></pre><p id="4d41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，安装以下软件包。<em class="nj">请注意，它也可能适用于更高或更低版本的CUDA。</em></p><pre class="kj kk kl km gt mz na nb bn nc nd bi"><span id="aba1" class="ne md it na b be nf ng l nh ni">conda install pytorch pytorch-cuda=11.6 -c pytorch -c nvidia<br/>conda install -c conda-forge transformers<br/>conda install -c conda-forge accelerate</span></pre><p id="b0ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至于硬件，我可以在我的nVidia RTX3060 12 Gb和16gb CPU RAM上运行67亿参数模型。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="0c6f" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">被遗忘</h1><p id="27a3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">正如我上面提到的，我将使用抱脸轮毂的<a class="ae ky" href="https://huggingface.co/facebook/opt-6.7b" rel="noopener ugc nofollow" target="_blank"> OPT-6.7B。</a></p><p id="7ffd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我的目标是使用这个模型来制作一个生成语言的应用程序。</p><p id="7e91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">加载和使用带有变压器的模型的标准方法可以这样实现(如变压器文档所建议的<a class="ae ky" href="https://huggingface.co/docs/transformers/model_doc/opt" rel="noopener ugc nofollow" target="_blank">):</a></p><pre class="kj kk kl km gt mz na nb bn nc nd bi"><span id="6b76" class="ne md it na b be nf ng l nh ni">from transformers import GPT2Tokenizer, OPTForCausalLM<br/><br/>#Load the model<br/>model = OPTForCausalLM.from_pretrained("facebook/opt-6.7b")<br/><br/>#Load the tokenizer<br/>tokenizer = GPT2Tokenizer.from_pretrained("facebook/opt-6.7b")<br/><br/>prompt = "Hey, are you consciours? Can you talk to me?"<br/>inputs = tokenizer(prompt, return_tensors="pt")<br/><br/># Generate<br/>generate_ids = model.generate(inputs.input_ids, max_length=30)<br/>tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]</span></pre><p id="e2d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您运行这段代码时，它将首先下载模型(除非您以前使用过它)，该模型被分成9.96 GB和3.36 GB两部分。因此，您至少应该在硬盘上留出20 GB来存放这个模型。</p><p id="4382" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，它将在内存中加载模型…并崩溃。</p><p id="079c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然硬盘上的型号大小为13.4 GB (9.96+3.36)，但它需要扩展并完全加载到CPU RAM中才能使用。由于该模型具有6.7B个参数，并且1个参数需要4字节的内存，因此该模型将需要4*6700000=26.8 GB的CPU RAM。</p><p id="6052" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PyTorch首先在内存中创建模型，然后加载另一个副本来获取模型的权重，所以实际上，您需要两倍的内存:53.6 GB。</p><p id="1662" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拥有如此多CPU RAM的消费级硬件配置仍然非常罕见。</p><p id="1f2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的电脑和Google Colab免费实例没有这么多内存，因此在进程结束前就终止了它。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="14e4" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">模型分裂</h1><p id="df4d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">为了能够运行模型，我们需要对它进行拆分:模型的一些部分将位于GPU VRAM、CPU RAM和硬盘上。</p><p id="d56a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">借助Accelerate，我们可以非常轻松地实现这种拆分。</p><p id="9ee0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用下面的代码来看看拆分是如何工作的:</p><pre class="kj kk kl km gt mz na nb bn nc nd bi"><span id="4737" class="ne md it na b be nf ng l nh ni">from accelerate import infer_auto_device_map, init_empty_weights<br/>from transformers import AutoConfig, AutoModelForCausalLM<br/><br/>config = AutoConfig.from_pretrained("facebook/opt-6.7b")<br/><br/>with init_empty_weights():<br/>  model = AutoModelForCausalLM.from_config(config)<br/><br/>device_map = infer_auto_device_map(model)</span></pre><p id="19fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里“infer_auto_device_map”将推断模型的最佳分割，以将模型尽可能多地加载到GPU VRAM上，然后加载到CPU RAM上，最后加载到硬盘上。</p><p id="c049" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我得到了这样的分割:</p><pre class="kj kk kl km gt mz na nb bn nc nd bi"><span id="04a1" class="ne md it na b be nf ng l nh ni">{'model.decoder.embed_tokens': 0, 'model.decoder.embed_positions': 0, <br/>'model.decoder.final_layer_norm': 0, <br/>'model.decoder.layers.0': 0, 'model.decoder.layers.1': 0, <br/>'model.decoder.layers.2': 0, 'model.decoder.layers.3': 0, <br/>'model.decoder.layers.4': 0, 'model.decoder.layers.5': 0, <br/>'model.decoder.layers.6': 0, 'model.decoder.layers.7.self_attn': 0, <br/>'model.decoder.layers.7.activation_fn': 0, <br/>'model.decoder.layers.7.self_attn_layer_norm': 0, <br/>'model.decoder.layers.7.fc1': 0, 'model.decoder.layers.7.fc2': 'cpu', <br/>'model.decoder.layers.7.final_layer_norm': 'cpu', <br/>'model.decoder.layers.8': 'cpu', 'model.decoder.layers.9': 'cpu', <br/>'model.decoder.layers.10': 'cpu', 'model.decoder.layers.11': 'cpu', <br/>'model.decoder.layers.12': 'cpu', 'model.decoder.layers.13': 'cpu', <br/>'model.decoder.layers.14': 'cpu', <br/>'model.decoder.layers.15.self_attn.k_proj': 'cpu', <br/>'model.decoder.layers.15.self_attn.v_proj': 'disk',<br/>'model.decoder.layers.15.self_attn.q_proj': 'disk', <br/>'model.decoder.layers.15.self_attn.out_proj': 'disk', <br/>'model.decoder.layers.15.activation_fn': 'disk', <br/>'model.decoder.layers.15.self_attn_layer_norm': 'disk', <br/>'model.decoder.layers.15.fc1': 'disk', 'model.decoder.layers.15.fc2': 'disk', <br/>'model.decoder.layers.15.final_layer_norm': 'disk', <br/>'model.decoder.layers.16': 'disk', 'model.decoder.layers.17': 'disk', <br/>'model.decoder.layers.18': 'disk', 'model.decoder.layers.19': 'disk', <br/>'model.decoder.layers.20': 'disk', 'model.decoder.layers.21': 'disk', <br/>'model.decoder.layers.22': 'disk', 'model.decoder.layers.23': 'disk', <br/>'model.decoder.layers.24': 'disk', 'model.decoder.layers.25': 'disk', <br/>'model.decoder.layers.26': 'disk', 'model.decoder.layers.27': 'disk', <br/>'model.decoder.layers.28': 'disk', 'model.decoder.layers.29': 'disk', <br/>'model.decoder.layers.30': 'disk', 'model.decoder.layers.31': 'disk', <br/>'lm_head': 'disk'}</span></pre><p id="1438" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到，拆分不是在层级别完成的，同一层的组件可能位于不同的设备上。例如，对于第7层，自我关注层规范在GPU上，但最终层规范在CPU RAM上。</p><p id="97ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这不是最佳选择，因为它可能会破坏层内部的连接。我们需要指出层不应该被分割。我们可以通过添加另一个参数来实现这一点，如下所示:</p><pre class="kj kk kl km gt mz na nb bn nc nd bi"><span id="463b" class="ne md it na b be nf ng l nh ni">device_map = infer_auto_device_map(model,no_split_module_classes=["OPTDecoderLayer"])</span></pre><p id="7410" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“no_split_module_classes”是指应该由模型实现的“_no_split_modules”变量。例如，对于OPT，如果您查看<a class="ae ky" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py" rel="noopener ugc nofollow" target="_blank">源代码</a>，您会在那里找到它:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/12ffcd034e85e8d864f220a05fcd66b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pp6NgEBP4NtgYRl6"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">截图来自<a class="ae ky" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py" rel="noopener ugc nofollow" target="_blank">的源代码OPT来自拥抱脸变形金刚</a></figcaption></figure><p id="cef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模块的名称因型号而异。例如，对于GPT-J-6B，它被称为“GPTJBlock”:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/a2ab982a70e28baa751768425fe703a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jjAsV2S9JNHN5TZH"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">截图来自《拥抱脸变形金刚》中<a class="ae ky" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py" rel="noopener ugc nofollow" target="_blank"> GPT-J的源代码</a></figcaption></figure><p id="0755" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新映射将每个层保留在同一设备上:</p><pre class="kj kk kl km gt mz na nb bn nc nd bi"><span id="35f7" class="ne md it na b be nf ng l nh ni">{'model.decoder.embed_tokens': 0, 'model.decoder.embed_positions': 0, <br/>'model.decoder.final_layer_norm': 0, 'model.decoder.layers.0': 0, <br/>'model.decoder.layers.1': 0, 'model.decoder.layers.2': 0, <br/>'model.decoder.layers.3': 0, 'model.decoder.layers.4': 0, <br/>'model.decoder.layers.5': 0, 'model.decoder.layers.6': 0, <br/>'model.decoder.layers.7': 'cpu', 'model.decoder.layers.8': 'cpu', <br/>'model.decoder.layers.9': 'cpu', 'model.decoder.layers.10': 'cpu', <br/>'model.decoder.layers.11': 'cpu', 'model.decoder.layers.12': 'cpu', <br/>'model.decoder.layers.13': 'cpu', 'model.decoder.layers.14': 'disk', <br/>'model.decoder.layers.15': 'disk', 'model.decoder.layers.16': 'disk', <br/>'model.decoder.layers.17': 'disk', 'model.decoder.layers.18': 'disk', <br/>'model.decoder.layers.19': 'disk', 'model.decoder.layers.20': 'disk', <br/>'model.decoder.layers.21': 'disk', 'model.decoder.layers.22': 'disk', <br/>'model.decoder.layers.23': 'disk', 'model.decoder.layers.24': 'disk', <br/>'model.decoder.layers.25': 'disk', 'model.decoder.layers.26': 'disk', <br/>'model.decoder.layers.27': 'disk', 'model.decoder.layers.28': 'disk', <br/>'model.decoder.layers.29': 'disk', 'model.decoder.layers.30': 'disk', <br/>'model.decoder.layers.31': 'disk', 'lm_head': 'disk'}</span></pre><p id="2df7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以随意操纵这张地图。例如，由于Accelerate会在转到硬盘之前尝试最大化CPU RAM的使用，因此您系统的操作系统可能没有更多CPU RAM可供自己使用。</p><p id="738d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果发生这种情况，您可以通过修改“device_map”的值将一个层卸载到磁盘，如下所示:</p><pre class="kj kk kl km gt mz na nb bn nc nd bi"><span id="f3fb" class="ne md it na b be nf ng l nh ni">device_map["model.decoder.layers.13"] = "disk"</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="fba5" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">包扎</h1><p id="dc10" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">最后，为了利用这种分割，我们只需要在加载模型时提供“device_map”参数，准确地说是“offload_folder”，这是一个将层存储在硬盘上的文件夹:</p><pre class="kj kk kl km gt mz na nb bn nc nd bi"><span id="635a" class="ne md it na b be nf ng l nh ni">model = OPTForCausalLM.from_pretrained("facebook/opt-6.7b", device_map="auto", offload_folder="offload")</span></pre><p id="6752" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将device_map设置为“auto”，但是您也可以将“auto”替换为用“infer_auto_device_map”创建的自定义映射，您可以对其进行修改。</p><p id="4991" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代码将会运行，但是当然，由于模型的某些部分在硬盘上，它可能会很慢。</p><p id="1d61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">硬盘上的可用空间是唯一的限制。如果你有更多的空间和耐心，你可以尝试更大的模型。</p></div></div>    
</body>
</html>