<html>
<head>
<title>Adversarial Attacks in Textual Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本深度神经网络中的对抗性攻击</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/adversarial-attacks-in-textual-deep-neural-networks-245dc90029df?source=collection_archive---------0-----------------------#2019-10-13">https://pub.towardsai.net/adversarial-attacks-in-textual-deep-neural-networks-245dc90029df?source=collection_archive---------0-----------------------#2019-10-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1f16" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">在DNNs | <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">中对AI </a>进行对抗性攻击</h2><div class=""/><div class=""><h2 id="42fb" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">什么是对抗性攻击？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a8b328c8c3a27ce5d4df889313e8122f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uZ1-Hg8MrWP4W5RT"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lh" href="https://unsplash.com/@svkj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Solal Ohayon </a>拍摄的照片</figcaption></figure><p id="a641" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对立的例子旨在使目标模型在预测上出错。导致模型表现不佳的原因可能是有意的，也可能是无意的。例如，当我们使用Gmail并使<a class="ae lh" href="https://ai.google/research/pubs/pub45189" rel="noopener ugc nofollow" target="_blank">智能回复</a>无法提供撰写电子邮件的建议时，可能会出现打字错误。</p><p id="5e7b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">无论是有意还是无意的对抗性攻击，评估对抗性例子已经成为构建健壮的深度学习模型和理解模型缺点的趋势。这个故事将讨论对抗性攻击，以及我们如何产生对抗性的例子来避免危险。在下一篇文章中，我们将介绍更多关于模拟对抗性攻击的细节。</p><h1 id="6373" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">图像对抗性攻击</h1><p id="7abe" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">第一个对立的例子来自计算机视觉领域。通过改变足够小的像素，在人眼不能区分原始图像和对手图像之间的差异时，对分类图像进行错误建模。</p><p id="c0a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在下图中，我们有两组(a组和b组)例子来演示原始图像和对立图像之间的差异。左栏是原始图像，而右栏是敌对图像。噪声(中间一栏)就是差异。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/a675f2e5dec5f6f75a3483aede27a9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PR47S6-fPfJsFgqaQmeKUA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">用于图像分类的对立图像(Szegedy等人，2014年)</figcaption></figure><h1 id="98e8" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">文本对抗性攻击</h1><p id="868d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">文本对抗性攻击不同于图像对抗性攻击。主要区别是:</p><ul class=""><li id="3782" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated"><strong class="lk jd">离散与连续输入:</strong>文本输入是离散特征，而图像输入是连续特征。</li><li id="a057" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><strong class="lk jd">可感知与不可感知:</strong>我们可以修改一些像素，做一个模型来错误分类，而人类却不容易区分。另一方面，我们只能修改人类能够注意到的字符或单词。</li><li id="c3a2" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><strong class="lk jd">语义vs无语义:</strong>图像的微小变化不会改变图像的语义。然而，当改变句子中的一个单词时，textual的意思可能完全不同。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/0df9fc4b5da000613c44d500229ecf6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*D50-xyvMcxdHZQGeK53GTA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">角色级别对抗性攻击的示例(Pruthi等人，2019年)</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/0c10f4788afe330126112c90628649d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SorjefNfbUobf2Civv_YYw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">词级对抗性攻击的例子(易卜拉希米等，2018)</figcaption></figure><h1 id="65d3" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">有针对性和无针对性的攻击</h1><ul class=""><li id="3f89" class="nc nd it lk b ll mw lo mx lr ns lv nt lz nu md nh ni nj nk bi translated"><strong class="lk jd">有针对性:</strong>使目标模型将对立的例子分类到特定的错误类别。</li><li id="e5bb" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><strong class="lk jd">无针对性:</strong>使目标模型将对立的例子分类到除正确类别之外的任何标签。</li></ul><h1 id="4e8e" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">白盒和黑盒攻击</h1><ul class=""><li id="756c" class="nc nd it lk b ll mw lo mx lr ns lv nt lz nu md nh ni nj nk bi translated"><strong class="lk jd">白盒:</strong>白盒攻击需要访问模型信息，包括网络架构、参数、输入、输出和其他模型属性。这种方法非常有效，因为它可以访问任何东西。</li><li id="7da5" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><strong class="lk jd">黑盒:</strong>除了输入输出，不能访问任何模型信息。换句话说，它只能通过输入来收集预测结果。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/c1a24f2f02968309c4cae0341421e9a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pSGM36UlTB5L_AjMW9PJXA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">白盒攻击和黑盒攻击的成功率。白盒攻击的性能优于黑盒攻击。(易卜拉希米等人，2018年)</figcaption></figure><h1 id="4545" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">生成对立示例的示例</h1><p id="7b2c" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><strong class="lk jd"><em class="nw"/></strong></p><p id="5155" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/pdf/1712.06751.pdf" rel="noopener ugc nofollow" target="_blank">热翻转</a>由易卜拉希米等人(2018)引入。它在字符级别上执行原子操作。它提供交换、插入和删除操作来生成对立的例子。易卜拉希米等人将这种技术应用于白盒攻击。</p><p id="0ce6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="nw">合成和自然噪声</em> </strong></p><p id="39eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/pdf/1711.02173.pdf" rel="noopener ugc nofollow" target="_blank"> Belinkov et al. (2017) </a>介绍了字符级操作(如交换相邻操作、键盘错别字)和单词级操作(如拼写错误)。<strong class="lk jd"> Swap </strong>、<strong class="lk jd"> Mid </strong>、<strong class="lk jd"> Rand、</strong>和<strong class="lk jd"> Key </strong>建议生成合成数据。</p><ul class=""><li id="d4d2" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated"><em class="nw">交换</em>:交换两个字母。例如，将<em class="nw">噪音</em>改为<em class="nw">噪音。</em></li><li id="8ee5" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nw">中期</em>:洗牌字符顺序除了第一个和最后一个。例如，将<em class="nw">噪音</em>改为<em class="nw">噪音</em>。</li><li id="c60a" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nw"> Rand </em>:洗牌所有角色顺序。例如，将<em class="nw">噪声</em>改为<em class="nw">噪声</em></li><li id="856d" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nw">键</em>:人类常见的错别字。例如，将<em class="nw">噪声改为</em>噪声。</li></ul><p id="61c0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="nw">过灵敏和过稳定</em> </strong></p><p id="7080" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/pdf/1809.02079.pdf" rel="noopener ugc nofollow" target="_blank">牛等(2018) </a>提出了两种验证过敏过稳模型的策略。引入了几个字级操作来验证模型性能。</p><ul class=""><li id="b690" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated"><em class="nw">随机交换</em>:随机交换相邻单词。</li><li id="c010" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nw">停用词丢失</em>:停用词携带的信息较少，会随机丢失。</li><li id="67e3" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nw">数据级释义</em>:用相似的意思替换单词。</li><li id="1a20" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">生成级释义:生成一个新句子，同时保持相同的意思。</li><li id="4d9b" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nw">语法错误</em>:故意用错误的on替换正确的单词/短语。</li><li id="2ac8" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nw">加否定</em>:加否定，引起相反的意思。</li><li id="78d9" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nw">反义词</em>:类似<em class="nw">加否定</em>但使用反义词。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f8603c53117678311ed092f0789bd785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*l2QTD7q7ylVzsK-xPPSKZw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">随机互换效应(牛等，2018)</figcaption></figure><h1 id="8d63" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">拿走</h1><ul class=""><li id="0273" class="nc nd it lk b ll mw lo mx lr ns lv nt lz nu md nh ni nj nk bi translated">对立的例子有助于建立一个强大的深度学习模型。</li><li id="fee9" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">前面提到的方法都在这个<a class="ae lh" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank">库</a>中实现。</li></ul><h1 id="94ec" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。在<a class="ae lh" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae lh" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上随意联系<a class="ae lh" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>。</p><h1 id="be7f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">延伸阅读</h1><ul class=""><li id="a617" class="nc nd it lk b ll mw lo mx lr ns lv nt lz nu md nh ni nj nk bi translated"><a class="ae lh" href="https://medium.com/hackernoon/does-your-nlp-model-able-to-prevent-adversarial-attack-45b5ab75129c" rel="noopener">您的NLP模型能够防止恶意攻击吗？</a></li><li id="1e1a" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">NLP中的数据扩充库(<a class="ae lh" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank"> nlpaug </a>)</li></ul><h1 id="1470" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">参考</h1><ul class=""><li id="3330" class="nc nd it lk b ll mw lo mx lr ns lv nt lz nu md nh ni nj nk bi translated">C.塞格迪、w .扎伦巴、I .苏茨基弗、j .布鲁纳、d .埃汉、I .古德菲勒和r .弗格斯。神经网络的有趣特性。2014</li><li id="5b8d" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">Y.别林科夫和y .比斯克。<a class="ae lh" href="https://arxiv.org/pdf/1711.02173.pdf" rel="noopener ugc nofollow" target="_blank">合成和自然噪音都打断神经机器翻译</a>。2017</li><li id="a5d0" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">J.和窦。<a class="ae lh" href="https://arxiv.org/pdf/1806.09030.pdf" rel="noopener ugc nofollow" target="_blank">关于字符级神经机器翻译的对立例子</a>。2018</li><li id="93f9" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">J.饶、劳德和窦。<a class="ae lh" href="https://arxiv.org/pdf/1712.06751.pdf" rel="noopener ugc nofollow" target="_blank"> HotFlip:用于文本分类的白盒对抗示例</a>。2018</li><li id="7847" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">T.牛和班萨尔。<a class="ae lh" href="https://arxiv.org/pdf/1809.02079.pdf" rel="noopener ugc nofollow" target="_blank">对话模式的对抗性过度敏感和过度稳定策略</a>。2018</li><li id="9162" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">D.普鲁西、b .丁格拉和Z. C .利普顿。<a class="ae lh" href="https://arxiv.org/pdf/1905.11268.pdf" rel="noopener ugc nofollow" target="_blank">利用强大的单词识别功能对抗对抗性拼写错误</a>。2019</li><li id="ad8e" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">页（page的缩写）尼卡拉、侯赛因、杜布诺夫和库尚法尔。<a class="ae lh" href="https://arxiv.org/pdf/1809.01829.pdf" rel="noopener ugc nofollow" target="_blank">序列分类神经网络的对抗性重编程</a>。2019</li><li id="7860" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">W.张，盛庆洲，阿尔哈兹米和李。<a class="ae lh" href="https://arxiv.org/pdf/1901.06796.pdf" rel="noopener ugc nofollow" target="_blank">对自然语言处理中深度学习模型的对抗性攻击:一项调查</a>。2019</li></ul></div></div>    
</body>
</html>