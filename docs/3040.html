<html>
<head>
<title>PySpark For Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark适合初学者</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pyspark-for-beginners-part-1-introduction-638fb16c5092?source=collection_archive---------0-----------------------#2022-08-13">https://pub.towardsai.net/pyspark-for-beginners-part-1-introduction-638fb16c5092?source=collection_archive---------0-----------------------#2022-08-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2436" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PySpark是一个用于Apache Spark的Python API。使用PySpark，我们可以在分布式集群(多个节点)上并行运行应用程序。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/41f33ab17a9e5d69db99183c267b98a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FtY21RZ91GeO2SJe.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">资料来源:Databricks</figcaption></figure><p id="f5e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们将首先从理论部分开始，介绍我们为什么需要Pyspark以及Apache Spark的背景、特性和集群管理器类型，以及Pyspark模块和软件包。</p><p id="2ab9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Apache Spark是一个用于大规模、强大的分布式数据处理和机器学习应用程序的分析处理引擎。一般来说，Spark是用Scala编写的，但是为了适应工业应用，Python API——py Spark被发布来使用Spark和Python。实际上，PySpark在机器学习和数据科学家社区中被大量使用；Spark在分布式集群上运行数十亿和数万亿数据的操作比传统python应用程序快100倍。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lc"><img src="../Images/c0a33d9e7606a265fdb0ed8cf99285df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oyOUbFIBw7j5o8bY.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">Pyspark的特性和优势</figcaption></figure><h1 id="50bf" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">Pyspark建筑</h1><p id="d428" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">Apache Spark采用主从架构，其中主设备被称为“驱动程序”，从设备被称为“工人”。Spark Driver创建一个Spark上下文作为应用程序的入口点，所有操作都在worker节点上运行，资源由集群管理器管理。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/e102133eba282277073bf820304efccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*9J_YbNE0gEIMKePm.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">来源:Spark。Apache.org</figcaption></figure><h1 id="63cb" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">集群管理器类型</h1><p id="5073" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">该系统目前支持多个集群管理器。此外，我们还可以在我们的桌面/系统上本地运行spark:</p><ul class=""><li id="9fbe" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mm mn mo mp bi translated"><em class="kl">独立【Spark附带的一个简单的集群管理器，可以轻松设置集群。</em></li><li id="3d84" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated"><em class="kl">Apache Mesos</em>——通用集群管理器，也可以运行Hadoop MapReduce和服务应用。(已弃用)</li><li id="ebb2" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated"><em class="kl">Hadoop YARN</em>—Hadoop 2和3中的资源管理器。最常用的集群管理器</li><li id="753c" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated">Kubernetes  —一个用于自动化部署、扩展和管理容器化应用的开源系统。</li></ul><h1 id="736d" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">Pyspark配置、Pyspark上下文和Pyspark会话:</h1><blockquote class="mv mw mx"><p id="b15d" class="jn jo kl jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">py spark conf:Spark conf为任何Spark应用程序提供配置。要在本地集群或数据集上启动任何Spark应用程序，我们需要设置一些配置和参数，可以使用SparkConf来完成。</p></blockquote><p id="0814" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Pyspark会议的特点:</p><ul class=""><li id="c2a3" class="mh mi iq jp b jq jr ju jv jy mj kc mk kg ml kk mm mn mo mp bi translated"><strong class="jp ir"> set(key，value)——</strong>设置一个配置属性。</li><li id="fbe2" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated"><strong class="jp ir"> setMaster(value) — </strong>设置要连接的主URL。</li><li id="b555" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated"><strong class="jp ir">设置应用名称(值)——</strong>设置应用名称。</li><li id="48dd" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated"><strong class="jp ir"> get(key，default value = None)——</strong>获取某个键的配置值，否则返回默认值。</li><li id="5f37" class="mh mi iq jp b jq mq ju mr jy ms kc mt kg mu kk mm mn mo mp bi translated"><strong class="jp ir">setSparkHome(value)——</strong>设置工作者节点上安装Spark的路径。</li></ul><blockquote class="mv mw mx"><p id="adb9" class="jn jo kl jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">py spark context:spark context是任何spark功能的入口点。当我们运行任何Spark应用程序时，都会启动一个驱动程序，它具有主函数，并且您的SparkContext会在这里启动。然后，驱动程序在工作节点上的执行器内部运行操作。</p></blockquote><p id="0a12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Spark驱动程序创建并使用SparkContext连接到集群管理器来提交PySpark作业，并知道与哪个资源管理器通信。它是PySpark应用程序的核心。</p><p id="6f9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们只能为每个JVM创建一个SparkContext。为了创建另一个第一，你需要通过使用<code class="fe nb nc nd ne b">stop()</code>方法停止现有的一个。SparkContext默认为“sc”。所以创建另一个变量而不是sc将会产生错误。</p><blockquote class="mv mw mx"><p id="be8a" class="jn jo kl jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated"><strong class="jp ir"> Pyspark session: </strong>从Spark 2.0开始，spark session就成为Pyspark与RDD、DataFrame合作的切入点。在2.0之前，SparkContext曾经是一个入口点。SparkSession是2.0版本之前所有不同上下文的组合类(SQLContext和HiveContext e.t.c)。因为2.0 SparkSession可用于replace with SQLContext、HiveContext和2.0之前定义的其他上下文。</p></blockquote><p id="9cf7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管SparkContext在2.0之前曾是一个入口点，但它并没有被SparkSession完全取代；SparkContext的许多特性在Spark 2.0和更高版本中仍然可用。SparkSession在内部使用提供的配置创建SparkConfig和SparkContext。</p><p id="631f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以使用<code class="fe nb nc nd ne b">SparkSession.builder()</code>或<code class="fe nb nc nd ne b">SparkSession.newSession()</code>在PySpark应用程序中创建任意多的<code class="fe nb nc nd ne b">SparkSession</code>。当您希望保持PySpark表(关系实体)在逻辑上分离时，需要许多Spark会话对象。</p><p id="56ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">创建火花会话</strong></p><pre class="kn ko kp kq gt nf ne ng nh aw ni bi"><span id="8751" class="nj le iq ne b gy nk nl l nm nn">from pyspark.sql import SparkSession</span><span id="6168" class="nj le iq ne b gy no nl l nm nn">spark = SparkSession.builder.appName("Practice").getOrCreate()</span><span id="d282" class="nj le iq ne b gy no nl l nm nn">spark</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7e40317eb8e8fcb1bc29cf20cca8be92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*4SIDeAtn-ME6X9p34bPSgg.png"/></div></figure><h1 id="35ab" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">Pyspark模块和封装</h1><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c72651b72ce7d27829da3393ec34b938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/0*8hrZ_iiP2xgxbP44.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">Pyspark模块和封装</figcaption></figure><p id="8115" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> PySpark RDD —弹性分布式数据集:</strong></p><blockquote class="mv mw mx"><p id="a181" class="jn jo kl jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">“弹性分布式数据集(RDD)是一种分布式内存抽象，可帮助程序员在大型集群上执行内存计算。”RDD的重要优势之一是<strong class="jp ir">容错，</strong>这意味着如果发生任何故障，它会自动恢复。RDD在被创建时变得不可变，即一旦被创建就不能被改变。</p></blockquote><p id="37cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RDD根据一个键将数据分成更小的部分。将数据分成更小的块的好处是，如果一个执行器节点出现故障，另一个节点仍将处理数据。因此，当相同的数据块在多个执行器节点上复制时，它能够从任何问题中快速恢复。RDD通过绑定多个节点，提供了针对数据集快速执行函数计算的功能。</p><div class="nr ns gp gr nt nu"><a href="https://muttinenisairohith.medium.com/pyspark-for-beginners-part-4-pyspark-rdd-7b5587347b4c" rel="noopener follow" target="_blank"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">Pyspark初学者|第4部分:Pyspark RDD</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">弹性分布式数据集(rdd)是Pyspark的基本构件，py spark是一个分布式内存…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">muttinenisairohith.medium.com</p></div></div><div class="od l"><div class="oe l of og oh od oi kw nu"/></div></div></a></div><p id="f216" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Pyspark数据帧:</strong></p><blockquote class="mv mw mx"><p id="91dd" class="jn jo kl jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">DataFrame <strong class="jp ir"> </strong>是组织成指定列的分布式数据集合。它在概念上相当于关系数据库中的一个表或R/Python中的一个数据框，但是在底层有更丰富的优化。数据帧可以从各种来源构建，如结构化数据文件、Hive中的表、外部数据库或现有的rdd。</p></blockquote><p id="c2b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于在多台机器上的所有内核上并行执行，PySpark运行操作的速度比pandas更快。换句话说，pandas DataFrames在单个节点上运行操作，而PySpark在多台机器上运行。</p><div class="nr ns gp gr nt nu"><a href="https://muttinenisairohith.medium.com/pyspark-for-begineers-part-2-pyspark-dataframe-60008da53e30" rel="noopener follow" target="_blank"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">初学者用Pyspark |第2部分:Pyspark数据框架</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">DataFrame现在是一个行业流行语，人们倾向于在各种情况下使用它。在这篇文章中，我们将学习…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">muttinenisairohith.medium.com</p></div></div><div class="od l"><div class="oj l of og oh od oi kw nu"/></div></div></a></div><p id="9261" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Pyspark SQL: </strong></p><blockquote class="mv mw mx"><p id="e7fa" class="jn jo kl jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">PySpark SQL是Spark中的一个模块，它将关系处理与Spark的函数式编程API集成在一起。我们可以使用SQL查询语言提取数据。我们可以像使用SQL语言一样使用查询。</p></blockquote><p id="3b30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说，Spark SQL在Spark上引入了原生的原始SQL查询，这意味着您可以在Spark数据帧上运行传统的ANSI SQL，在本PySpark教程的上一节中，您将详细了解如何使用SQL <code class="fe nb nc nd ne b">select</code>、<code class="fe nb nc nd ne b">where</code>、<code class="fe nb nc nd ne b">group by</code>、<code class="fe nb nc nd ne b">join</code>、<code class="fe nb nc nd ne b">union</code>等等</p><p id="d780" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PySpark SQL将易于使用，您可以扩展传统关系数据处理的限制。Spark也支持Hive查询语言，但是存在Hive数据库的限制。开发Spark SQL是为了消除Hive数据库的缺点。</p><p id="b85e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Pyspark MLlib: </strong></p><blockquote class="mv mw mx"><p id="2898" class="jn jo kl jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">Apache Spark提供了一个名为<strong class="jp ir"> MLlib </strong>的机器学习API。PySpark在Python中也有这个机器学习API。</p></blockquote><p id="7d18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它支持不同种类的算法，下面会提到</p><p id="0a06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">—ml lib . clustering<br/>—ml lib . class ification<br/>—ml lib . fpm<br/>—ml lib . Lina LG<br/>—ml lib . recommendation<br/>—ml lib . regression</em></p><div class="nr ns gp gr nt nu"><a href="https://muttinenisairohith.medium.com/pyspark-mllib-classification-using-pyspark-ml-ec7e99e5176f" rel="noopener follow" target="_blank"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">Pyspark MLlib |使用Pyspark ML分类</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">在前面几节中，我们讨论了RDD、数据帧和Pyspark概念。在本文中，我们将讨论…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">muttinenisairohith.medium.com</p></div></div><div class="od l"><div class="ok l of og oh od oi kw nu"/></div></div></a></div><p id="8344" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Pyspark GraphFrames: </p><p id="5225" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PySpark GraphFrames是在Spark 3.0版本中引入的，用于支持数据帧上的图形。在3.0之前，Spark有GraphX库，理想情况下运行在RDD上，失去了所有数据帧功能。</p><blockquote class="mv mw mx"><p id="257d" class="jn jo kl jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">GraphFrames是Apache Spark的一个包，它提供了基于数据帧的图形。它提供了Scala、Java和Python的高级API。它旨在提供GraphX的功能和利用Spark数据帧的扩展功能。这种扩展的功能包括主题查找、基于数据帧的序列化和高度表达的图形查询。</p></blockquote><p id="0daf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Pyspark流:</strong></p><blockquote class="mv mw mx"><p id="d8d0" class="jn jo kl jp b jq jr js jt ju jv jw jx my jz ka kb mz kd ke kf na kh ki kj kk ij bi translated">PySpark Streaming是一个可扩展、高吞吐量、容错的流处理系统，支持批处理和流工作负载。它用于处理来自文件系统文件夹、TCP套接字、S3、Kafka、Flume、Twitter和Amazon Kinesis等来源的实时数据。处理后的数据可以推送到数据库、Kafka、实时仪表盘等。</p></blockquote><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/553c39c4f2e438553c1c88b5850d3a61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_t9uqxRHJtjKboL3.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk translated">火花流</figcaption></figure><p id="784e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是Pyspark的理论部分</p><p id="aeb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">浏览上面提供的内容链接，理解并实现Pyspark。</p><p id="c488" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">快乐编码…</p></div></div>    
</body>
</html>