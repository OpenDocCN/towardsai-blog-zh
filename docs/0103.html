<html>
<head>
<title>A Simple Neural Attentive Meta-Learner — SNAIL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个简单的神经注意力元学习者——蜗牛</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-simple-neural-attentive-meta-learner-snail-1e6b1d487623?source=collection_archive---------0-----------------------#2019-07-11">https://pub.towardsai.net/a-simple-neural-attentive-meta-learner-snail-1e6b1d487623?source=collection_archive---------0-----------------------#2019-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bac631e5280f3ee1da400846eb7331bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AspxLM10u3ZTPHXD2hpqEg.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">一只攀爬的蜗牛试图看看外面的世界|来源:Pinterest</figcaption></figure><h2 id="7da6" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph">潜入蜗牛| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">向着艾</a></h2><div class=""/><h1 id="edea" class="kl km jf bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">介绍</h1><p id="4078" class="pw-post-body-paragraph lj lk jf ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">传统的强化学习算法训练一个智能体去解决一个单一的任务，期望它能很好地推广到来自相似数据分布的未知样本。元学习训练元学习者如何分配相似的任务，希望通过学习一种捕捉到要求它解决的问题的本质的高级策略，将其概括为一种新颖但相关的任务。</p><p id="6be9" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">闫端等人在2016年构造了一个元学习器，即RL，作为一个递归神经网络，除了正常接收的观察值外，它还接收过去的奖励，动作和终止标志作为输入。尽管这种方法简单且普遍，但在实践中却很难令人满意。Mishara等人假设，这是因为传统的RNN架构通过将信息从一个时间步长到下一个时间步长保持在隐藏状态来传播信息；这种时间线性依赖性限制了它们对输入流执行复杂计算的能力。相反，他们提出了一个<strong class="ll jp">S</strong>simple<strong class="ll jp">N</strong>eural<strong class="ll jp">A</strong>ttent<strong class="ll jp">I</strong>ve meta-<strong class="ll jp">L</strong>leaker(蜗牛),它结合了时间卷积和自我关注，从收集的经验中提取有用的信息。这个通用模型已经在各种实验中显示了它的功效，包括一些拍摄的图像分类和强化学习任务。</p><p id="ed3a" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">在本文中，我们将首先介绍SNAIL的结构组件，特别是时间卷积和注意力。然后我们讨论它们的优缺点，看看它们是如何互补的。像往常一样，这篇文章以讨论我自己的想法结束。</p><h2 id="5f52" class="mm km jf bd kn mn mo dn kr mp mq dp kv lu mr ms kz ly mt mu ld mc mv mw lh jl bi translated">简单神经注意力元学习者</h2><p id="949b" class="pw-post-body-paragraph lj lk jf ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">首先是SNAIL的整体架构</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mx"><img src="../Images/f8b955db73680808007f51baa5321707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zuL6OLCf--53-IyHNDLl8Q.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图一。绿色节点表示注意块，橙色节点表示时间卷积块。来源:一个简单的神经注意力元学习者</figcaption></figure><p id="6a4f" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">现在让我们更深入地看看每个组件。</p><p id="4783" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated"><strong class="ll jp">时间卷积</strong></p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/f74e1f8d7ca1f343f053ba8d2ef46f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tdD1jJsjmPhmp9Li49otXA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图二。扩张的因果卷积。来源:wave net:Rar音频的生成模型</figcaption></figure><p id="804f" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">在讨论时间卷积(TC)的结构之前，我们首先引入一个<em class="nd">密集块</em>，它应用一个具有内核大小<em class="nd"> 2 </em>、膨胀率<em class="nd"> R </em>和<em class="nd"> D </em>(例如16个)滤波器的单个因果1D卷积，然后将结果与其输入连接起来。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ne"><img src="../Images/720cf1e3e52c846e011c0907e849c751.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HKlwgGEdTWInohRJAO7Rtg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">密集块的伪代码</figcaption></figure><p id="9a80" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">因果1D卷积滤波器由图2中的红色三角形表示，膨胀率从上到下为8，4，2，1。请注意，1D卷积应用于序列维度，数据维度被视为通道维度。因果卷积有助于概括时间信息，就像二维卷积概括空间信息一样。在第三行中，我们使用门控激活功能，这已经在LSTM和GRUs中广泛使用。</p><p id="274f" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">TC块由一系列密集块组成，这些密集块的膨胀率呈指数增加，直到它们的接收场超过期望的序列长度<em class="nd"> T </em>，使得最后一层中的节点捕获所有过去的信息。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/38315e4cd33938c252a25f2275b3741c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*9Glfmq92FHXz0xFqX8LJqA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">TC块的伪代码。</figcaption></figure><p id="9d00" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated"><strong class="ll jp">注意</strong></p><p id="6cd1" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">注意块执行键值查找；我们在缩放后的点积注意力之后设计这个操作，这已经在<a class="ae ng" href="https://medium.com/towards-artificial-intelligence/attention-is-all-you-need-transformer-4c34aa78308f?source=friends_link&amp;sk=a259e84597d542f812a155711e9c8e97" rel="noopener">以前的文章</a>中介绍过，这里，为了完整起见，我们只提供伪代码</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/c5856fbd770253ab5ee0cbde0821cb30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FxoL1sC97kaXAw2uQMMY9A.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">注意块的伪代码</figcaption></figure><p id="53b9" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">请注意，SNAIL使用密集连接(在密集和注意块的末尾连接<em class="nd"> x </em>和<em class="nd"> y </em>)来防止渐变消失问题。</p><h2 id="bfbb" class="mm km jf bd kn mn mo dn kr mp mq dp kv lu mr ms kz ly mt mu ld mc mv mw lh jl bi translated">时间回旋与注意的合作</h2><p id="19a4" class="pw-post-body-paragraph lj lk jf ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">与传统的rnn相比，由于扩展的因果卷积支持指数扩展的感受野而不损失分辨率或覆盖范围，时间卷积提供了对过去信息的更直接、高带宽的访问。这允许它们在固定大小的时间上下文上执行更复杂的计算。然而，为了扩展到长序列，膨胀率通常呈指数增长，因此所需的层数与序列长度成对数关系。他们有限的能力和位置依赖性在元学习者中是不受欢迎的，元学习者应该能够充分利用越来越多的经验。</p><p id="94e7" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">相比之下，软注意允许模型从潜在的无限大的上下文中精确定位特定的信息。然而，缺乏位置依赖性也是不可取的，特别是在强化学习中，观察、动作和奖励本质上是连续的。</p><p id="5fd0" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">尽管存在各自的缺点，但时间卷积和注意力是相辅相成的:前者以有限的上下文大小为代价提供高带宽访问，而后者在无限大的上下文上提供精确访问。通过将TC层与因果注意力层交错，SNAIL可以对其过去的经验进行高带宽访问，而对其可以有效使用的经验数量没有限制。通过在一个端到端训练的模型中的多个阶段使用注意力，SNAIL可以学习从它收集的经验中挑选出哪些信息，以及容易做到这一点的特征表示。简而言之，时间回旋学习如何聚集上下文信息，注意力从中学习如何提取特定的信息片段。</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h2 id="1f95" class="mm km jf bd kn mn mo dn kr mp mq dp kv lu mr ms kz ly mt mu ld mc mv mw lh jl bi translated">讨论</h2><p id="f9f7" class="pw-post-body-paragraph lj lk jf ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ll jp">蜗牛如何做决定？</strong></p><p id="f91c" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">我个人认为SNAIL使用T大小的小批量做决策，除了上一集的观察-动作对之外，还包括当前的观察。我不明白的是，作者声称蜗牛维持着内部状态:</p><blockquote class="np nq nr"><p id="5410" class="lj lk nd ll b lm mh lo lp lq mi ls lt ns mj lw lx nt mk ma mb nu ml me mf mg ij bi translated"><em class="jf">至关重要的是，遵循meta-RL中的现有工作(段等人，2016；王等，2016)，我们跨情节边界保留了蜗牛的内部状态，这使得它具有跨越多个情节的记忆。观察结果还包含一个指示发作终止的二进制输入。</em></p></blockquote><p id="f8dc" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">欢迎在<a class="ae ng" href="https://ai.stackexchange.com/questions/11557/what-is-the-internal-state-of-a-simple-neural-attentive-meta-learnersnail" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>上讨论这个问题。</p><h1 id="de87" class="kl km jf bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">参考</h1><p id="5d6b" class="pw-post-body-paragraph lj lk jf ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">严端等RL:通过慢速强化学习实现快速强化学习</p><p id="feda" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">Fisher Yu等.通过扩展卷积进行多尺度上下文聚合</p><p id="e77c" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">一个简单的神经注意元学习者</p><p id="b654" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">你所需要的只是关注</p><p id="4ef5" class="pw-post-body-paragraph lj lk jf ll b lm mh lo lp lq mi ls lt lu mj lw lx ly mk ma mb mc ml me mf mg ij bi translated">Aaron van den Oord等人. wave net:Rar音频的生成模型</p></div></div>    
</body>
</html>