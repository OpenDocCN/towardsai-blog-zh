<html>
<head>
<title>CycleGAN as a Denoising Engine for OCR Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CycleGAN作为OCR图像去噪引擎</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/cyclegan-as-a-denoising-engine-for-ocr-images-8d2a4988f769?source=collection_archive---------0-----------------------#2020-08-15">https://pub.towardsai.net/cyclegan-as-a-denoising-engine-for-ocr-images-8d2a4988f769?source=collection_archive---------0-----------------------#2020-08-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0ab3" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="8963" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">将扫描但不干净的文档清理到原始形式。对于干净的文档，执行OCR将是一项更加精确的任务。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a9feead32cc8c1fd1354afa79d710a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NhKwK8SdIejf-WCMjMLesg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图一。脏的输入图像(左)，干净的输出图像(右)</figcaption></figure><p id="eafc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi ma translated"><span class="l mb mc md bm me mf mg mh mi di"> W </span>随着数字化的快速发展，对数字化内容的需求对于数据处理、存储和传输至关重要。光学字符识别(“OCR”)是将键入的、手写的或印刷的文本转换成可编辑、可搜索和可解释的数字化格式的过程，同时避免了将数据输入系统的需要。</p><p id="e525" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">大多数情况下，扫描的文档包含噪声，这使得OCR无法识别文本的全部内容。扫描过程通常导致噪声的引入，例如水印、背景噪声、由于相机运动或抖动引起的模糊、褪色的文本、皱纹或咖啡污渍。这些噪声对现有的文本识别算法提出了许多可读性挑战，显著降低了它们的性能。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mj"><img src="../Images/906b876f7b1a227b313075a711876971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fbKmgCBhLtf3-zh53Fri-w.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图二。使用OCR将扫描的文档转换为文本文档</figcaption></figure><h1 id="fa44" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak">基本OCR预处理方法</strong></h1><ul class=""><li id="3646" class="nc nd iq lg b lh ne lk nf ln ng lr nh lv ni lz nj nk nl nm bi translated"><strong class="lg ja">二值化</strong>通过固定阈值，将彩色图像转换成仅由黑白像素组成的图像。</li><li id="d03f" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated"><strong class="lg ja">歪斜校正</strong>通常涉及歪斜角度确定和基于歪斜角度校正文档图像。</li><li id="6d1f" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated"><strong class="lg ja">噪声去除</strong>通过去除比图像其余部分亮度更高的小点或小块，帮助平滑图像。</li><li id="6d99" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated"><strong class="lg ja">细化和骨架化</strong>确保手写文本笔画宽度的一致性，因为不同的书写者具有不同的书写风格。</li></ul><h1 id="889a" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak"> CycleGAN作为一种高级OCR预处理方法</strong></h1><p id="f523" class="pw-post-body-paragraph le lf iq lg b lh ne ka lj lk nf kd lm ln ns lp lq lr nt lt lu lv nu lx ly lz ij bi translated"><a class="ae nv" href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="noopener ugc nofollow" target="_blank">生成对抗网络</a>(“GANs”)是一种基于深度学习的生成模型。GAN模型体系结构包括两个子模型:用于生成新示例的生成器模型，以及用于对所生成的示例是来自该领域的真实示例还是由生成器模型生成的虚假示例进行分类的鉴别器模型。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/147d40b7976d254846d71034169fc18b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7-rAdgxl1dgaoQC35j7PA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图三。GAN网络概念图(来源:<a class="ae nv" href="https://developers.google.com/machine-learning/gan/gan_structure" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/GAN/GAN _ structure</a>)</figcaption></figure><p id="6ac3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">CycleGAN被选为使用<strong class="lg ja"> TensorFlow </strong>实施，作为一种高级OCR预处理方法。CycleGAN的一个优点是它<strong class="lg ja">不需要成对的训练数据</strong>。通常，配对数据是这样的数据集，其中一个独立样本中的每个数据点都与另一个独立样本中的数据点唯一配对。</p><p id="f3e2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">虽然仍然需要输入和输出变量，但它们不需要直接相互对应。由于配对数据在大多数领域中很难找到，因此CycleGAN的无监督训练功能确实非常有用。</p><p id="7195" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在缺少用于训练的成对图像的情况下，CycleGAN能够使用不成对的数据学习噪声图像的分布与去噪图像之间的映射，以实现用于清理噪声文档的<strong class="lg ja">图像到图像转换</strong>。</p><p id="b2c6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">图像到图像的转换是将图像从一个域(即噪声文档图像)，到另一个(即干净的文档图像)。图像的其他特征，如文本，应该保持可识别的相同，而不是与任何领域不直接相关的特征，如背景。</p><h1 id="d8c6" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak"> CycleGAN架构</strong></h1><p id="ad38" class="pw-post-body-paragraph le lf iq lg b lh ne ka lj lk nf kd lm ln ns lp lq lr nt lt lu lv nu lx ly lz ij bi translated">CycleGAN的架构包括<strong class="lg ja">两对生成器</strong> <strong class="lg ja">和鉴别器</strong>。每个发生器都有一个相应的鉴别器，它试图从真实图像中评估其合成图像。与任何GANs一样，<strong class="lg ja">发生器和鉴别器以相反的方式学习</strong>。每个生成器都试图“愚弄”相应的鉴别器，而鉴别器则学会不被“愚弄”。</p><p id="4611" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了让生成器保留脏文档的文本，该模型计算了<strong class="lg ja">循环一致性损失</strong>，该损失评估了从其域来回转换的图像与其原始版本的相似程度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/4a217a1ec8dbcacf7d460a7be2a9f953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7JKDTvulO6o4t4RRU5MJQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图4(a)原始脏输入到其转换的干净输出的转换</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/04b1b1dc3d0d1991b6a4f0dc2bebd578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0C34D2bEHmiyTbNzH8o5nQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图4(b)原始干净输入到其转换的脏输出的转换</figcaption></figure><p id="ee34" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">第一个生成器，<strong class="lg ja"> G-x2y </strong>，将原始的脏输入转换成转换后的干净输出。鉴别器<strong class="lg ja"> D-y </strong>将尝试评估转换后的清晰输出是真实图像还是生成的图像。鉴别器然后将提供被评估的图像是真实图像的概率。</p><p id="2bfd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">第二个生成器，<strong class="lg ja"> G-y2x </strong>，将原始的干净输入转换成转换后的脏输出。鉴别器<strong class="lg ja"> D-x </strong>将试图从生成的图像中分辨出真正的脏图像。创建的模型将在两个方向上进行训练，一组脏图像和一组干净图像，如上图所示。</p><h1 id="ab80" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak">方法和设计</strong></h1><p id="d940" class="pw-post-body-paragraph le lf iq lg b lh ne ka lj lk nf kd lm ln ns lp lq lr nt lt lu lv nu lx ly lz ij bi translated"><strong class="lg ja">背景噪声去除</strong>是去除背景噪声的过程，如对比度不均匀、背景斑点、折角页、褪色的黑子、文档上的皱纹等。背景噪声限制了OCR的性能，因为很难将文本与其背景区分开。</p><p id="69da" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">使用<a class="ae nv" href="https://www.kaggle.com/c/denoising-dirty-documents/data" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja"> Kaggle文档去噪数据集</strong> </a>来训练CycleGAN模型，该数据集由具有各种形式的噪声的噪声文档组成，例如咖啡污渍、褪色的黑子、折角页和皱纹。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nz"><img src="../Images/3cb13b74d85370e5e840dcbe25da0502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y1OO-VUZGEN4L3eBW9DXcg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图五。Kaggle文档去噪数据集中的脏文档类型</figcaption></figure><p id="f83f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了微调模型训练，执行了<strong class="lg ja">合成文本生成</strong>以引入除Kaggle数据集之外的更多噪声。这是使用<a class="ae nv" href="https://doc-creator.labri.fr/" rel="noopener ugc nofollow" target="_blank"> DocCreator </a>程序实现的，这是一个开源的多平台软件，可以基于少量的真实图像创建几乎无限量的不同地面真实合成文档图像。各种逼真的退化模型已被应用于原始语料库，从而生成合成图像。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/6b15c77e64c3589cb7e5a4b6b4876893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNZM-FD5vuaeN3L9HtL7VQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图六。生成的合成文本中的脏文档类型</figcaption></figure><p id="d674" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> train </strong>数据分组在trainA和trainB文件夹下，包含有噪声和干净的文档图像。<strong class="lg ja">验证</strong>数据分类在文件夹testA和testB下，也包括有噪声和干净的文档图像。在测试文件夹下的看不见的噪声文档图像的<strong class="lg ja">测试</strong>数据集用于测试训练的网络，并且评估用于从文档图像中去除背景噪声的模型。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/d6860b31064325f15e748796d090decd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AK94WJLagpGu-WDponGo1A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图7。培训、验证和测试数据的分类</figcaption></figure><p id="6f1c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于模型训练，使用Adam优化器，学习率为0.0002，动量为0.5，带有大小为256 X 512的噪声输入图像。由于硬件限制，最好的结果是通过训练CycleGAN模型300个历元获得的，批次大小为3。</p><h1 id="246c" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak">结果评估</strong></h1><p id="105f" class="pw-post-body-paragraph le lf iq lg b lh ne ka lj lk nf kd lm ln ns lp lq lr nt lt lu lv nu lx ly lz ij bi translated">进行了一项<strong class="lg ja">析因设计</strong>实验，以检验多个因素如何影响因变量，包括独立因素和共同因素。在Adam优化器、输入图像大小和批量大小等训练参数保持不变的情况下，在该项目中评估了四个不同的因素。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/9860c7fcb636ee555a1c731df7971787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*BElLZSlwIz_oLnl-2bgwPA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图8。本实验中考虑的因素</figcaption></figure><p id="d77b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于CycleGAN架构的复杂性，我们使用以下各种指标对CycleGAN性能进行了深入评估:</p><ul class=""><li id="e77e" class="nc nd iq lg b lh li lk ll ln od lr oe lv of lz nj nk nl nm bi translated"><strong class="lg ja">鉴别器损耗</strong>函数接受两个输入——真实图像和生成图像。真实损失是真实图像的<a class="ae nv" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank"> sigmoid交叉熵损失</a>和一个阵列，因为这些是真实图像。生成损失是生成图像和零阵列的sigmoid交叉熵损失，因为这些是伪图像。总鉴频器损耗是实际损耗和产生损耗的均方误差之和。</li><li id="b279" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated"><strong class="lg ja">精度</strong>通过将总鉴频器损耗表示为百分比来计算。鉴频器损耗越低，精度越高。</li><li id="d6a9" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated"><strong class="lg ja">生成器损失</strong>是生成的图像和一个数组的sigmoid交叉熵损失。这包括L1损失，它是生成的图像和目标图像之间的平均绝对误差，因此允许生成的图像在结构上变得与目标图像相似。</li><li id="de63" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated">在<strong class="lg ja">循环一致性丢失</strong>中，原始脏图像通过第一生成器传递以产生生成的图像。该生成的图像经由第二生成器传递，以产生重建的图像。计算原始脏图像和重建图像之间的平均绝对误差。平均绝对误差越低，与原始脏图像相比，重建图像的结构越相似。</li><li id="9acd" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated"><strong class="lg ja">峰值信噪比(“PSNR”)</strong>被定义为信号的最大可能功率与使其表现质量恶化的失真噪声的功率之比。PSNR通常用均方误差来表示。PSNR值越高，图像质量越好。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/5d9eaa3d88e30ddae49c1cab5bf3760e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uTfq0VbUi_6gOd4khDMKoA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图九。可量化绩效评估指标的平均结果</figcaption></figure><p id="cb95" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">比较从四个因素获得的结果，精确度<strong class="lg ja">从61%到99%的显著提高</strong>反映了从原始图像到生成图像转换期间获得的减少的<strong class="lg ja">鉴别器损失</strong>。随着<strong class="lg ja">发生器los </strong>的减少，生成的图像在结构上与原始图像相似。</p><p id="d6bd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于<strong class="lg ja">周期一致性损失</strong>，随着训练时期和数据集的增加，可以看到显著的改善。平均绝对误差的降低意味着重建图像在结构上类似于原始脏图像。在获得高图像质量时，<strong class="lg ja"> PSNR值</strong>没有显著差异。</p><p id="4910" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们来看看使用组合数据集为<strong class="lg ja"> 300个时期训练模型时的学习曲线。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/912f4c5e1a593d18fef0fb453f72a62b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MQviXe2hobjM_LqdvgEFvg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图10(a)发电机和鉴频器损耗</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/32aab4ac825d573a564969df28be0dfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uj1h826jlxSZcFyCo3t8Ng.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图10(b)训练周期一致性损失</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/2ffec472e33476ecdeebfea09a4651d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyFp2hQ_lj3nA9k-2afwig.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图10(c)训练准确度</figcaption></figure><p id="3dd2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">从训练数据集得出的上述学习曲线让我们了解了模型的学习效果。在每次前进过程中，对单个批次的这些学习曲线进行评估。</p><p id="c70c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">损失曲线</strong>反映了良好的拟合，因为它已经下降到一个稳定点。损失中的“波动”量与批次大小有关。当批量大小为1时，波动会相对较大。当批量大小为完整数据集时，波动将最小，因为每次梯度更新都应单调地改善损失函数(除非学习率设置得太高)。</p><p id="6e6c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">至于<strong class="lg ja">训练准确度图</strong>，考虑到学习算法的随机性，会有一些波动。虽然使用了较慢的学习速率0.0002，但我们已经将动量增加到0.5，以确保模型学习良好。</p><ul class=""><li id="f822" class="nc nd iq lg b lh li lk ll ln od lr oe lv of lz nj nk nl nm bi translated"><strong class="lg ja">模板匹配</strong>是一种在更大的图像中搜索并找到模板图像位置的方法。模板图像在输入图像上滑动，并且在模板和模板图像下的输入图像的小块之间进行比较。返回灰度图像，其中每个像素表示该像素的邻域与模板匹配的程度。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/87059d5673b8d01bdf8bf020452407d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kqX--93co2Aqj7WciS_kLg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图11(a)仅使用<strong class="bd mm"> Kaggle数据集</strong>的<strong class="bd mm"> 100个时期</strong>的模板匹配</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/df7fd8a3217dfda33b9f8f29f747c5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o0Xkg6U7n93VpAZzOQ4nEQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图11(b)仅使用<strong class="bd mm"> Kaggle数据集</strong>的<strong class="bd mm"> 300 </strong>和<strong class="bd mm">历元</strong>的模板匹配</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/73b5e1e5b84de7dcba9790b2e3930110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hl-AktZE4bOCMZuX_R8SCg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图11(c)仅使用生成的<strong class="bd mm">合成文本的<strong class="bd mm"> 100个时期</strong>的模板匹配</strong></figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/8f90b91e652dfd71a4c8f6c87bd12d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Svz5a5SeWnOFCd11-rbSyg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图11(d)使用<strong class="bd mm">组合数据集</strong>对<strong class="bd mm"> 300个时期</strong>的模板匹配</figcaption></figure><p id="5050" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">总体而言，CycleGAN模型在<strong class="lg ja">模板匹配</strong>方面表现良好。匹配结果显示了灰度图像，该灰度图像由该像素的邻域与模板图像匹配的程度的强度水平来表示。检测到的点指示被检测的文本的良好覆盖，并与模板图像匹配。</p><ul class=""><li id="a4e0" class="nc nd iq lg b lh li lk ll ln od lr oe lv of lz nj nk nl nm bi translated"><strong class="lg ja">均方误差(“MSE”)</strong>测量误差平方的平均值，即估计值和估计值之间的平均平方差。MSE的零值表示完全相似，而大于1的值意味着相似度较低，并且将随着像素强度之间的平均差异的增加而继续增加。</li><li id="0fed" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated"><strong class="lg ja">结构相似性指数(“SSIM”)</strong>试图对图像的结构信息中的感知变化进行建模，而MSE是实际估计的感知误差。与MSE不同，SSIM值可以在负1和1之间变化，1表示完全相似。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/4949059df7ecf541af4728f04c781ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5W7OoUXWQ2pyYSrPEfKfzQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图12(a)仅使用<strong class="bd mm"> Kaggle数据集</strong>的<strong class="bd mm"> 100个时期</strong>的输出</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/16419b042b901e3baf93604945963d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ZmGQzHBhZNJjaPopxbr3Q.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图12(b)仅使用<strong class="bd mm"> Kaggle数据集</strong>的300个时期的输出</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi op"><img src="../Images/ce16c8148368b9b0e1e11f7ded042cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HpRJuMu8mqQJfGkD0QOgJQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图12(c)仅使用生成的<strong class="bd mm">合成文本的<strong class="bd mm"> 100个时期</strong>的输出</strong></figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/a7c5b654c9911306d4f38b30dca60481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VB9msGmMfwXWOa5N-UBL7A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图12(d)使用<strong class="bd mm">组合数据集</strong>的<strong class="bd mm"> 300个时期</strong>的输出</figcaption></figure><p id="8c6c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">随着训练时期和数据集的增加，<strong class="lg ja"> MSE </strong>的减少随着<strong class="lg ja"> SSIM </strong>值的提高而显著。这表明测试图像和生成的输出图像之间的结构相似性更高，并且像素强度差异减小。</p><ul class=""><li id="f1e5" class="nc nd iq lg b lh li lk ll ln od lr oe lv of lz nj nk nl nm bi translated"><strong class="lg ja">图像相减</strong>是从生成的图像中减去脏图像的过程。目的是比较两幅图像的像素强度，以确定使用训练的模型清洁脏图像的程度。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi or"><img src="../Images/33710fd5b75f3c0e609f9fd4bd35db6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_L6BVh3tVkkGkq7jvQI0Ug.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图13(a)仅使用<strong class="bd mm"> Kaggle数据集</strong>的<strong class="bd mm"> 100个时期</strong>(左)和<strong class="bd mm"> 300个时期</strong>(右)的图像相减输出</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/8f33a8fffea7ad02ee76098ef8b2e61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uNgoh1miZzSCrfxI0pOGUA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图13(b)仅使用生成的的<strong class="bd mm">合成文本的<strong class="bd mm"> 100个时期</strong>的图像减法输出(左)，以及使用<strong class="bd mm">组合数据集</strong>的<strong class="bd mm"> 300个时期</strong>(右)</strong></figcaption></figure><p id="6af5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">从生成的输出图像中减去测试图像<strong class="lg ja"/>，以确定使用训练模型清洁脏图像的程度。通过比较输出图像的像素强度，有利的结果(即几乎全黑输出图像)是由CycleGAN使用组合数据集以更高数量的训练时期获得的。</p></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><p id="cbf9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">CycleGAN针对各种训练参数和因子生成的输出图像示例如下所示。</p><p id="86bd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">原始列:</strong>原始脏/干净图像；</p><p id="b033" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">翻译栏:</strong>训练原始图像后对应的干净/脏图像；</p><p id="8804" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">重建栏:</strong>使用翻译后的图像重建回原始脏/干净状态的图像。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/76d2d68db31bae2120ddbb6e2d53cc1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xPadYzVgPKdFN0TQhhw50g.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图14(a)仅使用<strong class="bd mm"> Kaggle数据集</strong>为<strong class="bd mm"> 100个时期</strong>训练的输出图像</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/ab4c809abc246df9774ebd80de00ed61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*i7aJazPcVF1dFQ_JjY8YAg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图14(b)在<strong class="bd mm">翻译的</strong>图像中缺少文本</figcaption></figure><p id="4488" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">仅使用Kaggle数据集为<strong class="lg ja"> 100个时期训练的输出图像对于OCR来说并不理想。在翻译和重建的图像中都发现了缺失的文本。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/7d618f3c2d071e705b51e4287eb76000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORTvr6pro21icxGpK_DdCw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图15(a)仅使用<strong class="bd mm"> Kaggle数据集</strong>为<strong class="bd mm"> 300个时期</strong>训练的输出图像</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/0582c30dbd9d12d6ed981228a5c72524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*_-22b2MrOxKtIKotYTWLFw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图15(b)在<strong class="bd mm">翻译的</strong>图像中缺少文本</figcaption></figure><p id="c393" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">仅使用Kaggle数据集，针对<strong class="lg ja"> 300个时期训练的输出图像逐渐得到改善。随着训练时期数量的增加，在翻译和重建的图像中丢失文本的出现都减少了。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pe"><img src="../Images/7a84ab6340c4378381b85e07c240ba77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*em8AkG2dJyXvlsjwOk8FUg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图16(a)仅使用生成的<strong class="bd mm">合成文本为<strong class="bd mm"> 100个时期</strong>训练的输出图像</strong></figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/39f7f43aba09a98b49d9f2aec2211c33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*2Gpb-IOZgv6ee-SiwpvQxA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图16(b)翻译后的<strong class="bd mm">图像中的文本不清晰</strong></figcaption></figure><p id="453d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">仅使用生成的合成文本为<strong class="lg ja"> 100个时期训练的输出图像难以辨认。因此，增加用于训练的历元数将有助于提高输出图像的可读性。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pg"><img src="../Images/69572bf3940dd402b9ab3dd54e211a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jn8tk-VoIPXXtoYswIRkAA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图17。使用<strong class="bd mm">组合数据集</strong>为<strong class="bd mm"> 300个时期</strong>训练的输出图像</figcaption></figure><p id="39c3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">从上面使用组合数据集对<strong class="lg ja"> 300个历元进行训练的输出图像可以看出，随着训练历元数量的增加，CycleGAN的模型性能有了显著提高。使用合成文本生成添加噪声有助于增加训练数据量，从而提高模型性能。深度网络如CycleGAN在大量训练数据的情况下表现更好。通过将训练模型暴露于数量增加的训练样本来减轻过度拟合的风险。</strong></p><h1 id="cf26" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak">建议</strong></h1><p id="4ba4" class="pw-post-body-paragraph le lf iq lg b lh ne ka lj lk nf kd lm ln ns lp lq lr nt lt lu lv nu lx ly lz ij bi translated">为了提高CycleGAN的性能，可以考虑以下方面:</p><ul class=""><li id="0f97" class="nc nd iq lg b lh li lk ll ln od lr oe lv of lz nj nk nl nm bi translated">通过数据扩充增加数据集</li><li id="a673" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated">增加训练的时期数</li><li id="5f16" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz nj nk nl nm bi translated">尝试不同的学习率和学习率计划以增强收敛性</li></ul><p id="e6d1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">上述建议的基础是基于通过合成文本生成和用于训练的时期数量的增加而在数据集的增加中观察到的改进的性能。CycleGAN架构的复杂性包括管理用于训练的大规模数据集的能力，从而为模型性能的进一步改进提供了良好的基础。</p><h1 id="c168" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak">结论</strong></h1><p id="ab63" class="pw-post-body-paragraph le lf iq lg b lh ne ka lj lk nf kd lm ln ns lp lq lr nt lt lu lv nu lx ly lz ij bi translated">CycleGAN已被证明是一个有效的去噪引擎，可以对OCR文档进行去噪和清理。</p><p id="4a3c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在没有成对训练数据的情况下，CycleGAN独特地使用循环一致性损失解决了用不成对数据学习有意义的转换的问题。它允许生成器生成干净的图像，通过图像到图像的转换保留脏输入图像的文本。通过生成合成文本，训练时期数量的增加与数据集的增加相结合，也有助于显著提高CycleGAN模型的性能。</p></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><h1 id="acdb" class="mk ml iq bd mm mn ph mp mq mr pi mt mu kf pj kg mw ki pk kj my kl pl km na nb bi translated"><strong class="ak">参考文献</strong></h1><ol class=""><li id="2fcb" class="nc nd iq lg b lh ne lk nf ln ng lr nh lv ni lz pm nk nl nm bi translated"><a class="ae nv" href="https://arxiv.org/abs/1901.11382" rel="noopener ugc nofollow" target="_blank">莫妮卡·夏尔马、阿布舍克·维尔马、洛夫凯什·维格的《学习清洁:甘视角》</a></li><li id="5fc2" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz pm nk nl nm bi translated"><a class="ae nv" href="https://towardsdatascience.com/pre-processing-in-ocr-fc231c6035a7" rel="noopener" target="_blank">OCR中的预处理</a></li><li id="d8f6" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz pm nk nl nm bi translated"><a class="ae nv" href="https://www.tensorflow.org/tutorials/generative/cyclegan" rel="noopener ugc nofollow" target="_blank">tensor flow核心教程中的cycle gan</a></li><li id="9c38" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz pm nk nl nm bi translated"><a class="ae nv" href="https://github.com/eriklindernoren/Keras-GAN/tree/master/cyclegan" rel="noopener ugc nofollow" target="_blank"> Keras实现的GAN </a></li><li id="53c4" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz pm nk nl nm bi translated"><a class="ae nv" href="https://medium.com/illuin/cleaning-up-dirty-scanned-documents-with-deep-learning-2e8e6de6cfa6" rel="noopener">用深度学习清理脏的扫描文档</a></li><li id="4e02" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz pm nk nl nm bi translated"><a class="ae nv" href="https://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">斯坦福CS Class CS231n:用于视觉识别的卷积神经网络</a></li><li id="00f6" class="nc nd iq lg b lh nn lk no ln np lr nq lv nr lz pm nk nl nm bi translated"><a class="ae nv" href="https://www.tensorflow.org/tutorials/keras/overfit_and_underfit" rel="noopener ugc nofollow" target="_blank">过拟合和欠拟合</a></li></ol></div></div>    
</body>
</html>