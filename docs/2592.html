<html>
<head>
<title>Logistic Regression for Multi-Class Classification: Hands-On with SciKit-Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多类分类的逻辑回归:SciKit-Learn实践</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/logistic-regression-for-multi-class-classification-hands-on-with-scikit-learn-bcc0bbad1def?source=collection_archive---------1-----------------------#2022-03-04">https://pub.towardsai.net/logistic-regression-for-multi-class-classification-hands-on-with-scikit-learn-bcc0bbad1def?source=collection_archive---------1-----------------------#2022-03-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7896" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python和Google Colab</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4a53872c0498c9896edeaddd8b0280cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KMosW7XhtD65-oTIPGQDlw.png"/></div></div></figure><p id="9266" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在之前的帖子中，我解释了二进制分类的逻辑回归、背后的数学推理以及如何使用Scikit-Learn库进行计算。在这篇文章中，我将解释为多类分类问题实现逻辑回归所需的修改。</p><p id="1d16" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本来，逻辑回归只支持二元分类，由于从逻辑方程获得的曲线的性质，这很容易理解。然而，有两个选项可以使这个模型“适应”多类问题。</p><h2 id="eafb" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">一比一休息:</h2><p id="df7f" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">使用one-vs-rest(“ovr”)选项，我们的模型将做的是将一个类与所有其他类进行比较，并对所有类执行这一步骤。这样，我们将多类问题转化为多二元问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/9b31aede33e0dffd22117d6c90444aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VKaGFydeFynomife5TfLqg.jpeg"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk translated">左边是使用“ovr”技术的多类分类。右边是简单的二进制分类。</figcaption></figure><h2 id="ca31" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">多项逻辑回归；</h2><p id="2e57" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">另一种方法是使用多项式逻辑回归。为了应用这种技术，我们的数据需要满足预测变量的非完美分离和独立性的假设。多项逻辑回归同样基于<a class="ae lq" href="https://code.likeagirl.io/how-to-implement-and-select-the-best-linear-regression-model-2315b9a26348" rel="noopener ugc nofollow" target="_blank">线性回归</a>，公式为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/56d8975003cca1ecbfe3f51faf3e93a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*WVE6o67SfVUkxJTXAPkswA.png"/></div></figure><p id="39c0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<strong class="kw iu"> <em class="mv"> y </em> </strong>是我们的结果变量，<strong class="kw iu"> <em class="mv"> m </em> </strong>是曲线斜率，<strong class="kw iu"> <em class="mv"> x </em> </strong>是预测变量，<strong class="kw iu"> <em class="mv"> b </em> </strong>是与<strong class="kw iu"> <em class="mv"> y轴</em> </strong>的截距。如果我们有一个以上的预测变量，我们的公式将是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/aac2500a905981a8c3807f0b628d4547.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QU1ZXxF850urvNA_HC_NmQ.png"/></div></div></figure><p id="debd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">逻辑方程也被转换，以允许超过两个类别的概率。三类分类任务的多项式逻辑回归概率方程如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/d1fac8a526f5fb3f0600e967c4c1e2a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*jd98FV71eiDO3t1JbAMAnw.png"/></div></div></figure><p id="3a1b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<strong class="kw iu"> <em class="mv"> z </em> </strong>是从:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/63ecf823458735353b21a8ca0222659f.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*Y64yAUGwnb3K63lv3wsvmA.png"/></div></figure><p id="9163" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意，<strong class="kw iu"><em class="mv"/></strong>是模型中所有类的<strong class="kw iu"><em class="mv">【e^f(x】</em></strong><em class="mv"/>的总和。要知道<strong class="kw iu"> <em class="mv"> z </em> </strong>对于任何给定的模型和数据都是常数，但不一定不等于1。<strong class="kw iu"> <em class="mv"> z </em> </strong>可以取很大范围的值，因为没有明确的类别划分。该值仅在计算了变量和结果类别的回归系数后确定(如所解释的，应用<a class="ae lq" href="https://code.likeagirl.io/how-to-implement-and-select-the-best-linear-regression-model-2315b9a26348" rel="noopener ugc nofollow" target="_blank">线性回归</a>计算<strong class="kw iu"> <em class="mv"> f(x) </em> </strong>)。</p><h2 id="3ac9" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">C参数:</h2><p id="e23a" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">多项式逻辑回归也有一个C参数，可以调整该参数以找到最佳拟合，而不会过度拟合或拟合不足。</p><h2 id="c70b" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">动手操作:</h2><p id="d73a" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我们将使用带有预建函数的Scikit-Learn库来计算多项式逻辑回归。您可以按照本教程使用我的代码来构建您自己的数据库，或者您可以替换变量名称并使用您自己的数据。</p><p id="ae5d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">导入必要的库:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="5b81" class="lr ls it na b gy ne nf l ng nh">from random import random<br/>from random import randint</span><span id="2f02" class="lr ls it na b gy ni nf l ng nh">import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="1368" class="lr ls it na b gy ni nf l ng nh">from sklearn.linear_model import LogisticRegression<br/>from sklearn.svm import LinearSVC<br/>from sklearn.model_selection import train_test_split</span><span id="ecfd" class="lr ls it na b gy ni nf l ng nh">from mlxtend.plotting import plot_decision_regions</span></pre><p id="e72e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">创建数据:</strong></p><p id="d2af" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将创建一个满足多项逻辑回归假设的数据集，其中包含三个预测变量和一个包含三个类别的结果变量:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="ad9c" class="lr ls it na b gy ne nf l ng nh"><strong class="na iu">#Fabricating variables:</strong></span><span id="ae6c" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu">#Creating values for FeNO with 3 classes:</strong><br/>FeNO_0 = np.random.normal(15,20, 100)<br/>FeNO_1 = np.random.normal(35,20, 100)<br/>FeNO_2 = np.random.normal(65, 20, 100)</span><span id="f8ca" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu">#Creating values for FEV1 with 3 classes:</strong><br/>FEV1_0 = np.random.normal(4.50, 1, 100)<br/>FEV1_1 = np.random.normal(3.75, 1.2, 100)<br/>FEV1_2 = np.random.normal(2.35, 1.2, 100)</span><span id="0850" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu">#Creating values for Broncho Dilation with 3 classes:</strong><br/>BD_0 = np.random.normal(150,49, 100)<br/>BD_1 = np.random.normal(250,50,100)<br/>BD_2 = np.random.normal(350, 50, 100)</span><span id="9209" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu">#Creating labels variable with three classes:(2)disease (1)possible disease (0)no disease:</strong><br/>not_asthma = np.zeros((100,), dtype=int)<br/>poss_asthma = np.ones((100,), dtype=int)<br/>asthma = np.full((100,), 2, dtype=int)</span></pre><p id="7ca8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">连接变量并创建数据帧:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a215" class="lr ls it na b gy ne nf l ng nh"><strong class="na iu">#Concatenate classes into one variable:</strong><br/>FeNO = np.concatenate([FeNO_0, FeNO_1, FeNO_2])<br/>FEV1 = np.concatenate([FEV1_0, FEV1_1, FEV1_2])<br/>BD = np.concatenate([BD_0, BD_1, BD_2])<br/>dx = np.concatenate([not_asthma, poss_asthma, asthma])</span><span id="b89a" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu">#Create DataFrame:</strong><br/>df = pd.DataFrame()</span><span id="bcca" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu">#Add variables to DataFrame:</strong><br/>df['FeNO'] = FeNO.tolist()<br/>df['FEV1'] = FEV1.tolist()<br/>df['BD'] = BD.tolist()<br/>df['dx'] = dx.tolist()</span></pre><p id="4c5c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们可以看看我们的数据框架:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/013bfe4867cb9c0d2970e54cf3491362.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*qTm63smVER4wotRM6kiB3A.png"/></div></figure><p id="8251" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如所料，我们有3行4列。</p><p id="f3ea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">探索性数据分析:</strong></p><p id="12f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">与任何数据项目一样，当我们准备好数据时，我们应该执行探索性数据分析(EDA)。我将简单地进行一些描述性的练习，但是你应该根据你的数据类型(或者为了发现它)应用最好的EDA。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="84e1" class="lr ls it na b gy ne nf l ng nh"><strong class="na iu">#Exploring dataset:</strong><br/>sns.pairplot(df, kind="scatter", hue="dx")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ba46b643409d9992d404992be838d136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*3Q2H3qiXbZczYh6Zs1iYhw.png"/></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="93f2" class="lr ls it na b gy ne nf l ng nh">sns.boxplot(x=df["dx"], y=df["FEV1"])</span><span id="f710" class="lr ls it na b gy ni nf l ng nh">sns.boxplot(x=df["dx"], y=df["FeNO"])</span><span id="78d9" class="lr ls it na b gy ni nf l ng nh">sns.boxplot(x=df["dx"], y=df["BD"])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/026c027246e579ce252c3bae15862d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S9PJVC1voa4xxAA89tcLEw.png"/></div></div></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="77c1" class="lr ls it na b gy ne nf l ng nh">sns.lmplot(x="FEV1", y="FeNO", data=df, fit_reg=True, hue='dx', legend=True)</span><span id="a572" class="lr ls it na b gy ni nf l ng nh">sns.lmplot(x="FEV1", y="BD", data=df, fit_reg=True, hue='dx', legend=True)</span><span id="8e70" class="lr ls it na b gy ni nf l ng nh">sns.lmplot(x="FeNO", y="BD", data=df, fit_reg=True, hue='dx', legend=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/65c7c078e2bf6c9874777b7bc76ded59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v_4MP9yCILyxQHOnF1Y_1g.png"/></div></div></figure><p id="6523" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尚不完全清楚，但到目前为止，通过目测，变量“FeNO”和“BD”似乎是区分这三组的最佳方法。在分析之前，我们不需要知道这些，当我们构建决策边界图时，这在接下来的步骤中会很有用。</p><p id="3bf3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">将数据分为训练和测试数据集:</strong></p><p id="7985" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们需要一部分数据来训练模型，另一部分数据来测试模型。通常，训练模型需要大量数据，因此我们将90%用于训练，10%用于测试:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="f0f0" class="lr ls it na b gy ne nf l ng nh"><strong class="na iu">#Creating X and y:</strong><br/>X = df.drop('dx', axis=1)<br/>y = df['dx']</span><span id="7aaa" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu">#Data split into train and test:</strong><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)</span></pre><p id="cde7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">建立并评估模型:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6c3e" class="lr ls it na b gy ne nf l ng nh"><strong class="na iu">#Fit Logistic Regression model:</strong><br/>logisticregression = LogisticRegression().fit(X_train, y_train)</span><span id="9873" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu">#Evaluate Logistic Regression model:</strong><br/>print("training set score: %f" % logisticregression.score(X_train, y_train))<br/>print("test set score: %f" % logisticregression.score(X_test, y_test))<br/>print("coefficients shape: ", logisticregression.coef_.shape)<br/>print("intercept shape: ", logisticregression.intercept_.shape)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/5b3ec12c4123ff4424060cde3247079f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1k6_n949zb90FeGopRKpgA.png"/></div></div></figure><p id="9bd0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们分析我们的输出。如果你读过我以前关于<a class="ae lq" href="https://medium.com/p/a5c06b0f2d60" rel="noopener">二元逻辑回归</a>的文章，你会发现输出只包含训练集分数和测试集分数。这里，除此之外，我们还有系数形状和截距形状。(3，3)的系数形状意味着我们有3个类别和3个特征。截距形状是添加到决策函数中的常数，与类的数量相同。</p><p id="eead" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过分析我们的训练和测试集分数，可以看出我们的模型是不合适的。我们现在将学习如何调整模型中的一些参数。</p><p id="a247" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">调整<em class="mv"> C </em>，多类技术，迭代次数:</strong></p><p id="2a96" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我前面已经解释过<strong class="kw iu"><em class="mv"/></strong><a class="ae lq" href="https://medium.com/p/a5c06b0f2d60" rel="noopener"><strong class="kw iu"><em class="mv">C</em></strong>值在逻辑回归</a>中的作用。在本文的开始，我解释了我们的机器可以用来计算多类逻辑回归的两种技术。默认情况下，Scikit-Learn将使用多项逻辑回归，但我们可以更改one-vs-rest(“ovr”):</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="3aa7" class="lr ls it na b gy ne nf l ng nh"><strong class="na iu">#Fit Logistic Regression model:<br/></strong>logisticregression = LogisticRegression(C=1, multi_class='ovr',<br/>                                 max_iter=100).fit(X_train, y_train)</span><span id="24d9" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu">#Evaluate Logistic Regression model:<br/></strong>print("training set score: %f" % logisticregression.score(X_train, y_train))<br/>print("test set score: %f" % logisticregression.score(X_test, y_test))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/191c79ee5d084ed4bfae6954426049c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*aGeeyqQuaBlcMFcfPzlwFw.png"/></div></figure><p id="5642" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里，我们将多类技术更改为“ovr ”,我们看到结果比“多项式”稍差。</p><p id="0a6e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以改变的另一个参数是最大迭代次数。再次切换到多项式，如果我们再次尝试，我们将会看到<strong class="kw iu">警告消息</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/8178d65fc098b28a57440544d2065e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xiLg99AaRhX955uT-0Gueg.png"/></div></div></figure><p id="71d8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了解决这个问题，我们只需要增加最大迭代次数。改为“max_iter = 1000 ”,我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e2b499a04bb1465e3b8bc9a5e6defbbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*Efn1nY7ucFwxc71ADHi7rw.png"/></div></figure><p id="c182" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">寻找最佳<em class="mv"> C </em>值:</strong></p><p id="5415" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了找到最佳的<strong class="kw iu"> <em class="mv"> C </em> </strong>值，程序与二进制分类相同。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="57a4" class="lr ls it na b gy ne nf l ng nh">training_accuracy = []<br/>test_accuracy = []</span><span id="2239" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu"># try c values from 0.001 to 100:<br/></strong>c_settings = np.arange(0.001, 100, 1)</span><span id="b3c9" class="lr ls it na b gy ni nf l ng nh">for i in c_settings:<br/>    <strong class="na iu"># build the model</strong><br/>    clf = LogisticRegression(C=i, multi_class='auto', max_iter=1000)<br/>    clf.fit(X_train, y_train)<br/>    <strong class="na iu"># record training set accuracy</strong><br/>    training_accuracy.append(clf.score(X_train, y_train))<br/>    <strong class="na iu"># record generalization accuracy</strong><br/>    test_accuracy.append(clf.score(X_test, y_test))</span><span id="eaa8" class="lr ls it na b gy ni nf l ng nh">plt.plot(c_settings, training_accuracy, label="training accuracy")<br/>plt.plot(c_settings, test_accuracy, label="test accuracy")<br/>plt.legend()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/fca3198aeb7bab9592a6765936287336.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*UY6J4U2FelnywviDpfHRpg.png"/></div></figure><p id="4294" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在图表中，我们看到非常接近于<strong class="kw iu"> <em class="mv"> C </em> </strong> =1的值几乎没有改善。所以我们会保持这个值不变。</p><p id="0810" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以使用“ovr”技术进行同样的尝试:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="2351" class="lr ls it na b gy ne nf l ng nh">training_accuracy = []<br/>test_accuracy = []</span><span id="5126" class="lr ls it na b gy ni nf l ng nh"><strong class="na iu"># try c values from 0.001 to 5:</strong><br/>c_settings = np.arange(0.001, 5, 0.5)</span><span id="7680" class="lr ls it na b gy ni nf l ng nh">for i in c_settings:<br/>    <strong class="na iu"># build the model</strong><br/>    clf = LogisticRegression(C=i, multi_class='ovr', max_iter=1000)<br/>    clf.fit(X_train, y_train)<br/>    <strong class="na iu"># record training set accuracy</strong><br/>    training_accuracy.append(clf.score(X_train, y_train))<br/>    <strong class="na iu"># record generalization accuracy</strong><br/>    test_accuracy.append(clf.score(X_test, y_test))</span><span id="ed60" class="lr ls it na b gy ni nf l ng nh">plt.plot(c_settings, training_accuracy, label="training accuracy")<br/>plt.plot(c_settings, test_accuracy, label="test accuracy")<br/>plt.legend()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/5a28afc1541de0828f7bb27e65816526.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*ULE1uM14XpxQOK_UOKlw5A.png"/></div></figure><p id="93ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">One-vs-rest在<strong class="kw iu"> <em class="mv"> C </em> </strong> =0.5时似乎表现更好。</p><p id="a859" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">构建模型的可视化:</strong></p><p id="e4f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们可以用决策边界为我们的模型构建一个图形化的可视化。由于2D最容易解释，我们将只使用两个特征来构建图表。在EDA过程中，我们发现FeNO和BD是区分能力较强的变量。</p><blockquote class="nr ns nt"><p id="9358" class="ku kv mv kw b kx ky ju kz la lb jx lc nu le lf lg nv li lj lk nw lm ln lo lp im bi translated">对于多项式:</p></blockquote><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="57b3" class="lr ls it na b gy ne nf l ng nh">def logisticReg_comparison(data,c):<br/>    x = data[['BD','FeNO',]].values<br/>    y = data['dx'].astype(int).values<br/>    LogReg = LogisticRegression(C=c, multi_class='multinomial',<br/>                                                      max_iter=1000)<br/>    LogReg.fit(x,y)<br/>    print(LogReg.score(x,y))<br/>    <strong class="na iu">#Plot decision region:</strong><br/>    plot_decision_regions(x,y, clf=LogReg, legend=1)<br/>    <strong class="na iu">#Adding axes annotations:</strong><br/>    plt.xlabel('X_train')<br/>    plt.ylabel('y_train')<br/>    plt.title('LogReg with C='+str(c))<br/>    plt.show()</span><span id="d06c" class="lr ls it na b gy ni nf l ng nh">logisticReg_comparison(data,1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/204481d4e00ea72cdab2eec06fe70dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*mQLtU-Iu9UoQdFZil8Ysgw.png"/></div></figure><blockquote class="nr ns nt"><p id="4465" class="ku kv mv kw b kx ky ju kz la lb jx lc nu le lf lg nv li lj lk nw lm ln lo lp im bi translated">对于一比一休息:</p></blockquote><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7db7" class="lr ls it na b gy ne nf l ng nh">def logisticReg_comparison(data,c):<br/>    x = data[['BD','FeNO',]].values<br/>    y = data['dx'].astype(int).values<br/>    LogReg = LogisticRegression(C=c, multi_class='ovr',<br/>                                               max_iter=1000)<br/>    LogReg.fit(x,y)<br/>    print(LogReg.score(x,y))<br/>    <strong class="na iu">#Plot decision region:</strong><br/>    plot_decision_regions(x,y, clf=LogReg, legend=1)<br/>    <strong class="na iu">#Adding axes annotations:</strong><br/>    plt.xlabel('X_train')<br/>    plt.ylabel('y_train')<br/>    plt.title('LogReg with C='+str(c))<br/>    plt.show()</span><span id="6ec7" class="lr ls it na b gy ni nf l ng nh">logisticReg_comparison(data,0.5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f5af430deaf9634fd19634db0158cf26.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*54H-8GEzIanFLhfXZWTkaQ.png"/></div></figure><p id="0f8b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感谢您的阅读！如果您有任何更正或建议，请告诉我，不要忘记订阅以接收关于我未来出版物的通知。</p><p id="2f2b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你喜欢这篇文章，别忘了关注我，这样你就能收到所有关于新出版物的更新。</p><p id="fe7f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你想了解更多，你可以通过<a class="ae lq" href="https://cdanielaam.medium.com/membership" rel="noopener">我的推荐链接</a>订阅媒体会员。它不会花你更多的钱，但会支付我一杯咖啡。</p><p id="92b6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">其他:</strong>谢谢！</p></div></div>    
</body>
</html>