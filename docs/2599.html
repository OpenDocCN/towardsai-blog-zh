<html>
<head>
<title>Simple, Good Sentiment Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单，好的情感分类</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/simple-good-sentiment-classification-726984bd73d4?source=collection_archive---------3-----------------------#2022-03-07">https://pub.towardsai.net/simple-good-sentiment-classification-726984bd73d4?source=collection_archive---------3-----------------------#2022-03-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="55f9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用多项式的情感分类</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ad45a8175990dbaa2714a34fe6b6ec46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*upiE1X-ZssIJzYtNDk_dfw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">坦纳·博瑞克在Unsplash<a class="ae ky" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">拍摄的照片</a></figcaption></figure><p id="1bd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我引用了一篇最先进的论文[1]，该论文使用多项式NB对情感进行分类。读完这篇文章后，您将能够体会到朴素贝叶斯这样简单的技术在一些分类任务中是多么的有效。</p><h2 id="0e5e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">什么是多项式NB？</h2><p id="cb41" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在他的书[2]中，<em class="mt"> Manning等人</em>介绍了多项式朴素贝叶斯或MNB模型，一种概率学习方法。根据朴素贝叶斯，文档<em class="mt"> d </em>在类别<em class="mt"> c </em>中的概率计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/bb9238d3fb5a9a5005a4f5c414814d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*D1EmW6fsWUbKJPiaLe6WGQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">资料来源:c . Manning，Raghavan，p .，&amp; Schütze，H. (2008年)。<em class="mv">信息检索简介</em>。剑桥:剑桥大学出版社。doi:10.1017/CBO 978051180971</figcaption></figure><p id="5756" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="mt"> P(tₖ|c) </em>是术语<em class="mt"> tₖ </em>出现在类别<em class="mt"> c </em>文档中的条件概率。我们将<em class="mt"> P(tₖ |c) </em>解释为对<em class="mt"> tₖ </em>贡献了多少证据来证明<em class="mt"> c </em>是正确的类的一种度量。P(c) 是文档出现在类别<em class="mt"> c </em>中的先验概率。<em class="mt"> (t₁，t₂，。。。，tₙ) </em>是<em class="mt"> d </em>中的记号，是我们用于分类的词汇的一部分。例如，单句文档<em class="mt">西雅图不是晴天</em>的标记可能是{ <em class="mt">西雅图</em>，<em class="mt">是，不是，晴天} </em>，n = 4。</p><h2 id="4967" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">多项式NB启发的情感分类器</h2><p id="6093" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在论文[1]中，研究人员使用MNB(以及其他类似的算法)来预测不同数据集的情绪。他们使用计数向量来记录文档中的字数。</p><p id="0709" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我首先尝试了一个简单的例子来展示如何填充计数向量，然后是如何进行预测。最后，我在一个相当大的数据集中使用了相同的方法对文档进行分类。</p><h2 id="35f5" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">定义p和q</h2><p id="51bc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在本文中，我们假设任何文档<em class="mt"> d </em>都可以归为这两类中的一类。文件以客户评论的形式出现，类别为<em class="mt">正面</em>和<em class="mt">负面</em>。我们定义词汇表<code class="fe mw mx my mz b"><em class="mt">V</em></code>，它是数据集中所有标记的通用集合。</p><p id="2a2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过下面给出的公式定义<code class="fe mw mx my mz b"><em class="mt">p </em></code>。简单来说，<code class="fe mw mx my mz b"><em class="mt">p</em></code>是一个向量，为<strong class="lb iu">正面评论</strong>保存词汇<code class="fe mw mx my mz b"><em class="mt">V</em></code> <em class="mt"> </em>中每个token出现的次数。这里的<code class="fe mw mx my mz b"><strong class="lb iu"><em class="mt">f</em></strong></code> <strong class="lb iu"> </strong>是<em class="mt"> 1 </em>如果令牌出现在<code class="fe mw mx my mz b"><strong class="lb iu"><em class="mt">i</em></strong></code> <strong class="lb iu"> <em class="mt"> </em> </strong>否则<em class="mt"> 0。</em> <code class="fe mw mx my mz b"><em class="mt">α</em></code> <strong class="lb iu"> <em class="mt"> </em> </strong>为平滑因子(取为1)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5f82930e71d06335e9125e26a5279cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*RWHI4IRKBLgqtIOkVnM07A.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者:计算<strong class="bd lx"> p </strong>计数向量</figcaption></figure><p id="e8aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，我们将<code class="fe mw mx my mz b"><em class="mt">q</em></code>定义为针对<strong class="lb iu">负面</strong>评论的词汇表中每个单词出现的计数向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/27718a4fb0dc93dd6f75b3641e02f277.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*783-2sBeoiyziyu_3Osy7w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者:计算<strong class="bd lx"> q </strong>计数向量</figcaption></figure><h2 id="d3d0" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">生成p和q向量的示例</h2><p id="80ce" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">让我们通过一个例子来理解p和q值。为了简单起见，我采取了一个审查，每一个相应的积极和消极的类别。</p><ol class=""><li id="7866" class="nc nd it lb b lc ld lf lg li ne lm nf lq ng lu nh ni nj nk bi translated">我们用<code class="fe mw mx my mz b"><em class="mt">α</em></code> <em class="mt"> </em> (1)初始化<code class="fe mw mx my mz b"><em class="mt">p</em></code>和<code class="fe mw mx my mz b"><em class="mt">q</em></code>字典中的记号</li><li id="464c" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">我们创造并列举词汇<code class="fe mw mx my mz b"><em class="mt">V</em></code> <em class="mt">。</em></li><li id="7070" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">我们遍历每一个正面评论，如果评论中有令牌，则递增<code class="fe mw mx my mz b"><em class="mt">p</em></code>中令牌的计数。</li><li id="7c92" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">当迭代负面评论时，我们类似地填充<code class="fe mw mx my mz b"><em class="mt">q</em></code>字典。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">用于填充p和q计数向量的代码片段</figcaption></figure><p id="f63e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的例子为词汇<em class="mt"> V. </em>产生了下面的值<em class="mt"> p </em>和<em class="mt"> q </em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">词汇的p和q向量</figcaption></figure><h2 id="e34f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">理解r矢量</h2><p id="26d0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">一旦我们有了<code class="fe mw mx my mz b"><em class="mt">p</em></code>和<code class="fe mw mx my mz b"><em class="mt">q</em></code>，我们通过下面的公式最终定义<code class="fe mw mx my mz b"><em class="mt">r</em></code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b99e8ada816fc7774ed3620fe755cbb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*KStml_kwZpgtf_fYIWd-aw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:填充r向量的计算</figcaption></figure><p id="3723" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mt"> r </em>试图通过找到该表征的正(<em class="mt"> p </em>)和负(<em class="mt"> q </em>)出现之间的归一化对数差来捕捉表征的情感。简单来说，如果<code class="fe mw mx my mz b"><em class="mt">r</em> &lt; 0</code>，这意味着令牌似乎具有负面情绪，如果<code class="fe mw mx my mz b"><em class="mt">r</em> &gt; 0</code>，那么它具有正面情绪。</p><p id="7669" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面这段python代码试图为<em class="mt"> V. </em>中的每个标记计算<em class="mt"> r </em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">计算r向量</figcaption></figure><p id="1057" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于上述例子，下表说明了对应于词汇表<code class="fe mw mx my mz b"><em class="mt">V</em></code>中每个标记的r值。我们可以仔细观察到，令牌如<em class="mt">简单</em>和<em class="mt">好</em>有<strong class="lb iu">正</strong>T3】，而令牌如<em class="mt">确实</em>和<em class="mt">坏</em>有<strong class="lb iu">负</strong> <code class="fe mw mx my mz b"><em class="mt">r</em></code>值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">词汇的r向量值</figcaption></figure><h2 id="ab32" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">介绍分类模型</h2><p id="5753" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">一旦计算出了<code class="fe mw mx my mz b"><em class="mt">r</em></code>值，论文就使用一个简单的线性模型来预测评论的情绪。该模型描述如下。我们通过采用朴素贝叶斯公式(在文档开始时定义)并对两边取对数，最终得到这个线性模型。我们这样做是因为许多条件概率是成倍增加的，一个条件概率对应一篇评论中的一个标记。这可能导致浮点下溢。因此，通过增加概率的对数而不是乘以概率来执行计算是更好的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ec87a8d7a03a6fa3846e60a416360325.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*A_ICYAANwVOzXic7_xNcPQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像:文档分类的线性模型</figcaption></figure><p id="06bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，<br/><em class="mt">b = log(N+/N-)<br/>w = r<br/>f =大小为|V| </em>的向量</p><p id="3d97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<em class="mt"> N+ </em>和<em class="mt"> N- </em>是正面和负面评论的计数。下面的例子将有助于更好地理解模型变量。同样，为了简单起见，我们采取了另一个测试审查<code class="fe mw mx my mz b">test_review</code>进行分类。我们使用词汇表<code class="fe mw mx my mz b"><em class="mt">V</em></code>和上例中计算出的<code class="fe mw mx my mz b"><em class="mt">r</em></code>值(将其视为训练集)对<code class="fe mw mx my mz b">test_review</code>进行分类。</p><p id="9288" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过将词汇表<code class="fe mw mx my mz b"><em class="mt">V</em></code> <em class="mt"> </em>中出现在<code class="fe mw mx my mz b">test_review</code>中的所有记号标记为<em class="mt"> 1 </em>，其余标记为<em class="mt"> 0，来创建大小为<code class="fe mw mx my mz b"><em class="mt">|V|</em></code>的向量<code class="fe mw mx my mz b"><em class="mt">f</em></code>。</em>我们定义<code class="fe mw mx my mz b"><em class="mt">w = r</em></code> <em class="mt">、</em>最后计算转置<code class="fe mw mx my mz b"><em class="mt">w</em></code>和<code class="fe mw mx my mz b"><em class="mt">f</em></code>的点积。简单地说，我们只是对出现在词汇<code class="fe mw mx my mz b"><em class="mt">V</em></code> <em class="mt">中的<code class="fe mw mx my mz b">test_review </code>中的所有记号的<code class="fe mw mx my mz b"><em class="mt">r</em></code>值求和。</em>python脚本计算<code class="fe mw mx my mz b"><em class="mt">y</em></code>并使用其符号来确定评论应该被分类为正面还是负面。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">预测班级</figcaption></figure><pre class="kj kk kl km gt nu mz nv nw aw nx bi"><span id="a19f" class="lv lw it mz b gy ny nz l oa ob">V = ['This', 'is', 'a', 'simple', 'good', 'review', 'indeed', 'bad']<br/>w = [ 0.    0.    0.    0.69  0.69  0.   -0.69 -0.69]<br/>f = [1, 1, 1, 0, 0, 1, 0, 1]<br/>b = 0.0<br/>y = -0.69<br/>Predicted a negative review</span></pre><h2 id="c49b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">在大型数据集上建模</h2><p id="3e2e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我在IMDb评论数据集[3]上使用了上述方法(做了一些改进)。数据集中有25k条正面和25k条负面评论。我构建模型的步骤[4](如论文中所述)是:</p><ol class=""><li id="2539" class="nc nd it lb b lc ld lf lg li ne lm nf lq ng lu nh ni nj nk bi translated"><strong class="lb iu">数据预处理:</strong>我只从评论内容中去掉标点、数字、特殊字符。我做了<strong class="lb iu">而不是</strong>清除停用词或使标记词条化。</li><li id="4f87" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated"><strong class="lb iu">标记化:</strong>我使用了两种不同的标记变体——单字和双字[5]。</li><li id="9e81" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated"><strong class="lb iu">计算</strong> <code class="fe mw mx my mz b"><strong class="lb iu"><em class="mt">p, q, r</em></strong></code> <strong class="lb iu"> : </strong>这些向量是使用本文前面描述的方法计算的。</li><li id="8831" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated"><strong class="lb iu">预测类别:</strong>上述线性模型用于计算<code class="fe mw mx my mz b"><em class="mt">y</em></code>。</li></ol><p id="1b92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果与论文中发表的结果一致。<br/> -一元模型的精确度为<strong class="lb iu"> 0.85 </strong> <br/> -二元模型的精确度明显更高，为<strong class="lb iu"> 0.88 </strong></p><p id="6774" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里找到IMDb评论的分类代码。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="e8c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您抽出时间阅读本文。受最近ML研究论文的启发，我计划每月至少发表两篇文章。敬请关注。✌️</p><p id="d888" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您对数据可视化和讲故事感兴趣，这篇文章可能对您有用。</p><div class="oj ok gp gr ol om"><a href="https://towardsdatascience.com/arrows-in-python-plots-51fb27d3077b" rel="noopener follow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd iu gy z fp or fr fs os fu fw is bi translated">python图中的箭头</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">matplotlib图中箭头和文本注释的介绍</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa ks om"/></div></div></a></div><h2 id="99e2" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="21b7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1]:基线和二元模型<br/><a class="ae ky" href="https://aclanthology.org/P12-2018.pdf" rel="noopener ugc nofollow" target="_blank">https://aclanthology.org/P12-2018.pdf</a></p><p id="8093" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]:信息检索导论<br/> Manning，c .，Raghavan，p .，&amp; Schütze，H. (2008)。<em class="mt">信息检索概论</em>。剑桥:剑桥大学出版社。doi:10.1017/CBO 978051180971</p><p id="418e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]:大型电影评论数据集<a class="ae ky" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">http://ai.stanford.edu/~amaas/data/sentiment/</a></p><p id="5c31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4]:https://www.kaggle.com/pratishgupta915/notebookfb2823693f为IMDb分类的笔记本<br/></p><p id="10ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5]:n-gram<br/><a class="ae ky" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/N-gram</a></p></div></div>    
</body>
</html>