<html>
<head>
<title>Bias-variance Decomposition 101: Step-by-Step Computation.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">偏差-方差分解101:逐步计算。</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/bias-variance-decomposition-101-a-step-by-step-computation-9d5f3694877?source=collection_archive---------0-----------------------#2021-10-20">https://pub.towardsai.net/bias-variance-decomposition-101-a-step-by-step-computation-9d5f3694877?source=collection_archive---------0-----------------------#2021-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ea24ee683f0d0d8f49e56b649b9bad0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hxatlCK3bZYsRMSu"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kc" href="https://unsplash.com/@bekkybekks?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Bekky Bekks </a>拍摄的照片</figcaption></figure><p id="f8bf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你听说过ML中的“<strong class="kf ir">偏差-方差困境”</strong>吗？我敢肯定你的答案是<em class="lb">是的</em>如果你正在这里阅读这篇文章:)，而且我还能肯定一点:你在这里是因为你希望最终找到达到著名的<strong class="kf ir">最佳权衡的终极方法。</strong></p><p id="7c92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嗯，我仍然没有这个神奇的子弹，但我今天在这篇文章中可以提供给你的是一种分析你的ML算法的错误的方法，把它分成三个部分，因此得到一个<strong class="kf ir">理解和具体解决偏差-方差困境的直接方法</strong>。</p><p id="71ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还将以最简单的方式一步一步地执行所有推导，因为<strong class="kf ir"> <em class="lb">没有数学，就不可能完全理解偏差和方差分量之间的关系，从而采取行动建立我们的最佳模型！</em>T15】</strong></p><h1 id="9167" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">目录</h1><ol class=""><li id="18a0" class="ma mb iq kf b kg mc kk md ko me ks mf kw mg la mh mi mj mk bi translated">介绍和注释</li><li id="6fea" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la mh mi mj mk bi translated">逐步计算</li><li id="9047" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la mh mi mj mk bi translated">图形视图和著名的权衡</li><li id="07aa" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la mh mi mj mk bi translated">过拟合、欠拟合、合奏和具体应用</li><li id="1a31" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la mh mi mj mk bi translated">结论</li><li id="d385" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la mh mi mj mk bi translated">参考</li></ol></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="6771" class="lc ld iq bd le lf mx lh li lj my ll lm ln mz lp lq lr na lt lu lv nb lx ly lz bi translated">1.介绍和注释</h1><p id="ecc6" class="pw-post-body-paragraph kd ke iq kf b kg mc ki kj kk md km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">在本文中，我们将分析训练数据变化时ML算法的错误行为，并且我们将理解<strong class="kf ir">偏差和方差分量如何成为在我们的假设空间中选择最佳模型的关键点。</strong></p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><p id="6651" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi nf translated">当我们改变数据集时，给定的模型会发生什么？错误是如何表现的？想象拥有并维护相同的模型架构(例如，一个简单的MLP)，并假设训练集的一些<strong class="kf ir">变化。我们的目的是识别和分析模型在学习和复杂性方面发生了什么。</strong></p><p id="ce0b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法提供了一种替代方法(通用经验风险方法)来估计测试误差。</p><h2 id="0ac9" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">我们在这篇文章中要做的。</h2><p id="3dc6" class="pw-post-body-paragraph kd ke iq kf b kg mc ki kj kk md km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">给定训练集的变化，我们<strong class="kf ir">将测试集</strong>的某个点<em class="lb"> x </em>的预期误差分解为三个元素:</p><ul class=""><li id="9393" class="ma mb iq kf b kg kh kk kl ko oa ks ob kw oc la od mi mj mk bi translated"><strong class="kf ir">偏差</strong>，它量化了(未知)真函数<em class="lb"> f(x) </em>和我们的假设(模型)<em class="lb"> h(x) </em>之间的<strong class="kf ir">差异</strong>，对数据进行平均。它相当于一个系统误差。</li><li id="c0d2" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><strong class="kf ir">方差</strong>，针对训练数据的不同实现，量化模型<em class="lb"> h </em>的响应的<strong class="kf ir">可变性(训练集的变化导致非常不同的解决方案)。</strong></li><li id="7927" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><strong class="kf ir">噪声</strong>因为标签<em class="lb"> d </em>包含随机误差:对于一个给定点<em class="lb"> x </em>有不止一个可能的<em class="lb"> d </em>(即，即使对输入中的同一点<em class="lb"> x </em>进行采样，也不会获得相同的目标<em class="lb"> d </em>)。这意味着即使是最优解也可能是错误的！</li></ul><blockquote class="oe of og"><p id="38e8" class="kd ke lb kf b kg kh ki kj kk kl km kn oh kp kq kr oi kt ku kv oj kx ky kz la ij bi translated"><strong class="kf ir">注意:</strong>不要将本上下文中使用的“偏差”术语与同一单词的其他用法混淆，以表示ML中完全不同的概念(例如，感应偏差、神经单元的偏差)。</p></blockquote><h2 id="f3d6" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">背景和情景</h2><p id="7a94" class="pw-post-body-paragraph kd ke iq kf b kg mc ki kj kk md km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">我们在一个<strong class="kf ir">监督的</strong>学习集中，特别地，我们假设一个<strong class="kf ir">回归</strong>任务场景，目标<em class="lb"> y </em>和<strong class="kf ir">平方误差损失</strong>。</p><p id="84c7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们假设数据点是从一个唯一(且未知)的潜在概率分布<strong class="kf ir"> <em class="lb"> P </em> </strong>中抽取<em class="lb"> i.i.d. </em>(独立且同分布)。</p><p id="4c91" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们有例子&lt;<strong class="kf ir"> x </strong>，<em class="lb"> y </em> &gt;其中真(未知)函数为</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/2692df2621b0aaba0bbcb1aa2d31c342.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*npqNhEuKJ459Caw2tqYD_w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">指标函数</figcaption></figure><p id="6f47" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中ɛ是具有零均值和标准差<em class="lb"> σ的高斯噪声。</em></p><p id="c87f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在线性回归中，给定一组例子&lt;<strong class="kf ir"> x_i </strong>，<em class="lb"> y_i </em> &gt;(其中<em class="lb"> i = 1，…，l) </em>我们拟合一个线性假设<em class="lb">h(</em><strong class="kf ir"><em class="lb">)x</em></strong>【T52)=<strong class="kf ir"><em class="lb">wx</em></strong><em class="lb">+w _ 0</em>等以使误差平方和最小</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi op"><img src="../Images/2ef4853484adc99881fd521facdb2445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*IhjipFmhAjBcXKfU9o9Tdw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">训练数据的平方和误差(误差函数)</figcaption></figure><p id="f9d7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于错误函数，我们还提供了一个Python代码片段。</p><figure class="ol om on oo gt jr"><div class="bz fp l di"><div class="oq or l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">作者的平方误差损失公式的代码片段</figcaption></figure><p id="6440" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">值得指出两个有用的观察结果:</p><ul class=""><li id="8f2b" class="ma mb iq kf b kg kh kk kl ko oa ks ob kw oc la od mi mj mk bi translated">由于我们为某个函数<em class="lb"> f </em>选择的假设类别(线性的),我们将会有一个系统的预测误差(即偏差)。</li><li id="313b" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated">根据我们拥有的数据集，找到的参数<em class="lb">和</em>会有所不同。</li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="fd44" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">完整概述我们场景的图形示例:</h2><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi os"><img src="../Images/9912d231eb855f5d56cfc9293471ba7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jlmCtcQwQrjjK-fdaEEK1g.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图1(图片来自作者)</figcaption></figure><p id="de6a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<strong class="kf ir">图1 </strong>中，很容易看到在真实函数(曲线)<strong class="kf ir"> <em class="lb"> y = f(x) + ε </em> </strong>上采样的20个点。所以我们只知道原始分布的20个点。我们的假设是试图逼近数据的线性假设。</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ot"><img src="../Images/8397834f1505f9fc394e53f6bd8cdc12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MiVM3dN0mW07we__DTulFQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图2(图片来自作者)</figcaption></figure><p id="f1cb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<strong class="kf ir">图2 </strong>中，我们使用不同的数据样本进行了50次拟合，每次拟合20个点(即改变训练集)。<strong class="kf ir">根据不同的训练数据得到不同的模型(线)。</strong>不同的数据集导致不同的假设，即不同的(线性)模型。</p><p id="9c51" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">我们的观点:了解</em> <strong class="kf ir"> <em class="lb">模型的误差如何根据不同的训练集变化</em> </strong> <em class="lb">？</em></p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="576f" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">我们的目标</h2><p id="5c99" class="pw-post-body-paragraph kd ke iq kf b kg mc ki kj kk md km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">给定一个新的数据点<em class="lb"> x </em>，预期的预测误差是多少？我们分析的<strong class="kf ir">目标是计算任意新点<em class="lb"> x </em>，</strong></p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/d8eb13bd7f137b36b1207c1dc9487a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*c_GyYrIWBaZaEUpFDPx7pg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">根据误差的概率分布P的期望误差</figcaption></figure><p id="79b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中期望是根据<strong class="kf ir"> <em class="lb"> P </em> </strong>绘制的整体训练集。</p><p id="9c07" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，每个不同的“提取”训练集都有不同的<em class="lb"> h </em>(和<em class="lb"> y </em>)。</p><p id="815a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">我们将把这种期望分解为上述三个部分，分析它们如何影响误差，以及如何利用这种p.o.v .来建立和改进有效的ML模型。</em></p><h1 id="2fc1" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">2.逐步计算</h1><h2 id="154f" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">2.1召回基础统计</h2><p id="0bb7" class="pw-post-body-paragraph kd ke iq kf b kg mc ki kj kk md km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">设Z是一个<strong class="kf ir">离散随机变量</strong>，有可能值<em class="lb"> z_i </em>，其中<em class="lb"> i = 1，…，l </em>，概率分布P(Z)。</p><ul class=""><li id="e728" class="ma mb iq kf b kg kh kk kl ko oa ks ob kw oc la od mi mj mk bi translated"><strong class="kf ir">期望值</strong>或<strong class="kf ir">是指<strong class="kf ir"> Z </strong>的</strong></li></ul><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/f13310f4b72d01b79ff5d2440f2ca853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*MOdC1RyLcsgsVeiRKj8CLw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">已知概率分布P，平均值计算为每个i ( <em class="ow"> z_i </em>项<em class="ow"> ) </em>的随机变量的值乘以拥有它的概率(P( <em class="ow"> z_i </em>)</figcaption></figure><ul class=""><li id="3646" class="ma mb iq kf b kg kh kk kl ko oa ks ob kw oc la od mi mj mk bi translated">Z的<strong class="kf ir">方差</strong></li></ul><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/dbd9183ac7b5dce2fe4062cad5319572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*tYtBxHvyha1FrHsXPWDzyg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">方差引理</figcaption></figure><p id="05d5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了更清楚，我们现在将证明后一个公式。</p><h2 id="215d" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">2.2方差证明引理</h2><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oy"><img src="../Images/8b30b2bd6d8675c16380cdb215b64c7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gGA_oE0C3ChtXXW83tN-Gg.png"/></div></div></figure><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/70585781d8f3c6dee2ca5fd919d0d9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjKBEwP8F_pu-eaqMIOlkQ.png"/></div></div></figure><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/66ea559030130064de9135ffd6c867b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*0SocSLIMIHWvMnMwIer_vw.png"/></div></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="f297" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">2.3偏差-方差分解</h2><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pb"><img src="../Images/550e7c957bb13ed1fc49cea3fb135198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6vRaQbXSLdnMp8bKllq1cA.jpeg"/></div></div></figure><p id="c472" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:可以将<em class="lb"> y </em>和<em class="lb"> h(x) </em>之间的乘积的平均值视为平均值的乘积，因为它们是<strong class="kf ir">独立变量</strong>，因为一旦固定了测试集上的点<em class="lb"> x </em>，我们建立的假设(模型)<em class="lb"> h(x) </em>就不依赖于目标<em class="lb"> y </em>(也不依赖于<em class="lb">x<em class="lb"/></em></p><p id="0e44" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pc"><img src="../Images/12e037b48d2ad8c194fc1a949dd7a647.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*MhdPVne9l1cVCqLc8blyoQ.jpeg"/></div></div></figure><p id="f6dc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当使用从<em class="lb"> P </em>中提取的数据对<em class="lb"> h </em>进行训练时，在<em class="lb"> x </em>处的假设上标注<strong class="kf ir">均值预测</strong>(即在训练集的所有不同变量上训练的模型的均值)。所以是我们用不同的训练数据对模型进行不同的训练所能得到的结果的期望值，估计在<em class="lb"> x </em>。现在我们分别考虑<strong class="kf ir">公式(5) </strong>的各项。</p><p id="2e8a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用方差引理<strong class="kf ir">(公式4.1) </strong>，我们有:</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/8476936bcc31b3171b2018277c687ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*Ai8mUxMvH393JLIK7HWoYQ.jpeg"/></div></figure><p id="073d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，对于<strong class="kf ir">公式(0) </strong>，对目标值的期望等于在<em class="lb"> x </em>上评估的目标函数:</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/f7b3ba2e2f55c7bb22e93af2719c2ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*_ZHMclam_RMsE32fSxdcuQ.jpeg"/></div></figure><p id="80d7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为根据定义，噪声ɛ具有零均值，并且因为真实函数<em class="lb"> f(x) </em>被假设为已知，所以对它的期望仅仅是它本身。为此，我们可以写:</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/f368de7312a29f48e664049ea6e664eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*IHRX6wOE5cGZIOCfTPc9mA.jpeg"/></div></figure><p id="106b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于剩下的(第三)项，因为分别是<strong class="kf ir">【公式(6)</strong><strong class="kf ir">【公式(3) </strong>。我们可以简单地把它改写成</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/69188a31c27d51b530d7ce8fa5af07e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*Mw-6ZMjBrXprwxaKRHoWZg.png"/></div></figure><p id="f333" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">把所有东西放在一起，重新排序，我们可以写出初始方程，即<strong class="kf ir">式(2) </strong>:</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/d24e5e3ebeea465367616163e3bf22b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*GjiUuhGANdqe0HtU79DWjQ.png"/></div></figure><p id="aff7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简单来说就是:</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/9aeec8fb4559ace923ba0a1e68543a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*LFOR2mi35cLWtZplnA191Q.png"/></div></figure><p id="1afc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很容易看出，红色的项构成了二项式的一个正方形。为了进一步简化，再次对术语进行重新排序，并以紧凑的方式重写二项式的平方:</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pj"><img src="../Images/4242b71b4bc944825a9f24628d1ec181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_f80Tc0o9iXkEkNgGAR3rQ.png"/></div></div></figure><p id="37ad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">公式(7)的树项正是我们正在寻找的三个组成部分:</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/e1d7823b57a2b74d3423e23e26a51f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*ZfUuwsBOd1i-oEPLtDnAdg.png"/></div></figure><p id="687e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预期预测误差现在最终分解为</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/f4922a18ecb238f3c052b4c0c5251e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*pMHDK2ip6XU7msv4DbiR-w.png"/></div></figure><p id="3232" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并且，由于噪声根据定义(公式(0))具有零均值，我们可以写出<strong class="kf ir">公式(8):偏差-方差分解结果。</strong></p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/1138dfd5554f0169599e73790c55cbfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*_i0CcoPUaatHcMnOTpWGtg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">偏差-方差分解结果</figcaption></figure><blockquote class="oe of og"><p id="fce1" class="kd ke lb kf b kg kh ki kj kk kl km kn oh kp kq kr oi kt ku kv oj kx ky kz la ij bi translated"><em class="iq">或者，继</em> <a class="pn po ep" href="https://medium.com/u/bfdc14624165?source=post_page-----9d5f3694877--------------------------------" rel="noopener" target="_blank"> <em class="iq">之后</em> </a> <em class="iq">批注:</em></p><p id="9b55" class="kd ke lb kf b kg kh ki kj kk kl km kn oh kp kq kr oi kt ku kv oj kx ky kz la ij bi translated"><strong class="kf ir">Err(x)<em class="iq">=偏差+方差+不可约误差</em> </strong></p></blockquote></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><p id="1cb2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，<strong class="kf ir">噪声</strong>通常被称为<strong class="kf ir">不可约误差</strong>，因为它依赖于数据，所以无论使用什么算法都不可能消除它(任何模型都无法从根本上减少)。</p><p id="d2c4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相反，<strong class="kf ir">偏差和方差</strong>是<strong class="kf ir">可减少的误差，因为</strong>我们可以尝试尽可能地最小化它们。</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pp"><img src="../Images/9a6dee20747735afee578d6a2ce118e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_dJaxwprhmSjf93XWAacTw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图片来自<a class="pn po ep" href="https://medium.com/u/370952daf49?source=post_page-----9d5f3694877--------------------------------" rel="noopener" target="_blank"> Opex分析</a></figcaption></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="a5ad" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">2.4每个术语的含义</h2><ul class=""><li id="7fb5" class="ma mb iq kf b kg mc kk md ko me ks mf kw mg la od mi mj mk bi translated"><strong class="kf ir">方差项</strong>被定义为每个单一假设(模型)和所有不同假设(从不同训练集获得的不同模型)的平均值之间差异的期望值。</li><li id="6785" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><strong class="kf ir">偏差项</strong>定义为所有假设的平均值(即从不同训练集获得的所有可能模型的平均值)与点<em class="lb"> x. </em>上的目标值之间的差异</li><li id="7b0f" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><strong class="kf ir">噪声项</strong>定义为在<em class="lb"> x </em>上计算的目标值和目标函数之差的期望值(即该分量实际上对应于噪声的方差)。</li></ul><p id="3016" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">现在你已经具备了所有的要素，可以再次阅读第1节(简介)中给出的这三个组成部分的定义，并完全理解和欣赏它们了！</em></p><h1 id="e87f" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">3.图形视图和著名的权衡</h1><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi os"><img src="../Images/986600dfc628324fb47afa39e920b443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4xGH77BZ697W4C0w_BQozA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图3描述了通过击中目标上的点的偏差和变化。目标上的每个点代表模型的不同迭代，适合不同的训练数据集。(<a class="ae kc" href="https://community.alteryx.com/t5/Data-Science/Bias-Versus-Variance/ba-p/351862?lightbox-message-images-351862=52874iE986B6E19F3248CF" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="2329" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理想情况下，你希望看到既有<strong class="kf ir">低方差又有</strong>低偏差的情况，如图<strong class="kf ir">图3 </strong>(任何有监督机器学习算法的目标)。然而，在最优偏差和最优方差之间通常存在一个<strong class="kf ir">权衡</strong>。机器学习算法的参数化通常是一场平衡两者的战斗，因为<strong class="kf ir">偏差和方差之间的关系是不可避免的:增加偏差会减少方差，反之亦然。</strong></p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/0a13840e87a6d94d15c819751b4e20a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*VYhZxSxsl_83uqt_WLDQCg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">偏差、方差和模型复杂度之间的关系。(<a class="ae kc" href="https://community.alteryx.com/t5/image/serverpage/image-id/52875iEF64CA7B6C0D4F43/image-size/large?v=v2&amp;px=999" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="71c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://medium.com/p/9a60f697239" rel="noopener">处理偏差和方差实际上就是处理过度拟合和欠拟合</a>。与模型复杂性相关的偏差减少，方差增加(见<strong class="kf ir">图4 </strong>)。随着越来越多的参数被添加到模型中，模型的复杂性增加，方差成为我们主要关注的问题，而偏差稳步下降。换句话说，偏差有一个负的一阶导数来响应模型的复杂性，而方差有一个正斜率。</p><p id="0339" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理解偏差和方差对于理解预测模型的行为至关重要，但是一般来说<strong class="kf ir">你真正关心的是总体误差</strong>，而不是具体的分解。任何模型的最佳点都是复杂程度，在这个复杂程度上，偏差的增加相当于方差的减少。</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/be54d7f29142089b92ab9b218aa73982.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*3CvUHLxClSVRzJcO_Sy4_A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">公式9:偏差和方差的关系。(<a class="ae kc" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="6988" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi nf translated">这就是我们谈论取舍的原因！<strong class="kf ir">一个增加，另一个减少，</strong>反之亦然。这正是它们之间的关系，我们的推导有助于把我们带到这里。<strong class="kf ir">没有数学表达式，就不可能理解这些组件之间的联系，从而采取行动构建我们的最佳模型！</strong></p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><p id="7bc3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为最后一步，我们来介绍三个误差分量的图形表示。</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/068e437fdc4d0ccca9f2f7024c9fa495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*75xl-sZI-y6tpi_0YbIX-g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">误差分解为偏差、方差和噪声:图形视图(图片由作者提供)。</figcaption></figure><p id="f4da" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很容易识别:</p><ul class=""><li id="b517" class="ma mb iq kf b kg kh kk kl ko oa ks ob kw oc la od mi mj mk bi translated"><strong class="kf ir">偏差</strong>是我们获得的解(红色)和数据平均真实解(白色)之间的差异(箭头)。</li><li id="6fb3" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><strong class="kf ir">方差</strong>(深蓝色圆圈)<strong class="kf ir"> </strong>如果使用不同的训练数据，目标函数将改变多少，即，对数据集中的小方差的敏感度(如果使用不同的数据集，给定数据点的估计将改变多少)。</li></ul><p id="28e0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，如果偏差增加，方差减少，反之亦然。</p><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ps"><img src="../Images/50ddebc866966c73534d82d31308a418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LHE18cLy8nvIe391iu1oMw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">偏倚和方差的反比关系(图片由作者提供)。</figcaption></figure><p id="d2d5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很容易用图形验证我们之前所说的关于<strong class="kf ir">噪声</strong>的内容:由于它依赖于数据，所以没有办法修改它(这就是为什么它被称为<strong class="kf ir">不可约误差</strong>)。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><p id="87a6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，如果偏差和方差都会导致误差，请记住<strong class="kf ir">您想要最小化的是(整体)预测误差</strong>，而不是偏差或方差。</p><h1 id="724b" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">4.过拟合、欠拟合、合奏和具体应用</h1><figure class="ol om on oo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pp"><img src="../Images/cce0bf94b4070b8eca0d4cdb7571b1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ndS-I5hRBO9SK0JkyH9X1Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图片来自<a class="pn po ep" href="https://medium.com/u/dc327e50cf35?source=post_page-----9d5f3694877--------------------------------" rel="noopener" target="_blank"> Juhi Ramzai </a></figcaption></figure><p id="2174" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我们的最终目的是建立最好的可能的ML模型，如果不与这些相关的含义和相关的主题联系起来，这种推导几乎是无用的:</p><ol class=""><li id="97a4" class="ma mb iq kf b kg kh kk kl ko oa ks ob kw oc la mh mi mj mk bi translated"><a class="ae kc" href="https://medium.com/p/9a60f697239" rel="noopener"> <strong class="kf ir">模型复杂度和正则化中的偏差和方差。</strong></a>ML+中偏差方差和<em class="lb">λ</em>项的相互作用及其对欠拟合和过拟合的影响。</li><li id="2e3c" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la mh mi mj mk bi translated"><a class="ae kc" href="https://medium.com/p/8d4b985fb011" rel="noopener"> <strong class="kf ir">更多关于著名的偏差-方差权衡</strong> </a></li><li id="0084" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la mh mi mj mk bi translated"><strong class="kf ir">一个处理偏差和方差的实用实现:</strong> <a class="ae kc" href="https://medium.com/p/7aabb44a97ee" rel="noopener"> <strong class="kf ir">集成方法。</strong> </a></li></ol><blockquote class="pt"><p id="733c" class="pu pv iq bd pw px py pz qa qb qc la dk translated">我已经在最近的文章中谈到了这些话题，所以如果你感兴趣，请点击这里的链接查看below⬇</p></blockquote><ul class=""><li id="7c07" class="ma mb iq kf b kg qd kk qe ko qf ks qg kw qh la od mi mj mk bi translated"><a class="ae kc" href="https://medium.com/p/9a60f697239" rel="noopener"> <em class="lb">不费吹灰之力就明白过拟合和欠拟合</em> </a></li><li id="a5f7" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><a class="ae kc" href="https://medium.com/p/8d4b985fb011" rel="noopener"> <em class="lb">全面总结你能找到的关于ML中偏差和方差的一切</em> </a></li><li id="24c4" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><a class="ae kc" href="https://medium.com/p/7aabb44a97ee" rel="noopener"> <em class="lb">集成学习:利用多样性。</em>T11】</a></li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="1875" class="lc ld iq bd le lf mx lh li lj my ll lm ln mz lp lq lr na lt lu lv nb lx ly lz bi translated">5.结论</h1><p id="fa32" class="pw-post-body-paragraph kd ke iq kf b kg mc ki kj kk md km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">我们今天看到的是一种<strong class="kf ir">纯理论方法</strong>:对其进行推理以充分理解每个误差分量的含义，并有一种具体的方法来接近您的ML算法的模型选择阶段，这非常有趣。但是为了进行计算，人们应该知道真实函数<em class="lb"> f(x) </em>和概率分布<em class="lb"> P. </em></p><blockquote class="oe of og"><p id="79bf" class="kd ke lb kf b kg kh ki kj kk kl km kn oh kp kq kr oi kt ku kv oj kx ky kz la ij bi translated">事实上，我们无法计算真正的偏差和方差误差项，因为我们不知道实际的潜在目标函数。然而，作为一个框架，偏差和方差提供了理解机器学习算法在追求预测性能时的行为的工具。<em class="iq"> ( </em> <a class="ae kc" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="iq">来源</em> </a> <em class="iq"> ) </em></p></blockquote></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><p id="f8ad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望这篇文章和相关的文章能帮助你理解偏倚-方差困境，特别是提供一种快速简单的方法来处理ML模型的选择和优化。我希望您现在将有额外的工具来通过偏差和方差成分来处理和修复您的模型复杂性、灵活性和泛化能力。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="7522" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">如果你喜欢这篇文章，请留下👏🏻。谢谢你。</h2></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="d9d5" class="lc ld iq bd le lf mx lh li lj my ll lm ln mz lp lq lr na lt lu lv nb lx ly lz bi translated">6.参考</h1><h2 id="d6d6" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">主要:</h2><ul class=""><li id="dfd3" class="ma mb iq kf b kg mc kk md ko me ks mf kw mg la od mi mj mk bi translated">比萨大学<a class="ae kc" href="http://pages.di.unipi.it/micheli/" rel="noopener ugc nofollow" target="_blank"> A. Micheli </a>、<a class="ae kc" href="https://didattica.di.unipi.it/en/master-programme-in-computer-science/curricula-2/curriculum-%c2%93artificial-intelligence%c2%94-2/" rel="noopener ugc nofollow" target="_blank">计算机科学硕士课程(人工智能课程)教授的机器学习讲座</a></li><li id="1dde" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated">有关介质参考的完整列表，请查看<a class="ae kc" href="https://medium.com/@d.goglia/list/bias-var-66b0dcb8a389" rel="noopener"> <strong class="kf ir">此链接</strong> </a> <strong class="kf ir">。</strong></li></ul><h2 id="d85b" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">其他:</h2><ul class=""><li id="ebe8" class="ma mb iq kf b kg mc kk md ko me ks mf kw mg la od mi mj mk bi translated"><a class="ae kc" href="https://en.wikipedia.org/wiki/Stuart_Geman" rel="noopener ugc nofollow" target="_blank">葛曼，司徒</a>；比恩斯托克，lie勒内·杜尔萨特(1992年)。<a class="ae kc" href="http://web.mit.edu/6.435/www/Geman92.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lb">神经网络与偏差/方差困境</em></a>(PDF)。<em class="lb">神经计算</em>。<strong class="kf ir">4</strong>:1–58。<a class="ae kc" href="https://en.wikipedia.org/wiki/Doi_(identifier)" rel="noopener ugc nofollow" target="_blank">doi</a>:<a class="ae kc" href="https://doi.org/10.1162%2Fneco.1992.4.1.1" rel="noopener ugc nofollow" target="_blank">10.1162/neco . 1992 . 4 . 1 . 1</a>。</li><li id="dd14" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated">弗拉基米尔·瓦普尼克(2000年)。<a class="ae kc" href="https://dx.doi.org/10.1007/978-1-4757-3264-1" rel="noopener ugc nofollow" target="_blank"> <em class="lb">统计学习理论的本质</em> </a>。纽约:施普林格出版社。</li><li id="b0cf" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated">格雷格·沙赫纳罗维奇(2011年)。<a class="ae kc" href="https://web.archive.org/web/20140821063842/http://ttic.uchicago.edu/~gregory/courses/wis-ml2012/lectures/biasVarDecom.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lb">线性回归中偏差-方差分解推导的注记</em></a></li><li id="6947" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated">詹姆斯，加雷斯；<a class="ae kc" href="https://en.wikipedia.org/wiki/Daniela_Witten" rel="noopener ugc nofollow" target="_blank">威滕，丹妮拉</a>；<a class="ae kc" href="https://en.wikipedia.org/wiki/Trevor_Hastie" rel="noopener ugc nofollow" target="_blank">哈斯蒂，特雷弗</a>；罗伯特·蒂布拉尼(2013年)。<a class="ae kc" href="http://www-bcf.usc.edu/~gareth/ISL/" rel="noopener ugc nofollow" target="_blank"> <em class="lb">统计学习入门</em> </a>。斯普林格。</li><li id="fae8" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated">机器学习中的偏差和方差——初学者的绝佳指南！ ，analyticsvidhya.com</li><li id="b3ce" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><a class="ae kc" href="https://community.alteryx.com/t5/Data-Science/Bias-Versus-Variance/ba-p/351862" rel="noopener ugc nofollow" target="_blank"> <em class="lb">偏差对方差</em> </a>，在<a class="ae kc" href="https://community.alteryx.com/?category.id=external" rel="noopener ugc nofollow" target="_blank">community.alteryx.com</a></li><li id="2144" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><a class="ae kc" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank"> <em class="lb">理解偏差-方差权衡</em> </a>，作者<a class="pn po ep" href="https://medium.com/u/bfdc14624165?source=post_page-----9d5f3694877--------------------------------" rel="noopener" target="_blank">斯科特·福特曼-罗</a></li><li id="a6e6" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><a class="ae kc" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="lb">温和介绍机器学习中的偏差-方差权衡</em> </a>，machinelearningmastery.com</li><li id="5b41" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><a class="ae kc" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank"> <em class="lb">偏差–方差权衡</em> </a>，在维基百科中</li></ul><h2 id="5d63" class="no ld iq bd le np nq dn li nr ns dp lm ko nt nu lq ks nv nw lu kw nx ny ly nz bi translated">更多有用的来源</h2><p id="d2d0" class="pw-post-body-paragraph kd ke iq kf b kg mc ki kj kk md km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">用<a class="ae kc" href="http://rasbt.github.io/mlxtend/" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> mlxtend </strong> Python库</a>进行偏差方差分解计算:</p><ul class=""><li id="2321" class="ma mb iq kf b kg kh kk kl ko oa ks ob kw oc la od mi mj mk bi translated"><a class="ae kc" href="https://medium.com/analytics-vidhya/calculation-of-bias-variance-in-python-8f96463c8942" rel="noopener"> <em class="lb">用<a class="pn po ep" href="https://medium.com/u/99cf6a63c6f0?source=post_page-----9d5f3694877--------------------------------" rel="noopener" target="_blank"> Nallaperumal </a>计算python </em> </a>中的偏差&amp;方差</li><li id="679a" class="ma mb iq kf b kg ml kk mm ko mn ks mo kw mp la od mi mj mk bi translated"><a class="ae kc" href="https://towardsdatascience.com/bias-variance-decomposition-d0e22d1506b1" rel="noopener" target="_blank"> <em class="lb">偏差方差分解</em> </a> <em class="lb">乘</em><a class="pn po ep" href="https://medium.com/u/f3231bce2410?source=post_page-----9d5f3694877--------------------------------" rel="noopener" target="_blank"><em class="lb">vid hi Chugh</em></a></li></ul><p id="2eed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于偏倚-方差权衡理论的更多内容可以在前面提到的<a class="ae kc" href="https://medium.com/@d.goglia/list/bias-var-66b0dcb8a389" rel="noopener"> <strong class="kf ir">列表</strong> </a>中找到。</p></div></div>    
</body>
</html>