<html>
<head>
<title>PySpark Snowflake Data Warehouse Read Write operations — Part2 (Read-Write)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark雪花数据仓库读写操作—第二部分(读写)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pyspark-snowflake-data-warehouse-read-write-operations-part2-read-write-c129a1260f7f?source=collection_archive---------2-----------------------#2021-02-11">https://pub.towardsai.net/pyspark-snowflake-data-warehouse-read-write-operations-part2-read-write-c129a1260f7f?source=collection_archive---------2-----------------------#2021-02-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1de9" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><p id="0156" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个故事的目标是使用Apache Spark API，Pyspark来理解对雪花数据仓库表的读写操作。作为我上一篇关于PySpark雪花读操作的博客的续篇，这是我目前的博客，我已经介绍了在雪花数据库表上执行写操作的用例。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ku"><img src="../Images/035d64e83264f7fba4e0cbea81b26d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yMttURoMOptxGtJl3Wbdig.png"/></div></div></figure><p id="8268" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">第一部分可以b</p><div class="lg lh gp gr li lj"><a href="https://medium.com/towards-artificial-intelligence/pyspark-snowflake-data-warehouse-read-write-operations-part1-read-only-3331d113635e" rel="noopener follow" target="_blank"><div class="lk ab fo"><div class="ll ab lm cl cj ln"><h2 class="bd ja gy z fp lo fr fs lp fu fw iz bi translated">PySpark雪花数据仓库读写操作—第1部分(只读)</h2><div class="lq l"><h3 class="bd b gy z fp lo fr fs lp fu fw dk translated">这个故事的目标是建立对雪花数据读写操作的理解…</h3></div><div class="lr l"><p class="bd b dl z fp lo fr fs lp fu fw dk translated">medium.com</p></div></div><div class="ls l"><div class="lt l lu lv lw ls lx le lj"/></div></div></a></div><p id="54d0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这篇博客中，为了使事情更加多样化或实时，我们有多个数据源，我使用了不同的数据源，如HDFS上的Apache Parquet文件(安装在本地系统上)，Oracle数据库。我们将提取数据，对数据集执行简单的转换，并将其写入雪花数据库。</p><ol class=""><li id="e5c2" class="ly lz iq jy b jz ka kd ke kh ma kl mb kp mc kt md me mf mg bi translated"><strong class="jy ja"> Spark连接和导入数据集</strong></li></ol><pre class="kv kw kx ky gt mh mi mj mk aw ml bi"><span id="af6c" class="mm mn iq mi b gy mo mp l mq mr">import pyspark<br/>from pyspark.sql import SparkSession<br/>print(‘modules imported’)</span><span id="e1b4" class="mm mn iq mi b gy ms mp l mq mr">spark= SparkSession.builder.appName(‘Pyspark_snowflake’).getOrCreate()<br/>print(‘app created’)</span><span id="5bf3" class="mm mn iq mi b gy ms mp l mq mr">#snowflake property setting spark._jvm.net.snowflake.spark.snowflake.SnowflakeConnectorUtils.enablePushdownSession(spark._jvm.org.apache.spark.sql.SparkSession.builder().getOrCreate())</span></pre><p id="a89a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">从HDFS本地导入拼花文件:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi mt"><img src="../Images/3341c8a12b9dd2f7fa0e83b794dfa793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eKcIu-lA8mCseDK91rFIQ.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">HDFS的拼花地板</figcaption></figure><pre class="kv kw kx ky gt mh mi mj mk aw ml bi"><span id="1b0c" class="mm mn iq mi b gy mo mp l mq mr"><br/>emp_df=spark.read.parquet(r’hdfs://localhost:9000/learning/emp’)<br/>emp_df.show(15)</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c4f694a50f37a11bdedaf1ae8a61b515.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*M9FNmX5HJHY8j-4zw15DJQ.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">Emp数据</figcaption></figure><p id="ab3d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">导入Oracle数据库表记录:</p><pre class="kv kw kx ky gt mh mi mj mk aw ml bi"><span id="665f" class="mm mn iq mi b gy mo mp l mq mr">#<br/>dept_df = spark.read.format(‘jdbc’).option(‘url’, ‘jdbc:oracle:thin:scott/scott@//localhost:1522/oracle’).option(‘dbtable’, ‘dept’).option(‘user’, ‘scott’).option(‘password’, ‘scott’).option(‘driver’, ‘oracle.jdbc.driver.OracleDriver’).load()</span><span id="2ad7" class="mm mn iq mi b gy ms mp l mq mr">dept_df.show()</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/0bdb85928936b884db1c76986983df8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*W4p6GuWXAG-de4ZH2q1Y7w.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">部门数据</figcaption></figure><p id="6667" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 2。数据转换</strong></p><p id="c50f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在数据转换中，我们将执行简单的转换，如重命名列和连接数据集，因为本故事的范围是理解与雪花的连接。</p><pre class="kv kw kx ky gt mh mi mj mk aw ml bi"><span id="5580" class="mm mn iq mi b gy mo mp l mq mr">#rename Dataframe column</span><span id="8314" class="mm mn iq mi b gy ms mp l mq mr">emp_df=emp_df.withColumnRenamed(‘DEPTNO’,’DEPTNO_E’)</span><span id="0967" class="mm mn iq mi b gy ms mp l mq mr">#join the emp and dept datasets</span><span id="6a90" class="mm mn iq mi b gy ms mp l mq mr">joined_df=dept_df.join(emp_df,emp_df.DEPTNO_E==dept_df.DEPTNO, how=’inner’)</span><span id="ed10" class="mm mn iq mi b gy ms mp l mq mr">#create final dataframe</span><span id="afba" class="mm mn iq mi b gy ms mp l mq mr">final_df=joined_df.select(‘EMPNO’,’ENAME’,’SAL’,’DEPTNO’,’DNAME’)<br/>final_df.show()</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi na"><img src="../Images/dcfc590be9343f5f3f2e5a953cbecd5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*CrJbnKaK_5Sx8F9U05rTfg.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">最终数据集</figcaption></figure><p id="8526" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 3。雪花设置</strong></p><pre class="kv kw kx ky gt mh mi mj mk aw ml bi"><span id="ba08" class="mm mn iq mi b gy mo mp l mq mr">#set the below snowflake properties for connectivity</span><span id="05ac" class="mm mn iq mi b gy ms mp l mq mr">sfOptions = {<br/> “sfURL” : “wa29709.ap-south-1.aws.snowflakecomputing.com”,<br/> “sfAccount” : “xxxxxxx”,<br/> “sfUser” : “xxxxxxxxx”,<br/> “sfPassword” : “xxxxxxxx”,<br/> “sfDatabase” : “learning_db”,<br/> “sfSchema” : “public”,<br/> “sfWarehouse” : “compute_wh”,<br/> “sfRole” : “sysadmin”,<br/>}<br/> <br/>SNOWFLAKE_SOURCE_NAME = “net.snowflake.spark.snowflake”</span></pre><p id="7800" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用下面的脚本创建雪花目标表:</p><pre class="kv kw kx ky gt mh mi mj mk aw ml bi"><span id="7a16" class="mm mn iq mi b gy mo mp l mq mr">create table emp_dept (empno integer, <br/> ename string, <br/> sal integer, <br/> deptno integer, <br/> dname string);</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/db2a70f7408276ded780cfbf6e1c527a.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*12vqSEwjNTLatDA0DPPeoA.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">雪花表</figcaption></figure><p id="ef73" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 4。将Pyspark数据帧加载到雪花目标</strong></p><pre class="kv kw kx ky gt mh mi mj mk aw ml bi"><span id="3405" class="mm mn iq mi b gy mo mp l mq mr">#pyspark dataframe to snowflake</span><span id="ff3c" class="mm mn iq mi b gy ms mp l mq mr">final_df.write.format(“snowflake”).options(**sfOptions).option(“dbtable”, “emp_dept”).mode(‘append’).options(header=True).save()</span></pre><p id="ca4d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用SnowSQL验证雪花中的数据:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi nc"><img src="../Images/6fe2b86b69d31d6d49b58521e33041c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*31ywc_OblosNknEkEfXosQ.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk translated">数据有效性</figcaption></figure><p id="d56f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们能够成功地从不同来源读取数据集，并将Spark数据框加载到雪花数据库表中。</p><p id="ed3b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">感谢大家阅读我的博客。请分享您的观点和反馈。</p></div></div>    
</body>
</html>