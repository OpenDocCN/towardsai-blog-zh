<html>
<head>
<title>Text Classification with RNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用RNN进行文本分类</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/text-classification-with-rnn-62f98767caf1?source=collection_archive---------2-----------------------#2020-11-21">https://pub.towardsai.net/text-classification-with-rnn-62f98767caf1?source=collection_archive---------2-----------------------#2020-11-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3362" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><p id="65d0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">递归神经网络，又名RNN，是一种著名的监督深度学习方法。其他常用的深度学习神经网络有卷积神经网络和人工神经网络。深度学习背后的主要目标是通过机器来重申大脑的功能。因此，不严格地说，每个神经网络结构都是大脑的一部分。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ku"><img src="../Images/4749d0b56cdaf9811982b3c1adf57e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*16-RJKyWd9dqhBRwEiotvA.jpeg"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk translated">脑叶(图片来源:【https://www.nbia.ca/brain-structure-function/】T2</figcaption></figure><p id="d98d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">人工神经网络，又名ANN，长期存储数据，颞叶也是。所以它与颞叶有关。卷积神经网络，又名CNN，用于图像分类和计算机视觉任务。我们大脑中同样的工作是由枕叶完成的，所以CNN可以参考枕叶。现在，RNN主要用于时间序列分析，我们必须处理一系列数据。在这样的工作中，网络从它刚刚观察到的东西中学习，即短期记忆。因此，它类似于大脑的额叶。</p><p id="fd6f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">导入数据</strong></p><p id="ac63" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在本文中，我们将使用IMDB电影评论数据集进行文本分类。这个数据集有5万条不同电影的评论。它是在文本分类中使用的基准数据集，用于训练和测试机器学习和深度学习模型。我们将创建一个模型来预测电影评论是正面还是负面。这是一个二元分类问题。该数据集可以使用<em class="lh"> Tensorflow </em>直接导入，也可以从<a class="ae lg" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank"> Kaggle下载。</a></p><pre class="kv kw kx ky gt li lj lk ll aw lm bi"><span id="1df0" class="ln lo iq lj b gy lp lq l lr ls">from tensorflow.keras.datasets import imdb</span></pre><p id="3e34" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">数据预处理</strong></p><p id="9807" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">对一部电影的评价并不统一。一些评论可能由4-5个单词组成。有些可能由17-18个单词组成。但是当我们把数据输入到我们的神经网络时，我们需要统一的数据。所以我们填充数据。在将数据传递到神经网络之前，我们需要遵循两个步骤:嵌入和填充。在嵌入过程中，单词用向量表示。向量空间中单词的位置是从文本中学习的，它从它周围的单词中学习更多。<em class="lh"> Keras </em>中的嵌入层需要统一的输入，所以我们通过定义统一的长度来填充数据。</p><pre class="kv kw kx ky gt li lj lk ll aw lm bi"><span id="7c82" class="ln lo iq lj b gy lp lq l lr ls">sentence=['Fast cars are good',<br/>          'Football is a famous sport',<br/>          'Be happy Be positive']</span><span id="0d39" class="ln lo iq lj b gy lt lq l lr ls">After padding:</span><span id="7fb1" class="ln lo iq lj b gy lt lq l lr ls">[[364  50  95 313   0   0   0   0   0   0]  <br/> [527 723 350 333 722   0   0   0   0   0]  <br/> [238 216 238 775   0   0   0   0   0   0]]</span></pre><p id="e111" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在上面的代码片段中，每个句子都用零填充。要输入的每个句子的长度是10，所以每个句子都用零填充。你可以在我的<a class="ae lg" href="https://github.com/aaryaab/Text-Classification-with-RNN" rel="noopener ugc nofollow" target="_blank"> GitHub </a>档案中找到单词嵌入和填充的完整代码。</p><p id="e386" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">建造RNN模型</strong></p><p id="26f0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">递归神经网络分三个阶段工作。在第一阶段，它通过隐藏层向前移动并进行预测。在第二阶段，使用损失函数将其预测值与真实值进行比较。损失函数展示了模型的表现。损失函数值越低，模型越好。在最后阶段，它使用反向传播中的误差值，进一步计算每个点(节点)的梯度。梯度是用于调整每个点的网络权重的值。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/75dfce9e1197f5fdd515fd9fb40af3a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*7FezCQqb9Ss7I6ANV0C29w.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk translated">递归神经网络</figcaption></figure><p id="f326" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">当我们处理序列数据时，经常使用递归神经网络。原因是，模型使用的层赋予了模型短期记忆。利用这种记忆，它可以更准确地预测下一个数据。关于过去数据的信息的保留时间是不固定的，但它取决于分配给它的权重。因此，RNN被用于情感分析、序列标注、语音标注等。</p><p id="d0f5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">模型的第一层是<strong class="jy ja">嵌入层</strong>:</p><pre class="kv kw kx ky gt li lj lk ll aw lm bi"><span id="d181" class="ln lo iq lj b gy lp lq l lr ls"># Embedding Layer</span><span id="e6dd" class="ln lo iq lj b gy lt lq l lr ls">imdb_model.add(tf.keras.layers.Embedding(word_size, embed_size, input_shape=(x_train.shape[1],)))</span></pre><p id="61bf" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">嵌入层的第一个参数是数据集中不同单词的数量。这个参数被定义为足够大，使得语料库中的每个单词都可以被唯一地编码。在这个项目中，我们将word_size定义为20000。第二个参数显示了嵌入向量的数量。语料库中的每个单词都将通过嵌入的大小来显示。</p><p id="9f0f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">模型的第二层是<strong class="jy ja"> LSTM层:</strong></p><pre class="kv kw kx ky gt li lj lk ll aw lm bi"><span id="2e89" class="ln lo iq lj b gy lp lq l lr ls"># LSTM Layer</span><span id="ccab" class="ln lo iq lj b gy lt lq l lr ls">imdb_model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))</span></pre><p id="e709" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这是迄今为止递归神经网络最重要的概念。LSTM-长期短期记忆层解决了梯度消失的问题，从而为模型提供了使用最近的过去记忆来预测下一个单词的记忆。</p><p id="b8d9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">消失渐变:</p><p id="3757" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如前所述，梯度是用于调整每个点的权重的值。梯度越大。调整幅度越大，反之亦然。现在的问题是，在反向传播中，层中的每个节点根据前一层的梯度值计算其梯度值。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/42624af3a1755a2801fe11ffb6f3310d.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*4O7AuaBG3xKnO8vVkqkHwg.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk translated">消失梯度</figcaption></figure><p id="26f3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，如果前一层的梯度值较小，该节点的梯度值也会较小，反之亦然。因此，沿着反向传播的方向，梯度的值变得非常小。因为梯度非常小，接近于零。每个点的权重几乎没有调整，因此他们的学习是最小的。通过最少的学习，模型无法理解上下文数据。</p><pre class="kv kw kx ky gt li lj lk ll aw lm bi"><span id="7c71" class="ln lo iq lj b gy lp lq l lr ls">Wrec: Recorded weight at each point</span><span id="9bcf" class="ln lo iq lj b gy lt lq l lr ls">Wrec &lt; 1: Vanishing Gradient   <br/>Wrec &gt; 1: Exploding Gradient</span></pre><p id="1734" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这个问题的解决方案是由hoch Reiter schmid Huber在1997年提出的。是LSTM。长短期记忆会控制反向传播中的数据流。内部机制中有计算信息流的门，并防止重量减少超过某个值。通过将模型与LSTM层堆叠，模型变得更深，深度学习模型的成功在于模型的深度。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lw"><img src="../Images/73b6462219bbd1c85aa726bc2fc18cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DbzpEg77lJr5r_ZKgXfALw.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk translated">LSTM节点的内部结构。(图片来源:【https://colah.github.io/posts/2015-08-Understanding-LSTMs/】T2)</figcaption></figure><p id="2d16" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在LSTM中，内部结构中的门只传递相关的信息，而丢弃不相关的信息，因此沿着序列向下，它正确地预测了序列。关于LSTM工作的详细信息，请阅读<a class="ae lg" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">克里斯多佛·奥拉</a>的文章。</p><p id="a098" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">激活功能</strong></p><p id="b694" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在RNN模型中，使用“双曲正切(tanh(x))”的激活函数，因为它将值保持在-1到1之间。在反向传播过程中，节点处的权重乘以梯度进行调整。如果渐变值更大，则该特定节点的权重值将增加很多。因此，其他节点的权重将是最小的，并且不会计入学习过程。反过来，这将导致模型中的高偏差。为了避免这种情况，使用了双曲函数。它使值介于-1和1之间，并在网络的权重中保持均匀分布。</p><pre class="kv kw kx ky gt li lj lk ll aw lm bi"><span id="3edc" class="ln lo iq lj b gy lp lq l lr ls">tanh(z) = [exp(z) - exp(-z)] / [exp(z) + exp(-z)]</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/38a65dcbe12dafe5557697f0a4c61407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*t4RQKW0u38jc1LD2wpw6lQ.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk translated">双曲正切图</figcaption></figure><p id="a8bc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">双曲正切激活函数的另一个优点是该函数比另一个函数收敛得更快，并且计算也更便宜。</p><pre class="kv kw kx ky gt li lj lk ll aw lm bi"><span id="4076" class="ln lo iq lj b gy lp lq l lr ls"># Output Layer</span><span id="32e6" class="ln lo iq lj b gy lt lq l lr ls">imdb_model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))</span></pre><p id="631b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在输出层，使用“Sigmoid”激活函数。像“双曲正切”一样，它也收缩值，但它是在0到1之间收缩的。这背后的推理是，如果一个值乘以0，它将是零，可以丢弃。如果一个值乘以1，它将保持为零，并且只在这里。因此，通过使用sigmoid函数，只有相关和重要的值将用于预测。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/f917b2740ce14621a4ad4cf7b541df2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*5JVn82B0SoBaQ-jAqijPmQ.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk translated">Sigmoid函数的图形</figcaption></figure><p id="6a8c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">编译图层:</strong></p><pre class="kv kw kx ky gt li lj lk ll aw lm bi"><span id="3af8" class="ln lo iq lj b gy lp lq l lr ls">imdb_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])</span></pre><p id="a7ed" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这个文本分类问题中，我们预测的是正面评论还是负面评论。因此，我们正在研究一个二元分类问题。所以我们使用“二元交叉熵”的损失函数此外，使用的指标将是“准确性”当我们处理多类分类问题时，我们使用“稀疏分类交叉熵”和“稀疏准确度”多类分类问题主要用CNN。更多信息，可以看我在<a class="ae lg" href="https://towardsdatascience.com/deep-learning-with-cifar-10-image-classification-64ab92110d79" rel="noopener" target="_blank"> CNN </a>上的文章。</p><p id="bd29" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在训练模型的同时，我们分批训练模型。我们不是一次训练一个复习，而是分批次进行。这降低了计算能力。我们对模型使用了128的批量大小。</p><pre class="kv kw kx ky gt li lj lk ll aw lm bi"><span id="73db" class="ln lo iq lj b gy lp lq l lr ls">imdb_model.fit(x_train, y_train, epochs=5, batch_size=128)</span></pre><p id="dd52" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">您可以通过更改epochs和batch_size来临时改变模型。但是也要注意过度合身！通过使用这个模型，我得到了接近84%的准确率。</p><p id="e6d3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">因此，在这篇文章中，我们了解了什么是递归神经网络。我们讲述了预处理的重要性，以及如何在RNN结构中完成预处理。我们学习了消失梯度的问题，以及如何用LSTM解决它。最后，我们阅读激活函数以及它们在RNN模型中是如何工作的。</p><p id="6819" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你可以在我的<a class="ae lg" href="https://github.com/aaryaab/Text-Classification-with-RNN" rel="noopener ugc nofollow" target="_blank"> GitHub简介</a>中找到这个模型的完整代码。</p><p id="577e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">请随时在https://www.linkedin.com/in/aarya-brahmane-4b6986128/<a class="ae lg" href="https://www.linkedin.com/in/aarya-brahmane-4b6986128/" rel="noopener ugc nofollow" target="_blank">与我联系</a></p><p id="817a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">参考资料:</p><p id="119f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这是一篇很棒的文章，可以用很棒的视觉表现方式<a class="ae lg" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>深入了解LSTM</p><p id="96c3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你可以在https://www . mathsisfun . com/data/function-grapher . PHP # functions找到并制作一些有趣的图表</p><p id="b20e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">快乐深度学习！</p><p id="80ca" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">和平！</p></div></div>    
</body>
</html>