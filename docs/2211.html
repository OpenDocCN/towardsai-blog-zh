<html>
<head>
<title>Sometimes Bigger Machine Learning Models and Larger Datasets Can Hurt Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有时，更大的机器学习模型和更大的数据集会影响性能</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/sometimes-bigger-machine-learning-models-and-larger-datasets-can-hurt-performance-ae26ab530e67?source=collection_archive---------0-----------------------#2021-09-30">https://pub.towardsai.net/sometimes-bigger-machine-learning-models-and-larger-datasets-can-hurt-performance-ae26ab530e67?source=collection_archive---------0-----------------------#2021-09-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="15c6" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="ce5f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">OpenAI双下降假设研究显示了一种现象，这种现象挑战了传统的统计学习理论和机器学习实践者的传统智慧。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b53ebf0c518691f117b0057cf20fca07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oxjtuRpILBL6PZUV.jpg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:https://www.youtube.com/watch?v=Kih-VPHL3gA<a class="ae lh" href="https://www.youtube.com/watch?v=Kih-VPHL3gA" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><blockquote class="li lj lk"><p id="fbc2" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过10万名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到102，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="395e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">越大越好当然适用于现代深度学习范式。拥有数百万个参数的大型神经网络通常比专注于特定任务的小型网络集合表现更好。过去几年中一些最著名的模型，如Google BERT、Microsoft T-NLG或OpenAI GPT-2，是如此之大，以至于它们的计算成本结果对大多数组织来说是禁止的。然而，模型的性能并不随着其大小线性增加。双重下降是一种现象，当我们增加模型规模时，性能先变坏，然后变好。最近，<a class="ae lh" href="https://arxiv.org/abs/1912.02292" rel="noopener ugc nofollow" target="_blank"> OpenAI的研究人员研究了有多少现代深度学习模型容易受到双下降现象</a>的影响。</p><p id="d042" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">多年来，模型的性能与其大小之间的关系无疑一直困扰着深度学习研究人员。在传统的统计学习中，偏差-方差权衡表明，更高复杂性的模型具有更低的偏差但更高的方差。根据这一理论，一旦模型复杂性超过某个阈值，模型就会“过度拟合”,方差项将主导测试误差，因此从这一点开始，增加模型复杂性只会降低性能。从这个角度来看，统计学习告诉我们<em class="ln">“更大的模型更糟糕”</em>。然而，现代深度学习模型挑战了这一传统智慧。</p><p id="7153" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">与偏差-方差权衡相比，具有数百万个参数的深度神经网络已被证明优于较小的模型。此外，许多这些模型随着更多的训练数据而线性改进。因此，深度学习从业者中的传统观点是<em class="ln">“更大的模型和更多的数据总是更好的”</em>。</p><p id="19d4" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">哪个理论是正确的？统计学习还是深度学习模型的经验证据？双重下降现象向我们表明，这两种理论可以调和，但他们的一些假设也是错误的。</p><h1 id="a360" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">深度双重下降</h1><p id="e3c1" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">思考双重下降现象的一个简单方法是</p><blockquote class="li lj lk"><p id="1f55" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><em class="it">“在训练深度学习模型时，事情变得更糟”。</em></p></blockquote><p id="dd33" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在许多传统的深度学习模型中，如CNN、RNNs或transformers，我们可以注意到，性能峰值可预测地出现在“临界状态”，其中模型几乎无法适应训练集。随着神经网络中参数数量的增加，测试误差最初会减小、增大，并且就在模型能够拟合训练集时，会经历第二次下降。</p><p id="97ea" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">深度双下降理论的要点是，无论是经典统计学家的传统智慧“<em class="ln">太大的模型更糟糕”</em>，还是现代深度学习范式“<em class="ln">更大的模型更好”</em>都不支持。这完全取决于模型的状态。OpenAI在一个非常简单的数学模型中概述了这一概念，该模型被称为双重下降假设。</p><h2 id="124e" class="oa ne it bd nf ob oc dn nj od oe dp nn na of og np nb oh oi nr nc oj ok nt iz bi translated">双重下降假说</h2><p id="4cb0" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">让我们为训练过程T引入有效模型复杂度(EMC)的概念。T的EMC或EMC(T)将是T平均达到大约0训练误差的最大样本数n。使用该定义，我们可以将深度学习模型分为三种关键状态:</p><p id="8082" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd">欠参数化:</strong>如果EMC(T)充分小于n，T的任何增加其有效复杂度的扰动都会降低测试误差。</p><p id="8326" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd">过参数化:</strong>如果EMCD(T)充分大于n，T的任何增加其有效复杂度的扰动都会降低测试误差。</p><p id="3a8a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd">临界参数化:</strong>如果EMCD(T) ≈ n，那么增加其有效复杂性的T的扰动可能减少或增加测试误差</p><p id="fd35" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">通过限定模型的状态，深度双体面假设揭示了优化算法、模型大小和测试性能之间的相互作用，并有助于调和关于它们的一些相互竞争的直觉。在参数化不足的情况下，模型复杂性与样本数量相比较小，作为模型复杂性函数的测试误差遵循由经典偏差/方差权衡预测的U型行为。然而，一旦模型复杂度大到足以进行插值，即达到(接近)零训练误差，那么增加复杂度只会降低测试误差，遵循“模型越大越好”的现代直觉。</p><p id="0b98" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">根据深度双重下降假说，OpenAI在模型的学习生命周期中观察到三个关键状态。</p><p id="04b7" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd">1)</strong><strong class="lo jd">Model-Wise Double Descent:</strong>描述模型越大越差的状态。</p><p id="d361" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd"> 2) </strong> <strong class="lo jd">历元双下降:</strong>描述训练时间越长，越适应的状态。</p><p id="9bf0" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><strong class="lo jd"> 3) </strong> <strong class="lo jd">样本态非单调性:</strong>描述更多样本伤害模型性能的状态。</p><h2 id="3087" class="oa ne it bd nf ob oc dn nj od oe dp nn na of og np nb oh oi nr nc oj ok nt iz bi translated">模型式双重下降</h2><p id="a5d5" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">模型式双重下降描述了模型参数化不足时的现象。在这种状态下，测试误差的峰值出现在插值阈值附近，此时模型刚好大到足以适合训练集。模型式双下降现象还表明，影响插值阈值的变化(例如改变优化算法、训练样本的数量或标签噪声的量)也相应地影响测试误差峰值的位置。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/ff35ab79bc6ee48e325546be34c482c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fhK2n0uN6s-AmmiMjyOOrg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://openai.com/blog/deep-double-descent/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/deep-double-descent/</a></figcaption></figure><h2 id="137c" class="oa ne it bd nf ob oc dn nj od oe dp nn na of og np nb oh oi nr nc oj ok nt iz bi translated">时代式双重下降</h2><p id="f55c" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">逐时双下降描述了一种状态，其中足够大的模型在训练过程中从欠参数化过渡到过参数化。在这种状态下，足够大的模型可以经历“双下降”行为，其中测试误差首先减小，然后在插值阈值附近增大，然后再次减小。相比之下，对于“中等大小”的模型，训练到完成仅勉强达到接近0的误差水平，作为训练时间的函数的测试误差将遵循经典的U形曲线，在该曲线处最好尽早停止。太小而达不到近似阈值的模型将保持在“参数化不足”状态，其中增加训练时间单调地减少测试误差。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/bafb5871e4479ceba7f866bce5ca3d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1I2XW_4Wv91Zawp8Uaxllg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:https://openai.com/blog/deep-double-descent/</figcaption></figure><h2 id="c0a7" class="oa ne it bd nf ob oc dn nj od oe dp nn na of og np nb oh oi nr nc oj ok nt iz bi translated">样本非单调性</h2><p id="39b0" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">样本非单调性是指添加更多训练样本会损害模型性能的状态。具体来说，增加样本数量对测试误差与模型复杂度图有两种不同的影响。一方面，(如预期的那样)增加样本数量会缩小曲线下的面积。另一方面，增加样本数量也会产生“曲线右移”的效果，并增加测试误差达到峰值时的模型复杂性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/48e9e384cb0ccbadd0512b1045164505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0S7yime_HgUXuu1LkQ9ABw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://openai.com/blog/deep-double-descent/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/deep-double-descent/</a></figcaption></figure><p id="25cc" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">双下降假说增加了一些有趣的背景，以帮助理解深度学习模型随着时间的推移而表现出来的性能。实践实验表明，无论是经典统计学家的传统智慧“<em class="ln">太大的模型更糟糕”</em>还是现代深度学习范式“<em class="ln">模型越大越好”</em>的统计学习理论都不完全正确。对深度学习实践者来说，将深度双下降现象考虑在内可能是一个重要的工具，因为它强调了选择正确的数据集、架构和训练程序来优化模型性能的重要性。</p></div></div>    
</body>
</html>