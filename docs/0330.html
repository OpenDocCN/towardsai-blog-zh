<html>
<head>
<title>NLP News Cypher | 02.23.20</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP新闻密码| 02.23.20</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/nlp-news-cypher-02-23-20-446285e828f9?source=collection_archive---------2-----------------------#2020-02-24">https://pub.towardsai.net/nlp-news-cypher-02-23-20-446285e828f9?source=collection_archive---------2-----------------------#2020-02-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/7769206ffb4bc5fa58179ddc45722556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8-q5jit6O0tGYQH-"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">马西米利亚诺·莫罗西诺托在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="0dc4" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph">自然语言处理每周时事通讯</h2><div class=""/><div class=""><h2 id="d958" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">达库努riști努câștigi</h2></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4e8e" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">如果你想知道副标题是什么意思，那是罗马尼亚语。意思是这样的:</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="4190" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">你这周过得怎么样？</p><p id="224b" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">上周我们再次更新了大坏NLP数据库，感谢艾曼·阿尔赫尔巴维的贡献！</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="mn mm l"/></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="0a0f" class="mo mp jg bd mq mr ms mt mu mv mw mx my kv mz kw na ky nb kz nc lb nd lc ne nf bi translated">本周:</h1><blockquote class="ng nh ni"><p id="e3c9" class="ll lm nj ln b lo lp kq lq lr ls kt lt nk lv lw lx nl lz ma mb nm md me mf mg ij bi translated">Los趋势</p><p id="a57f" class="ll lm nj ln b lo lp kq lq lr ls kt lt nk lv lw lx nl lz ma mb nm md me mf mg ij bi translated">真相伤人</p><p id="fc12" class="ll lm nj ln b lo lp kq lq lr ls kt lt nk lv lw lx nl lz ma mb nm md me mf mg ij bi translated">总结一下我</p><p id="deb6" class="ll lm nj ln b lo lp kq lq lr ls kt lt nk lv lw lx nl lz ma mb nm md me mf mg ij bi translated">拥抱NER</p><p id="2f2c" class="ll lm nj ln b lo lp kq lq lr ls kt lt nk lv lw lx nl lz ma mb nm md me mf mg ij bi translated">嘿，邻居，我是语言模特</p><p id="0ddf" class="ll lm nj ln b lo lp kq lq lr ls kt lt nk lv lw lx nl lz ma mb nm md me mf mg ij bi translated">(红色)帽子的尖端</p><p id="71d8" class="ll lm nj ln b lo lp kq lq lr ls kt lt nk lv lw lx nl lz ma mb nm md me mf mg ij bi translated">带注释的GPT-2</p><p id="2c05" class="ll lm nj ln b lo lp kq lq lr ls kt lt nk lv lw lx nl lz ma mb nm md me mf mg ij bi translated">本周数据集:对话NLI</p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="0ae9" class="mo mp jg bd mq mr ms mt mu mv mw mx my kv mz kw na ky nb kz nc lb nd lc ne nf bi translated">Los趋势</h1><p id="8e9b" class="pw-post-body-paragraph ll lm jg ln b lo nn kq lq lr no kt lt lu np lw lx ly nq ma mb mc nr me mf mg ij bi translated">我们阅读的所有书籍的作者奥赖利分析了他们的在线学习平台，以洞察最流行的科技趋势。</p><p id="f856" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">TL；博士:</p><ol class=""><li id="1703" class="ns nt jg ln b lo lp lr ls lu nu ly nv mc nw mg nx ny nz oa bi translated">Python正在杀死它(R.I.P. to <em class="nj"> R </em></li><li id="e2f9" class="ns nt jg ln b lo ob lr oc lu od ly oe mc of mg nx ny nz oa bi translated">云的使用正在杀死它(微服务和它们的容器)</li><li id="395e" class="ns nt jg ln b lo ob lr oc lu od ly oe mc of mg nx ny nz oa bi translated">2019年，对NLP的兴趣增长了+22%。👀</li></ol><p id="d7f3" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">全图:</strong></p><div class="ip iq gp gr ir og"><a href="https://www.oreilly.com/radar/oreilly-2020-platform-analysis/" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jq gy z fp ol fr fs om fu fw jp bi translated">2020年技术领导者需要关注的5个关键领域</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">O'Reilly在线学习包含有关趋势、主题和问题的信息，技术领导者需要关注和…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">www.oreilly.com</p></div></div><div class="op l"><div class="oq l or os ot op ou ix og"/></div></div></a></div><h1 id="65de" class="mo mp jg bd mq mr ov mt mu mv ow mx my kv ox kw na ky oy kz nc lb oz lc ne nf bi translated">真相伤人</h1><p id="d995" class="pw-post-body-paragraph ll lm jg ln b lo nn kq lq lr no kt lt lu np lw lx ly nq ma mb mc nr me mf mg ij bi translated">艾伦研究所有大量的演示供你玩。最近，Twitter上的一个演示引起了我的注意。这个模型能够根据你在自然语言中赋予它的条件来判断一个陈述是真还是假。根据AI2的说法:</p><blockquote class="ng nh ni"><p id="41eb" class="ll lm nj ln b lo lp kq lq lr ls kt lt nk lv lw lx nl lz ma mb nm md me mf mg ij bi translated">“ROVER(<strong class="ln jq">R</strong>eason<strong class="ln jq">ov</strong>R<strong class="ln jq">R</strong>ules)模型读入自然语言事实和规则的新规则库，并在封闭世界(以及否定即失败)假设下决定新语句的真值。”</p></blockquote><p id="28e0" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">演示:</strong></p><div class="ip iq gp gr ir og"><a href="https://rule-reasoning.apps.allenai.org/" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jq gy z fp ol fr fs om fu fw jp bi translated">漫游者:对规则的推理</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">编辑描述</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">rule-reasoning.apps.allenai.org</p></div></div></div></a></div><h1 id="8c9d" class="mo mp jg bd mq mr ov mt mu mv ow mx my kv ox kw na ky oy kz nc lb oz lc ne nf bi translated">总结一下我</h1><p id="0680" class="pw-post-body-paragraph ll lm jg ln b lo nn kq lq lr no kt lt lu np lw lx ly nq ma mb mc nr me mf mg ij bi translated">从整洁有序的文本中总结很难。但是从论坛/对话数据中总结就更难了。微软亚洲的Peeps发表了一篇关于摘录摘要的新论文，讨论了注意力的用法。</p><p id="e526" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">该模型在Trip Advisor论坛讨论数据集上获得了SOTA胭脂的分数！</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="pa mm l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://arxiv.org/pdf/2002.03405.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="a0b9" class="mo mp jg bd mq mr ov mt mu mv ow mx my kv ox kw na ky oy kz nc lb oz lc ne nf bi translated">拥抱NER</h1><p id="88d8" class="pw-post-body-paragraph ll lm jg ln b lo nn kq lq lr no kt lt lu np lw lx ly nq ma mb mc nr me mf mg ij bi translated">嘿，想通过拥抱脸库用SOTA变形金刚做一些命名实体识别吗？想要密码吗👇</p><p id="5aac" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq"> Colab: </strong></p><div class="ip iq gp gr ir og"><a href="https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jq gy z fp ol fr fs om fu fw jp bi translated">谷歌联合实验室</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">编辑描述</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">colab.research.google.com</p></div></div><div class="op l"><div class="pb l or os ot op ou ix og"/></div></div></a></div><p id="24e5" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">GitHub: </p><div class="ip iq gp gr ir og"><a href="https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jq gy z fp ol fr fs om fu fw jp bi translated">拥抱脸/变形金刚</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">🤗变形金刚:用于TensorFlow 2.0和PyTorch的最新自然语言处理。…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">github.com</p></div></div><div class="op l"><div class="pc l or os ot op ou ix og"/></div></div></a></div><h1 id="6bd5" class="mo mp jg bd mq mr ov mt mu mv ow mx my kv ox kw na ky oy kz nc lb oz lc ne nf bi translated">嘿，邻居，我是语言模特</h1><p id="d57d" class="pw-post-body-paragraph ll lm jg ln b lo nn kq lq lr no kt lt lu np lw lx ly nq ma mb mc nr me mf mg ij bi translated">根据脸书研究中心和斯坦福大学的最新研究，将k近邻(kNN)应用于语言模型可以提高性能。在Wikitext-103语言模型数据集上，这种新方法在没有任何额外训练的情况下，在困惑度上提高了2.9个百分点！</p><p id="77f7" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">论文:</strong></p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="pa mm l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://arxiv.org/pdf/1911.00172.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="a500" class="mo mp jg bd mq mr ov mt mu mv ow mx my kv ox kw na ky oy kz nc lb oz lc ne nf bi translated">(红色)帽子的尖端</h1><p id="0b91" class="pw-post-body-paragraph ll lm jg ln b lo nn kq lq lr no kt lt lu np lw lx ly nq ma mb mc nr me mf mg ij bi translated">红帽发布了一份关于开源软件状况的新报告！根据该报告，专有软件预计将在未来几年暴跌，开源软件预计将增加。他们还分享了为什么开源如此受欢迎(除了免费之外)以及在企业中采用的主要领域。如果你卖开源软件，你应该看看这个:</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="pa mm l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="http://quantumstat.com/wp-content/uploads/rh-enterprise-open-source-report-detail-f21756-202002-en.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="4898" class="mo mp jg bd mq mr ov mt mu mv ow mx my kv ox kw na ky oy kz nc lb oz lc ne nf bi translated">带注释的GPT-2</h1><p id="26d0" class="pw-post-body-paragraph ll lm jg ln b lo nn kq lq lr no kt lt lu np lw lx ly nq ma mb mc nr me mf mg ij bi translated">嘿，还记得<a class="ae jd" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">带注释的变形金刚</a>吗？那时，拉什先生用代码注释了臭名昭著的“你所需要的只是注意力”这篇论文。好吧，看起来我们现在有一个来自阿罗拉先生的GPT-2。尽情享受吧！</p><p id="3e14" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">GPT-2: </p><div class="ip iq gp gr ir og"><a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jq gy z fp ol fr fs om fu fw jp bi translated">带注释的GPT-2</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">介绍先决条件语言模型是无监督的多任务学习者抽象模型架构(GPT-2)…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">amaarora.github.io</p></div></div></div></a></div><h1 id="f2c6" class="mo mp jg bd mq mr ov mt mu mv ow mx my kv ox kw na ky oy kz nc lb oz lc ne nf bi translated">伯特学习编码</h1><p id="d8c7" class="pw-post-body-paragraph ll lm jg ln b lo nn kq lq lr no kt lt lu np lw lx ly nq ma mb mc nr me mf mg ij bi translated">微软正在使用BERT来编码👀。在本月发表的一篇论文中，他们的研究团队使用了自然语言文本和来自几种编程语言的代码，并在预训练期间部署了它们。具体来说，它是用自然语言和代码对以及单峰数据(没有自然语言对的代码)来训练的。</p><p id="abfa" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">该模型实现了下游任务的SOTA，如自然语言代码搜索和代码到文档的生成。</p><p id="735f" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">论文</strong>:</p><figure class="mh mi mj mk gt is"><div class="bz fp l di"><div class="pa mm l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://arxiv.org/pdf/2002.08155.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="3aa3" class="mo mp jg bd mq mr ov mt mu mv ow mx my kv ox kw na ky oy kz nc lb oz lc ne nf bi translated">本周数据集:对话NLI</h1><p id="1a6d" class="pw-post-body-paragraph ll lm jg ln b lo nn kq lq lr no kt lt lu np lw lx ly nq ma mb mc nr me mf mg ij bi translated"><strong class="ln jq">什么事？</strong></p><p id="f190" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">数据集由标记为蕴涵、中性或矛盾的句子对组成，用于自然语言推理任务。</p><p id="332b" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">样本:</strong></p><figure class="mh mi mj mk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pd"><img src="../Images/8ae6c10664ce49c32dbbd6cab650fc78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Mtdw1zf6iM27foxa.png"/></div></div></figure><p id="ce57" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">在哪里？</strong></p><div class="ip iq gp gr ir og"><a href="https://wellecks.github.io/dialogue_nli/" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jq gy z fp ol fr fs om fu fw jp bi translated">对话自然语言推理</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">摘要:一致性是对话模型面临的一个长期问题。在本文中，我们构造了…的一致性</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">wellecks.github.io</p></div></div></div></a></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><blockquote class="pe"><p id="e4d3" class="pf pg jg bd ph pi pj pk pl pm pn mg dk translated">每周日，我们都会对来自世界各地研究人员的NLP新闻和代码进行一次每周综述。</p><p id="9a10" class="pf pg jg bd ph pi pj pk pl pm pn mg dk translated">如果您喜欢这篇文章，请帮助我们，并与朋友或社交媒体分享！</p><p id="f2bf" class="pf pg jg bd ph pi pj pk pl pm pn mg dk translated">如需完整报道，请关注我们的twitter: @Quantum_Stat</p></blockquote><figure class="pp pq pr ps pt is gh gi paragraph-image"><div class="gh gi po"><img src="../Images/fa79b97bd19181b4e7455f5c3c2fec1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:108/0*hyN8qCObI58Udjp8"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="http://www.quantumstat.com/" rel="noopener ugc nofollow" target="_blank">www.quantumstat.com</a></figcaption></figure></div></div>    
</body>
</html>