<html>
<head>
<title>LDA vs. PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LDA与PCA</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/lda-vs-pca-f3920a7310db?source=collection_archive---------0-----------------------#2021-02-16">https://pub.towardsai.net/lda-vs-pca-f3920a7310db?source=collection_archive---------0-----------------------#2021-02-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/e0530da0390432673cdeb6a3941f1a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0DdfqVyLA6f4qBUa"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">罗伯特·卡茨克在T2的照片</figcaption></figure><h2 id="48f9" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a></h2><div class=""/><p id="7700" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">关于线性判别分析降维技术与主成分分析有多相似或不相似的精确概述</p><p id="3ce5" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">这是我之前文章的第二部分</em><a class="ae jd" href="https://medium.com/wicds/the-power-of-eigenvectors-and-eigenvalues-in-dimension-reduction-techniques-such-as-pca-8540322124ea" rel="noopener"><em class="lk">PCA</em></a><em class="lk">等降维技术中特征向量和特征值的幂。</em></p><p id="68ae" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在本文中，我将首先简要解释LDA和PCA之间的区别。接下来，让我们深入探讨线性判别分析的工作原理，并揭开其中的奥秘，它是如何实现数据分类和降维的。</p><h1 id="7ce4" class="ll lm jg bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">简介</strong></h1><h2 id="882a" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated"><strong class="ak"> LDA与PCA </strong></h2><p id="34df" class="pw-post-body-paragraph km kn jg ko b kp mu kr ks kt mv kv kw kx mw kz la lb mx ld le lf my lh li lj ij bi translated">线性判别分析非常类似于PCA，两者都寻找能够最好地解释数据的特征的线性组合。</p><p id="2216" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">主要区别在于，线性判别分析是一种<strong class="ko jq">监督的</strong>降维技术，同时也实现了数据的分类。</p><blockquote class="mz na nb"><p id="ca8a" class="km kn lk ko b kp kq kr ks kt ku kv kw nc ky kz la nd lc ld le ne lg lh li lj ij bi translated"><strong class="ko jq"> LDA </strong>关注于寻找一个特征子空间，该特征子空间<strong class="ko jq">最大化组之间的可分性</strong>。</p></blockquote><p id="f7d0" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">虽然主成分分析是一种<strong class="ko jq">无监督的</strong>降维技术，但它忽略了类别标签。</p><blockquote class="mz na nb"><p id="15fe" class="km kn lk ko b kp kq kr ks kt ku kv kw nc ky kz la nd lc ld le ne lg lh li lj ij bi translated"><strong class="ko jq"> PCA </strong>主要是捕捉数据集中<strong class="ko jq">最大变化</strong>的方向。</p></blockquote><p id="f63b" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">LDA和PCA都形成了一组新的成分。</p><p id="311b" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">PCA形成的第一个主成分将解释数据中的最大变化。<strong class="ko jq"> PC2 </strong>在捕捉最大变化等方面做得第二好。</p><p id="7aa4" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq"> LD1 </strong>由线性判别分析创建的第一个新轴将考虑捕捉组或类别之间的大多数变化，然后是LD2，以此类推。</p><p id="b1b7" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">注</strong>:在LDA中，目标因变量可以有二进制或多类标签。</p><h2 id="d0d5" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated"><strong class="ak">线性判别分析的工作</strong></h2><p id="c643" class="pw-post-body-paragraph km kn jg ko b kp mu kr ks kt mv kv kw kx mw kz la lb mx ld le lf my lh li lj ij bi translated">让我们借助一个例子来理解线性判别分析的工作原理。</p><p id="9907" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设您有一个信用卡贷款数据集，其目标标签包含两个类:违约者和非违约者。</p><p id="bf4a" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">级‘1’</strong>为<strong class="ko jq">违约者</strong>，<strong class="ko jq">级‘0’</strong>为<strong class="ko jq">非违约者</strong>。</p><p id="87b9" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq"> <em class="lk">在进行LDA投影</em> </strong>之前，理解基本的一维和二维图形</p><p id="1ed0" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当你只有<strong class="ko jq">一个属性</strong>比如说<strong class="ko jq">信用评分</strong>时，这个图将是一个一维图，它是一条数字线。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/fdc0973148ca1d9237a40b58961d5bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*kNKOwWT5qJUnIU65tNLikw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</figcaption></figure><p id="19a5" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">只有一个属性，虽然我们能够分离类别，但由于没有具体的分界点，某些点已经<strong class="ko jq">重叠</strong>。实际上，当数据集中有大量观察值时，可能会有许多这样的重叠数据点。</p><p id="ef42" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们看看当我们有两个属性时会发生什么，我们能够更好地分类吗？ </p><p id="9e12" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">考虑另一个属性年收入连同信用评分。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/a33a4b3f184b54d8b54f916a235fda9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*BfrSQg2wZoCHxs89zjcW4w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</figcaption></figure><p id="7f54" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通过添加另一个特性，我们能够比之前使用单一属性的情况减少重叠。但是，当我们处理具有许多特征和观察值的大型数据集时，仍然会留下许多重叠的点。</p><p id="fc4f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有趣的是，我们注意到增加了一些功能，我们能够减少重叠点的数量，更好地区分。但是，当我们有高维数据集时，这变得非常难以可视化。</p><p id="eca0" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这就是LDA的实施发挥关键作用的地方。</p><h2 id="c336" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated">LDA如何规划数据？</h2><blockquote class="mz na nb"><p id="4a27" class="km kn lk ko b kp kq kr ks kt ku kv kw nc ky kz la nd lc ld le ne lg lh li lj ij bi translated"><strong class="ko jq">线性判别分析</strong>将数据点投影到新轴上，使得这些新分量<strong class="ko jq">最大化</strong>类别间的可分性，同时将每个类别内的变化保持在最小<strong class="ko jq">值</strong>。</p></blockquote><p id="c276" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在让我们详细了解一下LDA是如何预测数据点的。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/56263a5f974f9f48499ad5627fb4a000.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*28ZuwxwaLOOAb20CKaT0WQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</figcaption></figure><p id="69ce" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">1.LDA使用来自两个属性的信息，并将数据投影到新的轴上。</p><p id="440a" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.它以这样一种方式投影数据点，即同时满足组间最大分离和组内最小变化的标准。</p><h2 id="9ac6" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated"><strong class="ak">第一步:</strong></h2><p id="82a9" class="pw-post-body-paragraph km kn jg ko b kp mu kr ks kt mv kv kw kx mw kz la lb mx ld le lf my lh li lj ij bi translated">投影点和新轴</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/27978f72180adb61c8b6aa54a16692ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*hakp_InY0wrBAKHGCD1CPw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</figcaption></figure><h2 id="31f5" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated"><strong class="ak">第二步</strong></h2><p id="d645" class="pw-post-body-paragraph km kn jg ko b kp mu kr ks kt mv kv kw kx mw kz la lb mx ld le lf my lh li lj ij bi translated">适用于投影点的标准LDA如下。</p><p id="e9b8" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">1.它最大化了每个类别的平均值之间的距离。</p><p id="1762" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.它最小化了由s表示的每个类别内的变化或分散</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/38cdeaa3300d3d46db9abe452eac3192.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*HbTNA8TCZMsnQ1Op1PMoxg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</figcaption></figure><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/9bafd671bb36c6c0dcb0c0ae59490329.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*3oR3CLCt91feelsNqDkWIw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</figcaption></figure><p id="d785" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设类别1违约者的平均值是平均值1，平均值2是类别非违约者的平均值。</p><p id="b06d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">同样的，</p><p id="b52c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">S1是第一类的分散</p><p id="acf9" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">S2是第二类的散布者。</p><p id="5a76" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它现在计算公式。</p><p id="4454" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">(均值1-均值2) /(S1 +S2)</p><p id="813c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">注</strong>:分子是平方的，以避免负值。</p><p id="3e04" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">设mean1-mean2用d表示。</p><p id="b69c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">公式现在将是。</p><p id="9f85" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">d /(S1 +S2)</p><p id="2253" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">注:</strong>理想情况下<strong class="ko jq">，</strong>越大，分子越大，组间分离。而分母越小，组内方差越小。</p><h2 id="2ca7" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated">当有两个以上的类别时，LDA如何工作？</h2><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/8411c1d84079e30a0c6918184dd193c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*-Pbn6Oo3xli5PBidXfH2fg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</figcaption></figure><p id="49a0" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当有两个以上的类别时，LDA计算所有类别的中心点以及每个类别的中心点到该点的距离。</p><p id="08c4" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，它将数据投影到新的轴上，使各组之间的间隔最大，组内的变化最小。</p><p id="fc7d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">公式现在将是。</p><p id="2bc9" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">(d1 +d2 +d3 )/(s1 +S2 +S3)</p><p id="2a55" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">现在是有趣的部分……</em></p><h2 id="a99d" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated">LDA是如何进行预测的？</h2><p id="5f30" class="pw-post-body-paragraph km kn jg ko b kp mu kr ks kt mv kv kw kx mw kz la lb mx ld le lf my lh li lj ij bi translated">线性判别分析使用贝耶定理来估计概率。</p><p id="68e3" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它首先从给定的数据集中计算先验概率。在这些先验概率的帮助下，它使用Baye定理计算后验概率。</p><p id="d501" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从贝叶斯定理我们知道。</p><p id="d9ff" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">P(A|B)=P(B|A)*P(A)/P(B)</p><p id="644f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中A，B是事件，P(B)不等于零</p><p id="153c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设我们在数据集中有三个类类0，类1，类2。</p><p id="9672" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">第一步</strong></p><p id="a045" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">LDA计算数据集的每个类P(y=0)，P(y=1)，P(y=2)的<strong class="ko jq">先验</strong>概率。</p><p id="ac78" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">接下来，它计算观察值的条件概率。</p><p id="f08a" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">第二步</strong></p><p id="98eb" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们考虑一个观察x。</p><p id="40fb" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">P(x|y=0)，P(x|y=1)，P(x|y=2)代表<strong class="ko jq">似然函数</strong>。</p><p id="8a09" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">第三步</strong></p><p id="37ab" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">LDA现在计算<strong class="ko jq">后验概率</strong>以做出预测<strong class="ko jq">。</strong></p><p id="4dc0" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">P(y=0|x)=P(x|y=0)*P(y=0)/P(x)</p><p id="e4b2" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">P(y=1|x)=P(x|y=1)*P(y=1)/P(x)</p><p id="5222" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">P(y=2|x)=P(x|y=2)*P(y=2)/P(x)</p><p id="cfb2" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">通式</strong>为一组【c】<strong class="ko jq"/>类</p><p id="d424" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">设y1，y2，..yc是“c”类的集合，并且考虑i=1，2，..，c。</p><p id="ce51" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">P(x|yi)将代表<strong class="ko jq">可能性</strong>函数或<strong class="ko jq">条件</strong>概率。</p><p id="a368" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">P(yi)将是数据集中每个类的<strong class="ko jq">先验</strong>概率。它只不过是该特定类中的观察值数量与所有类中的观察值总数的比率。</p><p id="eb02" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">后验概率</strong>将是<strong class="ko jq">可能性</strong> * <strong class="ko jq">先验</strong> / <strong class="ko jq">证据。</strong></p><p id="023d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">P(y=yi|x)=P(x|y=yi)*P(yi)/P(x)</p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><h2 id="3149" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated">线性判别分析的假设</h2><p id="0900" class="pw-post-body-paragraph km kn jg ko b kp mu kr ks kt mv kv kw kx mw kz la lb mx ld le lf my lh li lj ij bi translated">线性判别分析对数据集有一定的假设。</p><p id="973c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">假设1 </strong></p><p id="5e6e" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">LDA假设每个类别的独立变量呈正态分布。</p><p id="39a1" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">假设2 </strong></p><p id="ee9c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Homoscedasticity" rel="noopener ugc nofollow" target="_blank"><strong class="ko jq"/></a></p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/050daf6b93362102b9fe89729dd895b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/0*Xs-vATVvVwrgVtaq.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">用显示方差齐性的数据绘图，即对于每个x值，y值具有相同的方差。来源:维基媒体的知识共享</figcaption></figure><p id="0d95" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">LDA假设独立变量在所有类别中具有相等的方差和协方差。这可以用<a class="ae jd" href="https://en.wikipedia.org/wiki/Box%27s_M_test" rel="noopener ugc nofollow" target="_blank">盒的M </a>统计量来测试。</p><p id="b80f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">简而言之，假设数据集中类别的所有变量的方差等于y=0，y=1时，变量之间的协方差也必须相等。</p><p id="96eb" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个假设帮助<strong class="ko jq">线性判别分析</strong>创建类别之间的<strong class="ko jq">线性</strong> <strong class="ko jq">判定边界</strong>。</p><p id="ddae" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">假设3 </strong></p><p id="26f0" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">多重共线性</strong></p><p id="0482" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">预测的性能会随着自变量之间相关性的增加而降低。</p><p id="90b1" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注意:研究表明LDA对这些假设的轻微违反是稳健的。</p><h2 id="a98d" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated">当第二个假设失败时会发生什么？</h2><figure class="ng nh ni nj gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1b9ed8704ad35a7c0b29a2c6c62d0deb.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*w_uxud_0gGPfqEjz6lgFPA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:作者图片</figcaption></figure><p id="e5c1" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当这一假设失败时，数据集中的类别之间存在巨大差异，并且使用判别分析的另一种变体，即<strong class="ko jq">二次判别分析</strong> ( <strong class="ko jq"> QDA)。</strong></p><p id="ce93" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在<a class="ae jd" href="https://en.wikipedia.org/wiki/Quadratic_classifier" rel="noopener ugc nofollow" target="_blank">二次判别分析</a>中，分离类别的数学函数现在将是二次的而不是线性的，以实现分类。</p><h2 id="7816" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated"><strong class="ak">LDA的应用</strong></h2><ol class=""><li id="e672" class="nz oa jg ko b kp mu kt mv kx ob lb oc lf od lj oe of og oh bi translated">LDA在模式识别任务中应用最为广泛。比如基于属性分析客户行为模式。</li><li id="7cce" class="nz oa jg ko b kp oi kt oj kx ok lb ol lf om lj oe of og oh bi translated">图像识别，LDA可以区分类别。例如，面和非面、对象和非对象。</li><li id="f890" class="nz oa jg ko b kp oi kt oj kx ok lb ol lf om lj oe of og oh bi translated">在医学领域，根据特定疾病的症状将患者分为不同的组。</li></ol><h2 id="6bb8" class="mj lm jg bd ln mk ml dn lr mm mn dp lv kx mo mp lz lb mq mr md lf ms mt mh jm bi translated">结论</h2><p id="63ab" class="pw-post-body-paragraph km kn jg ko b kp mu kr ks kt mv kv kw kx mw kz la lb mx ld le lf my lh li lj ij bi translated">本文阐述了两种常用的降维技术主成分分析和线性判别分析的区别。然后介绍了线性判别分析的工作原理，以及它如何实现分类和降维。</p><p id="a7a2" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">希望你喜欢阅读这篇文章！</p><p id="7d47" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">请随时查看我关于</em><a class="ae jd" href="https://pranaviduvva.medium.com/" rel="noopener"><em class="lk">pranaviduvva at medium</em></a><em class="lk">的其他文章。</em></p><p id="9f96" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">感谢阅读！</em></p><p id="ad67" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">参考文献</strong></p><ol class=""><li id="7a57" class="nz oa jg ko b kp kq kt ku kx on lb oo lf op lj oe of og oh bi translated"><a class="ae jd" href="https://scikit-learn.org/stable/modules/lda_qda.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/lda_qda.html</a></li><li id="5311" class="nz oa jg ko b kp oi kt oj kx ok lb ol lf om lj oe of og oh bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Bayes%27_theorem</a></li><li id="53a0" class="nz oa jg ko b kp oi kt oj kx ok lb ol lf om lj oe of og oh bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Linear_discriminant_analysis</a></li><li id="867c" class="nz oa jg ko b kp oi kt oj kx ok lb ol lf om lj oe of og oh bi translated"><a class="ae jd" href="https://youtu.be/azXCzI57Yfc" rel="noopener ugc nofollow" target="_blank"> LDA </a>上的Statquest视频。</li></ol></div></div>    
</body>
</html>