<html>
<head>
<title>Why Should Euclidean Distance Not Be the Default Distance Measure?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么欧几里德距离不应该是默认的距离度量？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/why-should-euclidean-distance-not-be-the-default-distance-measure-e55d72bd16e2?source=collection_archive---------0-----------------------#2022-10-19">https://pub.towardsai.net/why-should-euclidean-distance-not-be-the-default-distance-measure-e55d72bd16e2?source=collection_archive---------0-----------------------#2022-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3e87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">技术、数据存储资源和计算资源的持续发展，导致数据量<strong class="jp ir"><em class="kl"/></strong>呈指数级增长。通常，数据挖掘应用程序涉及到处理这种海量的高维数据。</p><p id="81a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据挖掘中使用的主要技术依赖于从<strong class="jp ir"> <em class="kl">欧几里德距离</em> </strong>中推导出的度量来评估对象之间的距离差异。然而，许多研究表明，由于被称为<em class="kl">“维数灾难”的现象，这些现有技术中的大多数不适用于高维数据。</em>当邻近、距离或最近邻等概念随着数据集维度的增加变得不那么有意义时，这种挖掘高维数据的诅咒就出现了。<a class="ae km" href="http://kops.uni-konstanz.de/bitstream/handle/123456789/5849/P506.pdf?sequence=1&amp;isAllowed=y" rel="noopener ugc nofollow" target="_blank">研究</a>表明<strong class="jp ir">随着维度的增加，最远点和最近点之间的相对距离收敛到0<em class="kl">d:</em></strong></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kn"><img src="../Images/0f7f622797dbb785adc1fdf2aa7bbc61.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*zO1OG9Wh60IScQvOlr4v5w.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">来源:高维空间中欧氏距离的有效性</figcaption></figure><p id="1354" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说，样本总体中最近邻和最远邻之间的区分在高维空间中变得相当弱。</p><p id="ecc0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，<a class="ae km" href="https://bib.dbvis.de/uploadedFiles/155.pdf" rel="noopener ugc nofollow" target="_blank">关于高维空间中距离度量的令人惊讶的行为的论文</a>证明了欧几里德(L <em class="kl"> 2 </em>范数<em class="kl">(在此阅读更多关于向量范数</em><a class="ae km" href="https://machinelearningmastery.com/vector-norms-machine-learning/" rel="noopener ugc nofollow" target="_blank"><em class="kl"/></a><em class="kl">)</em>)对于高维数据挖掘应用来说通常不是一个理想的度量。对于种类繁多的距离函数，<em class="kl">由于距离在高维空间中的集中性，最近邻和最远邻到给定目标的距离之比几乎是1</em>。因此，不同数据点之间的距离<strong class="jp ir">没有变化。</strong></p><blockquote class="kz la lb"><p id="313c" class="jn jo kl jp b jq jr js jt ju jv jw jx lc jz ka kb ld kd ke kf le kh ki kj kk ij bi translated">通常，大多数现实世界的问题在高维数据空间中操作，因此，欧几里德距离通常不是高维数据挖掘应用的理想度量。</p></blockquote></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="bbbb" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated"><strong class="ak">对于高维数据有没有更好的距离度量？</strong></h1><p id="0cab" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">前面提到的<a class="ae km" href="https://bib.dbvis.de/uploadedFiles/155.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>研究了高维空间中L <em class="kl"> k </em>范数的行为。基于这些结果，<strong class="jp ir">对于高维度的给定值<em class="kl"> d </em>，使用较低值的<em class="kl"> k </em> </strong>可能更好。换句话说，对于高维应用来说，<em class="kl"> L1距离</em>比L <em class="kl"> 2 </em>更有利。</p><p id="a6a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Sohangir和王在论文<a class="ae km" href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0083-6" rel="noopener ugc nofollow" target="_blank">中提出了一种<strong class="jp ir">改进的sqrt-cosine (ISC) </strong>相似性度量方法。ISC表述如下:</a></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/e9a7942c73e6d38c6a416ad12d318b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*WziO_Q2RO8EtLd5FwJDa2g.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">来源:<a class="ae km" href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0083-6" rel="noopener ugc nofollow" target="_blank">改进的sqrt余弦相似性度量</a></figcaption></figure><p id="d783" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> ISC </strong>是<a class="ae km" href="https://en.wikipedia.org/wiki/Hellinger_distance" rel="noopener ugc nofollow" target="_blank">海灵格距离</a>的延伸，属于L <em class="kl"> 1 </em>范数(已经证明在高维数据中，L <em class="kl"> 1 </em>范数比L <em class="kl"> 2 </em>范数效果更好)。在上面的等式中，我们没有使用L <em class="kl"> 1 </em>范数，<strong class="jp ir">T33】而是使用L1范数的平方根。 </strong></p><p id="a70f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文认为余弦相似性是相似性度量的“最新发展”。<em class="kl">与余弦相似度和其他用于测量高维数据空间相似度的流行技术</em>相比，ISC表现良好。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/7515a78f61f82f6fb3b7ba76a98f6a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*o4GIvmOKOMqm8T89jXkjBg.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">来源:<a class="ae km" href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0083-6" rel="noopener ugc nofollow" target="_blank">改进的sqrt余弦相似性度量</a></figcaption></figure></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="cfda" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">选择“最佳”距离测量的路标</h1><p id="9b33" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">我们必须明白，也许永远不会有“最佳”的距离度量，但总会有一个<em class="kl">“正确的】度量</em>。选择正确的距离度量取决于数据分布、数据维度、数据类型、我们追求的期望/目标等因素。例如，如果我们正在处理文本数据，我们知道<em class="kl">余弦相似度</em>往往工作得更好。类似地，具有较高噪声和异常值的数据可能不容易用欧几里德距离来处理。同样，欧几里德距离在二维数据上表现非常好，其目的是测量大小。</p><p id="44f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">论文摘要<a class="ae km" href="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0144059&amp;type=printable" rel="noopener ugc nofollow" target="_blank">对连续数据聚类中相似性和相异性度量的比较研究</a>，为选择<em class="kl">“正确的”距离度量提供了相当多的指导。</em></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/3881c1750dddc09673c8901fca6c687e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*9aMeSDwu3W-C7s7sTyAWbg.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">来源:【https://journals.plos.org/plosone/article/file? T4】id = 10.1371/journal . pone . 0144059&amp;type = printable</figcaption></figure></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="a7da" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">结论</h1><p id="8c3d" class="pw-post-body-paragraph jn jo iq jp b jq mk js jt ju ml jw jx jy mm ka kb kc mn ke kf kg mo ki kj kk ij bi translated">通常，大多数现实世界的问题在高维数据空间中操作，这使得欧几里德距离不是最理想的度量。对于高维数据空间，<strong class="jp ir">改进的sqrt-cosine (ISC) </strong>已经被证明比大多数测度表现更好。然而，不可能有一种适合所有情况的距离度量。数据从业者必须仔细检查数据分布、数据维度、数据类型、噪音、我们追求的期望/目标等因素。，在选择‘右’度量和<strong class="jp ir">之前，停止设置‘欧几里德距离为默认距离度量’。</strong></p><p id="b143" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，感谢你耐心阅读到最后，如果你觉得这篇文章有用，那么给我一两下掌声吧！如果没有，请回复你的评论和问题；我很乐意回答并连接到Linkedin 上进行讨论。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="f382" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献:</strong></p><p id="a4a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae km" href="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0144059&amp;type=printable" rel="noopener ugc nofollow" target="_blank">https://journals.plos.org/plosone/article/file?id = 10.1371/journal . pone . 0144059&amp;type = printable</a></p><p id="505e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">b)<a class="ae km" href="https://www.researchgate.net/profile/Jonathan-Goldstein-8/publication/2845566_When_Is_Nearest_Neighbor_Meaningful/links/09e4150b3eb298bf21000000/When-Is-Nearest-Neighbor-Meaningful.pdf" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/profile/Jonathan-Goldstein-8/publication/2845566 _ When _ Is _ Nearest _ Neighbor _ Meaningful/links/09e 4150 B3 EB 298 BF 21000000/When-Is-Nearest-Neighbor-Meaningful . pdf</a></p><p id="35d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">c)<a class="ae km" href="http://kops.uni-konstanz.de/bitstream/handle/123456789/5849/P506.pdf?sequence=1&amp;isAllowed=y" rel="noopener ugc nofollow" target="_blank">http://kops . uni-Konstanz . de/bitstream/handle/123456789/5849/p506 . pdf？sequence=1 &amp; isAllowed=y </a></p><p id="a538" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">https://bib.dbvis.de/uploadedFiles/155.pdf</p><p id="65d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">e) <a class="ae km" href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0083-6" rel="noopener ugc nofollow" target="_blank">https://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0083-6 </a></p></div></div>    
</body>
</html>