<html>
<head>
<title>A Lite BERT for Reducing Inference Time</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个减少推理时间的Lite BERT</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-lite-bert-for-reducing-inference-time-bed8d990daac?source=collection_archive---------2-----------------------#2019-12-08">https://pub.towardsai.net/a-lite-bert-for-reducing-inference-time-bed8d990daac?source=collection_archive---------2-----------------------#2019-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8966" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">伯特</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/44ae5fd84209957019e6214694d45308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y-GFg7qKlXoF8Oi2"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@dearseymour?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Ksenia Makagonova </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="d5a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank"> BERT </a> (Devlin等人，2018)在2018年取得了许多最先进的成果。然而，在生产中使用BERT (Devlin等人，2018年)并不容易，即使是小尺寸实验。BERT的基础版本(Devlin等人，2018年)包括108M参数，而X-Large版本包括1270M参数。</p><p id="78fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于极大数量的参数，引入了两个主要问题。首先，它需要较大的占地面积，并且由于资金问题，它可能无法在生产中轻松扩展。另一方面，与其他简单模型相比，训练和推理时间更长。</p><p id="01c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，不同的研究人员提出了一种减少模型足迹的方法。在这个故事中，我们将经历<strong class="lb iu">一个Lite BERT </strong> ( <code class="fe lv lw lx ly b">ALBERT</code>)(兰等，2019)</p><h1 id="8513" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">小伯特</h1><p id="dfbf" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated"><code class="fe lv lw lx ly b">ALBERT</code>(兰等，2019)提出减少模型占用空间，提高模型稳健性。参数数量仅在4.7%到18之间。%的传统BERT (Devlin等人，2018年)，训练速度比传统BERT (Devlin等人，2018年)快约1.7倍，而al BERT通过用句序预测(SOP)代替下一句预测(NSP)，实现了比传统BERT (Devlin等人，2018年)明显更好的性能。</p><p id="bc43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，Lan等人进行了几项研究，以确定最佳超参数，如网络宽度、附加数据、漏失效应。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/ec2d3132cf2cac5c7fcb1dc95431b9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUbbxIwo90KnSE1B0n2Pvw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">伯特和阿尔伯特之间的模型配置(兰等，2019)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/f31e4acfbcc39acf3bcc09df441dfa62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MzEYBW1ilNPIHDEAreCCfQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">模型间的比较(兰等，2019)</figcaption></figure><h2 id="4fae" class="my ma it bd mb mz na dn mf nb nc dp mj li nd ne ml lm nf ng mn lq nh ni mp nj bi translated">参数缩减</h2><p id="d7aa" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">Lan等人使用因子分解嵌入参数化和跨层共享参数技术来削减传统的BERT (Devlin等人，2018)。</p><p id="c87a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分解嵌入参数化指的是将令牌嵌入分解成两个小的嵌入矩阵。在应用这种分解之后，嵌入参数可以从(记号数目*隐藏层大小)减少到(记号数目*记号嵌入大小+记号嵌入大小*隐藏层大小)。当隐藏层尺寸远大于令牌嵌入尺寸时(例如1024 &gt;&gt; 300)，减少的比率是显著的。</p><p id="d43d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较大的嵌入尺寸(下图中的E)并不能保证较好的结果。当参数共享设置中嵌入大小为128时，效果最好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/e5fc0b1c30e0cfe6d3d3c8315e2d922f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sY23saVFnuxEizdKBBepfw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">改变嵌入尺寸的影响(即，Lan等人，2019年)</figcaption></figure><p id="b832" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">共享参数确实大大减少了参数的数量。ALBERT被配置为跨层共享所有参数，包括前馈网络和注意力参数。</p><p id="975c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从下图中可以看出，共享所有参数比其他参数效果更好，占用空间最小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/a4721ee0302eb68086ceb9a06b66dffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qn6niFmEkdkLRvqZdLkDpw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">改变参数共享的影响(Lan等人，2019年)</figcaption></figure><h2 id="28e3" class="my ma it bd mb mz na dn mf nb nc dp mj li nd ne ml lm nf ng mn lq nh ni mp nj bi translated">培训目标</h2><p id="bdad" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">除了减少参数外，Lan等人提出用句序预测(SOP)代替下一句预测(NSP)进行自监督损失训练，以提高模型性能。</p><p id="04dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NSP正在预测第一段和第二段是否是连续的段。几名研究人员发现，这可能不可靠，甚至是最糟糕的。蓝等推测难度太低，与掩蔽LM (MLM)训练重叠。SOP类似于NSP，但它仅预测两个段是否交换。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/6657e0936c68baa67ffeca16dddb4da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wtLd9KU0MokVLd8ZF07GxA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">改变培训目标的影响(兰等，2019)</figcaption></figure><h2 id="914a" class="my ma it bd mb mz na dn mf nb nc dp mj li nd ne ml lm nf ng mn lq nh ni mp nj bi translated">网络深度和宽度</h2><p id="0c2b" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">还是那句话，越深越好吗？Lan等人注意到，当层数达到48层时，性能出现下降。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/9ee9b575e1b7509da1837eb21afa5d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZ91DzrZ-8ML5dYhQJijTA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">改变ALBERT-xlarge模型层数的影响(Lan等人，2019年)</figcaption></figure><p id="7fd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">宽度也是这个问题，越宽越好？与网络深度实验相同，6144个隐藏大小的性能比4096个隐藏大小的性能差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/356316b6b06b8397db66730d0e3709ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hy-oaTcgYq5ia3xEXuYMHg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">更改ALBERT-xlarge模型的隐藏层大小的影响(Lan等人，2019年)</figcaption></figure><h2 id="4b11" class="my ma it bd mb mz na dn mf nb nc dp mj li nd ne ml lm nf ng mn lq nh ni mp nj bi translated">附加数据</h2><p id="1faa" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">本实验的目的是评估引入额外训练数据的效果。<a class="ae ky" href="https://medium.com/dataseries/why-does-xlnet-outperform-bert-da98a8503d5b" rel="noopener">、</a>(杨等，2019)和<a class="ae ky" href="https://medium.com/towards-artificial-intelligence/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6" rel="noopener">罗伯塔</a>(利特等，2019)证明，数据越多，性能越好。</p><p id="51df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更多的数据可能不会提高所有下游任务的模型性能。至少，当引入额外数据时，模型性能降级。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/7deb3cee7f54b426b84e0520dfdb05c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_Upn0tmjCx5PM9fy3PLUA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">额外数据的影响(兰等人，2019年)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/46818b5e7ba7b1b95caf7694e28f000e.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*htGcH8E_GaBhMFiYQGsrRw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">dev集合中附加数据的影响屏蔽了LM准确性(Lan等人，2019年)</figcaption></figure><h2 id="1933" class="my ma it bd mb mz na dn mf nb nc dp mj li nd ne ml lm nf ng mn lq nh ni mp nj bi translated">脱落效应</h2><p id="70d3" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">其他研究指出，批处理规范化和丢失可能会损害下游应用程序。兰等人做了一个关于脱层效应的实验。他们发现引入辍学会损害表现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/be8fd78047a2a1639d921f9a2d2df5d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ATV_dzLE3WPPJaG0EyLaPw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">辍学的影响(兰等，2019)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d3844d3a4a8fbdc3278d2b5d56345e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*sshzwzeHyz-ewiG-kbcZyA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">dev集合中的丢失对掩蔽LM精度的影响(Lan等人，2019年)</figcaption></figure><h1 id="e98e" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学的最新发展，尤其是NLP、数据扩充和平台相关领域。在<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上随时联系<a class="ae ky" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>。</p><h1 id="be7f" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">延长</h1><ul class=""><li id="8164" class="nt nu it lb b lc mr lf ms li nv lm nw lq nx lu ny nz oa ob bi translated"><a class="ae ky" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">伯特解说</a></li><li id="17b2" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><a class="ae ky" href="https://medium.com/dataseries/why-does-xlnet-outperform-bert-da98a8503d5b" rel="noopener"> XLM解释</a></li><li id="59c1" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><a class="ae ky" href="https://medium.com/towards-artificial-intelligence/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6" rel="noopener">罗伯塔解释</a></li><li id="c0f6" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">艾伯特实现(<a class="ae ky" href="https://github.com/google-research/google-research/tree/master/albert" rel="noopener ugc nofollow" target="_blank">张量流</a>)</li></ul><h1 id="1470" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">参考</h1><ul class=""><li id="e210" class="nt nu it lb b lc mr lf ms li nv lm nw lq nx lu ny nz oa ob bi translated">Z.兰、陈、古德曼、金佩尔、夏尔马和索里科特。<a class="ae ky" href="https://arxiv.org/pdf/1909.11942.pdf" rel="noopener ugc nofollow" target="_blank"> ALBERT:一个用于语言表达自我监督学习的Lite BERT</a>。2019</li></ul></div></div>    
</body>
</html>