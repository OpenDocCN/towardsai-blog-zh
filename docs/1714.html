<html>
<head>
<title>Fully Explained BIRCH Clustering for Outliers with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python对异常值进行BIRCH聚类的完整解释</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fully-explained-birch-clustering-for-outliers-with-python-2ad6243f126b?source=collection_archive---------0-----------------------#2021-03-28">https://pub.towardsai.net/fully-explained-birch-clustering-for-outliers-with-python-2ad6243f126b?source=collection_archive---------0-----------------------#2021-03-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="bbb2" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="b067" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">聚类中的无监督学习为数据建立树</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/8d3197309fac646dceb807e79aec8977.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*79TIYTgoB76mtmJ_X5yP4g.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">一张来自<a class="ae ld" href="https://scikit-learn.org/stable/modules/clustering.html#birch" rel="noopener ugc nofollow" target="_blank"> Sklearn </a>的照片</figcaption></figure><p id="88fe" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">该系列的另一篇文章全面解释了机器学习算法，即无监督学习中的BIRCH聚类。</p><h2 id="d022" class="ma mb it bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iz bi translated">介绍</h2><p id="9690" class="pw-post-body-paragraph le lf it lg b lh ms kd lj lk mt kg lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">该算法用于执行基于树的层次聚类。这些树被称为CFT，即聚类特征树。桦树的完整形式是用<strong class="lg jd"> H </strong>层次平衡<strong class="lg jd"> B </strong>平衡<strong class="lg jd"> I </strong>平衡<strong class="lg jd"> R </strong>导出<strong class="lg jd"> C </strong>光泽。BIRCH集群的使用情形如下:</p><ul class=""><li id="140a" class="mx my it lg b lh li lk ll ln mz lr na lv nb lz nc nd ne nf bi translated">大型数据集</li><li id="d61f" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated">离群点检测</li><li id="3645" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated">数据简化。</li></ul><p id="89e1" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">在这个群集中用于测量距离的度量是欧几里得距离测量。</p><h2 id="fb4c" class="ma mb it bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iz bi translated">优势</h2><p id="2c88" class="pw-post-body-paragraph le lf it lg b lh ms kd lj lk mt kg lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">有几点BIRCH在聚类算法中非常有用，如下所示:</p><ul class=""><li id="17b9" class="mx my it lg b lh li lk ll ln mz lr na lv nb lz nc nd ne nf bi translated">处理数据集中的噪声非常有用。</li><li id="9cfd" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated">发现聚类和子聚类的良好质量是有用的。</li><li id="336d" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated">需要较少的数据集扫描过程以降低I/O成本，这是存储器有效的。</li><li id="15ca" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated">它远远优于DBSCAN聚类算法。</li></ul><h2 id="bfc9" class="ma mb it bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iz bi translated">不足之处</h2><p id="b88d" class="pw-post-body-paragraph le lf it lg b lh ms kd lj lk mt kg lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">每种算法都有优点，但也有缺点，需要研究人员进一步研究，如下所示:</p><ul class=""><li id="23da" class="mx my it lg b lh li lk ll ln mz lr na lv nb lz nc nd ne nf bi translated">用SS值进行距离计算的数值问题会得到较差的精度。</li></ul><div class="nl nm gp gr nn no"><a rel="noopener  ugc nofollow" target="_blank" href="/fully-explained-svm-classification-with-python-eda124997bcd"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd jd gy z fp nt fr fs nu fu fw jc bi translated">用Python全面解释了SVM分类</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">如何用一个真实的例子解决分类问题。</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">pub.towardsai.net</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc kx no"/></div></div></a></div><div class="nl nm gp gr nn no"><a rel="noopener  ugc nofollow" target="_blank" href="/step-by-step-basic-understanding-of-neural-networks-with-keras-in-python-94f4afd026e5"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd jd gy z fp nt fr fs nu fu fw jc bi translated">使用Python中的Keras逐步基本了解神经网络</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">具有定义的神经网络的学习</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">pub.towardsai.net</p></div></div><div class="nx l"><div class="od l nz oa ob nx oc kx no"/></div></div></a></div><h2 id="c4ab" class="ma mb it bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iz bi translated">什么是MiniBatchKMeans？</h2><p id="59ef" class="pw-post-body-paragraph le lf it lg b lh ms kd lj lk mt kg lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">当我们得到一个大规模数据集，而BIRCH由于使用整个数据集的内存限制而无法满足要求时，我们应该考虑从数据集中提取固定大小的小批量数据，以减少运行时间。</p><p id="1a58" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">通过在每一次迭代中获得学习率来降低计算成本，但是它也影响聚类质量。</p><h2 id="e4b7" class="ma mb it bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iz bi translated">桦树聚类的步骤</h2><p id="82f9" class="pw-post-body-paragraph le lf it lg b lh ms kd lj lk mt kg lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">BIRCH算法由下面讨论的4个主要步骤组成:</p><ul class=""><li id="8d53" class="mx my it lg b lh li lk ll ln mz lr na lv nb lz nc nd ne nf bi translated"><strong class="lg jd">第一步:</strong>从输入数据建立CF树，CF由三个值组成。第一个是输入(N)，第二个是线性和(LS)，第三个是数据的平方和(SS)。</li><li id="ca06" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated"><strong class="lg jd">第二步:</strong>为了重建更小的CF树，它在初始CF树中搜索叶条目。在此过程中，它将离群值和子聚类组移至主聚类。</li><li id="c553" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated"><strong class="lg jd">第三步:</strong>在这一步中，用户可以用一个阈值参数定义多个聚类。在叶条目上使用凝聚聚类来从CF向量中产生聚类。</li><li id="d1ad" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated"><strong class="lg jd">在第四步:</strong>在步骤3中引入质心或种子，以将聚类重新排列成新的聚类，同时减少离群值。</li></ul><h2 id="4555" class="ma mb it bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iz bi translated">桦木参数</h2><p id="75a0" class="pw-post-body-paragraph le lf it lg b lh ms kd lj lk mt kg lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">BIRCH聚类的主要参数如下所示:</p><ul class=""><li id="648a" class="mx my it lg b lh li lk ll ln mz lr na lv nb lz nc nd ne nf bi translated"><strong class="lg jd">阈值:</strong>新样本所在子簇的半径。阈值的默认值为0.5，在启动时应该尽可能低。</li><li id="ae41" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated"><strong class="lg jd">分支因子:</strong>用于统计每个节点的子簇总数。如果新样本在上述值之后进入，则子集群在该节点处进一步分裂。默认值是50个分支。</li><li id="4fee" class="mx my it lg b lh ng lk nh ln ni lr nj lv nk lz nc nd ne nf bi translated"><strong class="lg jd"> N_clusters: </strong>就是聚类的个数。</li></ul><h2 id="d56f" class="ma mb it bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iz bi translated">python的例子</h2><p id="aae7" class="pw-post-body-paragraph le lf it lg b lh ms kd lj lk mt kg lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">python示例将生成五个包含500个随机数据点的聚类。</p><pre class="ks kt ku kv gt oe of og oh aw oi bi"><span id="6ac0" class="ma mb it of b gy oj ok l ol om"># Import the libraries<br/>import matplotlib.pyplot as plt<br/>from sklearn.cluster import Birchfrom sklearn.datasets.samples_generator import make_blobs<br/><br/>  <br/># Randomly generating 500 samples using make_blobs<br/>data, clusters = make_blobs(n_samples = 500, centers = 5, cluster_std = 0.75, random_state = 0)<br/>  <br/># BIRCH<em class="on"> </em>Model algorithm<br/>model = Birch(branching_factor = 50, n_clusters = None, threshold = 1.5)<br/>  <br/># Fitting the training data<br/>model.fit(data)<br/>  <br/># Predict the same data<br/>pred = model.predict(data)</span></pre><p id="3549" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">借助Matplotlib库可视化聚类</p><pre class="ks kt ku kv gt oe of og oh aw oi bi"><span id="ab82" class="ma mb it of b gy oj ok l ol om">plt.scatter(dataset[:, 0], dataset[:, 1], c = pred, cmap =<br/>            'rainbow', alpha = 0.9, edgecolors = 'b')<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/b9b0e03db3f981c37da7bb2a0e3daa51.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*pPPH7q2ovb13bPJyol01vQ.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">集群。作者的照片</figcaption></figure><h2 id="a715" class="ma mb it bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr iz bi translated">结论:</h2><p id="87e7" class="pw-post-body-paragraph le lf it lg b lh ms kd lj lk mt kg lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated">BIRCH聚类优于另一种聚类算法，尤其是K-Means聚类。这有利于剔除异常值和提高内存效率。虽然，SS的数值问题可以通过具有均值和偏差方法的桦树聚类特征来解决。</p><div class="nl nm gp gr nn no"><a rel="noopener  ugc nofollow" target="_blank" href="/become-a-data-scientist-in-2021-with-these-following-steps-5bf70a0fe0a1"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd jd gy z fp nt fr fs nu fu fw jc bi translated">按照以下步骤，在2021年成为一名数据科学家</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">走上数据科学家之路需要具备的基本点</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">pub.towardsai.net</p></div></div><div class="nx l"><div class="op l nz oa ob nx oc kx no"/></div></div></a></div><p id="66dd" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae ld" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae ld" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="18dd" class="oq mb it bd mc or os ot mf ou ov ow mi ki ox kj ml kl oy km mo ko oz kp mr pa bi translated">推荐文章</h1><p id="c36b" class="pw-post-body-paragraph le lf it lg b lh ms kd lj lk mt kg lm ln mu lp lq lr mv lt lu lv mw lx ly lz im bi translated"><a class="ae ld" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄与Python </a> <br/> 2。<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a>T5】3 .<a class="ae ld" rel="noopener ugc nofollow" target="_blank" href="/data-preprocessing-concepts-with-python-b93c63f14bb6?source=friends_link&amp;sk=5cc4ac66c6c02a6f02077fd43df9681a">数据预处理概念同Python </a> <br/> 4。<a class="ae ld" rel="noopener ugc nofollow" target="_blank" href="/principal-component-analysis-in-dimensionality-reduction-with-python-1a613006d531?source=friends_link&amp;sk=3ed0671fdc04ba395dd36478bcea8a55">用Python进行主成分分析降维</a> <br/> 5。<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a?source=friends_link&amp;sk=9c5c613ceb10f2d203712634f3b6fb28" rel="noopener">用Python全面讲解K-means聚类</a> <br/> 6。<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/basic-of-time-series-with-python-a2f7cb451a76?source=friends_link&amp;sk=09d77be2d6b8779973e41ab54ebcf6c5" rel="noopener">用Python实现时间序列的基础知识</a> <br/> 9。<a class="ae ld" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a> <br/> 10。<a class="ae ld" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>