<html>
<head>
<title>NTM: Neural Turing Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NTM:神经图灵机</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/neural-turing-machines-eaada7e7a6cc?source=collection_archive---------0-----------------------#2019-12-01">https://pub.towardsai.net/neural-turing-machines-eaada7e7a6cc?source=collection_archive---------0-----------------------#2019-12-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4b19" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">神经图灵机的详细走查</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5c9301986551bb40648d0b169870bf90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BYeIzZbSPLxrxdS_qvxdJQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/s/photos/technology?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@freeche?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Kvistholt摄影</a>拍摄</figcaption></figure><h1 id="da99" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="465a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们讨论神经图灵机(NTM)，一种由Graves等人在DeepMind中提出的架构。ntm旨在解决需要向外部存储器写入信息和从外部存储器检索信息的任务，这使其类似于工作记忆系统，可以通过信息的短期存储(记忆)及其基于规则的操作来描述。与具有内部存储器的RNN结构相比，ntm利用注意机制来高效地读写外部存储器，这使得它们成为捕获长程相关性的更有利的选择。但是，正如我们将看到的，这两者并不是相互独立的，可以结合起来形成一个更强大的体系结构。</p><h1 id="e7e5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">神经图灵机</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/02bdb85d480f585eef3dde7d2ad41a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S11Qm_G1ng7VVG8l_t6T9w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">资料来源:亚历克斯·格雷夫斯、格雷格·韦恩和伊沃·达尼埃尔卡。2014.“神经图灵机”</figcaption></figure><p id="c598" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">NTM的整体架构如图1所示，其中<em class="mt">控制器</em>是一个普通的神经网络，一个MLP或RNN，它接收输入和先前的读取向量，并作为响应省略输出。此外，它通过一组并行读写头从存储器矩阵中读取和写入。<em class="mt">内存</em>是一个<em class="mt"> N ⨉ W </em>矩阵，其中<em class="mt"> N </em>是内存位置(行)的数量，<em class="mt"> W </em>是每个位置的向量大小。</p><h2 id="bc1d" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">阅读</h2><p id="3c04" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在每个时间步<em class="mt"> t </em>，读取头输出在<em class="mt"> N </em>个位置上的归一化权重的矢量<strong class="lt iu"><em class="mt">w</em></strong><em class="mt">_ t</em>。磁头返回的长度为<em class="mt"> W </em>的读取向量<strong class="lt iu"><em class="mt">r</em></strong><em class="mt">_ t</em>定义为行向量<strong class="lt iu"><em class="mt">M</em></strong><em class="mt">_ t(I)</em>的加权组合:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/8086c70cbafda968efb7afd3099a67d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*eafnrj0FKMHdXmhiEU3PnQ.png"/></div></div></figure><p id="4644" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们可以将上面的等式矢量化如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/0b25b7f17b83a86f5dc987eca0935a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*QtZo5CPODZ7aJMMeT39Mvg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">等式1</figcaption></figure><h1 id="6ade" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">写作</h1><p id="3f4f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">写操作被分解成两个部分:一个<em class="mt">擦除</em>，随后是一个<em class="mt">添加</em>。在每个时间步<em class="mt"> t </em>，写头发出三个向量:一个<em class="mt">N</em>-维度加权向量<strong class="lt iu"> <em class="mt"> w </em> </strong> <em class="mt"> _t </em>，一个<em class="mt">W</em>-维度擦除向量<strong class="lt iu"><em class="mt">e</em></strong><em class="mt">_ t</em>，以及一个<em class="mt">W</em>-维度相加向量<strong class="lt iu"> <em class="mt"> a </em> </strong> <em class="mt">每个存储器向量<strong class="lt iu"><em class="mt">M</em></strong><em class="mt">_ { t-1 }(t)</em>首先被擦除向量修改:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1183a1408ce39a98ba8c3263a5fc1288.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*3TtBiDeMCuYJKs5yRKOBPw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">其中<strong class="bd lb"> <em class="nj"> 1 </em> </strong>是全1的行向量，针对存储位置的乘法是逐点进行的</figcaption></figure><p id="42c9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">则应用添加向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/210699046be75f81ac69859cc3b8a0c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*Fqzqjk9ym6Ay29Yl2PsdgA.png"/></div></figure><p id="6df4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们还可以将上述两种操作结合起来，并对它们进行矢量化，如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/50eec9a6268541b4c557a5eb0e48ea19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*qnmGLrQfDCfjR3lUSIGbgQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">等式2。其中<strong class="bd lb"> <em class="nj"> E </em> </strong>是具有与<strong class="bd lb"> <em class="nj"> M </em> </strong>相同形状的1的矩阵，并且∘表示逐元素乘法</figcaption></figure><h1 id="e5fa" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">寻址机制</h1><p id="165e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">到目前为止，我们已经展示了读写的等式，但是我们还没有描述权重是如何产生的。这些权重是通过将两种寻址机制与互补设施相结合而产生的:</p><ol class=""><li id="a9ba" class="nm nn it lt b lu mo lx mp ma no me np mi nq mm nr ns nt nu bi translated">第一种机制是基于内容的寻址，它基于位置与控制器发出的权重的相似性来关注位置。这可能是一种建立在余弦相似性基础上的注意机制，我们在讨论<a class="ae ky" href="https://medium.com/towards-artificial-intelligence/attention-is-all-you-need-transformer-4c34aa78308f?source=friends_link&amp;sk=a259e84597d542f812a155711e9c8e97" rel="noopener">变压器</a>时已经看到过。</li><li id="481d" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated">第二种机制是基于位置的寻址，它根据变量的位置而不是内容来识别变量。这种机制对于算术问题非常有用:例如，变量<em class="mt"> x </em>和<em class="mt"> y </em>可以取任何值，但是只要它们的地址被识别，过程<em class="mt"> f(x，y)= x ⨉ y </em>仍然应该被定义。</li></ol><p id="339a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">请注意，基于内容的寻址比基于位置的寻址更普遍，因为内存位置的内容可能包含位置信息。然而，在实验中，Graves等人发现提供基于位置的寻址作为原始操作对于某些形式的一般化是必要的，因此他们同时使用了这两种机制。图2展示了整个寻址机制。我们将在下面的小节中一步一步地介绍它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/0fde94a0b886b58daa9128c74de687d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t3hZm1SLsgpxL-HTo7E2sA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">资料来源:亚历克斯·格雷夫斯、格雷格·韦恩和伊沃·达尼埃尔卡。2014.“神经图灵机”</figcaption></figure><h2 id="9954" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">基于内容的寻址</h2><p id="d918" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于内容寻址，我们基于<strong class="lt iu"> <em class="mt"> M </em> </strong> <em class="mt"> _t </em>和<em class="mt">w<strong class="lt iu"><em class="mt">k</em></strong><em class="mt">_ t</em>的行之间的余弦相似度来计算归一化权重<strong class="lt iu"> <em class="mt"> w </em> </strong> <em class="mt"> _t^c </em>，为了调整相似度的有效性，我们另外引入了一个正的密钥强度<em class="mt"> 𝛽_t </em>，它可以这导致以下加权</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e604a6034b1b306d6c48689ff486f0ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*TSE13Sj3f1yFO5k5HXNBtQ.png"/></div></figure><p id="de78" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们可以将这种计算矢量化为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c58a667c81e3fee9007157bacaf3530d.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*SDAQV99WhO6pffwrmp6_6w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">等式3。其中\波浪号M是具有l2规范化行的M，而\波浪号k是l2规范化的k，</figcaption></figure><h2 id="a7e6" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">基于位置的寻址</h2><p id="91f8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">基于位置的寻址机制被设计成便于跨越存储器位置的简单迭代和随机存取跳转。这是通过实现加权的旋转移位来实现的。</p><p id="26ac" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在旋转之前，每个头发出一个范围为<em class="mt"> (0，1 ) </em>的标量插值门<em class="mt"> g_t </em>。<em class="mt"> g_t </em>的值用于融合头部在前一时间步产生的权重<em class="mt"/><strong class="lt iu"><em class="mt">w</em></strong><em class="mt">_ { t-1 }</em>和基于内容的权重<strong class="lt iu"><em class="mt">w</em></strong><em class="mt">_t^c</em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9b47b72dceae44b44ac9ba131124636d.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*LhXLOVmqZvPQNAoJ9r93Zg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">等式4</figcaption></figure><p id="df21" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">尤其是当<em class="mt"> g_t=0 </em>、<strong class="lt iu">、<em class="mt"> w </em>、</strong>、<em class="mt"> _t^g </em>等于<strong class="lt iu">、<em class="mt"> w </em>、</strong>、<em class="mt"> _{t-1}、</em>时，完全省略了基于内容的加权，简单地专注于位置移位。</p><p id="12a9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">插值后，我们对加权应用1-D卷积移位核，其参数<strong class="lt iu"><em class="mt">s</em></strong><em class="mt">_ t</em>是由头部产生的归一化分类分布，必要时用零填充(例如参见最后的代码):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/353214a43c2bb8fa5a926704d7521687.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*waQZR4OtFbrc6_M-PJSaQA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">等式5</figcaption></figure><p id="f921" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">其中所有索引算法都是以<em class="mt"> N </em>为模计算的。如果移位加权<strong class="lt iu"><em class="mt"/></strong><em class="mt">_ t</em>不尖锐，这种卷积运算会导致权重随时间的泄漏或分散。为了解决这一问题，每个头发射另一个标量<em class="mt"> 𝛾_t≥1 </em>，其作用是锐化最终权重，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/92b254cb3089ece6fcdfbfa29c742e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*1ndi0wUSOJDpvIEmW9pYeA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">等式6</figcaption></figure><p id="9a4f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">值得注意的是，正如Graves等人在实验中提到的，他们发现方程4和方程6在训练数据范围之外的NTM推广中起着重要作用。</p><h2 id="0e9b" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">摘要</h2><p id="866b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在我们将寻址机制总结如下:</p><ol class=""><li id="a13a" class="nm nn it lt b lu mo lx mp ma no me np mi nq mm nr ns nt nu bi translated">使用等式3计算内容权重<strong class="lt iu"><em class="mt">w</em></strong><em class="mt">_t^c</em>。</li><li id="b467" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated">通过将内容权重<strong class="lt iu"> <em class="mt"> w </em> </strong> <em class="mt"> _t^c </em>内插到先前的权重<em class="mt"/><strong class="lt iu"><em class="mt">w</em></strong><em class="mt">_ { t-1 }</em>中来计算门控权重<strong class="lt iu"> <em class="mt"> w </em> </strong> <em class="mt"> _t^g </em>，这是通过等式4来完成的。</li><li id="5c99" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated">将1D卷积核应用于门控加权，以获得基于局部的加权<em class="mt">\波浪号</em><strong class="lt iu"><em class="mt">w</em></strong><em class="mt">_ t</em>遵循等式5。</li><li id="9d96" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated">使用等式6通过锐化<em class="mt">\波浪号</em><strong class="lt iu"><em class="mt">w</em></strong><em class="mt">_ t</em>来计算最终权重。</li></ol><p id="a6aa" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">加权插值和基于内容和位置的寻址的组合寻址系统可以在三种互补模式下操作:</p><ol class=""><li id="cc1d" class="nm nn it lt b lu mo lx mp ma no me np mi nq mm nr ns nt nu bi translated">内容系统可以选择权重，而无需位置系统进行任何修改，在这种情况下，对于j≠0，和<em class="mt">γt = 1</em>，我们有<em class="mt"> g_t=1，s_t(0)=1，s_t(j)=0。</em></li><li id="04e4" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated">由内容寻址系统产生的权重可以被选择，然后被移动。这允许焦点跳转到内容访问的地址旁边的位置，而不是在该地址上；在计算方面，这允许磁头找到一个连续的数据块，然后访问该数据块中的特定元素</li><li id="9c2c" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated">来自先前时间步长的权重可以被旋转，而无需来自基于内容的寻址系统的任何输入(<em class="mt"> g_t=0 </em>)。这允许加权通过在每个时间步长推进相同的距离来迭代通过地址序列。</li></ol><h1 id="3df4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">履行</h1><p id="fc65" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本节中，我们将进一步讨论Collier&amp;Beel工作中的一些实现细节。</p><h2 id="4098" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">存储器内容初始化</h2><p id="c568" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">他们尝试了三种不同的内存内容初始化:</p><ol class=""><li id="b25d" class="nm nn it lt b lu mo lx mp ma no me np mi nq mm nr ns nt nu bi translated"><em class="mt">常量初始化</em>:所有内存位置初始化为<em class="mt"> 10^{-6} </em></li><li id="fc58" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><em class="mt">学习初始化</em>:我们在训练时通过内存初始化反向传播</li><li id="a3b2" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><em class="mt">随机初始化</em>:每个内存位置被初始化为一个从截尾正态分布中提取的值，平均值<em class="mt"> 0 </em>和标准偏差<em class="mt"> 0.5 </em>。</li></ol><p id="8795" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">他们发现常量初始化比其他两种方案更有效。</p><h2 id="c086" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">其他参数初始化</h2><p id="952a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">它们通过读取向量<strong class="lt iu"><em class="mt">r</em></strong><em class="mt">_ 0</em>和地址权重<strong class="lt iu"><em class="mt">w</em></strong><em class="mt">_ 0</em>的初始化来反向传播，而不是将它们初始化为偏置值。</p><h2 id="5882" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">相似性测度</h2><p id="be67" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">他们使用余弦相似性，如等式3所示</p><h2 id="816b" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">控制器输入</h2><p id="9418" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在每个时间步<em class="mt"> t </em>，控制器接收来自环境的输入，并从NTM的所有读取头读取向量<strong class="lt iu"><em class="mt"/></strong><em class="mt">_ { t-1 }</em>。</p><h2 id="8054" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">参数非线性</h2><p id="a936" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">他们申请</p><ul class=""><li id="fcd0" class="nm nn it lt b lu mo lx mp ma no me np mi nq mm og ns nt nu bi translated">tanh函数对关键向量<strong class="lt iu"> <em class="mt"> k </em> </strong> <em class="mt"> _r </em>和添加向量<strong class="lt iu"><em class="mt">a</em></strong><em class="mt">_ t</em></li><li id="9b31" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm og ns nt nu bi translated">sigmoid函数对擦除向量<strong class="lt iu"><em class="mt">e</em></strong><em class="mt">_ t</em>和门<em class="mt"> g_t </em></li><li id="472c" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm og ns nt nu bi translated">softplus函数用于键入强度以满足约束<em class="mt"> 𝛽_t &gt; 0 </em></li><li id="40d3" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm og ns nt nu bi translated">softmax函数对卷积移位向量<strong class="lt iu"><em class="mt">s</em></strong><em class="mt">_ t</em></li><li id="6b2e" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm og ns nt nu bi translated">oneplus功能(<em class="mt">【1+log(1+e^x】)</em>)锐化因子</li></ul><h2 id="f4e3" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">等式5的代码</h2><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="02db" class="mu la it oi b gy om on l oo op">s = tf.concat(<br/>  [s[:, :shift_range + 1],<br/>  tf.zeros([s.get_shape()[0], memory_size - (shift_range * 2 + 1)]),<br/>  s[:, -shift_range:]], axis=1<br/>)<br/>t = tf.concat([tf.reverse(s, axis=[1]), tf.reverse(s, axis=[1])], axis=1)<br/>s_matrix = tf.stack(<br/>    [t[:, memory_size - i - 1: memory_size * 2 - i - 1] <br/>     for i in range(memory_size)],<br/>    axis=1<br/>)<br/>w_ = tf.reduce_sum(tf.expand_dims(w_g, axis=1) * s_matrix, axis=2)      # Equation 5</span></pre><p id="2dd8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">让我们举一个简单的例子来更清楚地看到这个过程。假设内存大小(即<em class="mt"> N </em>)为<em class="mt"> 4 </em>，移位范围为<em class="mt"> 1 </em>，<em class="mt">s =【0.5，0.2，0.3】</em>，其中<em class="mt"> 0.5 </em>为中心质量(为简单起见，此处省略批量尺寸)。那么我们有</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="518f" class="mu la it oi b gy om on l oo op">w_g = np.array([<br/>  [.1, .2, .3, .4]<br/>])<br/>s = np.array([.5, .2, 0, .3])<br/>t = np.array([.3, 0, .2, .5, .3, 0, .2, .5])<br/>s_matrix = np.array([<br/>  [.5, .3, 0, .2],<br/>  [.2, .5, .3, 0],<br/>  [0, .2, .5, .3],<br/>  [.3, 0, .2, .5]<br/>])<br/>w_ = np.sum(w_g * s_matrix, axis=1)<br/># &gt;&gt;&gt; array([0.19, 0.21, 0.31, 0.29])</span></pre><h1 id="b102" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="2d90" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">亚历克斯·格雷夫斯，格雷格·韦恩和伊沃·丹尼卡。2014.《神经图灵机》1–26。<a class="ae ky" href="http://arxiv.org/abs/1410.5401" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1410.5401</a>。</p><p id="14fa" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">科利尔，马克和乔兰·比尔。2018."实现神经图灵机。"计算机科学讲义(包括人工智能的子系列讲义和生物信息学的讲义)11141 LNCS:94–104。<a class="ae ky" href="https://doi.org/10.1007/978-3-030-01424-7_10" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/978-3-030-01424-7_10</a>。</p><p id="b050" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">代号:<a class="ae ky" href="https://github.com/MarkPKCollier/NeuralTuringMachine" rel="noopener ugc nofollow" target="_blank">https://github.com/MarkPKCollier/NeuralTuringMachine</a></p></div></div>    
</body>
</html>