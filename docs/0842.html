<html>
<head>
<title>Create your Mini-Word-Embedding from Scratch using Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pytorch从头开始创建迷你单词嵌入</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/create-your-own-mini-word-embedding-from-scratch-c7b32bd84f8e?source=collection_archive---------2-----------------------#2020-08-24">https://pub.towardsai.net/create-your-own-mini-word-embedding-from-scratch-c7b32bd84f8e?source=collection_archive---------2-----------------------#2020-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6034" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/81a251cbd672d0a5497dc4cded434a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-2NN_7MJZ1bAanADm67Ng.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">作者插图</figcaption></figure><h1 id="69d6" class="ko kp it bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">简介:</h1><p id="1c6d" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">更简单地说，特定单词<strong class="lo jd">(高维)</strong>的嵌入只不过是该单词<strong class="lo jd">(低维)的向量表示。</strong>凡词义相近的词<strong class="lo jd"> <em class="mk"> Ex。</em> </strong>“喜气洋洋”和其他密切相关的词如<strong class="lo jd"> <em class="mk"> Ex。“钱”和“银行”</em> </strong>，在低维投影时得到更接近的矢量表示。</p><p id="f1fe" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">从单词到向量的转换称为<a class="ae mq" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank"> <em class="mk">单词嵌入</em> </a></p><p id="6912" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">因此，创建迷你单词嵌入的基本概念可以归结为用一些文本数据训练一个简单的自动编码器。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="1205" class="ko kp it bd kq kr my kt ku kv mz kx ky kz na lb lc ld nb lf lg lh nc lj lk ll bi translated">一些基础知识:</h1><p id="6f06" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">在我们继续创建迷你单词嵌入之前，最好重温一下深度学习社区迄今为止提供的单词嵌入的基本概念。</p><p id="25df" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">流行的和最先进的单词嵌入模型如下</p><ol class=""><li id="e74b" class="nd ne it lo b lp ml lt mm lx nf mb ng mf nh mj ni nj nk nl bi translated">Word2Vec(谷歌)</li><li id="d00d" class="nd ne it lo b lp nm lt nn lx no mb np mf nq mj ni nj nk nl bi translated">手套(斯坦福大学)</li></ol><p id="6d47" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">他们在维基百科或整个网络搜集的海量文本语料上接受训练，高达<strong class="lo jd"><em class="mk">60亿字</em> </strong> <strong class="lo jd">(高维)</strong>，并将其投影到低至<em class="mk"> 100、200、300</em><strong class="lo jd">(低维)。</strong></p><p id="f7ed" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">在我们的模型中，我们将它们投射到两个密集嵌入中。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="6c29" class="ko kp it bd kq kr my kt ku kv mz kx ky kz na lb lc ld nb lf lg lh nc lj lk ll bi translated">使用的技术:</h1><p id="1056" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">上述最先进的模型使用两种主要技术中的任何一种来完成任务。</p><ol class=""><li id="cde0" class="nd ne it lo b lp ml lt mm lx nf mb ng mf nh mj ni nj nk nl bi translated">连续词袋</li><li id="6226" class="nd ne it lo b lp nm lt nn lx no mb np mf nq mj ni nj nk nl bi translated">跳跃图</li></ol><h2 id="5fda" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">1.CBOW:</h2><p id="d145" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">CBOW试图从其相邻单词(上下文单词)中猜测输出(目标单词)。窗口大小在这里是一个超参数。</p><h2 id="fab6" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">示例:</h2><p id="fd65" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">句子:猫和老鼠是好朋友</p><p id="e642" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">目标词(输出):<strong class="lo jd">老鼠</strong>(比方说)</p><p id="3540" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">语境词(输入):<strong class="lo jd">猫和</strong> _ <strong class="lo jd">是好朋友</strong></p><h2 id="ad45" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">2.跳过程序:</h2><p id="2738" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">Skip-Gram根据目标单词猜测上下文单词。我们将在本帖中实现这一点。</p><p id="62a1" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">句子:猫和老鼠是好朋友</p><p id="a240" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">目标词(输出):<strong class="lo jd">和</strong>，<strong class="lo jd">老鼠… </strong></p><p id="a229" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">上下文单词(输入):<strong class="lo jd">猫</strong>，<strong class="lo jd">猫……</strong></p><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/63db65971e6ac6923430a29ce06d09a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*SANI-0E8qSTHzGHg4HHtNA.png"/></div></figure><p id="a85b" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">稍后将详细介绍这些技术。</p><figure class="od oe of og gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oh"><img src="../Images/13a7f820e8118305f6167fd557bd10a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dEB7_sqWOHhV82EI.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">CBOW与Skip-gram</figcaption></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="354d" class="ko kp it bd kq kr my kt ku kv mz kx ky kz na lb lc ld nb lf lg lh nc lj lk ll bi translated">迷你单词嵌入过程:</h1><h2 id="4785" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">1.数据准备和数据预处理</h2><h2 id="c1dd" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">2.超参数选择和模型建立</h2><h2 id="4e5c" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">3.模型推理</h2></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="99d8" class="ko kp it bd kq kr my kt ku kv mz kx ky kz na lb lc ld nb lf lg lh nc lj lk ll bi translated">1.数据准备和数据预处理</h1><p id="6c84" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">有趣的部分来了，正如我之前所说，模型的上述状态使用了大量的文本数据来训练这些模型，因为我们对它的迷你版本感兴趣，所以我们选择一个小数据集。</p><p id="ca49" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">为了使事情更令人兴奋，我选择了汤姆和杰里卡通剧作为我们的数据语料库。</p><div class="od oe of og gt ab cb"><figure class="oi kd oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/b53fb00f942a9c6599a6ac3658818dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ans6Gx0TfQcZEeJC6NvvCQ.png"/></div></figure><figure class="oi kd oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/63c9d05119034cbe3c0a022eb0dd3acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*zf_Tv4CnX8JKr_TSa6i6iQ.png"/></div></figure><figure class="oi kd oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/dda430713d6a834290707101c94737a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*DIXgbr2QGmhGNIAy0Jq__w.png"/></div></figure></div><div class="ab cb"><figure class="oi kd oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/c87b1d8e730aad78a0ee79f2d6568de6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*SA3e1xjO1Y7nbZXSdzVtfA.png"/></div></figure><figure class="oi kd oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/e662b8d9665bab6de26d7c3451461b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*LCTxfRjWSTm1Kn4mQfVQ8A.png"/></div></figure><figure class="oi kd oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/56a475ea2eb0a7fe99ec799c4cc3db8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Xg92yNkLQLiUImKDP1Y3_A.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk oo di op oq translated">汤姆和杰瑞——玩</figcaption></figure></div><p id="75ef" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">我们的迷你数据集看起来像这样，</p><figure class="od oe of og gt kd"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="d4b8" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">因此，我们将使用上述数据，现在我们将开始预处理步骤。</p><ol class=""><li id="34ab" class="nd ne it lo b lp ml lt mm lx nf mb ng mf nh mj ni nj nk nl bi translated">首先，我们需要将每个唯一的单词映射到一个整数中，然后将这个整数映射到一键编码中。</li></ol><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/10a2573de654a36861da0abd3ddf4b56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*pDeswCr7eBhEDtiywMkAqQ.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">数据预处理</figcaption></figure><p id="429c" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">2.然后，一旦我们为每个单词创建了整数和一个热映射，现在我们将为训练创建批处理。</p><p id="6d19" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">由于我们具有有限的数据并实现了迷你单词嵌入，我们将考虑具有2的<strong class="lo jd">窗口大小的<strong class="lo jd">跳格</strong>模型(考虑相邻的2个单词作为目标)</strong>，并在给定上下文单词(输入)的情况下预测目标单词。</p><p id="f5bf" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">参考下图来了解我们的skip-gram模型。</p><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/63db65971e6ac6923430a29ce06d09a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*SANI-0E8qSTHzGHg4HHtNA.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">我们的训练批次</figcaption></figure><figure class="od oe of og gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ou"><img src="../Images/c5e468b5198db879edb4c0cab6382a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r_9zfDywyD1TbVpxKENHjw.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">样本数据格式</figcaption></figure><p id="b189" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">上述批量准备的代码实现如下所示。</p><figure class="od oe of og gt kd"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="od oe of og gt kd"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="od oe of og gt kd"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="od oe of og gt kd"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="od oe of og gt kd"><div class="bz fp l di"><div class="or os l"/></div></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="ca86" class="ko kp it bd kq kr my kt ku kv mz kx ky kz na lb lc ld nb lf lg lh nc lj lk ll bi translated">2.超参数选择和模型建立</h1><p id="186f" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">既然我们已经完成了批处理的创建，现在让我们为训练构建一个简单的自动编码器类型的模型。简而言之，它是一个神经网络，将高维压缩到低维，然后解压缩到高维。</p><p id="f8da" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">因此可以理解，较低的维度捕获了输入的重要特征，在我们的情况下，这是我们的<strong class="lo jd"> <em class="mk">单词嵌入了目标单词的</em> </strong>。</p><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/0464fe6261894da604c477631a49e9ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*rb9i3dT_rH3DB31atto_Ag.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">自动编码器设计</figcaption></figure><p id="e24a" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">这里从上面的设计，我已经修改了我们的神经网络功能，以提供最终层(30D)的输出，以及中间层(2D) <strong class="lo jd">【我们的单词嵌入】的输出。</strong></p><p id="2386" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">为了设计神经网络，我将使用PyTorch框架。</p><h2 id="8461" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">超参数选择:</h2><ol class=""><li id="c580" class="nd ne it lo b lp lq lt lu lx ow mb ox mf oy mj ni nj nk nl bi translated">input_size = 30(输入和输出维度)</li><li id="ccd6" class="nd ne it lo b lp nm lt nn lx no mb np mf nq mj ni nj nk nl bi translated">hidden_size = 2(隐藏层尺寸)</li><li id="e3c9" class="nd ne it lo b lp nm lt nn lx no mb np mf nq mj ni nj nk nl bi translated">learning_rate = 0.01 (lr用于权重优化)</li><li id="e598" class="nd ne it lo b lp nm lt nn lx no mb np mf nq mj ni nj nk nl bi translated">num_epochs = 5000(对整个数据训练模型的次数)</li></ol><p id="b5d8" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">因此，对于上述规格，我已经在Pytorch中设计了模型。</p><p id="9593" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">请参见下面的代码实现。</p><figure class="od oe of og gt kd"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="f85a" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">现在让我们开始训练过程。</p><figure class="od oe of og gt kd"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/29a6971f98a7303ca999eac397f63b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*0lvgniLyuPSzzamjrlsjmg.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">培训损失</figcaption></figure><p id="5c86" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">损失图看起来很好，我们的模型不会过拟合或欠拟合。现在，让我们传递所有输入，并获得输入单词的2D [ <strong class="lo jd">单词嵌入</strong> ](低维表示)，并绘制它们，以查看我们的模型是否已经学习了我们的数据语料库中的语义。如果你有更大的训练数据，你必须训练它更多的时期。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h2 id="f386" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">损失函数的选择(可选阅读) :</h2><p id="7269" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">因为我们所有的向量都是一位热编码的，这意味着，在我们的输出中，我们有一个30个数组的向量，其中“1”表示索引词，“0”表示其他地方。</p><h2 id="f2ae" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">示例:</h2><p id="1917" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">单词—猫</p><p id="ff4b" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">整数编码:1</p><p id="f715" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">一键编码:0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</p><p id="cb1d" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">由于这是<strong class="lo jd">而不是</strong>多类或多标签分类，我们将使用<strong class="lo jd"> BCELoss </strong> (Pytorch)或<strong class="lo jd">BinaryCrossEntropy</strong>(Keras/tensor flow)作为我们的损失函数。</p><p id="c1b4" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">在最后一层，在输出之前，我使用了<strong class="lo jd"> Softmax Activation </strong>函数，因为我们想比较输出(实际)和概率(预测)。</p><p id="24d6" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated"><strong class="lo jd"> SoftMax </strong> — Soft(较低的值被软处理成较低的概率，而不是归零)，Max(较高的值被投影到较高的概率&lt; 1)。</p><p id="1b73" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">我经常搞不清楚是使用Softmax还是sigmoid，其中输出softmax在总和为1时得分，Sigmoid在0-1的范围内挤压输出。因此，我提供了一个代码片段供您考虑输出损失，我想用softmax，因为它适合我。</p><h2 id="08c4" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">使用Softmax:</h2><p id="fc51" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">预测值→ (0.9，0.0，0.0)</p><p id="6bfc" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">预测+ SOFTMAX → (0.5515，0.2242，0.2242)</p><p id="1fcd" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">实际目标→(1.0，0.0，0.0)</p><p id="0bb9" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">损失值→ 0.36</p><pre class="od oe of og gt pa pb pc pd aw pe bi"><span id="154a" class="nr kp it pb b gy pf pg l ph pi">m = nn.Softmax()<br/>loss = nn.BCELoss()</span><span id="1eec" class="nr kp it pb b gy pj pg l ph pi">input = torch.tensor([0.9,0.0,0.0])<br/>target = torch.tensor([1.0,0.0,0.0])<br/>output = loss(m(input), target)</span><span id="0961" class="nr kp it pb b gy pj pg l ph pi">print(input, m(input), target) <br/>print("Loss",output)</span><span id="c0cc" class="nr kp it pb b gy pj pg l ph pi">### DISPLAYED OUTPUTS ###</span><span id="4d8c" class="nr kp it pb b gy pj pg l ph pi">tensor([0.9000, 0.0000, 0.0000]) <br/>tensor([0.5515, 0.2242, 0.2242]) <br/>tensor([1., 0., 0.])</span><span id="06ae" class="nr kp it pb b gy pj pg l ph pi">Loss([0.36])</span></pre><h2 id="564d" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">使用乙状结肠:</h2><p id="e661" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">预测值→ (0.9，0.0，0.0)</p><p id="4ec2" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">预测+ SIGMOID → (0.7109，0.5000，0.5000)</p><p id="022f" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">实际目标→(1.0，0.0，0.0)</p><p id="1257" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">损失值→ 0.57</p><pre class="od oe of og gt pa pb pc pd aw pe bi"><span id="2ef8" class="nr kp it pb b gy pf pg l ph pi">m = nn.Sigmoid()<br/>loss = nn.BCELoss()</span><span id="f740" class="nr kp it pb b gy pj pg l ph pi">input = torch.tensor([0.9,0.0,0.0])<br/>target = torch.tensor([1.0,0.0,0.0])<br/>output = loss(m(input), target)</span><span id="2116" class="nr kp it pb b gy pj pg l ph pi">print(input, m(input), target) <br/>print("Loss",output)</span><span id="cc5e" class="nr kp it pb b gy pj pg l ph pi">### DISPLAYED OUTPUTS ###</span><span id="d07a" class="nr kp it pb b gy pj pg l ph pi">tensor([0.9000, 0.0000, 0.0000]) <br/>tensor([0.7109, 0.5000, 0.5000]) <br/>tensor([1., 0., 0.])</span><span id="5e76" class="nr kp it pb b gy pj pg l ph pi">Loss([0.5758])</span></pre></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="68cd" class="ko kp it bd kq kr my kt ku kv mz kx ky kz na lb lc ld nb lf lg lh nc lj lk ll bi translated">3.模型推理</h1><p id="4948" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">这里，我们将传递语料库中的每个单词，并提取由模型学习的2D潜在表示(<strong class="lo jd">单词嵌入</strong>)。</p><figure class="od oe of og gt kd"><div class="bz fp l di"><div class="or os l"/></div></figure><h2 id="caa6" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">比较图-训练与未训练模型:</h2><div class="od oe of og gt ab cb"><figure class="oi kd pk ok ol om on paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/8416f95bd91b40bd95ec8d40a3039bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*nVUCnGntlWBDN1g1H6ZNoA.png"/></div></figure><figure class="oi kd pl ok ol om on paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/1774bbb8632964b98999c64bf79af3cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*A-8Vamsw022kacX2NnCVFg.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk pm di pn oq translated">已训练模型与未训练模型的输出。</figcaption></figure></div><p id="19c1" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">我们做到了，正如我们所料，我们可以看到单词<strong class="lo jd">“Mice&amp;Cat”</strong>在嵌入维度上非常接近，这一特征是从数据语料库中学习的，因为它们一个接一个地出现得非常频繁。</p><p id="99ff" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">此外，<strong class="lo jd">“哥们、伙伴和密友”、</strong> <strong class="lo jd">“生活、睡眠&amp;房子”和“捕捉、追逐”</strong>这些词在嵌入维度上也更接近。</p><p id="afd1" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">原因是那些加粗的单词之间带有某种语义。例如<strong class="lo jd">“哥们、伙伴和密友”，</strong>通常指相同的意思——朋友/伙伴，我们的模型捕捉到了这一点。</p><p id="09f0" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">同样，我们知道汤姆(<strong class="lo jd">猫</strong>)和杰瑞(<strong class="lo jd">老鼠</strong>)这两个词经常出现，所以模型解释了它们之间的关系，并将它们投射到潜在维度的附近。</p><p id="7878" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">这正是word2vec模型内部在更大规模上发生的事情，但是它有不同的架构(具有不同窗口大小和多个目标单词的CBOW或Skip-gram ),并且它是在大容量数据上训练的。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="2ddb" class="ko kp it bd kq kr my kt ku kv mz kx ky kz na lb lc ld nb lf lg lh nc lj lk ll bi translated"><strong class="ak">改进空间:</strong></h1><h2 id="b7bf" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">1.改变模型的架构:</h2><p id="a25c" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">这个模型不能从大量的数据语料库中获取特征，所以我们需要改变模型的架构来完成这个任务。</p><p id="2b18" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">在上面的实现中，我们对每个输入单词使用单个目标单词进行预测，但它可以像右图一样进行扩展，其中相同的神经网络可以用于对给定输入单词的多个目标单词进行预测，这使得模型可以捕捉数据集中的细微差别。</p><div class="od oe of og gt ab cb"><figure class="oi kd po ok ol om on paragraph-image"><img src="../Images/6040da0a3ba436547dd6a86cbd1de53a.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*Zv06Z8c8AG328JF5VI7NlA.png"/></figure><figure class="oi kd pp ok ol om on paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><img src="../Images/6b13427c91123f3fee11be6909e047e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*7UfFNLo0BKf2TrcV6koWtw.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk pq di pr oq translated">我们的旧建筑与新建筑，来源-作者</figcaption></figure></div><p id="2b16" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">因为我希望这篇文章在媒体上的其他单词嵌入文章中简单而独特，所以我使用了单个目标单词预测模型，但现在如果你牢固地理解了基本概念，你可以很容易地理解其他文章。</p><h2 id="3922" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">2.子采样:</h2><p id="75a2" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">在上面的实现中，由于我们只有非常少的数据(Vocab size&lt; 30), we converted each word into One-Hot Encodings of length 30, with the value 1 for the corresponding word and the rest of them 0. Imagine we had 6 billion words to train and having applied the same concept would not be so useful. So one solution to minimize the issue is to do sub-sampling to remove the rare and frequent words.</p><p id="ea61" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">In the corpus words such as <strong class="lo jd"><em class="mk">“of，the，and，for，etc……”(停用词)</em> </strong>)，所以没有为附近的词提供太多的上下文。由于我们感兴趣的是找到单词之间的语义，我们可以安全地丢弃它们，从而从数据中去除噪声，这反过来为我们提供了更高的准确性、更快的训练和更好的表示。这个过程被称为子采样。</p><p id="963b" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">再次，识别每个单词并手动删除它们是一项艰巨的任务，因此我们寻求概率的帮助，我们可以实际计算每个单词的概率，并设置一个阈值来考虑或丢弃它。</p><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/114288c5c6d4d2559efe44b6c0057325.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*-5vid_dIqmBF5NOK001YMQ.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">一个词的概率</figcaption></figure><p id="79e9" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">其中，<br/>T5 p(Wi)→该词被丢弃的概率<br/>(如果1 →丢弃，0.3→保留)</p><p id="9de8" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated"><em class="mk"> t →阈值参数(比方说0.001) <br/> f(Wi) →一个词在总数据集中的出现频率(Wi)<br/>一个词的出现频率=(该词在文档中出现的次数/文档中的总字数)。</em></p><p id="ad0a" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">出于考虑，假设我们在文档中的总字数是60，并且单词<em class="mk">(the，cat，floccinaucinihilipilification)</em>出现了(12，5，1)次。</p><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/4af5913c4c78230481afac0fcd4d1867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*1Knq5uGrlj-pS9SwKy3F0w.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">p(<em class="pu">“the”</em>)丢弃概率</figcaption></figure><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/24336b653da178433d1be067ec4e3087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*udXC0rSgk7WD2akE2T7xOg.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">P( <em class="pu">“猫”</em>)丢弃概率</figcaption></figure><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/b459373e5cf2a5cdde6ea61f7253a3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*RgZ7FX0KK4JPiEroLEdr-Q.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">p(<em class="pu">“floccinaucinihililification”</em>)丢弃概率</figcaption></figure><p id="f050" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">因此，从上面的值，我们可以明智地设置，以抽样的话，位于概率0.80-0.90。整体数值见图。</p><figure class="od oe of og gt kd gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/e4e922fa75e6f1021679b02615217d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*Hi4ZXwI54xL4m5W0flKy0g.png"/></div></figure><h2 id="ed5b" class="nr kp it bd kq ns nt dn ku nu nv dp ky lx nw nx lc mb ny nz lg mf oa ob lk iz bi translated">3.负采样:</h2><p id="6ae8" class="pw-post-body-paragraph lm ln it lo b lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">在我们处理大型文本语料库的实时实现中，如果我们查看输出层，假设有10，000个编码的独热标签，即使我们只有一个真实示例(..0010000..)，这使得培训的效率非常低。因此，一种解决方法是只更新一小部分权重，其中我们更新正确标签和少量不正确标签的权重。这种技术被称为负采样，它已经在训练时用于word2vec模型。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="41ec" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">我希望我能够为我们的迷你单词嵌入提供一些视觉上的理解，让我知道你在评论区的想法。</p><p id="1f2b" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">查看包含全部代码实现的笔记本，并随意破解它。</p><p id="5357" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">参见GitHub 中的<strong class="lo jd">，</strong></p><div class="px py gp gr pz qa"><a href="https://github.com/bala-codes/Mini_Word_Embeddings/blob/master/codes/Mini_Word_Embedding.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="qb ab fo"><div class="qc ab qd cl cj qe"><h2 class="bd jd gy z fp qf fr fs qg fu fw jc bi translated">巴拉码/迷你词嵌入</h2><div class="qh l"><h3 class="bd b gy z fp qf fr fs qg fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="qi l"><p class="bd b dl z fp qf fr fs qg fu fw dk translated">github.com</p></div></div><div class="qj l"><div class="qk l ql qm qn qj qo ki qa"/></div></div></a></div><p id="dd64" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">运行在<strong class="lo jd"> Google Colab，</strong></p><div class="px py gp gr pz qa"><a href="https://colab.research.google.com/github/bala-codes/Mini_Word_Embeddings/blob/master/codes/Mini_Word_Embedding.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="qb ab fo"><div class="qc ab qd cl cj qe"><h2 class="bd jd gy z fp qf fr fs qg fu fw jc bi translated">谷歌联合实验室</h2><div class="qh l"><h3 class="bd b gy z fp qf fr fs qg fu fw dk translated">编辑描述</h3></div><div class="qi l"><p class="bd b dl z fp qf fr fs qg fu fw dk translated">colab.research.google.com</p></div></div><div class="qj l"><div class="qp l ql qm qn qj qo ki qa"/></div></div></a></div><p id="10c7" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">或者如果你喜欢<strong class="lo jd"> Kaggle </strong>，</p><div class="px py gp gr pz qa"><a href="https://www.kaggle.com/balakrishcodes/create-mini-word-embedding-from-scratch" rel="noopener  ugc nofollow" target="_blank"><div class="qb ab fo"><div class="qc ab qd cl cj qe"><h2 class="bd jd gy z fp qf fr fs qg fu fw jc bi translated">从头开始创建迷你单词嵌入</h2><div class="qh l"><h3 class="bd b gy z fp qf fr fs qg fu fw dk translated">使用Kaggle笔记本探索和运行机器学习代码|使用来自非数据源的数据</h3></div><div class="qi l"><p class="bd b dl z fp qf fr fs qg fu fw dk translated">www.kaggle.com</p></div></div><div class="qj l"><div class="qq l ql qm qn qj qo ki qa"/></div></div></a></div><p id="406e" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated">在那之前，下次见。</p><p id="d7b0" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated"><strong class="lo jd">文章作者:</strong></p><p id="2bff" class="pw-post-body-paragraph lm ln it lo b lp ml lr ls lt mm lv lw lx mn lz ma mb mo md me mf mp mh mi mj im bi translated"><strong class="lo jd"> BALAKRISHNAKUMAR V </strong></p></div></div>    
</body>
</html>