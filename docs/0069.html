<html>
<head>
<title>Prediction of Relative Locations of CT Slices in CT Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CT图像中CT切片相对位置的预测</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/prediction-of-relative-locations-of-ct-slices-in-ct-images-99f2bc8402e1?source=collection_archive---------1-----------------------#2019-06-09">https://pub.towardsai.net/prediction-of-relative-locations-of-ct-slices-in-ct-images-99f2bc8402e1?source=collection_archive---------1-----------------------#2019-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8d65" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">使用主成分和弹性网| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">向AI </a>回归</h2><div class=""/><div class=""><h2 id="d85f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用对非常高维数据的回归技术来预测CT切片在人体轴上的相对位置</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/943346945216adec835ba03f11f13a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyXQfMrBAzmyPp7u7JYlpA.jpeg"/></div></div></figure><p id="3167" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回归是机器学习中最基本的技术之一。简单来说就是‘通过其他独立的分类/连续变量来预测一个连续变量’。挑战来了，当我们有高维，即太多的独立变量。在本文中，我们将讨论一种使用主成分和ElasticNet对高维数据进行回归建模的技术。我们还将看到如何保存该模型以供将来使用。</p><h2 id="f361" class="lz ma it bd mb mc md dn me mf mg dp mh lm mi mj mk lq ml mm mn lu mo mp mq iz bi translated">获取数据和问题定义</h2><p id="bfad" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">我们将使用Python 3.x作为编程语言，使用“sci-kit learn”、“seaborn”作为本文的库。</p><p id="4e14" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这里使用的数据可以在<a class="ae mw" href="https://archive.ics.uci.edu/ml/datasets/Relative+location+of+CT+slices+on+axial+axis" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>找到。数据集名称为“轴向轴数据集上CT切片的相对位置”。该文件包含各种患者(男性&amp;女性)的医学CT扫描图像的提取特征。特征本质上是数字的。按照UCI的说法，目标是“预测CT切片在人体轴上的相对位置”。</p><p id="2478" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们探索数据集，以便更清楚地理解它</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mz"><img src="../Images/7af58106b946b1920f56759f7c8948d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eWkqPAi62jfgccyG02ktFg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图1</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/eeded4d6cfdbae6f29b4dfc92ace0c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0vvxGhF-G8E2n9TA8a3UYg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图2</figcaption></figure><p id="ded4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从上面的数据集中，我们可以看到名为' value0 '，' value1 '，..“值383”包含每个患者的CT扫描图像的特征值。最后一个变量是“reference”。这个“参考”是我们的目标变量，它包含CT切片的相对位置。</p><p id="fcaf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所以，我们的问题是预测其他特征的“引用”。总共有384个特征(独立变量，如“值0”、“值1..等等)并且总的数据集大小是53500。由于我们的目标变量‘reference’本质上是连续的，这是一个回归问题。</p><h2 id="4ba4" class="lz ma it bd mb mc md dn me mf mg dp mh lm mi mj mk lq ml mm mn lu mo mp mq iz bi translated">分析数据</h2><p id="b933" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">由于有太多的功能，我们的任务变得更加复杂。在这里，选择正确的一组非常重要。所有这些特性都同样重要吗？或者这些是相互关联的？我们将努力找到这些答案。</p><p id="e4d2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，我们需要删除不必要的变量<strong class="lf jd">‘patient id</strong>’，分离特征和目标变量。</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="f63c" class="lz ma it ng b gy nk nl l nm nn">df = df.drop(['patientId'], axis=1)<br/>df_y = df['reference']<br/>df_x = df.drop(['reference'], axis=1)</span></pre><p id="0566" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因此，数据框“df_y”是我们的目标变量，数据框“df_x”包含所有特征。</p><p id="c1f5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">主成分分析(PCA)可以揭示很多细节，减少特征数量。让我们看看如何做到这一点。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/f9bab92bb1ac519b3d9e354d6b6fd6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b965ehu8b4ZBVKkKtNBivg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图3</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/9e9e73a57ea1a57d4810e7c41b8a3441.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lMvczBC1uf7ycxgDQgk_JQ.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图4</figcaption></figure><p id="5e73" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们做了主成分分析，它可以容纳高达95%的数据差异。结果表明，总共有212个主成分(PC)负责95%的方差。</p><p id="2ec1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">“pca_vectors”看起来像这样:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/85602035d9e1d9c0b9aa6f9d72786817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-1ra9VBM3oCXCPhXzCVpJA.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图5</figcaption></figure><p id="67ae" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以把这些PC当做功能。因此，通过这种方式，我们能够将维度从384减少到212(维度减少了近44.7 %)。</p><p id="eb3b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以减少更多，但可能不得不牺牲解决方案的准确性。出于这个原因，我们将在这里停下来，使用这些电脑作为我们的功能。请记住，电脑是虚拟功能，并不实际存在，也就是说，不可能在物理上与数据相似。此外，这些PC中的每一个都彼此不相关，因此多重共线性问题消失了。</p><p id="b227" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们现在将看到，这些电脑是如何解释我们的目标变量'参考'。我们将使用前3个(最重要的)和后3个(最不重要的)电脑做一个“<em class="nr">回归图</em>”。(因为这些是按解释的差异百分比降序排列的)</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="8d69" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回归图</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/9c502f541467a48116179cfa73d9b3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nZEFfvPS6mLdyEZBZ_ZeQg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图6</figcaption></figure><p id="4b2b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到，前3个PC相当占优势，并影响着“参考”。接下来是最后3件。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="344c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回归图</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/614b3d1c9168e3118e24855a725c6cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXtsBtWGf0kNN1ysJBgZYA.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图7</figcaption></figure><p id="8ca8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于回归线几乎平行于x轴，我们可以说最后3个是最不重要的。</p><h2 id="e312" class="lz ma it bd mb mc md dn me mf mg dp mh lm mi mj mk lq ml mm mn lu mo mp mq iz bi translated">构建机器学习模型</h2><p id="4992" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">现在，是做实际工作的时候了，即建立模型。我们已经有了一个特性集。我们将使用正则化的线性回归模型。在处理大量特征时，ElasticNet提供了更好的准确性。这是“套索”和“山脊”回归之间的权衡。它的正则化参数由α<em class="nr">(α)给出。</em>当α = 1时，它变成“套索”,当α = 0时，它变成“脊”。为了更准确，我们应该将α设置在0和1之间。α是这里的超参数。</p><p id="fc0f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，我们应该将数据分成训练集和测试集</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="ac45" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们应该对最佳超参数进行交叉验证。我们将把α值保持在测试范围内(0.1，0.3，0.5，0.7，1.0)</p><p id="c0c9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们看看准确性</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="8c4a" class="lz ma it ng b gy nk nl l nm nn">print('R2 value : ', r2_en)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/cee86ec297d2f8ad3c31ded43959205f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*r_z5cPRz6hrpwoWFP_gkMA.png"/></div></figure><p id="88c2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个模型解释了几乎85%的差异。这个挺好的。</p><p id="6492" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们看看系数、α值和截距的值</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="09c0" class="lz ma it ng b gy nk nl l nm nn">print('Intercept: ', regr_en.intercept_) <br/>print('Alpha: ', regr_en.alpha_) <br/>print('Coefficients: ' , regr_en.coef_)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/aebfdae1a1ceee3888c531e9bc78aeeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gl5hcqcKRSOY1dp7cYm7-w.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图8</figcaption></figure><h2 id="a749" class="lz ma it bd mb mc md dn me mf mg dp mh lm mi mj mk lq ml mm mn lu mo mp mq iz bi translated">分析结果</h2><p id="b317" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">根据线性回归方法，我们得到了85%的准确率。现在，我们将看看它是如何受到最重要和最不重要的电脑的影响的，就像前面的分析一样。</p><p id="3145" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将使用我们的模型估算的<em class="nr">【参考】</em>值，而不是原始值，绘制<em class="nr">【回归图】</em>(与之前的分析不同)。</p><p id="a0fe" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">使用前3台电脑</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="dc1f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回归图</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/b2f2104ebb64c748589c8133c062af81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MT6GPuww8bZHX5JMKPTONw.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图9</figcaption></figure><p id="4d1d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">使用最后3台电脑</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="f6ee" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">回归图</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/4ce081aa883e658152dbb2782eed5b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SVEzLILNsPCJD0253wBdsQ.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图10</figcaption></figure><p id="c401" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">与图7相比，我们在图10中看到了显著的改进。这意味着估计值表现良好。</p><p id="ef67" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以得到如下测试数据的估计残差或误差</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="3dea" class="lz ma it ng b gy nk nl l nm nn">residuals = test_y - regr_en.predict(test_x)</span></pre><p id="7791" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，我们将看到<em class="nr">【残差图】</em>的测试数据</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/164a9d74e2f1f6bba7bbef6dbaa90673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pxt1VdWKVok9gYk1wIwxxg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图11</figcaption></figure><p id="38e6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以看到‘残积’没有模式，完全是随机的。根据线性回归方法，它表明是一个好模型。</p><h2 id="4a71" class="lz ma it bd mb mc md dn me mf mg dp mh lm mi mj mk lq ml mm mn lu mo mp mq iz bi translated">为生产就绪模型构建管道</h2><p id="2b81" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">为了在生产中使用，我们需要以机器学习流水线的方式构建模型。</p><p id="be7d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以使用这个管道模型预测任何实时数据实例</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="79cf" class="lz ma it ng b gy nk nl l nm nn">pl_test_x[20:21]</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/e1d319aa2bdfd7e404edadb434ace86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5ZkX8NFJHWxYQ7OtSyP9w.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图12</figcaption></figure><p id="7171" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">原始数据集中的“引用”值</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="d641" class="lz ma it ng b gy nk nl l nm nn">pl_test_y[20:21].values</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ff90d23e78c2093d7b9d2ba1526646a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*l31qvM436HPCVBkGZAK0fQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图13</figcaption></figure><p id="66ce" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，让我们使用我们的模型预测“参考”值，并与原始数据集值进行比较</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="e7dc" class="lz ma it ng b gy nk nl l nm nn">pl_model.predict(pl_test_x[20:21])</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/9242d41cb00e8900547f7eb5f37f1ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*vpAi7xKPXLkqejZo3tFoJQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图14</figcaption></figure><p id="f0d1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有一个可以忽略的偏差。</p><h2 id="e29a" class="lz ma it bd mb mc md dn me mf mg dp mh lm mi mj mk lq ml mm mn lu mo mp mq iz bi translated">持久化模型以供将来使用</h2><p id="d531" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">我们可以在模型中持久化，并按需加载它以备将来使用</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a7e72480ca47a37250764b4badb05779.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*349kvp7sqE2CbOUiAAb00Q.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk translated">图15</figcaption></figure><h2 id="3777" class="lz ma it bd mb mc md dn me mf mg dp mh lm mi mj mk lq ml mm mn lu mo mp mq iz bi translated">结论</h2><p id="8cbf" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">我们学习了如何使用主成分来描述特征和相关性，建立回归模型和预测值。还有其他回归模型。这篇文章的读者可以尝试一下。Jupyter笔记本可以在Github上找到。</p><h2 id="1e93" class="lz ma it bd mb mc md dn me mf mg dp mh lm mi mj mk lq ml mm mn lu mo mp mq iz bi translated">参考</h2><p id="7a3d" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">[1]主成分分析—<a class="ae mw" href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c" rel="noopener" target="_blank">https://towards data science . com/a-一站式主成分分析-5582fb7e0a9c </a></p><p id="86df" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[2]弹性网，套索&amp;脊回归—<a class="ae mw" href="https://medium.com/@yongddeng/regression-analysis-lasso-ridge-and-elastic-net-9e65dc61d6d3" rel="noopener">https://medium . com/@ Yong Deng/regression-analysis-Lasso-Ridge-and-Elastic-Net-9 e 65 DC 61 d6d 3</a></p><p id="ced3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最近，我写了一本关于ML(<a class="ae mw" href="https://twitter.com/bpbonline/status/1256146448346988546" rel="noopener ugc nofollow" target="_blank">https://twitter.com/bpbonline/status/1256146448346988546</a>)的书</p></div></div>    
</body>
</html>