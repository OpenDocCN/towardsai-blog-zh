<html>
<head>
<title>Principal Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/principal-component-analysis-pca-7e104a77a254?source=collection_archive---------0-----------------------#2020-10-24">https://pub.towardsai.net/principal-component-analysis-pca-7e104a77a254?source=collection_archive---------0-----------------------#2020-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0cab" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-mining" rel="noopener ugc nofollow" target="_blank">数据挖掘</a></h2><div class=""/><div class=""><h2 id="0864" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">降维技术</h2></div><h2 id="20f5" class="ko kp iq bd kq kr ks dn kt ku kv dp kw kx ky kz la lb lc ld le lf lg lh li iw bi translated">数据挖掘历史</h2><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/aa990063b50ddb9cfb8dbbebe2530acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TS_6-XUic_zxKt-U92nnPw.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated"><a class="ae lz" href="http://recommender-systems.readthedocs.io/en/latest/_images/Kdd-process.png" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="758a" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">在数据挖掘过程中，我们得到原始数据。在可视化或解释数据之前，我们必须确保在数据可用于分析之前，对数据应用了某些细化方法。这个细化过程包括预处理或清理数据，例如从数据中删除空值或空白值。接下来是特征选择或特征提取技术，其在PCA中使用，其中根据需要忽略或移除贡献最小的特征。最后一个阶段是数据转换，用户将应用归一化技术来缩放同一范围内的所有要素。</p><h2 id="9e1a" class="ko kp iq bd kq kr ks dn kt ku kv dp kw kx ky kz la lb lc ld le lf lg lh li iw bi translated">什么是PCA？</h2><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mt"><img src="../Images/86d05826bc9fad4c40be464e8a8815ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*21s55cuk-mcfgmki_kcs2w.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="61ff" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">在当前世界中，当我们执行数据分析任务时，我们分析复杂的数据，即多维数据。在分析数据之后，我们绘制它并观察一些模式，或者利用它来训练其他机器学习模型。维度可以被认为是一种视图，利用它我们可以从任意轴观察空间中的一点。随着点的维度增加，它变得难以可视化，并且对其执行计算变得复杂。因此，有必要对维度进行检查，我们需要找到减少维度的方法。为了解决这个问题，我们引入了PCA。</p><p id="54b5" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">主成分分析是无监督学习算法。PCA实现了降维技术。PCA的目标是在开发模型时移除不相关的特征。PCA提取对输出有贡献的最相关的特征。</p><h2 id="18e5" class="ko kp iq bd kq kr ks dn kt ku kv dp kw kx ky kz la lb lc ld le lf lg lh li iw bi translated">我们为什么需要它？</h2><ul class=""><li id="e68a" class="mu mv iq mc b md mw mg mx kx my lb mz lf na ms nb nc nd ne bi translated">去除不相关的特征。</li><li id="ca34" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">提高模型的预测精度。</li><li id="9751" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">促进存储和计算成本的降低。</li><li id="e6c7" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">提高对数据和模型的理解。</li></ul><h2 id="8d53" class="ko kp iq bd kq kr ks dn kt ku kv dp kw kx ky kz la lb lc ld le lf lg lh li iw bi translated">特征值和特征向量的修正</h2><p id="c0da" class="pw-post-body-paragraph ma mb iq mc b md mw ka mf mg mx kd mi kx nk mk ml lb nl mn mo lf nm mq mr ms ij bi translated">在我们试图理解PCA的处理流程之前，我们先来复习一下高中数学科目特征值和特征向量。</p><p id="119a" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">考虑下面给出的矩阵A，将其乘以n*1矩阵x，我们得到的输出可以表示为一个标量乘以矩阵x的乘积。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b111acae7918927dddc7761b6a150c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*bY8iEbXsLln5-CXtBq-8OA.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="3a0c" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">特征值问题是具有以下格式的任何问题:</p><p id="d713" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja"> A*X = λ*X </strong></p><p id="25f5" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja"> A = n*n矩阵</strong></p><p id="75f8" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja"> X = n*1个非零矩阵</strong></p><p id="cd88" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja"> λ =标量</strong></p><p id="c82e" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">这个方程有解的<strong class="mc ja"> λ </strong>的任何一个值称为A的特征值，与这个值对应的向量X称为A的特征向量。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f59337be80e58b1386b7743f7c3907d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*risWdP5dzkIwvLOn07RdEQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="be90" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">因此，(3，2)是方阵<strong class="mc ja"> A </strong>的特征向量，4是<strong class="mc ja"> A </strong>的特征值</p><p id="8ce4" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">给定矩阵A，我们如何计算A的特征向量和特征值？</strong></p><p id="deb5" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">AX = λX</p><p id="94e5" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">AX - λIX =0</p><p id="6275" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">[A* - λI] * X =0</p><p id="5a73" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">根据特征向量的规则，X的值总是非零的，因此我们可以使|A - λI | = 0。</p><p id="9ee1" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">例题求解:</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi np"><img src="../Images/de6214ba3f64100c52b0b6fabfdc3740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*IebTVynm4ola5eqMtv1vvQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="90ee" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">现在我们需要找到X，它表示对应于特征值λ = -1和λ = -2的特征向量，使得</p><p id="1689" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">AX = λX</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi nq"><img src="../Images/13d5a25b373cb6823d2a772bce56fba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x1MWN8xqaQSQbHm0Ha9wNg.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="5b86" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">所以特征值和特征向量的概念是在PCA中实现的。</p><h2 id="c48a" class="ko kp iq bd kq kr ks dn kt ku kv dp kw kx ky kz la lb lc ld le lf lg lh li iw bi translated"><strong class="ak">协方差公式</strong></h2><p id="9363" class="pw-post-body-paragraph ma mb iq mc b md mw ka mf mg mx kd mi kx nk mk ml lb nl mn mo lf nm mq mr ms ij bi translated">协方差是一种在给定的一组特征之间寻找关联的方法。它帮助我们识别独立组件之间的依赖关系，这对于生成输出是必不可少的。</p><p id="eb26" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">考虑两个要素X和Y，它们有一个100条记录的数据集。为了求协方差，我们计算X和y的平均值。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi nr"><img src="../Images/2cfa6217b9b101a717c29ef0495c0ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t5MABqcxPKhT8-2308S7LQ.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated"><a class="ae lz" href="https://miro.medium.com/max/700/1*hawqb3jJUW4UrHT7ERjHow.png" rel="noopener">来源</a></figcaption></figure><h2 id="5a3f" class="ko kp iq bd kq kr ks dn kt ku kv dp kw kx ky kz la lb lc ld le lf lg lh li iw bi translated">PCA中的步骤</h2><p id="fb7b" class="pw-post-body-paragraph ma mb iq mc b md mw ka mf mg mx kd mi kx nk mk ml lb nl mn mo lf nm mq mr ms ij bi translated"><strong class="mc ja">第一步:归一化所有特征</strong></p><p id="f0e4" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">标准化特征意味着我们必须确保所有特征都在相同的比例或范围内。例如，如果我有一个包含厘米和米值的列X1和列X2，那么使用这些特征预测模型是不可行的。在继续数据集的分析部分之前，我们必须以厘米或米为单位转换要素。在实时场景中，我们使用均值归一化方法来确保所有特征都在相似的范围内放大。</p><p id="c60b" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">这里，我们从每个要素的给定数据集计算平均值，然后从平均值中减去特定要素的每个数据，因此任何给定要素的数据值的范围为[-1，1]。</p><p id="3ffd" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">下面是给我们的数据集，它具有特征x1和x2。我们计算了x1和x2的平均值，并将数据转换为归一化形式。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/64a8124486a8ef2ccad382af6642b57f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*zHjivxUpfpN4Um-nbpp11g.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="24bf" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">步骤2:创建协方差矩阵</strong></p><p id="762b" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">协方差矩阵由矩阵中的特征*特征项组成。它用于查找对输出要素有贡献的自身要素和其他要素的协方差。因此特征X1和X2的矩阵将如下所示。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/692a0bee04724468d6b1d8cbff2c575d.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*ve7SiDPt2ICgFTKYt46WPw.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/41734be9a18dc030654de34614841b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*emUXtQFZTZVQ0ZXFYh-Kzg.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="bf87" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">步骤3:特征值和特征向量的计算</strong></p><p id="0bbd" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">使用特征值和特征向量的规则<strong class="mc ja"> |A — λI | = 0，</strong>我们将找到给定矩阵a的特征值和特征向量，其由Cov(X₁、X₂表示)计算如上。</p><p id="059c" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">利用上述方法，我们得到了特征值<strong class="mc ja">λ</strong>₁<strong class="mc ja">t23】和<strong class="mc ja"> λ </strong> ₂以及相应的特征向量。</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/d52049cb6e42d038c08d61e26906d25e.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*hEwOSkPH7TZEEFqxtBicIw.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="cd22" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">步骤4:按照特征值降序排列特征向量</strong></p><p id="2761" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">在计算了特征值和对应的向量之后，我们在一个特征值按降序排列的表中创建一个由这两者组成的表。然后，我们继续计算总方差，它将等于特征值的总和。</p><p id="d9f6" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">总方差= 1.2840 + 0.0490 = 1.333</p><p id="5655" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">将特征值<strong class="mc ja"> λ </strong> ₁除以总方差，我们得到</p><p id="34c0" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi">(1.2840/1.333) = 0.963 = 96.3%</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/260f60098e1eab274c1d55e0488e8d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*GOGP9AMRvlsh6ALNhgTjAg.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="2774" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">上述值表示特征λ₁成为我们的PC1。</strong></p><p id="2060" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">将特征值<strong class="mc ja"> λ </strong> ₂除以总方差，我们得到</p><p id="47e4" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi">(0.0490/1.333) = 0.037 = 3.7%</p><p id="8f1f" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">上述值表示特征λ </strong> ₂ <strong class="mc ja">成为我们的PC2。</strong></p><p id="5bdf" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">使用上述计算，我们可以得出结论，PC1对输出的贡献超过96%。因此，我们可以利用与<strong class="mc ja"> λ </strong> ₂相关联的特征向量来计算新的或更新的集合。</p><p id="2b14" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">步骤5:变化矩阵(V)的创建</strong></p><p id="17e8" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">在找到主分量之后，我们然后利用与PC相关联的特征向量，并且创建表示变化矩阵(V)的矩阵。在上面的步骤4中，我们发现PC1是比PC2更重要的组件。我们使用它的特征向量创建变化矩阵。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/5fbb7eccafd776c9a6584772a56435e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*BwTj_ch4JnoJm74b7dYdZQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="6679" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">第六步:找到新的数据集。</strong></p><p id="4360" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated">在确定最重要的主成分之后，我们希望使用新的特征来重组数据集，这将减少维度，同时提高模型的性能和准确性。</p><p id="5afa" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">新数据=数据*特征向量</strong></p><p id="0240" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja"> Y = X*V </strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi no"><img src="../Images/024700e18085aa5414193d571b26b552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*gUyyf9TRIus2RrlKxb9L5w.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</figcaption></figure><p id="7aed" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi kx mj mk ml lb mm mn mo lf mp mq mr ms ij bi translated"><strong class="mc ja">应用</strong></p><ul class=""><li id="071e" class="mu mv iq mc b md me mg mh kx ny lb nz lf oa ms nb nc nd ne bi translated">人脸识别</li><li id="ca6e" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">图像合成</li><li id="3dbf" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">基因表达分析</li><li id="3325" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">数据整理</li><li id="ee8e" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">数据分类</li><li id="3429" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">趋势分析</li><li id="ba6f" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">要素分析</li><li id="5098" class="mu mv iq mc b md nf mg ng kx nh lb ni lf nj ms nb nc nd ne bi translated">噪声降低</li></ul></div></div>    
</body>
</html>