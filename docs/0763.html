<html>
<head>
<title>Similar Texts Search In Python With A Few Lines Of Code: An NLP Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用几行代码在Python中搜索相似的文本:一个NLP项目</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/similar-texts-search-in-python-with-a-few-lines-of-code-an-nlp-project-9ace2861d261?source=collection_archive---------0-----------------------#2020-08-05">https://pub.towardsai.net/similar-texts-search-in-python-with-a-few-lines-of-code-an-nlp-project-9ace2861d261?source=collection_archive---------0-----------------------#2020-08-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="37a2" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="3903" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在Python中使用计数矢量器和最近邻法查找相似的维基百科资料，这是一个简单而有用的自然语言处理(NLP)项目</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/004ef8dba5c1500ee2cac7d593d6df8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jhQAT768XribaPuI"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@amartino20?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安东尼·马蒂诺</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h2 id="1937" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">什么是自然语言处理？</strong></h2><p id="2459" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated">自然语言处理(NLP)是指开发理解人类语言的应用程序。如今NLP有如此多的用例。因为人们每天都在通过博客、社交媒体评论、产品评论、新闻档案、官方报告等等产生数千千兆字节的文本数据。搜索引擎是NLPs最大的例子。我不认为你会发现你周围有很多人从来没有使用过搜索引擎。</p><h2 id="8ded" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">项目概述</strong></h2><p id="66c2" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated">根据我的经验，最好的学习方法是做一个项目。在本文中，我将用一个真实的项目来解释NLP。我将使用的数据集名为“people_wiki.csv”。我在Kaggle找到了这个数据集。<strong class="mf jd">请随意从这里下载数据集:</strong></p><div class="mw mx gp gr my mz"><a href="https://www.kaggle.com/sameersmahajan/people-wikipedia-data" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">人民维基百科数据</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">维基百科关于各种人的信息</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">www.kaggle.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn lb mz"/></div></div></a></div><p id="1d21" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">该数据集包含一些名人的名字、他们的维基百科URL以及他们的维基百科页面的文本。因此，数据集非常大。这个项目的目标是，找到有相关背景的人。最后，如果你给算法提供一个名人的名字，它将根据维基百科的信息返回预定数量的具有相似背景的人的名字。这听起来是不是有点像搜索引擎？</p><h2 id="d5af" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">分步实施</strong></h2><ol class=""><li id="4de7" class="nt nu it mf b mg mh mj mk lr nv lv nw lz nx mv ny nz oa ob bi translated">导入必要的包和数据集。</li></ol><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="8496" class="li lj it od b gy oh oi l oj ok"><strong class="od jd">import</strong> <strong class="od jd">numpy</strong> <strong class="od jd">as</strong> <strong class="od jd">np</strong><br/><strong class="od jd">import</strong> <strong class="od jd">pandas</strong> <strong class="od jd">as</strong> <strong class="od jd">pd</strong><br/><strong class="od jd">from</strong> <strong class="od jd">sklearn.neighbors</strong> <strong class="od jd">import</strong> NearestNeighbors<br/><strong class="od jd">from</strong> <strong class="od jd">sklearn.feature_extraction.text</strong> <strong class="od jd">import</strong> CountVectorizer<br/>df = pd.read_csv('people_wiki.csv')<br/>df.head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/10b785e57204fc8dccd54cd63a337658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/0*9Z5C05QGj6dVzZte.png"/></div></figure><p id="9278" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">2.向量化“文本”列</p><h2 id="aab8" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">如何矢量化？</strong></h2><p id="51aa" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated">在Python的scikit-learn库中，有一个名为'<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器</a>'的函数。这个函数为每个单词提供一个索引，并生成一个向量，其中包含每个单词在一段文本中出现的次数。在这里，我用一小段文字来演示一下，供大家理解。假设，这是我们的文本:</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="5ba7" class="li lj it od b gy oh oi l oj ok">text = ["Jen is a good student. Jen plays guiter as well"]</span></pre><p id="a1b8" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">让我们从scikit_learn库中导入函数，并将文本放入函数中。</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="b884" class="li lj it od b gy oh oi l oj ok">vectorizer = CountVectorizer()<br/>vectorizer.fit(text)</span></pre><p id="7bf2" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">在这里，我打印词汇:</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="37ce" class="li lj it od b gy oh oi l oj ok">print(vectorizer.vocabulary_)#Output:<br/>{'jen': 4, 'is': 3, 'good': 1, 'student': 6, 'plays': 5, 'guiter': 2, 'as': 0, 'well': 7}</span></pre><p id="a28f" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">看，课文中的每个单词都有一个数字。那些数字是那个词的索引。它有八个重要的单词。所以，指数是从0到7。接下来，我们需要转换文本。我将把转换后的向量打印成一个数组。</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="cf04" class="li lj it od b gy oh oi l oj ok">vector = vectorizer.transform(text)<br/>print(vector.toarray())</span></pre><p id="1447" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">下面是输出:[[1 1 1 1 2 1 1]]。“Jen”的索引为4，并且出现了两次。所以在这个输出向量中，第四个索引元素是2。所有其他的单词只出现了一次。所以矢量的元素是1。</p><p id="5631" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated"><strong class="mf jd">现在，使用相同的技术对数据集的“文本”列进行矢量化。</strong></p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="1622" class="li lj it od b gy oh oi l oj ok">vect = CountVectorizer()<br/>word_weight = vect.fit_transform(df['text'])</span></pre><p id="1b3a" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">在演示中，我先使用了“fit ”,然后使用了“transform”。但方便的是，您可以同时使用拟合和变换。这个单词权重是我之前解释过的数字向量。“文本”列中的每一行文本都有一个这样的向量。</p><p id="601c" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">3.在<a class="ae lh" href="https://scikit-learn.org/stable/modules/neighbors.html" rel="noopener ugc nofollow" target="_blank">最近邻</a>函数中拟合上一步的“单词权重”。</p><p id="ef35" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">最近邻函数的思想是计算预定义数量的训练点到所需点的距离。如果不清楚，不要担心。看看实现，对你来说会容易一些。</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="ee02" class="li lj it od b gy oh oi l oj ok">nn = NearestNeighbors(metric = 'euclidean')<br/>nn.fit(word_weight)</span></pre><p id="6868" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">4.找出10个与巴拉克·奥巴马总统背景相似的人。</p><p id="ab40" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">首先，从数据集中找到‘巴拉克·奥巴马’的索引。</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="efa8" class="li lj it od b gy oh oi l oj ok">obama_index = df[df['name'] == 'Barack Obama'].index[0]</span></pre><p id="dfa5" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">计算与奥巴马总统背景最接近的10个人的距离和指数。在单词权重向量中，包含关于“Barak Obama”的信息的文本的索引应该与数据集在同一索引中。我们需要传递那个索引和我们想要的人的号码。这将返回这些人与“巴拉克·奥巴马”的计算距离以及这些人的指数。</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="100a" class="li lj it od b gy oh oi l oj ok">distances, indices = nn.kneighbors(word_weight[obama_index], n_neighbors = 10)</span></pre><p id="cf68" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">将结果组织在数据帧中。</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="6367" class="li lj it od b gy oh oi l oj ok">neighbors = pd.DataFrame({'distance': distances.flatten(), 'id': indices.flatten()})<br/>print(neighbors)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/aa85be552dd85b726ab90bae28dc814b.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/0*qZaR13mElJcYHggd.png"/></div></figure><p id="d2d0" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">让我们从索引中找出这些人的名字。有几种方法可以从索引中找到名字。我用了合并功能。我只是将上面的“邻居”数据帧与原始数据帧“df”合并，使用id列作为公共列。按距离排序的值。奥巴马总统应该和自己没有距离。所以，他是第一名。</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="fae4" class="li lj it od b gy oh oi l oj ok">nearest_info = (df.merge(neighbors, right_on = 'id', left_index = <strong class="od jd">True</strong>).sort_values('distance')[['id', 'name', 'distance']])<br/>print(nearest_info)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/499e702424527dfaf5485e5357210973.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/0*gNKa2qVmD1atZ_fE.png"/></div></figure><p id="3ede" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">根据维基百科提供的信息，这是与奥巴马总统最亲近的10个人。结果是有意义的，对吗？</p><p id="82b8" class="pw-post-body-paragraph md me it mf b mg no kd mi mj np kg ml lr nq mn mo lv nr mq mr lz ns mt mu mv im bi translated">类似的文本搜索可以在许多领域有用，如搜索类似的文章，类似的简历，类似的项目简介，类似的新闻条目，类似的歌曲。希望你觉得这个小项目有用。</p><h2 id="313f" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">推荐阅读:</h2><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/a-complete-k-mean-clustering-algorithm-from-scratch-in-python-step-by-step-guide-1eb05cdcd461" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">Python中从头开始的完整K均值聚类算法:分步指南</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">还有，如何使用K均值聚类算法对图像进行降维</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="oo l nk nl nm ni nn lb mz"/></div></div></a></div><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/great-quality-free-courses-to-learn-machine-learning-and-deep-learning-1029048fd0fc" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">学习机器学习和深度学习的优质免费课程</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">顶级大学高质量免费课程的链接</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="op l nk nl nm ni nn lb mz"/></div></div></a></div><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/a-complete-guide-to-confidence-interval-and-examples-in-python-ff417c5cb593" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">置信区间的完整指南，以及Python中的示例</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">对统计学中一个非常流行的参数——置信区间及其计算的深入理解</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="oq l nk nl nm ni nn lb mz"/></div></div></a></div><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/want-to-become-a-data-scientist-in-12-weeks-3926d8eacee2" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">想在12周内成为数据科学家？</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">花钱前再想一想</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="or l nk nl nm ni nn lb mz"/></div></div></a></div><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/an-ultimate-cheat-sheet-for-numpy-bb1112b0488f" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">Numpy的终极备忘单</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">对学习也有好处</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="os l nk nl nm ni nn lb mz"/></div></div></a></div><div class="mw mx gp gr my mz"><a href="https://towardsdatascience.com/an-ultimate-cheat-sheet-for-data-visualization-in-pandas-4010e1b16b5c" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">熊猫数据可视化的终极备忘单</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">熊猫所有的基本视觉类型和一些非常高级的视觉…</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="ot l nk nl nm ni nn lb mz"/></div></div></a></div></div></div>    
</body>
</html>