<html>
<head>
<title>This AI makes blurry faces look 8 times sharper! SwinIR: Photo Upsampling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这个人工智能使模糊的脸看起来清晰8倍！SwinIR:照片上采样</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/this-ai-makes-blurry-faces-look-8-times-sharper-swinir-photo-upsampling-b41d394b41e9?source=collection_archive---------1-----------------------#2021-11-21">https://pub.towardsai.net/this-ai-makes-blurry-faces-look-8-times-sharper-swinir-photo-upsampling-b41d394b41e9?source=collection_archive---------1-----------------------#2021-11-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1da5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="850b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用AI把你的小512像素大图转换成4k！</h2></div><blockquote class="kr ks kt"><p id="ccbb" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/swinir/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/swinir/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/4e4173144136c02b6ef7570dddfb23b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MOno1xMYOYcLAG7AL5WkXQ.png"/></div></div></figure><h2 id="6ff2" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">看视频！</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="d404" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">你是否曾经有过一个你非常喜欢的图像，但只能找到一个小版本，看起来像上面左边的这个图像？如果你能把这张图片放大两倍，那该有多酷？这很棒，但是如果你能把它的清晰度提高四到八倍呢？现在我们在谈话，看看那个。在这里，我们将图像的分辨率提高了四倍，这意味着我们有四倍多的高度和宽度像素来获得更多的细节，使它看起来更加平滑。最棒的是，这是在几秒钟内完成的，完全自动，几乎可以处理任何图像。哦，你甚至可以在他们提供的演示中使用它，我们将在视频中看到。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi nb"><img src="../Images/4cf3d6c69846fdd20af10021ffb39d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OC8eGXqTgUGtXCgE.png"/></div></a></figure><h2 id="f902" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">图像上采样</h2><p id="2bbb" class="pw-post-body-paragraph ku kv it kx b ky nc kd la lb nd kg ld mn ne lg lh mr nf lk ll mv ng lo lp lq im bi translated">在进入这个令人惊叹的模型之前，我们必须介绍照片上采样或图像超分辨率的概念。这里的目标是从相应的低分辨率输入图像构建高分辨率图像，在本例中是人脸，但它可以是任何物体、动物或风景。低分辨率将是512像素或更小，没有那么模糊，但当你全屏显示时，它显然不是高清的。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nh"><img src="../Images/bc26f390e4200cecaa4f9db065fbabdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0JbMIfGK4WbB3mR6GcZBwg.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">低清晰度图像示例。</figcaption></figure><p id="0cac" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">只要花几秒钟看一看，你就会看到那些艺术品。我们把这张低清晰度的图像转换成一张高清晰度的图像，使脸部更加清晰。在这种情况下，一个2048像素的正方形图像的高清度提高了四倍:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nh"><img src="../Images/bc26f390e4200cecaa4f9db065fbabdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0JbMIfGK4WbB3mR6GcZBwg.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">使用SwinIR放大图像。</figcaption></figure><p id="14a8" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">为了实现这一点，我们通常有一个典型的类似UNet的卷积神经网络架构，我在以前的许多文章中都提到过，如果您想了解更多关于它们如何工作的信息，我邀请您阅读。主要的缺点是，CNN很难适应极其广泛的数据集，因为它们对所有图像使用相同的核，这使得它们对于局部结果和泛化很好，但当我们想要每个单独图像的最佳结果时，对于整体结果却不太强大。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nm"><img src="../Images/4321f8379f7bf9882bf098466a1cfad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*8GPsf622HvOTyEoBGyhYlw.gif"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">《变形金刚》中的卷积与自我关注。</figcaption></figure><p id="8245" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">另一方面，由于自注意机制捕捉每个图像的上下文之间的全局交互，变压器是有前途的架构，但是具有不适用于图像的繁重计算。</p><p id="c818" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">在这里，他们没有使用CNN或变压器，而是创建了相同的类似UNet的架构，同时具有卷积和注意力机制。或者更准确地说，使用Swin变压器。Swin transformer令人惊叹，因为它具有CNN的优势，可以处理更大尺寸的图像，并为注意力机制做好准备。这些注意力机制将会建立远程连接，这样模型最终会更好地理解整体图像，并以更好的方式重建图像。我不会进入Swin transformer 的细节，因为我已经在几个月前讨论过这个架构，并解释了它与自然语言处理中使用的CNN和经典transformer架构的区别。<br/>如果你想了解更多关于它以及研究人员如何将变形金刚应用于视觉的信息，请查看<a class="ae lr" rel="noopener ugc nofollow" target="_blank" href="/will-transformers-replace-cnns-in-computer-vision-55657a196833">我关于“走向人工智能”</a>的文章，并回来解释这个上采样模型！</p><h2 id="f291" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">斯温尼尔</h2><p id="fcaf" class="pw-post-body-paragraph ku kv it kx b ky nc kd la lb nd kg ld mn ne lg lh mr nf lk ll mv ng lo lp lq im bi translated">该模型被称为SwinIR，可以完成许多任务，包括图像上采样。正如我所说，它使用卷积来支持更大的图像。更准确地说，他们使用卷积层来减小图像的大小，你可以在这里看到。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nn"><img src="../Images/15c6ff3ba1677dd618a7298c35b514b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KVY_iG1OGSYNx_PtbGjYog.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">用于图像恢复的SwinIR的体系结构。图片来自<a class="ae lr" href="https://arxiv.org/pdf/2108.10257.pdf" rel="noopener ugc nofollow" target="_blank"> SwinIR [1] </a>。</figcaption></figure><p id="a3c5" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">该简化的图像被发送到模型中，并且还被直接传递到重建模块，以向模型提供关于图像的一般信息。这种表示基本上看起来像图像的许多奇怪的模糊版本，为向上缩放模块提供了关于整体图像应该看起来如何的有价值的信息。然后，我们看到Swin变压器层与卷积耦合。这是为了进一步压缩图像，并且总是提取关于风格和细节的更有价值的精确信息，而忘记整体图像。这就是为什么我们然后添加回错综复杂的图像，以给出我们通过跳过连接所缺少的整体信息。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="https://www.louisbouchard.ai/learnai/"><div class="gh gi no"><img src="../Images/9938b72f73abcefef69f1da1404cd179.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*trlxHefylCBKNJku.png"/></div></a></figure><p id="982e" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">所有这些最终被发送到一个名为子像素的重建模块，它看起来像这样，并使用我们刚刚创建的较大的一般特征和较小的细节特征来重建更高清晰度的图像。你可以把它看作是一个卷积神经网络，但反过来，或者简单地说是一个解码器，提取我们拥有的压缩特征，并从中重建一个更大的图像。同样，如果你想了解更多关于CNN和解码器的知识，你应该看看我写的一些关于它们的文章。</p><p id="86b3" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">因此，你基本上是在CNN中发送你的图像，获得这个新的表示，保存它供以后使用，同时在Swin transformer架构中发送它，以进一步压缩信息，并了解要重建的最重要的特征。然后，你将这些新功能与保存的功能结合起来，使用解码器来重建高清版本。</p><p id="a442" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">瞧！</p><p id="e453" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">现在你只需要足够的数据，就会有这样的结果(见<a class="ae lr" href="https://youtu.be/GFm3RfrtDoU" rel="noopener ugc nofollow" target="_blank">视频</a>！).</p><p id="4ff7" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">当然，和所有的研究一样，也有一些局限性。<br/>在这种情况下，可能是由于最初的卷积层，它对200像素以下的非常小的图像不太适用。您可能会看到像这样的伪像和奇怪的结果:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi np"><img src="../Images/8060bee40c912549cfb5334ee82a1d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*REF2oTYnMt0kJBswIqrSWw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk translated">带有SwinIR的175 x 183图像的升级版本。</figcaption></figure><p id="bd18" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">看起来你也可以使用更大的升级器来消除皱纹，如果你想这么做，这可能是一个有用的神器。除此之外，结果是相当疯狂的，因为在过去几天里玩了很多，四倍的升级是令人难以置信的。</p><p id="c873" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">你也可以玩它！他们让每个人都可以使用GitHub repo，有预先训练好的模型，甚至有无需任何代码就可以立即使用的演示。当然，这只是对这个令人惊叹的新模型的概述，我强烈邀请您阅读他们的论文以获得更深入的技术理解。所有的链接都在下面的参考文献中。让我知道你的想法，我希望你喜欢这篇文章！</p><p id="aa75" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">再次感谢<a class="ae lr" href="https://wandb.ai/site" rel="noopener ugc nofollow" target="_blank">重量&amp;偏见</a>赞助视频和文章，以及任何还在阅读的人！</p><p id="6948" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">下周带着另一篇激动人心的论文再见！</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="5b4c" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">简讯</strong> </a>！</p><h2 id="2198" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">支持我:</h2><ul class=""><li id="e254" class="nx ny it kx b ky nc lb nd mn nz mr oa mv ob lq oc od oe of bi translated">支持我的最好方式是成为这个网站<strong class="kx jd"> </strong>的成员，或者如果你喜欢视频格式，在<strong class="kx jd"> YouTube </strong>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="4869" class="nx ny it kx b ky og lb oh mn oi mr oj mv ok lq oc od oe of bi translated">跟我来这里上<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a></li><li id="d7a9" class="nx ny it kx b ky og lb oh mn oi mr oj mv ok lq oc od oe of bi translated">想进入AI或者提升技能，<a class="ae lr" href="https://www.louisbouchard.ai/learnai/" rel="noopener ugc nofollow" target="_blank">看这个</a>！</li></ul><h2 id="a9e7" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">参考</h2><ul class=""><li id="f2e0" class="nx ny it kx b ky nc lb nd mn nz mr oa mv ob lq oc od oe of bi translated">梁军，曹军，孙，张，范古尔和，2021。SwinIR:使用swin transformer进行图像恢复。IEEE/CVF国际计算机视觉会议论文集(第1833-1844页)。</li><li id="a5c6" class="nx ny it kx b ky og lb oh mn oi mr oj mv ok lq oc od oe of bi translated">代号:<a class="ae lr" href="https://github.com/JingyunLiang/SwinIR" rel="noopener ugc nofollow" target="_blank">https://github.com/JingyunLiang/SwinIR</a></li><li id="a7a5" class="nx ny it kx b ky og lb oh mn oi mr oj mv ok lq oc od oe of bi translated">演示:【https://replicate.ai/jingyunliang/swinir T2】</li></ul></div></div>    
</body>
</html>