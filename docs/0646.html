<html>
<head>
<title>ResNet Architecture: Deep Learning with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ResNet架构:用PyTorch进行深度学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/resnet-architecture-deep-learning-with-pytorch-19ecb7ca359e?source=collection_archive---------0-----------------------#2020-07-04">https://pub.towardsai.net/resnet-architecture-deep-learning-with-pytorch-19ecb7ca359e?source=collection_archive---------0-----------------------#2020-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9144" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><p id="9ca7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">从2015年赢得ImageNet大规模视觉识别挑战赛(ILSVRC)到现在，ResNet架构在数据科学领域表现卓越。除了ILSVRC，ResNet还赢得了检测和定位挑战以及MSCOCO检测和分割挑战。相当的壮举！</p><p id="603f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">那么，与之前的卷积网络相比，ResNet有什么独特之处呢？</p><h1 id="afac" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">雷斯内特:背后的工作</h1><p id="e593" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">因为输入=输出，所以实际上层次越多，网络越好。实际上不是。但是在现实世界中，由于消失梯度和维数灾难问题，这是不可能的。事实上，训练误差在特定时期后增加。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/eff2024e4f6e0ea973bd3cf8bfc0c089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AKG18fNQxcov2QeaAzkClA.jpeg"/></div></div></figure><p id="1aba" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">ResNet的工作非常简单，它将原始输入添加回通过一个或多个卷积层传递输入而获得的输出特征映射。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/c9cdf3200789c4597f296b5d194911ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*5UAu2uxrMIi4EAaII6IY8g.png"/></div></figure><h2 id="ef6c" class="mk kv iq bd kw ml mm dn la mn mo dp le kh mp mq li kl mr ms lm kp mt mu lq iw bi translated">让我们理解左边的图片。发生的是Relu(输入+输出)，其中输入是第一个数据或前一个块的数据，输出是Relu(W2(W1+b) + I)，其中W1和W2是两层的权重，b是前一层的偏差。</h2><h2 id="641f" class="mk kv iq bd kw ml mm dn la mn mo dp le kh mp mq li kl mr ms lm kp mt mu lq iw bi translated">现在我们知道了ResNet架构背后的基础，所以让我们创建一个。</h2><pre class="ly lz ma mb gt mv mw mx my aw mz bi"><span id="029e" class="mk kv iq mw b gy na nb l nc nd">class <strong class="mw ja">SimpleResidualBlock</strong>(nn.Module):<br/>    def <strong class="mw ja">__init__</strong>(self):<br/>        super().__init__()<br/>        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)<br/>        self.relu1 = nn.ReLU()<br/>        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)<br/>        self.relu2 = nn.ReLU()<br/>        <br/>    def <strong class="mw ja">forward</strong>(self, x):<br/>        out = self.conv1(x)<br/>        out = self.relu1(out)<br/>        out = self.conv2(out)<br/>        return self.relu2(out) + x <em class="ne"># ReLU can be applied before or after adding the input</em></span></pre><p id="a2de" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">请注意，如果输出的维数与残差不匹配，则我们不能执行Relu加法，因此，我们不仅要对输入应用池化，而且残差也将通过步长为1×1的conv变换，该变换会将滤波器投影为与输出相同，并且步长为2将是维数的一半，就像最大池一样。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nf"><img src="../Images/1efe57480dc87e8682d183e6667b6b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hb4l_paTxs7mFuLENeR3HQ.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk translated">VGG-19，34层平原与34层残余网的区别</figcaption></figure><p id="cbca" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在上图中，我们看到了ResNet34架构。第一层具有7x7卷积和步长2，以通过因子2对输入进行下采样。然后是3个单位块，之后是2个单位块的下采样。在最后一个平均池图层中，它创建了1000个要素地图，并对每个要素地图进行平均。结果是一个1000维的向量，然后直接馈入Softmax层，使其完全卷积。总共有6种不同类型的ResNet架构，即ResNet9、ResNet18、ResNet34、ResNet50、Resnet101和ResNet150，它们的层数不同。解释完一切之后，我现在应该给出完整的答案。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nk"><img src="../Images/839ed76d310fbd3e8fa350be1fd009c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nQKteTXtvkoifOiwynexzQ.png"/></div></div></figure><h1 id="58ff" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">PyTorch中使用ResNet和正则化技术对CIFAR10图像进行分类</h1><p id="4580" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">以下是数据集中的一些图像:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nl"><img src="../Images/ee319edd141f6f3d9ea9a17bcf96abb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B0LLVdmcxQoI78jgfuF3wA.png"/></div></div></figure><h1 id="31f8" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">系统设置</h1><pre class="ly lz ma mb gt mv mw mx my aw mz bi"><span id="9c05" class="mk kv iq mw b gy na nb l nc nd"><em class="ne"># Uncomment and run the commands below if imports fail</em><br/><em class="ne"># !conda install numpy pandas pytorch torchvision cpuonly -c pytorch -y</em><br/><em class="ne"># !pip install matplotlib --upgrade --quiet<br/></em>import os<br/>import torch<br/>import torchvision<br/>import tarfile<br/>import torch.nn as nn<br/>import numpy as np<br/>import torch.nn.functional as F<br/>from torchvision.datasets.utils import download_url<br/>from torchvision.datasets import ImageFolder<br/>from torch.utils.data import DataLoader<br/>import torchvision.transforms as tt<br/>from torch.utils.data import random_split<br/>from torchvision.utils import make_grid<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="6793" class="mk kv iq mw b gy nm nb l nc nd">project_name='05b-cifar10-resnet'</span></pre><h1 id="88f6" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">准备数据</h1><p id="2ba0" class="pw-post-body-paragraph jw jx iq jy b jz ls kb kc kd lt kf kg kh lu kj kk kl lv kn ko kp lw kr ks kt ij bi translated">让我们从下载数据集并创建PyTorch数据集来加载数据开始，就像我们在上一教程中所做的那样。</p><pre class="ly lz ma mb gt mv mw mx my aw mz bi"><span id="ac79" class="mk kv iq mw b gy na nb l nc nd"><em class="ne"># Dowload the dataset</em><br/>dataset_url = "http://files.fast.ai/data/cifar10.tgz"<br/>download_url(dataset_url, '.')<br/><br/><em class="ne"># Extract from archive</em><br/>with tarfile.open('./cifar10.tgz', 'r:gz') as tar:<br/>    tar.extractall(path='./data')<br/>    <br/><em class="ne"># Look into the data directory</em><br/>data_dir = './data/cifar10'<br/>print(os.listdir(data_dir))<br/>classes = os.listdir(data_dir + "/train")<br/>print(classes)</span><span id="f217" class="mk kv iq mw b gy nm nb l nc nd"><em class="ne"># Data transforms (normalization &amp; data augmentation)</em><br/>stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))<br/>train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'), <br/>                         tt.RandomHorizontalFlip(), <br/>                         tt.ToTensor(), <br/>                         tt.Normalize(*stats,inplace=True)])<br/>valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])<br/><em class="ne"># PyTorch datasets</em><br/>train_ds = ImageFolder(data_dir+'/train', train_tfms)<br/>valid_ds = ImageFolder(data_dir+'/test', valid_tfms)<br/>batch_size = 400<br/><em class="ne"># PyTorch data loaders</em><br/>train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)<br/>valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)<br/>def <strong class="mw ja">show_batch</strong>(dl):<br/>    for images, labels in dl:<br/>        fig, ax = plt.subplots(figsize=(12, 12))<br/>        ax.set_xticks([]); ax.set_yticks([])<br/>        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))<br/>        break<br/>show_batch(train_dl)</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/0b4ef7661c3eb4af1e2724fd170e1342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*HvzS__ZYyMxrK3H-e95isA.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk translated">因为标准化，颜色看起来不合适。</figcaption></figure><h1 id="25a5" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">使用GPU</h1><pre class="ly lz ma mb gt mv mw mx my aw mz bi"><span id="7478" class="mk kv iq mw b gy na nb l nc nd">def <strong class="mw ja">get_default_device</strong>():<br/>    """Pick GPU if available, else CPU"""<br/>    if torch.cuda.is_available():<br/>        return torch.device('cuda')<br/>    else:<br/>        return torch.device('cpu')<br/>    <br/>def <strong class="mw ja">to_device</strong>(data, device):<br/>    """Move tensor(s) to chosen device"""<br/>    if isinstance(data, (list,tuple)):<br/>        return [to_device(x, device) for x in data]<br/>    return data.to(device, non_blocking=True)<br/><br/>class <strong class="mw ja">DeviceDataLoader</strong>():<br/>    """Wrap a dataloader to move data to a device"""<br/>    def <strong class="mw ja">__init__</strong>(self, dl, device):<br/>        self.dl = dl<br/>        self.device = device<br/>        <br/>    def <strong class="mw ja">__iter__</strong>(self):<br/>        """Yield a batch of data after moving it to device"""<br/>        for b in self.dl: <br/>            yield to_device(b, self.device)<br/><br/>    def <strong class="mw ja">__len__</strong>(self):<br/>        """Number of batches"""<br/>        return len(self.dl)<br/>train_dl = DeviceDataLoader(train_dl, device)<br/>valid_dl = DeviceDataLoader(valid_dl, device)</span><span id="27f4" class="mk kv iq mw b gy nm nb l nc nd">def <strong class="mw ja">accuracy</strong>(outputs, labels):<br/>    _, preds = torch.max(outputs, dim=1)<br/>    return torch.tensor(torch.sum(preds == labels).item() / len(preds))<br/><br/>class <strong class="mw ja">ImageClassificationBase</strong>(nn.Module):<br/>    def <strong class="mw ja">training_step</strong>(self, batch):<br/>        images, labels = batch <br/>        out = self(images)                  <em class="ne"># Generate predictions</em><br/>        loss = F.cross_entropy(out, labels) <em class="ne"># Calculate loss</em><br/>        return loss<br/>    <br/>    def <strong class="mw ja">validation_step</strong>(self, batch):<br/>        images, labels = batch <br/>        out = self(images)                    <em class="ne"># Generate predictions</em><br/>        loss = F.cross_entropy(out, labels)   <em class="ne"># Calculate loss</em><br/>        acc = accuracy(out, labels)           <em class="ne"># Calculate accuracy</em><br/>        return {'val_loss': loss.detach(), 'val_acc': acc}<br/>        <br/>    def <strong class="mw ja">validation_epoch_end</strong>(self, outputs):<br/>        batch_losses = [x['val_loss'] for x in outputs]<br/>        epoch_loss = torch.stack(batch_losses).mean()   <em class="ne"># Combine losses</em><br/>        batch_accs = [x['val_acc'] for x in outputs]<br/>        epoch_acc = torch.stack(batch_accs).mean()      <em class="ne"># Combine accuracies</em><br/>        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}<br/>    <br/>    def <strong class="mw ja">epoch_end</strong>(self, epoch, result):<br/>        print("Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}".format(<br/>            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))</span><span id="1bf6" class="mk kv iq mw b gy nm nb l nc nd">def <strong class="mw ja">conv_block</strong>(in_channels, out_channels, pool=False):<br/>    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), <br/>              nn.BatchNorm2d(out_channels), <br/>              nn.ReLU(inplace=True)]<br/>    if pool: layers.append(nn.MaxPool2d(2))<br/>    return nn.Sequential(*layers)<br/><br/>class <strong class="mw ja">ResNet9</strong>(ImageClassificationBase):<br/>    def <strong class="mw ja">__init__</strong>(self, in_channels, num_classes):<br/>        super().__init__()<br/>        <br/>        self.conv1 = conv_block(in_channels, 64)<br/>        self.conv2 = conv_block(64, 128, pool=True)<br/>        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))<br/>        <br/>        self.conv3 = conv_block(128, 256, pool=True)<br/>        self.conv4 = conv_block(256, 512, pool=True)<br/>        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))<br/>        <br/>        self.classifier = nn.Sequential(nn.MaxPool2d(4), <br/>                                        nn.Flatten(), <br/>                                        nn.Linear(512, num_classes))<br/>        <br/>    def <strong class="mw ja">forward</strong>(self, xb):<br/>        out = self.conv1(xb)<br/>        out = self.conv2(out)<br/>        out = self.res1(out) + out<br/>        out = self.conv3(out)<br/>        out = self.conv4(out)<br/>        out = self.res2(out) + out<br/>        out = self.classifier(out)<br/>        return out<br/>model = to_device(ResNet9(3, 10), device)<br/>model</span></pre><h1 id="beaf" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">训练模型</h1><pre class="ly lz ma mb gt mv mw mx my aw mz bi"><span id="75a7" class="mk kv iq mw b gy na nb l nc nd"><strong class="mw ja">@torch.no_grad()</strong><br/>def <strong class="mw ja">evaluate</strong>(model, val_loader):<br/>    model.eval()<br/>    outputs = [model.validation_step(batch) for batch in val_loader]<br/>    return model.validation_epoch_end(outputs)<br/><br/>def <strong class="mw ja">get_lr</strong>(optimizer):<br/>    for param_group in optimizer.param_groups:<br/>        return param_group['lr']<br/><br/>def <strong class="mw ja">fit_one_cycle</strong>(epochs, max_lr, model, train_loader, val_loader, <br/>                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):<br/>    torch.cuda.empty_cache()<br/>    history = []<br/>    <br/>    <em class="ne"># Set up cutom optimizer with weight decay</em><br/>    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)<br/>    <em class="ne"># Set up one-cycle learning rate scheduler</em><br/>    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, <br/>                                                steps_per_epoch=len(train_loader))<br/>    <br/>    for epoch in range(epochs):<br/>        <em class="ne"># Training Phase </em><br/>        model.train()<br/>        train_losses = []<br/>        lrs = []<br/>        for batch in train_loader:<br/>            loss = model.training_step(batch)<br/>            train_losses.append(loss)<br/>            loss.backward()<br/>            <br/>            <em class="ne"># Gradient clipping</em><br/>            if grad_clip: <br/>                nn.utils.clip_grad_value_(model.parameters(), grad_clip)<br/>            <br/>            optimizer.step()<br/>            optimizer.zero_grad()<br/>            <br/>            <em class="ne"># Record &amp; update learning rate</em><br/>            lrs.append(get_lr(optimizer))<br/>            sched.step()<br/>        <br/>        <em class="ne"># Validation phase</em><br/>        result = evaluate(model, val_loader)<br/>        result['train_loss'] = torch.stack(train_losses).mean().item()<br/>        result['lrs'] = lrs<br/>        model.epoch_end(epoch, result)<br/>        history.append(result)<br/>    return history</span><span id="64af" class="mk kv iq mw b gy nm nb l nc nd">history = [evaluate(model, valid_dl)]<br/>history<br/>epochs = 8<br/>max_lr = 0.01<br/>grad_clip = 0.1<br/>weight_decay = 1e-4<br/>opt_func = torch.optim.Adam<br/>%%time<br/>history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, <br/>                             grad_clip=grad_clip, <br/>                             weight_decay=weight_decay, <br/>                             opt_func=opt_func)</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi no"><img src="../Images/2d80962f5563c48e8b1e7dc370d5d46f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xKU8w7BmENgDGeoF0s61Sw.png"/></div></div></figure><pre class="ly lz ma mb gt mv mw mx my aw mz bi"><span id="dc07" class="mk kv iq mw b gy na nb l nc nd">def <strong class="mw ja">plot_accuracies</strong>(history):<br/>    accuracies = [x['val_acc'] for x in history]<br/>    plt.plot(accuracies, '-x')<br/>    plt.xlabel('epoch')<br/>    plt.ylabel('accuracy')<br/>    plt.title('Accuracy vs. No. of epochs');<br/>plot_accuracies(history)</span><span id="beec" class="mk kv iq mw b gy nm nb l nc nd">def <strong class="mw ja">plot_losses</strong>(history):<br/>    train_losses = [x.get('train_loss') for x in history]<br/>    val_losses = [x['val_loss'] for x in history]<br/>    plt.plot(train_losses, '-bx')<br/>    plt.plot(val_losses, '-rx')<br/>    plt.xlabel('epoch')<br/>    plt.ylabel('loss')<br/>    plt.legend(['Training', 'Validation'])<br/>    plt.title('Loss vs. No. of epochs');<br/>plot_losses(history)</span><span id="71cd" class="mk kv iq mw b gy nm nb l nc nd">def <strong class="mw ja">plot_lrs</strong>(history):<br/>    lrs = np.concatenate([x.get('lrs', []) for x in history])<br/>    plt.plot(lrs)<br/>    plt.xlabel('Batch no.')<br/>    plt.ylabel('Learning rate')<br/>    plt.title('Learning Rate vs. Batch no.');<br/>plot_lrs(history)</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi np"><img src="../Images/154004f47229c7d090f0775e2eba4eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*af9tZ6z78uT-gECw_zbZ-A.png"/></div></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7a300876c3c7500fc27ab3adaef2a777.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*4S6SJ7wFj8hY8noRAHPSGw.png"/></div></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/91943fe6c75feaa7e9e6dcbe3cf9943f.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*6P65xDoNUVVm5v15y1e4zw.png"/></div></figure><p id="5f70" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">祝贺你走到这一步。我知道有很多东西需要学习，但是相信我，这个话题值得一读。</p><p id="5ea3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在，为了进一步阅读，这里有一些你应该检查的令人敬畏的链接。</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="nr ns l"/></div></figure><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="nt ns l"/></div></figure><p id="6bba" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">希望你喜欢这个，以后的帖子会在深度学习的GANs上。</p><p id="3dcc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">祝你过得愉快！</p><p id="a788" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">你可以通过@satyamkumar073在twitter和GitHub上找到我。</p></div></div>    
</body>
</html>