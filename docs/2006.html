<html>
<head>
<title>Understand Feature Selection in Machine Learning with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解Python机器学习中的特征选择</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understand-feature-selection-in-machine-learning-with-python-a0e99dbb7426?source=collection_archive---------0-----------------------#2021-07-21">https://pub.towardsai.net/understand-feature-selection-in-machine-learning-with-python-a0e99dbb7426?source=collection_archive---------0-----------------------#2021-07-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="eb92" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="186b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated"><strong class="ak">从数据中选择最佳特征集的技术</strong></h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/aa688c79a1063680837b7b1f6ab95898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dQXJL_iBwJzGh4ct"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@jontyson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔恩·泰森</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><blockquote class="li lj lk"><p id="4df6" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">特征选择及其类型</em> </strong></p></blockquote><p id="ea5c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我们都处理大量的数据，并不是每一列对我们的模型都很重要。</p><p id="b0ba" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">例如，考虑一个包含姓名、年龄、性别、学习时间和学校名称等特征的学生数据集。如果你必须建立一个模型来预测学生的分数，很明显，这个特性的学习时间比其他特性更有帮助。</p><p id="ddb0" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">当你在一个大的数据集上工作时，知道一个特性的重要性并不像我上面说的那么容易。因此，我们需要一种自动为我们工作的技术。特征选择就是这样一种方法。</p><p id="d312" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">顾名思义，特征选择是一种从数据集中选择最佳特征集来建立良好预测模型的技术。这个过程也可以称为变量选择或属性选择。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ml"><img src="../Images/82cfbd384bf93d121ffb0314fbe4d27b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MSsy8BY-fWAKPWxx26FNwA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图像<a class="ae lh" href="https://thecleverprogrammer.com/2020/10/18/feature-selection-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="b2d9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">特征选择的重要性和优势。</strong></p><p id="aede" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我们都知道为了得到一个准确的模型，数据清洗是多么重要。特征选择的意义也是如此。过多和多余的功能会导致许多问题，例如:</p><ul class=""><li id="5d75" class="mm mn it lo b lp lq ls lt mi mo mj mp mk mq mh mr ms mt mu bi translated">不必要的信息分配。</li><li id="7142" class="mm mn it lo b lp mv ls mw mi mx mj my mk mz mh mr ms mt mu bi translated">需要更多的时间来训练算法。</li><li id="1af5" class="mm mn it lo b lp mv ls mw mi mx mj my mk mz mh mr ms mt mu bi translated">当包含不必要的数据时，它也可能有噪声，导致模型过拟合。</li></ul><p id="f569" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">因此，当在用数据训练模型之前使用特征选择时，模型将是<strong class="lo jd">准确的，较少的训练时间减少了过度拟合。</strong></p><div class="na nb gp gr nc nd"><a rel="noopener  ugc nofollow" target="_blank" href="/latest-programming-languages-for-ai-5252d39e1c51"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd jd gy z fp ni fr fs nj fu fw jc bi translated">最新的人工智能编程语言</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">人工智能未来娱乐它的语言</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">pub.towardsai.net</p></div></div><div class="nm l"><div class="nn l no np nq nm nr lb nd"/></div></div></a></div><p id="218f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">有许多类型的特征选择方法，例如:</p><ul class=""><li id="1dc8" class="mm mn it lo b lp lq ls lt mi mo mj mp mk mq mh mr ms mt mu bi translated">过滤方法。</li><li id="3e36" class="mm mn it lo b lp mv ls mw mi mx mj my mk mz mh mr ms mt mu bi translated">包装方法。</li><li id="1f7f" class="mm mn it lo b lp mv ls mw mi mx mj my mk mz mh mr ms mt mu bi translated">嵌入式方法。</li></ul><blockquote class="li lj lk"><p id="758c" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">过滤方法</em> </strong></p></blockquote><p id="1655" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这是一种作为预处理步骤完成的特征选择。这将根据每个变量的重要性对其进行排序。这也称为单变量方法，因为它一次只作用于一个特征。这种方法是所有其他方法中最简单和容易的。</p><p id="a0a1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这种方法可以使用许多技术来实现，如<strong class="lo jd"> <em class="ln">卡方检验、信息增益、费希尔评分和相关系数。</em>T25】</strong></p><p id="e9c6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">使用python的例子:</strong></p><p id="9788" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这里有一段代码解释了使用热图的相关系数。</p><p id="f755" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在这里，我使用广告数据集来创建一个模型，该模型将根据给定的数据预测用户是否会点击广告。它包含以下几列。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5f62d6472f0caf8f7794fdcc174fbfbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*J2UVmzUJlXZ8uK6hJ_kXYQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="f96b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">现在，我将使用seaborn库中的热点图来可视化数据集中最相关的要素。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="df01" class="ny nz it nu b gy oa ob l oc od">sns.heatmap(data1.corr(), annot = True)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/63fb37e06372968d6864856cc36f0111.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*NJ-L_aUN0kFO1jr0prljJw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="4e0d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">相关性范围从-1到1。值为1的要素具有最大的相关性。在这里,“每天花在网站上的时间”是最相关的特性。我们可以去掉与目标变量相关系数较低的特征。如果一个以上的变量之间存在相关性，则称为多重共线性。</p><div class="na nb gp gr nc nd"><a rel="noopener  ugc nofollow" target="_blank" href="/useful-tips-of-if-else-control-statement-using-python-a9d652a74e4"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd jd gy z fp ni fr fs nj fu fw jc bi translated">使用Python的If-Else控制语句的实用技巧</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">通过示例对控制回路有基本的了解</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">pub.towardsai.net</p></div></div><div class="nm l"><div class="of l no np nq nm nr lb nd"/></div></div></a></div><div class="na nb gp gr nc nd"><a rel="noopener  ugc nofollow" target="_blank" href="/numpy-linear-algebra-on-images-ed3180978cdb"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd jd gy z fp ni fr fs nj fu fw jc bi translated">NumPy:图像上的线性代数</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">SVD来生成图像的压缩近似</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">pub.towardsai.net</p></div></div><div class="nm l"><div class="og l no np nq nm nr lb nd"/></div></div></a></div><blockquote class="li lj lk"><p id="bf21" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">包装方法</em> </strong></p></blockquote><p id="301c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">包装方法也称为贪婪方法，因为它搜索和评估所有可能的特征组合以满足评估标准。</p><p id="7bd9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">该方法采用了<strong class="lo jd"> </strong>前向特征选择、后向特征消除、穷举特征选择、递归特征消除等技术。</p><p id="7bab" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">使用python的例子:</strong></p><p id="9f99" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这里有一小段代码解释了糖尿病数据集的递归特征消除。这提供了使用scikit的最重要的特性。scikit库中的特征选择方法。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="39a7" class="ny nz it nu b gy oa ob l oc od">from sklearn.linear_model import LogisticRegression<br/>dataframe = pd.read_csv("diabetes.csv")<br/>array =dataframe.values</span><span id="c702" class="ny nz it nu b gy oh ob l oc od">x = array[:, 0:8]<br/>y = array[:,8]</span><span id="48d7" class="ny nz it nu b gy oh ob l oc od">#gives top 5 features<br/>rfe = RFE(model, 5)<br/>fit = rfe.fit(x,y)</span><span id="e909" class="ny nz it nu b gy oh ob l oc od">print("Num Features: %s" %(fit.n_features_))</span><span id="5cd6" class="ny nz it nu b gy oh ob l oc od">#this shows TRUE fpr features that are important and other features #as FALSE<br/>print("Selected Features: %s" %(fit.support_))</span><span id="960b" class="ny nz it nu b gy oh ob l oc od">#This is the ranking of the feature in order<br/>print("Feature Ranking: %s" %(fit.ranking_))</span><span id="4240" class="ny nz it nu b gy oh ob l oc od"><strong class="nu jd">#output:</strong><br/>Num Features: 5<br/>Selected Features: [True True False True True False False True]<br/>Feature Ranking: [1 1 2 3 4 1 1 1]</span></pre><blockquote class="li lj lk"><p id="b86c" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">嵌入方法</em> </strong></p></blockquote><p id="5e53" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">将特征选择作为其模型的一部分(内置)的机器学习模型被称为嵌入式或内在方法。内置的特征选择意味着模型包括有助于最大限度提高准确性的预测因子。</p><p id="150c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">使用嵌入式方法的模型有<strong class="lo jd"> <em class="ln">套索回归、岭回归、决策树、随机森林算法</em> </strong> <em class="ln">。</em></p><p id="ff75" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">使用python的例子:</strong></p><p id="1a4e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">这里有一段代码解释了套索回归，也称为L1正则化。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="a38b" class="ny nz it nu b gy oa ob l oc od">from sklearn.linear_model import LogisticRegression<br/>from sklearn.feature_selection import SelectFromModel</span><span id="34ba" class="ny nz it nu b gy oh ob l oc od">#set the regularization parameter C=1<br/>logistic = LogisticRegression(C=1, penalty ="l1", solver="liblinear'<br/>                          , random_state = 7).fit(x,y)</span><span id="6487" class="ny nz it nu b gy oh ob l oc od">model = SelectFromModel(logistic, prefit = True)</span><span id="5c71" class="ny nz it nu b gy oh ob l oc od">x_new = model.transform(x)</span><span id="3d0e" class="ny nz it nu b gy oh ob l oc od">#dropped columns have value of all zeros, keep other columns<br/>selected_columns = selected_features.columns[selected_features.var()        <br/>                                                 != 0]</span><span id="f7b8" class="ny nz it nu b gy oh ob l oc od">selected_columns</span><span id="e12d" class="ny nz it nu b gy oh ob l oc od"><strong class="nu jd">#output:</strong><br/>Index([0, 1, 2, 3, 4, 5, 6, 7], dtype = 'int64')</span></pre><p id="5100" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">还有一种叫做混合方法的方法，它是过滤器和包装器方法的组合。与其他方法相比，这些模型提供了更好的准确性。</p><blockquote class="li lj lk"><p id="201b" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">结论</em> </strong></p></blockquote><p id="0d93" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在本文中，我们主要讨论了特征选择、它的重要性和类型。希望你对这个话题有所了解。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="be38" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae lh" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="cb74" class="op nz it bd oq or os ot ou ov ow ox oy ki oz kj pa kl pb km pc ko pd kp pe pf bi translated">推荐文章</h1><p id="9da7" class="pw-post-body-paragraph ll lm it lo b lp pg kd lr ls ph kg lu mi pi lx ly mj pj mb mc mk pk mf mg mh im bi translated"><a class="ae lh" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄与Python </a> <br/> 2。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a>T5】3 .<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/deep-learning-88e218b74a14?source=friends_link&amp;sk=540bf9088d31859d50dbddab7524ba35">为什么LSTM在深度学习方面比RNN更有用？</a> <br/> 5。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88?source=friends_link&amp;sk=6844935e3de14e478ce00f0b22e419eb">神经网络:递归神经网络的兴起</a> <br/> 6。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/differences-between-concat-merge-and-join-with-python-1a6541abc08d?source=friends_link&amp;sk=3b37b694fb90db16275059ea752fc16a">concat()、merge()和join()与Python </a> <br/>的区别9。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a> <br/> 10。<a class="ae lh" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>