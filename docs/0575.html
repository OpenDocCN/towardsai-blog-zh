<html>
<head>
<title>Understanding the Universal Approximation Theorem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解通用逼近定理</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understanding-the-universal-approximation-theorem-327ceeed81ee?source=collection_archive---------2-----------------------#2020-06-09">https://pub.towardsai.net/understanding-the-universal-approximation-theorem-327ceeed81ee?source=collection_archive---------2-----------------------#2020-06-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2d69" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/8531e3b4994ae6bbae3adddd783c0b7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kiqDwoRpNU0jNkXH"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated"><a class="ae kl" href="https://unsplash.com/@jeremythomasphoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰瑞米·托马斯</a>在<a class="ae kl" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><h2 id="bbb4" class="km kn iq bd ko kp kq dn kr ks kt dp ku kv kw kx ky kz la lb lc ld le lf lg iw bi translated">让我们花些时间来理解神经网络的重要性</h2><p id="4256" class="pw-post-body-paragraph lh li iq lj b lk ll lm ln lo lp lq lr kv ls lt lu kz lv lw lx ld ly lz ma mb ij bi translated">神经网络是有史以来发明的最美丽的编程范例之一。在传统的编程方法中，我们告诉计算机做什么，将大问题分解成许多小的、精确定义的、计算机可以轻松执行的任务。相比之下，在神经网络中，我们不会告诉计算机如何解决我们的问题。相反，它从观察数据中学习，找出手头问题的解决方案。</p><p id="a9aa" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">直到最近，我们还不知道如何训练神经网络来超越更传统的方法，除了一些专门的问题。改变的是在所谓的深度神经网络中学习技术的发现。这些技术现在被称为深度学习。它们得到了进一步的发展，今天深度神经网络和深度学习在计算机视觉、语音识别和自然语言处理的许多重要问题上取得了出色的性能。</p><p id="ef33" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">话虽如此，还是让我们深入研究一下<strong class="lj ja">通用逼近定理吧。</strong></p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="d598" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">假设有人给了你一个wiggly函数，像下面这样说<strong class="lj ja"> f(x) </strong>。</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/dadbc26fe9d0cbdbf4f099e0e409578d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*zBAaD5rfMUDiQ6FSqJLXQg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">图片来源于<a class="ae kl" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森</a></figcaption></figure><p id="0d26" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">使用神经网络的一个显著特点是，它可以计算任何函数，不管它有多复杂。</p><p id="bec6" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">保证对于任何函数都有一个神经网络，以便对于每个可能的输入，<strong class="lj ja"> x </strong>，<strong class="lj ja"> </strong>值f(x)(或一些接近的近似值)从网络输出，例如:</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a12059cbc93dff787ffb77612d9af900.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*5PSUat6YC1KTfoF-DLam1A.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">图片来源于<a class="ae kl" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森</a></figcaption></figure><p id="8862" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">即使函数有多个输入f=f(x1，…，xm)和多个输出，上述结果也成立。例如，这是一个m=3个输入和n=2个输出的函数的网络计算:</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ffde1a5d4a4ed4a940c89ff0b78834e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*uXWM-SUqlCjmFV02Fn1_nw.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">图片来源于<a class="ae kl" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森</a></figcaption></figure><p id="6d08" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">这告诉我们，神经网络在其中具有某种普遍性，即无论我们想要计算的函数是什么，都已经有一个可用的神经网络。</p><p id="f0e4" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated"><strong class="lj ja"> <em class="mv">普适定理</em> </strong>为使用神经网络的人所熟知。但是为什么它是真的并没有被广泛理解。</p><blockquote class="mw mx my"><p id="f765" class="lh li mv lj b lk mc lm ln lo md lq lr mz me lt lu na mf lw lx nb mg lz ma mb ij bi translated">几乎任何你能想象到的过程都可以被认为是函数计算。考虑一下根据一小段音乐样本给一段音乐命名的问题。可以认为是计算一个函数。或者考虑将中文文本翻译成英文的问题。同样，这可以被认为是计算一个函数。</p></blockquote><p id="8436" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated"><strong class="lj ja"> <em class="mv">当然，仅仅因为我们知道有一个神经网络可以将中文文本翻译成英文，并不意味着我们有很好的技术来构建甚至识别这样的网络。这个限制也适用于传统的普适性定理，如布尔电路模型。</em> </strong></p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="63a5" class="nc kn iq bd ko nd ne nf kr ng nh ni ku nj nk nl ky nm nn no lc np nq nr lg ns bi translated">一个输入一个输出的通用性</h1><p id="10d3" class="pw-post-body-paragraph lh li iq lj b lk ll lm ln lo lp lq lr kv ls lt lu kz lv lw lx ld ly lz ma mb ij bi translated">让我们从了解如何构建一个神经网络开始，该网络用一个输入和一个输出来逼近一个函数:</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/dadbc26fe9d0cbdbf4f099e0e409578d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*zBAaD5rfMUDiQ6FSqJLXQg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">图片来源于<a class="ae kl" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森</a></figcaption></figure><p id="f79b" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">如果敏锐地观察，这就是普遍性问题的核心。一旦我们理解了这个特例，就很容易扩展到具有多个输入和多个输出的函数。</p><p id="667a" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">为了深入了解如何构建一个网络来计算f，让我们从一个仅包含一个隐藏层、两个隐藏神经元和一个包含单个输出神经元的输出层的网络开始:</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/11069f90270e1e9265bc10dd203dd87e.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*uMGlikouJ26RLUBtazOzgw.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">图片来源于<a class="ae kl" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森</a></figcaption></figure><p id="1d99" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">为了了解网络中的组件是如何工作的，让我们把注意力集中在最上面的隐藏神经元上。让我们从权重w = 0和偏差b = 0开始。</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/92884e16f1250bd9dd777c979b5f431b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*jzww-34j3pDKd-8FWeNRuw.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">权重= 0，偏差= 0</figcaption></figure><p id="6836" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">让我们通过保持偏差不变来增加权重‘w’。下图显示了2D空间中不同权重值“w”的函数f(x)。</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6e21eefd818f55730598fcc5b5c4c222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*PnwRZKgQr9OPB_k6t5W4ZQ.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">权重= 1，偏差= 0</figcaption></figure><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/839b95274c84ebd1183b5f1aa086292f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*pMamomITS8NIMmjv1la4gw.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">权重= 4，偏差= 0</figcaption></figure><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/4cec809e36d22cdfe807f8368ef74d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*gE93J3nnUMIA-4EqP9p8vQ.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">权重= -4，偏差= 0</figcaption></figure><p id="e9a2" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">现在让我们通过保持重量不变来改变偏差。</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7d2b90f133e08ed31bfcfc85be4e26f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*aNfMbOJI01cwEr4oNRLsPQ.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">权重= 3，偏差= 0</figcaption></figure><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/4b112385cb36ee4c6f9177c02f8dc2ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*BY3LYkeBdEsIZzMgqf169g.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">权重= 3，偏差= -1</figcaption></figure><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1ddfe849a7f0503477a68934e3cd4e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*fKXU346JtlNoXpeSGYhQ5g.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">权重= 3，偏差= 1</figcaption></figure><p id="68e2" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">这种简单神经网络中权重和偏差的可视化表示一定有助于您理解通用逼近定理背后的直觉。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="adaf" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated"><strong class="lj ja">多输入变量</strong></p><p id="8826" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">让我们将这种方法扩展到有许多输入变量的情况。这听起来可能很复杂，但我们需要的所有想法都可以在只有两个输入的情况下理解。因此，让我们解决两个输入的情况。</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/2ac8070819408af6701b583d3bdd0750.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*V1dyBuw546r-mL6pPiC4nw.png"/></div></figure><p id="235d" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">这里，我们有输入x和y，具有相应的权重w1和w2，以及神经元上的偏置b。让我们将权重w2设置为0，然后试验第一个权重w1和偏差b，看看它们如何影响神经元的输出:</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/122e77022c0e8a5998c9f1165ee372b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*Xf9EdAlWb82KQ0gOW_Ix0g.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">w1=0 w2=0且b=0</figcaption></figure><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/648f5f173eaaad8d833a614d2374225c.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*eMkA2pIkfwzPdQFfjd4ecQ.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">w1=3 w2=0，b=0</figcaption></figure><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d87dcfe24fef70a30a2d69b7428846fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*bzhji1Yp5Op0548I4KTMxA.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">w1=-7 w2=0且b=0</figcaption></figure><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b2b450ea63ecef381356325128716381.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*aWtOm8EFOPsMf9MIdSuDEw.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">w1=1 w2=0且b=1</figcaption></figure><p id="a90b" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">如你所见，当w2=0时，输入y对神经元的输出没有影响。好像x是唯一的输入。</p><p id="1e71" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">让我们也改变偏见，玩玩图表，了解发生了什么。</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/bfafe5aac7daac1eb7ed09a1d5e0a9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*lAT_mF7cjGd_gbH0LLe_wg.png"/></div></figure><p id="036f" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">随着输入权重变大，输出接近阶跃函数。不同的是，现在阶梯函数是三维的。和以前一样，我们可以通过修改偏移来移动步进点的位置。阶跃点的实际位置为Sx≠b/w1</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/80a25470e80b1123de5683b3b3b49bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*TFDREfE6rzi6m0MnHAkDsQ.png"/></div></figure><p id="223f" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">我们可以用刚刚构造的阶梯函数来计算一个三维凹凸函数。为此，我们使用两个神经元，每个计算x方向的阶跃函数。然后，我们将这些阶跃函数分别与权重h和h相结合，其中h是所需的凸起高度。下图说明了这一切:</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi nx"><img src="../Images/05e68e51e265d39ebb01c41153147b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8faWuQHznToRQUfrnFnKZQ.png"/></div></div></figure><p id="1211" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">我们已经知道了如何在x方向创建一个凹凸函数。当然，通过在y方向使用两步函数，我们可以很容易地在y方向创建一个凹凸函数。回想一下，我们通过在“y”输入上增加权重，在x输入上增加权重0来实现这一点。结果如下:</p><p id="5a7b" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated"><strong class="lj ja"> <em class="mv">这看起来和早期的网络几乎一模一样！</em>T3】</strong></p><p id="34ca" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">让我们考虑一下，当我们把两个凸起函数相加时会发生什么，一个在x方向，另一个在y方向，两个高度都是h:</p><figure class="mp mq mr ms gt ka gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/dfcf2b547d275cfc6664ba2728235961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*JaenpU-hbbYYpHhvpDDi-A.png"/></div></figure><p id="abf5" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">我们可以继续这样去逼近任何一种函数。但是我们会在这里结束这个讨论，我会在底部给你一些链接，让你玩这些神经网络。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="476a" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated"><strong class="lj ja">结论</strong></p><p id="45b2" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">我们所讨论的对普遍性的解释当然不是如何使用神经网络进行计算的实用处方！</p><p id="4213" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">虽然这个结果对构建网络没有直接用处，但它很重要，因为它解决了是否可以用神经网络计算任何特定函数的问题。</p><p id="380b" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">这个问题的答案总是“是”。所以正确的问题不是任何特定的函数是否是可计算的，而是计算函数的好方法是什么。</p><p id="a72f" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated">本文所有部分均改编自<a class="ae kl" href="http://michaelnielsen.org/" rel="noopener ugc nofollow" target="_blank">迈克尔·尼尔森</a>的《神经网络与深度学习》一书。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="2303" class="pw-post-body-paragraph lh li iq lj b lk mc lm ln lo md lq lr kv me lt lu kz mf lw lx ld mg lz ma mb ij bi translated"><strong class="lj ja">参考文献:</strong></p><ol class=""><li id="5d18" class="nz oa iq lj b lk mc lo md kv ob kz oc ld od mb oe of og oh bi translated"><a class="ae kl" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">神经网络可以计算任何函数的视觉证明</a>迈克尔·尼尔森著。</li><li id="1ef5" class="nz oa iq lj b lk oi lo oj kv ok kz ol ld om mb oe of og oh bi translated">本文是与<a class="ae kl" href="https://www.freecodecamp.org/" rel="noopener ugc nofollow" target="_blank">自由代码营</a>合作提供的<a class="ae kl" href="http://zerotogans.com/" rel="noopener ugc nofollow" target="_blank"> ZeroToGANs </a>课程<a class="ae kl" href="https://jovian.ml/" rel="noopener ugc nofollow" target="_blank"> Jovian.ml </a>作业的一部分。</li><li id="0af9" class="nz oa iq lj b lk oi lo oj kv ok kz ol ld om mb oe of og oh bi translated"><a class="ae kl" href="https://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow的游乐场</a>用于直观理解神经网络。</li><li id="9971" class="nz oa iq lj b lk oi lo oj kv ok kz ol ld om mb oe of og oh bi translated"><a class="ae kl" href="https://ml-playground.com/" rel="noopener ugc nofollow" target="_blank">机器学习游乐场</a></li></ol></div></div>    
</body>
</html>