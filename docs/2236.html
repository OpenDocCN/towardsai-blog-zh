<html>
<head>
<title>Dimensional Reduction — Feature Selection Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维—特征选择第1部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/dimensional-reduction-feature-selection-part-1-d5e4fac63a11?source=collection_archive---------3-----------------------#2021-10-09">https://pub.towardsai.net/dimensional-reduction-feature-selection-part-1-d5e4fac63a11?source=collection_archive---------3-----------------------#2021-10-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2b9e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/ebd7938b54ec3b0b489cccb7cc240d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*36lEsGXGfArzDrxawljORQ.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">照片由<a class="ae ko" href="https://unsplash.com/@edgr?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Edu格兰德</a>在<a class="ae ko" href="https://unsplash.com/s/photos/selection?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="9e20" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在上一篇文章中，我们了解了什么是降维，为什么我们需要降维，以及我们有什么样的降维方法/技术。</p><p id="b2c0" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你还没有检查它，那么我建议你检查前一篇文章</p><div class="ln lo gp gr lp lq"><a rel="noopener  ugc nofollow" target="_blank" href="/lets-learn-about-dimensionality-reduction-df4622f30c84"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jd gy z fp lv fr fs lw fu fw jc bi translated">让我们来学习降维</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">降维，或称降维，是将数据从高维空间转换到多维空间</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">pub.towardsai.net</p></div></div><div class="lz l"><div class="ma l mb mc md lz me ki lq"/></div></div></a></div><p id="c917" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，我们将了解特性选择方法，为什么我们需要使用它，以及有哪些方法可用于特性选择。</p><p id="4a84" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们开始吧</p><blockquote class="mf mg mh"><p id="2709" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">什么是特征选择</strong></p></blockquote><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mm"><img src="../Images/0d8b9ed16b3bc45915ef05271b39c709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q0z1UIRp4kzKZd0lc3jydw.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">照片由<a class="ae ko" href="https://unsplash.com/@patrickian4?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">帕特里克·福尔</a>在<a class="ae ko" href="https://unsplash.com/s/photos/selection?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="6447" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi mr translated"><span class="l ms mt mu bm mv mw mx my mz di">正如</span>我们在上一篇文章中所讨论的，真实世界的数据集可能包含许多不同特征的大量信息，但并不是所有的特征都适合我们的模型，我们只选择“最佳”的特征，这些特征表现良好，具有更高的准确性，并且在“特征选择”中计算开销较少</p><p id="3e7d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">特征选择是这样一个过程，在这个过程中，我们试图减少特征的数量，只找到对我们的模型有益的相关特征，并为我们开发预测模型提供高准确性。</p><p id="7b0e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">特征数量的减少也降低了计算成本，并且在某些情况下，还提高了模型的性能。</p><blockquote class="mf mg mh"><p id="09c8" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">为什么选择功能？</strong></p></blockquote><p id="071e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi mr translated">一些预测建模问题具有大量的变量，这些变量会减慢模型的开发和训练过程。</p><p id="beb5" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">功能选择有助于</p><ul class=""><li id="7a97" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated">它可以提高精确度</li><li id="e1c7" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">它降低了计算成本</li><li id="52c3" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">它提高了模型的可理解性，并且我们可以正确地将其可视化</li><li id="2cb7" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">它使机器学习模型能够更快地训练</li><li id="0d0f" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">它减少了过度拟合</li></ul><blockquote class="mf mg mh"><p id="53a7" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">特征选择方法</strong></p></blockquote><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div class="gh gi no"><img src="../Images/73c74a895818eb9dbf7dae29e2f4f6ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*tj5TMyp_whK7cd-nZioqYg.jpeg"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://littleml.files.wordpress.com/2016/07/feature-selection2.jpg?w=640" rel="noopener ugc nofollow" target="_blank">https://little ml . files . WordPress . com/2016/07/feature-selection 2 . jpg？w=640 </a></figcaption></figure><p id="8866" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi mr translated"><span class="l ms mt mu bm mv mw mx my mz di">特征选择技术大致分为以下两类</span></p><ul class=""><li id="ab34" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated"><strong class="kr jd">监督技术:</strong>这些技术可用于标记数据，并用于识别相关特征，以提高分类和回归等监督模型的效率</li><li id="a9ca" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated"><strong class="kr jd">无监督技术:</strong>这些技术可以用于未标记的数据</li></ul><blockquote class="mf mg mh"><p id="2333" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">监督特征选择:</strong></p></blockquote><p id="4f61" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在有监督的特征选择方法中，我们有3种方法</p><ul class=""><li id="afdc" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated">过滤方法</li><li id="b961" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">包装方法</li><li id="5db9" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">嵌入式方法</li></ul><p id="679b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们逐一了解它们</p><blockquote class="mf mg mh"><p id="c2bb" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">过滤方法</strong></p></blockquote><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi np"><img src="../Images/0e483611bd068082e0dc2effd5350e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2fXQCE4hyiT7tpObK8pGg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://en.wikipedia.org/wiki/Feature_selection#/media/File:Filter_Methode.png" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Feature _ selection #/media/File:Filter _ methode . png</a></figcaption></figure><p id="7503" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi mr translated"><span class="l ms mt mu bm mv mw mx my mz di"> F </span>过滤方法选择变量，而不考虑模型。它们仅基于一般特征，如与要预测的变量的相关性。过滤方法抑制最不感兴趣的变量。其他变量将是用于分类或预测数据的分类或回归模型的一部分。这些方法在计算时间上特别有效，并且对过拟合具有鲁棒性。[维基]</p><p id="0a80" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些方法比其他方法(即包装器、嵌入式混合方法)速度更快，计算成本更低。当处理高维数据时，使用过滤方法是很好的，因为它们在计算上更便宜。</p><p id="d7b6" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些方法通常用作预处理步骤。特征的选择独立于任何机器学习算法，相反，特征是基于它们在各种统计测试中与结果变量的相关性的分数来选择的。</p><p id="efd0" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">用于过滤方法的技术</p><ul class=""><li id="726f" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated">单因素分析</li><li id="9da4" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">统计算法</li><li id="3651" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">卡方检验</li><li id="d4c7" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">费希尔评分</li><li id="3440" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">方差分析</li><li id="2658" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">交互信息</li></ul><blockquote class="mf mg mh"><p id="4a91" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">优点:</strong></p></blockquote><ul class=""><li id="049f" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated">计算成本更低</li><li id="d9a6" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">对目标空间的特征使用相关能力</li><li id="837f" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">有助于快速筛选</li></ul><blockquote class="mf mg mh"><p id="8400" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">包装方法</strong></p></blockquote><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nq"><img src="../Images/d2311f792b15c3ff851c06909e19e050.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wEIOPfh2LXolRwiGe56gw.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://en.wikipedia.org/wiki/Feature_selection#/media/File:Feature_selection_Wrapper_Method.png" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Feature _ selection #/media/File:Feature _ selection _ Wrapper _ method . png</a></figcaption></figure><p id="8644" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi mr translated">包装器方法评估变量的子集，与过滤方法不同，它允许检测变量之间可能的相互作用。[维基]</p><p id="9b28" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这里，我们将特征集的选择视为一个搜索问题，其中不同的组合被准备、评估并与其他组合进行比较。</p><blockquote class="mf mg mh"><p id="d317" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated">包装方法中使用的技术</p></blockquote><ul class=""><li id="f444" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated"><strong class="kr jd">前进特征选择</strong></li></ul><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nr"><img src="../Images/00289defcfe0cd9ccafc5d1eb9d05671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-m6F2JQcOBIQMm_qkBaNg.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@gaellemarcel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">盖尔·马塞尔</a>在<a class="ae ko" href="https://unsplash.com/s/photos/forward?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="59fd" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi mr translated">这是一种迭代方法，我们从与目标相关的最佳表现变量开始。接下来，我们选择另一个变量，该变量与第一个选择的变量相结合可提供最佳性能</p><ul class=""><li id="3e13" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated"><strong class="kr jd">反向特征消除</strong></li></ul><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ns"><img src="../Images/7fd1f9e97e304e2e74ece43a4371ac3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhWStsS2glvFPoz-gGQZEQ.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://theridgeathleticclub.files.wordpress.com/2012/05/backwards-arrow.jpg" rel="noopener ugc nofollow" target="_blank">https://theridgeathleticclub . files . WordPress . com/2012/05/backward-arrow . jpg</a></figcaption></figure><p id="7bef" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi mr translated"><span class="l ms mt mu bm mv mw mx my mz di"> T </span>该方法与正向特征选择方法完全相反。我们从构建模型的所有特征开始。接下来，我们从模型中选择给出最佳评估度量值的变量</p><p id="39ed" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">包装方法的性能取决于分类器。基于分类器的结果选择特征的最佳子集。</p><ul class=""><li id="56d8" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated"><strong class="kr jd">递归特征消除</strong></li></ul><p id="9ec6" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi mr translated"><span class="l ms mt mu bm mv mw mx my mz di"> It </span>是一种贪婪优化算法，旨在找到性能最佳的特征子集。它重复地创建模型，并在每次迭代中保留性能最好或最差的特性。它用剩下的特征构造下一个模型，直到所有的特征都用完。然后，它根据要素被消除的顺序对其进行排序。</p><p id="112a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">优点:</p><ul class=""><li id="bd39" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated">使用变量组合来确定预测能力</li><li id="d390" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">找到变量的最佳组合</li></ul><p id="d571" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">缺点:</p><ul class=""><li id="65be" class="na nb it kr b ks kt kw kx la nc le nd li ne lm nf ng nh ni bi translated">在计算上比过滤方法昂贵</li><li id="285f" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">性能优于过滤方法</li><li id="439d" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">不建议在大量功能上使用</li><li id="c5f8" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">当观察次数不足时，过拟合风险增加。</li><li id="56ca" class="na nb it kr b ks nj kw nk la nl le nm li nn lm nf ng nh ni bi translated">当变量数量很大时，计算时间很长。</li></ul><blockquote class="mf mg mh"><p id="4e32" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">嵌入方法</strong></p></blockquote><figure class="mn mo mp mq gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nt"><img src="../Images/13d0ef6f323817aebc59a1e6125bd407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zzlTz5-yjZ8QBR8PTnAKrw.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://en.wikipedia.org/wiki/Feature_selection#/media/File:Feature_selection_Embedded_Method.png" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Feature _ selection #/media/File:Feature _ selection _ Embedded _ method . png</a></figcaption></figure><p id="0d9d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi mr translated">它结合了过滤器和包装器方法的特点，既包含了特征的交互，又保持了合理的计算成本。学习算法利用其自身的变量选择过程，同时执行特征选择和分类，</p><p id="1183" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些方法中一些最受欢迎的例子是套索和岭回归，它们具有内置的惩罚功能来减少过度拟合</p><p id="b120" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">LASSO回归执行L1正则化，它增加了一个等价于系数大小绝对值的惩罚</p><p id="f193" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">岭回归执行L2正则化，该正则化增加了等价于系数大小的平方的惩罚</p><p id="7cd7" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将通过Python实现来详细了解这些方法中的每一个，现在只需要知道这些方法是存在的…</p><p id="01b6" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">暂时就这样了👏👏。下一篇文章再见。</p><blockquote class="mf mg mh"><p id="9927" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">参考:- </strong></p></blockquote><p id="a08b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae ko" href="https://en.wikipedia.org/wiki/Feature_selection#Filter_method" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Feature _ select # Filter _ method</a></p><p id="cc2b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae ko" href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2016/12/introduction-to-feature-selection-methods-with-a-example-or-how-to-select-the-right-variables/</a></p><div class="ln lo gp gr lp lq"><a href="https://www.kdnuggets.com/2021/06/feature-selection-overview.html" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jd gy z fp lv fr fs lw fu fw jc bi translated">功能选择-所有你想知道的- KDnuggets</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">尽管您的数据集可能包含许多不同功能的大量信息，但只选择“最佳”的…</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">www.kdnuggets.com</p></div></div><div class="lz l"><div class="nu l mb mc md lz me ki lq"/></div></div></a></div><blockquote class="mf mg mh"><p id="f7c8" class="kp kq mi kr b ks kt ku kv kw kx ky kz mj lb lc ld mk lf lg lh ml lj lk ll lm im bi translated"><strong class="kr jd">看看我以前的文章:</strong></p></blockquote><div class="ln lo gp gr lp lq"><a rel="noopener  ugc nofollow" target="_blank" href="/lets-learn-about-dimensionality-reduction-df4622f30c84"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jd gy z fp lv fr fs lw fu fw jc bi translated">让我们来学习降维</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">降维，或称降维，是将数据从高维空间转换到多维空间</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">pub.towardsai.net</p></div></div><div class="lz l"><div class="ma l mb mc md lz me ki lq"/></div></div></a></div><div class="ln lo gp gr lp lq"><a href="https://medium.com/nerd-for-tech/machine-learning-automation-1c112e099005" rel="noopener follow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jd gy z fp lv fr fs lw fu fw jc bi translated">机器学习自动化…</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">"仅仅因为你能使某件事自动化，并不意味着它就应该自动化."</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">medium.com</p></div></div><div class="lz l"><div class="nv l mb mc md lz me ki lq"/></div></div></a></div><div class="ln lo gp gr lp lq"><a href="https://medium.com/@iamhimanshutripathi0/product-recommendation-based-on-visual-similarity-on-the-web-machine-learning-project-end-to-end-6d38d68d414f" rel="noopener follow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jd gy z fp lv fr fs lw fu fw jc bi translated">基于网页视觉相似性的产品推荐:机器学习项目…</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">众所周知，谷歌、亚马逊、网飞等大型科技公司都在使用推荐系统…</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">medium.com</p></div></div><div class="lz l"><div class="nw l mb mc md lz me ki lq"/></div></div></a></div><div class="ln lo gp gr lp lq"><a href="https://medium.com/datadriveninvestor/natural-langauge-processing-nlp-for-indian-language-hindi-on-web-64d83f16544a" rel="noopener follow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jd gy z fp lv fr fs lw fu fw jc bi translated">Web上印度语言(印地语)的自然语言处理(NLP)</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">"语言是一个秘密，每个人都可以处理，对我来说，这是美丽的."</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">medium.com</p></div></div><div class="lz l"><div class="nx l mb mc md lz me ki lq"/></div></div></a></div><div class="ln lo gp gr lp lq"><a href="https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5" rel="noopener follow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jd gy z fp lv fr fs lw fu fw jc bi translated">什么是平衡和不平衡数据集？</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">不平衡数据集到平衡数据集的转换技术及其比较</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">medium.com</p></div></div><div class="lz l"><div class="ny l mb mc md lz me ki lq"/></div></div></a></div><div class="ln lo gp gr lp lq"><a href="https://medium.com/analytics-vidhya/brain-tumor-classification-transfer-learning-e04f84f96443" rel="noopener follow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jd gy z fp lv fr fs lw fu fw jc bi translated">基于迁移学习的脑肿瘤分类</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">迁移学习的详细解释以及如何使用它进行分类</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">medium.com</p></div></div><div class="lz l"><div class="nz l mb mc md lz me ki lq"/></div></div></a></div><div class="ln lo gp gr lp lq"><a href="https://medium.com/analytics-vidhya/different-type-of-feature-engineering-encoding-techniques-for-categorical-variable-encoding-214363a016fb" rel="noopener follow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jd gy z fp lv fr fs lw fu fw jc bi translated">用于分类变量编码的不同类型的特征工程编码技术</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">"让我们从现有的功能创建新的功能."</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">medium.com</p></div></div><div class="lz l"><div class="oa l mb mc md lz me ki lq"/></div></div></a></div><p id="183d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你觉得这篇文章有趣，有帮助，如果你从这篇文章中学到了什么，请鼓掌(👏👏)<strong class="kr jd">并留下反馈。</strong></p><p id="e4ca" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">我们连线上</strong><a class="ae ko" href="https://www.linkedin.com/in/iamhimanshu0/" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd">Linkedin</strong></a><strong class="kr jd">，</strong><a class="ae ko" href="https://twitter.com/iam_himanshu0" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd">Twitter</strong></a><strong class="kr jd">，</strong><a class="ae ko" href="https://instagram.com/iamhimanshu0/" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd">insta gram</strong></a><strong class="kr jd">，</strong><a class="ae ko" href="https://github.com/iamhimanshu0" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd">Github</strong></a><strong class="kr jd">，以及</strong> <a class="ae ko" href="https://www.facebook.com/iamhimanshu0" rel="noopener ugc nofollow" target="_blank"> <strong class="kr jd">脸书</strong> </a> <strong class="kr jd">。</strong></p><p id="eb74" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">感谢阅读！</strong></p></div></div>    
</body>
</html>