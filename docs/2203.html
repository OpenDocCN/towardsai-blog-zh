<html>
<head>
<title>DeepMind and OpenAI Use Human Feedback to Maximize the Performance of Reinforcement Learning Agents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepMind和OpenAI使用人类反馈来最大化强化学习代理的性能</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/deepmind-and-openai-use-human-feedback-to-maximize-the-performance-of-reinforcement-learning-agents-50069cce8111?source=collection_archive---------1-----------------------#2021-09-27">https://pub.towardsai.net/deepmind-and-openai-use-human-feedback-to-maximize-the-performance-of-reinforcement-learning-agents-50069cce8111?source=collection_archive---------1-----------------------#2021-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="af2b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="1d95" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">2018年的一篇研究论文介绍了一种结合人类反馈和奖励优化的训练模型，以最大化RL代理的知识。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c37713f451297a5bdb6618932279c352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VgVu_wv3z4enBAyQGU4A6Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:<a class="ae lh" href="https://cloud.withgoogle.com/build/data-analytics/explore-history-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://cloud . with Google . com/build/data-analytics/explore-history-machine-learning/</a></figcaption></figure><blockquote class="li lj lk"><p id="db1e" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过10万名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到102，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="05a8" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">DeepMind和OpenAI是两家人工智能(AI)公司，处于强化学习(RL)进步的中心。从AlphaGo到Dota2 Five，DeepMind和OpenAI都在推动RL应用的边界，以在复杂的认知任务中超越人类。2018年，这两个研究巨头决定<a class="ae lh" href="https://arxiv.org/abs/1811.06521" rel="noopener ugc nofollow" target="_blank">在一篇新论文</a>中合作，该论文提出了一种新的方法来训练RL代理，使他们能够实现超人的性能。</p><p id="549b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">标题为<a class="ae lh" href="https://arxiv.org/abs/1811.06521" rel="noopener ugc nofollow" target="_blank">“奖励从人类偏好和Atari演示中学习”</a>的新研究论文介绍了一种结合人类反馈和奖励优化的训练模型，以最大化RL代理的知识。本文的核心论点是试图解决现代RL应用的一个主要限制。大多数成功的RL应用程序在诸如多人游戏的环境中运行，这些环境具有可以硬编码到RL代理中的良好建立的奖励模型。然而，我们在现实生活中面临的许多任务都没有给出明确的回报。考虑在室内环境中寻找物体的任务。在这种情况下，很难预先决定在执行了特定任务(如在床底下搜索，但没有找到目标对象)后如何奖励代理人？我们应该把代理送到隔壁房间还是房子的对面？在这些情况下，人类依靠直觉来解决复杂的任务，但我们对直觉在RL代理中的复制知之甚少。因此，RL依赖于下一个最好的东西:人类反馈。</p><p id="b021" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">反向强化学习(IRL)或模仿学习是一种RL变体，它专注于从人类反馈中学习奖励函数。虽然IRL克服了传统RL在回报稀少的环境中的一些限制，但它有一些基本的可伸缩性限制，因为它需要领域专家来训练代理。此外，如果一个RL代理只是简单地模仿人类法官，它怎么可能超越人类的表现？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/535778a2fe6f5ab564f8b92eee008ef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4TRT7HzPvaVrrz0gjgstvw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片鸣谢:OpenAI，DeepMind</figcaption></figure><h1 id="5395" class="ne nf it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">奖励从人类偏好中学习</h1><p id="0cfd" class="pw-post-body-paragraph ll lm it lo b lp nw kd lr ls nx kg lu na ny lx ly nb nz mb mc nc oa mf mg mh im bi translated">为了解决RL和IRL模型的一些限制，DeepMind和OpenAI提出了一种结合人类反馈和RL优化的方法，以实现RL任务的超人性能。所提出的技术通过利用两个主要反馈渠道来学习奖励函数，而不是假设特定的奖励模型:</p><p id="eb23" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">1) <strong class="lo jd">演示:</strong>任务上人类行为的几种轨迹。</p><p id="b1e0" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">2) <strong class="lo jd">偏好:</strong>人类成对比较智能体行为的短轨迹段，并偏好那些更接近预定目标的轨迹段。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/97a195804cb711d6c0cbb5e19a3cf2f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JEDq8LYm4HjgowNW6fSWrA.png"/></div></div></figure><p id="1646" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在一个典型的环境中，演示从实验开始就可用，而首选项是在训练期间动态构建的。步骤1允许RL代理近似人类训练者的行为，而步骤2优化从偏好和演示推断的奖励函数。从这个意义上来说，第二步提供了一个窗口来超越人类在RL任务中的表现。</p><p id="e9ae" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">带有人类偏好的奖励学习模型有两个主要组成部分:</p><p id="7db3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">A)深度Q学习网络，其从给定的一组观察中学习动作值函数。动作-值对是从演示和代理的偏好中学习的。在预培训阶段，代理仅从专家演示中学习行动价值。在训练期间，代理的经验被添加到奖励函数中。</p><p id="7f34" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">b)卷积神经网络(CNN ),其将观察值作为输入，并输出相应回报的估计值。CNN使用模型的偏好进行训练，并优化奖励函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/c43c11e391b15462b3ef5e7011955afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MnAYnxK03DyXmmqGq2cDzw.png"/></div></div></figure><p id="18c8" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">使用这种简单的架构，具有人类偏好模型的奖励学习不仅可以匹配使用深度Q学习网络学习的人类表现，还可以使用CNN来优化它。换句话说，使用这个模型允许RL代理不仅从演示(像传统的RL)中学习，而且从经验中学习。</p><h1 id="5f5e" class="ne nf it bd ng nh ni nj nk nl nm nn no ki np kj nq kl nr km ns ko nt kp nu nv bi translated">从头开始学习玩雅达利游戏</h1><p id="b855" class="pw-post-body-paragraph ll lm it lo b lp nw kd lr ls nx kg lu na ny lx ly nb nz mb mc nc oa mf mg mh im bi translated">Atari游戏是RL模型的经典基准测试环境，因为它们非常多样化，而且还包括众所周知的奖励功能。从这个角度来看，Atari游戏中的大多数RL代理都在模型本身中指定了奖励函数。然而，如果RL代理不能访问奖励功能，会发生什么呢？它如何能够处理Atari环境的多样性？这是DeepMind和OpenAI决定通过在Arcade Learning Environment上测试人类偏好模型的奖励学习来应对的挑战，Arcade Learning Environment是一个用于设计可以玩Atari 2600游戏的AI代理的开源框架。实验使用了四种基本设置:</p><p id="a105" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">1) <strong class="lo jd">模仿学习(第一基线):</strong>纯粹从示范中学习，没有强化学习。在这种设置中，不向代理提供偏好反馈。</p><p id="dfd3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">2) <strong class="lo jd">没有演示(第二基线):</strong>从偏好中学习，没有专家演示。</p><p id="bb6e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">3) <strong class="lo jd">演示+偏好:</strong>从偏好和专家演示中学习。</p><p id="0f7e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">4) <strong class="lo jd">演示+偏好+自动标记:</strong>从偏好和专家演示中学习，通过偏好演示片段而非来自初始轨迹的片段来自动收集附加偏好。</p><p id="c6d5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">不同的配置是针对9个雅达利游戏进行评估的，如Beamrider、Breakout、Enduro、Pong、Q*bert、Seaquest、Hero、Montezuma的复仇和Private Eye。后三款游戏是专门因为探索难度而选的。实验的结果是显著的:在5000万个步骤和6800个标签的完整时间表之后，人类偏好模型的奖励学习在所有游戏中的表现都优于替代方案，除了《私人眼睛》,这是一个众所周知的有利于模仿的游戏环境。不足为奇的是，实验表明，当有完整的反馈和演示时，大多数配置都达到了最佳性能。更值得注意的事实是，在大多数情况下，代理人学习的奖励函数与游戏的真实奖励函数一致，在某些情况下，甚至优于游戏的真实奖励函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/65eac1bbfcce02f2a7a6370c224bf668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYu0s2eeoI6tzdzlDoSLMg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片鸣谢:OpenAI，DeepMind</figcaption></figure><p id="56bd" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">OpenAI和DeepMind的工作表明，结合演示和偏好是在缺乏明确奖励的环境中引导RL代理的有效方法。实验表明，即使少量的偏好反馈也有助于RL模型胜过传统的模仿学习技术。这是结合人类和机器如何产生更好的人工智能的另一个例子。</p></div></div>    
</body>
</html>