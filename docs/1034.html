<html>
<head>
<title>Topic Modeling with NMF for User Reviews Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于NMF的用户评论分类主题建模</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/topic-modeling-with-nmf-for-user-reviews-classification-65913d0b44fe?source=collection_archive---------1-----------------------#2020-10-12">https://pub.towardsai.net/topic-modeling-with-nmf-for-user-reviews-classification-65913d0b44fe?source=collection_archive---------1-----------------------#2020-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="d465" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="ef6f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用文本挖掘技术识别一组文档的主题并按主题对其进行聚类的实用指南</h2></div><p id="c48d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">主题建模是一种基于内容主题的无监督学习文档聚类方法。</p><p id="ca57" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在本文中，我们将使用名为非负矩阵分解(NMF)的主题建模技术创建一个模型，以推断酒店评论数据集中存在的主要主题，分析这种分类在所有文档中的准确性，并使用我们训练好的模型预测新文档的主题。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/00435133c3d5f506eafbf88f889687d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wvv5E_ivVI4X-4_Ko5CtLw.jpeg"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">在<a class="ae ma" href="https://unsplash.com/@annietheby?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ma" href="https://unsplash.com/@annietheby?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Annie Theby </a>拍摄的照片</figcaption></figure><h1 id="4a42" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">主题建模简介</h1><p id="9da3" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">在这个领域中，一个<strong class="kq ja"> <em class="my">主题</em> </strong>指的是在同一主题的文档组合中频繁使用的术语的集合。因此，主题建模的关键输出是:主题列表<strong class="kq ja"><em class="my"/></strong>和与每个主题相关的文档列表<strong class="kq ja"><em class="my"/></strong>。</p><p id="7b17" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">主题建模在NLP中有几个实际应用，例如:</p><ul class=""><li id="44c7" class="mz na iq kq b kr ks ku kv kx nb lb nc lf nd lj ne nf ng nh bi translated">决定聊天机器人的对话主题。</li><li id="0c50" class="mz na iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated">检测给定主题的相似用户意见。</li><li id="b50f" class="mz na iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated">推断一组文档中隐藏的主题。</li><li id="44ce" class="mz na iq kq b kr ni ku nj kx nk lb nl lf nm lj ne nf ng nh bi translated">聚集客户反馈。</li></ul><h1 id="83d8" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">数据预处理</h1><p id="89d8" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">对于本文，我们将使用<a class="ae ma" href="https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe" rel="noopener ugc nofollow" target="_blank">这个数据集</a>在<em class="my"> Kaggle </em>中可用，其中包含51.5万条英文客户评论，对他们在欧洲各地酒店的体验进行评级。</p><pre class="ll lm ln lo gt nn no np nq aw nr bi"><span id="5311" class="ns mc iq no b gy nt nu l nv nw">import pandas as pd<br/>df = pd.read_csv('data/Hotel_Reviews.csv')</span><span id="16ad" class="ns mc iq no b gy nx nu l nv nw"># Join positive and negative review<br/>df["Review"] = df["Negative_Review"] + " " + df["Positive_Review"]</span><span id="2df7" class="ns mc iq no b gy nx nu l nv nw"># Keep only relevant columns<br/>df = df[['Review']]</span></pre><p id="5f0f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以下是该数据集中包含的文档示例:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ny"><img src="../Images/ae3e3eec8d1af670a6204dfba4b6359f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rvL75UL6aHA--9n3tpvR6A.png"/></div></div></figure><p id="fdfc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了获得可能的最佳结果，我们应该通过删除停用词、特殊字符、HTML标签、URL、换行符来清理数据集，并通过应用词干化或词干处理来标准化数据集。</p><p id="fa74" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，由于我们希望在本文中关注主题建模的应用，我们将只保留文档中的名词，假设这些名词拥有足够的信息来从评论中推断出主题。我们将使用<strong class="kq ja">s<em class="my">paCy</em>T25】来提取它们的引理，这些引理就是单词的规范形式。</strong></p><pre class="ll lm ln lo gt nn no np nq aw nr bi"><span id="14d2" class="ns mc iq no b gy nt nu l nv nw">import spacy<br/>import en_core_web_sm<br/>nlp = en_core_web_sm.load()</span><span id="498a" class="ns mc iq no b gy nx nu l nv nw">def extract_nouns(texts):<br/>  output = []<br/>  for doc in nlp.pipe(texts):<br/>    nouns = " ".join(token.lemma_ for token in doc if token.pos_ == 'NOUN')<br/>    output.append(nouns)<br/>  return output</span><span id="8025" class="ns mc iq no b gy nx nu l nv nw">df['Review_Only_Nouns'] = extract_nouns(df['Review'])</span></pre><p id="8997" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">结果如下:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nz"><img src="../Images/9f58798c1cf1271d614e8eb80b78bc55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j3olUiRr9pPkr9pFHgKRww.png"/></div></div></figure><h1 id="05e4" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">探索性数据分析</h1><p id="11c9" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">让我们首先做一些探索性的数据分析，以熟悉数据并找到在建模过程中可以帮助我们的有见地的事实。</p><p id="12c1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，如果我们的数据集仅由几个单词的文档组成，那么主题建模可能会很有挑战性。因此，我们有兴趣了解每个文档的字数分布:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oa"><img src="../Images/27a706a0dc13353744a7e76d6496f00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mU6PdJyNYvFZWkettwAH8w.png"/></div></div></figure><p id="7d53" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">更准确地说，我们可以使用<strong class="kq ja"> <em class="my">中的<em class="my">描述</em>功能，熊猫</em> </strong>发现我们正在处理平均37个单词的文本，并且75%的文档包含46个术语:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e25dfc92fb999bca5582163ca6e3bed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*dg74mE8iDVZ4Xqf5jWnqnw.png"/></div></figure><p id="1da6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们还应该验证我们的预处理管道的有效性。为此，我们可以按频率检查前20个术语，并确保没有特殊字符或同一术语的多个变体出现:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oc"><img src="../Images/b7a98533f33ce776c7b867eda3f09c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UVjaZo_cXSim9Z7QClkQzw.png"/></div></div></figure><p id="4fcc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于我们只考虑名词，这是一个可以接受的结果。最后，我们了解到我们预处理过的数据集由总共32 220个独特的名词组成，这将对我们以后的特征选择有用。</p><h1 id="b8ae" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">什么是非负矩阵分解？</h1><p id="950d" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">非负矩阵分解(NMF)是一种线性代数算法，它将高维向量(如术语文档矩阵)分解为低维表示，只有一个约束:分解的矩阵仅由非负值组成。</p><p id="1392" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">与其他流行的算法(如LDA)不同，LDA是一种概率算法，NMF是一种确定性算法，它根据潜在主题来实现语料库的单一表示。</p><p id="0992" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实质上，给定一个按单词排列的文档矩阵(A)，NMF将给我们两个矩阵:一个按单词排列主题的矩阵W，和按主题排列文档的系数矩阵H。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi od"><img src="../Images/0130797d7bb2f93d6ca9d05f868d7aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rvUWvFeeVj2qJ7kq.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">来源:<a class="ae ma" href="https://www.researchgate.net/figure/Conceptual-illustration-of-non-negative-matrix-factorization-NMF-decomposition-of-a_fig1_312157184" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/Conceptual-illustration-of-non-negative-matrix-factorization-NMF分解-of-a_fig1_312157184 </a></figcaption></figure><p id="e951" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">NMF算法将迭代地调整W和H的值，直到它们乘积的近似误差收敛。</p><h1 id="18e9" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">特征创建和主题建模</h1><p id="def9" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">对于NMF的输入矩阵，我们可以使用任何提供术语文档矩阵的特征创建技术，比如TFIDF、词袋或word2vec。</p><p id="0fc6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们将使用TFIDF或tf-idf，术语频率-逆文档频率的缩写，包括前5K个最频繁的术语<em class="my"> (max_features) </em>，并排除那些出现在少于2个文档<em class="my"> (min_df) </em>或超过95% <em class="my"> (max_df) </em>的文档中的术语。</p><pre class="ll lm ln lo gt nn no np nq aw nr bi"><span id="54d1" class="ns mc iq no b gy nt nu l nv nw">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer</span><span id="36fc" class="ns mc iq no b gy nx nu l nv nw">vec = TfidfVectorizer(<br/>  max_features=5000, <br/>  stop_words="english", <br/>  max_df=0.95, <br/>  min_df=2<br/>)</span><span id="f234" class="ns mc iq no b gy nx nu l nv nw">features = vec.fit_transform(df.only_nouns)</span></pre><p id="365e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一旦我们将我们的文档转换成特征矩阵，我们将适合我们的NMF模型。对于我们的第一次迭代，我们将我们想要强制模型推断的主题数量定义为10。</p><p id="9a06" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">主题的数量会对你的主题的连贯性产生巨大的影响。这可能是超参数化的，但是我不会在本文中讨论这个过程。</p><pre class="ll lm ln lo gt nn no np nq aw nr bi"><span id="4ab8" class="ns mc iq no b gy nt nu l nv nw">from sklearn.decomposition import NMF<br/>n_topics = 10<br/>random_state = 42</span><span id="e90b" class="ns mc iq no b gy nx nu l nv nw">nmf = NMF(n_components=n_topics, random_state=random_state)<br/>nmf.fit(features)</span></pre><p id="d5bc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了直观显示组成每个主题的单词集，我们将创建一个助手函数，打印每个主题的前15个单词:</p><pre class="ll lm ln lo gt nn no np nq aw nr bi"><span id="1efd" class="ns mc iq no b gy nt nu l nv nw">def display_top_words(n_top_words=15, feature_names, nmf):<br/>  for i, topic_vec in enumerate(nmf.components_):<br/>  print(i, end=' ')<br/>  for fid in topic_vec.argsort()[-1:-n_top_words-1:-1]:<br/>    print(feature_names[fid], end=' ')<br/>  print()</span><span id="5357" class="ns mc iq no b gy nx nu l nv nw">display_top_words(15, vec.get_feature_names(), nmf)</span></pre><p id="6499" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这些是代表10个主题的词，我们的模型也将根据这些词对未来的文档进行分类:</p><pre class="ll lm ln lo gt nn no np nq aw nr bi"><span id="7623" class="ns mc iq no b gy nt nu l nv nw">Topic 0: room size window air floor noise bathroom space coffee night door people tea day bath</span><span id="66eb" class="ns mc iq no b gy nx nu l nv nw">Topic 1: location facility value cleanliness distance size stuff attraction tube money shopping design access comfort decor</span><span id="481d" class="ns mc iq no b gy nx nu l nv nw">Topic 2 staff reception stay food desk member friendliness experience trouble atmosphere cleanliness decor facility check way</span><span id="c0dc" class="ns mc iq no b gy nx nu l nv nw">Topic 3: hotel star stay time city thing experience parking boutique minute centre day center way design</span><span id="a7c2" class="ns mc iq no b gy nx nu l nv nw">Topic 4: breakfast choice buffet selection variety option egg quality morning table cost coffee parking dinner day</span><span id="8c42" class="ns mc iq no b gy nx nu l nv nw">Topic 5: bed bathroom shower comfy size pillow mattress sheet bath king sofa tv space sleep linen</span><span id="48ea" class="ns mc iq no b gy nx nu l nv nw">Topic 6: service food customer quality reception charge desk concierge menu time check positive level star experience</span><span id="2910" class="ns mc iq no b gy nx nu l nv nw">Topic 7: bar bit restaurant station bathroom facility area night shower time food reception place stay day</span><span id="393d" class="ns mc iq no b gy nx nu l nv nw">Topic 8: view window floor city river pool terrace balcony roof building rooftop wall tower suite canal</span><span id="dc67" class="ns mc iq no b gy nx nu l nv nw">Topic 9: price value money quality parking drink size star com night food place euro bar comfort</span></pre><p id="6316" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从质量上来说，我们可以得出结论，这些主题彼此之间有很大的不同，甚至可以尝试给它们贴上一个标签，说明它们所代表的主题:</p><pre class="ll lm ln lo gt nn no np nq aw nr bi"><span id="9777" class="ns mc iq no b gy nt nu l nv nw">Topic 0: Room<br/>Topic 1: Location<br/>Topic 2: Reception<br/>Topic 3: Hotel<br/>Topic 4: Breakfast<br/>Topic 5: Comfort<br/>Topic 6: Customer service<br/>Topic 7: Facilities<br/>Topic 8: Views<br/>Topic 9: Value for money</span></pre><p id="458e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们还可以通过定义评分函数来定量地确定我们的模型的有效性，并相应地调整我们的模型，直到我们获得最佳结果。</p><h1 id="5770" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">用主题标记数据集</h1><p id="a54f" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">我们现在可以创建一个由前5个术语和我们的自定义标签标记的主题的辅助表，并使用它将聚类应用到我们的数据集，并用最具代表性的主题标记每个文档。</p><pre class="ll lm ln lo gt nn no np nq aw nr bi"><span id="7deb" class="ns mc iq no b gy nt nu l nv nw">def topic_table(n_top_words, feature_names, nmf):<br/>  topics = {}<br/>  for i, topic_vec in enumerate(nmf.components_):<br/>    topic_descr = ''<br/>    for fid in topic_vec.argsort()[-1:-n_top_words-1:-1]:<br/>      topic_descr = topic_descr + feature_names[fid] + " "<br/>    topics[i] = topic_descr<br/>  return pd.DataFrame({'Top_Topic_Terms': topics})</span><span id="9327" class="ns mc iq no b gy nx nu l nv nw"># Label topics with top 5 terms<br/>topic_df = topic_table(5, vec.get_feature_names(), nmf)</span><span id="f8ea" class="ns mc iq no b gy nx nu l nv nw"># Manually label topics<br/>topic_df['Label'] = ['Room', 'Location', 'Reception', 'Hotel', 'Breakfast', 'Comfort', 'Customer service', 'Facilities', 'Views', 'Value for money']</span><span id="dc3e" class="ns mc iq no b gy nx nu l nv nw"># Getting weights to classify our dataset<br/>document_weights = nmf.transform(vec.transform(df['Review_Only_Nouns']))</span><span id="5c04" class="ns mc iq no b gy nx nu l nv nw"># Store most representative topic<br/>df["Topic_idx"] = document_weights.argmax(axis=1)</span><span id="28cf" class="ns mc iq no b gy nx nu l nv nw"># Joining the original dataset with labels<br/>df = pd.merge(df, topic_df, left_on='Topic_idx', right_index=True, how='left')</span></pre><p id="8e53" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了确认在我们的评论数据集中是否准确地分配了主题，我们可以检查每个主题的几个样本。例如，以下是一些标有主题5的示例评论，从理论上讲，这些评论应该描述住在酒店的舒适程度:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oe"><img src="../Images/ae0dc1c8371b7f2d7ab7fd14310bb1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xm6NAlV_MQzVfps-xcjAJg.png"/></div></div></figure><h1 id="8498" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">预测新文档的主题</h1><p id="acc9" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">这项技术不仅有助于对源数据集中的文档进行分类或聚类。我们还可以给模型一个新的检查，让模型预测它最有代表性的主题。我们只需要使用<em class="my"> TfidfVectorizer </em>将文本转换成特征，并将其传递给我们模型的转换函数。</p><pre class="ll lm ln lo gt nn no np nq aw nr bi"><span id="78bc" class="ns mc iq no b gy nt nu l nv nw">new_review = [<br/>  "At breakfast you need to gaurd your table. Once I had finished my cereal went to get some toast by the time I had returned the table had been cleared and other guests had been seated when I complained to the waitress as they seemed to think it was my fault when a simple sorry would have been a more acceptable outcome. Quiet location great swimming pool"<br/>]</span><span id="7430" class="ns mc iq no b gy nx nu l nv nw">nmf.transform(vec.transform(new_review)).argsort(axis=1)[:,-1]</span></pre><p id="4774" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">结果是一个形状为[2，n_topics]的矩阵，每个主题都有一个分数。最高分代表文档中最具代表性的主题。</p><p id="a244" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这种情况下，即使这篇评论有一些拼写错误，我们的模型预测这篇评论本质上是关于主题4的，之前我们将其标记为与早餐体验相关。</p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><h2 id="82fe" class="ns mc iq bd md om on dn mh oo op dp ml kx oq or mn lb os ot mp lf ou ov mr iw bi translated">参考</h2><p id="bf00" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">[1]法特玛，f .，<a class="ae ma" href="https://medium.com/@fatmafatma/industrial-applications-of-topic-model-100e48a15ce4" rel="noopener">主题模型的工业应用</a></p><p id="9139" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[2]李苏珊，<a class="ae ma" href="https://towardsdatascience.com/topic-modeling-quora-questions-with-lda-nmf-aff8dce5e1dd" rel="noopener" target="_blank">话题建模Quora问题与LDA &amp;，</a>，<em class="my">走向数据科学</em></p><p id="6b0e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[3]罗布·萨尔加多，<a class="ae ma" href="https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45" rel="noopener" target="_blank">主题建模文章与NMF </a>，<em class="my">走向数据科学</em></p><p id="6cda" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[4] Sanjaya Subedi，<a class="ae ma" href="https://sanjayasubedi.com.np/nlp/nlp-with-python-topic-modeling/" rel="noopener ugc nofollow" target="_blank">使用Python的NLP:主题建模</a></p><p id="1f88" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[5] Keith Stevens等人，<a class="ae ma" href="https://www.aclweb.org/anthology/D12-1087.pdf" rel="noopener ugc nofollow" target="_blank">探索许多模型和许多主题的主题一致性</a></p></div></div>    
</body>
</html>