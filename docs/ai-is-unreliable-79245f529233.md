# AI 不靠谱

> 原文：<https://pub.towardsai.net/ai-is-unreliable-79245f529233?source=collection_archive---------1----------------------->

## 不是夸夸其谈，只是事实

![](img/daa9b2d90740b765a8f099f1a3a19178.png)

安德烈·梅特列夫在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

不要误会我；我不是一个反艾的人；恰恰相反:当我第一次看到我的研究生课程中的人工智能选项时，我只是觉得它是专门为我设计的。我在那里注册，再也没有回头。然后我在这个领域做了 30 多年的研究员，忍受了人工智能的冬天，缺乏预算，等等。那是在人工智能在本世纪变得很酷之前。

所以，相信我，我很熟悉真实人工智能的痛苦和局限性，它通常对公众隐藏起来，因为只有壮观的成就才能成为头条新闻。

为了热身，让我们面对关于人工智能的第一个事实:

> 人工智能一开始就不被认为是可靠的。

你可能会问，通过制造不可靠的系统，我们能赢得什么？

“传统”软件——也就是说，不是人工智能——的问题在于它太死板了。

从软件需求来看，程序员应该找出所有可能的用例，并处理每一个用例。实际上，这太难做到了，这就是为什么像 Windows Vista 这样的操作系统在发布时会有成千上万的错误。

那么，如果我们不是在设计时就想出所有可能的场景，而是给软件一些推理能力来处理发生的情况，会怎么样呢？哇，这对开发者和用户来说都是一件好事。

# 输入启发法

但到目前为止，处理每一种可能的情况是不可能的，至少对于过去和现在的计算机来说是这样。提高效率的一个办法是只探索最有希望的可能性，而忽略其他的可能性。

只考虑“有希望的”选择被称为“启发式使用”我们试图猜测什么是“有前途的”，什么是没有前途的。不用说，这是一场赌博。

> 试探法是一种“经验法则”,众所周知，它适用于大多数情况下的*。*

但是*其他*情况呢？根据启发式思想，其他案例被“过滤”或“切割”，因为它们被认为是不重要的。

砍伐通常在替代树木中进行；例如，在一个玩家轮流移动的游戏中，我有一些选项，然后，对于我的每个选择，另一个玩家也有一些选项，以此类推。

可能性之树长得非常快，因此需要砍掉一些树枝；这被称为“修剪”不砍的树枝探索到一定程度，最终我的启发式评价最高的第一个选项就是我的选择。你可以在很多[标准 AI 教材](http://aima.cs.berkeley.edu/)里查智能搜索。

# 我如何在手机游戏中使用试探法

大约在 2006 年，我编写了一个交互式的简化的类似象棋的游戏，用不同的颜色命名为“ [Kromate](https://www.palmgamingworld.com/palm/kromate.shtml) ”，与机器对弈(先是 Palm Pilot，后来是 iPhone，我知道你从未见过 Palm Pilot)。我使用了一个两级树，每个分支都用试探法进行了评估。得分最高的分支给机器下一步棋。

对搜索树叶(每个分支的末端)的评估使用“经验法则”，即启发式。其中之一是，移动代币的选择越多越好(因为当失败的玩家不能移动时，游戏结束)。另一个启发是“威胁”对手棋子是好事，反之则是坏事。你可以看到我的试探法非常简单。

结果出来的自动 Kromate 播放器好到我都打不过。我见过的人中没有一个人能一直打败它。以至于为了让 Kromate 更容易被人类玩家使用，我加入了一些关卡。最低等级使用随机移动，较高等级使用增加百分比的机器计算移动。使用的“忍者”等级只能从启发式搜索树移动。

这表明，在许多情况下，试探法发挥了作用，我们最终得到了一个高效、强大和灵活的计算机系统。

# 这是为了灵活性，而不是一致性

此外，智能搜索系统忽略了一些所谓“不重要”的案例。这是因为试探法使用的规则对*大多数*案例有意义，但不一定对*所有*案例有意义。

好处是我们可以拥有灵活而强大的系统，但正如生活中的一切一样，这是有代价的。被排除在外的选择并非“不可能”:它们只是被认为“不重要”，这是不一样的。

# 有时我们不想要灵活性

对于许多计算机系统来说，灵活性不是理想的属性，例如 ATM。你希望自动取款机尽可能的可预测和无趣。想象一下，自动取款机拒绝给你钱，因为，根据屏幕上的信息，“现在，这对你来说不是一个明智的选择。”你会继续作为那家银行的客户吗？当然不是。

在很多 AI 应用中，你需要灵活性；如果一些不准确的东西妨碍了我们，那也没关系。假设你有一张旧照片，到处都有损坏的部分。[最近的人工智能图像处理系统](https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life)可以“猜测”缺失的部分，并给你一个可信的好图像。随着缺失的部分越来越多，人工智能做出的假设将越来越远离照片拍摄时的内容。无论如何，它看起来是“自然”的，但是这个图像在某种程度上是“虚构”的。如果你想让图像忠实于拍摄时镜头前的东西，*那么人工智能就不是你要走的路*。

# 准确性——或缺乏准确性

在分类任务中，比如面部识别，系统正确完成的百分比被称为“准确率”，它从来不是 100%。因此，人工智能分类器出错的情况是有一定比例的，当然每年都在减少。这并不意味着人工智能系统是垃圾；你必须接受错误，这是生活的现实。甚至人类在识别人脸时也会出错，以至于一些 AI 系统比人类更好，对于大多数目的来说，这就足够了。对于其他情况——比如执法——确定性可能至关重要；你不能逮捕一个人，然后发现这是一个“分类错误”

另一项棘手的任务是医学诊断。机器出错的可能性意味着，任何具有重大后果的诊断都不应该信以为真，应该由医生仔细审查。这并不意味着基于人工智能的诊断是无用的，因为自动化系统可以包含数百万例的经验。相比之下，在人类医生的情况下，你，病人，局限于你面前的民间知识。

# 软件中的形式化方法

有一段时间，我参与了所谓“形式方法”的教学和研究，这是一种说法，即不要使用希望它们能工作的技巧来编程，而是用数学证明来编写软件，它将按预期工作。美国国家航空航天局在其部分软件中涉及了正式方法，因为你知道，在太空中软件错误的代价是非常昂贵的——甚至是人命。他们称之为“关键任务软件”我会把核电站的软件加到清单上。

就我个人而言，我使用了“基于模型”的软件，如 [NuSMV](https://nusmv.fbk.eu/) 或 Jackson 的 Alloy 语言和系统，这些软件功能强大，也易于理解。为了学习使用 Alloy，甚至有一本书(“[软件抽象](https://mitpress.mit.edu/books/software-abstractions-revised-edition)”)是杰克逊本人写的，他开发 Alloy 是他博士学位的一部分。

与大多数人工智能相比，形式化方法处于相反的一端:数学的确定性与不可靠的灵活性。

# 生成任务

除了分类之外的任务对人工智能产生的非自然的阐述更加宽容，因为，嘿，人类也不是统一的。例如，文本生成系统可以生成关于特定主题的令人难以置信的可信文本。这将使一些学校作业变得不可信，例如，“明天*给我带来一篇一页纸的论文，内容是关于二十世纪早期沙皇尼可莱瀑布的重要性*”学生可以简单地让一个 GPT 式的系统想出一些相关的东西，也许经过几次尝试后，这个文本看起来可以接受作为家庭作业，就这样。检测抄袭的工具将会失败，因为它们是为检测复制粘贴抄袭而设计的，而不是人工智能生成的文本。我们需要新一代的剽窃检测工具，但我不知道这是否可行。

*如果你是一名教师，请为新一代人工智能工具做好准备。*

# 人工智能不是为了可靠性

AI 不是单一的技术；它更多的是松散相关技术的集合，比如智能搜索、图像识别或生成、机器人、自动推理等等。他们唯一的共同点是，当应用时，产生的行为“看起来像人类的智能”，这绝不是一个确切的定义。大多数人工智能技术的另一个共同点是，它们本来就不可靠。

也许人工智能的这两个方面导致了相同的结果:*你不应该期待甚至没有很好定义的东西的可靠性*。