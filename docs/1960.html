<html>
<head>
<title>This Google Model Learns by Comparing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这个谷歌模型通过比较来学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/this-google-model-learns-by-comparing-ee28da6aa60d?source=collection_archive---------0-----------------------#2021-07-05">https://pub.towardsai.net/this-google-model-learns-by-comparing-ee28da6aa60d?source=collection_archive---------0-----------------------#2021-07-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6385" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="a28a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">奇异向量典型相关分析是一种掌握两个数据集之间表示相似性的技术。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e2a0411019f60c0e71cd038c7326c69e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Jm7q07pkD3ouqgXy.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://www.digitalvidya.com/blog/representation-learning-101-get-ready-set-go/" rel="noopener ugc nofollow" target="_blank">https://www . digitalvidya . com/blog/representation-learning-101-get-ready-set-go/</a></figcaption></figure><blockquote class="li lj lk"><p id="3101" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过90，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到85，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="075d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">解释和理解深度神经网络的行为仍然是深度学习生态系统中的主要挑战之一。作为人类，我们经常试图通过与我们熟悉的其他知识领域进行比较来理解新的主题。从这个角度来看，比较不同的神经网络似乎是提高其可解释性的一种自然手段。然而，深度学习模型的比较远不是一件容易的事情。给定两个深度神经网络，我们如何确定它们有多相似，并将其与它们的性能相关联？来自Google Brain的一组人工智能(AI)研究人员一直在这个领域做一些开创性的工作，开发了表征相似性的概念，以量化不同深度神经网络之间的相似性。</p><h1 id="6c1c" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">SVCCA与表征相似性</h1><p id="b6cf" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">我们能否定量比较两种不同深度学习模型学习到的表征？代表性相似性的概念试图使用统计模型来计算不同深度神经网络之间的相似性。在一篇论文“<a class="ae lh" href="https://arxiv.org/abs/1706.05806" rel="noopener ugc nofollow" target="_blank"> SVCCA:深度学习动力学和可解释性的奇异向量典型相关分析</a>”中，谷歌引入了一种简单且可扩展的统计方法，用于评估深度神经网络之间的相似性。SVCCA是一种统计技术，用于关联由基础过程产生的两组观察值。它识别两组多维变量之间的“最佳”(最大化相关性)线性关系(在相互正交和范数约束下)。</p><p id="9f11" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在深度学习模型的情况下，SVCCA通过评估隐藏层中的神经元激活来测量两个深度神经网络之间的接近度。使用这种技术，谷歌的研究人员专注于两个主要场景:</p><p id="c3f3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">a)评估由两个不同的神经网络学习的表示之间的相似性。</p><p id="6cbc" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">b)评估由同一深度神经网络中的隐藏层学习的表示之间的相似性。</p><p id="10d1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">作为一种技术，SVCCA接受两组神经元，并输出它们都学习的对齐的特征图。该方法还考虑了表面差异，如神经元排序的排列(对比较不同的网络至关重要)，并可以在其他更直接的比较失败的情况下检测相似性。例如，让我们使用<a class="ae lh" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>数据集训练两个卷积神经网络(绿色和红色)。左边的图像显示了使用传统技术的比较，该技术测量网络中最高水平的神经激活。如您所见，这两个网络没有明显的相似之处，对吗？没那么快……应用SVCCA后我们可以清楚地看到，两个网络学习到的表征确实相当相似(右图)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b911eb1a34a4bd3a801b74df538ee397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*2HAguMABRscJZWVMqFGOOg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd nf">图片来源:谷歌</strong></figcaption></figure><p id="3043" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">Google还应用SVCCA来评估同一网络中不同隐藏层学习到的表示之间的相似性。例如，下图显示了一个矩阵，用于评估不同训练阶段中隐藏层之间的相似性。矩阵中的每个[i，j]单元测量层I和j之间的相似性。该实验表明，更接近输入的深层神经网络中的层比更高层收敛得更快。这被称为自下而上收敛，是使用SVCCA获得的最重要的发现之一。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/da3c7620142ad3ca41f72aa783b6de68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*8qiXpaJJmEIEFkJsE0RGkw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd nf">图片来源:谷歌</strong></figcaption></figure><p id="14a6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">Github 提供了SVCCA技术<a class="ae lh" href="https://github.com/google/svcca" rel="noopener ugc nofollow" target="_blank">的基本实现。</a></p><h1 id="5778" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">记忆和概括网络</h1><p id="8c15" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">在SVCCA论文发表后，谷歌大脑团队将他们的工作扩展到更复杂的CNN架构以及递归神经网络(RNNs)。这项工作在最近的一篇研究论文中得到了强调，该论文题为“关于具有典型相关性的神经网络中的表征相似性的见解”。在CNN的案例中，谷歌团队专注于两个主要架构:</p><ul class=""><li id="56ea" class="ob oc it lo b lp lq ls lt na od nb oe nc of mh og oh oi oj bi translated"><em class="ln">一般化网络</em>:CNN通过未修改的、精确的标签对数据进行训练，并学习推广到新数据的解决方案。</li><li id="a337" class="ob oc it lo b lp ok ls ol na om nb on nc oo mh og oh oi oj bi translated"><em class="ln">记忆网络</em>:在带有随机标签的数据集上训练的CNN，使得它们必须记忆训练数据，并且根据定义不能概括。</li></ul><p id="0a0e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在将SVCCA应用于每种CNN的多个实例后，实验表明，<em class="ln">不同的</em>一般化网络组始终比记忆网络组收敛到更相似的表示(特别是在后面的层中)。在这两种情况下，每组概括和记忆网络的CCA距离都显著减小，因为每个单独组中的网络都做出了类似的预测。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/10c75b5a50a8877ef5e23c369f9903bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O8dIc7vn1ktscYVjJTk9xw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd nf">图片来源:谷歌</strong></figcaption></figure><h1 id="c7bd" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">SVCCA和RNNs</h1><p id="8021" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1806.05759" rel="noopener ugc nofollow" target="_blank">在他们最近的研究</a>中，谷歌还应用了SVCCA来研究递归神经网络(RNNs)中的相似性。rnn的架构与CNN有一些相同的复杂性，但rnn提出了额外的挑战，即它们的表示在序列的过程中会发生变化。</p><p id="b78c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">将SVCCA应用于rnn的主要目的是确定后者是否表现出与CNN相同的自下而上的收敛模式。为了测试这一点，谷歌大脑团队测量了训练过程中RNN每一层的表示与其在训练结束时的最终表示之间的CCA距离。结果表明，就像CNN一样，RNNs也以自下而上的方式收敛。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/5c81d12a1d6f5e346ddb55c6408ac6c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EpVZtMnSlI_By0HDvKF6Lw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd nf">图片来源:谷歌</strong></figcaption></figure><p id="bbf8" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">比较和类比是人类认知中的重要元素。像SVCCA这样的技术已经使用相似性来揭示CNN和rnn的重要和未知的特征。正如生活中的一切一样，比较可能成为帮助我们理解深度学习网络行为的重要机制。</p></div></div>    
</body>
</html>