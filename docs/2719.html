<html>
<head>
<title>A Peak at How the Brain can Perform Principal Component Analysis.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大脑如何进行主成分分析的高峰。</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-peak-at-how-the-brain-can-perform-principal-component-analysis-241e83225e5?source=collection_archive---------3-----------------------#2022-04-29">https://pub.towardsai.net/a-peak-at-how-the-brain-can-perform-principal-component-analysis-241e83225e5?source=collection_archive---------3-----------------------#2022-04-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="bf62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大脑和现代世界有一个共同点:输出来自对巨大信息数据集的分析。最广为人知的数据分析方法之一叫做<em class="kl">主成分分析</em>或<em class="kl"> PCA </em>。该方法的目标是将输入转换成新的表示，其中变量是成对去相关的。换句话说，该方法删除了数据集中的冗余。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/cd84037a0a738848cb96eb7bb7a18542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e20zIRxHTPXFMA9ZSzt_YA.png"/></div></div></figure><p id="dadd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图代表了在开源iris flower数据集上应用的PCA方法(<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/datasets/plot _ iris _ dataset . html</a>)。这个数据集将是本文中使用的一个例子。尽管它很简单，但它可以很好地解释不同的方法。</p><p id="55da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以看出，在主成分分析空间中，不同的物种是可以清楚识别的。PCA是分析数据结构的有用工具，但它也有一些局限性。我要说的一个主要问题是，构成PCA空间基础的PCA向量必须是正交的。然而，没有理由认为它适用于所有数据集。尽管如此，PCA仍然是更高级的数据分析技术的基本模块之一。</p><p id="54db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">近年来，大量工作表明，视觉系统早期阶段的神经元响应特性类似于自然图像的冗余减少技术。大脑中的神经元不是独立的，而是相互连接的。这些神经元连接不是静态的，它们的动力学被称为突触可塑性。虽然突触可塑性是神经科学中一个活跃的研究课题，但这种神经生物学现象已由Donald Hebb于1949年开始描述:</p><p id="765f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">让我们假设回响活动(或“迹线”)的持续或重复往往会诱导持久的细胞变化，从而增加其稳定性……当细胞A的轴突足够接近以激发细胞B并重复或持续地参与激发它时，在一个或两个细胞中会发生一些生长过程或代谢变化，从而增加A作为激发B的细胞之一的效率。</strong></p><p id="c245" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然这个规则看起来很罗嗦，但事实上它可以被简化为以下内容:<strong class="jp ir">连接在一起的细胞，一起触发</strong>。或者换句话说，当神经元一起活动时，它们的突触连接会增加。</p><p id="a0da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经科学中的Hebb规则可以归为无监督学习规则。事实上，它描述了神经可塑性的基本机制，其中由于突触前神经元引起的重复刺激增加了与突触后神经元的连接。我将使用一个简单的神经元模型，由Oja于1982年提出，该模型是线性的，神经元的输出实际上是接收到的输出的线性组合。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kz"><img src="../Images/63c638cfc5339a2a91e806a6ace54df2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X86Z2moby9xYAhXg9I0cgg.png"/></div></div></figure><p id="a70d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，神经元的活动<strong class="jp ir"> V </strong>由下式描述:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi la"><img src="../Images/616bbbe7ddff58498697443721d8f1b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*Z9_EEiMB7HuRRj1ofJokmg.png"/></div></figure><p id="e999" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另外，我们可以注意到，神经元的输出<strong class="jp ir"> V(t) </strong>实际上是一个权重向量<strong class="jp ir"> W </strong>和一个输入向量<strong class="jp ir"> x </strong>的标量积。</p><p id="7c61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，如果在每个时间步，神经元接收从概率分布<strong class="jp ir"> P </strong>中提取的特定刺激，权重向量将根据(Hebb规则)进行修改:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/e0593ff37e37ef6cb15f52a7cf0f46b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*3tK-SfYaE75wfLiLTSAbiw.png"/></div></figure><p id="ffa8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">λ是一个称为学习率的量。该学习规则对应于Hebb规则，因为如果输入<strong class="jp ir"> I </strong>和输出<strong class="jp ir"> V </strong>同时为高，则权重将增加。</p><h1 id="c9b7" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">几何解释</h1><p id="9828" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">为了深入了解这些修改在数据分布<strong class="jp ir"> P </strong>中捕捉到了什么，让我们看一个二维示例。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mf"><img src="../Images/d44e23aebb52c5de61721aacb3d51fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NLpYqYGSIscnbTz7lWglZQ.png"/></div></div></figure><p id="73b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分布<strong class="jp ir"> P </strong>由蓝色区域表示。由于<strong class="jp ir"> P </strong>的强方向性，输入呈现后学习规则的每次迭代都将根据红色和粉色箭头修改权重<strong class="jp ir"> W </strong>。突触权重向量的最终方向将是分布<strong class="jp ir"> P </strong>的最大方差的方向，或者换句话说，是<strong class="jp ir"> P </strong>最宽的方向。然而，这个学习规则有一个问题。<strong class="jp ir"> W </strong>的范数是无界的，因为它总是沿着输入协方差矩阵的特征向量的方向增加。</p><h1 id="be3d" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">抑制增长:Oja的规则</h1><p id="1c04" class="pw-post-body-paragraph jn jo iq jp b jq ma js jt ju mb jw jx jy mc ka kb kc md ke kf kg me ki kj kk ij bi translated">这个规则的想法是通过增加一个减的第二项来修改Hebb规则，这个减的第二项将限制<strong class="jp ir"> W </strong>的增长。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/81079d22265f9a125d96811f399c2e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*luYgXwFZ_Qe_raVUhWrxqQ.png"/></div></figure><p id="a088" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么这个约束规则如此有趣？一旦无监督学习完成，权重遵循以下等式:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi la"><img src="../Images/f5b4bc255d9a99ab40da19275e9124f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*cRNZCWO9OuivaFb7GGWBvg.png"/></div></figure><p id="0076" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<strong class="jp ir"> W </strong>和<strong class="jp ir"> x </strong>为多维向量。该等式的第一项可以改写如下:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/d74d6336ed56625c1ab9a9daf54b583b.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*vgM8ZQfE9cL5Ggq4jZy7AQ.png"/></div></figure><p id="dd35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用<strong class="jp ir"> C </strong>输入协方差矩阵。因此，我们得到:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/82786a29969d7dabac2bf2a3d21f6482.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*JyQhzYojsn2XpXhn8CZXpg.png"/></div></figure><p id="e485" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的权重向量<strong class="jp ir"> W </strong>是均衡时输入协方差矩阵<strong class="jp ir"> C </strong>的特征向量。此外，该特征向量的范数为1，并且它对应于具有最大特征值的特征向量。该神经网络的权重沿着捕获输入分布中的最大变化量的方向移动，这是第一主成分的属性。</p><p id="ec50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要得到一个以上的组件，有必要考虑扩展<strong class="jp ir">桑德法则</strong>。首先，感知器网络将有多个输出(每个组件一个)。下一步是实现渐进Oja规则，但是以渐进的方式减去每个输出的预测输入。因此，每个权重向量从不同的输入集合中学习，当它们是输出时，减去更多的结构。这种广义Hebbian算法的Python实现如下:</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="bcba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">函数<strong class="jp ir"> update_data </strong>使用Sander规则计算网络权重的Hebbian学习规则。该功能针对2个输出进行校准(在2个维度上减少)。</p><p id="d406" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图显示了这个2输出感知器网络的输出确实充当了PCA。数据集在权重向量上的投影结构非常类似于PCA的前两个分量，并且三个物种的聚类是可清楚区分的。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ml"><img src="../Images/9e0493bc8ca7b0d88db72af85d0d8c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-zwdeSJwLLwmYrVnyy7nQ.png"/></div></div></figure><p id="d6ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于这种神经网络的下一个问题是:这些网络能否执行更高级的方法，如聚类和独立成分分析？因为我们的网络可以学习的统计取决于感知器输出的形状。学习不同的结构需要查看非线性输出函数，从而查看非线性Hebbian算法。</p></div></div>    
</body>
</html>