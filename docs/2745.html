<html>
<head>
<title>Understanding K-means Clustering: Hands-On with SciKit-Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解K-均值聚类:SciKit实践-学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understanding-k-means-clustering-hands-on-with-scikit-learn-b522c0698c81?source=collection_archive---------0-----------------------#2022-05-09">https://pub.towardsai.net/understanding-k-means-clustering-hands-on-with-scikit-learn-b522c0698c81?source=collection_archive---------0-----------------------#2022-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a716" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python和Google Colab</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1035eec8b2847fd993b0b8024fc371cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2jcaGvC5tjyrp3WFnjBjmg.png"/></div></div></figure><p id="7efa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">K-means是一种无监督的机器学习算法，它试图找到能够聚集与任何特征或特征组更接近的某些数据点的聚类中心。这种技术在探索性数据分析中非常有用，尤其是当研究人员不太了解数据和变量行为时。</p><h2 id="dcb2" class="lq lr it bd ls lt lu dn lv lw lx dp ly ld lz ma mb lh mc md me ll mf mg mh mi bi translated">K-Means算法是如何工作的？</h2><p id="c033" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">实现k-means算法有三个主要步骤:</p><p id="4604" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="mo"> 1。设置研究员想要的集群数量</em> </strong></p><p id="ebab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当K均值函数被定义时，聚类的数量必须由研究者选择。有两种选择K值的基本技术，但我将在后面讨论它们。</p><p id="8bc8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="mo"> 2。定义质心(或初始均值)</em> </strong></p><p id="2553" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，该算法选择k个随机点作为质心，并将剩余的数据点分配给最近的质心。在所有的点都被指定之后，一个新的k形心的平均值被计算出来。</p><p id="42f9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="mo"> 3。更新质心平均值并重新分配观测值</em> </strong></p><p id="12bb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当计算新的平均值时，一些数据点可以被重新分配给不同的质心。当对不同集群的分配不再改变时，该算法完成其工作。</p><blockquote class="mp mq mr"><p id="926a" class="ku kv mo kw b kx ky ju kz la lb jx lc ms le lf lg mt li lj lk mu lm ln lo lp im bi translated">让我们做一些实际的工作:</p></blockquote><p id="5210" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可以从Kaggle 下载用于本教程的数据集，但是我强烈建议你尝试对我们自己的数据实现该算法。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="d777" class="lq lr it mx b gy nb nc l nd ne"><strong class="mx iu">#Import libraries:</strong><br/>import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>from sklearn.cluster import KMeans<br/>import matplotlib.pyplot as plt<br/>from sklearn.metrics import silhouette_score<br/>from yellowbrick.cluster import SilhouetteVisualizer</span><span id="c834" class="lq lr it mx b gy nf nc l nd ne"><strong class="mx iu">#Load and read data frame:</strong><br/>df = pd.read_csv('/content/Wine.csv')<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/116ee18470eb6b596c04ee41707220c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iso3fPdxJH9LcjsCkaRuDg.png"/></div></div></figure><p id="6865" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们需要定义K-Means函数，并通过猜测尝试k=3:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="c0e7" class="lq lr it mx b gy nb nc l nd ne"><strong class="mx iu">#Define function:</strong><br/>kmeans = KMeans(n_clusters=3)</span></pre><p id="fb48" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于我们使用的是无监督算法，因此没有必要定义y，但我们仍将数据称为X:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="60fa" class="lq lr it mx b gy nb nc l nd ne"><strong class="mx iu">#Define X as numpy array:</strong><br/>X = np.array(df)</span><span id="cc92" class="lq lr it mx b gy nf nc l nd ne"><strong class="mx iu">#Fit the model:</strong><br/>km = kmeans.fit(X)</span><span id="caed" class="lq lr it mx b gy nf nc l nd ne"><strong class="mx iu">#Print results:</strong><br/>print(kmeans.labels_)</span><span id="9070" class="lq lr it mx b gy nf nc l nd ne"><strong class="mx iu">#Visualise results:</strong><br/>plt.scatter(X[:, 0], X[:, 1], <br/>            c=kmeans.labels_,      <br/>            s=70, cmap='Paired')<br/>plt.scatter(kmeans.cluster_centers_[:, 0],<br/>            kmeans.cluster_centers_[:, 1],<br/>            marker='^', s=100, linewidth=2, <br/>            c=[0, 1, 2])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/4a16d7a8fd0bfa0eb536682e1c2adef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*84sTyEPgiYYqrdbI5X6JcA.png"/></div></figure><blockquote class="mp mq mr"><p id="bb15" class="ku kv mo kw b kx ky ju kz la lb jx lc ms le lf lg mt li lj lk mu lm ln lo lp im bi translated">但是我们如何找到K的最佳值呢？</p></blockquote><p id="3be6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">选择最佳K值对于研究人员来说可能并不明显，因为K-means是一种经常用于数据探索的无监督ML技术，而研究人员对数据集和要素行为知之甚少。有两种主要方法来选择k值:</p><h2 id="5e14" class="lq lr it bd ls lt lu dn lv lw lx dp ly ld lz ma mb lh mc md me ll mf mg mh mi bi translated"><strong class="ak">肘法</strong></h2><p id="a8c1" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">该方法选取K的一系列随机值，然后计算每个聚类中各点之间的平均距离。对于K值为1的情况，平均距离较高，随着K值的增加，平均距离会降低。然而，降低的距离有一个极限，肘形法找到这个极限，并使用更接近的整数作为k。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="16c4" class="lq lr it mx b gy nb nc l nd ne"><strong class="mx iu">#Define variable distortions:</strong><br/>distortions = []</span><span id="6dd2" class="lq lr it mx b gy nf nc l nd ne"><strong class="mx iu">#Set a range of values for K:<br/></strong>K = range(1,10)</span><span id="9f99" class="lq lr it mx b gy nf nc l nd ne"><strong class="mx iu">#Initialise the loop:<br/></strong>for k in K:<br/>    kmeanModel = KMeans(n_clusters=k)<br/>    kmeanModel.fit(X)<br/>    distortions.append(kmeanModel.inertia_)</span><span id="cb4c" class="lq lr it mx b gy nf nc l nd ne"><strong class="mx iu">#Plot the graph:</strong><br/>plt.figure(figsize=(10,5))<br/>plt.plot(K, distortions, 'bx-')<br/>plt.xlabel('K')<br/>plt.ylabel('Distortion')<br/>plt.title('Optimal K value:')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/8251ecd3c63336bb03826abea000e910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*6BeEuqaXk3zvU3TKprPhuw.png"/></div></div></figure><p id="21c4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过图形观察，我们可以看到最佳K值为3，在该值之后，失真的下降并不显著。</p><h2 id="7e0d" class="lq lr it bd ls lt lu dn lv lw lx dp ly ld lz ma mb lh mc md me ll mf mg mh mi bi translated"><strong class="ak">剪影法</strong></h2><p id="d7b6" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">与肘部方法类似，剪影方法首先为K选择一个随机值，然后为质心赋值。当所有的值都被赋值后，剪影方法计算每个观察值与同一质心中所有其他观察值之间的平均距离(<em class="mo"> x </em>)，以及一个观察值与最近聚类中的点之间的平均距离(<em class="mo"> y </em>)。然后我们应用剪影公式:</p><blockquote class="nj"><p id="ab79" class="nk nl it bd nm nn no np nq nr ns lp dk translated"><strong class="ak"> <em class="nt"> SF = (y — x)/Max(x，y) </em> </strong></p></blockquote><p id="9c4e" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">轮廓得分的值从-1到1不等。当SF值接近1时，聚类的理想数量将是。我们可以打印SF值:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="314a" class="lq lr it mx b gy nb nc l nd ne"><strong class="mx iu">#Compute and print SF value while trying different k values:</strong><br/>score = silhouette_score(X, km.labels_, metric='euclidean')<br/>print('Silhouetter Score: %.3f' % score)</span></pre><p id="8dc4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">或者我们可以用不同的k值构建一个图形可视化:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="7be5" class="lq lr it mx b gy nb nc l nd ne"><strong class="mx iu">#Build graphical visualisation for Silhouette Score:</strong><br/>fig, ax = plt.subplots(2, 2, figsize=(16,8))<br/>for i in [2, 3, 4, 5]:<br/>    km = KMeans(n_clusters=i, init='k-means++', n_init=12, <br/>                max_iter=1000, random_state=42)<br/>    q, mod = divmod(i, 2)<br/>    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', <br/>                 ax=ax[q-1][mod])<br/>    visualizer.fit(X)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/7b0db0ec8fa9ad3c4e2c687ffbe98b36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZoeHg73qblJB1z3BBSlYA.png"/></div></div></figure><p id="b737" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过图形可视化，可以看到使用轮廓分数的理想聚类数是K=2(红色虚线)。</p><p id="c3e4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="mo">结论</em> </strong></p><p id="6388" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">K-Means算法有助于发现数据中的模式，尤其是在研究者对数据集知之甚少的情况下。除了构建K均值算法之外，选择理想的K值也很重要。</p><p id="618a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感谢您的阅读！如果你有建议要添加到这个列表中，请告诉我，不要忘记订阅以接收关于我未来出版物的通知。</p><p id="b9f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果:你喜欢这篇文章，别忘了关注我，这样你就能收到所有关于新出版物的更新。</p><p id="a18f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">否则如果:你想了解更多，你可以通过<a class="ae mv" href="https://cdanielaam.medium.com/membership" rel="noopener">我的推荐链接</a>订阅媒体会员。它不会花你更多的钱，但会支付我一杯咖啡。</p><p id="b6f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Else:谢谢！</p></div></div>    
</body>
</html>