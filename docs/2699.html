<html>
<head>
<title>Autoencoder For Anomaly Detection Using Tensorflow Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用张量流Keras进行异常检测的自动编码器</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/autoencoder-for-anomaly-detection-using-tensorflow-keras-7fdfa9f3ad99?source=collection_archive---------0-----------------------#2022-04-24">https://pub.towardsai.net/autoencoder-for-anomaly-detection-using-tensorflow-keras-7fdfa9f3ad99?source=collection_archive---------0-----------------------#2022-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="5e75" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用无监督深度学习模型的异常检测</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e65ceedf0305ef99f49bca8f456640e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iBh0unRMBmlOh75l"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@santesson89?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安德里亚·德·森蒂斯峰</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="d999" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Autoencoder是一种无监督的神经网络模型，它使用重建误差来检测异常或异常值。重建误差是重建数据和输入数据之间的差异。</p><p id="9f56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Autoencoder仅使用正常数据来训练模型，并使用所有数据来进行预测。因此，我们期望异常值具有更高的重建误差，因为它们不同于常规数据。</p><p id="834c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本文中，我们将使用Python <code class="fe lf lg lh li b">Tensorflow</code> <code class="fe lf lg lh li b">Keras</code>库来说明使用自动编码器识别异常值的过程。具体来说，我们将涵盖:</p><ul class=""><li id="f261" class="lj lk it js b jt ju jx jy kb ll kf lm kj ln kn lo lp lq lr bi translated">autoencoder用于异常检测的算法是什么？</li><li id="3c41" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">如何训练一个自动编码器模型？</li><li id="e88d" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">如何设置autoencoder异常检测的阈值？</li><li id="1d24" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">如何评价autoencoder异常检测性能？</li></ul><p id="f7f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">本帖资源:</strong></p><ul class=""><li id="d4ae" class="lj lk it js b jt ju jx jy kb ll kf lm kj ln kn lo lp lq lr bi translated">YouTube<a class="ae le" href="https://www.youtube.com/watch?v=IsLXROuJoEo&amp;list=PLVppujud2yJo0qnXjWVAa8h7fxbFJHtfJ&amp;index=7" rel="noopener ugc nofollow" target="_blank">上的视频教程</a></li><li id="b671" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">Python代码在帖子最后。点击<a class="ae le" href="https://mailchi.mp/0533d92d0b6e/p8t1t9jgqc" rel="noopener ugc nofollow" target="_blank">此处</a>为笔记本。</li><li id="ad97" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">更多关于<a class="ae le" href="https://www.youtube.com/playlist?list=PLVppujud2yJo0qnXjWVAa8h7fxbFJHtfJ" rel="noopener ugc nofollow" target="_blank">异常检测的视频教程</a></li><li id="e66d" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">更多关于<a class="ae le" href="https://medium.com/@AmyGrabNGoInfo/list/imbalanced-classification-and-anomalies-detection-dc908de4382d" rel="noopener">异常检测</a>的博文</li></ul><p id="0e4a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们开始吧！</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="1b40" class="me mf it bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">步骤1:导入库</h1><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="df54" class="ng mf it li b gy nh ni l nj nk"># Synthetic dataset<br/>from sklearn.datasets import make_classification</span><span id="635e" class="ng mf it li b gy nl ni l nj nk"># Data processing<br/>import pandas as pd<br/>import numpy as np<br/>from collections import Counter</span><span id="51b6" class="ng mf it li b gy nl ni l nj nk"># Visualization<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="39ff" class="ng mf it li b gy nl ni l nj nk"># Model and performance<br/>import tensorflow as tf<br/>from tensorflow.keras import layers, losses<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import classification_report</span></pre><h1 id="7621" class="me mf it bd mg mh nm mj mk ml nn mn mo mp no mr ms mt np mv mw mx nq mz na nb bi translated">步骤2:创建带有异常的数据集</h1><p id="91bb" class="pw-post-body-paragraph jq jr it js b jt nr jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj nv kl km kn im bi translated">使用sklearn库中的make_classification，我们创建了两个类，多数类和少数类的比率为0.995:0.005。32个信息特征作为预测因子。我们没有在该数据集中包含任何冗余或重复的要素。</p><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="40de" class="ng mf it li b gy nh ni l nj nk"># Create an imbalanced dataset<br/>X, y = make_classification(n_samples=100000, n_features=32, n_informative=32,<br/>                           n_redundant=0, n_repeated=0, n_classes=2,<br/>                           n_clusters_per_class=1,<br/>                           weights=[0.995, 0.005],<br/>                           class_sep=0.5, random_state=0)</span></pre><h1 id="6add" class="me mf it bd mg mh nm mj mk ml nn mn mo mp no mr ms mt np mv mw mx nq mz na nb bi translated">步骤3:训练测试分割</h1><p id="a409" class="pw-post-body-paragraph jq jr it js b jt nr jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj nv kl km kn im bi translated">在这一步中，我们将数据集分成80%的训练数据和20%的验证数据。random_state确保我们每次都有相同的训练测试分割。random_state的种子号不一定是42，可以是任何数字。</p><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="912b" class="ng mf it li b gy nh ni l nj nk"># Train test split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span><span id="159f" class="ng mf it li b gy nl ni l nj nk"># Check the number of records<br/>print('The number of records in the training dataset is', X_train.shape[0])<br/>print('The number of records in the test dataset is', X_test.shape[0])<br/>print(f"The training dataset has {sorted(Counter(y_train).items())[0][1]} records for the majority class and {sorted(Counter(y_train).items())[1][1]} records for the minority class.")</span></pre><p id="0c39" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练测试拆分为我们提供了80，000条训练数据集记录和20，000条验证数据集记录。在训练数据集中，我们有79，200个数据点来自多数群体，800个数据点来自少数群体。少数类数据点是异常值或异常值。</p><h1 id="ca75" class="me mf it bd mg mh nm mj mk ml nn mn mo mp no mr ms mt np mv mw mx nq mz na nb bi translated">步骤4:用于异常检测的自动编码器算法</h1><p id="3986" class="pw-post-body-paragraph jq jr it js b jt nr jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj nv kl km kn im bi translated">用于异常检测的自动编码器模型有六个步骤。前三步是模型训练，后三步是模型预测。</p><ul class=""><li id="7a35" class="lj lk it js b jt ju jx jy kb ll kf lm kj ln kn lo lp lq lr bi translated">步骤1是编码器步骤。在该步骤中，通过神经网络模型提取基本信息。</li><li id="3686" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">步骤2是解码器步骤。在此步骤中，模型使用提取的信息重建数据。</li><li id="2b24" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">步骤3:重复步骤1和步骤2来调整模型，以最小化输入和重构输出之间的差异，直到我们为训练数据集获得良好的重构结果。</li><li id="c737" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">步骤4:对包含异常值的数据集进行预测。</li><li id="49d5" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">步骤5:通过比较自动编码器模型重建值和实际值之间的差异来设置异常值/异常值的阈值。</li><li id="ca47" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">步骤6:将差异高于阈值的数据点识别为异常值或异常值。</li></ul><h1 id="10d7" class="me mf it bd mg mh nm mj mk ml nn mn mo mp no mr ms mt np mv mw mx nq mz na nb bi translated">步骤5:自动编码器模型训练</h1><p id="a2a6" class="pw-post-body-paragraph jq jr it js b jt nr jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj nv kl km kn im bi translated">autoencoder模型在正常数据集上训练，因此我们必须首先将预期数据与异常数据分开。</p><p id="71aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们创建了输入层、编码器层和解码器层。</p><p id="1e52" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在输入层，我们指定了数据集的形状。因为建模数据集有32个要素，所以这里的形状是32。</p><p id="7c42" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">编码器由3层组成，分别具有16、8和4个神经元。注意，编码器要求神经元的数量随着层的增加而减少。编码器中的最后一层是编码表示的大小，也称为瓶颈。</p><p id="6b4c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">解码器由3层组成，分别具有8、16和32个神经元。与编码器相反，解码器要求神经元的数量随着层数的增加而增加。解码器中的输出层与输入层具有相同的大小。</p><p id="1935" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe lf lg lh li b">relu</code>激活功能用于除解码器输出层之外的每一层。<code class="fe lf lg lh li b">relu</code>是一个比较流行的激活功能，不过你可以试试其他的激活功能，对比一下型号性能。</p><p id="4c8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在定义了输入层、编码器层和解码器层之后，我们创建了autoencoder模型来组合这些层。</p><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="06f6" class="ng mf it li b gy nh ni l nj nk"># Keep only the normal data for the training dataset<br/>X_train_normal = X_train[np.where(y_train == 0)]</span><span id="dea9" class="ng mf it li b gy nl ni l nj nk"># Input layer<br/>input = tf.keras.layers.Input(shape=(32,))</span><span id="aa2d" class="ng mf it li b gy nl ni l nj nk"># Encoder layers<br/>encoder = tf.keras.Sequential([<br/>  layers.Dense(16, activation='relu'),<br/>  layers.Dense(8, activation='relu'),<br/>  layers.Dense(4, activation='relu')])(input)</span><span id="cca5" class="ng mf it li b gy nl ni l nj nk"># Decoder layers<br/>decoder = tf.keras.Sequential([<br/>      layers.Dense(8, activation="relu"),<br/>      layers.Dense(16, activation="relu"),<br/>      layers.Dense(32, activation="sigmoid")])(encoder)</span><span id="f8e7" class="ng mf it li b gy nl ni l nj nk"># Create the autoencoder<br/>autoencoder = tf.keras.Model(inputs=input, outputs=decoder)</span></pre><p id="5d59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在定义了输入层、编码器层和解码器层之后，我们创建了autoencoder模型来组合这些层。</p><p id="8e8c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">创建自动编码器模型后，我们用优化器<code class="fe lf lg lh li b">adam</code>和损失<code class="fe lf lg lh li b">mae</code>(平均绝对误差)编译模型。</p><p id="d024" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在拟合autoencoder模型时，我们可以看到输入和输出数据集是相同的，即只包含正常数据点的数据集。</p><p id="ff93" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">验证数据是包含正常和异常数据点的测试数据集。</p><p id="63f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">20的<code class="fe lf lg lh li b">epochs</code>和64的<code class="fe lf lg lh li b">batch_size</code>意味着模型在每次迭代中使用64个数据点来更新权重，并且模型将遍历整个训练数据集20次。</p><p id="df8b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe lf lg lh li b">shuffle=True</code>将在每个历元之前随机播放数据集。</p><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="c141" class="ng mf it li b gy nh ni l nj nk"># Compile the autoencoder<br/>autoencoder.compile(optimizer='adam', loss='mae')</span><span id="c571" class="ng mf it li b gy nl ni l nj nk"># Fit the autoencoder<br/>history = autoencoder.fit(X_train_normal, X_train_normal, <br/>          epochs=20, <br/>          batch_size=64,<br/>          validation_data=(X_test, X_test),<br/>          shuffle=True)</span></pre><p id="5d05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出</p><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="890c" class="ng mf it li b gy nh ni l nj nk">Epoch 1/20<br/>1238/1238 [==============================] - 2s 1ms/step - loss: 2.5375 - val_loss: 2.5047<br/>Epoch 2/20<br/>1238/1238 [==============================] - 2s 1ms/step - loss: 2.4882 - val_loss: 2.4829<br/>Epoch 3/20<br/>.........<br/>.........<br/>Epoch 19/20<br/>1238/1238 [==============================] - 2s 1ms/step - loss: 2.4568 - val_loss: 2.4602<br/>Epoch 20/20<br/>1238/1238 [==============================] - 2s 1ms/step - loss: 2.4560 - val_loss: 2.4593</span></pre><p id="fd0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此图表显示了模型拟合过程中训练和验证损失的变化。x轴是历元数，y轴是损耗。我们可以看到，训练和验证损失都随着时代的增加而减少。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/00843557e8d7e4bd069718db0306e613.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TcW_EmRzK0laDI_2.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图片由作者提供。</figcaption></figure><h1 id="6c68" class="me mf it bd mg mh nm mj mk ml nn mn mo mp no mr ms mt np mv mw mx nq mz na nb bi translated">步骤6:自动编码器异常检测阈值</h1><p id="642c" class="pw-post-body-paragraph jq jr it js b jt nr jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj nv kl km kn im bi translated">现在我们有了一个自动编码器模型，让我们用它来预测离群值。</p><p id="83fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们使用<code class="fe lf lg lh li b">.predict</code>来获得包含正常数据点和异常值的测试数据集的重建值。</p><p id="f077" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们使用平均绝对误差来计算实际和重建之间的损失值。</p><p id="ea37" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">之后，设置阈值来识别异常值。该阈值可以基于百分位数、标准偏差或其他方法。在本例中，我们使用98%的损失作为阈值，将2%的数据识别为异常值。</p><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="c10f" class="ng mf it li b gy nh ni l nj nk"># Predict anomalies/outliers in the training dataset<br/>prediction = autoencoder.predict(X_test)</span><span id="6afa" class="ng mf it li b gy nl ni l nj nk"># Get the mean absolute error between actual and reconstruction/prediction<br/>prediction_loss = tf.keras.losses.mae(prediction, X_test)</span><span id="cd55" class="ng mf it li b gy nl ni l nj nk"># Check the prediction loss threshold for 2% of outliers<br/>loss_threshold = np.percentile(prediction_loss, 98)<br/>print(f'The prediction loss threshold for 2% of outliers is {loss_threshold:.2f}')</span><span id="88e6" class="ng mf it li b gy nl ni l nj nk"># Visualize the threshold<br/>sns.histplot(prediction_loss, bins=30, alpha=0.8)<br/>plt.axvline(x=loss_threshold, color='orange')</span></pre><p id="93d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可视化图表显示，预测损失接近于平均值为2.5左右的正态分布。2%异常值的预测损失阈值约为3.5。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/b3a5759a25f8c431ad75ab2fd395316f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9txa1uf_Fa-2cPIa.png"/></div></div></figure><h1 id="9a20" class="me mf it bd mg mh nm mj mk ml nn mn mo mp no mr ms mt np mv mw mx nq mz na nb bi translated">步骤7:自动编码器异常检测性能</h1><p id="1d11" class="pw-post-body-paragraph jq jr it js b jt nr jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj nv kl km kn im bi translated">有时数据集有异常的基本事实标签，而数据集通常没有。当有异常的标签时，我们可以评估模型的性能。</p><p id="5c38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据我们在上一步中确定的阈值，如果预测损失小于阈值，我们预测正常数据点。否则，我们预测该数据点为异常值或异常值。我们将正常预测值0和异常值预测值1标记为与基本事实标签一致。</p><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="5fee" class="ng mf it li b gy nh ni l nj nk"># Check the model performance at 2% threshold<br/>threshold_prediction = [0 if i &lt; loss_threshold else 1 for i in prediction_loss]</span><span id="acb0" class="ng mf it li b gy nl ni l nj nk"># # Check the prediction performance<br/>print(classification_report(y_test, threshold_prediction))</span></pre><p id="6fa1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">0.01的召回值表明大约1%的异常值被自动编码器捕获。</p><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="8eb9" class="ng mf it li b gy nh ni l nj nk">precision    recall  f1-score   support</span><span id="def8" class="ng mf it li b gy nl ni l nj nk">           0       0.99      0.98      0.98     19803<br/>           1       0.01      0.01      0.01       197</span><span id="d954" class="ng mf it li b gy nl ni l nj nk">    accuracy                           0.97     20000<br/>   macro avg       0.50      0.50      0.50     20000<br/>weighted avg       0.98      0.97      0.98     20000</span></pre><h1 id="6804" class="me mf it bd mg mh nm mj mk ml nn mn mo mp no mr ms mt np mv mw mx nq mz na nb bi translated">步骤8:将所有代码放在一起</h1><pre class="kp kq kr ks gt nc li nd ne aw nf bi"><span id="2830" class="ng mf it li b gy nh ni l nj nk">###### Step 1: Import Libraries</span><span id="6e39" class="ng mf it li b gy nl ni l nj nk"># Synthetic dataset<br/>from sklearn.datasets import make_classification</span><span id="27d7" class="ng mf it li b gy nl ni l nj nk"># Data processing<br/>import pandas as pd<br/>import numpy as np<br/>from collections import Counter</span><span id="984a" class="ng mf it li b gy nl ni l nj nk"># Visualization<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="5868" class="ng mf it li b gy nl ni l nj nk"># Model and performance<br/>import tensorflow as tf<br/>from tensorflow.keras import layers, losses<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import classification_report<br/></span><span id="b19b" class="ng mf it li b gy nl ni l nj nk">###### Step 2: Create Dataset With Anomalies</span><span id="96b3" class="ng mf it li b gy nl ni l nj nk"># Create an imbalanced dataset<br/>X, y = make_classification(n_samples=100000, n_features=32, n_informative=32,<br/>                           n_redundant=0, n_repeated=0, n_classes=2,<br/>                           n_clusters_per_class=1,<br/>                           weights=[0.995, 0.005],<br/>                           class_sep=0.5, random_state=0)<br/></span><span id="bf78" class="ng mf it li b gy nl ni l nj nk">###### Step 3: Train Test Split</span><span id="9cd1" class="ng mf it li b gy nl ni l nj nk"># Train test split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span><span id="10ef" class="ng mf it li b gy nl ni l nj nk"># Check the number of records<br/>print('The number of records in the training dataset is', X_train.shape[0])<br/>print('The number of records in the test dataset is', X_test.shape[0])<br/>print(f"The training dataset has {sorted(Counter(y_train).items())[0][1]} records for the majority class and {sorted(Counter(y_train).items())[1][1]} records for the minority class.")<br/></span><span id="cf21" class="ng mf it li b gy nl ni l nj nk">###### Step 4: Autoencoder Algorithm For Anomaly Detection</span><span id="53fc" class="ng mf it li b gy nl ni l nj nk"># No code for this step<br/></span><span id="c7c9" class="ng mf it li b gy nl ni l nj nk">###### Step 5: Autoencoder Model Training</span><span id="2185" class="ng mf it li b gy nl ni l nj nk"># Keep only the normal data for the training dataset<br/>X_train_normal = X_train[np.where(y_train == 0)]</span><span id="be23" class="ng mf it li b gy nl ni l nj nk"># Input layer<br/>input = tf.keras.layers.Input(shape=(32,))</span><span id="c5e3" class="ng mf it li b gy nl ni l nj nk"># Encoder layers<br/>encoder = tf.keras.Sequential([<br/>  layers.Dense(16, activation='relu'),<br/>  layers.Dense(8, activation='relu'),<br/>  layers.Dense(4, activation='relu')])(input)</span><span id="5874" class="ng mf it li b gy nl ni l nj nk"># Decoder layers<br/>decoder = tf.keras.Sequential([<br/>      layers.Dense(8, activation="relu"),<br/>      layers.Dense(16, activation="relu"),<br/>      layers.Dense(32, activation="sigmoid")])(encoder)</span><span id="879c" class="ng mf it li b gy nl ni l nj nk"># Create the autoencoder<br/>autoencoder = tf.keras.Model(inputs=input, outputs=decoder)<br/></span><span id="f743" class="ng mf it li b gy nl ni l nj nk">###### Step 6: Autoencoder Anomaly Detection Threshold</span><span id="e93a" class="ng mf it li b gy nl ni l nj nk"># Predict anomalies/outliers in the training dataset<br/>prediction = autoencoder.predict(X_test)</span><span id="7f8d" class="ng mf it li b gy nl ni l nj nk"># Get the mean absolute error between actual and reconstruction/prediction<br/>prediction_loss = tf.keras.losses.mae(prediction, X_test)</span><span id="8bf5" class="ng mf it li b gy nl ni l nj nk"># Check the prediction loss threshold for 2% of outliers<br/>loss_threshold = np.percentile(prediction_loss, 98)<br/>print(f'The prediction loss threshold for 2% of outliers is {loss_threshold:.2f}')</span><span id="d3b5" class="ng mf it li b gy nl ni l nj nk"># Visualize the threshold<br/>sns.histplot(prediction_loss, bins=30, alpha=0.8)<br/>plt.axvline(x=loss_threshold, color='orange')<br/></span><span id="b444" class="ng mf it li b gy nl ni l nj nk">###### Step 7: Autoencoder Anomaly Dectection Performance</span><span id="1b3d" class="ng mf it li b gy nl ni l nj nk"># Check the model performance at 2% threshold<br/>threshold_prediction = [0 if i &lt; loss_threshold else 1 for i in prediction_loss]</span><span id="aa17" class="ng mf it li b gy nl ni l nj nk"># # Check the prediction performance<br/>print(classification_report(y_test, threshold_prediction))</span></pre></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="b11e" class="me mf it bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">摘要</h1><p id="f85f" class="pw-post-body-paragraph jq jr it js b jt nr jv jw jx ns jz ka kb nt kd ke kf nu kh ki kj nv kl km kn im bi translated">在本文中，我们通过自动编码器神经网络模型进行异常检测。</p><p id="3e78" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用Python中的Tensorflow Keras API，我们介绍了:</p><ul class=""><li id="8ec7" class="lj lk it js b jt ju jx jy kb ll kf lm kj ln kn lo lp lq lr bi translated">什么是自动编码器？</li><li id="e941" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">autoencoder用于异常检测的算法是什么？</li><li id="fbdb" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">如何训练一个自动编码器模型？</li><li id="848e" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">如何设置autoencoder异常检测的阈值？</li><li id="1a6b" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">如何评价autoencoder异常检测性能？</li></ul><p id="93d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更多教程可在GrabNGoInfo.com的GrabNGoInfo <a class="ae le" href="https://www.youtube.com/channel/UCmbA7XB6Wb7bLwJw9ARPcYg" rel="noopener ugc nofollow" target="_blank"> YouTube频道</a>和<a class="ae le" href="https://grabngoinfo.com/tutorials/" rel="noopener ugc nofollow" target="_blank">获得。</a></p><h1 id="d5cd" class="me mf it bd mg mh nm mj mk ml nn mn mo mp no mr ms mt np mv mw mx nq mz na nb bi translated">推荐教程</h1><ul class=""><li id="aa7c" class="lj lk it js b jt nr jx ns kb ny kf nz kj oa kn lo lp lq lr bi translated"><a class="ae le" href="https://medium.com/grabngoinfo/grabngoinfo-machine-learning-tutorials-inventory-9b9d78ebdd67" rel="noopener"> GrabNGoInfo机器学习教程盘点</a></li><li id="e74c" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated"><a class="ae le" href="https://medium.com/p/one-class-svm-for-anomaly-detection-6c97fdd6d8af" rel="noopener">用于异常检测的单级SVM</a></li><li id="6179" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated"><a class="ae le" href="https://medium.com/p/3-ways-for-multiple-time-series-forecasting-using-prophet-in-python-7a0709a117f9" rel="noopener">使用Python中的Prophet进行多时间序列预测的3种方式</a></li><li id="992c" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated"><a class="ae le" href="https://medium.com/p/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python-7304aedf9037" rel="noopener">使用Python实现不平衡分类的四种过采样和欠采样方法</a></li><li id="3b3e" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated"><a class="ae le" href="https://medium.com/p/multivariate-time-series-forecasting-with-seasonality-and-holiday-effect-using-prophet-in-python-d5d4150eeb57" rel="noopener">利用Python中的Prophet进行具有季节性和假日效应的多元时间序列预测</a></li><li id="4e02" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated"><a class="ae le" href="https://medium.com/p/how-to-detect-outliers-data-science-interview-questions-and-answers-1e400284f6b4" rel="noopener">如何检测离群值|数据科学面试问答</a></li><li id="d14f" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated"><a class="ae le" href="https://medium.com/p/time-series-anomaly-detection-using-prophet-in-python-877d2b7b14b4" rel="noopener">利用Python中的Prophet进行时间序列异常检测</a></li><li id="1668" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated"><a class="ae le" href="https://medium.com/p/how-to-use-r-with-google-colab-notebook-610c3a2f0eab" rel="noopener">如何配合谷歌Colab笔记本使用R</a></li></ul><h1 id="4b7d" class="me mf it bd mg mh nm mj mk ml nn mn mo mp no mr ms mt np mv mw mx nq mz na nb bi translated">参考</h1><ul class=""><li id="8654" class="lj lk it js b jt nr jx ns kb ny kf nz kj oa kn lo lp lq lr bi translated">[1]张量流。2022.<em class="ob">自动编码器介绍| TensorFlow核心</em>。[在线]可在:&lt;https://www.tensorflow.org/tutorials/generative/autoencoder&gt;[2022年4月23日访问]。</li><li id="c2c6" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">[2]异常图。2022.<em class="ob"> Anomagram:自动编码器的交互式可视化</em>。[在线]见:&lt;https://anomagram.fastforwardlabs.com/#/&gt;[2022年4月23日访问]。</li><li id="e3ec" class="lj lk it js b jt ls jx lt kb lu kf lv kj lw kn lo lp lq lr bi translated">[3] Blog.keras.io. 2022。<em class="ob">在Keras中构建自动编码器</em>。[在线]见:&lt;https://blog.keras.io/building-autoencoders-in-keras.html&gt;[2022年4月23日访问]。</li></ul><div class="oc od gp gr oe of"><a href="https://medium.com/@AmyGrabNGoInfo/membership" rel="noopener follow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">通过我的推荐链接加入媒体-艾米GrabNGoInfo</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">medium.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ky of"/></div></div></a></div></div></div>    
</body>
</html>