<html>
<head>
<title>Introduction To Pooling Layers In CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CNN中的池层介绍</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/introduction-to-pooling-layers-in-cnn-dafe61eabe34?source=collection_archive---------0-----------------------#2022-08-12">https://pub.towardsai.net/introduction-to-pooling-layers-in-cnn-dafe61eabe34?source=collection_archive---------0-----------------------#2022-08-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8a78" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">卷积神经网络(CNN)是一种特殊类型的人工神经网络，由于其能够识别图像中的模式，因此通常用于图像识别和处理。它消除了从视觉数据中手动提取特征的需要。它通过在图像上滑动一定大小的过滤器来学习图像，不仅从数据中学习特征，还保持平移不变性。</p><p id="204c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CNN的典型结构包括三个基本层</p><ol class=""><li id="dd68" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">卷积层:</strong>这些层<strong class="jp ir">通过在输入图像上滑动过滤器并识别图像中的模式来生成特征图</strong>。</li><li id="249e" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">池层:</strong>这些层<strong class="jp ir">对特征图</strong>进行下采样以引入平移不变性，这减少了CNN模型的过拟合。</li><li id="be11" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">全连接密集层:</strong>该层包含与类别数量相同的<strong class="jp ir">个单元，以及输出激活函数，如“softmax”或“sigmoid”</strong></li></ol><h2 id="28d3" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">什么是池层？</h2><p id="91fa" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">池层是卷积神经网络的构造块之一。卷积层<strong class="jp ir">从图像中提取特征</strong>，汇集层<strong class="jp ir">巩固CNN学习的特征</strong>。其目的是逐渐缩小表示的空间维度，以最小化网络中的参数和计算的数量。</p><h2 id="093e" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">为什么需要池化图层？</h2><p id="a202" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">由卷积层的滤波器产生的特征图是位置相关的。例如，如果图像中的对象移动了一点，卷积层可能无法识别它。因此，这意味着要素地图记录了要素在输入中的精确位置。池层提供的是“平移不变性”，这使得CNN对于平移不变，即，即使CNN的输入被平移，CNN仍将能够识别输入中的特征。</p><blockquote class="lx ly lz"><p id="363c" class="jn jo ma jp b jq jr js jt ju jv jw jx mb jz ka kb mc kd ke kf md kh ki kj kk ij bi translated">在所有情况下，池有助于使表示变得对输入的小平移近似不变。对平移的不变性意味着，如果我们对输入进行少量的平移，大部分汇集输出的值不会改变—第342页，深度学习，Ian Goodfellow，2016。</p></blockquote><p id="67c1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">池层如何实现这一点？如上面CNN的结构所示，在卷积层之后添加了一个汇集层。它通过以某个步长滑动某个大小的滤波器并计算输入的最大值或平均值来对卷积层的输出进行下采样。</p><p id="3e30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有两种类型的池<strong class="jp ir">被使用:</strong></p><ol class=""><li id="30f7" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jp ir">最大池化</strong>:从每个池中选择最大值。Max Pooling保留了特征图的<strong class="jp ir">最突出的</strong>特征，返回的图像比原始图像更清晰。</li><li id="8f24" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated"><strong class="jp ir">平均池</strong>:这个池层通过获取池的平均值来工作。平均池保留特征地图的特征的平均值。它平滑图像，同时保持图像中特征的本质。</li></ol><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi me"><img src="../Images/1d1618dcebf36660ed78491e3f1b26ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*ypIfJX7iWX6h6Kbkfq85Kg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><a class="ae mq" href="https://www.researchgate.net/figure/Toy-example-illustrating-the-drawbacks-of-max-pooling-and-average-pooling_fig2_300020038" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><p id="5c35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来探索使用TensorFlow合并图层的工作方式。创建一个NumPy数组并重塑它。</p><pre class="mf mg mh mi gt mr ms mt mu aw mv bi"><span id="b4e4" class="kz la iq ms b gy mw mx l my mz">matrix=np.array([[3.,2.,0.,0.],<br/>                [0.,7.,1.,3.],<br/>                [5.,2.,3.,0.],<br/>                [0.,9.,2.,3.]]).reshape(1,4,4,1)</span></pre><h2 id="5830" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated"><strong class="ak">最大池化</strong></h2><p id="29c3" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">使用pool_size=2和strides=2创建MaxPool2D层。将MaxPool2D图层应用到矩阵，您将获得张量形式的MaxPooled输出。通过将其应用于矩阵，最大池层将通过计算每个2x2池的最大值(跳跃2)来遍历矩阵。打印张量的形状。使用tf.squeeze从张量形状中移除大小为1的维度。</p><pre class="mf mg mh mi gt mr ms mt mu aw mv bi"><span id="d6ce" class="kz la iq ms b gy mw mx l my mz">max_pooling=tf.keras.layers.MaxPool2D(pool_size=2,strides=2)<br/>max_pooled_matrix=max_pooling(matrix)<br/>print(max_pooled_matrix.shape)<br/>print(tf.squeeze(max_pooled_matrix))</span></pre><h2 id="a63d" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">平均池</h2><p id="aa7c" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">用同样的2 pool_size和步幅创建一个AveragePooling2D层。将AveragePooling2D图层应用于矩阵。通过将其应用于矩阵，平均池层将通过计算每个池的2x2平均值(跳跃为2)来遍历矩阵。打印矩阵的形状，并使用tf.squeeze通过删除所有1 size维将输出转换为可读形式。</p><pre class="mf mg mh mi gt mr ms mt mu aw mv bi"><span id="c6d1" class="kz la iq ms b gy mw mx l my mz">average_pooling=tf.keras.layers.AveragePooling2D(pool_size=2,<br/>                                                 strides=2)<br/>average_pooled_matrix=average_pooling(matrix)<br/>print(averge_pooled_matrix.shape)<br/>print(tf.squeeze(average_pooled_matrix))</span></pre><p id="8d71" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的GIF解释了这些池层如何通过输入矩阵，并分别计算最大池和平均池的最大值或平均值。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/074c6880887135c804d8c9d772a9ca3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*fXxDBsJ96FKEtMOa9vNgjA.gif"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">正在执行的最大池化和平均池化— <a class="ae mq" href="https://towardsdatascience.com/convolutional-neural-networks-explained-how-to-successfully-classify-images-in-python-df829d4ba761" rel="noopener" target="_blank">来源</a></figcaption></figure><h2 id="5c4b" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">全局池层</h2><p id="23d5" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">全局池层通常会替换分类器的完全连接或展平层。相反，该模型以一个卷积层结束，该卷积层生成与目标类一样多的要素地图，并对每个要素地图执行全局平均汇集，以将每个要素地图组合成一个值。</p><p id="c7df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建相同的NumPy数组，但形状不同。通过保持与上面相同的形状，全局池层将把它们减少到一个值。</p><pre class="mf mg mh mi gt mr ms mt mu aw mv bi"><span id="c1ea" class="kz la iq ms b gy mw mx l my mz">matrix=np.array([[[3.,2.,0.,0.],<br/>                [0.,7.,1.,3.]],<br/>                [[5.,2.,3.,0.],<br/>                [0.,9.,2.,3.]]]).reshape(1,2,2,4)</span></pre><p id="8034" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">全球平均池</strong></p><p id="cdf2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑形状为<strong class="jp ir"> h*w*n </strong>的张量，全局平均池层的输出是跨越<strong class="jp ir"> h*w </strong>的单个值，该值概括了特征的存在。全局平均池图层不是缩小输入要素地图的面片，而是通过取平均值将整个<strong class="jp ir"> h*w </strong>缩小为1个值。</p><pre class="mf mg mh mi gt mr ms mt mu aw mv bi"><span id="fe98" class="kz la iq ms b gy mw mx l my mz">global_average_pooling=tf.keras.layers.GlobalAveragePooling2D()<br/>global_average_pooled_matrix=global_average_pooling(matrix)<br/>print(global_average_pooled_matrix)</span></pre><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/11b76e76d0fc0a6fbd5c72c774daab21.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*NO7O1Vcd53DPnWzItcROIw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">GlobalAveragePooled图层的输出</figcaption></figure><p id="a31a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">全局最大池</strong></p><p id="cc6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用形状张量<strong class="jp ir"> h*w*n </strong>，全局最大池图层的输出是跨越<strong class="jp ir"> h*w </strong>的单个值，该值概括了要素的存在。全局最大池图层通过取最大值将整个<strong class="jp ir"> h*w </strong>缩小为1个值，而不是缩小输入要素地图的面片。</p><pre class="mf mg mh mi gt mr ms mt mu aw mv bi"><span id="f14b" class="kz la iq ms b gy mw mx l my mz">global_max_pooling=tf.keras.layers.GlobalMaxPool2D()<br/>global_max_pooled_matrix=global_max_pooling(matrix)<br/>print(global_max_pooled_matrix)</span></pre><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/da222a2f71efb7b0a2b2c1c1e5a3a307.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*AQHYAt4TEWb7pMgeNFr2aQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated">GlobalMaxPooled图层的输出</figcaption></figure><h2 id="ba87" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">结论</h2><p id="4552" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">通常，当您想要检测图像中的对象，而不管其在图像中的位置如何时，池化层非常有用。添加池层的结果是减少了过度拟合，提高了效率，并加快了CNN模型中的训练时间。当最大池层提取出图像最突出的特征时，平均池会平滑图像，保留其特征的本质。全局池图层通常会替换平坦或密集输出图层。</p><p id="f29e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">阅读<a class="ae mq" href="https://keras.io/api/layers/pooling_layers/" rel="noopener ugc nofollow" target="_blank"> Keras Pooling layers API </a>和Franç ois Chollet的《用Python进行深度学习》第5章，了解详细信息。此外，查看<a class="ae mq" href="https://poloclub.github.io/cnn-explainer/" rel="noopener ugc nofollow" target="_blank"> CNN解说</a>获取CNN模型的直观解释。</p></div></div>    
</body>
</html>