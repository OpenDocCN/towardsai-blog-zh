<html>
<head>
<title>NLP using Deep Learning Tutorials: Understand Loss Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习教程的NLP:理解损失函数</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/nlp-using-deep-learning-tutorials-understand-loss-function-2aaf73ec6c2b?source=collection_archive---------2-----------------------#2021-03-03">https://pub.towardsai.net/nlp-using-deep-learning-tutorials-understand-loss-function-2aaf73ec6c2b?source=collection_archive---------2-----------------------#2021-03-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7f01" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/f9c59b994e361404e3a7f781bad00a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VZ3_McssPpH4y9vQUEiGew.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">版权所有Timur Garipov等人的NeurIPS 2018年论文，ARXIV 1802.10026，losslandscape.com</figcaption></figure><blockquote class="ko kp kq"><p id="2f41" class="kr ks kt ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="it">这篇文章是我正在撰写的系列文章的一部分，在这里我将尝试解决在NLP中使用深度学习的主题。首先，我正在写一篇关于使用感知器进行文本分类的例子的文章，但我认为最好先复习一些基础知识，如激活和损失函数。</em></p></blockquote><p id="bb7d" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">损失函数也称为目标函数，是基于标记数据的监督机器学习算法的主要组成部分之一。损失函数指导训练算法以正确的方式更新参数。在一个非常简单的定义中，损失函数将真实值<strong class="ku jd"> (y) </strong>和预测值<strong class="ku jd"> (ŷ) </strong>作为输入，并给出实数值的分数。该值表示预测与事实的接近程度。该值越高，模型的预测越差，反之亦然。</p><p id="e35f" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">在这篇文章中，我提出了三个最常用的损失函数。</p><h1 id="b5d4" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">1.均方误差损失函数</h1><p id="5b40" class="pw-post-body-paragraph kr ks it ku b kv mr kx ky kz ms lb lc lq mt lf lg lr mu lj lk ls mv ln lo lp im bi translated">均方误差损失函数，也称为MSE，最常用于具有连续目标(y)值和预测(ŷ)值的回归问题。MSE是目标值和预测值之差的平方的平均值。MSE还有其他替代方法，如平均绝对误差(MAE)或均方根误差(r MSE)，但所有这些函数都是基于计算目标和预测(输出)之间的实值距离。</p><p id="41fa" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">MSE的数学公式:</p><figure class="mx my mz na gt kd gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/e5020ed3eaabc02179776aeb2be08d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*wuRwKp9E_4DKN3L1HAVLzA.png"/></div></figure><p id="6de4" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">以及一个使用Pytorch实现的示例:</p><pre class="mx my mz na gt nb nc nd ne aw nf bi"><span id="9b0a" class="ng lu it nc b gy nh ni l nj nk">import torch<br/>import torch.nn as nn<br/><br/><em class="kt"># Mean Squared Error Loss<br/></em>mse_loss = nn.MSELoss()<br/>outputs = torch.randn(3, 5, requires_grad=True)<br/>targets = torch.randn(3, 5)<br/>loss = mse_loss(outputs, targets)<br/>print(<strong class="nc jd">f'Mean Squared Erro loss : </strong>{loss}<strong class="nc jd">'</strong>)</span><span id="e9ba" class="ng lu it nc b gy nl ni l nj nk"># Output <br/># Mean Squared Erro loss : 3.128143787384033</span></pre><h1 id="e336" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">2.分类交叉熵损失函数</h1><p id="656a" class="pw-post-body-paragraph kr ks it ku b kv mr kx ky kz ms lb lc lq mt lf lg lr mu lj lk ls mv ln lo lp im bi translated">类别交叉熵损失函数通常用于多类分类，其输出(ŷ)是目标类的概率。目标真值(y)是代表真实多项式分布的n个元素的向量。这需要(y)值的两个性质:所有元素之和等于1，所有元素都是正的。如果一个类是正确的，则(y)向量是一个热点向量。预测输出(ŷ)与(y)具有相同的属性。</p><p id="3555" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">交叉熵损失的数学公式为:</p><figure class="mx my mz na gt kd gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/533d44900a882be10056547d605fcffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*zZ1a4hcXMmKy9VdjhayN5A.png"/></div></figure><p id="2c12" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">为了更好地使用交叉熵损失，您需要理解三个数学方面:</p><ul class=""><li id="e3fd" class="nn no it ku b kv kw kz la lq np lr nq ls nr lp ns nt nu nv bi translated">一个数字的大小是有限度的。为了避免这种情况，您可以在输出和/或输入中添加一个“定标函数”。(例如:sk learn . preprocessing . standard scaler)</li><li id="37ae" class="nn no it ku b kv nw kz nx lq ny lr nz ls oa lp ns nt nu nv bi translated">如果softmax公式中使用的指数函数的输入是负数，则结果是一个指数小数，如果是正数，则结果是一个指数大数。</li><li id="c862" class="nn no it ku b kv nw kz nx lq ny lr nz ls oa lp ns nt nu nv bi translated">而对数函数是指数函数的反函数，也就是说log(exp(x))等于x。</li></ul><p id="6134" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">因此，为了使用交叉熵损失获得优化的概率分布，在网络的训练阶段，您需要避免使用softmax函数。然后，当模型定型时，您可以使用softmax函数来获得预测概率。</p><p id="f257" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">最后，这是一个使用Pytorch实现交叉熵损失的示例:</p><pre class="mx my mz na gt nb nc nd ne aw nf bi"><span id="0564" class="ng lu it nc b gy nh ni l nj nk">import torch<br/>import torch.nn as nn<br/><br/><em class="kt"># Cross-entropy Loss<br/></em>ce_loss = nn.CrossEntropyLoss()<br/>outputs = torch.randn(3, 5, requires_grad=True)<br/>targets = torch.tensor([1, 0, 3], dtype=torch.int64)<br/>loss = ce_loss(outputs, targets)<br/>print(<strong class="nc jd">f'Cross Entropy Loss : </strong>{loss}<strong class="nc jd">'</strong>)</span><span id="a108" class="ng lu it nc b gy nl ni l nj nk"># OutPut :<br/># Cross Entropy Loss : 1.7309303283691406</span></pre><p id="d5bd" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">在这个例子中，我们假设每个输入都有一个特定的类。这就是为什么targets向量有三个整数元素，代表每个输入的正确类的索引。</p><h1 id="ec36" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">3.二元交叉熵损失函数</h1><p id="0286" class="pw-post-body-paragraph kr ks it ku b kv mr kx ky kz ms lb lc lq mt lf lg lr mu lj lk ls mv ln lo lp im bi translated">二元交叉熵损失函数用于涉及区分两个类别的分类问题，称为二元分类。</p><p id="5751" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">数学公式是:</p><figure class="mx my mz na gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ob"><img src="../Images/743adbfe28acb805856dea6b5d3bdc5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OK6rDQgULPOh2UdKWnigvQ.png"/></div></div></figure><p id="1014" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">下面是一个使用Pytorch的实现示例:</p><pre class="mx my mz na gt nb nc nd ne aw nf bi"><span id="62b1" class="ng lu it nc b gy nh ni l nj nk">import torch<br/>import torch.nn as nn<br/><br/><em class="kt"># Binary Cross-Entropy Loss<br/></em>bce_loss = nn.BCELoss()<br/>sigmoid = nn.Sigmoid()<br/>probabilities = sigmoid(torch.randn(4, 1, requires_grad=True))<br/>targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32).view(4, 1)<br/>loss = bce_loss(probabilities, targets)<br/>print(<strong class="nc jd">f'This is probabilities : </strong>{probabilities}<strong class="nc jd">'</strong>)<br/>print(<strong class="nc jd">f'bce loss : </strong>{loss}<strong class="nc jd">'</strong>)</span><span id="14d9" class="ng lu it nc b gy nl ni l nj nk"># Output <br/># This is probabilities : tensor([[0.8276],<br/>#        [0.4056],<br/>#        [0.4190],<br/>#        [0.5984]], grad_fn=&lt;SigmoidBackward&gt;)<br/># bce loss : 0.6229268312454224</span></pre><p id="bee0" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated">在下面的例子中，我们使用激活函数sigmoid创建了一个二元概率输出向量“probabilities”。接下来，我们实例化0和1的目标向量，它们代表两个目标类的索引。最后，我们使用概率和目标这两个变量，使用二元交叉熵函数来计算损失值。</p><h1 id="50c8" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">结论</h1><p id="a320" class="pw-post-body-paragraph kr ks it ku b kv mr kx ky kz ms lb lc lq mt lf lg lr mu lj lk ls mv ln lo lp im bi translated">在这篇文章中，我提出了三个损失函数。注意，Pytorch在它的nn包中实现了更多的损失函数，你可以在这个链接中找到。<a class="ae oc" href="https://pytorch.org/docs/stable/nn.html#loss-functions" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/nn.html#loss-functions</a><br/>每个损失函数都是针对某些情况推荐的。然而，如果可能的话，你必须毫不犹豫地在不同的情况下尝试其他的损失函数。</p><p id="5c47" class="pw-post-body-paragraph kr ks it ku b kv kw kx ky kz la lb lc lq le lf lg lr li lj lk ls lm ln lo lp im bi translated"><strong class="ku jd">推荐人:</strong></p><ol class=""><li id="5067" class="nn no it ku b kv kw kz la lq np lr nq ls nr lp od nt nu nv bi translated">《用Pytorch进行自然语言处理》一书(<a class="ae oc" href="https://www.amazon.fr/Natural-Language-Processing-Pytorch-Applications/dp/1491978236" rel="noopener ugc nofollow" target="_blank">https://www . Amazon . fr/Natural-Language-Processing-py torch-Applications/DP/1491978236</a>)</li></ol></div></div>    
</body>
</html>