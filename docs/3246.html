<html>
<head>
<title>The Gradient Descent Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降算法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-gradient-descent-algorithm-defddd1d312e?source=collection_archive---------0-----------------------#2022-10-24">https://pub.towardsai.net/the-gradient-descent-algorithm-defddd1d312e?source=collection_archive---------0-----------------------#2022-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/78aff1dec16f71db3ce1e77e4a8338bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FeBV-53fx-E6r_p_dTs_Tw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://pixabay.com/users/cocoparisienne-127419/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2100050" rel="noopener ugc nofollow" target="_blank"> Anja </a>来自<a class="ae jd" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2100050" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><div class=""/><div class=""><h2 id="8a68" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">梯度下降算法的内容、原因和方法</h2></div><p id="50d3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">作者:</strong> <a class="ae jd" href="https://www.linkedin.com/in/pratik-shukla28/" rel="noopener ugc nofollow" target="_blank">普拉蒂克·舒克拉</a></p><blockquote class="lr"><p id="a024" class="ls lt jg bd lu lv lw lx ly lz ma lq dk translated">“无聊的解药是好奇心。好奇心是无法治愈的。”— <a class="ae jd" href="https://en.wikipedia.org/wiki/Dorothy_Parker" rel="noopener ugc nofollow" target="_blank">多萝西·帕克</a></p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="dc1b" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">梯度下降系列博客:</h2><ol class=""><li id="46be" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated"><a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/the-gradient-descent-algorithm-defddd1d312e">梯度下降算法</a>(你来了！)</li><li id="8aa3" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/mathematical-intuition-behind-the-gradient-descent-algorithm-143a051c3fa9">梯度下降算法背后的数学直觉</a></li><li id="10ef" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" rel="noopener ugc nofollow" target="_blank" href="/the-gradient-descent-algorithm-and-its-variants-e0915796dbf2">梯度下降算法&amp;及其变种</a></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="f6d5" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">目录:</h2><ol class=""><li id="216a" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated"><a class="ae jd" href="#67a3" rel="noopener ugc nofollow">梯度下降系列的动机</a></li><li id="7c28" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#3464" rel="noopener ugc nofollow">什么是梯度下降算法？</a></li><li id="5d1f" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#d9ed" rel="noopener ugc nofollow">梯度下降算法背后的直觉</a></li><li id="0dbc" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#15cf" rel="noopener ugc nofollow">为什么我们需要梯度下降算法？</a></li><li id="0f24" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#d4b3" rel="noopener ugc nofollow">梯度下降算法是如何工作的？</a></li><li id="0a6f" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#d596" rel="noopener ugc nofollow">梯度下降算法的公式</a></li><li id="52de" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">为什么我们要使用渐变？</li><li id="4396" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#bf11" rel="noopener ugc nofollow">方向衍生工具简介</a></li><li id="9899" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">最陡的上坡方向是什么？</li><li id="8d41" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#dea5" rel="noopener ugc nofollow">证明最陡上坡方向的例子</a></li><li id="d631" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#a8f7" rel="noopener ugc nofollow">梯度下降算法中(—)符号的解释</a></li><li id="83ec" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#bd08" rel="noopener ugc nofollow">为什么学习率？</a></li><li id="a2cd" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#28f3" rel="noopener ugc nofollow">微分的一些基本规则</a></li><li id="162d" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#0a99" rel="noopener ugc nofollow">单变量梯度下降算法</a></li><li id="2d06" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#640b" rel="noopener ugc nofollow">两变量梯度下降算法</a></li><li id="36e9" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#4e1d" rel="noopener ugc nofollow">结论</a></li><li id="191d" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated"><a class="ae jd" href="#8853" rel="noopener ugc nofollow">参考资料和资源</a></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="67a3" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">梯度下降系列的动机:</h2><p id="19d8" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们很高兴介绍我们的第一个关于机器学习算法的博客系列！我们希望让我们的读者了解机器学习算法背后的基本原理。如今，众多Python包中的一个可以用来实现大多数机器学习算法。我们可以使用这些Python包在几分钟内快速实现任何机器学习方法。我们觉得很有趣，你觉得呢？然而，当许多学生和专业人士需要对算法进行更改时，他们会很纠结。为了了解机器学习算法如何在其核心发挥作用，我们开发了这一系列博客。我们打算在未来提供一个关于更多机器学习算法的简短系列，我们希望你会发现这一个令人兴奋和有价值的！</p><p id="2923" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">优化是机器学习的核心——它是让算法的结果以我们希望的方式“好”的很大一部分。许多机器学习算法使用梯度下降算法来寻找其参数的最优值。因此，理解梯度下降算法对于理解AI如何产生好的结果是必不可少的。</p><p id="edd0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本系列的第一部分中，我们将提供梯度下降算法是什么、为什么和如何的背景知识。在第二部分，我们将为您提供一个强大的数学直觉，关于梯度下降算法如何找到其参数的最佳值。在本系列的最后一部分，我们将比较梯度下降算法的变体和它们在Python中精心编写的代码示例。本系列面向初学者和专家——一起来，都来！</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="3464" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">什么是梯度下降算法？</h2><p id="1ffc" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">维基百科对短语梯度下降的正式定义如下:</p><blockquote class="nu nv nw"><p id="78ca" class="kv kw nx kx b ky kz kh la lb lc kk ld ny lf lg lh nz lj lk ll oa ln lo lp lq ij bi translated">数学上，梯度下降是<strong class="kx jh">一种寻找可微函数局部极小值的一阶迭代优化算法</strong>。</p></blockquote><p id="5d79" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">梯度下降是一种机器学习算法，它迭代地寻找其参数的最佳值。该算法在更新参数值时考虑函数的梯度、用户定义的学习率和初始参数值。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="d9ed" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">梯度下降算法背后的直觉:</h2><p id="9cb6" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">让我们用一个比喻来形象化梯度下降在行动中是什么样子。假设我们正在爬山，不幸的是，当我们正在爬山的时候，天开始下雨了。我们的目标是尽快下山寻找避难所。那么，我们的策略是什么呢？记住，因为下雨，我们看不到很远。在我们周围的所有方向，我们只能感知附近的运动。</p><p id="d227" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我想到的是。我们将扫描我们周围的区域，寻找一个能让我们尽快降落的地方。一旦我们发现<strong class="kx jh"> <em class="nx">方向的</em> </strong>，我们就会朝那个方向迈出<strong class="kx jh"> <em class="nx">小步</em> </strong>。我们将继续这样做，直到到达山脚。所以，本质上，这就是梯度下降法定位全局最小值(我们正在分析的整个数据集的最低点)的方式。下面是我们如何将这个例子与梯度下降算法联系起来。</p><blockquote class="nu nv nw"><p id="ad0d" class="kv kw nx kx b ky kz kh la lb lc kk ld ny lf lg lh nz lj lk ll oa ln lo lp lq ij bi translated">当前位置→ → →初始参数<br/>婴儿步→ → →学习速率<br/>方向→ → →偏导数(梯度)</p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="a1f5" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">为什么我们需要梯度下降算法？</h2><p id="43ac" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">在许多机器学习模型中，我们的最终目标是找到最佳参数值，以降低与预测相关的成本。为此，我们首先从这些参数的随机值开始，并试图找到最佳值。为了找到最优值，我们使用梯度下降算法。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="d4b3" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">梯度下降算法是如何工作的？</h2><ol class=""><li id="b5d9" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated">从参数的随机初始值开始。</li><li id="d9aa" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">使用当前参数预测目标变量的值。</li><li id="7089" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">计算与预测相关的成本。</li><li id="8f07" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">我们把成本降到最低了吗？如果是，则转到步骤6。如果没有，则转到步骤5。</li><li id="b0f0" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">使用梯度下降算法更新参数值，并返回步骤2。</li><li id="8d5c" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">我们有了最终的更新参数。</li><li id="cb81" class="nb nc jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">我们的模型可以(下山)了！</li></ol><figure class="oc od oe of gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/694e05347b6b5ae5c58e307678fa6461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlZfJlaJl3OYTWP9bqTuKg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图1:梯度下降算法如何工作</figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="d596" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">梯度下降算法的公式:</h2><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi og"><img src="../Images/64755bad2d934201f38cdefe571ec484.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*2hn1hqV5YqA47gj5i77AEw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图2:梯度下降算法的公式</figcaption></figure><p id="6e52" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们来理解上面公式中提到的每个术语背后的含义。我们先从理解方向导数开始。</p><blockquote class="nu nv nw"><p id="1977" class="kv kw nx kx b ky kz kh la lb lc kk ld ny lf lg lh nz lj lk ll oa ln lo lp lq ij bi translated"><strong class="kx jh">注:</strong>我们的最终目标是尽快找到最优参数。所以，我们需要一些东西来帮助我们尽快朝着正确的方向前进。</p></blockquote></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="c59e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">为什么我们要使用渐变？</h2><blockquote class="nu nv nw"><p id="4bfd" class="kv kw nx kx b ky kz kh la lb lc kk ld ny lf lg lh nz lj lk ll oa ln lo lp lq ij bi translated"><strong class="kx jh">渐变:</strong>渐变只不过是一个向量，其条目是函数的偏导数。</p></blockquote><p id="0e8c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设我们有一个一元函数<strong class="kx jh"><em class="nx">【f(x)</em></strong><strong class="kx jh"><em class="nx">x</em></strong>。在这种情况下，我们将只有一个偏导数。下图所示的偏导数给出了函数在<strong class="kx jh"> <em class="nx"> x </em> </strong>方向(沿<strong class="kx jh"> <em class="nx"> x轴</em> </strong>)变化(增加或减少)的速度值。我们可以把偏导数写成梯度形式如下。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/26c2178ccdc21f19fb85a4393c8de49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*RZ52ygKK9YV06ODpwx3OMg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图3:一个元素的渐变</figcaption></figure><p id="d3f5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设我们有一个函数<strong class="kx jh"> <em class="nx"> f(x，y) </em> </strong>两个变量，<strong class="kx jh"> <em class="nx"> x </em> </strong>和<strong class="kx jh"> <em class="nx"> y </em> </strong>。在这种情况下，我们将有两个偏导数。下图所示的偏导数给出了函数在<strong class="kx jh"> <em class="nx"> x </em> </strong>方向和<strong class="kx jh"> <em class="nx"> y </em> </strong>方向(沿<strong class="kx jh"> <em class="nx"> x轴</em> </strong>和<strong class="kx jh"> <em class="nx"> y轴</em> </strong>)变化(增加或减少)的速度值。我们可以把偏导数写成梯度形式如下。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/730a285bfe60decb6f4578e9888054ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*cq89LuY_57OTvKRKpVPQ0w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图4:两个元素的渐变</figcaption></figure><p id="a10e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">概括一下这个，我们可以有一个函数有<strong class="kx jh"> <em class="nx"> n个</em> </strong>变量，它的梯度会有<strong class="kx jh"> <em class="nx"> n个</em> </strong>元素。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/5cb440ff10f625d44ec257f8006e6088.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*ghSMBnyn0KuPyp1O4W6NyA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图5:n个元素的渐变</figcaption></figure><p id="24d0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但现在的问题是，如果我们想在一些方向上求导，而不仅仅是沿着轴，会怎么样？我们知道，从一个给定的点出发，我们可以向无数个方向行进。现在，为了找到任意方向的梯度，我们将使用方向导数的概念。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/bd6a3d4dc32371fec2dc63c7f9e8c099.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*oljPN-DjZiDzGlu3508Qrg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图6:我们可以从一个点向无数个方向迈出一步。</figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="bf11" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">方向导数简介；</h2><blockquote class="nu nv nw"><p id="7571" class="kv kw nx kx b ky kz kh la lb lc kk ld ny lf lg lh nz lj lk ll oa ln lo lp lq ij bi translated"><strong class="kx jh">单位向量</strong>:单位向量是一个大小为1的向量。</p></blockquote><p id="1ae7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们如何找到向量的长度或大小？</p><p id="4999" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">考虑向量u的下列情况。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/e9456e7ef8aba69461f101532dfe0065.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*SL-79JkiXYBXoP7LjWtobg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图7:向量u</figcaption></figure><p id="6c4d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，向量的长度被计算为其所有分量平方之和的平方根。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi om"><img src="../Images/01cbba9fbca0a0aed0297d1ea4d1e622.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*WXdQJQNaPR7nchXhAcKAhA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图8:向量u的长度</figcaption></figure><p id="8886" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个函数<strong class="kx jh"> <em class="nx"> f(x，y) </em> </strong>在<strong class="kx jh"> <em class="nx">向量u </em> </strong>(一个单位向量)方向上的导数由该函数的<strong class="kx jh"> <em class="nx">与<strong class="kx jh"> <em class="nx">单位向量u </em> </strong>的梯度的<strong class="kx jh"><em class="nx">点积给出。数学上，我们可以用下面的形式来表示。</em></strong></em></strong></p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/13456c528a4d0231dc89f544bfe4d0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*eoyzN41X1geseAesumZQXA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图9:方向导数</figcaption></figure><p id="7d52" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上式给出了<strong class="kx jh"> <em class="nx"> f(x，y) </em> </strong>在任意方向的偏导数。现在，让我们看看它是如何工作的，如果我们想找到沿x轴的偏导数。第一，如果要求x方向的偏导数，那么<strong class="kx jh"> <em class="nx">单位向量u </em> </strong>就会是<strong class="kx jh"> <em class="nx"> (1，0) </em> </strong>。现在，我们来计算沿<strong class="kx jh"> <em class="nx"> x轴</em> </strong>的偏导数。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/dc6d0a284d74a10f4a0844a4fb82b6c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*BIxoGYLE-S8Aui3bpOaDOw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图10:沿x轴的偏导数</figcaption></figure><p id="61e3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，让我们看看，如果我们想找到沿y轴的偏导数，它是如何工作的。首先，如果要求y方向的偏导数，那么<strong class="kx jh"> <em class="nx">单位向量u </em> </strong>就会是<strong class="kx jh"> <em class="nx"> (0，1) </em> </strong>。现在，让我们计算沿<strong class="kx jh"> <em class="nx"> y轴</em> </strong>的偏导数。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi op"><img src="../Images/6e3d4906c92467673ca6f0fb980ba576.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*b5RZW8s9IYZT7WLzeyMf6w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图11:沿y轴的偏导数</figcaption></figure><blockquote class="nu nv nw"><p id="cb33" class="kv kw nx kx b ky kz kh la lb lc kk ld ny lf lg lh nz lj lk ll oa ln lo lp lq ij bi translated"><strong class="kx jh">注意:</strong>单位向量的长度(大小)必须为1。</p></blockquote><p id="a0ee" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们知道了如何找到所有方向的偏导数，我们需要找到偏导数给我们带来最大变化的方向，因为，在我们的情况下，我们希望尽快找到最佳值。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="62cf" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">最陡的上坡方向是什么？</h2><p id="4302" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，我们知道方向导数如下所示。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/5cc739a403a5827ab5228f2b4a8c4071.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*9lPrEP2sB8PYN-EiFedA5g.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图12:方向导数</figcaption></figure><p id="f689" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们可以用两个向量之间角度的余弦值来代替它们之间的点积。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi or"><img src="../Images/279c72dc5b53f6444997b76947507d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*TY4oPC7R94bnH1HD25QskA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图13:方向导数</figcaption></figure><p id="d15c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，因为u是一个单位向量，它的大小总是1。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi os"><img src="../Images/629eb28b68c5fec63d3cae3831d575e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*Q7wy1l-FhGT1ql6MzXvhFQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图14:方向导数</figcaption></figure><p id="d59c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，在上面的等式中，我们无法控制梯度的大小。我们只能控制角度<strong class="kx jh"> <em class="nx"> θ </em> </strong>。所以，为了最大化函数的偏导数，我们需要最大化<strong class="kx jh"> <em class="nx"> cosθ </em> </strong>。现在我们都知道<strong class="kx jh"> <em class="nx"> cosθ </em> </strong>最大化(1)当<strong class="kx jh"><em class="nx">θ= 0</em></strong>(<strong class="kx jh"><em class="nx">cos 0 = 1</em></strong>)。这意味着当梯度和单位向量之间的角度为0时，导数的值最大。换句话说，我们可以说，当单位向量(方向向量)指向梯度的方向时，偏导数的值最大。</p><p id="e16e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以，总之，我们可以说，找到梯度方向的偏导数，给了我们最快速的上升。现在，让我们借助一个例子来理解这一点。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="dea5" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">证明最陡上升方向的示例:</h2><p id="2d89" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">求函数<strong class="kx jh"> <em class="nx"> f(x，y) = x + y </em> </strong>在<strong class="kx jh"> <em class="nx"> (3，2) </em> </strong>点的梯度。</p><h2 id="1499" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">1.第一步:</h2><p id="074a" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们有一个函数<strong class="kx jh"> <em class="nx"> f(x，y) </em> </strong>两个变量<strong class="kx jh"> <em class="nx"> x和</em> </strong>。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/cd9af7c77bb081aaa9f7e86b5e70ca2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*k_fpghlbFTA9VC0Hkf-cAw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图15:函数f(x，y)</figcaption></figure><h2 id="5acb" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">2.第二步:</h2><p id="364d" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们将找到函数的梯度。因为函数中有两个变量，所以梯度向量中有两个元素。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/043cc23134cf038fc7df9a3948057ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*Cpo8v-UN3x3fzE_K_1MBDQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图16:f(x，y)的梯度</figcaption></figure><h2 id="64be" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">3.第三步:</h2><p id="7993" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来我们在计算函数<strong class="kx jh"> <em class="nx"> f(x，y) = x + y </em> </strong>的梯度。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/9f47cfa8643909dce1afff8ed79dfc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*Y85-G8OM2oFY-cMtrH_V4Q.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图17:f(x，y)的偏导数</figcaption></figure><h2 id="8ddd" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">4.第四步:</h2><p id="3d65" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">函数的梯度可以写成如下形式。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/97dd3efaf785801532179bc7a6376c62.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*qcff-azn5-RQXXufEAVOFA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图18:f(x，y)的梯度</figcaption></figure><h2 id="cf49" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">5.第五步:</h2><p id="b7b2" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们计算函数在<strong class="kx jh"> <em class="nx"> (3，2) </em> </strong>点的梯度。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/25e5a57264d8233e6a59817fa1da9f49.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*jjk2z3rCZBE_Pq4vLrSEnA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图19:点(3，2)处f(x，y)的梯度</figcaption></figure><h2 id="815e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">6.第六步:</h2><p id="d545" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来我们在求函数<strong class="kx jh"> <em class="nx"> f(x，y) </em> </strong>沿x轴<strong class="kx jh"> <em class="nx"> (1，0) </em> </strong>的偏导数。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/3c55f438f5c69568c0a6b199bae0185e.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*WarKy2m9NPRbig5B08z3Hw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图20:沿x轴在点(3，2)处f(x，y)的梯度</figcaption></figure><h2 id="da77" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">7.第七步:</h2><p id="dee9" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来我们在求函数f(x，y)沿y轴的偏导数<strong class="kx jh"> <em class="nx"> (0，1) </em> </strong>。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/3032303dcf21fff218529448530b72fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*yleWk3TqmRE3IefF_gzsrA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图-21:f(x，y)在点(3，2)处沿y轴的梯度</figcaption></figure><h2 id="00a1" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">8.第8步:</h2><p id="f142" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来我们求出函数<strong class="kx jh"> <em class="nx"> f(x，y) </em> </strong>在<strong class="kx jh"> <em class="nx"> (1，1) </em> </strong>方向的偏导数。注意，这里我们必须考虑单位向量的大小。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/4048e315ba126ad6afe32c0563ad21a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*nNbRfoqT424TGuQ9z4Nn3w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图-22:f(x，y)在点(3，2)处沿(1，1)方向的梯度</figcaption></figure><h2 id="3b96" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">9.第九步:</h2><p id="6227" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来我们求出函数<strong class="kx jh"><em class="nx">【x，y】</em></strong>在梯度<strong class="kx jh"><em class="nx">【3，2】</em></strong>方向的偏导数。请注意，这是梯度向量的方向。同样，这里我们必须考虑单位向量的大小。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/4385185915ca3d11317cb06e5777021f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*yB53B_I1j-8m8YM7cUpTcg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图-23:f(x，y)在点(3，2)处沿(3，2)方向的梯度</figcaption></figure><h2 id="9dda" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">10.第十步:</h2><p id="5b28" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">所以，根据<a class="ae jd" href="#815e" rel="noopener ugc nofollow"> Step — 6 </a>、<a class="ae jd" href="#da77" rel="noopener ugc nofollow"> Step — 7 </a>、<a class="ae jd" href="#00a1" rel="noopener ugc nofollow"> Step — 8 </a>、<a class="ae jd" href="#3b96" rel="noopener ugc nofollow"> Step — 9 </a>的计算，我们可以很有信心地说<strong class="kx jh">、<em class="nx">最陡上坡</em>、</strong>的方向就是坡度的方向。</p><p id="2e01" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在梯度下降算法中，我们的目标是尽可能快地找到最优参数。这就是我们在梯度下降算法中使用偏导数的原因。</p><p id="b3c2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是等等…有一个陷阱！</p><p id="b479" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在梯度下降算法中，我们希望找到最小点。然而，使用梯度将引导我们到最高点，因为它给我们最陡的上升。那么，我们该怎么办？</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="a8f7" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">梯度下降算法中(—)符号的解释:</h2><p id="642e" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，我们知道坡度给了我们最陡的上升。所以，如果我们沿着最陡的上升方向前进，我们将永远不会到达最低点。我们的最终目标是快速找到一个到达最小点的方法。所以，要往<strong class="kx jh"> <em class="nx">最陡下降</em> </strong>的方向走，我们就要往<strong class="kx jh"> <em class="nx">最陡上升的</em> </strong>的相反方向走。这就是我们使用<strong class="kx jh"> <em class="nx"> ( — ) </em> </strong>符号的原因。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="bd08" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">为什么是学习率？</h2><p id="b8ac" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">请注意，我们无法控制梯度的大小。偶尔我们可能会得到一个非常高的梯度值。因此，如果我们不设法减缓变化的速度，我们最终会取得很大的进步。重要的是要记住，高学习率可能只给我们提供次优的参数值。相反，较低的学习速率可能需要更多的训练时期来获得最佳值。</p><p id="5273" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">梯度下降方法有一个超参数，它调节我们的模型学习新信息的速度。这个超参数被称为学习率。我们模型的学习率决定了参数值改变的速度。我们必须将学习速度设定在最佳值。如果学习率太高，我们的模型可能会大步前进，错过最小值。因此，较高的学习速率可能导致模型不收敛。另一方面，如果学习量太小，模型收敛的时间就会太长。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/26ee343cb31bb1c0b6a5f9649611bdd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NTxALAEMs8KpsCIHivuLcA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图24:学习率</figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="28f3" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">区分的一些基本规则:</h2><p id="e894" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">1.标量乘法规则:</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/5861d3065c5c235ee4f64c8af19fdad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*zR-bbVcUXS50f32nTVknVA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图25:标量乘法规则</figcaption></figure><p id="600c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2.求和规则:</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/b0bd21bff5dd472ff0163901cd9c1df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*VGWrCX9C_vwdbPDdCibP7g.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图26:求和规则</figcaption></figure><p id="8d73" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">3.权力法则:</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/6e719ec5cefac1d54da22539a040f87e.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*qxYKbD0ZUaIH9i2dvusbwA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图27:权力法则</figcaption></figure><p id="71cb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">4.连锁法则:</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/c59e878d5e28efa7c40ca13b9288b6ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*pRCyZdhRzdrx4PYL362HKw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图28:链式法则</figcaption></figure><p id="dddb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们举几个例子来理解梯度下降算法是如何工作的。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="0a99" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">一个变量的梯度下降:</h2><p id="97f6" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">让我们从一个非常简单的成本函数开始。假设我们有一个成本函数<em class="nx">(</em><strong class="kx jh"><em class="nx">J(θ)=θ)</em></strong>只涉及一个参数<em class="nx">(</em><strong class="kx jh"><em class="nx">【θ】</em></strong>，我们的目标是找到参数的最优值(<strong class="kx jh"> <em class="nx"> θ </em> </strong>)，使其最小化成本函数<em class="nx"> ( </em> <strong class="kx jh"> <em class="nx"> J(θ) = θ </em></strong></p><p id="e4b0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从我们的代价函数<em class="nx">(</em><strong class="kx jh"><em class="nx">J(θ)=θ</em></strong><em class="nx">)，</em>我们可以很清楚的说在<strong class="kx jh"> <em class="nx"> θ=0时会最小。然而，当我们处理更复杂的函数时，得出这样的结论并不容易。为此，我们将使用梯度下降算法。让我们看看如何应用梯度下降算法找到参数的最优值(<strong class="kx jh"> <em class="nx"> θ </em> </strong>)。</em></strong></p><h2 id="4c2a" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">1.第一步:</h2><p id="64ed" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们的带有一个参数的成本函数(<strong class="kx jh"><em class="nx">【θ】</em></strong>)由下式给出:</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/86618ef58240b550647d298eaea98dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*M7o4IL8S9g3HEVD2vA7Y6A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图29:一个变量的成本函数</figcaption></figure><h2 id="50c8" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">2.第二步:</h2><p id="eb59" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们的最终目标是通过寻找参数<strong class="kx jh"> <em class="nx"> θ的最优值来最小化成本函数。</em> </strong></p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/0cf04bbc9a81144f5822ba1cffd97dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:138/format:webp/1*Wh3IlorsGSGq8TC6sbe0Fw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图30:最小化成本函数</figcaption></figure><h2 id="6fb1" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">3.第三步:</h2><p id="d861" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">梯度下降算法的公式如下。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/74edf821ed6fcf5b9181af02c8dcbb2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*i4h-yp2vCtDf2gtM6IMSJw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图31:梯度下降算法</figcaption></figure><h2 id="df2e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">4.第四步:</h2><p id="462c" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">为了简化计算，我们考虑学习率为0.1。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/95fffc45ccb10527a90c71f0c005077c.png" data-original-src="https://miro.medium.com/v2/resize:fit:126/format:webp/1*a8Oa7-bQep5jIwUzP-55rQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图32:学习率</figcaption></figure><h2 id="02a5" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">5.第五步:</h2><p id="385f" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们找到成本函数的偏导数。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/d5aa3029158b028fa9486017ea2dec42.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*0_N9Z0As98DkZ5EkkZNhHA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图33:成本函数的偏导数</figcaption></figure><h2 id="4508" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">6.第六步:</h2><p id="52eb" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们使用<a class="ae jd" href="#02a5" rel="noopener ugc nofollow"> Step — 5 </a>的偏导数，并将其代入<a class="ae jd" href="#6fb1" rel="noopener ugc nofollow"> Step — 3 </a>中给出的公式。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/907acea5cb8883f553c86a0125fb11aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*veMwD6zAw32lX2oiQzA-Fg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图-34:使用梯度下降算法更新参数</figcaption></figure><h2 id="70db" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">7.第七步:</h2><p id="0d39" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，让我们借助一个例子来理解梯度下降算法是如何工作的。这里，我们从<strong class="kx jh"> <em class="nx"> θ=5，</em> </strong>的值开始，我们将找到<strong class="kx jh"> <em class="nx"> θ </em> </strong>的最佳值，使得它最小化成本函数。接下来，我们也将从<strong class="kx jh"> <em class="nx"> θ=-5 </em> </strong>的值开始，检查它是否能找到成本函数的最优值。请注意，这里我们使用上面导出的梯度下降规则来更新参数<strong class="kx jh"> <em class="nx"> θ的值。</em> </strong></p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/71a1d6547b586ce8facb2a07fed3ad9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*PjkfP-RbojQKeTzd3tf-5Q.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图35:不同数据点的成本值</figcaption></figure><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/c3a737c11b1beaa6940f748afaf74fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*VM58jIcovAgu-UNuRfAfvA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图36:不同数据点的成本值</figcaption></figure><h2 id="923e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">8.第8步:</h2><p id="4d6d" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们绘制上表所示的数据图表。从图中可以看出，梯度下降算法能够找到<strong class="kx jh"> <em class="nx"> θ </em> </strong>的最优值，并使代价函数<strong class="kx jh"> <em class="nx"> J(θ)最小化。</em>T51】</strong></p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi po"><img src="../Images/c68a18b324b7a8fccad0f568cc103097.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*DeAziOl9uLLopvEE4LiKKw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图— 37:参数与成本函数的关系图</figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="640b" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">两个变量的梯度下降:</h2><p id="5c4e" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，让我们转到有两个变量的成本函数，看看情况如何。</p><h2 id="0c42" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">1.第一步:</h2><p id="1c5d" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们的带有两个参数<strong class="kx jh"> <em class="nx"> (θ1和θ2) </em> </strong>的成本函数由下式给出:</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/bf48c79ef758f994d59e7041cff44801.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*ScLO8Tw5bmQLfebmWOG2rg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图38:带有两个参数的成本函数</figcaption></figure><h2 id="79b9" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">2.第二步:</h2><p id="178c" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们的最终目标是通过寻找参数<strong class="kx jh"> <em class="nx"> θ1和θ2的最优值来最小化代价函数。</em>T3】</strong></p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/97724382c8f69a48d7fe7eaeecba2f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*ioeuNCLPzUt5okyYPzFsZg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图39:最小化成本函数</figcaption></figure><h2 id="5324" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">3.第三步:</h2><p id="9958" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">梯度下降算法的公式如下。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/74edf821ed6fcf5b9181af02c8dcbb2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*i4h-yp2vCtDf2gtM6IMSJw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图40:梯度下降算法</figcaption></figure><h2 id="d0ff" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">4.第四步:</h2><p id="6bda" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们将使用在<a class="ae jd" href="#5324" rel="noopener ugc nofollow">步骤3 </a>中给出的公式来找到我们的参数<strong class="kx jh"> <em class="nx"> θ1和θ2的最佳值。</em>T9】</strong></p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/f9a9a4cdb232e97d2230b86c4c09e442.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*VFpaHbBLYLePIutbl0AUlQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图-41:使用梯度下降算法更新参数</figcaption></figure><h2 id="57da" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">4.第四步:</h2><p id="f7ff" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们找到成本函数相对于参数<strong class="kx jh"> <em class="nx"> θ1和θ2的偏导数。</em>T13】</strong></p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/32bd213258372d0ba50f00e4e48bdd0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*Y8FQ-UPcazLXRjkoiseSlA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图42:成本函数的偏导数</figcaption></figure><h2 id="2726" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">5.第五步:</h2><p id="f6a4" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们使用在<a class="ae jd" href="#57da" rel="noopener ugc nofollow">步骤— 4 </a>中导出的偏导数来替换<a class="ae jd" href="#5324" rel="noopener ugc nofollow">步骤— 3 </a>。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/fda5c7afb16dfa2e2e26a88819c243e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*q-Y6r94n1GQyAqAFVDsijg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图-43:使用梯度下降算法更新参数</figcaption></figure><h2 id="6041" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">6.第六步:</h2><p id="6cb7" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">为了简化计算，我们将使用0.1的学习率。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/95fffc45ccb10527a90c71f0c005077c.png" data-original-src="https://miro.medium.com/v2/resize:fit:126/format:webp/1*a8Oa7-bQep5jIwUzP-55rQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图44:学习率</figcaption></figure><h2 id="da85" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">7.第七步:</h2><p id="07ea" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">现在，让我们借助一个例子来理解梯度下降算法是如何工作的。这里，我们从<strong class="kx jh"> <em class="nx"> θ1=1和θ2=1，</em> </strong>的值开始，我们将找到<strong class="kx jh"> <em class="nx"> θ1和θ2 </em> </strong>的最优值，使得它最小化成本函数。接下来，我们还将从<strong class="kx jh"> <em class="nx"> θ1=-1和θ2=-1 </em> </strong>的值开始，检查梯度下降算法是否能找到代价函数的最优值。</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/47ee429b0e2c958ceea23316ff192714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*8DocvsvZiLXc9Wp56QYWlQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图45:不同数据点的成本值</figcaption></figure><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/274ec7432c6ec088774dddc7ffde3cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*umyA9GuqynOqhymIvJYaLg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图46:不同数据点的成本值</figcaption></figure><h2 id="1ea2" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">8.第8步:</h2><p id="4aea" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">接下来，我们绘制上表所示的数据图表。从图中可以看出，梯度下降算法能够找到<strong class="kx jh"> <em class="nx"> θ1和θ2 </em> </strong>的最优值，并使代价函数<strong class="kx jh"> <em class="nx"> J(θ)最小化。</em> </strong></p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/0c705b00aff4dfb116475d84f561d4f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*l6fM8RNXLzY0onIhtHbJZg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图— 47:参数与成本函数的关系图</figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="eeb2" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">结论:</h2><p id="957a" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">你有它！我们已经复习了梯度下降算法的基础知识及其在机器学习中的重要作用。随意检查第一遍可能不清楚的计算或概念。现在你已经成功地学习了如何下山，在梯度下降系列的下一期文章中学习梯度下降帮助解决问题的其他方法。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><figure class="oc od oe of gt is gh gi paragraph-image"><a href="https://www.buymeacoffee.com/pratu"><div class="gh gi pw"><img src="../Images/bf19b95960da2e6db6fc0dd7ba17c7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/0*jR9Jyfl35goV8nO-.png"/></div></a><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">给普拉蒂克买杯咖啡！</figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="d70e" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">引用:</h2><p id="57d0" class="pw-post-body-paragraph kv kw jg kx b ky nd kh la lb ne kk ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">对于学术背景下的归属，请引用该工作为:</p><pre class="oc od oe of gt px py pz qa aw qb bi"><span id="492a" class="mi mj jg py b gy qc qd l qe qf">Shukla, et al., “The Gradient Descent Algorithm”, Towards AI, 2022</span></pre></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="15b4" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">BibTex引文:</h2><pre class="oc od oe of gt px py pz qa aw qb bi"><span id="99e5" class="mi mj jg py b gy qc qd l qe qf">@article{pratik_2022, <br/> title={The Gradient Descent Algorithm}, <br/> url={<a class="ae jd" href="https://towardsai.net/p/l/the-gradient-descent-algorithm" rel="noopener ugc nofollow" target="_blank">https://towardsai.net/p/l/the-gradient-descent-algorithm</a>}, <br/> journal={Towards AI}, <br/> publisher={Towards AI Co.}, <br/> author={Pratik, Shukla},<br/> editor={Lauren, Keegan},  <br/> year={2022}, <br/> month={Oct}<br/>}</span></pre></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="8853" class="mi mj jg bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">参考资料和资源:</h2><ol class=""><li id="6f61" class="nb nc jg kx b ky nd lb ne le nf li ng lm nh lq ni nj nk nl bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降——维基百科</a></li></ol></div></div>    
</body>
</html>