<html>
<head>
<title>Fake News Detection using BERT Model Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于BERT模型Python的假新闻检测</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fake-news-detection-using-bert-model-python-de005c5809ed?source=collection_archive---------0-----------------------#2022-09-07">https://pub.towardsai.net/fake-news-detection-using-bert-model-python-de005c5809ed?source=collection_archive---------0-----------------------#2022-09-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="00ea" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在本文中，我们将尝试使用Python构建一个BERT分类模型来检测假新闻。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/86487662058c593e00ba39870cf16290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GfA5PbNAhkgV7tI_"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">Jorge Franganillo 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="61c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">社交媒体的兴起放大了假新闻在我们社会中的影响力。人们经常认为他们读到/听到的是真的，这在政治上和经济上都在很大程度上影响着世界。所以今天我们要用BERT模型和Python开发一个可以自动检测假新闻的应用。</p><p id="6ccd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">加载数据</strong></p><p id="f43b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文中用到的数据可以在  <strong class="ky ir">这里找到<a class="ae kv" href="https://github.com/muttinenisairohith/FakeNewsDetection/tree/main/Data" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">。</strong>复制数据，解压并粘贴到你的文件夹中。让我们从导入库开始我们的代码—</a></strong></p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="e91f" class="lx ly iq lt b gy lz ma l mb mc">import pandas as pd<br/>import csv</span></pre><p id="6190" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦你复制了数据—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="9ce2" class="lx ly iq lt b gy lz ma l mb mc">df_fake = pd.read_csv("Fake.csv")</span><span id="b95c" class="lx ly iq lt b gy md ma l mb mc">df_true = pd.read_csv("True.csv")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi me"><img src="../Images/9967637c7e4eb1fc57f0db1b55d7631d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b5A5ZqG8uxNfgSQZ0nZeTA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:产出</figcaption></figure><p id="e1ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经导入了数据并创建了两个数据框架。现在让我们做一些数据清理和分析—</p><p id="aaa1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将通过合并上面的两个数据帧来创建一个数据帧。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="af55" class="lx ly iq lt b gy lz ma l mb mc">df_fake["Label"] = "Fake"<br/>df_true["Label"] = "True"</span></pre><p id="8575" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了区分真假新闻，我们增加了一个新的栏目——标签..这将是我们的从属专栏。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="ad96" class="lx ly iq lt b gy lz ma l mb mc">df = pd.concat([df_fake,df_true])</span><span id="8fbb" class="lx ly iq lt b gy md ma l mb mc">df = df.sample(frac=1).reset_index(drop=True)</span></pre><p id="8828" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上图中，我们正在合并数据帧并对其进行洗牌。</p><p id="7400" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们准备好了数据—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="d9cd" class="lx ly iq lt b gy lz ma l mb mc">df.head(5)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mf"><img src="../Images/2afd0b77d6cf2a563e7015e00f48f08c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t86oVT7FNjGhFLv13xInUw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:产出</figcaption></figure><p id="e2a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们检查一下是否有丢失的值—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="69ef" class="lx ly iq lt b gy lz ma l mb mc">df.isnull().sum()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/e2e8ba4d0c64312d829461d1eac8d770.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*4A40-iZIGvph1gk_9rmY_A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:产出</figcaption></figure><p id="665f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据分析</strong></p><p id="33a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们尝试分析数据中的一些列——正如我们所知，标题和文本是必要的，因为我们必须根据它们来预测标签；很明显，日期在预测新闻的真假方面并没有起到很大的作用。让我们使用Seaborn库来可视化主题列—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="df43" class="lx ly iq lt b gy lz ma l mb mc">import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="d072" class="lx ly iq lt b gy md ma l mb mc">#Creating Figure<br/>fig, axes = plt.subplots(1,2, figsize = (15,6))</span><span id="8326" class="lx ly iq lt b gy md ma l mb mc">#Adding the histogram1 - Fake News<br/>sns.histplot(df_fake.subject, palette = 'Set1', alpha = 0.5, ax = axes[0])<br/>axes[0].tick_params(axis = 'x', rotation = 90)<br/>axes[0].set_title('Fake News Subject')</span><span id="c05a" class="lx ly iq lt b gy md ma l mb mc">#Adding the histogram2 - True News<br/>sns.histplot(df_true.subject, palette = 'Set1', alpha = 0.5, ax = axes[1])<br/>axes[1].tick_params(axis = 'x', rotation = 90)<br/>axes[1].set_title('True  News Subject')</span><span id="157f" class="lx ly iq lt b gy md ma l mb mc">#Printing the count of Subject<br/>print("Fake News Subject : ",dict(df_fake.subject.value_counts()))<br/>print("True News Subject : ",dict(df_true.subject.value_counts()))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mh"><img src="../Images/a048179702d2fa615da34143a896b298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OveTr5inMk0ZspZLvQaSXw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:产出</figcaption></figure><p id="96f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们所看到的，subject列在两个数据帧中有不同的值。所以我们不能继续包括该专栏。</p><p id="d235" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们来看看数据是否平衡。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="e660" class="lx ly iq lt b gy lz ma l mb mc">sns.histplot(df.Label, palette = 'Set1', alpha = 0.5)</span><span id="e49f" class="lx ly iq lt b gy md ma l mb mc">plt.tick_params(axis = 'x', rotation = 90)</span><span id="c279" class="lx ly iq lt b gy md ma l mb mc">plt.title('True VS Fake News')</span><span id="6fbc" class="lx ly iq lt b gy md ma l mb mc">df.Label.value_counts()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/49318e7b04329b5699d9e31bef5341ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*oAvJvhNEzO0FdZL-UAV9Ow.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:产出</figcaption></figure><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="1fd5" class="lx ly iq lt b gy lz ma l mb mc">df["text"] = df["title"]+df["text"] #considering text and title as X</span></pre><p id="18a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们所看到的，标签列是对象格式的，让我们将其编码为数字格式。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="c842" class="lx ly iq lt b gy lz ma l mb mc">df['Label'] = df['Label'].map({'True':1, 'Fake':0})</span></pre><p id="8f54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们只需要对两个值进行编码，我使用了DataFrame Python中的map()方法。但是对于Python编码方法的详细解释，请参考我下面的文章:</p><div class="mj mk gp gr ml mm"><a href="https://blog.devgenius.io/encoding-methods-to-encode-categorical-data-in-machine-learning-717b5509933c" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd ir gy z fp mr fr fs ms fu fw ip bi translated">机器学习中分类数据的编码方法</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">在机器学习领域，在进行建模之前，数据准备是一项强制性任务。有…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">blog.devgenius.io</p></div></div><div class="mv l"><div class="mw l mx my mz mv na kp mm"/></div></div></a></div><p id="956e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以我们的数据准备好了。让我们进行数据的训练-测试-分割—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="9fec" class="lx ly iq lt b gy lz ma l mb mc">from sklearn.model_selection import train_test_split</span><span id="96b8" class="lx ly iq lt b gy md ma l mb mc">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 10)</span></pre><p id="3c6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经准备好了训练和测试数据，现在让我们来学习一下BERT模型。</p><p id="d71e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">伯特模型</strong></p><p id="f34a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BERT代表来自变压器 的<strong class="ky ir"> <em class="nb">双向编码器表示。它由几个堆叠在一起的变压器编码器组成。它使用转换器来理解句子/文本中单词之间的上下文关系。BERT Transformer通常有两种机制:读取文本输入的编码器和预测给定任务的解码器。</em></strong></p><p id="aac4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BERT在不同NLP任务上表现良好的主要原因之一是使用了<strong class="ky ir">半监督学习</strong>。这意味着模型是为特定的任务而训练的，使它能够理解语言的模式。训练后，模型(BERT)具有语言处理能力，可用于增强我们使用监督学习构建和训练的其他模型。</p><p id="613e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初，BERT以两种尺寸发布——基本型号和大型型号。BERT(BASE)在编码器堆栈中有1 <em class="nb"> 2层，</em>而BERT(LARGE)在编码器堆栈中有<em class="nb"> 24层</em>。</p><p id="26f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BERT模型首先将分类标记(CLS)作为输入，然后是单词序列。然后，它将输入传递给上面的层。每一层应用自我关注并通过前馈网络传递结果，然后，它移交给下一个编码器。该模型根据BERT的大小输出大小的向量。如果我们想从这个模型中输出一个分类器，我们可以得到对应于CLS令牌的输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/5378989d75ae757a9eecba6743aa434a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nTGLTfbum7nP-XlWgYQaPQ.png"/></div></div></figure><p id="23ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个经过训练的向量可以用于执行许多任务，例如分类、翻译等。,</p><h1 id="22ef" class="nd ly iq bd ne nf ng nh ni nj nk nl nm jw nn jx no jz np ka nq kc nr kd ns nt bi translated"><strong class="ak">假新闻检测</strong></h1><p id="e0d0" class="pw-post-body-paragraph kw kx iq ky b kz nu jr lb lc nv ju le lf nw lh li lj nx ll lm ln ny lp lq lr ij bi translated">因此，让我们专注于我们的任务并开始分类——如前所述，我们需要将符号化的值作为输入传递给BERT模型。所以让我们使用transformers库中的tokenizer。</p><p id="547c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">标记化</strong></p><p id="a0a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从安装提供BERT模型的变压器开始。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="5fe7" class="lx ly iq lt b gy lz ma l mb mc">!pip install transformers</span></pre><p id="5971" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们开始标记化—</p><p id="cc86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种方法中，我们将使用来自BERT(基本)模型的自动提词器—</p><p id="a654" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与所有神经网络类似，transformers不能直接处理原始输入文本，因此我们需要对输入进行标记，并将其转换为数字。标记化执行以下功能—它将输入文本拆分成标记(单词、字母等)。)，用唯一的整数映射每个令牌，并根据模型排列它们。</p><p id="1f1d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们在应用程序中使用了BERT基本模型，所以我们也使用了相应的自动分词器来进行分词。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="8405" class="lx ly iq lt b gy lz ma l mb mc">from transformers import AutoTokenizer</span><span id="b82f" class="lx ly iq lt b gy md ma l mb mc">def tokenize(X):</span><span id="33b5" class="lx ly iq lt b gy md ma l mb mc">    X = tokenizer(<br/>        text = list(X),<br/>        add_special_tokens = True,<br/>        max_length = 100,<br/>        truncation = True,<br/>        padding = 'max_length',<br/>        return_tensors = 'tf',<br/>        return_token_type_ids = False,<br/>        return_attention_mask = True,<br/>        verbose = True<br/>        )</span><span id="8db4" class="lx ly iq lt b gy md ma l mb mc">    return X</span><span id="b4af" class="lx ly iq lt b gy md ma l mb mc">tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')</span></pre><p id="0f9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们把这些值符号化—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="0af5" class="lx ly iq lt b gy lz ma l mb mc">X_train_tokens = tokenize(X_train)<br/>X_test_tokens = tokenize(X_test)</span></pre><p id="b26f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">设计模型</strong></p><p id="7c8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">导入必要的库—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="6bff" class="lx ly iq lt b gy lz ma l mb mc">import tensorflow as tf<br/>from keras.models import Model, Sequential<br/>from keras.layers import Input, Dense, Dropout, Embedding<br/>from tensorflow.keras.optimizers import Adam<br/>from transformers import TFBertModel</span></pre><p id="a5ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">设计伯特函数—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="1e03" class="lx ly iq lt b gy lz ma l mb mc">Length = 100</span><span id="09f1" class="lx ly iq lt b gy md ma l mb mc">def get_model():</span><span id="a35e" class="lx ly iq lt b gy md ma l mb mc">    dropout_rate = 0.2</span><span id="f7a0" class="lx ly iq lt b gy md ma l mb mc">    input_ids = Input(shape = (Length,), dtype = tf.int32, name = 'input_ids')<br/>    input_mask = Input(shape = (Length,), dtype = tf.int32, name = 'input_mask')</span><span id="4d8b" class="lx ly iq lt b gy md ma l mb mc">    embeddings = bert([input_ids, input_mask])[1] #pooler output<br/>    print(embeddings)<br/>    out = Dropout(0.2)(embeddings)</span><span id="b80d" class="lx ly iq lt b gy md ma l mb mc">    #64 units dense layer</span><span id="43b7" class="lx ly iq lt b gy md ma l mb mc">    out = Dense(64,activation = 'relu')(out)<br/>    out = Dropout(0.2)(out)</span><span id="27ad" class="lx ly iq lt b gy md ma l mb mc">    y = Dense(1,activation = 'sigmoid')(out)</span><span id="c8bb" class="lx ly iq lt b gy md ma l mb mc">    model = Model(inputs=[input_ids, input_mask], outputs=y)<br/>    model.layers[2].trainable = True</span><span id="ee86" class="lx ly iq lt b gy md ma l mb mc">   #define optimizer<br/>   optimizer = Adam(learning_rate=1e-05, epsilon=1e-08, decay=0.01,clipnorm=1.0)</span><span id="42b2" class="lx ly iq lt b gy md ma l mb mc">    #complile the model<br/>    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = 'accuracy')</span><span id="01f5" class="lx ly iq lt b gy md ma l mb mc">    return model</span></pre><p id="e8b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">加载伯特模型—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="b2d2" class="lx ly iq lt b gy lz ma l mb mc">bert = TFBertModel.from_pretrained('bert-base-uncased')</span></pre><p id="0d6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们画出伯特模型——</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="0bcd" class="lx ly iq lt b gy lz ma l mb mc">model = get_model()<br/>tf.keras.utils.plot_model(model)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/12ce2ccb16039e9c6241b44bf0dcbc61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ujHBbPCXVbFhQ6_H_WdeQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:产出</figcaption></figure><p id="a9f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们根据上面的解释创建了模型。</p><p id="04b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">训练我们的模型</strong></p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="673f" class="lx ly iq lt b gy lz ma l mb mc">from keras.callbacks import EarlyStopping</span><span id="2fa6" class="lx ly iq lt b gy md ma l mb mc">history = model.fit(x = {'input_ids':X_train_tokens['input_ids'],'input_mask':X_train_tokens['attention_mask']}, y = y_train, epochs=3, validation_split = 0.2, batch_size = 64, callbacks=[EarlyStopping( monitor='val_accuracy' ,mode='max', patience=3,verbose=False,restore_best_weights=True)])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/301081de06a6a061788d9a997bd41ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lEYPSnSjihjAt6jfBH5kjQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:产出</figcaption></figure><p id="9e86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如我们所见，我们获得了99.9%的训练准确率。让我们来评估我们的模型—</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="8a68" class="lx ly iq lt b gy lz ma l mb mc">yhat = np.where(model.predict({ 'input_ids' : X_test_seq['input_ids'] , 'input_mask' : X_test_seq['attention_mask']}) &gt;=0.5,1,0)</span><span id="3dea" class="lx ly iq lt b gy md ma l mb mc">print(classification_report(y_test,yhat))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/d3ebdd5bf8ddf7fbf4b0063c628f66a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*nvTupjFH05ElPov_NeS0Mw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">来源:产出</figcaption></figure><p id="cd70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，即使在评估数据上，我们的模型也表现得非常好。</p><p id="c5e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用这个模型，甚至评估我们看到的任何日常新闻。</p><p id="92a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就这些了，朋友们……要关注我的内容和了解我，请参考我下面的文章——</p><div class="mj mk gp gr ml mm"><a href="https://muttinenisairohith.medium.com/medium-partner-program-and-my-next-journey-8951cbd1650" rel="noopener follow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd ir gy z fp mr fr fs ms fu fw ip bi translated">中型合作伙伴计划和我的下一次旅程</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">嗨，伙计们，</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">muttinenisairohith.medium.com</p></div></div><div class="mv l"><div class="oc l mx my mz mv na kp mm"/></div></div></a></div><p id="d63c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">快乐编码…</p></div></div>    
</body>
</html>