<html>
<head>
<title>[2019-CVPR] D2-Net: Matching Problem among Images Under Extreme Appearance Changes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[2019-CVPR] D2网:极端外观变化下图像间的匹配问题</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/d2-net-matching-problem-among-images-under-an-extreme-appearance-changes-9f059f33a2ef?source=collection_archive---------1-----------------------#2020-01-17">https://pub.towardsai.net/d2-net-matching-problem-among-images-under-an-extreme-appearance-changes-9f059f33a2ef?source=collection_archive---------1-----------------------#2020-01-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/ca649db0ee821e95f25815f266434998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YTRPXUakFV6cA92PwHYjbw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">在穹顶上使用计算机视觉。</figcaption></figure><div class=""/><p id="5bdc" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">本文基于2019年在CVPR发表的论文'<strong class="ke jg"> D2网络:一个可训练的联合描述和检测局部特征的CNN，</strong>'。如需进一步阅读，请参考https://arxiv.org/pdf/1905.03561.pdf<a class="ae la" href="https://arxiv.org/pdf/1905.03561.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="51aa" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">本文介绍了一种解决<strong class="ke jg">极端条件变化</strong>中匹配问题的最新方法，特别是在特定数据集上。在具有挑战性的条件下创建通用的匹配算法是一个具有挑战性的问题。这篇文章总结了这篇论文，以便你能容易地理解这篇论文的基本概念。</p><h1 id="7e15" class="lb lc jf bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">匹配问题是什么？</h1><p id="43fd" class="pw-post-body-paragraph kc kd jf ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">匹配问题是寻找两幅图像之间对应关系的一般方法。对于人类方面，识别两幅图像之间的相同点是一个容易的问题。然而，对于计算机方面，由于计算机将图像识别为像素阵列，因此匹配问题相当具有挑战性。</p><p id="b76e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">与匹配问题相关的研究的主要贡献是<strong class="ke jg">找到一种提取两幅图像之间对应关系的算法。</strong>下图显示了匹配问题的一个例子。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/f8b4e4a8b9e4437e8c61b34fcb8be4d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*Wtn3a0TVerqIOZeznbYpxg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">昼夜差异下的匹配问题</figcaption></figure><p id="49e1" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">匹配问题受到<strong class="ke jg"> SIFT(尺度不变特征变换)</strong> (2004)的强烈影响，这是长期以来最先进的特征提取。在深度学习一代之后，研究人员发现了大量的特征提取方法。因此，到目前为止，匹配问题是可以在计算机视觉领域探索的。</p><h1 id="04e8" class="lb lc jf bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">为什么需要这项研究？</h1><p id="e21a" class="pw-post-body-paragraph kc kd jf ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">先前的工作发现了许多很好地解决匹配问题的方法。然而，这些方法在具有挑战性的条件下缺乏鲁棒性。这项研究侧重于各种图像条件下的像素级对应。条件可以是昼夜差异、季节变化和弱纹理场景。</p><p id="3eb8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">先前的工作区分了特征检测器和特征描述符，因此它被称为<strong class="ke jg">检测然后描述</strong>方法。他们首先检测图像中的特征，并在检测到的关键点周围创建补丁。然后，使用特征描述符，指定特征，例如生成一个N维向量。然而，这种方法有一个限制:局部描述符考虑更大的块并可能编码更高级的结构，关键点检测器仅发现小的图像区域。因此，这些方法在极端的外观变化中遭受显著的性能下降。</p><p id="5b5f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这项研究提出了一种<strong class="ke jg">描述和检测</strong>的方法:<strong class="ke jg">不是在低级信息的早期执行特征检测，而是同时创建特征检测器和特征描述符</strong>。它使用CNN架构生成特征地图，其主干是<strong class="ke jg"> VGGNet </strong>。(<strong class="ke jg"> VGGNe </strong> t比<strong class="ke jg"> ResNet </strong>更具有光照不变性)</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/1bc40d7d9d094032b1ec16d6097b9989.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*-pgGWfM-a-vKnf86ILsimQ.png"/></div></figure><h1 id="5a60" class="lb lc jf bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">特征抽出</h1><p id="2d1c" class="pw-post-body-paragraph kc kd jf ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">如上所述，D2网络使用CNN层进行特征提取。关于实现细节，作者建议您阅读原文。它最终输出一个三维张量f。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/d96a610fe919e4f7198ce1ad68a4aeca.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/0*J_JReNkV6AJZMvGQ.png"/></div></figure><p id="027e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">h，w是特征图的空间分辨率，n是通道的数量。那么(I，j)中的每个向量被用作这种位置的描述符。他们将描述符表示为d。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/e4d4b98d7b0641656f82ce4ec1da67f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/0*hgajHu7a5nAQ5BF9.png"/></div></figure><p id="b374" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在训练阶段，<strong class="ke jg">无论极端条件如何变化，这些描述符都会为相同的场景产生相似的值。</strong></p><p id="9663" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">每个通道都是特征的检测器。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/2530d21ed500be0c40b618591bb8fc96.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/0*JSUruWMXNIaqpecr.png"/></div></figure><p id="2b45" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当且仅当(I，j)必须是通道k中的局部最大值，即张量f中通道中位置(I，j)处的最大值时，点(I，j)被检测。</p><h1 id="3422" class="lb lc jf bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">如何设计损失函数</h1><p id="97d8" class="pw-post-body-paragraph kc kd jf ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">要实现这个目标，设计一个损失函数是最关键的部分。当我们的必要条件不满足时，损失函数必须是高值，当我们的必要条件满足时，损失函数必须几乎为零。我们可以总结必须满足的条件。</p><p id="0f8e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">(1)检测点在其邻域内趋于局部最大值。</p><p id="8733" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">(2)在不同通道的同一位置，检测点往往最大。</p><p id="2d57" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">(3)对应点会相似。即对应点将具有较小的欧几里德距离。</p><p id="52fc" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">(4)对应点的邻居不会相似。即，对应的邻域将具有高欧几里德距离。</p><p id="2044" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，网络对损失函数使用检测+描述度量。</p><h2 id="3ce7" class="mn lc jf bd ld mo mp dn lh mq mr dp ll kn ms mt lp kr mu mv lt kv mw mx lx my bi translated">软特征检测</h2><p id="c3d0" class="pw-post-body-paragraph kc kd jf ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">为了使网络端到端，它使用软局部最大值，这是可区分的。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/c8893d7594b93eac102c31cd00680ee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*GLr8tnZzDS0TdjF5VguATg.png"/></div></figure><p id="6330" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">软局部最大值正在计算<strong class="ke jg">它在它的邻居</strong>中有多少区别。因为它与邻居相比是有区别的，所以它返回较大的值。但是，如果它的区分度不够，它将返回较小的值。使用的下一个指标如下。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi na"><img src="../Images/74567c1f29d5684ac239aadec164bacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*xecc9KLVrI5fxj-Ch1WSxg.png"/></div></figure><p id="87c3" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该指标使用最大比率法。正在计算<strong class="ke jg">是否接近不同通道</strong>中相同位置的最大值。然后将其最大化以获得单个得分图。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f74b27ea6617fc962e6fade49fcec6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*6w6qQAl7rcKYJ--sYa4dkw.png"/></div></figure><p id="8696" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后通过图像级归一化来完成。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/4b2a2b5aeb37d1466e4adeabc284d532.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*cCa-PAyZ1usyGBS7JZm9kw.png"/></div></figure><p id="9962" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">然后，当满足上面介绍的条件(1)和(2)时，这个分数将会很高。</p><h2 id="3444" class="mn lc jf bd ld mo mp dn lh mq mr dp ll kn ms mt lp kr mu mv lt kv mw mx lx my bi translated">描述符距离</h2><p id="8ec6" class="pw-post-body-paragraph kc kd jf ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">有两个距离，这将是主要的:正描述符p和负描述符n。让我们将c表示为像素A和像素b之间的对应关系。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/66b4abb8d0b6ee8824877f900128dd46.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*gfR2R_2xDSl4CuVBsNgEkg.png"/></div></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/0217b6ea51c611ad7794f7dd2ef79df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*z7K2SdJDT1iKynd6UG4TuA.png"/></div></figure><p id="65ba" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">正描述符p估计像素A和像素B之间的相似性。要让对应c更自信，分数一定要足够小。<strong class="ke jg">负距离n计算像素A和像素B的邻居之间的相似度，也计算像素B和像素A的邻居之间的相似度</strong>。然而，如果相邻像素是相邻像素，匹配问题是一个极具挑战性的问题。因此，本文提供了一个超参数K作为公差来缓解匹配问题。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/3b049c986ddd128d00d1e8958db14ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*OjpVjVIyBXd0-7y18jux0g.png"/></div></figure><p id="1d2c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后，将<strong class="ke jg">裕度函数</strong>设计为</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7cdd238373375e5d8f0a058af53c457a.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*a7_bMdzTP_FvJZBWo-uf2g.png"/></div></figure><p id="e690" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该余量满足要求(3)和(4)。<strong class="ke jg">因为错误的对应会导致更高的p和更低的n，所以这将是一个非常好的对应度量</strong>。最后，损失函数被设计为边缘函数和图像级归一化分数的组合。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a0b18754871e1ad3e9c1451a9d507e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*gdFRwEF2bjta_mfrtUqnvQ.png"/></div></figure><h1 id="f421" class="lb lc jf bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结果</h1><p id="7114" class="pw-post-body-paragraph kc kd jf ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">引入的评估协议是仅当通过单应性重新投影的点在阈值内时才认为匹配是正确的。实验在不同的阈值下进行，以检查性能趋势。</p><ol class=""><li id="f44d" class="ni nj jf ke b kf kg kj kk kn nk kr nl kv nm kz nn no np nq bi translated">与以前的匹配算法相比，<strong class="ke jg">在定位精度方面有较低的性能，而阈值</strong>很紧。实验是在HPatches数据集上进行的，h patches数据集是匹配问题的be基准。由于CNN同时执行描述符和检测器，因此个人性能可能会下降。检测器的后一种使用错过了低级的斑点状结构，该结构比高级特征定位得更好。</li><li id="2590" class="ni nj jf ke b kf nr kj ns kn nt kr nu kv nv kz nn no np nq bi translated">然而，由于阈值宽松(6.5像素或更高)，D2网络优于其他匹配算法。如下图所示，<strong class="ke jg">对光照变化和视点变化不变</strong>。</li></ol><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/baa4fb52dd75f8a617b166c063e30a95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EvQlhWSdad4He2tnYFpEGA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">基于HPatches图像对的评估</figcaption></figure><p id="41fd" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">3.即使D2网在严格的阈值下具有较低的性能，<strong class="ke jg">它足以构建3D重建(基于图像构建3D模型)</strong>。作者用了‘马德里大都会；、“Gendramenmarkt”和“伦敦塔”数据集。尽管D2网络与以前的作品相比准确性有所下降，但这并不是一个显著的下降。性能下降是因为3D重建任务需要在图像匹配中失败的良好定位的特征。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a6692ed17ef15bf2d307e93495d8e156.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*7GWy42TVBLzaoCz04meMsw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">对局部特征评估基准的评估</figcaption></figure><p id="d15d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">4.本文还检查了挑战性条件下的本地化性能。<strong class="ke jg"> D2网络在“亚琛昼夜”数据集和“因洛克”数据集上都优于本地化问题</strong></p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/bfae453c592022df35c6f0d511f0376b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qu02o7o-BDw4UU-OpVXp9w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">对亚琛昼夜数据集的评估</figcaption></figure><h1 id="1759" class="lb lc jf bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="9aff" class="pw-post-body-paragraph kc kd jf ke b kf lz kh ki kj ma kl km kn mb kp kq kr mc kt ku kv md kx ky kz ij bi translated">描述和检测方法在定位问题上是最先进的。然而，它在图像匹配问题和三维重建问题上仍然有局限性。这一研究是有意义的，因为它为匹配问题提供了一种新的方法。</p><p id="910e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">联系方式:给我发邮件</p><p id="6c16" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">jeongyw12382@postech.ac.kr</p></div></div>    
</body>
</html>