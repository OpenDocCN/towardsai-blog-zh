<html>
<head>
<title>AI Anyone Can Understand: Part 7 - Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">任何人都能理解的人工智能:第7部分-Q-学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/ai-anyone-can-understand-part-7-q-learning-f9b12300544b?source=collection_archive---------2-----------------------#2022-12-25">https://pub.towardsai.net/ai-anyone-can-understand-part-7-q-learning-f9b12300544b?source=collection_archive---------2-----------------------#2022-12-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5451" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">一定要看完剩下的</em> <a class="ae kj" href="https://medium.com/@MoneyAndData/list/ai-for-anyone-to-understand-a5db7a67e622" rel="noopener"> AI任何人都能看懂</a> <em class="ki">系列</em></h2></div><h1 id="4ba2" class="kk kl it bd km kn ko kp kq kr ks kt ku jz kv ka kw kc kx kd ky kf kz kg la lb bi translated">简化解释</h1><p id="40e6" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">Q-learning是AI通过试错来学习的一种方式。它尝试不同的行动，看看它们产生了什么结果，然后从结果中学习，找出在不同情况下采取哪些行动是最好的。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/0209435acf272c3e4931d8af92264c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hTUqoDALYqJT77Rk"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">照片由<a class="ae kj" href="https://unsplash.com/@leekos?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kostiantyn Li </a>在<a class="ae kj" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="3f60" class="kk kl it bd km kn mv kp kq kr mw kt ku jz mx ka kw kc my kd ky kf mz kg la lb bi translated">深入解释</h1><p id="7eb7" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">Q-learning是一种流行的强化学习算法，常用于人工智能(AI)应用中。简单来说，强化学习是一种机器学习，它涉及训练代理在一个环境中采取行动，以最大化回报。</p><p id="fc64" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">Q-learning是一种强化学习，它使用Q表来存储在给定状态下要采取的最佳行动的信息。Q表最初是空的，随着代理经历新的状态并接收奖励，Q学习算法随时间更新该表。</p><p id="65af" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">Q-learning的一个关键方面是它基于贝尔曼方程的概念，即一个状态的期望值是当前奖励加上下一个状态的贴现值的总和。这意味着Q表存储了在给定状态下采取特定行动的预期长期回报。</p><p id="25a3" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">在实践中，Q-learning可以用来训练AI智能体玩游戏，导航迷宫，或根据环境做出其他决定。例如，一个Q-learning智能体在玩国际象棋游戏时，最初可能会随机移动，但随着时间的推移，它会学习哪些移动最有可能导致胜利，并相应地调整其行为。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nf"><img src="../Images/ef86a00c02eaa45dfe01f0779e158af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ntfTOISDPNjAyXof"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae kj" href="https://unsplash.com/es/@charlesdeluvio?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> charlesdeluvio </a>在<a class="ae kj" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="be92" class="ng kl it bd km nh ni dn kq nj nk dp ku ll nl nm kw lp nn no ky lt np nq la nr bi translated">优势</h2><p id="fb11" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">Q-learning的主要优点之一是它可以用来解决大型或连续状态空间的问题。这是因为Q表是增量更新的，而不是每次都从头开始重新计算。这允许Q学习算法有效地从经验中学习并适应变化的环境。</p><p id="638d" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">总的来说，Q-learning是人工智能研究和应用的强大工具，它将继续成为研究和开发的活跃领域。随着人工智能技术的不断进步，我们可以期待在各个领域看到越来越多的Q-learning应用，从游戏和机器人到医疗保健和金融。</p><p id="0d5d" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">Q-learning的另一个重要方面是，它可以与深度学习等其他技术相结合，以创建更强大的人工智能系统。例如，深度Q学习算法使用神经网络来逼近Q表，允许它处理更大和更复杂的状态空间。</p><h2 id="dcfb" class="ng kl it bd km nh ni dn kq nj nk dp ku ll nl nm kw lp nn no ky lt np nq la nr bi translated">限制</h2><p id="8785" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">Q-learning的一个潜在限制是，它可能需要很长时间才能收敛到最优解，尤其是在复杂的环境中。这使得Q学习难以应用于需要快速决策的实时应用。然而，深度学习和并行计算等技术的最新进展有助于克服这一挑战，使Q-学习在许多应用中更加实用。</p><p id="0c6a" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">尽管有其局限性，Q-learning仍然是一种广泛使用的有效的强化学习算法，它仍然是人工智能领域的一个重要研究领域。随着我们继续开发更先进的人工智能技术，我们可以期待在未来看到更多令人兴奋的Q-learning应用。</p><p id="8d79" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">Q-learning可以<strong class="le iu">用于解决广泛的问题</strong>，从玩游戏到在复杂的环境中做出决策。它可以与深度学习等其他技术相结合，创建更强大的人工智能系统。虽然Q-learning有一些局限性，但人工智能技术的最新进展已经帮助克服了许多挑战。</p><p id="52a2" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">总之，Q-learning是一种广泛应用于人工智能应用的鲁棒强化学习算法。它基于贝尔曼方程的概念，即一个状态的期望值是当前回报加上下一个状态的贴现值之和。这使得Q-learning能够有效地从经验中学习，并适应不断变化的环境。</p><h1 id="5721" class="kk kl it bd km kn ko kp kq kr ks kt ku jz kv ka kw kc kx kd ky kf kz kg la lb bi translated">行动中的学习</h1><pre class="lz ma mb mc gt ns nt nu bn nv nw bi"><span id="adea" class="nx kl it nt b be ny nz l oa ob">import numpy as np<br/><br/># Initialize Q-table with all zeros<br/>Q = np.zeros((6,6))<br/><br/># Set learning rate and discount factor<br/>alpha = 0.1<br/>gamma = 0.9<br/><br/># Define the environment<br/># The agent starts in state (2, 2) and the goal is to reach state (5, 5)<br/># The possible actions are up, down, left, and right<br/>def get_possible_actions(state):<br/>  actions = []<br/>  if state[0] &gt; 0:<br/>    actions.append("up")<br/>  if state[0] &lt; 5:<br/>    actions.append("down")<br/>  if state[1] &gt; 0:<br/>    actions.append("left")<br/>  if state[1] &lt; 5:<br/>    actions.append("right")<br/>  return actions<br/><br/># Define the reward function<br/># The reward is -1 for all non-goal states, and 0 for the goal state<br/>def get_reward(state):<br/>  if state == (5,5):<br/>    return 0<br/>  else:<br/>    return -1<br/><br/># Define the function to update the Q-table<br/>def update_Q(state, action, next_state):<br/>  Q[state[0]][state[1]] = (1 - alpha) * Q[state[0]][state[1]] + alpha * (get_reward(next_state) + gamma * max(Q[next_state[0]][next_state[1]]))<br/><br/># Set the starting state<br/>state = (2,2)<br/><br/># Train the agent<br/>while state != (5,5):<br/>  # Get the possible actions<br/>  actions = get_possible_actions(state)<br/><br/>  # Choose a random action<br/>  action = np.random.choice(actions)<br/><br/>  # Take the action and get the next state<br/>  if action == "up":<br/>    next_state = (state[0] - 1, state[1])<br/>  elif action == "down":<br/>    next_state = (state[0] + 1, state[1])<br/>  elif action == "left":<br/>    next_state = (state[0], state[1] - 1)<br/>  elif action == "right":<br/>    next_state = (state[0], state[1] + 1)<br/><br/>  # Update the Q-table<br/>  update_Q(state, action, next_state)<br/><br/>  # Set the next state as the current state<br/>  state = next_state<br/><br/># Print the final Q-table<br/>print(Q)</span></pre><p id="fb87" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">在这个例子中，代理从状态(2，2)开始，目标是通过向上、向下、向左或向右移动到达状态(5，5)。Q表用全零初始化，代理在采取行动和接收奖励时更新该表。训练后，Q表将包含在每个状态下采取每个行动的预期长期回报。</p><h2 id="484b" class="ng kl it bd km nh ni dn kq nj nk dp ku ll nl nm kw lp nn no ky lt np nq la nr bi translated">初学者理解代码</h2><ol class=""><li id="47f4" class="oc od it le b lf lg li lj ll oe lp of lt og lx oh oi oj ok bi translated">该代码导入了NumPy库，该库用于处理数组和矩阵。</li><li id="762a" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">Q表被初始化为一个6x6的零矩阵，它将用于存储在每个状态下采取每个行动的预期长期回报。</li><li id="7986" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">学习率和折扣因子分别设置为0.1和0.9。学习率决定了Q表基于新的体验更新了多少，折扣因子决定了与即时奖励相比，代理人对未来奖励的重视程度。</li><li id="7c43" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">定义了环境，包括起始状态(2，2)、目标状态(5，5)和可能的动作(上、下、左、右)。</li><li id="f651" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">定义了奖励函数，对于所有非目标状态返回-1，对于目标状态返回0。这定义了代理因采取不同行动而获得的奖励。</li><li id="de4d" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">定义了更新Q表的函数。该函数将当前状态、动作和下一个状态作为输入，并基于贝尔曼方程更新Q表。</li><li id="bb14" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">起始状态设置为(2，2)，代理开始训练。</li><li id="81d8" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">在每次迭代中，代理获得可能的动作，选择一个随机动作，采取动作并获得下一个状态，更新Q表，并将下一个状态设置为当前状态。这个过程一直持续到达到目标状态。</li><li id="82df" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">训练结束后，打印出最终的Q表。此表包含在每个状态下采取每个行动的预期长期回报。</li></ol><p id="0cb1" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">总的来说，这段代码实现了一个简单的Q-learning算法来训练代理在网格中导航并到达目标状态。当代理经历新的状态并收到奖励时，它更新它的Q表，并且它学习采取最有可能导致最高奖励的行动。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="601f" class="kk kl it bd km kn mv kp kq kr mw kt ku jz mx ka kw kc my kd ky kf mz kg la lb bi translated">关键要点</h1><ol class=""><li id="fb7b" class="oc od it le b lf lg li lj ll oe lp of lt og lx oh oi oj ok bi translated">Q-learning是一种流行的强化学习算法，常用于人工智能(AI)应用中。</li><li id="28ce" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">Q-learning基于贝尔曼方程的概念，即一个状态的期望值是当前奖励加上下一个状态的贴现值之和。</li><li id="66a3" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">Q-learning可以用来训练人工智能代理玩游戏，导航迷宫，或根据其环境做出其他决定。</li><li id="7add" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">Q-learning可以与深度学习等其他技术结合起来，创造出更强大的人工智能系统。</li><li id="784a" class="oc od it le b lf ol li om ll on lp oo lt op lx oh oi oj ok bi translated">尽管有其局限性，Q-learning仍然是一种广泛使用的有效的强化学习算法，它仍然是人工智能领域的一个重要研究领域。</li></ol></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="5768" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated"><strong class="le iu">围绕让人工智能更容易理解，本系列还有更多的工作要做</strong></p><div class="oq or gp gr os"><div role="button" tabindex="0" class="ab bv gv cb fp ot ou bn ov mi ex"><div class="ow l"><div class="ab q"><div class="l di"><img alt="Andrew Austin" class="l de bw ox oy fe" src="../Images/a82df955f82ae8c27d119bf62316d7f9.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*lRR7VCX0kB9yC2fWlSs1ug.jpeg"/><div class="fb bw l ox oy fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://medium.com/@MoneyAndData?source=post_page-----f9b12300544b--------------------------------" rel="noopener follow" target="_top">安德鲁·奥斯</a></p></div></div><div class="pb pc gw l"><h2 class="bd iu ut oc fp uu fr fs qb fu fw is bi translated">AI任何人都能理解</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi uv au uw ux uy rl uz an eh ei va vb vc el em eo de bk ep" href="https://medium.com/@MoneyAndData/list/ai-anyone-can-understand-a5db7a67e622?source=post_page-----f9b12300544b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="vd l fo"><span class="bd b dl z dk">12 stories</span></div></div></div><div class="po dh pp fp ab pq fo di"><div class="di pg bv ph pi"><div class="dh l"><img alt="" class="dh" src="../Images/b3a871e88496546e986d1231e1e8f370.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*BMLu-4KKjLNJV9pPYSb8Ew.gif"/></div></div><div class="di pg bv pj pk pl"><div class="dh l"><img alt="" class="dh" src="../Images/bdbd8bc236cbfcdb89dd3481be28b1a4.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*DqboPr-gLhk_8lvcFKGhlA.gif"/></div></div><div class="di bv pm pn pl"><div class="dh l"><img alt="" class="dh" src="../Images/c6534ce4e3c7e62803e15d000f01bbd5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*Addp2SvtRYSGP0W5"/></div></div></div></div></div></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><div class="lz ma mb mc gt pv"><a href="https://medium.com/@MoneyAndData/subscribe" rel="noopener follow" target="_blank"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">每当安德鲁·奥斯汀发表文章时，就收到一封电子邮件。</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">每当安德鲁·奥斯汀发表文章时，就收到一封电子邮件。通过注册，您将创建一个中型帐户，如果您还没有…</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">medium.com</p></div></div><div class="qe l"><div class="qf l qg qh qi qe qj mi pv"/></div></div></a></div><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/67e5d34422cb6fb849bd3f591a191db6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*WnS6YPZ-ogxzdNrGiagNcA.png"/></div></figure><p id="0d7f" class="pw-post-body-paragraph lc ld it le b lf na ju lh li nb jx lk ll nc ln lo lp nd lr ls lt ne lv lw lx im bi translated">如果你开始爱上Medium来寻找一个好的内容来源，你也可以通过我的推荐链接得到一个Medium来支持我:<a class="ae kj" href="https://medium.com/@MoneyAndData/membership" rel="noopener">https://medium.com/@MoneyAndData/membership</a></p></div></div>    
</body>
</html>