<html>
<head>
<title>Speech Emotion Recognition (SER)Using CNN And LSTMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于CNN和LSTMs的语音情感识别</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/speech-emotion-recognition-ser-using-cnn-and-lstms-4a5dc4c314fd?source=collection_archive---------1-----------------------#2021-03-15">https://pub.towardsai.net/speech-emotion-recognition-ser-using-cnn-and-lstms-4a5dc4c314fd?source=collection_archive---------1-----------------------#2021-03-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e1bb" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="2f96" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过言语表达的情感对人类的行为和推理有着额外的洞察力。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/df7dfae1624d570585906fe7d5c9243a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZ4OUd5qJeodR-Z2Dh0uAg.jpeg"/></div></div></figure><p id="d449" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">情绪是人类心理的基本组成部分，直接转化为人类的行动。人类的声音是一种能反映这些情绪的神奇工具。通过言语表达的情感对人类的行为和推理有着额外的洞察力。深入研究这些关系，可以帮助我们更好地理解人的动机。因此，情感识别在人机交互中起着重要的作用。</p><p id="327c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我对这个主题的兴趣促使我创建了一个可以帮助分类人类基本情绪的模型。在这篇文章中，我将分享我是如何做到的。</p><p id="2a45" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该模型在来自Ryerson情感语音和歌曲视听数据库(RAVDESS)数据集的英语语言数据集上创建。基于最近的研究，Mel-Spectrogram有助于从音频数据中提取重要的特征，这些特征被用于CNN+LSTM模型。</p><p id="4c0e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我把我所有的代码都保存在了GitHub、【https://github.com/msaleem18/Speech_Emotion_Recognition T4】上</p><h1 id="fcdb" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">资料组</h1><p id="6659" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">对于我的模型，我使用了以下数据集:</p><div class="mx my gp gr mz na"><a href="https://zenodo.org/record/1188976#.Xo0dCFNKjOS" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd jd gy z fp nf fr fs ng fu fw jc bi translated">瑞尔森情感语音和歌曲视听数据库</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">引用ravdes ravdes是在知识共享署名许可下发布的，所以如果…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">zenodo.org</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no lb na"/></div></div></a></div><p id="75c5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了在Python中读取和处理音频数据，我使用了Librosa库；最终数据存储为NumPy数组。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="c2ec" class="nu mb it nq b gy nv nw l nx ny"><strong class="nq jd">import</strong> <strong class="nq jd">numpy</strong> <strong class="nq jd">as</strong> <strong class="nq jd">np</strong><br/><strong class="nq jd">import</strong> <strong class="nq jd">pandas</strong> <strong class="nq jd">as</strong> <strong class="nq jd">pd</strong><br/><strong class="nq jd">import</strong> <strong class="nq jd">librosa</strong> <strong class="nq jd">as</strong> <strong class="nq jd">lib</strong><br/><strong class="nq jd">import</strong> <strong class="nq jd">librosa.display</strong></span><span id="5721" class="nu mb it nq b gy nz nw l nx ny">path = "/Users/saad/Saad/Education/Ryerson/MRP/Dataset/Audio_Speech_Actors_01-24/ALL"</span><span id="d4fe" class="nu mb it nq b gy nz nw l nx ny"><em class="oa">#READ ENGLISH FILES</em><br/><br/>files = []<br/>modality =[]<br/>vocal =[]<br/>emotion =[]<br/>intensity =[]<br/>statement =[]<br/>repetition =[]<br/>actor =[]<br/>gender = []<br/>time = []<br/>audio_data = []<br/>sr = []<br/><br/>max_row = 0<br/>max_col = 0<br/>min_row = 1000<br/>min_col = 1000<br/><br/>n_fft = 2048<br/>hop_length = 512<br/>n_mels = 200<br/><br/><strong class="nq jd">for</strong> file_name <strong class="nq jd">in</strong> file_list:<br/>    file_path = path+'/'+file_name<br/>    mod, voc, emo, inten, state, repe, act = file_name.split('-')<br/>    act = act.split('.')[0]<br/>    <br/>    <strong class="nq jd">if</strong> emo != '02':<br/>        <em class="oa">#store metadata</em><br/>        files.append(file_name)<br/>        modality.append(mod)<br/>        vocal.append(voc)<br/>        <em class="oa">#emotion.append(emo)</em><br/>        intensity.append(inten)<br/>        statement.append(state)<br/>        repetition.append(repe)<br/>        actor.append(act)<br/><br/>        <strong class="nq jd">if</strong> (emo == '01'):<br/>            emotion.append('neutral')<br/>        <strong class="nq jd">elif</strong> (emo == '03'):<br/>            emotion.append('happy')<br/>        <strong class="nq jd">elif</strong> (emo == '04'):<br/>            emotion.append('sad')<br/>        <strong class="nq jd">elif</strong> (emo == '05'):<br/>            emotion.append('angry')<br/>        <strong class="nq jd">elif</strong> (emo == '06'):<br/>            emotion.append('fearful')<br/>        <strong class="nq jd">elif</strong> (emo == '07'):<br/>            emotion.append('disgust')<br/>        <strong class="nq jd">elif</strong> (emo == '08'):<br/>            emotion.append('surprised')<br/><br/>        <strong class="nq jd">if</strong> (int(act) % 2 == 0):<br/>            gender.append(1) <em class="oa">#female</em><br/>        <strong class="nq jd">else</strong>:<br/>            gender.append(2) <em class="oa">#male</em><br/><br/>        audio, sfreq = lib.load(file_path, sr=44100,offset=0.5,duration = 3.5)<br/>        time_line = np.arange(0,len(audio)) / sfreq<br/>        time.append(time_line)<br/>        audio_data.append(audio)<br/>        sr.append(sfreq)<br/><br/>        <em class="oa">#mfccs = lib.feature.mfcc(y=audio,sr=sfreq,n_mfcc=25)</em><br/>        mfccs = librosa.feature.mfcc(y=audio, sr=sfreq, n_mfcc=13)<br/>        mel_spec = lib.feature.melspectrogram(audio, sr=sfreq, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)<br/>        S_DB = lib.power_to_db(mel_spec, ref=np.max)<br/><br/>        <strong class="nq jd">if</strong> (S_DB.shape[0] &gt; max_row):<br/>            max_row = S_DB.shape[0]<br/><br/>        <strong class="nq jd">if</strong> (S_DB.shape[1] &gt; max_col):<br/>            max_col = S_DB.shape[1]<br/><br/>        <strong class="nq jd">if</strong> (S_DB.shape[0] &lt; min_row):<br/>            min_row = S_DB.shape[0]<br/><br/>        <strong class="nq jd">if</strong> (S_DB.shape[1] &lt; min_col):<br/>            min_col = S_DB.shape[1]<br/>            <br/>        <br/>print("DONE !")</span></pre><h1 id="5795" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">SER的研究现状</h1><p id="8c90" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">语音情感识别(SER)已经存在了二十多年，尽管它有许多应用，但SER仍然是一项具有挑战性的任务，主要因为情感是主观的。对于我们如何对不同的人类情感进行分类，几乎没有共识。akay和团队在提供SER系统概述方面做得非常出色，如图1-SER系统概述所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/224a76c8da73519141fadf84fd017a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJNrNvhOtE_LdM5hy7X84w.png"/></div></div><figcaption class="oc od gj gh gi oe of bd b be z dk translated">阿卡伊，文学学士(2020年)</figcaption></figure><h1 id="11dd" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">探索性数据分析</h1><p id="2737" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">该数据集包含7356个文件，其中24个专业演员(12个女性，12个男性)正在用中性北美口音发出两个词汇匹配的语句。语音包括平静、快乐、悲伤、愤怒、恐惧、惊讶和厌恶的表情，歌曲包含平静、快乐、悲伤、愤怒和恐惧的情绪。</p><p id="9f9a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于数据集包含音频文件，大多数探索性数据分析都是围绕波形和梅尔频率倒谱系数(MFCC)进行的。</p><p id="bdbb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有24个演员，每个人记录每一种情绪，除了中性情绪，有两种不同的声音强度，正常和强烈。所以除了中性情绪，我们每个情绪有48个文件。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/b0177a015b8e9ce999492c53005ef76e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2FasQzLkEoRQ9OWRi-jsA.png"/></div></div><figcaption class="oc od gj gh gi oe of bd b be z dk translated">情感的文件计数</figcaption></figure><p id="0687" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以用多种方法来分析音频数据，我从分析每种情绪的波形开始。我随机选择了一个正常强度的男演员来研究每种情绪的波形会有什么不同。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/ab737039924eb6b4ba9b5c96e77f89b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*Y2RHTxMRgf5WQrupK90g5A.png"/></div></div><figcaption class="oc od gj gh gi oe of bd b be z dk translated">基本情绪的波形</figcaption></figure><p id="056b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有趣的是，每个波形之间都有细微的差别；一些在振幅上变化，一些在频率(1 /时间)上变化，一些在波的整体形状上变化。</p><p id="9a47" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">理解人类发出的声音被包括舌头和牙齿在内的声道的形状过滤是很重要的。如果我们能准确地确定这种形状，这将会给我们一个正在产生的音素的准确表示。声道的形状在短时功率谱的包络中表现出来，而MFCCs的工作就是精确地表示这个包络。MFCC特征表示语音信号的短时功率谱。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/246e67e5d87100452f206d943093efd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rWfdLg1YN2ed2KTPTBmYrg.png"/></div></div><figcaption class="oc od gj gh gi oe of bd b be z dk translated">比较厌恶和惊讶情绪</figcaption></figure><p id="570a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以下代码用于创建上述图形:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="aea3" class="nu mb it nq b gy nv nw l nx ny"><em class="oa">#ANALYZING WAVEFORM</em><br/>file_path0 = path+'/'+files[0]<br/><br/>X0, sample_rate0 = librosa.load(file_path0)<br/><br/>mfcc0 = librosa.feature.mfcc(y=X0, sr=sample_rate0, n_mfcc=13)<br/><br/><em class="oa">#n_fft = 2048</em><br/><em class="oa">#hop_length = 512</em><br/><em class="oa">#n_mels = 128</em><br/><br/>S0 = librosa.feature.melspectrogram(X0, sr=sample_rate0, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)<br/>S_DB0 = librosa.power_to_db(S0, ref=np.max)<br/><br/><em class="oa"># audio wave</em><br/>plt.figure(figsize=(15, 20))<br/><br/>plt.subplot(4,1,1)<br/><em class="oa">#plt.legend(loc="upper right")</em><br/>plt.ylabel('Disgust - Amplitude')<br/>librosa.display.waveplot(X0, sr=sample_rate0)<br/><em class="oa">#plt.title('Audio sampled at 44100 hrz')</em><br/><br/>plt.subplot(4,1,2)<br/><em class="oa">#plt.legend(loc="upper right")</em><br/>librosa.display.specshow(mfcc0, x_axis='time')<br/>plt.colorbar()<br/>plt.ylabel('Disgust - MFCC')<br/><br/>plt.subplot(4,1,3)<br/>librosa.display.specshow(S_DB0, sr=sample_rate0, hop_length=hop_length, x_axis='time', y_axis='mel');<br/>plt.colorbar(format='<strong class="nq jd">%+2.0f</strong> dB');<br/>plt.ylabel('Disgust - Hz')</span></pre><h1 id="4e21" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">预处理</h1><p id="a6ba" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">预处理是在SER系统中收集用于训练分类器的数据之后的第一步。最近的大多数研究都使用梅尔频率倒谱系数(MFCC)来提取情感特征，然后将其输入分类模型。MFCC特征表示语音信号的短期功率谱，有助于更好地提取重要的音频特征。</p><p id="eacd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我利用Python的Librosa库，使用mel-spectogram函数来提取mel值。提取Mel值后，我对所有值进行了归一化处理，以消除任何可能会使模型产生偏差的极端异常值</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/a149f8596d6fbf899582389ee437cd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wbOi4VJ1ZH-DPfxLMFi7Ag.png"/></div></div><figcaption class="oc od gj gh gi oe of bd b be z dk translated">预处理管道粗流程</figcaption></figure><h2 id="983d" class="nu mb it bd mc ok ol dn mg om on dp mk lm oo op mm lq oq or mo lu os ot mq iz bi translated">列车测试分离</h2><p id="407d" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">数据集被随机分成两组，一组用80%的数据进行训练，另一组用20%的数据进行测试。</p><h2 id="2844" class="nu mb it bd mc ok ol dn mg om on dp mk lm oo op mm lq oq or mo lu os ot mq iz bi translated">数据扩充</h2><p id="8d9a" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">增加数据集的常见做法是使用数据扩充。从现有的训练数据中人工创建新的训练数据。这是通过将特定于领域的技术应用于来自训练数据的示例来实现的，这些示例创建新的不同的训练示例。此过程有助于更好地训练模型，因为它能够更好地概化数据集。</p><p id="3149" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这个项目中，我使用了两种音频数据增强技术:</p><ol class=""><li id="7b5f" class="ou ov it lf b lg lh lj lk lm ow lq ox lu oy ly oz pa pb pc bi translated">添加噪声:使用numpy，我将在音频波形中添加随机噪声</li><li id="5b2f" class="ou ov it lf b lg pd lj pe lm pf lq pg lu ph ly oz pa pb pc bi translated">修改音高:使用Librosa库来修改随机音频文件的音高</li></ol><p id="b493" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以下代码用于预处理音频文件的管道:</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="b20f" class="nu mb it nq b gy nv nw l nx ny"><em class="oa"># EXTRACT ONLY AUDIO DATA AND LABELS FROM DATAFRAME</em><br/><br/>audio_list = []<br/>audio_list_fr = []<br/><br/><strong class="nq jd">for</strong> file_name <strong class="nq jd">in</strong> file_list:<br/>    file_path = path+'/'+file_name<br/>    mod, voc, emo, inten, state, repe, act = file_name.split('-')<br/>    act = act.split('.')[0]<br/>    <br/>    <strong class="nq jd">if</strong> emo != '02':     <br/>        <em class="oa">#print(result.shape)</em><br/>        <em class="oa">#X, sample_rate = librosa.load(path,duration=2.5,sr=44100,offset=0.5)</em><br/>        X, sample_rate = lib.load(file_path,sr=44100,offset=0.5,duration = 3.5)<br/>        audio_list.append(X)<br/>    <br/><strong class="nq jd">for</strong> file_name <strong class="nq jd">in</strong> file_list_fr:<br/>    file_path = path_fr+'/'+file_name<br/>    <br/>    <strong class="nq jd">if</strong> file_name != '.DS_Store':<br/>        <em class="oa">#print(result.shape)</em><br/>        <em class="oa">#X, sample_rate = librosa.load(path,duration=2.5,sr=44100,offset=0.5)</em><br/>        X, sample_rate = lib.load(file_path,sr=44100,offset=0.5,duration = 3.5)<br/>        audio_list_fr.append(X)<br/>    <br/><br/>only_audio_array = np.asarray(audio_list)<br/>only_audio_array_fr = np.asarray(audio_list_fr)<br/><br/><em class="oa">#df_features = df_final.drop(['file_name', 'emotion','intensity','actor'], axis=1)</em><br/>df_label = df_files[['emotion']].copy()<br/>df_label_fr = df_files_fr[['emotion']].copy()<br/><br/>le = preprocessing.LabelEncoder()<br/>df_label.emotion = le.fit_transform(df_label.emotion)<br/>df_label_fr.emotion = le.transform(df_label_fr.emotion)<br/><br/>print(only_audio_array.shape)<br/>le.classes_</span><span id="7bbe" class="nu mb it nq b gy nz nw l nx ny"><em class="oa"># Split between train and test AND PERFORM DATA AUGMENTATION -----------------------------------</em><br/><br/>random_num = 42<br/>X_train, X_test, y_train, y_test = train_test_split((only_audio_array)<br/>                                                    , df_label<br/>                                                    , test_size=0.2<br/>                                                    , shuffle=<strong class="nq jd">True</strong><br/>                                                    , random_state=random_num)<br/>print("FIRST ARRAY SHAPES")<br/>print(X_train.shape)<br/>print(X_test.shape)<br/>print(y_train.shape)<br/>print(y_test.shape)<br/><br/>enc = OneHotEncoder()<br/>label_onehot_train = enc.fit_transform(y_train).toarray()<br/>label_onehot_test = enc.fit_transform(y_test).toarray()<br/>print(enc.categories_)<br/><br/><em class="oa">#AUGMENT DATA WITH NOISE -----------------------------------</em><br/><strong class="nq jd">def</strong> manipulate_noise(data, noise_factor):<br/>    noise = np.random.randn(len(data))<br/>    augmented_data = data + noise_factor * noise<br/>    <em class="oa"># Cast back to same data type</em><br/>    augmented_data = augmented_data.astype(type(data[0]))<br/>    <strong class="nq jd">return</strong> augmented_data<br/><br/>a_list = [1, 2]<br/>distribution = [0.0, 1.0]<br/><br/>manipulated_audio = []<br/>manipulated_onehot_label = []<br/><br/><strong class="nq jd">for</strong> i <strong class="nq jd">in</strong> range(len(X_train)):<br/>    random_number = np.random.choice(a_list, p = distribution)<br/>    <strong class="nq jd">if</strong> random_number == 2:<br/>        data = manipulate_noise(X_train[i],0.008)<br/>        manipulated_audio.append(data)<br/>        manipulated_onehot_label.append(label_onehot_train[i])<br/>        <br/>a_list = [1, 2]<br/>distribution = [0.0, 1.0]<br/><br/>manipulated_audio2 = []<br/>manipulated_onehot_label2 = []<br/><br/><strong class="nq jd">for</strong> i <strong class="nq jd">in</strong> range(len(X_train)):<br/>    random_number = np.random.choice(a_list, p = distribution)<br/>    <strong class="nq jd">if</strong> random_number == 2:<br/>        data = manipulate_noise(X_train[i],0.008)<br/>        manipulated_audio2.append(data)<br/>        manipulated_onehot_label2.append(label_onehot_train[i])<br/>    <br/>    <br/><em class="oa">#AUGMENT DATA WITH PITCH -----------------------------------</em><br/><strong class="nq jd">def</strong> manipulate_pitch(data):<br/>    <strong class="nq jd">return</strong> lib.effects.pitch_shift(data, sr=44100, n_steps=4)<br/>    <br/>a_list = [1, 2]<br/>distribution = [0.1, 0.9]<br/><br/>manipulated_audio_pitch = []<br/>manipulated_onehot_label_pitch = []<br/><br/><em class="oa">#y_train_array = np.array(y_train)</em><br/><strong class="nq jd">for</strong> i <strong class="nq jd">in</strong> range(len(X_train)):<br/>    random_number = np.random.choice(a_list, p = distribution)<br/>    <strong class="nq jd">if</strong> random_number == 2:<br/>        data = manipulate_pitch(X_train[i])<br/>        manipulated_audio_pitch.append(data)<br/>        manipulated_onehot_label_pitch.append(label_onehot_train[i])<br/>    <br/><em class="oa">#CONCATENATE DATA -----------------------------------</em><br/>X_train_manu = np.concatenate((X_train,np.asarray(manipulated_audio),<br/>                               np.asarray(manipulated_audio_pitch),<br/>                              np.asarray(manipulated_audio2)),axis = 0)<br/><br/><em class="oa">#y_train_dup = np.concatenate((y_train,y_train,y_train,y_train),axis = 0)</em><br/>label_onehot_train_manu = np.concatenate((label_onehot_train,np.asarray(manipulated_onehot_label),<br/>                                          np.asarray(manipulated_onehot_label_pitch),<br/>                                         np.asarray(manipulated_onehot_label2)),axis = 0)<br/><br/>print("<strong class="nq jd">\n</strong>CONCAT ARRAY SHAPES")<br/>print(X_train_manu.shape)<br/>print(X_test.shape)<br/>print(label_onehot_train_manu.shape)<br/>print(label_onehot_test.shape)<br/><br/><em class="oa">#CHANGE DATA TO SPECTOGRAM -----------------------------------</em><br/>spectogram_list_train = []<br/><strong class="nq jd">for</strong> item <strong class="nq jd">in</strong> X_train_manu:<br/>    mel_spec = lib.feature.melspectrogram(item, sr=44100, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)<br/>    S_DB = lib.power_to_db(mel_spec, ref=np.max)<br/><br/>    <strong class="nq jd">if</strong> S_DB.shape[1] &lt; max_col:<br/>        result = np.zeros((max_row,max_col - S_DB.shape[1]),dtype=float)<br/>        <em class="oa">#print(mfccs.shape)</em><br/>        <em class="oa">#print(mfccs)</em><br/>        a = np.hstack((S_DB,result))<br/>        spectogram_list_train.append(a)<br/>    <strong class="nq jd">else</strong>:<br/>        spectogram_list_train.append(S_DB)<br/>        <br/>spectogram_array_train = np.asarray(spectogram_list_train)<br/><br/><br/>spectogram_list_test = []<br/><strong class="nq jd">for</strong> item <strong class="nq jd">in</strong> X_test:<br/>    mel_spec = lib.feature.melspectrogram(item, sr=44100, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)<br/>    S_DB = lib.power_to_db(mel_spec, ref=np.max)<br/><br/>    <strong class="nq jd">if</strong> S_DB.shape[1] &lt; max_col:<br/>        result = np.zeros((max_row,max_col - S_DB.shape[1]),dtype=float)<br/>        <em class="oa">#print(mfccs.shape)</em><br/>        <em class="oa">#print(mfccs)</em><br/>        a = np.hstack((S_DB,result))<br/>        spectogram_list_test.append(a)<br/>    <strong class="nq jd">else</strong>:<br/>        spectogram_list_test.append(S_DB)<br/>        <br/>spectogram_array_test = np.asarray(spectogram_list_test)<br/>    <br/><br/><br/><em class="oa">#DO DATA NORMALIZATION -----------------------------------</em><br/>mean = np.mean(spectogram_array_train,axis = 0)<br/>std = np.std(spectogram_array_train,axis = 0)<br/><br/>x_train_temp = spectogram_array_train.reshape(spectogram_array_train.shape[0],spectogram_array_train.shape[1]*spectogram_array_train.shape[2])<br/>x_test_temp = spectogram_array_test.reshape(spectogram_array_test.shape[0],spectogram_array_test.shape[1]*spectogram_array_test.shape[2])<br/><br/>scaler = preprocessing.StandardScaler().fit(x_train_temp)<br/><br/>X_train_norm = scaler.transform(x_train_temp) <em class="oa">#(X_train - mean)#/std</em><br/>X_test_norm = scaler.transform(x_test_temp) <em class="oa">#(X_test - mean)#/std</em><br/><br/>X_train_norm_re = X_train_norm.reshape(X_train_norm.shape[0],spectogram_array_train.shape[1],spectogram_array_train.shape[2])<br/>X_test_norm_re = X_test_norm.reshape(X_test_norm.shape[0],spectogram_array_test.shape[1],spectogram_array_test.shape[2])<br/><br/>print("<strong class="nq jd">\n</strong>NORM ARRAY SHAPES")<br/>print(X_train_norm_re.shape)<br/>print(X_test_norm_re.shape)<br/>print(label_onehot_train_manu.shape)<br/>print(label_onehot_test.shape)<br/><br/>X_test_norm_re_n, X_vald, label_onehot_test_n, y_vald = train_test_split((X_test_norm_re)<br/>                                                    , label_onehot_test<br/>                                                    , test_size=0.1<br/>                                                    , shuffle=<strong class="nq jd">True</strong><br/>                                                    , random_state=random_num)</span><span id="1626" class="nu mb it nq b gy nz nw l nx ny"><em class="oa"># CONVERT TO ARRAY </em><br/>y_train = np.array(y_train)<br/>y_test = np.array(y_test)</span></pre><h1 id="106d" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">模型</h1><p id="f8b7" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">我使用的CNN + LSTM模型基于Jianfeng及其团队的论文《生物医学信号处理与控制》。根据他们研究中使用的数据集，他们能够达到89%或更高的准确率。这种架构背后的想法是，CNN层将能够捕捉更精细的特征，而LSTM层将考虑音频的时间序列部分。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="bc2d" class="nu mb it nq b gy nv nw l nx ny"><em class="oa">#MODEL BASED ON PAPER</em><br/><br/><em class="oa">#LOCAL FEATURE LEARNING BLOCK</em><br/>model_paper = keras.Sequential()<br/>model_paper.add(keras.Input(shape=(X_train_norm_re.shape[1],X_train_norm_re.shape[2])))<br/><br/>model_paper.add(layers.Conv1D(64,3, activation="relu",padding="same",strides = 1))<br/><em class="oa">#model_cnn1.add(layers.Conv1D(num_filter,kernel_size, activation="relu",padding="same"))</em><br/>model_paper.add(layers.BatchNormalization())<br/>model_paper.add(layers.Activation(activations.elu))<br/>model_paper.add(layers.MaxPooling1D(pool_size = 4))<br/><br/>model_paper.add(layers.Conv1D(128,3, activation="relu",padding="same",strides = 1))<br/><em class="oa">#model_cnn1.add(layers.Conv1D(num_filter,kernel_size, activation="relu",padding="same"))</em><br/>model_paper.add(layers.BatchNormalization())<br/>model_paper.add(layers.Activation(activations.elu))<br/>model_paper.add(layers.MaxPooling1D(pool_size = 4))<br/><br/>model_paper.add(layers.Conv1D(256,3, activation="relu",padding="same",strides = 1))<br/><em class="oa">#model_cnn1.add(layers.Conv1D(num_filter,kernel_size, activation="relu",padding="same"))</em><br/>model_paper.add(layers.BatchNormalization())<br/>model_paper.add(layers.Activation(activations.elu))<br/>model_paper.add(layers.MaxPooling1D(pool_size = 4))<br/><br/>model_paper.add(layers.LSTM(540,return_sequences=<strong class="nq jd">True</strong>))<br/>model_paper.add(layers.LSTM(256,return_sequences=<strong class="nq jd">True</strong>))<br/><br/>model_paper.add(layers.Flatten())<br/>model_paper.add(layers.Dense(100, activation="relu"))<br/>model_paper.add(layers.BatchNormalization())<br/>model_paper.add(layers.Dense(50, activation="relu"))<br/>model_paper.add(layers.BatchNormalization())<br/>model_paper.add(layers.Dense(20, activation="relu"))<br/>model_paper.add(layers.Dense(num_of_emotions, activation="softmax"))<br/><br/>model_paper.summary()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/d5c08ab653376a737c1dbab781907931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*gEI22P2i3cCpQwCWCGp1bg.png"/></div><figcaption class="oc od gj gh gi oe of bd b be z dk translated">模型架构</figcaption></figure><h1 id="e479" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">结果</h1><p id="ccbc" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">CNN+LSTM模型达到了73%的准确率，由于硬件限制，我无法添加更多的层来测试更复杂的模型，这可能有助于提高整体准确性。</p><p id="e901" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于CNN + LSTM，我期望CNN和LSTM的结合能让我提取更好的特征，同时记住数据的时间敏感性。在我测试的其他深度学习模型中，这个模型表现最好，但是任何进一步改变层数或神经元数量都会导致模型崩溃。硬件限制极大地阻碍了我的模型的性能。</p><p id="9eb5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">此外，人们注意到厌恶和恐惧的情绪表现最差。通过进一步分析可以看出，愤怒、厌恶和恐惧这三种情绪具有共同的谱图模式，这使得模型难以区分。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/9033a494854c4692472d942a2a6be844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g9Fy9x8COG4T9dI3BSsCNw.png"/></div></div><figcaption class="oc od gj gh gi oe of bd b be z dk translated">CNN + LSTM混乱矩阵</figcaption></figure><p id="3401" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于进一步的研究，我想:</p><ul class=""><li id="9563" class="ou ov it lf b lg lh lj lk lm ow lq ox lu oy ly pk pa pb pc bi translated">使用GPU和更高RAM的机器来帮助提高整体模型精度</li><li id="d5d7" class="ou ov it lf b lg pd lj pe lm pf lq pg lu ph ly pk pa pb pc bi translated">增加愤怒、厌恶和恐惧情绪的数据，以便更好地对这些情绪进行分类</li></ul><p id="2124" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">链接到github:<a class="ae lz" href="https://github.com/msaleem18/Speech_Emotion_Recognition" rel="noopener ugc nofollow" target="_blank">https://github.com/msaleem18/Speech_Emotion_Recognition</a></p></div><div class="ab cl pl pm hx pn" role="separator"><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq"/></div><div class="im in io ip iq"><p id="7947" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[1]akay，M. B. (2020)。peech情感识别:情感模型、数据库、特征、预处理方法、支持模态和分类器。语音通信，116</p><p id="a6a0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[2]詹姆斯。(2012).检索自实用密码学:<a class="ae lz" href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" rel="noopener ugc nofollow" target="_blank">http://Practical Cryptography . com/miscellaneous/machine-learning/guide-Mel-frequency-ceps tral-coefficients-mfccs/</a></p><p id="1b05" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[3]赵军，毛，谢，陈等(2019).使用深度1D和2D CNN LSTM网络的语音情感识别。<em class="oa">生物医学信号处理与控制</em>，<em class="oa"> 47 </em>，312–323</p></div></div>    
</body>
</html>