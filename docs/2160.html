<html>
<head>
<title>Manipulate Real Images With Text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用文本处理真实图像</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/manipulate-real-images-with-text-25b9f583e292?source=collection_archive---------2-----------------------#2021-09-07">https://pub.towardsai.net/manipulate-real-images-with-text-25b9f583e292?source=collection_archive---------2-----------------------#2021-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f68b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="8769" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">创意艺术家的人工智能！StyleCLIP解释</h2></div><blockquote class="kr ks kt"><p id="3f08" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/styleclip/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/styleclip/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/7b121c44f67922a8e5594a0c736e2fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CNNIvJihs2LH2wbMZClzDg.png"/></div></div></figure><h2 id="f795" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">观看视频并在YouTube上支持我:</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="e192" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">研究人员使用人工智能来生成图像。然后，他们可以利用它来拍摄图像，并按照特定的风格进行编辑，比如将它变成卡通人物或将任何脸变成笑脸。在实现一些现实的东西之前，这需要大量的调整和模型工程以及许多试验和错误。在这个领域已经有了许多进展，主要是StyleGAN，它具有在几乎任何领域生成逼真图像的惊人能力；现实生活中的人类、卡通、素描等。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nb"><img src="../Images/3c18c2569e5b605d2563d7c23e560641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*SIAdjnDS9rAQv5dgzpOVTg.gif"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">我在以前的文章中介绍过的StyleGAN应用程序示例</figcaption></figure><p id="d3e4" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">StyleGAN令人惊叹，但仍需要相当多的工作才能使结果看起来像预期的那样，这就是为什么许多人试图了解这些图像是如何制作的，尤其是如何控制它们。这是非常复杂的，因为我们编辑图像的表现方式对人类并不友好。它不是具有红色、贪婪和蓝色三个维度的常规图像，而是信息极其密集，因此包含数百个维度，具有关于图像可能包含的所有特征的信息。这就是为什么理解和定位我们想要改变的特征以生成相同图像的新版本需要如此多的工作。这里的关键词是“同一个图像”挑战在于只编辑想要的部分，而保持其他部分不变。如果我们改变眼睛的颜色，我们希望所有其他面部特征保持不变。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="https://www.louisbouchard.ai/learnai/"><div class="gh gi ng"><img src="../Images/d6d4f598ae72cf7f2fb082a3e0a0d220.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*fqRa4yjYoXrzncvQ.png"/></div></a></figure><p id="5529" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">我最近介绍了各种技术，研究人员试图通过使用<a class="ae lr" href="https://www.youtube.com/watch?v=vz_wEQkTLk0&amp;t=1s" rel="noopener ugc nofollow" target="_blank">几个图像示例</a>或<a class="ae lr" href="https://www.youtube.com/watch?v=xoEkSWJSm1k" rel="noopener ugc nofollow" target="_blank">快速草图</a>来让用户更容易地进行控制。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d8a4a9ed79fcb52cdc31bcf1a8c2cf82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/1*5JHsddlM06uYq52vnAqmfw.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">StyleCLIP GUI示例，可在<a class="ae lr" href="https://github.com/orpatashnik/StyleCLIP" rel="noopener ugc nofollow" target="_blank"> StyleCLIP的GitHub </a>上获得。</figcaption></figure><p id="a101" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">现在，你可以只用文本来完成。在这篇新论文中，Or Patashnik等人创建了一个模型，能够通过简单的文本输入来控制图像生成过程。你可以发送它几乎任何面部变换和使用样式和剪辑。它会明白你想要什么，并改变它。然后，你可以调整一些参数，以获得可能的最佳结果，这需要不到一秒钟的时间。</p><p id="5a58" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">我提到了斯泰勒根。StyleGAN是NVIDIA最先进的GAN架构，用于图像合成或图像生成。我在各种应用里做了很多覆盖它的视频，如果你不熟悉的话绝对应该看。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ni"><img src="../Images/627916672d445b419862391ed85fef97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0NysCbFK_Bd3QupjzSI3cw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">(左)来自<a class="ae lr" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> OpenAI的博客</a>的剪辑示例和(右)<a class="ae lr" href="https://github.com/haltakov/natural-language-image-search" rel="noopener ugc nofollow" target="_blank">使用剪辑的Unsplash图像搜索</a>。</figcaption></figure><p id="b67e" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">在进入细节之前，唯一要涉及的是我谈到的StyleGAN与之结合的另一个模型，即CLIP。很快，CLIP是OpenAI最近发布的一种强大的图像模型语言。正如我们将看到的，这个模型负责控制只使用我们的图像和文本输入对图像的修改。它接受了来自网络的大量图像-文本对的训练，基本上可以理解图像中出现的内容。由于CLIP是在这样的图像-文本对上训练的，它可以有效地将文本描述与现有图像进行匹配。因此，我们可以在当前的模型中使用相同的原理，将StyleGAN生成的图像定向到所需的文本转换。如果您想了解更多关于CLIP的知识，您应该阅读OpenAI的Distill文章。它在下面的参考文献中有链接。它已经被用来从文本输入和其他非常酷的应用程序中搜索Unsplash上的特定图像。在这种情况下，CLIP如何有用很快就会变得非常清楚。顺便说一句，如果你觉得这很有趣，花一秒钟来分享这个乐趣，并把这篇文章发送给一个朋友。很有帮助！</p><p id="983d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">正如我所说的，研究人员使用这些已经训练好的模型StyleGAN和CLIP来实现这一点。以下是如何…</p><p id="04e7" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">它接受一个输入图像，比如这个例子中的人脸。但是它也可以是一匹马，一只猫，或者一辆车…任何你能找到的东西，一个StyleGAN模型，用足够的数据在这样的图像上训练。然后，这个图像用编码器编码成潜码，就像这样，这里叫w。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nj"><img src="../Images/c6a729dd8f7d6eac6c80f76244a4a95c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OiVupzB1K-p9JhaXS2jLeg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">样式剪辑映射模型。图片来自Patashnik，Or，et al .(2021)，“Styleclip”。</figcaption></figure><p id="15c4" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">这种潜在代码只是卷积神经网络产生的图像的浓缩表示。它包含关于图像的最有用的信息，这些信息在模型的训练期间被识别。<br/>如果这已经太复杂了，我强烈建议你暂停阅读，看一下<a class="ae lr" href="https://www.youtube.com/watch?v=rt-J9YJVvv4" rel="noopener ugc nofollow" target="_blank">我制作的一分钟短片</a>，其中我解释了编码器部分通常是如何工作的。</p><p id="2f3d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">这种潜在的代码，或新的图像表示，然后被发送到三个映射器网络，这些网络被训练来处理图像的期望属性，同时保留其他特征。这些网络中的每一个都负责学习如何映射特定级别的细节，从粗略到精细，这是在从网络中不同深度的编码器提取信息时决定的，正如我在GAN视频中解释的那样。这样，他们可以单独操作一般或精细特征。这就是剪辑模型用于操作这些映射的地方。由于训练，映射将学习相应地移动到文本输入，因为剪辑模型理解图像的内容，并且以与图像编码相同的方式编码文本。因此，CLIP可以理解从一个文本到另一个文本的转换，如从“中性脸”到“惊讶脸”，并告诉映射网络如何将相同的转换应用到图像映射。这个变换是这里的delta向量，它由CLIP控制，并对潜在代码w应用相同的相对平移和旋转，就像对文本所发生的一样。然后，这个修改后的潜在代码被发送到StyleGAN生成器中，以创建我们转换后的图像。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nj"><img src="../Images/c6a729dd8f7d6eac6c80f76244a4a95c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OiVupzB1K-p9JhaXS2jLeg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">样式剪辑映射模型。图片来自Patashnik，Or，et al .(2021)，“Styleclip”。</figcaption></figure><p id="3507" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">总之，剪辑模型理解句子中发生的变化，如“<strong class="kx jd">一张中性的脸</strong>”到“<strong class="kx jd">一张惊讶的脸</strong>”，并且它们将相同的变换应用于编码图像表示。然后，这个新的转换后的潜在代码被发送到StyleGAN生成器，以生成新的图像。</p><p id="a6a0" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">瞧！这就是你如何发送一个图像，并根据这个新模型的一个简单的句子来改变它。他们还制作了一个<a class="ae lr" href="https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global.ipynb" rel="noopener ugc nofollow" target="_blank"> google colab </a>和一个<a class="ae lr" href="https://github.com/orpatashnik/StyleCLIP" rel="noopener ugc nofollow" target="_blank">本地GUI </a>来用任何图像测试它，并使用滑块直观地控制修改，轻松地玩它。</p><p id="32de" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">当然，<a class="ae lr" href="https://github.com/orpatashnik/StyleCLIP" rel="noopener ugc nofollow" target="_blank">代码</a>在GitHub上也有。唯一的限制是你必须训练映射网络，但是他们也在论文中讨论了这个问题。为了更深入地了解它是如何工作的，并了解这两种其他技术，他们介绍了用CLIP控制图像生成，而不需要任何培训。我强烈建议阅读他们的论文。值得花时间！所有的链接都在下面的参考文献中。</p><p id="3bc3" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">感谢您的阅读！</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><p id="658b" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mn lf lg lh mr lj lk ll mv ln lo lp lq im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)，并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">简讯</strong> </a>！</p><h2 id="a96e" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">支持我:</h2><ul class=""><li id="d565" class="nr ns it kx b ky nt lb nu mn nv mr nw mv nx lq ny nz oa ob bi translated">支持我的最好方式是成为这个网站<strong class="kx jd"> </strong>的会员，或者如果你喜欢视频格式，在<a class="ae lr" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"><strong class="kx jd">YouTube</strong></a><strong class="kx jd"/>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="612b" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">在经济上支持我在T21的工作</li><li id="342c" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">跟我来这里上<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a></li></ul><h2 id="5712" class="me mf it bd mg mh mi dn mj mk ml dp mm mn mo mp mq mr ms mt mu mv mw mx my iz bi translated">参考资料:</h2><ul class=""><li id="b6e9" class="nr ns it kx b ky nt lb nu mn nv mr nw mv nx lq ny nz oa ob bi translated">Patashnik，Or，et al .(2021)，“风格剪辑:风格图像的文本驱动操作。”，<a class="ae lr" href="https://arxiv.org/abs/2103.17249" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2103.17249</a></li><li id="a863" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">代码(与本地GUI或colab笔记本一起使用):<a class="ae lr" href="https://github.com/orpatashnik/StyleCLIP" rel="noopener ugc nofollow" target="_blank">https://github.com/orpatashnik/StyleCLIP</a></li><li id="05ec" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">演示:<a class="ae lr" href="https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global.ipynb" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/github/orpatashnik/style clip/blob/main/notebooks/style clip _ global . ipynb</a></li><li id="7124" class="nr ns it kx b ky oc lb od mn oe mr of mv og lq ny nz oa ob bi translated">OpenAI为CLIP撰写的文章:Gabriel Goh、Nick Cammarata、Chelsea Voss、Shan Carter、Michael Petrov、Ludwig Schubert、Alec拉德福德和Chris Olah。人工神经网络中的多模态神经元。蒸馏，<a class="ae lr" href="https://distill.pub/2021/multimodal-neurons/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2021/multimodal-neurons/</a>，2021。</li></ul></div></div>    
</body>
</html>