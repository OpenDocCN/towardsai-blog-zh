<html>
<head>
<title>Deep Dive Into Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入研究神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/deep-dive-into-neural-networks-1d942472e31b?source=collection_archive---------2-----------------------#2021-03-07">https://pub.towardsai.net/deep-dive-into-neural-networks-1d942472e31b?source=collection_archive---------2-----------------------#2021-03-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="64f1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/5ad42d4ff54b950f45122ef728ff8bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*doDbtnT2FEDDYupW"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">照片由<a class="ae kl" href="https://unsplash.com/@dizzyd718?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">德鲁·格拉汉姆</a>在<a class="ae kl" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="6000" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">除了图像和语音识别等众所周知的应用之外，神经网络还被用于在大量数据集中寻找复杂模式，例如当电子邮件引擎建议完成句子时，或者当机器将一种语言翻译成另一种语言时。为了解决如此复杂的问题，我们使用人工神经网络。</p><p id="f50d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这篇文章中，主题将涵盖:-</p><ul class=""><li id="1ee6" class="lk ll iq ko b kp kq kt ku kx lm lb ln lf lo lj lp lq lr ls bi translated">神经网络导论</li><li id="2949" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">使用神经网络的目的</li><li id="badc" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">神经网络体系结构</li><li id="3033" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">神经元的评估</li><li id="ff93" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">激活函数的几种类型解释</li><li id="ccb6" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">估计权重和偏差的反向传播</li></ul><h1 id="f987" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">介绍</h1><p id="af59" class="pw-post-body-paragraph km kn iq ko b kp mw kr ks kt mx kv kw kx my kz la lb mz ld le lf na lh li lj ij bi translated">人工神经网络(ANN)通常被称为黑盒技术，因为有时很难理解它们在做什么。这是一系列数学计算，通过神经网络可以很好地可视化。</p><figure class="nc nd ne nf gt ka gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/c159ac073a0c3329f85e716197ee5ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*9WBQ64izeaHxlYqXftIscA.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">来源:<a class="ae kl" href="https://commons.wikimedia.org/wiki/File:Blackbox3D-withGraphs.png" rel="noopener ugc nofollow" target="_blank"> Wikimedia Commons </a>，根据<a class="ae kl" href="https://en.wikipedia.org/wiki/en:Creative_Commons" rel="noopener ugc nofollow" target="_blank">Creative Commons</a><a class="ae kl" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" rel="noopener ugc nofollow" target="_blank">Attribution-Share like 4.0 International</a>许可证授权。</figcaption></figure><p id="5e12" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">安隐约受到构成人类大脑的生物神经网络的启发。人类大脑使用称为神经元的相互连接的细胞网络来提供学习能力。同样，ANN使用人工神经元或节点网络来解决具有挑战性的学习问题。</p><figure class="nc nd ne nf gt ka gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/40632b43d838b1d85b72e0a7ae3e8a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Fngc6aIV5bR4L4e8dSSHzg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">来源:<a class="ae kl" href="https://commons.wikimedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png" rel="noopener ugc nofollow" target="_blank"> Wikimedia Commons </a>，根据<a class="ae kl" href="https://en.wikipedia.org/wiki/en:Creative_Commons" rel="noopener ugc nofollow" target="_blank">Creative Commons</a><a class="ae kl" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" rel="noopener ugc nofollow" target="_blank">Attribution-Share like 4.0 International</a>许可协议授权。</figcaption></figure><figure class="nc nd ne nf gt ka gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/16365c950906df5dc1bd542e7be4ce8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*0RxXt5Ea5c7DGMnTQ2c-0w.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">来源:<a class="ae kl" href="https://commons.wikimedia.org/wiki/File:Neural_network_example.svg" rel="noopener ugc nofollow" target="_blank">维基共享</a>，根据<a class="ae kl" href="https://en.wikipedia.org/wiki/en:Creative_Commons" rel="noopener ugc nofollow" target="_blank">知识共享</a> <a class="ae kl" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" rel="noopener ugc nofollow" target="_blank">署名-分享4.0国际</a>许可授权。</figcaption></figure><p id="3de1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> <em class="ni">人脑的神经网络通过神经元显示信息的输入和输出</em> </strong></p><p id="b796" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> <em class="ni">人工神经网络</em> </strong></p><h2 id="287a" class="nj lz iq bd ma nk nl dn me nm nn dp mi kx no np mm lb nq nr mq lf ns nt mu iw bi translated">为什么要学习神经网络？</h2><ul class=""><li id="780c" class="lk ll iq ko b kp mw kt mx kx nu lb nv lf nw lj lp lq lr ls bi translated"><strong class="ko ja">学习能力</strong> -神经网络可以独立执行其功能。</li><li id="31a9" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">泛化能力</strong>——它可以很容易地为没有被教会如何处理的输入产生输出。</li><li id="d954" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">适应性</strong> —它可以很容易地保持变化的环境条件。</li></ul><h2 id="7b1f" class="nj lz iq bd ma nk nl dn me nm nn dp mi kx no np mm lb nq nr mq lf ns nt mu iw bi translated">神经网络体系结构</h2><p id="da21" class="pw-post-body-paragraph km kn iq ko b kp mw kr ks kt mx kv kw kx my kz la lb mz ld le lf na lh li lj ij bi translated">神经网络由上图所示的三层组成-</p><ul class=""><li id="93c3" class="lk ll iq ko b kp kq kt ku kx lm lb ln lf lo lj lp lq lr ls bi translated">输入层</li><li id="a25f" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">隐蔽层</li><li id="d5cf" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">输出层</li></ul><p id="f7b3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实际上，神经网络具有一个以上的输入模式，并且通常也具有一个以上的输出节点。在这两个节点之间，每层节点之间有一个连接的蜘蛛网。这些层是隐藏层。当我们建立一个神经网络时，要做的第一件事是决定我们需要多少个隐藏层。</p><figure class="nc nd ne nf gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi nx"><img src="../Images/9467f74152da19f83961589fc13e31e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ao34U7KbagFCedicKCqiOg.png"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated"><strong class="bd ma">来源:作者图片</strong></figcaption></figure><p id="31f0" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">神经元是信息处理单元，是神经网络运行的基础。神经元模型的三个基本要素:</p><p id="bf39" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">突触重量</p><p id="94ef" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">组合(加法)功能</p><ul class=""><li id="adf6" class="lk ll iq ko b kp kq kt ku kx lm lb ln lf lo lj lp lq lr ls bi translated">激活功能</li></ul><p id="f509" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">外部输入偏置，用于增加或降低激活函数的净输入</p><h2 id="d7b2" class="nj lz iq bd ma nk nl dn me nm nn dp mi kx no np mm lb nq nr mq lf ns nt mu iw bi translated">神经元评估</h2><figure class="nc nd ne nf gt ka gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8a0d59e8bf52c7da7674b0ebe4c6fb5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*XpLJMWLOkRmTv0o48rxuXw.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated"><strong class="bd ma">来源:作者图片</strong></figcaption></figure><p id="00c6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">重量级砝码</p><p id="a5d6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">n–输入数量</p><p id="7e80" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Xi-输入</p><p id="dd01" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">f(x)-激活功能</p><ul class=""><li id="8857" class="lk ll iq ko b kp kq kt ku kx lm lb ln lf lo lj lp lq lr ls bi translated">y(x)–输出</li></ul><p id="42d9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个评估可以通过一个简单的例子来理解，假设在输入节点中，我们有一个值x1，我们通过网络的突触将这个值从输入节点传递到隐藏层，每个突触或连接都有一些突触权重，即w，输入值x1乘以各自的突触权重，并被输入到隐藏层节点中，在这个隐藏层节点中，激活函数f(x)完成它们的工作。现在让我们理解什么是激活函数-</p><p id="98c1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">激活功能- </strong></p><p id="793d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在以线性组合的形式输入时，我们需要应用一些函数来达到优化的目的。这个功能被称为激活功能，以实现所需的输出。当我们建立一个神经网络时，我们必须决定我们想要使用哪个激活函数。有几种类型的激活功能可以使用-</p><ul class=""><li id="94a4" class="lk ll iq ko b kp kq kt ku kx lm lb ln lf lo lj lp lq lr ls bi translated"><strong class="ko ja"> RELU(整流线性单元)</strong> -对于负输入返回零，对于所有正值保持沉默使用函数- <strong class="ko ja"> <em class="ni"> y=max(z，0)。z是在隐藏层节点中接收的加权输入。</em> </strong></li></ul><figure class="nc nd ne nf gt ka gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a4f958bba6140596fd34f07e050dad4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*culARlfDd88G-oUVz2OpLA.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated"><strong class="bd ma">来源:作者图片</strong></figcaption></figure><p id="041e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> Sigmoid函数- </strong>输出值的范围可以是0到1之间的任何值。评估人—<strong class="ko ja"><em class="ni">f(x)=1/1+e^z.</em></strong></p><figure class="nc nd ne nf gt ka gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f2289f671f53c3d8d3882099fc17dc1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*M6c7C30WmzI9MmzWHLhtUg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated"><strong class="bd ma">来源:图片由作者提供</strong></figcaption></figure><ul class=""><li id="f15b" class="lk ll iq ko b kp kq kt ku kx lm lb ln lf lo lj lp lq lr ls bi translated"><strong class="ko ja">双曲正切函数— </strong>也称为双曲正切函数，是sigmoid函数的移位版本，具有广泛的输出范围。范围从-1到+1</li></ul><figure class="nc nd ne nf gt ka gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/10af95c2d9520e0a9f7eb35ed010beb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*FjTpQagL57hiLVK_0WMDtg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated"><strong class="bd ma">来源:图片由作者提供</strong></figcaption></figure><p id="2059" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在所有这些激活函数中，RELU是使用最多的激活函数，其背后的主要动机是将输入转化为有价值的输出单位。</p><p id="eef9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">反向传播</strong></p><p id="b86c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">神经网络从相同的激活函数开始，但在连接上使用不同的权重和偏差，它将激活函数翻转和拉伸成新的形状，该形状被转移以估计权重和偏差。反向传播使用两种不同的方法来估计神经网络中的权重和偏差参数</p><ol class=""><li id="f438" class="lk ll iq ko b kp kq kt ku kx lm lb ln lf lo lj oc lq lr ls bi translated">使用链式法则计算导数或</li><li id="6784" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj oc lq lr ls bi translated">将导数代入梯度下降以优化参数。</li></ol><h1 id="5b8f" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">神经网络的用途</h1><ul class=""><li id="3ffa" class="lk ll iq ko b kp mw kt mx kx nu lb nv lf nw lj lp lq lr ls bi translated">用于语音识别</li><li id="286d" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">面部识别</li><li id="0b2d" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">欺诈检测</li><li id="1554" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">情感分析</li><li id="d1d9" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated">社交媒体中的图片搜索</li></ul><h1 id="65a3" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">结论</h1><p id="15e9" class="pw-post-body-paragraph km kn iq ko b kp mw kr ks kt mx kv kw kx my kz la lb mz ld le lf na lh li lj ij bi translated">本文介绍了人工神经网络的概念、功能类型、用途以及如何估计其参数值。此外，它简要地解释了它是如何类似于人的大脑神经系统。</p><p id="8ead" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">要了解更多信息或深入研究神经网络，你可以参考Akruti Acharya的这篇精彩的案例研究文章</p><h2 id="c03c" class="nj lz iq bd ma nk nl dn me nm nn dp mi kx no np mm lb nq nr mq lf ns nt mu iw bi translated">链接-<a class="ae kl" href="https://neptune.ai/blog/neural-network-guide" rel="noopener ugc nofollow" target="_blank">https://neptune.ai/blog/neural-network-guide</a></h2><p id="e303" class="pw-post-body-paragraph km kn iq ko b kp mw kr ks kt mx kv kw kx my kz la lb mz ld le lf na lh li lj ij bi translated"><em class="ni">感谢阅读！</em></p><p id="5ea3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">参考文献</strong></p><ol class=""><li id="efd1" class="lk ll iq ko b kp kq kt ku kx lm lb ln lf lo lj oc lq lr ls bi translated"><a class="ae kl" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Artificial_neural_network</a></li><li id="ae4f" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj oc lq lr ls bi translated"><a class="ae kl" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Activation_function</a></li><li id="4f31" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj oc lq lr ls bi translated"><a class="ae kl" href="https://www.sas.com/en_in/insights/analytics/neural-networks.html" rel="noopener ugc nofollow" target="_blank">https://www . SAS . com/en _ in/insights/analytics/neural-networks . html</a></li></ol></div></div>    
</body>
</html>