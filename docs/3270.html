<html>
<head>
<title>Stop Using Elbow Diagram To Find Best K-Value And Use This Instead</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">停止使用肘图来寻找最佳K值，而使用这个</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/stop-using-elbow-diagram-to-find-best-k-value-and-use-this-instead-568b13d77561?source=collection_archive---------0-----------------------#2022-10-30">https://pub.towardsai.net/stop-using-elbow-diagram-to-find-best-k-value-and-use-this-instead-568b13d77561?source=collection_archive---------0-----------------------#2022-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="5c98" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ko translated"><span class="l kp kq kr bm ks kt ku kv kw di"> K </span> -Means是最重要的聚类算法之一，通常由数据科学家和机器学习实践者在无监督学习任务中使用。需要小心处理的该算法的参数之一是聚类数K。找到K的最佳值的最流行的方法是肘图，然而，它不是这样做的最有效的方法。在本文中，我们将探索更有效的方法来寻找K-Means聚类算法的最佳K值。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi kx"><img src="../Images/b5174688fd55d3d6b1290a926fe07066.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3co92B3QjJttxF9V"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">照片由<a class="ae ln" href="https://unsplash.com/@bogomi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">博戈米尔·米哈伊洛夫</a>在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h2 id="2719" class="lo lp it bd lq lr ls dn lt lu lv dp lw kb lx ly lz kf ma mb mc kj md me mf mg bi translated">目录:</h2><ol class=""><li id="afdd" class="mh mi it js b jt mj jx mk kb ml kf mm kj mn kn mo mp mq mr bi translated"><strong class="js iu">K均值聚类简介</strong></li><li id="ac00" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated"><strong class="js iu">使用肘图查找K值</strong></li><li id="bbda" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated"><strong class="js iu">寻找K值的剪影分数</strong></li><li id="ed9c" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated"><strong class="js iu">参考文献</strong></li></ol><h2 id="b51a" class="lo lp it bd lq lr ls dn lt lu lv dp lw kb lx ly lz kf ma mb mc kj md me mf mg bi translated"><strong class="ak">本文中使用的源代码和生成的图可以在这个GitHub存储库中找到:</strong></h2><div class="mx my gp gr mz na"><a href="https://github.com/youssefHosni/Machine-Learning-Practical-Guide" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd iu gy z fp nf fr fs ng fu fw is bi translated">GitHub-youssefHosni/机器学习-实用指南</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">在GitHub上创建一个帐户，为youssefHosni/Machine-Learning-Practical-Guide开发做贡献。</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">github.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no lh na"/></div></div></a></div></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="3b38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">如果你想免费学习数据科学和机器学习，看看这些资源:</strong></p><ul class=""><li id="73ea" class="mh mi it js b jt ju jx jy kb nw kf nx kj ny kn nz mp mq mr bi translated">免费互动路线图，自学数据科学和机器学习。从这里开始:<a class="ae ln" href="https://aigents.co/learn/roadmaps/intro" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn/roadmaps/intro</a></li><li id="04fc" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn nz mp mq mr bi translated">数据科学学习资源搜索引擎(免费)。将你最喜欢的资源加入书签，将文章标记为完整，并添加学习笔记。<a class="ae ln" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></li><li id="0fb9" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn nz mp mq mr bi translated">想要在导师和学习社区的支持下从头开始学习数据科学吗？免费加入这个学习圈:<a class="ae ln" href="https://community.aigents.co/spaces/9010170/" rel="noopener ugc nofollow" target="_blank">https://community.aigents.co/spaces/9010170/</a></li></ul><p id="c335" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你想在数据科学&amp;人工智能领域开始职业生涯，但你不知道如何开始。我提供数据科学指导课程和长期职业指导:</p><ul class=""><li id="e675" class="mh mi it js b jt ju jx jy kb nw kf nx kj ny kn nz mp mq mr bi translated">长期指导:<a class="ae ln" href="https://lnkd.in/dtdUYBrM" rel="noopener ugc nofollow" target="_blank">https://lnkd.in/dtdUYBrM</a></li><li id="b1f0" class="mh mi it js b jt ms jx mt kb mu kf mv kj mw kn nz mp mq mr bi translated">辅导会议:<a class="ae ln" href="https://lnkd.in/dXeg3KPW" rel="noopener ugc nofollow" target="_blank">https://lnkd.in/dXeg3KPW</a></li></ul><p id="2fa6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="oa">加入</em> </strong> <a class="ae ln" href="https://youssefraafat57.medium.com/membership" rel="noopener"> <strong class="js iu"> <em class="oa">中等会员</em> </strong> </a> <strong class="js iu"> <em class="oa">计划，只需5美元，继续无限制学习。如果你使用下面的链接，我会收到一小部分会员费，不需要你额外付费。</em>T13】</strong></p><div class="mx my gp gr mz na"><a href="https://youssefraafat57.medium.com/membership" rel="noopener follow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd iu gy z fp nf fr fs ng fu fw is bi translated">加入我的介绍链接媒体-优素福胡斯尼</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">阅读Youssef Hosni(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">youssefraafat57.medium.com</p></div></div><div class="nj l"><div class="ob l nl nm nn nj no lh na"/></div></div></a></div></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="269d" class="oc lp it bd lq od oe of lt og oh oi lw oj ok ol lz om on oo mc op oq or mf os bi translated">1.K-均值聚类简介</h1><p id="2c29" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ot kd ke kf ou kh ki kj ov kl km kn im bi translated">K-Means是一种聚类算法，由Stuart Lloyd于1957年在贝尔实验室提出，作为一种脉冲编码调制技术，但它只是在1982年在公司外部发表在一篇题为“PCM中的最小平方量化”的论文中。到那时，在1965年，Edward W. Forgy已经发表了几乎相同的算法，所以K-Means有时被称为Lloyd- Forgy。考虑图1所示的未标记数据集，使用以下代码绘制:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/374b7bd9554fdaf64c9f2babd9f0c036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*k8iEl6AlwJMX2VTd0QVT6g.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图一。未标记的数据由5组数据组成。</figcaption></figure><p id="4b92" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以清楚地观察到5组实例。K-Means算法是一种简单的算法，能够非常快速有效地对这种数据集进行聚类，通常只需几次迭代。</p><p id="7113" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们在这个数据集上训练一个K-Means聚类算法。它将尝试找到每个blob的中心，并使用下面的代码将每个实例分配给最近的blob:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><p id="7b44" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，您必须指定算法必须找到的聚类数k。对于这个数据集，从数据中可以很明显地看出k应该设置为5，但一般来说，这并不容易，需要做大量的工作才能有效地做到这一点。让我们看看每个数据点的聚类标签以及每个聚类的质心:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/c58ac21ae4e689a6e4c2adf59f00b9ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*SLZ0A6HonUxaQGsuB5FpWQ.png"/></div></figure><p id="3335" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这显示了每个数据点被分配到的集群。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/22e019a0e2f55bdf147c793366278b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*wFV-V88acXVsUr977BXYxA.png"/></div></figure><p id="33c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这显示了每个簇的质心的尺寸。现在，让我们使用下面的绘图函数来绘制集群:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi pb"><img src="../Images/d937285b50ea6e54717c596d59eee5c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgiunV5ohrS_DZu1P6wNgw.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图二。k-均值决策边界(Voronoi镶嵌)</figcaption></figure><p id="41ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如您所看到的，我们已经将集群的数量设置为5，因为仅从数据来看这是非常明显的。然而，一般来说，它不会那么明显，如果我们选择了错误的K值，就会导致非常糟糕的聚类模型。考虑这个例子，如果我们选择较低或较高的K值，将导致非常错误的数据聚类，如图3所示。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi pb"><img src="../Images/c6afd3478cd0d044b0d5803436da808d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wwYXRmd0RvkLZ6_trlEfDg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图3。对集群数量的错误选择</figcaption></figure><p id="a2ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如您所见，选择较低的K值(在本例中为3)会导致将数据分配给错误的集群，如图3(左)所示。而选择较高的K值将导致将相同的聚类分成较小的聚类，如图3(右)所示。因此，重要的是找到一种方法来估计K的最佳值，这将避免低估和高估其值，因为在这两种情况下，都会导致糟糕的聚类模型。我们将讨论寻找K的最佳值的两种方法:<strong class="js iu">肘图</strong>和<strong class="js iu">轮廓系数。</strong></p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="566a" class="oc lp it bd lq od oe of lt og oh oi lw oj ok ol lz om on oo mc op oq or mf os bi translated">2.用肘图求K值</h1><p id="efbb" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ot kd ke kf ou kh ki kj ov kl km kn im bi translated">下图是基于计算每个k值的<strong class="js iu">区间</strong>并绘制出来的。<strong class="js iu"> interia </strong>是每个实例与其最近质心之间的均方距离。因此，数据点越靠近质心，interia就越小。然而，仅依赖于这种测量有一个缺点，即随着集群数量的增加，interia将减少，这就产生了肘形图的重要性，因为我们可以选择最合适的k值。</p><p id="7cf2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们首先使用下面的代码绘制肘图:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/0b600f86af6a86c020085058960c8c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*BOnhmdyCDrUeVk5WQHGSag.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图4。用肘图求K的最佳值。</figcaption></figure><p id="030e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们所看到的，当k增加到4时，惯性下降得非常快，但当k继续增加时，惯性下降得更慢。</p><p id="2b73" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这条曲线大致具有手臂的形状，并且在k=4处有一个“<strong class="js iu">肘</strong>”，所以如果我们不知道更好的情况，它将是K值的一个好选择:任何更低的值都将是戏剧性的，而任何更高的值都不会有太大帮助，并且我们可能只是在没有重要原因的情况下将非常好的簇分成两半。</p><p id="c309" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以检查k =4的聚类决策边界。正如我们在图5中看到的，当选择k = 4时，有两个群集被检测为仅仅一个群集。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/42bc31873a52ae0f88d95cc8eaa936ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*B_YUYhNe70urgPmDE64iCw.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图5。k-表示k=4时的决策边界。</figcaption></figure><p id="8556" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，有时它有时会变得令人困惑。我们应该将4、5还是6作为最佳的聚类数？此外，它不像本例中那样精确，我们知道K的最佳值是5，但是，从这个肘形图中估计的最佳值是4。一种更精确的方法(但计算成本也更高)是使用<strong class="js iu">轮廓分数。</strong></p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="240b" class="oc lp it bd lq od oe of lt og oh oi lw oj ok ol lz om on oo mc op oq or mf os bi translated">3.<strong class="ak">寻找K值的剪影分数</strong></h1><p id="1106" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ot kd ke kf ou kh ki kj ov kl km kn im bi translated"><strong class="js iu">轮廓分数</strong>是所有实例的平均轮廓系数。实例的轮廓系数等于<strong class="js iu"> (b — a) / max(a，b) </strong>，其中a是到同一聚类中其他实例的平均距离(它是平均聚类内距离)，b是平均最近聚类距离，它是到下一个最近聚类的实例的平均距离(定义为最小化b的那个，不包括实例自己的聚类)。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/a4c2ffeb758d2ad3a259d88549fb4648.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*qMvV0KunbO3NPMym-Vh69Q.png"/></div></figure><p id="b5fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">轮廓系数可以在-1和+1之间变化:接近+1的系数意味着该实例在它自己的聚类内，并且远离其他聚类，而接近0的系数意味着它接近聚类边界，最后接近-1的系数意味着该实例可能被分配到错误的聚类。</p><p id="657c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要计算剪影得分，可以使用Scikit-Learn的silhouette_score()函数，给它数据集中的所有实例以及它们被分配的标签:</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/bd92d023d72ff05c7b7f3ffb0bf3710d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*D0MPgafkTSQ_fnq-eEUf4A.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图6。使用轮廓分数选择最佳数量的聚类k。</figcaption></figure><p id="8bb5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如您所见，这个可视化比肘图丰富得多。虽然它证实了k=4是一个非常好的选择，但它也强调了这样一个事实，即k=5也很好，比k=6或7好得多。这在比较惯性值时是不可见的。</p><p id="f30e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当您绘制每个实例的轮廓系数时，可以获得更丰富的可视化信息，这些系数按照它们被分配到的簇和系数的值进行排序。这被称为轮廓图，如图7所示。</p><figure class="ky kz la lb gt lc"><div class="bz fp l di"><div class="ow ox l"/></div></figure><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/3767c9eed0679d02303b5fca60baa072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*LwnSECL0XFbUwPDqamMJFQ.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk translated">图7。K = (3，4，5，6)值的轮廓图。</figcaption></figure><p id="580a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">垂直虚线表示每个聚类数的轮廓分数，每个条的宽度表示每个聚类的数据大小，每个聚类的长度表示其轮廓系数。</p><p id="b56c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当一个聚类中的大多数实例具有比这个分数更低的系数时(即，如果许多实例条在虚线之前停止，在虚线的左侧结束)，那么该聚类相当差，因为这意味着它的实例太靠近其他聚类。</p><p id="40f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到，当k=3和k=6时，我们得到的是坏簇。但是当k=4或k=5时，群集看起来相当不错，因为大多数实例都向右延伸到虚线之外，更接近1.0。</p><p id="8122" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当k=4时，索引1处的聚类(从上往下数第三个)相当大，而当k=5时，所有聚类都具有相似的大小，因此即使k=4的总体轮廓分数略大于k=5的，使用k=5来获得相似大小的聚类似乎是个好主意。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="66a3" class="oc lp it bd lq od oe of lt og oh oi lw oj ok ol lz om on oo mc op oq or mf os bi translated">4.参考</h1><ol class=""><li id="b73b" class="mh mi it js b jt mj jx mk kb ml kf mm kj mn kn mo mp mq mr bi translated"><a class="ae ln" href="https://www.amazon.com.br/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn、Keras和Tensorflow进行机器实践学习:构建智能系统的概念、工具和技术</a></li></ol></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="655c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="oa">喜爱文章？成为</em> </strong> <a class="ae ln" href="https://youssefraafat57.medium.com/membership" rel="noopener"> <strong class="js iu"> <em class="oa">中等会员</em> </strong> </a> <strong class="js iu"> <em class="oa">继续无限制学习。如果你免费使用下面的链接，我会收到一小部分会员费。</em>T13】</strong></p><div class="mx my gp gr mz na"><a href="https://youssefraafat57.medium.com/membership" rel="noopener follow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd iu gy z fp nf fr fs ng fu fw is bi translated">加入我的介绍链接媒体-优素福胡斯尼</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">阅读Youssef Hosni(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">youssefraafat57.medium.com</p></div></div><div class="nj l"><div class="ph l nl nm nn nj no lh na"/></div></div></a></div><p id="c122" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="oa">感谢阅读！如果喜欢文章，一定要鼓掌(最多50！)并在</em></strong><a class="ae ln" href="https://www.linkedin.com/in/youssef-hosni-b2960b135/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="oa">LinkedIn</em></strong></a><strong class="js iu"><em class="oa">上与我联系，并在</em> </strong> <a class="ae ln" href="https://youssefraafat57.medium.com/" rel="noopener"> <strong class="js iu"> <em class="oa">上关注我，以便随时更新我的新文章。</em> </strong></a></p></div></div>    
</body>
</html>