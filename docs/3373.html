<html>
<head>
<title>How to access Scientific Knowledge with Galactica</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何通过卡拉狄加获取科学知识</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-access-scientific-knowledge-with-galactica-8adc96ebe931?source=collection_archive---------2-----------------------#2022-12-06">https://pub.towardsai.net/how-to-access-scientific-knowledge-with-galactica-8adc96ebe931?source=collection_archive---------2-----------------------#2022-12-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c043" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Meta AI的大型语言模型执行科学NLP任务的教程</h2></div><p id="a5fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">科学知识的世界大得令人难以置信。搜索相关的研究论文、理解复杂的概念以及撰写学术文献可能会令人望而生畏，而且非常耗时，尤其是对于没有经验的研究人员而言。幸运的是，随着Meta AI大语言模型的出现，获取科学知识变得前所未有的简单。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/8212c0a37d10cc45f3b2b4b385016447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wWp_R4I4XxATH2MItv6AIg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">Galactica使用户能够访问科学知识和处理科学NLP任务(由作者和稳定传播创建)</figcaption></figure><p id="24f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本教程中，我将向您展示如何使用<a class="ae lr" href="https://galactica.org/static/paper.pdf" rel="noopener ugc nofollow" target="_blank"> Meta AI的Galactica </a>快速有效地执行各种科学NLP任务。我们将涵盖诸如查找相关引用、生成学术论文、处理多模态数据(例如，乳胶方程、代码片段、化学式等)等主题。)调研时经常遇到的，还有更多。</p><h1 id="cfb4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">Meta AI的Galactica是什么？</h1><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mk"><img src="../Images/47482f1ae904f6e6690403fb6f89de5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-g8jYZPvETGLj8SFThS3A.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">卡拉狄加训练代表科学现象的文本序列(来源:<a class="ae lr" href="https://galactica.org/static/paper.pdf" rel="noopener ugc nofollow" target="_blank">卡拉狄加论文</a></figcaption></figure><p id="4145" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Galactica是一个120B参数的大型语言模型，在精心策划的科学语料库上进行训练。训练数据不仅包括大量的科学文献，还包括下游科学NLP任务的数据集和代表科学现象的特殊标记。</p><p id="c2c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">专门化的标记化是Galactica不可或缺的一部分，因为它使模型能够预测引用或处理形式，如蛋白质序列或微笑公式:</p><ul class=""><li id="6620" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated"><strong class="kh ir">引用</strong>:引用用引用标记<code class="fe mu mv mw mx b">[START_REF]</code>和<code class="fe mu mv mw mx b">[END_REF]</code>包装。</li><li id="1f86" class="ml mm iq kh b ki my kl mz ko na ks nb kw nc la mq mr ms mt bi translated"><strong class="kh ir">推理</strong> : <code class="fe mu mv mw mx b">&lt;work&gt;</code> token通过模仿内部工作记忆来实现一步一步的推理(本教程不涉及)。</li><li id="45f9" class="ml mm iq kh b ki my kl mz ko na ks nb kw nc la mq mr ms mt bi translated"><strong class="kh ir"> SMILES公式</strong> : SMILES公式序列用记号<code class="fe mu mv mw mx b">[START_SMILES]</code>和<code class="fe mu mv mw mx b">[END_SMILES]</code>包裹。对于异构微笑，使用标记<code class="fe mu mv mw mx b">[START_I_SMILES]</code>和<code class="fe mu mv mw mx b">[END_I_SMILES]</code>。</li><li id="03ca" class="ml mm iq kh b ki my kl mz ko na ks nb kw nc la mq mr ms mt bi translated"><strong class="kh ir">氨基酸序列</strong>:氨基酸序列用记号<code class="fe mu mv mw mx b">[START_AMINO]</code>和<code class="fe mu mv mw mx b">[END_AMINO]</code>包裹。</li><li id="3326" class="ml mm iq kh b ki my kl mz ko na ks nb kw nc la mq mr ms mt bi translated"><strong class="kh ir"> DNA序列</strong> : DNA序列用令牌<code class="fe mu mv mw mx b">[START_DNA]</code>和<code class="fe mu mv mw mx b">[END_DNA]</code>包裹。</li></ul><p id="b3c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Meta AI报告称，Galactica的引文预测生成方法优于检索方法，这表明了语言模型取代搜索引擎的潜力。此外，Galactica在推理任务基准(如MMLU和MATH)上击败了现有方法，并在几个下游科学NLP任务(如PubMedQA和MedMCOA)上树立了新的艺术水平。</p><p id="86d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管拥有强大的能力，类似于大多数语言模型，卡拉狄加还是容易产生幻觉。也就是说，模型在某些情况下会输出无意义的结果。因此，使用Galactica的研究人员应该经常检查生成的结果。</p><h1 id="7ae2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如何使用卡拉狄加</h1><p id="8d19" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">卡拉狄加可以通过<code class="fe mu mv mw mx b">galai</code> Python库访问。可以用<code class="fe mu mv mw mx b">load_model</code>下载模型。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="79e4" class="nn lt iq mx b be no np l nq nr">import galai as gal<br/><br/>model = gal.load_model(name="standard", num_gpus=2)</span></pre><ul class=""><li id="2ec9" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">name</code>参数是要使用的模型版本的名称。该模型有五个版本可用(<code class="fe mu mv mw mx b">‘mini’, ‘base’, ‘standard’, ‘large’, ‘huge’</code>)，每个版本都有不同的参数大小(<code class="fe mu mv mw mx b">125M, 1.3B, 6.7B, 30B, 120B</code>)。</li><li id="7d3e" class="ml mm iq kh b ki my kl mz ko na ks nb kw nc la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">num_gpus</code>参数是要使用的GPU数量。我能够在两个英伟达RTX 3090 GPU上加载<code class="fe mu mv mw mx b">‘standard’</code>版本；该型号为每个设备占用了大约19GB的内存。</li></ul><h1 id="cf8d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">文本生成</h1><p id="f6da" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">与大多数大型语言模型相似，Galactica将每个NLP任务都框架为文本生成。您可以使用<code class="fe mu mv mw mx b">generate</code>来生成文本。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="16f3" class="nn lt iq mx b be no np l nq nr"># free-form text generation<br/>input_text = "The reason why Transformers replaced RNNs was because"<br/><br/>generated_text = model.generate(input_text=input_text, <br/>                                max_length=256, <br/>                                new_doc=False, <br/>                                top_p=None)<br/>print(generated_text)<br/><br/>"""<br/>The reason why Transformers replaced RNNs was because they were able to capture long-term dependencies in the input sequence.<br/><br/># 2.2.2. Attention Mechanism<br/><br/>The attention mechanism was introduced in [START_REF] Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau[END_REF] to improve the performance of the encoder-decoder model...<br/>"""</span></pre><ul class=""><li id="db35" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">input_text</code>是用于生成模型的输入上下文。卡拉狄加的其他高级功能可以通过输入环境的即时工程来使用。</li><li id="94dc" class="ml mm iq kh b ki my kl mz ko na ks nb kw nc la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">max_length</code>修改生成文本的最大令牌长度。默认值是60个令牌，因此对于更长的代，<code class="fe mu mv mw mx b">max_length</code>应该设置为更高的值。模型的最大上下文长度是2048个令牌。</li><li id="dfb8" class="ml mm iq kh b ki my kl mz ko na ks nb kw nc la mq mr ms mt bi translated">如果<code class="fe mu mv mw mx b">new_doc</code>被设置为<code class="fe mu mv mw mx b">True</code>，一个填充标记会自动附加到输入文本的前面，这样模型会把它当作一个新文档的开始。对于自由格式文本生成，<code class="fe mu mv mw mx b">new_doc</code>应设置为<code class="fe mu mv mw mx b">False</code>。</li><li id="4ed8" class="ml mm iq kh b ki my kl mz ko na ks nb kw nc la mq mr ms mt bi translated"><code class="fe mu mv mw mx b">top_p</code>参数用于细胞核取样。如果生成的结果看起来太重复，将值设置为1到0之间的浮点数，如<code class="fe mu mv mw mx b">0.9</code>。否则<code class="fe mu mv mw mx b">top_p</code>默认为<code class="fe mu mv mw mx b">None</code>并使用贪婪解码。</li></ul><h2 id="fdb4" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">论文和调查</h2><p id="4925" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">你可以通过prompt engineering用Galactica生成各种类型的学术文献。如果一个提示被设计成类似于某种文档，那么它的完成也是如此。对于纸质文件，使用<code class="fe mu mv mw mx b">Title:</code>。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="f7cd" class="nn lt iq mx b be no np l nq nr"># generate paper document<br/>input_text = "Title: Self-Supervised Learning, A Survey\n\nAuthors: John Smith\n\n"<br/><br/>generated_text = model.generate(input_text, new_doc=True)<br/>print(generated_text)<br/><br/>"""<br/>Title: Self-Supervised Learning, A Survey<br/><br/>Authors: John Smith<br/><br/># Abstract<br/><br/>Self-supervised learning is a class of machine learning methods that learn representations of data without the need for human-provided labels.\nIn this survey, we provide a comprehensive overview of the field<br/>"""</span></pre><p id="3178" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当您需要对特定主题进行全面调查时，此功能特别有用。只需将提示设计为<code class="fe mu mv mw mx b">Title: TOPIC, A Survey</code>，Galactica将自动为您生成一个。</p><h2 id="64a9" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">课堂讲稿和维基百科文章</h2><p id="664d" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">对于维基百科风格的文章或讲稿，以<code class="fe mu mv mw mx b">#</code>开始提示。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="23ac" class="nn lt iq mx b be no np l nq nr"># generate wiki style articles<br/>input_text = "# Multi-Head Attention\n\n"<br/><br/>generated_text = model.generate(input_text, new_doc=True)<br/>print(generated_text)<br/><br/>"""<br/># Multi-Head Attention<br/><br/>The multi-head attention mechanism is a generalization of the single-head attention mechanism. The multi-head attention mechanism is a combination of multiple single-head attention mechanisms. The multi-head attention mechanism is shown in Figure 2.<br/><br/>The multi- ...<br/>"""<br/><br/># generate lecture notes<br/>input_text = "# Lecture 1: The Ising Model\n\n"<br/><br/>generated_text = model.generate(input_text, new_doc=True)<br/>print(generated_text)<br/><br/>"""<br/># Lecture 1: The Ising Model<br/><br/># 1.1 The Ising Model<br/><br/>The Ising model is a simple model of ferromagnetism. It was introduced by Lenz in 1920 [[START_REF] Beitrag zur Theorie des Ferromagnetismus, Ising[END_REF]]<br/>"""<br/></span></pre><h1 id="aaac" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">引文预测</h1><p id="4991" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">Galactica在一个大型科学语料库上接受训练，该语料库包括超过3.6亿条上下文引用和超过5000万条不同来源的唯一引用。这使得模型能够建议引用并帮助发现相关论文。引用用<code class="fe mu mv mw mx b">[START_REF] TITLE, AUTHOR [END_REF]</code>表示。</p><h2 id="5c83" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">搜索</h2><p id="68f2" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">要搜索讨论某个主题的论文，请使用下面的提示— <code class="fe mu mv mw mx b">PAPER TOPIC [START_REF]</code>。由于<code class="fe mu mv mw mx b">[START_REF]</code>标记被附加到输入上下文的末尾，Galactica会将其视为引用的开始并完成其余部分。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="198a" class="nn lt iq mx b be no np l nq nr"># search citation<br/>input_text = "An NLP paper that compares different ways of encoding positions in Transformer-based architectures "<br/><br/>generated_text = model.generate(input_text + "[START_REF]")<br/>print(generated_text)<br/><br/>"""<br/>An NLP paper that compares different ways of encoding positions in Transformer-based architectures <br/>[START_REF] On Position Embeddings in BERT, Wang[END_REF]<br/>"""</span></pre><h2 id="c865" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">预言；预测；预告</h2><p id="44b3" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">除了明确地搜索引文，你还可以提示Galactica在一个文档中建议一篇相关的论文。该模型已经在许多学术文本上进行了训练，其中包括隐式引用图的表示。因此，给出一个提示，比如<code class="fe mu mv mw mx b">TEXT [START_REF]</code>，Galactica可以自动建议与<code class="fe mu mv mw mx b">TEXT</code>相关的引文。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="d6cc" class="nn lt iq mx b be no np l nq nr"># predict citation<br/>input_text = """Recurrent neural networks, long short-term memory and gated recurrent neural <br/>networks in particular, have been firmly established as state of the art <br/>approaches in sequence modeling and transduction problems such as language <br/>modeling and machine translation """<br/><br/>generated_text = model.generate(input_text + "[START_REF]")<br/>print(generated_text)<br/><br/>"""<br/>Recurrent neural networks, long short-term memory and gated recurrent neural <br/>networks in particular, have been firmly established as state of the art <br/>approaches in sequence modeling and transduction problems such as language <br/>modeling and machine translation [START_REF] Recurrent neural network based <br/>language model, Mikolov[END_REF][START_REF] Sequence to Sequence Learning with <br/>Neural Networks, Sutskever[END_REF][START_REF] Neural Machine Translation by <br/>Jointly Learning to Align and Translate, Bahdanau[END_REF] ...<br/>"""</span></pre><h2 id="156e" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">生成比检索好吗？</h2><p id="0e5c" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">人们可能会怀疑依靠Galactica进行引用预测，因为众所周知，大型语言模型会产生远离事实的内容。然而，Meta AI报告说，在引文预测任务中，Galactica的生成方法优于优化的稀疏和密集检索方法。Galactica远非完美，但实验表明，该模型比传统搜索引擎产生更好的结果。</p><h1 id="5e9a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">下游NLP任务</h1><p id="cfa8" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">您可以使用Galactica完成传统的下游NLP任务，例如摘要、实体提取和问题回答。虽然通用语言模型可能会与科学术语或医学术语发生冲突，但Galactica却不是这样。例如，Meta AI报告称，该模型在PubMedQA和MedMCOA问答基准上取得了新的最先进的结果；这两项任务都需要对高级生物医学概念有严格的理解。</p><h2 id="3e1a" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">摘要</h2><p id="e24f" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">要生成摘要，只需在文档末尾添加<code class="fe mu mv mw mx b">TLDR:</code>。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="1220" class="nn lt iq mx b be no np l nq nr"># summarization<br/>input_text = """Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community."""<br/><br/>generated_text = model.generate(input_text + "\n\nTLDR:", max_length=400)<br/>print(generated_text)<br/><br/>"""<br/>Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.<br/><br/>TLDR: We introduce Galactica, a large language model that can store, combine and reason about scientific knowledge.&lt;/s&gt;<br/>"""</span></pre><h2 id="80f7" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">实体提取</h2><p id="3343" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">您可以从问答格式的文档中提取实体。设计如下提示— <code class="fe mu mv mw mx b">TEXT\n\nQ:What scientific entities are mentioned in the abstract above?\n\nA:</code>。根据文档的主题，您可以用更特定于领域的术语来替换<code class="fe mu mv mw mx b">scientific entities</code>，比如<code class="fe mu mv mw mx b">biomedical entities</code>。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="350f" class="nn lt iq mx b be no np l nq nr"># entity extraction<br/>input_text = """Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community."""<br/>query = '\n\nQ: What scientific entities are mentioned in the abstract above?\n\nA:'<br/><br/>generated_text = model.generate(input_text + query, max_length=400)<br/>print(generated_text)<br/><br/>"""<br/>Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.<br/><br/>Q: What scientific entities are mentioned in the abstract above?<br/><br/>A: LaTeX equations, mathematical MMLU, MATH, PubMedQA, MedMCQA, BIG-bench&lt;/s&gt;<br/>"""</span></pre><h2 id="d8cd" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">问题回答</h2><p id="aa2f" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">在论文中，作者在问题前加上<code class="fe mu mv mw mx b">Q:</code>或<code class="fe mu mv mw mx b">Question:</code>。典型的格式是<code class="fe mu mv mw mx b">Question: QUERY\n\nAnswer:</code>。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="a243" class="nn lt iq mx b be no np l nq nr"># question answering<br/>query = "What is the notch signaling pathway?"<br/>input_text = f"Question: {query}\n\nAnswer:"<br/><br/>generated_text = model.generate(input_text)<br/>print(generated_text)<br/><br/>"""<br/>Question: What is the notch signaling pathway?<br/><br/>Answer: Notch signaling pathway is a cell-cell communication pathway that regulates cell fate decisions during development. It is involved in cell proliferation, differentiation, apoptosis, and cell migration. The Notch signaling pathway is activated by the binding of ...<br/>"""</span></pre><h1 id="5004" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">多模态任务</h1><p id="2422" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">如上所述，Galactica可以处理非结构化文本以外的模态，如LaTeX方程、代码、SMILES公式、DNA序列和氨基酸序列。该模型使用特定于任务的标记来支持各种形式的科学知识。这种设计使用户能够处理多模态任务，这些任务涉及自然语言和科学现象表示之间的交互。</p><h2 id="3691" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">数学</h2><p id="9646" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">数学方程用LaTeX表示。Galactica用括号<code class="fe mu mv mw mx b">\[ EQUATION \]</code>将LaTeX方程括起来，因此要生成数学描述，请确保以<code class="fe mu mv mw mx b">\[</code>结束提示。这是一个给定自然语言描述预测LaTeX方程的例子。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="3637" class="nn lt iq mx b be no np l nq nr"># predict math formula<br/>input_text = "The Schwarzschild radius is defined as: "<br/><br/>generated_text = model.generate(input_text + "\[")<br/>print(generated_text)<br/><br/>"""<br/>The Schwarzschild radius is defined as: \[r_{s}=\frac{2GM}{c^{2}}\]<br/>"""</span></pre><p id="be04" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果将自然语言转换成数学是可能的，那么反之亦然。卡拉狄加可以流利地在不同模态之间进行翻译，包括数学到自然语言的转换。当在研究论文中遇到难以理解的数学公式时，该功能尤其有用。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="01aa" class="nn lt iq mx b be no np l nq nr"># translate math formula to natural language<br/>math_formula= "\[\zeta(s) = \sum_{n=1}^{\infty} n^{-s}\]"<br/>input_text = f"Question: Translate the following Math formula: {math_formula} into plain English.\n\nAnswer:"<br/><br/>generated_text = model.generate(input_text, max_length=128)<br/>print(generated_text)<br/><br/>"""<br/>Question: Translate the following Math formula: \[\zeta(s) = \sum_{n=1}^{\infty} n^{-s}\] into plain English.<br/><br/>Answer: The Riemann zeta function is the sum of the reciprocals of all positive integers raised to the power of s.<br/>"""</span></pre><p id="52e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有趣的是，卡拉狄加不知何故学会了将数学方程式转换成代码。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="e318" class="nn lt iq mx b be no np l nq nr"># translate math formula to code<br/>math_formula= "\[\zeta(s) = \sum_{n=1}^{\infty} n^{-s}\]"<br/>input_text = f"Question: Translate the following Math formula: {math_formula} into Python code.\n\nAnswer:"<br/><br/>generated_text = model.generate(input_text, max_length=128)<br/>print(generated_text)<br/><br/>"""<br/>Question: Translate the following Math formula: \[\zeta(s) = \sum_{n=1}^{\infty} n^{-s}\]into Python code.<br/><br/>Answer:<br/><br/>def zeta(s):<br/>    return sum(n**(-s) for n in range(1, 1000000))<br/>"""</span></pre><h2 id="6f17" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">密码</h2><p id="596b" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">下面是一个将代码片段翻译成自然语言描述和数学描述的演示。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="0bd7" class="nn lt iq mx b be no np l nq nr"># translate code to natural language<br/>code_snippet = """<br/>def cheapestProduct(products: List[Product]) -&gt; Product:<br/>    return min(products, key=lambda p: p.price)<br/>"""<br/><br/>input_text = f"Question: Translate the following Python code:{code_snippet}\ninto plain English\n\nAnswer:"<br/><br/>generated_text = model.generate(input_text)<br/>print(generated_text)<br/><br/>"""<br/>Question: Translate the following Python code:<br/><br/>def cheapestProduct(products: List[Product]) -&gt; Product:<br/>    return min(products, key=lambda p: p.price)<br/><br/>into plain English.<br/><br/>Answer: The function returns the product with the lowest price.<br/>"""<br/><br/><br/># translate code to math formula<br/>input_text = f"Question: Translate the following Python code:{code_snippet}\ninto math formula\n\nAnswer:"<br/><br/>generated_text = model.generate(input_text)<br/>print(generated_text)<br/><br/>"""<br/>Question: Translate the following Python code:<br/><br/>def cheapestProduct(products: List[Product]) -&gt; Product:<br/>    return min(products, key=lambda p: p.price)<br/><br/>into math formula.<br/><br/>Answer: \operatorname{argmin}_{p \in \text{products}} p.\text{price}<br/>"""</span></pre><h2 id="c8c8" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">化学理解</h2><p id="c3f4" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">通过提供一个SMILES数据接口，Galactica可以帮助组织化学信息。SMILES公式将化学结构表示为一系列字符，并与自然语言描述或IUPAC名称(命名有机化合物的方法)一起出现在Galactica训练语料库中。这表明该模型可能已经学会在给定SMILES公式输入的情况下预测IUPAC名称。</p><p id="3002" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于IUPAC名称预测，设计一个类似于PubChem文档的提示— <code class="fe mu mv mw mx b">[START_I_SMILES] SMILES_FORMULA [END_I_SMILES]\n\n## Chemical and Physical Properties\n\nThe following are chemical properties for</code>。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="00ad" class="nn lt iq mx b be no np l nq nr"># IUPAC name prediction<br/>smiles_formula = "C(C(=O)O)N"<br/>input_text = f"[START_I_SMILES]{smiles_formula}[END_I_SMILES]\n\n## Chemical and Physical Properties\n\nThe following are chemical properties for"<br/><br/>generated_text = model.generate(input_text)<br/>print(generated_text)<br/><br/>"""<br/>[START_I_SMILES]C(C(=O)O)N[END_I_SMILES]<br/><br/>## Chemical and Physical Properties<br/><br/>The following are chemical properties for 2-amino-2-oxo-acetic acid<br/>"""<br/><br/># Note this is an incorrect prediction. IUPAC name prediction doesn't seem to work well with the standard model (6.7B)</span></pre><h2 id="2e99" class="ns lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">生物学理解</h2><p id="2327" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">Galactica处理生物形态(即DNA和氨基酸序列)的能力可能在帮助组织生物医学信息方面发挥作用。例如，该模型可以用功能关键字来注释蛋白质序列。作者声称，该模型学会了将序列与它在训练中看到的相似序列进行匹配，并可以利用这一点来预测功能关键词。</p><p id="3055" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">蛋白质注释的提示可以设计如下— <code class="fe mu mv mw mx b">[START_AMINO] AMINO_ACID_SEQUENCE [END_AMINO]\n\n## Keywords</code>。</p><pre class="lc ld le lf gt nj mx nk bn nl nm bi"><span id="4e7b" class="nn lt iq mx b be no np l nq nr"># protein functional keyword prediction<br/>protein_seq = "GHMQSITAGQKVISKHKNGRFYQCEVVRLTTETFYEVNFDDGSFSDNLYPEDIVSQDCLQFGPPAEGEVVQVRWTDGQVYGAKFVASHPIQMYQVEFEDGSQLVVKRDDVYTLDEELP"<br/>input_text = f"[START_AMINO]{protein_seq}[END_AMINO]\n\n## Keywords"<br/><br/>generated_text = model.generate(input_text, max_length=512)<br/>print(generated_text)<br/><br/>"""<br/>[START_AMINO]GHMQSITAGQKVISKHKNGRFYQCEVVRLTTETFYEVNFDDGSFSDNLYPEDIVSQDCLQFGPPAEGEVVQVRWTDGQVYGAKFVASHPIQMYQVEFEDGSQLVVKRDDVYTLDEELP[END_AMINO]<br/><br/>## Keywords<br/><br/>Cytoplasm, Methyltransferase, rRNA processing, S-adenosyl-L-methionine, Transferase<br/>"""</span></pre><h1 id="b1d5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="b082" class="pw-post-body-paragraph kf kg iq kh b ki ne jr kk kl nf ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">本教程概述了Meta AI的Galactica如何使用户能够访问科学知识并利用这些知识来处理各种科学NLP任务。这些任务从生成学术文献到处理多模态数据，所有这些任务都可能在科学发现中发挥作用。尽管该模型远非完美，但实验表明，该模型的预测往往优于传统方法。该模型令人印象深刻的性能和易用性值得进一步研究和探索。在未来，我希望我们能继续探索像这样的大型语言模型如何帮助科学研究和科学本身的进程。</p></div></div>    
</body>
</html>