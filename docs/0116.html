<html>
<head>
<title>MINE: Mutual Information Neural Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">采矿:互信息神经估计</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/mine-mutual-information-neural-estimation-26f6853febda?source=collection_archive---------0-----------------------#2019-07-23">https://pub.towardsai.net/mine-mutual-information-neural-estimation-26f6853febda?source=collection_archive---------0-----------------------#2019-07-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ac58" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="69b3" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">利用任意神经网络估计互信息</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/23889358aed39a41d64239da344c2d5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m2eVV-eX1oLqCP0Ylp99eA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:istock.com/ipopba</figcaption></figure><h1 id="45ba" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">介绍</h1><p id="0f91" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">互信息，也称为信息增益，已经成功地用于深度学习(我们很快就会看到)和深度强化学习(例如，<a class="ae mv" href="https://arxiv.org/abs/1605.09674" rel="noopener ugc nofollow" target="_blank"> VIME </a>，<a class="ae mv" href="https://arxiv.org/abs/1810.01176" rel="noopener ugc nofollow" target="_blank"> EMI </a>)的上下文中，来测量/增强两个表示之间的耦合。在这篇文章中，我们详细讨论了Mohamed Ishmael Belghazi等人在ICML 2018年发表的一个名为<a class="ae mv" href="https://arxiv.org/pdf/1801.04062.pdf" rel="noopener ugc nofollow" target="_blank"> MINE </a>(互信息神经估计)的神经估计器，它允许我们直接估计互信息。</p><p id="a4c8" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">本文由三部分组成:我们首先介绍互信息的概念，为更好地理解我们正在处理的事情建立一些直觉。然后我们介绍了挖掘算法，并讨论了它在深度学习中的一些应用。最后，我们会提供你需要的所有证据，让你明白我们在这里谈论的是什么。</p><p id="a94d" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">注意，这是我们关于互信息系列的第一篇文章。未来几周还会有更多。希望你旅途愉快！</p><h1 id="55e6" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">交互信息</h1><p id="e8cf" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">数学上，两个变量<em class="nb"> X </em>和<em class="nb"> Z </em>之间的互信息定义为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/601dca987ac1adeb4d120c5b27c7a7e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qjH5e-5K5LBc9cBWClbNFw.png"/></div></div></figure><p id="98ef" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">其中香农熵<em class="nb">H(X)=-E _ { p(X)}[log(p(X))]</em>量化了<em class="nb"> X </em>中的不确定性，条件熵<em class="nb"> H(X|Z)=-E_{p(x，z)}[log(p(x|z))] </em>度量了给定<em class="nb">Z</em>中的不确定性</p><p id="bdac" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">对于相互信息有三种略有不同的解释:</p><ol class=""><li id="d7cc" class="nd ne it mb b mc mw mf mx mi nf mm ng mq nh mu ni nj nk nl bi translated">形式上，互信息捕捉随机变量之间的统计相关性。(相比之下，<a class="ae mv" href="https://en.wikipedia.org/wiki/Correlation_coefficient" rel="noopener ugc nofollow" target="_blank">相关系数</a>仅捕捉线性相关性)。</li><li id="28ae" class="nd ne it mb b mc nm mf nn mi no mm np mq nq mu ni nj nk nl bi translated">直观地说，<em class="nb"> X </em>和<em class="nb"> Z </em>之间的互信息描述了从关于<em class="nb"> X </em>的<em class="nb"> Z </em>的知识中学习到的信息量，反之亦然。</li><li id="8aed" class="nd ne it mb b mc nm mf nn mi no mm np mq nq mu ni nj nk nl bi translated">直接的解释就是给定<em class="nb"> Z </em>或者反过来就是<em class="nb"> X </em>的不确定性的减少。</li></ol><p id="57bb" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">互信息也等价于联合概率<em class="nb"> P(X，Z) </em>和余量<em class="nb"> P(X) </em> ⊗ <em class="nb"> P(Z) </em>的乘积之间的KL-散度:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/6e72f62935e7b7119b97e8b205ea6337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3SuE3V1ihTu54fXyW9WEQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">等式1 .证明将在最后一节中给出</figcaption></figure><p id="f993" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">这种KL-divergence背后的直觉是，关节与边缘的乘积越不同，<em class="nb"> X </em>和<em class="nb"> Z </em>之间的依赖性就越强。</p><h1 id="6a43" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">挖掘算法</h1><p id="8edd" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">MINE估计量给出了互信息<em class="nb">I(X；Z) </em>通过计算</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/088cd2e6919aa7549593914a2578033f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*nHyER2kVKL0eU4p1s4CnmQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">等式2 .互信息I(X；z)。证明将在最后一节给出</figcaption></figure><p id="36ec" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">其中<em class="nb"> T </em>可以是将<em class="nb"> x </em>和<em class="nb"> z </em>作为输入并输出实数的任何函数。这个最大的下限表明，我们可以自由地使用任何神经网络来近似互信息，并且由于神经网络的表达能力，这样的近似可以获得任意的精度。下面的算法正是我们刚刚描述过的，其中<em class="nb"> θ </em>表示网络参数</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/68dfa47b3959abd2d04fef8534e35603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wksg_OVp5xrdgaKW2E0Zww.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">我的算法。来源:互信息神经估计</figcaption></figure><p id="c8f6" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><em class="nb"> V(θ) </em>的梯度为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/babcaf191860c6775408e6266e6c4659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PP5F3iMlpSd_eYbgpThy3w.png"/></div></div></figure><p id="12da" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">这是有偏差的，因为分母中的期望和小批梯度计算，作者建议用指数移动平均代替分母中的期望。对于小的学习率，这种改进的矿梯度估计器可以具有任意小的偏差。</p><h2 id="e485" class="nv li it bd lj nw nx dn ln ny nz dp lr mi oa ob lt mm oc od lv mq oe of lx iz bi translated"><strong class="ak">公平性</strong></h2><p id="1bb2" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">设<em class="nb"> Y=f(X)+σ⊙ϵ </em>，其中<em class="nb"> f </em>为确定性非线性变换，ϵ为随机噪声。互信息的属性<em class="nb">相等性</em>表示<em class="nb"> X </em>与<em class="nb"> Y、</em>T28】I(X；y)，对于噪声量<em class="nb"> σ⊙ϵ </em>是不变的，并且应该仅取决于噪声量。即无论<em class="nb"> f(x)=x，f(x)=x </em>还是别的什么，<em class="nb">I(X；只要σ⊙ϵ保持不变，就大致保持不变。下面的快照展示了我的捕获的公平性</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/362f0347d5ec3ced94ca22bc2eb683cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EVHrmn-PHQTnPhS0NxhQnQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">我的公平属性。来源:互信息神经估计</figcaption></figure><h2 id="e67a" class="nv li it bd lj nw nx dn ln ny nz dp lr mi oa ob lt mm oc od lv mq oe of lx iz bi translated">我的应用</h2><p id="6e6a" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">最大化互信息以改善GANs </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/7be15551380d66e8f45b3d430d36d3d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UAioUeLSobSMUEveC4DTTA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">InfoGAN。通过红色虚线的互信息损失反向传播的梯度</figcaption></figure><p id="98c3" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">GANs通常面临模式崩溃问题，即生成器未能捕捉到训练数据的多样性。InfoGANs(Chen等人[2])通过定义变量<em class="nb">、【c】</em>、噪声<em class="nb">、</em>和编码变量<em class="nb"> c </em>的串联的潜在向量来缓解这个问题，如上图所示。噪声向量<em class="nb"> ϵ </em>被视为不可压缩噪声源，而潜在代码<em class="nb"> c </em>捕捉数据分布的显著结构化语义特征。在一个普通的GAN中，生成器可以忽略<em class="nb"> c </em>,因为没有任何东西强制这种依赖性。为了加强这种依赖性，我们给发电机损耗增加了一个额外的互信息项，如下所示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/ce957e20ee159531cca93fbf4d053eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMizapCrtsucGkV2vXYteA.png"/></div></div></figure><p id="6c5d" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">请注意，互信息是无界的，因此<em class="nb"> Lₘ </em>可以压倒<em class="nb"> L_μ </em>，导致算法的失败模式，其中生成器将其所有注意力放在最大化互信息上，而忽略了与鉴别器的对抗博弈。作者提出自适应地从互信息中剪切梯度，使得其Frobenius范数至多是来自鉴别器的梯度的范数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/1c7a26915daf395c230dfb2fb02f0319.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*sDwL5B5LndCCTFITb7aRcQ.png"/></div></figure><p id="1f4a" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">现在我们回到<em class="nb"> c. </em> <em class="nb"> c </em>是从一个辅助分布<em class="nb"> Q </em>，一个被<em class="nb"> I(G([ϵ，c】优化的网络中取样的；c) </em>到<em class="nb"> c </em>(如红色虚线所示)。在大多数实验中，<em class="nb"> Q </em>与鉴别器<em class="nb"> D </em>共享所有卷积层，并且有一个附加的最终全连接层来输出条件分布<em class="nb"> Q(c|x) </em>的参数，例如连续潜在码的正态分布的均值和方差。此外，由于<em class="nb"> c </em>是从条件分布<em class="nb"> Q(c|x </em>中采样的，我们可能还必须求助于重新参数化技巧，以便更新<em class="nb"> Q </em>。</p><p id="ddff" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">最大化互信息以改善BiGANs </strong></p><p id="3085" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">BiGANs简介</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/956c3962676b6e893567da53afde69bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gXUnnd_x58JKqUWFQxz2dA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">比根的建筑。来源:对抗性特征学习</figcaption></figure><p id="c08b" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">双向生成对抗网络，BiGANs(Jeff Donahue等人[3])，顾名思义，是双向的，因为真实数据在被传递到鉴别器之前被编码。鉴别器将特征表示(<em class="nb"> z </em>和<em class="nb"> E(x) </em>)和完全代表性数据(<em class="nb"> G(z) </em>和<em class="nb"> x </em>)作为输入，以区分它们。发生器和编码器协作，通过接近<em class="nb"> E(x) </em>到<em class="nb"> z </em>和<em class="nb"> G(z) </em>到<em class="nb"> x </em>来欺骗鉴别器。</p><p id="16ad" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd"> BiGANs配合矿上</strong></p><p id="ad7f" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">为了减小重构误差，作者证明了重构误差<em class="nb"> R </em>有界</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a496effaabe24d230f2d20324411da4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*I0usFR9JScbjoeE2DCjgvA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">等式3重建误差的上限。证明将在最后展示</figcaption></figure><p id="74e1" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">其中<em class="nb"> p </em>表示发生器部分的联合概率，而<em class="nb"> q </em>是编码器部分的联合概率。从上面的不等式我们不难看出，最大化<em class="nb"> x </em>和<em class="nb"> E(x) </em>之间的互信息可以最小化重构误差。因此，我们将额外的互信息添加到编码器损耗中，得到</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/904d485ab9a30209cff6dfbc16368d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYnjbkNcqUJmg1U2eDovVQ.png"/></div></div></figure><p id="2c8f" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">信息瓶颈</strong></p><p id="b0bb" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">信息瓶颈的要旨是总结出一个随机变量<em class="nb"> X </em>从而达到精度和复杂度的最佳权衡。也就是说，我们要提取出最简洁的表示<em class="nb">Z<em class="nb">来捕捉<em class="nb">X</em>T20】与预测<em class="nb"> Y </em>最相关的</em>的因素。因此，信息瓶颈方法的目标是最小化拉格朗日量</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/9d7fbec50e11d9522ffa14a07cd03127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uq8Do0YZPLVKxCFfCkc3ug.png"/></div></div></figure><p id="0870" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">其中<em class="nb"> β </em>是拉格朗日乘数。第一项最小化给定<em class="nb"> Z </em>的<em class="nb"> Y </em>的不确定性，而第二项最小化<em class="nb"> X </em>和<em class="nb"> Z </em>之间的依赖性。</p><h1 id="f122" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">补充材料</h1><p id="7f93" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">公式1的证明</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/831f7071c6c0ccba229365850c65e718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iJQJPRx3YPgUGFXeR_Ebgw.png"/></div></div></figure><p id="e33d" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">证明方程2是<em class="nb">I(X；z)。</em> </strong></p><p id="9d0c" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">为了说明等式2给出了<em class="nb">I(X；Z) </em>，我们只需要证明下面的陈述</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/e5d6c339732f3d4736cd4de359f0e13f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*1lYvXK7OXV8Gjy6WyFDWrg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">等式4 .等式2的等效物</figcaption></figure><p id="3030" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">我们首先定义吉布斯分布</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/921cb1e2d19f3d6e2046b40235a4f120.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*pqUEdVwENSGyJ_fa-OgieQ.png"/></div></figure><p id="4f34" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">然后我们用我们刚刚定义的吉布斯分布<em class="nb"> G </em>来表示等式4的左边</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/32aa92a392d283a27e90471985752d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k1SqdjWf-0mzbHM2t2wvEQ.png"/></div></div></figure><p id="4dfd" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">现在我们计算等式4左右两边的差值</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/e4ebb044e73bd625cfb97024ff659682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bWkkp3OkU5sYGYnR9rSECA.png"/></div></div></figure><p id="d3a7" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">KL-散度的正定性确保δ≥0，因此等式4始终成立。</p><p id="0faa" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated"><strong class="mb jd">公式3的证明</strong></p><p id="8a81" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">重建误差定义为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/31eae5f66ca1b73e7777fb393fa2d567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*ysDuJazqqyTYYOVJZY0huw.png"/></div></figure><p id="49be" class="pw-post-body-paragraph lz ma it mb b mc mw kd me mf mx kg mh mi my mk ml mm mz mo mp mq na ms mt mu im bi translated">其中<em class="nb"> p </em>是发电机，<em class="nb"> q </em>是编码器。这大致测量了发生器从编码器编码的<em class="nb"> z </em>中恢复<em class="nb"> x </em>的保真度损失。现在我们有了</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/5e87c0364e3e7a381949739381a118f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WcNzrPpmozRmnYF5W5X7fQ.png"/></div></div></figure><h2 id="ee6d" class="nv li it bd lj nw nx dn ln ny nz dp lr mi oa ob lt mm oc od lv mq oe of lx iz bi translated">参考</h2><ol class=""><li id="e5d4" class="nd ne it mb b mc md mf mg mi ov mm ow mq ox mu ni nj nk nl bi translated">Mohamed Ishmael Belghazi等.互信息神经估计</li><li id="853a" class="nd ne it mb b mc nm mf nn mi no mm np mq nq mu ni nj nk nl bi translated">信息最大化生成对抗网络的可解释表示学习</li><li id="e58d" class="nd ne it mb b mc nm mf nn mi no mm np mq nq mu ni nj nk nl bi translated">杰夫·多纳休等,《对抗性特征学习》</li></ol></div></div>    
</body>
</html>