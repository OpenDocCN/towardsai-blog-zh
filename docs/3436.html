<html>
<head>
<title>Building A Recurrent Neural Network From Scratch In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头开始构建递归神经网络</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/building-a-recurrent-neural-network-from-scratch-in-python-3ad244b1054f?source=collection_archive---------1-----------------------#2022-12-23">https://pub.towardsai.net/building-a-recurrent-neural-network-from-scratch-in-python-3ad244b1054f?source=collection_archive---------1-----------------------#2022-12-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8c0e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用基本的Python库构建基本的RNN</h2></div><p id="f5e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">通用神经网络(RNN)是一个非常强大的自然语言处理和其他序列建模任务的模型，因为它们有一个所谓的记忆细胞。他们可以一次读取一个输入𝑥⟨𝑡⟩(如单词)，并通过从一个步骤传递到下一个步骤的隐藏层激活来记住一些上下文信息。这允许单向(单向)RNN从过去获取信息来处理后来的输入。双向(双向)RNN可以从过去和未来获取上下文。</p><p id="daa3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将使用Python编程语言的基本函数和库从头开始实现RNN模型。我们首先将开始构建RNN的前向传播，然后是RNN的后向传播，并将它们结合在一起以得到一个功能完整的RNN模型。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/35108f6bcb1eb70d6d05e32108cdfce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WfIMVnh1RCtk9oPy"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">照片由<a class="ae md" href="https://unsplash.com/@deepmind?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>在<a class="ae md" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h2 id="1c8e" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">目录:</h2><ol class=""><li id="4a0a" class="mx my it kk b kl mz ko na kr nb kv nc kz nd ld ne nf ng nh bi translated"><strong class="kk iu">递归神经网络的前向传播<br/> 1.1。RNN细胞<br/> 1.2。RNN向前传球</strong></li><li id="1ed1" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated"><strong class="kk iu">递归神经网络的反向传播<br/> 2.1。RNN向后单元格<br/> 2.2。RNN向后传球</strong></li><li id="ce41" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated"><strong class="kk iu">参考</strong></li></ol></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="3ee1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">如果你想免费学习数据科学和机器学习，看看这些资源:</strong></p><ul class=""><li id="6d73" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">免费互动路线图，自学数据科学和机器学习。从这里开始:<a class="ae md" href="https://aigents.co/learn/roadmaps/intro" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn/roadmaps/intro</a></li><li id="5ec4" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">数据科学学习资源搜索引擎(免费)。将你最喜欢的资源加入书签，将文章标记为完整，并添加学习笔记。<a class="ae md" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></li><li id="d26e" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">想要在导师和学习社区的支持下从头开始学习数据科学吗？免费加入这个学习圈:<a class="ae md" href="https://community.aigents.co/spaces/9010170/" rel="noopener ugc nofollow" target="_blank">https://community.aigents.co/spaces/9010170/</a></li></ul></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="c01f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想在数据科学&amp;人工智能领域开始职业生涯，但不知道如何开始。我提供数据科学指导课程和长期职业指导:</p><ul class=""><li id="922f" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">长期指导:<a class="ae md" href="https://lnkd.in/dtdUYBrM" rel="noopener ugc nofollow" target="_blank">https://lnkd.in/dtdUYBrM</a></li><li id="c794" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">辅导会议:<a class="ae md" href="https://lnkd.in/dXeg3KPW" rel="noopener ugc nofollow" target="_blank">https://lnkd.in/dXeg3KPW</a></li></ul><p id="1d09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ny">加入</em> </strong> <a class="ae md" href="https://youssefraafat57.medium.com/membership" rel="noopener"> <strong class="kk iu"> <em class="ny">中等会员</em> </strong> </a> <strong class="kk iu"> <em class="ny">计划继续无限制学习。如果你使用下面的链接，我会收到一小部分会员费，不需要你额外付费。</em>T13】</strong></p><div class="nz oa gp gr ob oc"><a href="https://youssefraafat57.medium.com/membership" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">加入我的介绍链接媒体-优素福胡斯尼</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">阅读Youssef Hosni(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">youssefraafat57.medium.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq lx oc"/></div></div></a></div></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="d040" class="or mf it bd mg os ot ou mj ov ow ox mm jz oy ka mp kc oz kd ms kf pa kg mv pb bi translated">1.递归神经网络的前向传播</h1><p id="3c59" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">我们将从RNN的正向传播开始。在本文中，我们将实现如图1所示的基本RNN模型。</p><p id="b398" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">输入𝑥:的尺寸</strong></p><p id="b3b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用𝑛𝑥单位数输入</p><ul class=""><li id="198d" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">对于单个输入示例的单个时间步长，𝑥(𝑖)⟨𝑡⟩是一维输入向量。</li><li id="cfed" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">以语言为例，一种有5000个单词的语言可以一次性编码成一个有5000个单位的向量。所以𝑥(𝑖)⟨𝑡⟩would有形状(5000，)。</li><li id="36a8" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">这里使用符号𝑛𝑥来表示单个训练示例的单个时间步长中的单位数。</li></ul><p id="f379" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">𝑇𝑥尺寸的时间步长</strong></p><ul class=""><li id="31ac" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">一个递归神经网络有多个时间步长，你可以用𝑡.来索引</li></ul><p id="4cd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">批量𝑚 </strong></p><ul class=""><li id="7f61" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">假设我们有小批量，每个小批量有20个训练样本</li><li id="6340" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">为了从矢量化中受益，您将堆叠20列𝑥(𝑖)示例</li><li id="a098" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">例如，这个张量的形状是(5000，20，10)</li><li id="301a" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">您将使用𝑚m来表示训练样本的数量</li><li id="5c7e" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">S9o，小批量的形状是(𝑛𝑥，𝑚，𝑇𝑥)</li></ul><p id="f216" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">三维形状张量(𝑛𝑥,𝑚,𝑇𝑥) </strong></p><ul class=""><li id="2992" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">形状的三维张量𝑥x(𝑛𝑥,𝑚,𝑇𝑥)表示被馈入RNN的输入𝑥。</li></ul><p id="d364" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为每个时间步长取一个2D切片:𝑥⟨𝑡⟩</p><ul class=""><li id="1a8a" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">在每个时间步骤，您将使用一个小批量的训练示例(不仅仅是一个示例)</li><li id="ce9e" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">因此，对于每一个时间步𝑡，你将使用一个二维切片的形状(𝑛𝑥,𝑚)</li><li id="e171" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">这个2D切片被称为𝑥⟨𝑡⟩.</li></ul><p id="d56f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">隐藏状态𝑎的定义</strong></p><ul class=""><li id="75dd" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">从一个时间步骤到另一个时间步骤传递给RNN的激活𝑎⟨𝑡⟩a⟨t⟩被称为“隐藏状态”</li></ul><p id="0fcb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">隐藏状态𝑎的维度</strong></p><ul class=""><li id="3f48" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">类似于输入张量𝑥x，单个训练示例的隐藏状态是长度为𝑛𝑎的向量</li><li id="a97f" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">如果包括小批量的𝑚m培训示例，小批量的形状为(𝑛𝑎,𝑚)</li><li id="8593" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">当包括时间步长维度时，隐藏状态的形状为(𝑛𝑎,𝑚,𝑇𝑥)</li><li id="b2cb" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">您将使用索引𝑡t循环时间步长，并使用3D张量的2D切片</li><li id="5355" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">这个2D切片被称为𝑎⟨𝑡⟩a⟨t⟩</li><li id="1782" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">这个二维切片的形状是(𝑛𝑎,𝑚)</li></ul><p id="d0ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">预测𝑦̂的维度</strong></p><ul class=""><li id="b08e" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">类似于输入和隐藏状态，𝑦̂是一个三维形状张量(𝑛𝑦,𝑚,𝑇𝑦)</li><li id="265e" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">代表预测的向量中的𝑛𝑦:单位数</li><li id="e842" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">小批量样品的𝑚:数量</li><li id="e019" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">预测中的𝑇𝑦:时间步数</li></ul><p id="6bb0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于单个时间步长𝑡，2D切片𝑦̂ ⟨𝑡⟩已经成形(𝑛𝑦,𝑚)</p><p id="9c4e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">下面是你如何实现一个RNN: </strong></p><ul class=""><li id="0273" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">实现RNN的一次性步骤所需的计算。</li><li id="5d0f" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">在𝑇𝑥Tx时间步长上执行循环，以便一次处理一个输入。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pf"><img src="../Images/7740a4e6bf4c76c9b00c566c643ae3bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oeHAJl_wJym37EFJE5dVZA.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图一。基本RNN模型</figcaption></figure></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="722f" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">1.1.递归神经网络细胞</h2><p id="0363" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">我们可以把图1所示的RNN模型看作是图2所示的单个细胞的重复使用。首先，我们将实现一个单个单元，然后我们可以循环遍历它，将多个单个单元相互堆叠起来，并创建RNN模型的正向传递。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pg"><img src="../Images/3ec430f4ed60a789e1d672171d6f0cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkDuISOMfs0sgsdIns5TzQ.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图二。基本RNN细胞。</figcaption></figure><p id="1131" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基本RNN单元将𝑥⟨𝑡⟩(当前输入)和𝑎⟨𝑡−1⟩(包含来自过去的信息的先前隐藏状态)作为输入，并输出𝑎⟨𝑡⟩a⟨，该被提供给下一个RNN单元，并且也用于预测𝑦̂ ⟨𝑡⟩.</p><p id="7bdf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们通过以下四个主要步骤来实现图2所示的RNN单元:</p><ol class=""><li id="0725" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld ne nf ng nh bi translated">用tanh激活计算隐藏状态:𝑎⟨𝑡⟩=tanh(𝑊⟨𝑎𝑎⟩*𝑎⟨𝑡−1⟩+𝑊⟨𝑎𝑥⟩*𝑥⟨𝑡⟩+𝑏⟨𝑎⟩)</li><li id="3e85" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">使用新的隐藏状态𝑎⟨𝑡⟩，计算预测𝑦̂ ⟨𝑡⟩ = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑊⟨𝑦𝑎⟩*𝑎⟨𝑡⟩ +𝑏⟨𝑦⟩).</li><li id="0d24" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">商店(𝑎⟨𝑡⟩、𝑎⟨𝑡−1⟩、𝑥⟨𝑡⟩、𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)中的一家<code class="fe ph pi pj pk b">cache</code></li><li id="0cf9" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">返回𝑎⟨𝑡⟩、𝑦̂ ⟨𝑡⟩和<code class="fe ph pi pj pk b">cache</code></li></ol><p id="b572" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先实现softmax激活函数:</p><pre class="lo lp lq lr gt pl pk pm bn pn po bi"><span id="5af9" class="pp mf it pk b be pq pr l ps pt">def softmax(x):<br/>    e_x = np.exp(x - np.max(x))<br/>    return e_x / e_x.sum(axis=0)</span></pre><p id="3b62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们使用下面的代码实现上面显示的四个步骤:</p><pre class="lo lp lq lr gt pl pk pm bn pn po bi"><span id="e32e" class="pp mf it pk b be pq pr l ps pt">def rnn_cell_forward(xt, a_prev, parameters):<br/>    """<br/>    Implements a single forward step of the RNN-cell as described in Figure (2)<br/><br/>    Arguments:<br/>    xt -- your input data at timestep "t", numpy array of shape (n_x, m).<br/>    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)<br/>    parameters -- python dictionary containing:<br/>                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)<br/>                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)<br/>                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)<br/>                        ba --  Bias, numpy array of shape (n_a, 1)<br/>                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)<br/>    Returns:<br/>    a_next -- next hidden state, of shape (n_a, m)<br/>    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)<br/>    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)<br/>    """<br/>    <br/>    # Retrieve parameters from "parameters"<br/>    Wax = parameters["Wax"]<br/>    Waa = parameters["Waa"]<br/>    Wya = parameters["Wya"]<br/>    ba = parameters["ba"]<br/>    by = parameters["by"]<br/>    <br/>    # compute next activation state using the formula given above<br/>    a_next = np.tanh(np.dot(Wax,xt) + np.dot(Waa,a_prev) + ba)<br/>    # compute output of the current cell using the formula given above<br/>    yt_pred = softmax(np.dot(Wya,a_next) + by) <br/>    <br/>    # store values you need for backward propagation in cache<br/>    cache = (a_next, a_prev, xt, parameters)<br/>    <br/>    return a_next, yt_pred, cache</span></pre></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="1188" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">1.2.RNN向前传球</h2><p id="34a0" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">递归神经网络(RNN)是我们刚刚建立的RNN细胞的重复。如果你的数据输入序列是10个时间步长，那么你将重复使用RNN单元10次。这如图3所示。</p><p id="6aef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个单元在每个时间步长接受两个输入:</p><ul class=""><li id="169a" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">𝑎⟨𝑡−1⟩:来自前一个单元格的隐藏状态</li><li id="31ff" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">𝑥⟨𝑡⟩:当前时间步的输入数据</li></ul><p id="d49a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它在每个时间步有两个输出:</p><ul class=""><li id="3681" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">一个隐藏的国家(𝑎⟨𝑡⟩)</li><li id="a47c" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated">一个预言(𝑦⟨𝑡⟩)</li></ul><p id="14b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">权重和偏差(𝑊𝑎𝑎、𝑏𝑎、𝑊𝑎𝑥、𝑏𝑥)在每个时间步重复使用</p><ul class=""><li id="049f" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated">它们在调用“参数”字典中的<code class="fe ph pi pj pk b">rnn_cell_forward</code>之间被维护</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pu"><img src="../Images/c63bfa9e4fa46776d0b29a934d6e3e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Shfskt65uZezymFQ5sawww.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图3。RNN模型</figcaption></figure><p id="9e4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">实现如图3 </strong>所示的RNN模型:</p><ol class=""><li id="5334" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld ne nf ng nh bi translated">创建一个由0和形状(𝑛𝑎、𝑚、𝑇𝑥)的<strong class="kk iu">和</strong>组成的3D数组，该数组将存储RNN计算出的所有隐藏状态。</li><li id="292f" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">创建一个形状为(𝑛𝑦，𝑚，𝑇𝑥)的零，𝑦̂的三维数组来存储预测。请注意，在这种情况下，𝑇𝑦=𝑇𝑥(预测和输入具有相同数量的时间步长)</li><li id="0920" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">通过将2D隐藏状态<code class="fe ph pi pj pk b">a_next</code>设置为初始隐藏状态𝑎0来初始化它</li><li id="7f6d" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">在每个时间步𝑡 : <br/> 4.1。<strong class="kk iu">得到</strong> 𝑥⟨𝑡⟩，这是一个单一时间步长𝑡.的𝑥的二维切片𝑥⟨𝑡⟩有形状(𝑛𝑥，𝑚)，x有形状(𝑛𝑥，𝑚，𝑇𝑥).<br/> 4.1。<strong class="kk iu">更新</strong>2d隐藏状态𝑎⟨𝑡⟩(变量名<code class="fe ph pi pj pk b">a_next</code>)、预测𝑦̂⟨𝑡⟩和缓存通过运行<code class="fe ph pi pj pk b">rnn_cell_forward .</code>a⟨t⟩已经成形(𝑛⟨𝑎⟩、𝑚).<br/> 4.3。<strong class="kk iu">存储</strong>3d张量𝑎中的2D隐藏状态，在𝑡-th位置a有形状(𝑛𝑎，𝑚，𝑇⟨𝑥⟩) <br/> 4.4。<strong class="kk iu">在𝑡-th位置的3D张量𝑦̂ 𝑝𝑟𝑒𝑑中存储</strong>2d𝑦̂⟨𝑡⟩预测(变量名<code class="fe ph pi pj pk b">yt_pred</code>)。𝑦̂ ⟨𝑡⟩有形状(𝑛𝑦,𝑚)和𝑦̂有形状(𝑛𝑦,𝑚,𝑇𝑥) <br/> 4.6。<strong class="kk iu">将缓存添加到缓存列表中</strong></li><li id="1c1b" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld ne nf ng nh bi translated">返回3D张量𝑎a和𝑦̂，以及缓存列表</li></ol><pre class="lo lp lq lr gt pl pk pm bn pn po bi"><span id="db8e" class="pp mf it pk b be pq pr l ps pt">def rnn_forward(x, a0, parameters):<br/>    """<br/>    Implement the forward propagation of the recurrent neural network described in Figure (3).<br/><br/>    Arguments:<br/>    x -- Input data for every time-step, of shape (n_x, m, T_x).<br/>    a0 -- Initial hidden state, of shape (n_a, m)<br/>    parameters -- python dictionary containing:<br/>                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)<br/>                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)<br/>                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)<br/>                        ba --  Bias numpy array of shape (n_a, 1)<br/>                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)<br/><br/>    Returns:<br/>    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)<br/>    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)<br/>    caches -- tuple of values needed for the backward pass, contains (list of caches, x)<br/>    """<br/>    <br/>    # Initialize "caches" which will contain the list of all caches<br/>    caches = []<br/>    <br/>    # Retrieve dimensions from shapes of x and parameters["Wya"]<br/>    n_x, m, T_x = x.shape<br/>    n_y, n_a = parameters["Wya"].shape<br/>       <br/>    # initialize "a" and "y_pred" with zeros (≈2 lines)<br/>    a = np.zeros((n_a, m, T_x))<br/>    y_pred = np.zeros((n_y, m, T_x))<br/>    <br/>    # Initialize a_next (≈1 line)<br/>    a_next = a0<br/>    <br/>    # loop over all time-steps<br/>    for t in range(T_x):<br/>        # Update next hidden state, compute the prediction, get the cache (≈1 line)<br/>        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)<br/>        # Save the value of the new "next" hidden state in a (≈1 line)<br/>        a[:,:,t] = a_next<br/>        # Save the value of the prediction in y (≈1 line)<br/>        y_pred[:,:,t] = yt_pred<br/>        # Append "cache" to "caches" (≈1 line)<br/>        caches.append(cache)<br/>    <br/>    # store values needed for backward propagation in cache<br/>    caches = (caches, x)<br/>    <br/>    return a, y_pred, caches</span></pre></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="17ef" class="or mf it bd mg os ot ou mj ov ow ox mm jz oy ka mp kc oz kd ms kf pa kg mv pb bi translated">2.递归神经网络的反向传播</h1><p id="5564" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">在所有现代的深度学习框架中，你只需要实现前向传递，框架负责后向传递，所以大多数深度学习工程师不需要为后向传递的细节费心。然而，如果你正处于学习阶段，或者你有足够的好奇心去看看RNN模型是如何真正工作的，那么看一看或者甚至从头开始实现它将是很重要的。</p><h2 id="35d8" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">2.1.<strong class="ak"> RNN落后的牢房</strong></h2><p id="c03a" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">RNN的向后传球如下图所示。我们将首先实现该单元，然后使用它来实现RNN的反向传播:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pv"><img src="../Images/b1ed6950354f88a4d398fb65693e5180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIl0MK3G6CMj71C1F_9u8g.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">图4。RNN的反向传递细胞。</figcaption></figure><p id="aad1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要计算RNN向后像元，可以使用以下公式:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi pw"><img src="../Images/9a96dec32478627a807f499310970994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yRtzgEqcGRgevD8JCgkoDQ.png"/></div></div></figure><p id="66aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们使用下面的代码来实现这些等式:</p><pre class="lo lp lq lr gt pl pk pm bn pn po bi"><span id="82c2" class="pp mf it pk b be pq pr l ps pt">def rnn_cell_backward(da_next, cache):<br/>    """<br/>    Implements the backward pass for the RNN-cell (single time-step).<br/><br/>    Arguments:<br/>    da_next -- Gradient of loss with respect to next hidden state<br/>    cache -- python dictionary containing useful values (output of rnn_cell_forward())<br/><br/>    Returns:<br/>    gradients -- python dictionary containing:<br/>                        dx -- Gradients of input data, of shape (n_x, m)<br/>                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)<br/>                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)<br/>                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)<br/>                        dba -- Gradients of bias vector, of shape (n_a, 1)<br/>    """<br/>    <br/>    # Retrieve values from cache<br/>    (a_next, a_prev, xt, parameters) = cache<br/>    <br/>    # Retrieve values from parameters<br/>    Wax = parameters["Wax"]<br/>    Waa = parameters["Waa"]<br/>    Wya = parameters["Wya"]<br/>    ba = parameters["ba"]<br/>    by = parameters["by"]<br/><br/>    # compute the gradient of tanh with respect to a_next (≈1 line)<br/>    dtanh = (1-a_next*a_next)*da_next<br/><br/>    # compute the gradient of the loss with respect to Wax (≈2 lines)<br/>    dxt = np.dot(Wax.T,  dtanh)<br/>    dWax = np.dot(dtanh,xt.T)<br/><br/>    # compute the gradient with respect to Waa (≈2 lines)<br/>    da_prev = np.dot(Waa.T, dtanh)  <br/>    dWaa = np.dot( dtanh,a_prev.T)<br/><br/>    # compute the gradient with respect to b (≈1 line)<br/>    dba = np.sum( dtanh,keepdims=True,axis=-1)<br/><br/>    <br/>    # Store the gradients in a python dictionary<br/>    gradients = {"dxt": dxt, "da_prev": da_prev, "dWax": dWax, "dWaa": dWaa, "dba": dba}<br/>    <br/><br/>    return gradients</span></pre></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="1479" class="me mf it bd mg mh mi dn mj mk ml dp mm kr mn mo mp kv mq mr ms kz mt mu mv mw bi translated">2.2.<strong class="ak"> RNN向后传球</strong></h2><p id="f771" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr pc kt ku kv pd kx ky kz pe lb lc ld im bi translated">在每个时间步长t计算成本相对于𝑎⟨𝑡⟩at的梯度是有用的，因为这有助于梯度反向传播到前一个RNN小区。要做到这一点，我们需要迭代所有的时间步骤，从最后开始，在每一步，你增加整个𝑑𝑏⟨𝑎⟩，𝑑𝑊⟨𝑎𝑎⟩，𝑑𝑊⟨𝑎𝑥⟩和你存储𝑑𝑥.</p><p id="fb78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">指令</strong>:</p><p id="702a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">执行<code class="fe ph pi pj pk b">rnn_backward</code>功能。首先用零初始化返回变量，然后在每个时间步长调用<code class="fe ph pi pj pk b">rnn_cell_backward</code>时循环所有时间步长，并相应地更新其他变量。</p><pre class="lo lp lq lr gt pl pk pm bn pn po bi"><span id="3377" class="pp mf it pk b be pq pr l ps pt">def rnn_backward(da, caches):<br/>    """<br/>    Implement the backward pass for a RNN over an entire sequence of input data.<br/><br/>    Arguments:<br/>    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)<br/>    caches -- tuple containing information from the forward pass (rnn_forward)<br/>    <br/>    Returns:<br/>    gradients -- python dictionary containing:<br/>                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)<br/>                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)<br/>                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)<br/>                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy array of shape (n_a, n_a)<br/>                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)<br/>    """<br/>        <br/>    ### START CODE HERE ###<br/>    <br/>    # Retrieve values from the first cache (t=1) of caches (≈2 lines)<br/>    (caches, x) = caches<br/>    (a1, a0, x1, parameters) = caches[0]<br/>    <br/>    # Retrieve dimensions from da's and x1's shapes (≈2 lines)<br/>    n_a, m, T_x = da.shape<br/>    n_x, m = x1.shape <br/>    <br/>    # initialize the gradients with the right sizes (≈6 lines)<br/>    dx = np.zeros((n_x, m, T_x)) <br/>    dWax = np.zeros((n_a, n_x))<br/>    dWaa = np.zeros((n_a, n_a))<br/>    dba = np.zeros((n_a, 1)) <br/>    da0 = np.zeros((n_a, m))<br/>    da_prevt = np.zeros((n_a, m))  <br/>    <br/>    # Loop through all the time steps<br/>    for t in reversed(range(T_x)):<br/>        # Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step. (≈1 line)<br/>        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])<br/>        # Retrieve derivatives from gradients (≈ 1 line)<br/>        dxt, da_prevt, dWaxt, dWaat, dbat = gradients["dxt"], gradients["da_prev"], gradients["dWax"], gradients["dWaa"], gradients["dba"]<br/>        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)<br/>        dx[:, :, t] = dxt  <br/>        dWax += dWaxt  <br/>        dWaa += dWaat  <br/>        dba += dbat  <br/>        <br/>    # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) <br/>    da0 = da_prevt<br/>    ### END CODE HERE ###<br/><br/>    # Store the gradients in a python dictionary<br/>    gradients = {"dx": dx, "da0": da0, "dWax": dWax, "dWaa": dWaa,"dba": dba}<br/>    <br/>    return gradients</span></pre></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="1a8b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ny">如果你喜欢这篇文章并愿意支持我，请务必:</em> </strong></p><ul class=""><li id="803e" class="mx my it kk b kl km ko kp kr nu kv nv kz nw ld nx nf ng nh bi translated"><strong class="kk iu">👏为这个故事鼓掌(50次)并跟我来👉</strong></li><li id="49d3" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><strong class="kk iu">📰查看我的媒体档案中的更多内容</strong></li><li id="4e8b" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><strong class="kk iu">🔔关注我:</strong><a class="ae md" href="https://www.linkedin.com/in/youssef-hosni-b2960b135/" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">LinkedIn</strong></a><strong class="kk iu">|</strong><a class="ae md" href="https://medium.com/@youssefraafat57" rel="noopener"><strong class="kk iu">Medium</strong></a><strong class="kk iu">|</strong><a class="ae md" href="https://github.com/youssefHosni" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">GitHub</strong></a><strong class="kk iu">|</strong><a class="ae md" href="https://twitter.com/Youssef70125494" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">Twitter</strong></a></li><li id="9d04" class="mx my it kk b kl ni ko nj kr nk kv nl kz nm ld nx nf ng nh bi translated"><em class="ny">🚀👉</em> <strong class="kk iu"> <em class="ny">加入</em> </strong> <a class="ae md" href="https://youssefraafat57.medium.com/membership" rel="noopener"> <strong class="kk iu"> <em class="ny">中等会员</em> </strong> </a> <strong class="kk iu"> <em class="ny">计划继续无限制学习。如果你使用下面的链接，我会收到一小部分会员费，不需要你额外付费。</em> </strong></li></ul><div class="nz oa gp gr ob oc"><a href="https://youssefraafat57.medium.com/membership" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">加入我的介绍链接媒体-优素福胡斯尼</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">阅读Youssef Hosni(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">youssefraafat57.medium.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq lx oc"/></div></div></a></div></div></div>    
</body>
</html>