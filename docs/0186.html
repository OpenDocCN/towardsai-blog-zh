<html>
<head>
<title>How To Train Your AI Dragon (Safely, Legally And Without Bias)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练你的AI龙(安全，合法，不带偏见)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-train-your-ai-dragon-safely-legally-and-without-bias-2c4efe40b6c0?source=collection_archive---------0-----------------------#2019-10-18">https://pub.towardsai.net/how-to-train-your-ai-dragon-safely-legally-and-without-bias-2c4efe40b6c0?source=collection_archive---------0-----------------------#2019-10-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="c9c6" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">人工智能中的安全训练原则| <a class="ae ep" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">走向人工智能</a></h2><div class=""/><div class=""><h2 id="2fba" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何为人工智能算法正确获取数据并减少偏差</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/5db3ffbadb862f358f1479679108d788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cr6oA6ewQtny0xRe"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">就像梦工厂2010年的电影《驯龙高手》中的龙一样，人工智能系统通常是……[+]梦工厂动画</figcaption></figure><p id="236f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">未经训练的龙会造成很大的伤害。同样，随着人工智能系统进一步传播并对我们的生活产生更大影响，确保它们得到适当培训变得更加重要。<strong class="lj jd">偏见可以很容易地潜入人工智能的推理中</strong>，要么通过不够多样化的数据集，要么通过附加到可行数据点的无关数据，导致有缺陷的结果，在某些情况下，会得出偏见或危险的结论。</p><p id="342f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">尽管像GDPR这样的法规保护我们的数据隐私，但个人消费者数据正越来越多地被公司用来改善服务或获得客户洞察力。具有讽刺意味的是，这些规定也使公司更难收集足够的数据来训练一个人工智能系统，或证明他们的人工智能如何达到其决策(这对许多深度学习系统来说是一项不可能的任务)。</p><p id="a439" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，随着人工智能的发展及其能力的增长，<strong class="lj jd">在不违反数据法规的情况下收集有用的数据将是至关重要的，以确保人工智能能够做出正确的决定</strong>，并且个人和敏感数据不会在错误的上下文中使用。</p><h1 id="dc0e" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">安全地获取数据</strong></h1><p id="6b3a" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">随着如此多的数据在网络空间流动，公司正在使用越来越多的粒度指标来衡量我们的行为并改善他们的服务。然而，<a class="ae na" href="https://gdpr-info.eu/" rel="noopener ugc nofollow" target="_blank"><em class="nb"/></a>【GDPR】的一般数据保护条例允许公司仅在征得个人明确同意的情况下，或者“如果出于公司追求合法利益的目的而有必要”，数据匿名化公司<a class="ae na" href="https://www.statice.ai/" rel="noopener ugc nofollow" target="_blank"> Statice </a>的首席执行官塞巴斯蒂安·文耶说。因为《GDPR》第6条(概述了合规数据处理的要求)对“合法利益”一词的解释是开放的，这意味着公司“在获得数据主体的直接同意时是最安全的，”Weyer说。</p><p id="fbfe" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，由于围绕公司使用我们的数据 a 的<em class="nb"> </em> <a class="ae na" href="https://www.forbes.com/sites/charlestowersclark/2019/01/23/the-ethics-of-data-governance-data-comes-with-benefits-and-liabilities/" rel="noopener ugc nofollow" target="_blank"> <em class="nb">的不信任气氛，怀尔指出，大多数客户“<em class="nb">不会同意将他们的数据用于产品测试和创新</em>，这限制了可用于训练人工智能和改进产品的有用数据的数量。这种“<em class="nb">缺乏关于数据在构建个性化产品和服务中的重要性的教育</em>”可能会扼杀人工智能的创新，Weyer认为，对数据泄露和商业滥用的担忧实际上可能会限制人工智能解决紧迫社会问题的能力。</em></a></p><blockquote class="nc"><p id="3f02" class="nd ne it bd nf ng nh ni nj nk nl mc dk translated">构建人工智能产品和服务的公司也需要对其自动化数据的使用保持透明，这并不总是像听起来那么容易。</p></blockquote><p id="9ffc" class="pw-post-body-paragraph lh li it lj b lk nn kd lm ln no kg lp lq np ls lt lu nq lw lx ly nr ma mb mc im bi translated">机器学习系统以令人难以置信的复杂方式使用数据，最先进的算法通常“<em class="nb">牺牲可解释性来换取性能</em>”Weyer说。然而，GDPR第15条要求公司必须能够向数据主体解释其算法的基本功能，以表明其数据是如何被使用的。</p><p id="362e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，从数据集中删除所有识别信息，即<strong class="lj jd">数据匿名化</strong>，在收集数据时非常重要，因为它允许从数据集中收集有用的信息，而不会损害数据隐私法规。例如，Statice创建一个合成数据集，该数据集遵循原始数据集的相同结构和统计属性，但没有附加任何识别信息。适当的数据匿名化不仅是收集数据时GDPR的要求，也有助于准确训练人工智能系统。“如果数据在用于建立机器学习模型之前没有正确匿名，学习到的模式可能会涉及敏感信息，<em class="nb">”Weyer说。这是因为算法通过识别数据中的模式来工作——如果数据集中有无关的数据，如一个人的年龄、种族或地址，那么模式可以在这些因素而不是相关数据之间绘制。</em></p><h1 id="e096" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">训练数据</strong></h1><p id="4a70" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">除了适当地匿名敏感信息之外，为特定算法获取正确的训练数据是至关重要的；事实上，<a class="ae na" href="https://towardsdatascience.com/ai-ml-practicalities-the-unreasonable-effectiveness-of-data-c0bfd44c5057" rel="noopener" target="_blank"> <em class="nb">数据可以看作是一个AI系统</em> </a>最重要的部分。不完整的数据集，具有过度或不足表示的元素，或者具有太多不相关的信息，很容易扭曲人工智能系统的推理。这在<a class="ae na" href="https://www.technologyreview.com/s/612775/algorithms-criminal-justice-ai/" rel="noopener ugc nofollow" target="_blank"> <em class="nb">有缺陷的刑事累犯制度</em> s </a>中得到了明显的证明，该制度表明，由于历史上有偏见的培训数据，非裔美国人比他们的白人同行更有可能重新犯罪。但是，从数据集中消除偏见并不容易，部分原因是历史不平等等问题，或者是因为数据集中缺乏多样性。Samasource的创始人兼首席执行官Leila Janah说:“你几乎总是会从一些元素的代表性过高和其他元素的代表性过低开始，”但如果没有适当的测试和审查，“不具包容性和多样性的数据集可能会导致涉及种族、性别和文化偏见的问题。”</p><p id="baf8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，偏见的问题不仅仅是表面的，对于像具有不同训练的自动驾驶汽车这样的图像识别系统来说，数据是首要的安全问题。“用于训练算法的数据是一个重要的组成部分，可以确保它能够适当地从停止标志中识别行人，从树上识别停止标志，”Janah说。例如，<a class="ae na" href="https://www.consumeraffairs.com/amp/news/whose-lives-matter-to-self-driving-cars-043019.html" rel="noopener ugc nofollow" target="_blank"> <em class="nb">数据集由肤色较深的人</em> </a>表示不足，这可能导致自动驾驶车辆不太可能“看到”肤色较深的行人过马路。虽然这似乎是一个极端的例子，但随着人工智能在越来越多的任务关键型应用程序中的使用，思考代表性数据集的重要性是恰当的。雇用一个多元化的团队来注释训练数据(如Samasource所做的那样)有助于确保所有相关的指标都得到考虑，文化偏见不会无意中进入系统，并且数据集能够代表一般人群。</p><blockquote class="nc"><p id="438b" class="nd ne it bd nf ng nh ni nj nk nl mc dk translated">正确注释和适当匿名的训练数据也是训练AI时的良好实践。</p></blockquote><p id="a587" class="pw-post-body-paragraph lh li it lj b lk nn kd lm ln no kg lp lq np ls lt lu nq lw lx ly nr ma mb mc im bi translated">从模型中删除不相关的信息，并确保训练数据尽可能多样化和具有代表性，这为算法提供了做出适当决策的最佳工具。Janah说:“对于一个给定的问题，你能捕捉的情况越多，你就越有机会建立一个全面、可靠的模型。在寻找适当的训练数据时，这也是一个知道你想从系统中得到什么，并确保不存在误导性数据的问题——例如，如果你正在训练一个人工智能去<a class="ae na" href="https://www.forbes.com/sites/charlestowersclark/2019/04/30/the-cutting-edge-of-ai-cancer-detection/" rel="noopener ugc nofollow" target="_blank"> <em class="nb">寻找肺癌结节</em> </a>，那么包括肝癌筛查是没有帮助的。总的来说，在选择训练数据时采用适当的偏差预防策略与收集的数据量同等重要，因为偏差会在人工智能的整个计算过程中造成损害。Janah认为“在生产之前、之后和整个生产过程中仔细测试你的模型的偏差将有助于你的模型走向成熟。”</p><h1 id="81e5" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">数据中的魔鬼</strong></h1><p id="63eb" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">尽管一家人工智能公司的表现往往被归因于其算法的复杂性，但决定一个人工智能系统成败的力量在于训练它的数据。不恰当地处理敏感数据不仅会导致公关噩梦，还会通过允许算法在不相关的数据之间绘制模式，从本质上破坏算法的推理。尽管法律要求正确匿名化数据(至少在欧盟)，但确保在训练算法之前从数据集中删除识别数据也是一种良好的做法，这样就不会出现偏见。</p><p id="fd5f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们的生活变得越来越自动化，现在大多数人每小时都在与人工智能系统互动，不管我们是否意识到这一点。在这种背景下，<strong class="lj jd">我们必须保持警惕，保护个人的数据隐私权，并确保歧视性的人工智能不会因有偏见的训练数据而在世界上传播</strong>。人工智能每天都变得越来越强大，适当的数据管理和评估将成为对训练不足的系统的致命后果的制衡。</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="41f9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="nb">原载于</em><a class="ae na" href="https://www.forbes.com/sites/charlestowersclark/2019/10/12/how-to-train-your-ai-dragon-safely-legally-and-without-bias/" rel="noopener ugc nofollow" target="_blank"><em class="nb"/></a><em class="nb">。</em></p></div></div>    
</body>
</html>