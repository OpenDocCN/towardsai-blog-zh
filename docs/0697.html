<html>
<head>
<title>Demystifying Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭开主成分分析的神秘面纱</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/demystifying-principal-component-analysis-9f13f6f681e6?source=collection_archive---------1-----------------------#2020-07-19">https://pub.towardsai.net/demystifying-principal-component-analysis-9f13f6f681e6?source=collection_archive---------1-----------------------#2020-07-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/15bbe7b1675743f3261ee67e6e15c727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6VNq4rMMyDwzr-N-hgf7ow.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">安托万·道特里在<a class="ae jd" href="https://unsplash.com/s/photos/math?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="e41f" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a></h2><div class=""/><p id="924c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">数据可视化一直是任何机器学习操作的重要部分。它有助于获得关于数据分布的非常清晰的直觉，这反过来有助于我们决定哪个模型最适合我们正在处理的问题。</p><p id="c30b" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">目前，随着机器学习的进步，我们更经常需要处理大数据集。数据集具有大量的特征，并且只能使用大的特征空间来可视化。现在，我们只能可视化二维平面，但是，数据的可视化似乎也是非常必要的，正如我们在上面的讨论中看到的。这就是主成分分析的用武之地。</p><blockquote class="lk ll lm"><p id="6657" class="km kn ln ko b kp kq kr ks kt ku kv kw lo ky kz la lp lc ld le lq lg lh li lj ij bi translated">主成分分析(PCA)是一种降维方法，通常用于降低大型数据集的维数，方法是将一个大型变量集转换为一个较小的变量集，该变量集仍包含大型数据集中的大部分信息。</p></blockquote><p id="686b" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">简而言之，它执行降维，即，它接受具有大量特征的大型数据集，并产生具有较少数量特征的数据集(如我们所指定的)，尽可能少地丢失信息。</p><p id="669e" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们需要明确一些事情。</p><ol class=""><li id="8d66" class="lr ls jg ko b kp kq kt ku kx lt lb lu lf lv lj lw lx ly lz bi translated">主成分分析是一种无监督的技术。</li><li id="936d" class="lr ls jg ko b kp ma kt mb kx mc lb md lf me lj lw lx ly lz bi translated">如果我们通过PCA传递一个具有5个特征的数据集，比如年龄、身高、体重、智商和性别，并告诉它使用2个特征来表示，PCA会给我们两个分量，主分量1和主分量2，这些不是来自我们的5个初始特征的任何特定特征，它们是包含最多信息的5个特征的组合。</li><li id="25d9" class="lr ls jg ko b kp ma kt mb kx mc lb md lf me lj lw lx ly lz bi translated">在数据集中，如果我们分别查看每个要素，我们会看到类似这样的内容。</li></ol><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mf"><img src="../Images/2b5c3e994f9ebc6d6be13deac04922ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DDIGzw8xB93rU6GVYxmDng.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图一</figcaption></figure><p id="f401" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们查看上面的图表，并将红色的球视为第1类，蓝色的球视为第2类，我们可以看到这两个类可以根据特征进行划分。特征的这种特性有助于模型基于特征的行为在两个类之间进行分离。虽然我们可以看到“体重”特征比“身高”特征更好地划分了类别。我们可以说，因为我们可以看到“重量”,两个类之间的距离更大。</p><p id="04b4" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个概念通过特征的变化来反映。方差由下式给出:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/f9c24b6483c190369b3385eb4fc92129.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*fmUA_Q8hTyZbAxyUajhIzw.png"/></div></figure><p id="e047" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以，它是距离平均值的平方的总和。点离得越远，方差就越大。所以这里体重的方差比身高大。这种差异可用于描述和比较某个特定要素为数据分布模型提供了多少信息。差异越大，提供的信息越多。</p><h2 id="a7ef" class="ml mm jg bd mn mo mp dn mq mr ms dp mt kx mu mv mw lb mx my mz lf na nb nc jm bi translated">理论直觉</h2><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/f9a9d39c67b532e8dd306e220a3e4ea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*p3gFFXLoflZYPY5wk_tUfw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图2</figcaption></figure><p id="4767" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这张图准确地描述了主成分分析。比方说，我们有两个特征X和y。我们将点分布在二维平面上，每个点可以由一个元组(X1，Y1)表示，其中它们是特征值。现在，我们想只用一个组件来表示它。所以需要找到如图所示的一个轴，并把位于XY平面上的点投影到这个轴上。现在，投影可以由单个值Z1表示，因为轴是一维的。符合我们目的的轴的标准是该轴上的点的方差必须是最大的，即，如果我们选择任何其他轴，将点投影到轴上，并找到方差，该值不应大于当前值。</p><p id="8426" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是现在，由于我们进行了降维，我们将丢失一些信息。我们的目标是最大限度地减少信息损失。现在，如果我们有10维数据，我们把它的维数减少到2，它可能看起来像这样。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/5e184bf7aa6205f5138c348845297ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*wpRJDpuuPOMt85vWv3lIDA.png"/></div></figure><p id="4714" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总的来说，我们试图找到沿着轴的方差最大的最佳可能轴，然后我们将n维点在由我们的轴形成的平面上的投影用于降维。</p><p id="81a9" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里我们需要解决的一个问题是，有时我们可能无法使用PCA的组件保存很多信息。即使我们寻找具有最大方差的最佳轴，仍然不可能在我们的组件中捕获全部信息。在这些情况下，我们不应该完全依赖PCA来判断我们的数据，因为它可能会错过关于数据的重要直觉。据说，如果PCA的n个分量加在一起不能捕获至少85%的信息，我们就不应该依赖于PCA的结果。</p><p id="f1c4" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们来看看数学直觉。</p><h2 id="3140" class="ml mm jg bd mn mo mp dn mq mr ms dp mt kx mu mv mw lb mx my mz lf na nb nc jm bi translated">PCA背后的数学</h2><p id="344e" class="pw-post-body-paragraph km kn jg ko b kp nf kr ks kt ng kv kw kx nh kz la lb ni ld le lf nj lh li lj ij bi translated">主成分分析依赖于线性代数。为此，我们必须对本征向量和本征值有一个非常清楚的概念。</p><p id="a715" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">那么，<strong class="ko jq">什么是特征向量和特征值？</strong></p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d6432e897cd09d05c84a880d64656825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*pZn_1ZU5QBbLIthtOVyW9Q.png"/></div></figure><p id="91c7" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们都知道一个点(2，3)，可以用一个向量表示为2i + 3j。我们通常用I和j作为单位来表示平面上的每个矢量。他们被称为<strong class="ko jq">基地</strong>。但是，现在如果有人想用不同的向量作为基础，或者换句话说，有人想用其他的向量来表示所有的向量，而不是I，j。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/1b7f81ea70a21a23a152e9a3ff34b595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*_sGn_FQDmMh_dp-A0uKJVA.png"/></div></figure><p id="d710" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设我们希望b1和b2是我们的基础。那么，我们应该如何关联这两个系统呢？</p><p id="bda3" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将尝试看看b1和b2在我们的系统中实际上意味着什么。</p><p id="0a67" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">比如说，我们发现b1=2i+j，b2=-1i+1j</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/747379d3c008028650c8e1ddf81d4fbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*ra29BPToupiNMhrjtLp4pg.png"/></div></figure><p id="6747" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它们是1x 2矩阵或2D向量。对不起，我实际上没有找到一个合适的编辑器，不管怎样，让我们继续。</p><p id="ad52" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以，我们可以用这种方式来表示我们关于I和j的基础。b1b2系统中的向量在ij系统中意味着什么。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ff749907967f2542a1a5e98de6ca1810.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*l8y4Aa4nTgkJSDV5bquDBg.png"/></div></figure><blockquote class="lk ll lm"><p id="7a53" class="km kn ln ko b kp kq kr ks kt ku kv kw lo ky kz la lp lc ld le lq lg lh li lj ij bi translated">请注意，由于编辑器的问题，我把向量写成了1 x 2的矩阵。对于应用，我们需要找到1×2矩阵的转置，以获得2D向量，然后执行乘法。我用[x，y]来表示矢量xi+yj。</p></blockquote><p id="545d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设我们需要将b1b2系统中的向量[-1，2]转换为ij系统，我们需要找到向量[-1，2]与给定矩阵的乘积。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9b6bc0075ad883f2fb64d4a4fd9371fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:134/format:webp/1*_OFPTf75n_q-cEBFW6Oflw.png"/></div></figure><p id="db36" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，如果我们从b1b2系统中选取任意一个向量，并与这个向量相乘，我们就可以找到它在ij系统中的对应表示。</p><p id="3efd" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">同样，我们找到这个矩阵的逆矩阵。我们将获得，</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c7a93c9cb89914d54faa3faa4d63bf05.png" data-original-src="https://miro.medium.com/v2/resize:fit:184/format:webp/1*1isuRxhikFwjhmr-lcoiyw.png"/></div></figure><p id="1ea6" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们将ij平面上的任何向量与这个矩阵相乘，我们将得到它在b1b2平面上的对应向量。</p><p id="4922" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们开始从ij平面选取向量，并与这个矩阵相乘，我们将为ij平面中的每个向量获得b1b2平面中的一个向量。因此，我们可以将整个ij平面转换为b1b2平面。这叫做<strong class="ko jq">转换。</strong></p><p id="ed79" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">事实证明，变换改变系统的基的方式，它将以相同的方式改变系统的任何向量，即，如果变换将基向量缩放2倍，它将对该系统的其他向量产生类似的影响，因为系统的每个向量都依赖于基向量来表示。因此，我们将在此基础上检查转换的变化，以便清楚地了解。</p><p id="7293" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有各种各样的变换，例如，我们可以旋转ij平面系统以获得新的平面系统。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5a9498e5f04d2739fd4c7dc1a4d567c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*KDzjqz4LmCDUNeT8LCq5KA.png"/></div></figure><p id="a7d6" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以看到上面显示了三种类型的转换。尽管它们没有按比例绘制。首先是旋转，其次是剪切。第三个是通过将下面给出的矩阵与ij平面的向量相乘得到的变换</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3be22a48246a4bbdb3a3cb9e2c3e7afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:166/format:webp/1*zH6ZgsGe810t0GFiYznj_w.png"/></div></figure><p id="34e5" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，要观察的一件事是，在第一种情况下，我们的I和j向量都改变了方向，或者说偏离了初始方向。在第二种情况下，I向量(在变换k1之后)保持在相同的方向，尽管它的值或幅度作为单位向量可能改变。在第三种情况下，两个矢量保持它们的方向，但是幅度改变，I矢量(现在是k1)变成2倍，j矢量(现在是k2)变成3倍。</p><p id="26cd" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一个系统经过变换后不改变方向(简单来说就是避免复杂化)的<strong class="ko jq">向量</strong>称为特征向量。本征向量虽然没有改变方向，但在数量上是成比例的。例如，在第三种情况下，我被缩放了两次，j被缩放了3次。这些值被称为向量的特征值，特征向量的大小通过这些值来缩放。需要注意的一点是，不仅是I和j向量，任何属于系统的向量，在变换后不改变方向的，都可以是特征向量。</p><p id="fa1d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">如何求特征值和向量？</strong></p><p id="21d2" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们看到变换后的特征向量，被一个标量值缩放了。我们知道变换是矩阵乘法。所以，当乘以一个矩阵时，可以说是向量的标量变化。因此，这由下式给出:</p><blockquote class="lk ll lm"><p id="be26" class="km kn ln ko b kp kq kr ks kt ku kv kw lo ky kz la lp lc ld le lq lg lh li lj ij bi translated">av =(λ)v</p></blockquote><p id="fb7f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中A是矩阵，v是特征向量，T4λT5是向量缩放的常数，因此特征值。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/9cc7cbbffef05b06a389d45c69df9a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/format:webp/1*yYASFVHuDe7gT4ZASUBKCA.png"/></div></figure><p id="a6b7" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以，我们最终得到了方程。现在的问题是，如果v =0，方程就是0，但这太明显了，而且我们希望v是非零向量。</p><p id="68d4" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们设(A-(λ)I)= 0—(1)，所以，我们把它的行列式等价为0。</p><p id="e1bc" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实际上，这背后还有另一个直觉，即矩阵的行列式，实际上描述了单位矩形面积的变化，如果我们用矩阵变换一个向量系统，就会发生这种变化。例如，如果我们看我们的第三种情况，我们将单位向量缩放了2和3个单位。因此，单位矩形的当前面积是6，最初是1。如果我们找到了变换矩阵的行列式，</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3be22a48246a4bbdb3a3cb9e2c3e7afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:166/format:webp/1*zH6ZgsGe810t0GFiYznj_w.png"/></div></figure><p id="fe90" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们会发现行列式是6。</p><p id="9406" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">回到等式1，我们找到λ值，并相应地找到特征向量。</p><h2 id="4f90" class="ml mm jg bd mn mo mp dn mq mr ms dp mt kx mu mv mw lb mx my mz lf na nb nc jm bi translated">五氯苯甲醚有什么关系？</h2><p id="a28e" class="pw-post-body-paragraph km kn jg ko b kp nf kr ks kt ng kv kw kx nh kz la lb ni ld le lf nj lh li lj ij bi translated">对于PCA，假设我们有两个特征X和y，我们首先计算X和y的协方差矩阵。</p><p id="0007" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">协方差由下式给出:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2b882a02c35d06fa7b217af84eefe466.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*9klYtqPc6yfD9ncBn-lBWw.png"/></div></figure><p id="b934" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">协方差矩阵由下式给出:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/60436291c1223975c924bdf0b00303fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*oHQQv-UVHAJ1RCZIVLL4cw.png"/></div></figure><p id="e98f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，协方差矩阵是从特征中获得的。现在我们初始化XY平面上的任何向量，比如说，[-1 1]。</p><p id="cc0f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们不断把这个向量和协方差矩阵一次又一次相乘，向量的变化或变换慢慢收敛到一个特定的向量，这个向量不变。这就给出了该平面的特征向量。</p><p id="a4ca" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，[-1 1]是我们取的向量，而</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/26cd322ef416a1e3a3d55085e03cee89.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*jbyfxDkRFB53wgJCpu1PNg.png"/></div></figure><p id="4c74" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">是协方差矩阵，得到的向量是[ x y] =[-2.5 -1.0]</p><p id="98f1" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">y=-1.0</p><p id="bfdd" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">x=-2.5</p><p id="beed" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以，-2.5i -1.0j是得到的向量。在这个向量方向上的单位向量是主分量1的特征向量。</p><p id="8dd7" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/ac4e83e08564aa890111b8d19f3ae296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*yXIBPKp7ltrzsP23P-Oi4w.png"/></div></figure><p id="e8d3" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">距离S1、S2、S3的平方之和……S8、S9、S10与0等于s</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/3ec47c433eeb47a6e496446601c60995.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*znsSqEeBClJtPzbIdFotcA.png"/></div></figure><p id="f102" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个S叫做主分量1方向的方差。</p><p id="69f6" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们考虑与分量1的特征向量正交的另一个向量。它给出了主分量2。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/0d04dd4034140596519815f2c3ed94bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*yPYq-vT0rQJc2eSEW9VdAQ.png"/></div></figure><p id="0f0d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">紫色线给出PC2。我们发现PC2的S值与PC1的一样。</p><p id="218f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，假设我们的分布中有N个值。</p><p id="e8a3" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="ln">(PC1/N-1的S值)</em>给出PC1的<strong class="ko jq">变化。</strong>同样，我们发现对于PC2。</p><p id="52af" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，如果PC1的变化量是10，PC2是2。然后，</p><p id="ea7c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">PC1占所表示信息的10/(10+2) x 100 =83%。</p><p id="ea5e" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于10维数据，这两个分量PC1和PC2可能不代表大部分信息，并且公式被修改，因为它将在分母中具有更多分量。</p><p id="fe49" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，如果我们想要使用1个主成分来表示X-Y特征分布，我们将表示83%的信息。</p><h2 id="8f54" class="ml mm jg bd mn mo mp dn mq mr ms dp mt kx mu mv mw lb mx my mz lf na nb nc jm bi translated">结论</h2><p id="9e3c" class="pw-post-body-paragraph km kn jg ko b kp nf kr ks kt ng kv kw kx nh kz la lb ni ld le lf nj lh li lj ij bi translated">在本文中，我们讨论了PCA的工作原理。</p><p id="c223" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">希望这有所帮助。</p></div></div>    
</body>
</html>