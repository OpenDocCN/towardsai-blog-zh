<html>
<head>
<title>Introduction to the Architecture of Recurrent Neural Networks (RNNs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络体系结构介绍</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/introduction-to-the-architecture-of-recurrent-neural-networks-rnns-a277007984b7?source=collection_archive---------0-----------------------#2019-06-29">https://pub.towardsai.net/introduction-to-the-architecture-of-recurrent-neural-networks-rnns-a277007984b7?source=collection_archive---------0-----------------------#2019-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="77ec" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">RNNs架构| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向AI </a></h2><div class=""/><h1 id="7f65" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">介绍</h1><p id="a155" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我之前的文章中，我解释了将文本表示为向量的不同方式。你可以阅读一下<a class="ae ls" href="https://medium.com/towards-artificial-intelligence/an-intuitive-introduction-of-word2vec-by-building-a-word2vec-from-scratch-a1647e1c266c" rel="noopener"> <strong class="kw ja"> Word2Vec </strong> </a>、<a class="ae ls" href="https://medium.com/towards-artificial-intelligence/an-intuitive-introduction-of-document-vector-doc2vec-42c6205ca5a2" rel="noopener"> <strong class="kw ja"> Doc2Vec </strong> </a>你也可以找一个jupyter笔记本用于<strong class="kw ja"> </strong> Word2Vec型号使用<a class="ae ls" href="https://github.com/nitwmanish/An-Intuitive-Introduction-Of-Word2Vec-By-Building-A-Word2Vec-From-Scratch/blob/master/Building%20word%20vectors%20using%20fastText%20library.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="kw ja">fast text</strong></a><strong class="kw ja">。</strong>我们可以使用密集层在这些表示之上执行情感分类。你可以在<strong class="kw ja"/><a class="ae ls" href="https://github.com/nitwmanish/An-Intuitive-Introduction-Of-Word2Vec-By-Building-A-Word2Vec-From-Scratch/blob/master/Building%20sentiment%20classification%20using%20word%20vectors.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="kw ja">GitHub</strong></a><strong class="kw ja">上找到一个用于<strong class="kw ja">情感分类</strong>的jupyter笔记本。</strong></p><p id="15ff" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">这种方法有一个问题，密集层不考虑单词的顺序。例如，考虑这两个句子</p><ol class=""><li id="ea16" class="ly lz iq kw b kx lt lb lu lf ma lj mb ln mc lr md me mf mg bi translated">鲍勃比汤姆强壮。</li><li id="3559" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr md me mf mg bi translated">汤姆比鲍勃强壮。</li></ol><p id="f22e" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">这里</p><ul class=""><li id="16eb" class="ly lz iq kw b kx lt lb lu lf ma lj mb ln mc lr mm me mf mg bi translated">在这两个句子中，单词完全相同。</li><li id="d142" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr mm me mf mg bi translated">但是单词的顺序不同。</li><li id="fe56" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr mm me mf mg bi translated">单词的矢量表示在两个句子中是相同的。</li></ul><p id="8a0f" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">我们知道这两个句子是不同的，它们不应该用数学结构来表示。</p><p id="fc17" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja">递归神经网络</strong> ( <strong class="kw ja"> RNNs </strong>)对于这类需要考虑单词顺序的场景很有用。</p><p id="0074" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">你可以把<strong class="kw ja"> RNNs </strong>看作一种保存内存的机制——内存包含在隐藏层中。</p><h1 id="2547" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">RNN建筑</h1><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mn"><img src="../Images/36f4c1c4aecd4a66bb1d8b4eecc21725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HgAY1lLMYSANqtgTgwWeXQ.png"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd jy">递归神经网络</strong> ( <strong class="bd jy"> RNNs </strong>)</figcaption></figure><p id="e2b2" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">右侧的网络是左侧网络的展开图，其中</p><ul class=""><li id="8691" class="ly lz iq kw b kx lt lb lu lf ma lj mb ln mc lr mm me mf mg bi translated"><strong class="kw ja"> Wxh: </strong>是输入层与隐藏层连接的权重。</li><li id="07aa" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr mm me mf mg bi translated"><strong class="kw ja"> W: </strong>是隐藏层到隐藏层的连接的权重。</li><li id="ce3f" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr mm me mf mg bi translated"><strong class="kw ja">为什么:</strong>是隐藏层到输出层连接的权重。</li><li id="6e75" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr mm me mf mg bi translated"><strong class="kw ja">答:</strong>是层的激活。</li></ul><p id="fe78" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">递归神经网络从左到右扫描数据。</p><p id="6407" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">它用于每个时间步长的参数是共享的。在上图中，参数<strong class="kw ja"> Wxh，Why </strong>和<strong class="kw ja"> W </strong>对于每个时间步都是相同的。</p><p id="6167" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">在<strong class="kw ja"> RNN </strong>在时间<strong class="kw ja"> t </strong>进行预测时，不仅使用时间<strong class="kw ja"> t </strong>的输入<strong class="kw ja">“XT”</strong>，还通过激活参数<strong class="kw ja">“a”</strong>和从前一个隐含层传递到当前隐含层的权重<strong class="kw ja">“W”</strong>使用来自时间<strong class="kw ja"> t-1 </strong>的前一个输入的信息。</p><p id="5bc6" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja">注:</strong>这种<strong class="kw ja"> RNN </strong>有一个缺点就是它只使用序列中较早的信息来进行预测。因此<strong class="kw ja"> RNN </strong>在某个时间的预测使用输入或使用来自序列中较早的信息，而不是序列中较晚的信息。</p><h1 id="ad03" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">穿越时间的反向传播</h1><p id="5186" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">误差从最后一个时间步长反向传播到第一个时间步长。计算每个时间步长的误差，这允许我们更新权重。下图是一段时间后向传播的可视化。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/0ba4915162d63467dcea5e557f0e191b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*HrjQbk7zZSfIfSuucH4BYQ.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd jy">通过时间反向传播</strong></figcaption></figure><h1 id="5e10" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">架构及其用例</strong></h1><p id="4ae5" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以有<strong class="kw ja">一个</strong>不同架构的<strong class="kw ja"> RNN。下面是一些可能的方法。</strong></p><ul class=""><li id="40d2" class="ly lz iq kw b kx lt lb lu lf ma lj mb ln mc lr mm me mf mg bi translated">这是一个标准的通用神经网络，我们不需要RNN。该神经网络用于固定大小的输入到固定大小的输出，例如图像分类。</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/95f4dc4cd110a92cdebb81eeef60be16.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*1NYlIy6HRTrfpOaQWkmGFg.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd jy">一对一</strong></figcaption></figure><ul class=""><li id="aef5" class="ly lz iq kw b kx lt lb lu lf ma lj mb ln mc lr mm me mf mg bi translated"><strong class="kw ja">一对多:</strong>图像字幕，用于图像字幕输入，是图像，输出是图像的字幕。音乐生成，对于音乐生成，一个输入音符被馈送到网络，它预测序列中的下一个音符。</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/f4ff4ddbc14c955aca58c0edb5ce39a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*aHA8iS6wOY5hGGW5EeBtqQ.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd jy">一对多</strong></figcaption></figure><ul class=""><li id="a29e" class="ly lz iq kw b kx lt lb lu lf ma lj mb ln mc lr mm me mf mg bi translated"><strong class="kw ja">多对一:</strong>输入是一部电影的评论(输入多个单词)，输出是与评论相关联的情感。</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/686a59535b093d735df4c187f3d86dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*atFuoe2fvA7nDsQOMcsHGw.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd jy">多对一</strong></figcaption></figure><ul class=""><li id="26e1" class="ly lz iq kw b kx lt lb lu lf ma lj mb ln mc lr mm me mf mg bi translated">多对多:把一种语言的句子翻译成另一种语言的句子的机器翻译。语音识别。</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/be38cd76904e3ea47029f60b560e4fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*igQb5WA-qHMAKg1hTnaChw.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd jy">多对多</strong></figcaption></figure><ul class=""><li id="80b3" class="ly lz iq kw b kx lt lb lu lf ma lj mb ln mc lr mm me mf mg bi translated"><strong class="kw ja">多对多:</strong>输入长度和输出长度相同的命名实体识别。</li></ul><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="ab gu cl ni"><img src="../Images/5990867ee53d11f817eb152ff4607235.png" data-original-src="https://miro.medium.com/v2/format:webp/1*F--NnI97c_u-GsscUzPcvQ.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd jy">多对多</strong></figcaption></figure><p id="5e61" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja">伴随jupyter本帖的笔记本可以在</strong> <a class="ae ls" href="https://github.com/nitwmanish/An-Introduction-Of-Recurrent-Neural-Networks" rel="noopener ugc nofollow" target="_blank"> <strong class="kw ja"> Github </strong> </a> <strong class="kw ja">上找到。</strong></p><h1 id="33c4" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">结论</h1><p id="75ad" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="kw ja">递归神经网络</strong> ( <strong class="kw ja"> RNNs </strong>)是用于处理序列数据的神经网络家族。<strong class="kw ja"> RNNs </strong>考虑并使用来自输入序列的先前信息进行预测。<strong class="kw ja"> RNNs </strong>也有限制，它不使用序列中后面的信息。</p><p id="b1a4" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja"> <em class="nj">我希望这篇文章能帮助你了解RNNs，穿越时间的反向传播(BPTT)，RNNs的不同架构，以及RNNs如何考虑单词的顺序和先前的输入来进行预测。</em>T19】</strong></p><h1 id="17c9" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">参考</h1><p id="1591" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">安德烈·卡帕西，递归神经网络的不合理有效性。<a class="ae ls" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p></div></div>    
</body>
</html>