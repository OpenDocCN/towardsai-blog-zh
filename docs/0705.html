<html>
<head>
<title>The Fundamentals of Neural Architecture Search (NAS)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经结构搜索(NAS)的基础</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-fundamentals-of-neural-architecture-search-nas-9bb25c0b75e2?source=collection_archive---------1-----------------------#2020-07-21">https://pub.towardsai.net/the-fundamentals-of-neural-architecture-search-nas-9bb25c0b75e2?source=collection_archive---------1-----------------------#2020-07-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9cfe" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><p id="201c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">神经结构搜索(NAS)已经成为机器学习科学领域的一个热门课题。Google的AutoML等商业服务和Auto-Keras等开源库[1]使NAS可以访问更广泛的机器学习环境。在这篇博文中，我们探讨了NAS的思想和方法，以帮助读者更好地理解这个领域，并找到实时应用的可能性。</p><h2 id="5d20" class="kx ky it bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo iz bi translated"><strong class="ak">什么是神经架构搜索(NAS)？</strong></h2><p id="d5fb" class="pw-post-body-paragraph jz ka it kb b kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks lt ku kv kw im bi translated">现代深度神经网络有时包含几层众多的类型[2]。跳过连接[2]和子模块[3]也被用于促进模型收敛。可能的模型架构的空间没有限制。大多数深度神经网络结构目前都是基于人类经验创建的，需要漫长而繁琐的试错过程。NAS试图在没有人工干预的情况下，检测特定深度学习问题的有效架构。</p><p id="cb54" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">一般来说，NAS可以分为三个维度——搜索空间、<em class="lu"> a </em>搜索策略、<em class="lu">和</em> <em class="lu"> a </em>性能评估策略<em class="lu"/>【4】。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/621cb08db2a01e3215db98be75c600e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*wlSR5_PYkIpriUHElr-xxg.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图1:神经结构搜索的基础</figcaption></figure><h2 id="63e8" class="kx ky it bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo iz bi translated"><strong class="ak">搜索空间:</strong></h2><p id="a2ec" class="pw-post-body-paragraph jz ka it kb b kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks lt ku kv kw im bi translated">搜索空间决定了要评估哪些神经架构。更好的搜索空间可以降低搜索合适的神经架构的复杂性。通常，不仅需要受限的搜索空间，还需要灵活的搜索空间。约束消除了非直觉的神经架构，以创建一个有限的搜索空间。搜索空间包含源于NAS方法的每一种体系结构设计(通常是无限的)。它可能涉及所有相互堆叠的层配置集合(图2a)，或者包括跳跃连接的更复杂的架构(图2b)。为了降低搜索空间维度，还可能涉及子模块设计。后面的子模块被堆叠在一起以生成模型架构(图2c)。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/5014d08929fce40f8596cd03742f877e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*ReprpI7zopX-up3NcI6qXg.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图2:不同建筑空间的示意图。图片摘自[4]</figcaption></figure><h2 id="80a5" class="kx ky it bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo iz bi translated"><strong class="ak">性能评估策略:</strong></h2><p id="b5a6" class="pw-post-body-paragraph jz ka it kb b kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks lt ku kv kw im bi translated">它将提供一个反映搜索空间中所有架构效率的数字。当参考数据集在预定义数量的时期内被训练并随后被测试时，这通常是模型架构的准确性。性能评估技术还可以经常考虑一些因素，例如训练或推断的计算难度。在任何情况下，评估架构的性能在计算上都是很昂贵的。</p><h2 id="8aaf" class="kx ky it bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo iz bi translated"><strong class="ak">搜索策略:</strong></h2><p id="2fb0" class="pw-post-body-paragraph jz ka it kb b kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks lt ku kv kw im bi translated">NAS实际上依赖于搜索策略。它应该确定有前途的架构来评估性能，并避免测试糟糕的架构。在下面的文章中，我们讨论了许多搜索策略，包括随机和网格搜索、基于梯度的策略、进化算法和强化学习策略。</p><p id="850d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">系统搜索之后是网格搜索。相比之下，随机搜索从搜索空间中随机选取架构，然后通过性能评估策略测试相应架构的准确性。这两种方法对于最小搜索区域都是可行的，尤其是当手头的问题涉及到少量超参数的调整时(随机搜索通常优于网格搜索)。</p><p id="2038" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">作为一个优化问题，NAS可以通过基于梯度的搜索很容易地公式化[5]。通常，NAS优化的目标是最大限度地提高验证准确性。由于NAS使用离散搜索空间，因此实现梯度很有挑战性。因此，它需要将一个离散的建筑空间转化为一个连续的空间，并从其连续的表现中衍生出建筑。NAS可以基于变换的连续空间从优化目标获得梯度。NAS上梯度搜索的理论基础是不寻常的。也很难证明全局最优收敛。然而，这种方法在实际应用中表现出很好的搜索效果。</p><p id="6bb4" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">进化算法受生物进化的启发。模型架构适用于能够产生后代(其他架构)或死亡并被排除在群体之外的个体。一种进化的NAS算法(NASNet架构[6])通过以下过程衍生而来(图3)。</p><p id="00bd" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">I .随机架构创建N个模型的初始群体。根据绩效评估策略评估每个人的产出(即架构)。</p><p id="7369" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">二。表现最好的个体被选为父母。可以为具有诱导“突变”的新一代架构复制各自的亲本，或者它们可以来自亲本组合。表现评估策略评估后代的表现。类似于添加或移除层、添加或移除连接、改变层大小或另一超参数的操作可以包括在可能的突变列表中。</p><p id="3494" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">三。选择n个架构来移除，这可能是群体中最差的个体或较老的个体。后代替代被移除的架构并重新开始循环。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/4d55ef33d70f41f15528d12979bf07ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*kFRQp1UePUJrq1cAHcOCLA.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图NAS的进化算法示意图</figcaption></figure><p id="0fcb" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">进化算法揭示了有能力的结果，并产生了最先进的模型[7]。</p><p id="9365" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">基于强化学习的NAS方法[8]近年来越来越受欢迎。网络控制器，通常是递归神经网络(RNN ),可以用于从具有特定概率分布的搜索域中进行采样。使用性能评估策略来形成和评估样本架构。由此产生的性能被用作更新控制器网络属性的奖励(图4)。在超时或收敛发生之前，重复这个循环。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/99374bc64710895ce899a9045d0a1c22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*mNAMRwqJXny3f7BLor5jfw.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图NAS的强化学习方法的基本设计。图片摘自[8]</figcaption></figure><p id="13c6" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">强化学习能够基于流行的基准数据集构建超越手工模型的网络架构，类似于进化算法。</p><p id="bff9" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">结论:</strong></p><p id="0a75" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">NAS已经成功地建立了更深层次的神经网络体系结构，其精确度超过了手动构建的体系结构。NAS生成的最先进的体系结构是使用进化算法和强化学习开发的，特别是在图像分类任务领域。它很昂贵，因为在NAS产生成功的结果之前，需要训练和测试数百或数千个特定的深度神经网络。NAS方法对于大多数实际的应用程序来说过于昂贵。因此，需要进一步研究以使NAS更通用。</p><p id="26ee" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">跟随作者<a class="ae mk" href="https://www.linkedin.com/in/arjun-ghosh-2005321a5" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h2 id="7a2f" class="kx ky it bd kz la lb dn lc ld le dp lf kk lg lh li ko lj lk ll ks lm ln lo iz bi translated"><strong class="ak">参考文献:</strong></h2><p id="0548" class="pw-post-body-paragraph jz ka it kb b kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks lt ku kv kw im bi translated">[1] H. Jin，Q. Song和X. Hu，Auto-Keras:基于网络态射的高效神经结构搜索，arXiv，2018 .</p><p id="7fcd" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[2]何国光，张，任，孙，深度残差学习在图像识别中的应用，arXiv，2015 .</p><p id="580c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[3] C. Szegedy等人，深入研究卷积，arXiv，2014年。</p><p id="5e30" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[4] T. Elsken，J.H. Metzen和F. Hutter，神经架构搜索:一项调查，机器学习研究杂志，2019年。</p><p id="6c09" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[5] H. Liu，K. Simonyan和Y. Yang，DARTS:可区分的架构搜索，arXiv，2019。</p><p id="be62" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[6] B. Zoph，V. Vasudevan，J. Shlens和Q.V. Le，可扩展图像识别的学习可转移架构，计算机视觉和模式识别会议论文集，2018年。</p><p id="1c77" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[7] E. Real等，图像分类器的大规模进化，第34届机器学习国际会议论文集，2017。</p><p id="1a9c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[8] B. Zoph和Q.V. Le，具有强化学习的神经架构搜索，arXiv 2016。</p></div></div>    
</body>
</html>