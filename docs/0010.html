<html>
<head>
<title>Machine Learning: Dimensionality Reduction via Linear Discriminant Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:通过线性判别分析进行降维</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/machine-learning-dimensionality-reduction-via-linear-discriminant-analysis-cc96b49d2757?source=collection_archive---------0-----------------------#2018-11-06">https://pub.towardsai.net/machine-learning-dimensionality-reduction-via-linear-discriminant-analysis-cc96b49d2757?source=collection_archive---------0-----------------------#2018-11-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="50cd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><p id="6336" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">机器学习算法(如分类、聚类或回归)使用训练数据集来确定权重因子，这些权重因子可应用于未知数据以实现预测目的。在实现机器学习算法之前，有必要在训练数据集中仅选择相关特征。转换数据集以便只选择训练所需的相关特征的过程称为降维。降维非常重要，原因有三:</p><ol class=""><li id="0aaa" class="kx ky it kb b kc kd kg kh kk kz ko la ks lb kw lc ld le lf bi translated"><strong class="kb jd">防止过度拟合</strong>:拥有太多特征的高维数据集有时会导致过度拟合(模型捕捉真实和随机效果)。</li><li id="a0ac" class="kx ky it kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">简单性</strong>:一个拥有太多特征的过于复杂的模型可能很难解释，尤其是当特征相互关联的时候。</li><li id="f933" class="kx ky it kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated"><strong class="kb jd">计算效率</strong>:在低维数据集上训练的模型在计算上是高效的(算法的执行需要较少的计算时间)。</li></ol><p id="6e5c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">因此，降维在数据预处理中起着至关重要的作用。</p><h1 id="c454" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">降维的实现</h1><p id="053a" class="pw-post-body-paragraph jz ka it kb b kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw im bi translated">机器学习中有几种降维模型，如主成分分析(PCA)、线性判别分析(LDA)、逐步回归和正则化回归(如LASSO)。我们在这里关注PCA和LDA，它们被广泛用于分类问题。</p><p id="0e44" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">PCA和LDA是两种数据预处理线性变换技术，通常用于降维，以便选择可以在最终机器学习算法中使用的相关特征。PCA是一种无监督算法，用于高维和相关数据中的特征提取。PCA通过将特征转换为数据集中方差最大的正交分量轴来实现降维。<strong class="kb jd">使用iris数据集的PCA实现可以在这里找到:</strong><a class="ae mo" href="https://github.com/bot13956/principal_component_analysis_iris_dataset" rel="noopener ugc nofollow" target="_blank">https://github . com/bot 13956/principal _ component _ analysis _ iris _ dataset</a></p><p id="e753" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">LDA的目标是找到优化类可分性和降维的特征子空间(见下图)。因此，LDA是一种监督算法。在本文中，我们使用iris数据集来说明LDA的实现。关于PCA和LDA的深入描述可以在本书中找到:Sebastian Raschka的Python机器学习，第5章。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/346eb737f15f22834543f2d52312e29e.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*GIkeCkVlNp5bM0gQ-ohOMA.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated"><strong class="bd ln">图1: LDA算法从旧的特征子空间转换到新的特征子空间，从而优化类别可分性并降低维数。图片改编自:《塞巴斯蒂安·拉什卡的Python机器学习》。</strong></figcaption></figure><p id="0c89" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">实现LDA的代码在这里找到:</strong><a class="ae mo" href="https://github.com/bot13956/linear-discriminant-analysis-iris-dataset/blob/master/LDA_irisdataset.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/bot 13956/linear-discriminal-analysis-iris-dataset/blob/master/LDA _ iris dataset . ipynb</a></p><p id="abf1" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">使用iris数据集进行LDA计算的输出如下图2所示:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f3d28d51f8f72bab801cbd29fb5c9167.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*_JyosSNWORNylD4pHlqIxw.png"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated"><strong class="bd ln">图LDA子空间中虹膜类别的线性可分性。请注意，LD1组件捕获了类的大部分可辨性。</strong></figcaption></figure><p id="19d1" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">通过分析累积可辨性(<strong class="kb jd">参见代码</strong>:<a class="ae mo" href="https://github.com/bot13956/linear-discriminant-analysis-iris-dataset/blob/master/LDA_irisdataset.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/bot 13956/linear-discriminant-analysis-iris-dataset/blob/master/LDA _ iris dataset . ipynb</a>)，我们可以显示LD1和LD2分量捕获了100%的总可辨性。因此，当我们在LDA子空间上执行分类(使用逻辑回归或支持向量机)时，我们可以在较低的二维LDA变换数据集上训练模型。由于原始虹膜数据集是四维的(4个特征)，我们注意到LDA变换实现了类别可分性以及维数减少。</p><p id="24c7" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">总之，为了说明的目的，我们已经展示了如何使用iris数据集实现LDA算法。在上一篇文章中，我们展示了如何使用iris数据集实现PCA算法。以下是两个链接:</p><p id="2e08" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd"> PCA实施:</strong></p><p id="f6cc" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><a class="ae mo" href="https://github.com/bot13956/principal_component_analysis_iris_dataset" rel="noopener ugc nofollow" target="_blank">https://github . com/bot 13956/principal _ component _ analysis _ iris _ dataset</a></p><p id="950b" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd"> LDA实施:</strong></p><p id="bc54" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><a class="ae mo" href="https://github.com/bot13956/linear-discriminant-analysis-iris-dataset" rel="noopener ugc nofollow" target="_blank">https://github . com/bot 13956/linear-discriminant-analysis-iris-dataset</a></p><p id="51ea" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">感谢阅读。</p></div></div>    
</body>
</html>