<html>
<head>
<title>How does Data Noising Help to Improve your NLP Model?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据噪声如何帮助改进您的NLP模型？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-does-data-noising-help-to-improve-your-nlp-model-480619f9fb10?source=collection_archive---------1-----------------------#2019-09-18">https://pub.towardsai.net/how-does-data-noising-help-to-improve-your-nlp-model-480619f9fb10?source=collection_archive---------1-----------------------#2019-09-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4bb4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">用数据噪声改进NLP模型| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向人工智能</a></h2><div class=""/><div class=""><h2 id="e7a6" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">注入数据噪声的目的</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/e99536904f23baf8795eb8c1122c0af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/0*cRIDZTw8INq_sW1J"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">照片由<a class="ae ld" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae ld" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="74fb" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">将数据噪声引入神经网络旨在提高模型的泛化能力和性能。谢等人提出了几种通过<code class="fe ma mb mc md b">unigram noising</code>和<code class="fe ma mb mc md b">blank noising</code>生成更多训练的方法，用于离散序列级设置，如语言建模。换句话说，它是在NLP上执行数据扩充的一种方式。</p><p id="c4b6" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">这个故事经历了<a class="ae ld" href="https://arxiv.org/pdf/1703.02573.pdf" rel="noopener ugc nofollow" target="_blank">神经网络语言模型中平滑的数据去噪</a>(谢等，2017)。它包括语言模型平滑和数据扩充方法两部分。</p><h1 id="b418" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">数据去噪方法</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/318511a2f4acd3be77de6b9dd7a53a5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cHeEqU28t7bapVSW"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">照片由<a class="ae ld" href="https://unsplash.com/@chairulfajar_?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">@ chairufajar _</a>在<a class="ae ld" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="2f4d" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><code class="fe ma mb mc md b">Unigram noising</code>方法是<strong class="lg jd">根据单字频率分布用其他单词替换目标单词</strong>。与<a class="ae ld" href="https://arxiv.org/pdf/1904.12848.pdf" rel="noopener ugc nofollow" target="_blank">无监督数据扩充</a>(谢等，2019)类似，在数据扩充之前计算字数(也称为词频)，而字典是从训练数据构建的。下面是<code class="fe ma mb mc md b">unigram noising</code>的例子。</p><pre class="ks kt ku kv gt nb md nc nd aw ne bi"><span id="7035" class="nf mf it md b gy ng nh l ni nj"># Original<br/>text = 'The quick brown <strong class="md jd"><em class="nk">fox</em></strong> jumps over the lazy dog'</span><span id="7e4a" class="nf mf it md b gy nl nh l ni nj">augmented = Augment(text)<br/>print(augmented)<br/>&gt;&gt;&gt; 'The quick brown <strong class="md jd"><em class="nk">dog</em></strong> jumps over the lazy dog'</span></pre><p id="42a3" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><code class="fe ma mb mc md b">Blank noising</code>方法是<strong class="lg jd">用占位符令牌</strong>替换目标单词。本文提出“_”作为占位符标记。这似乎太简单了，不可能是数据扩充。谢等人声称<code class="fe ma mb mc md b">blank noising</code>是一种避免过度适应特定语境的方法。下面是一个例子:</p><pre class="ks kt ku kv gt nb md nc nd aw ne bi"><span id="1c65" class="nf mf it md b gy ng nh l ni nj"># Original<br/>text = 'The quick brown <strong class="md jd"><em class="nk">fox</em></strong> jumps over the lazy dog'</span><span id="6a89" class="nf mf it md b gy nl nh l ni nj">augmented = Augment(text)<br/>print(augmented)<br/>&gt;&gt;&gt; 'The quick brown <strong class="md jd"><em class="nk">_</em></strong> jumps over the lazy dog'</span></pre><h1 id="2f34" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">语言模型中的平滑</h1><p id="ba99" class="pw-post-body-paragraph le lf it lg b lh nm kd lj lk nn kg lm ln no lp lq lr np lt lu lv nq lx ly lz im bi translated">谢等应用<a class="ae ld" href="https://en.wikipedia.org/wiki/N-gram#Smoothing_techniques" rel="noopener ugc nofollow" target="_blank">插值和</a>(陈和Goodman，1996)对LM进行平滑。术语平滑指的是通过调整最大似然估计来获得更准确概率的技术。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nr"><img src="../Images/1caf67dda9c511af1daaed770b7fb54e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dCPHGbsbdQleE2cA"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated"><a class="ae ld" href="https://unsplash.com/@smnzhu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">西蒙朱</a>在<a class="ae ld" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="b0ad" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">谢等借用平滑技术计算加噪的概率，使得<strong class="lg jd">常见的二元模型不太可能被加噪</strong>。例如，双字母“and the”比“Hong Kong”更常见。所以它可能有意不发出噪音。</p><p id="3a6e" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">另一个目的是<strong class="lg jd">防止从二元语法回退到一元语法。换句话说，它还能防止那些<code class="fe ma mb mc md b">sticky pair</code>发出噪音。例如，“孔”总是跟着“洪”来代表一个城市。谢等人不喜欢对这种粘着对二元组进行噪声处理，否则会将二元组(如“Hong Kam”)分解为两个单元组(如“Hong Kam”)。</strong></p><p id="ff1f" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">通过考虑上述行为，谢提出了三种不同的计算替换概率的方法。他们是<code class="fe ma mb mc md b">interpolation</code>、<code class="fe ma mb mc md b">absolute discounting</code>和<code class="fe ma mb mc md b">Kneser-Ney</code>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ns"><img src="../Images/2d3feab5ab2a53a7899f2d9eb5b8700a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23a2VKajEsWOiNGe3mCvvQ.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">去噪方案(谢等，2017)</figcaption></figure><p id="a28f" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><code class="fe ma mb mc md b">Absolute discounting</code>是用下面的公式计算的。简单地说，N1+(x1，)是以x1开始的不同双元组合(在训练集中)的数量，而x1是双元的前一个词。c(x1，x2)是x1和x2的二元组的总计数，而γ0是固定速率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c0772e504049d9d3c9ed42d78efef309.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*n5PgVLpw9ZRmsGYfejDJbg.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">绝对折现公式(谢等，2017)</figcaption></figure><p id="cbad" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><code class="fe ma mb mc md b">Modified Kneser-Ney</code>(陈&amp;古德曼，1998)是<code class="fe ma mb mc md b">absolute discounting</code>的延伸。它不使用单字码频率，而是考虑n字码频率。这种改进旨在解决通常跟随其他单词的一些单词。再次以“香港”为例，“孔”可能会频繁出现，但它通常出现在“洪”之后。因此，倾向于降低对这种二元图应用噪声的概率。</p><p id="b870" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">对于下面的公式，D1、D2和D3+是具有一个、两个和三个或更多的n元文法，而N1、N2和N3分别是一元文法、二元文法和三元文法的前字数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nu"><img src="../Images/4a67474a5ed6a0d5d28b7feb0765202e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VAbiU31hu49XY5IOIvTVwA.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">修正的Kneser-Ney公式(陈&amp; Goodman，1998)</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/295ef84c5bdecb789b7c72c02bc20bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*to0qAAz9TTE3XDGDZOC5Rg.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">Y、、和D3+的定义(陈&amp; Goodman，1998)</figcaption></figure><h1 id="4bfe" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">实验</h1><p id="555a" class="pw-post-body-paragraph le lf it lg b lh nm kd lj lk nn kg lm ln no lp lq lr np lt lu lv nq lx ly lz im bi translated">以下实验展示了在Penn Treebank和Text8数据上应用数据扩充的性能增益。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f6aab8a54f4f93d649184a3cce684485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*TouS2JhTDtm9w4q6sv6_rg.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">Penn Treebank不同去噪方案的困惑(谢等，2017)</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/206ab5fdd91554bfeb5b6c799504c431.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*XGtIMHphjQHpjqwN-0vAxw.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">文本8种不同去噪方案的困惑(谢等，2017)</figcaption></figure><h1 id="c77b" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">拿走</h1><ul class=""><li id="b0aa" class="ny nz it lg b lh nm lk nn ln oa lr ob lv oc lz od oe of og bi translated">数据扩充是提高模型泛化和性能的一种行之有效的方法。</li><li id="ac73" class="ny nz it lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated">前面提到的数据扩充是在nlpaug中实现的，NLP aug是用于文本库的python数据扩充。</li></ul><h1 id="94ec" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph le lf it lg b lh nm kd lj lk nn kg lm ln no lp lq lr np lt lu lv nq lx ly lz im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。在<a class="ae ld" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ld" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上随时联系<a class="ae ld" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>。</p><h1 id="be7f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">延伸阅读</h1><ul class=""><li id="a617" class="ny nz it lg b lh nm lk nn ln oa lr ob lv oc lz od oe of og bi translated"><a class="ae ld" href="https://github.com/stanfordmlgroup/nlm-noising" rel="noopener ugc nofollow" target="_blank">斯坦福大学正式实施</a></li><li id="ed1e" class="ny nz it lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated">全面的文本数据扩充库</li></ul><h1 id="1470" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">参考</h1><ul class=""><li id="fbf5" class="ny nz it lg b lh nm lk nn ln oa lr ob lv oc lz od oe of og bi translated">南陈和古德曼。<a class="ae ld" href="https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1" rel="noopener ugc nofollow" target="_blank">语言建模平滑技术的实证研究</a>。1998</li><li id="7368" class="ny nz it lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated">Z.谢，王世义，李军，李维，聂，朱拉夫斯基，吴亚英。<a class="ae ld" href="https://arxiv.org/pdf/1703.02573.pdf" rel="noopener ugc nofollow" target="_blank">神经网络语言模型中平滑数据噪声</a>。2017</li><li id="ce8c" class="ny nz it lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated">谢，戴，贺维，梁明堂，乐庆伟。<a class="ae ld" href="https://arxiv.org/pdf/1904.12848.pdf" rel="noopener ugc nofollow" target="_blank">无监督数据增强</a>。2019</li></ul></div></div>    
</body>
</html>