<html>
<head>
<title>Understand Optimizers in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解深度学习中的优化器</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understand-optimizers-in-deep-learning-694f4f0eb048?source=collection_archive---------1-----------------------#2021-06-18">https://pub.towardsai.net/understand-optimizers-in-deep-learning-694f4f0eb048?source=collection_archive---------1-----------------------#2021-06-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="de53" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="734e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">优化器是机器学习的典范</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c585a4a8384223572870fffe10930a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K7frQxVfyQK6NzvJ"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@brett_jordan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布雷特·乔丹</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><blockquote class="li lj lk"><p id="31f7" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">优化器</em> </strong></p></blockquote><p id="b365" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">优化器是机器学习的范例，特别是在深度学习中，通过减少或最小化我们模型中的损失，使月亮的工作变得美丽。优化器是用来改变我们的神经网络的属性的方法或算法，例如权重和学习率，以减少我们网络中的损失。</p><p id="383e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">优化算法或策略负责减少损失函数，并通过更新权重来提供最准确的结果。</p><p id="0b2c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">深度学习/神经网络中的反向传播</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/d594603cc2799ddaa3255cf9767cdce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*J9Du226w1oKBh-zv0MnKPw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图像<a class="ae lh" href="https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="444b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">反向传播由优化器函数完成，这是神经网络训练的本质。在这种情况下，基于在前一时期(即迭代)中获得的误差率来微调神经网络的权重。这种权重的减少使我们能够降低错误率，并通过提高其泛化能力来使模型可靠。这是训练人工神经网络的标准方法，有助于计算与网络中所有权重相关的损失函数的梯度。</p><blockquote class="li lj lk"><p id="8549" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">渐变下降</em> </strong></p></blockquote><p id="f6bc" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在理解梯度下降之前，我们必须知道它为什么会出现以及它的重要性。所以我建议你先看看<strong class="lo jd">强力算法</strong>，它指的是一种不包含任何提高性能的捷径的编程风格。</p><p id="fe87" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">比如说</strong>，就像我们在旅行商问题(TSP)中做的那样。使用它来解决具有大数据集的深度学习/ANN任务可能需要一周、一个月或一年，因此针对这种维数灾难，<strong class="lo jd"> <em class="ln">梯度下降</em> </strong>应运而生。</p><p id="b1b7" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">因此，<strong class="lo jd">梯度下降</strong>是一种优化技术，用于通过最小化成本函数来改进深度学习和基于神经网络的模型。</p><p id="1c0b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">借助于这个例子，可以很容易地理解梯度下降的工作原理。假设我们有一个大数据集，其中有数百万条记录，然后梯度下降法获取完整的数据集，对于一个时期(一轮迭代)，它进行前向和后向传播，然后为每个时期更新权重。所以用这种方法最终损失函数减少了。</p><p id="a560" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">更准确地说，梯度下降是一种算法，用于以最佳方式迭代不同的权重组合，以找到具有最小误差的最佳权重组合。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/d74fa03adfcf6e8707fd2eceeb62bae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*DJYm8X0hnfJOview9LcM8g.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图像<a class="ae lh" href="https://medium.com/@rndayala/gradient-descent-algorithm-2553ccc79750" rel="noopener">来源</a></figcaption></figure><p id="7dd3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">梯度下降的最终目标是找到如图所示的全局最小值。</p><p id="b75b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">优势:</strong></p><ul class=""><li id="9b77" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">易于理解和实现。</li><li id="1e13" class="mn mo it lo b lp mw ls mx mi my mj mz mk na mh ms mt mu mv bi translated">计算也很容易。</li></ul><p id="5e1c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">缺点:</strong></p><ul class=""><li id="43ce" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">计算成本高，因为需要更多资源和高RAM。</li><li id="a079" class="mn mo it lo b lp mw ls mx mi my mj mz mk na mh ms mt mu mv bi translated">它非常慢，因为对于大数据集来说，它要进行传播和权重更新，且转换也进行得很慢。</li><li id="527b" class="mn mo it lo b lp mw ls mx mi my mj mz mk na mh ms mt mu mv bi translated">它需要大量的内存来计算整个梯度</li></ul><blockquote class="li lj lk"><p id="4eba" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">渐变下降的类型:</em> </strong></p></blockquote><ul class=""><li id="008d" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">批量梯度下降或普通梯度下降2</li><li id="4fd0" class="mn mo it lo b lp mw ls mx mi my mj mz mk na mh ms mt mu mv bi translated">随机梯度下降</li><li id="3703" class="mn mo it lo b lp mw ls mx mi my mj mz mk na mh ms mt mu mv bi translated">小批量梯度下降</li></ul><h2 id="b8bb" class="nb nc it bd nd ne nf dn ng nh ni dp nj mi nk nl nm mj nn no np mk nq nr ns iz bi translated"><strong class="ak">随机梯度下降</strong></h2><p id="4d24" class="pw-post-body-paragraph ll lm it lo b lp nt kd lr ls nu kg lu mi nv lx ly mj nw mb mc mk nx mf mg mh im bi translated">让我们举个例子来理解一下。随机梯度下降每次迭代选择一些随机样本，而不是整个数据集。</p><p id="0b36" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">因此，研究人员和科学家开始为每个前向和后向传播逐个选择样本记录，并为每个记录更新记录。</p><p id="092e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">对于每个时期，将有一个迭代次数的介绍。假设对于一个时期，我们为传播进行10k次迭代，因此对于两个时期，对于前向和反向传播迭代次数将是2*10 k，以此类推，并为每个记录更新记录。</p><p id="30c0" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">优势:</strong></p><ul class=""><li id="3f75" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">与梯度下降相比，它需要更少的RAM和更少的其他计算资源来到达特定点。</li></ul><p id="3a83" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">缺点:</strong></p><ul class=""><li id="0544" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">转换很慢，因为每个时期都要进行多次迭代。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7f33ee6856b2e69f0261f9a5ecec3e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*cgiBxmb1yZcWCd-o8hPhXg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图像<a class="ae lh" href="https://towardsdatascience.com/the-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="43e9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在上图中，我们知道<strong class="lo jd">全局</strong> <strong class="lo jd">最小值</strong>的值大于1，那么我们必须为全局最小值选择最合适的值。这项工作由<strong class="lo jd"> SGD </strong>完成。</p><h2 id="dbd3" class="nb nc it bd nd ne nf dn ng nh ni dp nj mi nk nl nm mj nn no np mk nq nr ns iz bi translated"><strong class="ak">小批量梯度随机完成</strong></h2><p id="5ba3" class="pw-post-body-paragraph ll lm it lo b lp nt kd lr ls nu kg lu mi nv lx ly mj nw mb mc mk nx mf mg mh im bi translated">小批量梯度下降是梯度下降算法的一种变体，研究人员或科学家过去常常在每个时期将训练数据集分成小批量。对于每个时期，需要一些批量大小。</p><p id="9100" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">例如，</strong>让我们有10K的数据集，批量=1000条记录，那么，通过将数据集除以批量，可以计算出1个历元的迭代次数。在这种情况下，1个时期的迭代次数=10K/1000，即10。</p><p id="d7d3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">以这种方式，根据迭代次数，发生正向和反向传播，并且针对每个时期更新权重。</p><p id="5d88" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">它是深度学习领域中梯度下降最常见的实现。</p><p id="95d5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">优势:</strong></p><ul class=""><li id="243d" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">计算成本低于SGD。</li><li id="62b0" class="mn mo it lo b lp mw ls mx mi my mj mz mk na mh ms mt mu mv bi translated">需要更少的资源。</li></ul><p id="e424" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">劣势:</strong></p><ul class=""><li id="f1f5" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">我们的转换会有噪声，因为我们只知道批次，而不是全部数据。</li><li id="1c2f" class="mn mo it lo b lp mw ls mx mi my mj mz mk na mh ms mt mu mv bi translated">比SGD需要更多时间。</li></ul><div class="nz oa gp gr ob oc"><a rel="noopener  ugc nofollow" target="_blank" href="/step-by-step-basic-understanding-of-neural-networks-with-keras-in-python-94f4afd026e5"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jd gy z fp oh fr fs oi fu fw jc bi translated">使用Python中的Keras逐步基本了解神经网络</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">具有定义的神经网络的学习</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">pub.towardsai.net</p></div></div><div class="ol l"><div class="om l on oo op ol oq lb oc"/></div></div></a></div><blockquote class="li lj lk"><p id="2cc7" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">梯度下降带动量</em> </strong></p></blockquote><p id="bfb7" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">具有动量的梯度下降保持查看过去的梯度以平滑更新。它<strong class="lo jd">计算梯度<em class="ln"> </em> </strong>的指数加权平均值，然后使用该梯度更新您的权重，比标准梯度下降算法执行得更快。</p><p id="73c4" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">在反向传播期间，我们使用权重(dW)的导数和偏差(db)对学习速率(lr)的导数来更新我们的参数W和b，如下所示:</p><p id="7300" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">— — — <strong class="lo jd"> W = W — lr* dW </strong></p><p id="92b9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">————<strong class="lo jd">b = b—lr * db</strong></p><p id="73e9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">在动量</strong>中，我们取dW和db的<strong class="lo jd">指数</strong>加权平均值，而不是对每个历元独立使用权重(dW)的导数和偏差(db)的导数。</p><p id="b781" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">———<strong class="lo jd">VdW =βx VdW+(1—β)x dW</strong>———</p><p id="2476" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">———<strong class="lo jd">Vdb =βx VD b+(1—β)x db</strong>———</p><p id="9bc6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">其中VdW和Vdb是速度。</p><p id="8401" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">其中，β是另一个超参数，也称为动量，范围从0到1。(β)设置以前值的平均值和当前值之间的权重值，以计算新的加权平均值。</p><p id="f142" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我们将在指数计算加权平均值后更新我们的参数。</p><p id="415c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">— — — <strong class="lo jd"> W = W — lr *VdW </strong></p><p id="a747" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">— — — <strong class="lo jd"> b = b — lr * Vdb </strong> — — — —</p><p id="a189" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">优点</strong>:</p><ul class=""><li id="2e08" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">具有GD和SGD的所有特征。</li></ul><p id="01c6" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">缺点</strong>:</p><ul class=""><li id="b9c2" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">每次更新都要多计算一个变量。</li></ul><blockquote class="li lj lk"><p id="0a08" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it"> Ada-grad: </em> </strong> <em class="it">自适应梯度优化器</em></p></blockquote><p id="154a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd"> Ada-grad </strong>是一个<strong class="lo jd">优化器</strong>，具有特定于参数的学习率，该学习率根据参数在训练过程中反复更新的次数进行调整。更新的参数越多，学习率越小。</p><p id="d0e7" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd"> Ada-grad </strong>允许学习率根据参数进行调整。它也非常适合稀疏数据，如自然语言处理问题和图像相关的操作。</p><p id="c48e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">神经网络学习阶段的两个特征</strong></p><ul class=""><li id="bc57" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">稠密矩阵矩阵中的大部分值不为零。</li><li id="ab3d" class="mn mo it lo b lp mw ls mx mi my mj mz mk na mh ms mt mu mv bi translated">稀疏矩阵矩阵中的大部分值为零。</li></ul><p id="dd5f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">优势</strong>:</p><ul class=""><li id="820d" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">没有必要一次又一次地更新学习率，因为它随着迭代自适应地变化。</li></ul><p id="036f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">缺点</strong>:</p><ul class=""><li id="1b05" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">随着迭代次数变得非常大，学习率降低到非常小的数值。</li></ul><div class="nz oa gp gr ob oc"><a rel="noopener  ugc nofollow" target="_blank" href="/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jd gy z fp oh fr fs oi fu fw jc bi translated">神经网络:递归神经网络的兴起</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">深度学习中的渐进一代</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">pub.towardsai.net</p></div></div><div class="ol l"><div class="or l on oo op ol oq lb oc"/></div></div></a></div><blockquote class="li lj lk"><p id="5719" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it"> Ada-delta &amp; RMS Prop优化器</em> </strong></p></blockquote><p id="16c3" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">均方根prop是另一种自适应学习速率，是对<strong class="lo jd"> AdaGrad </strong>的改进。不像AdaGrad那样对平方梯度进行渐进求和，而是取这些梯度的指数移动平均值。</p><p id="8bfd" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">与RMSprop一样，Adadelta (2012)也是AdaGrad的另一个改进，专注于学习率组件。Adadelta是adaptive delta的简称，这里的<em class="ln"> delta </em>是指当前权重和新更新的权重之差。</p><p id="0ccb" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">Adadelta与RMSprop的不同之处在于，Adadelta完全取消了学习率参数的使用，代之以D，即平方delta的指数移动平均值。</p><p id="56d5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated"><strong class="lo jd">自适应矩估计(Adam)优化器</strong></p><ul class=""><li id="3c12" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">它也被称为optimi <strong class="lo jd"> z </strong> er之王。</li></ul><p id="29fa" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">————<strong class="lo jd">亚当=动量+均方根</strong> — — — —</p><p id="8ab2" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">其中动量负责执行平滑，RMS负责学习速率。</p><p id="18a2" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">更准确地说</p><ul class=""><li id="ffa6" class="mn mo it lo b lp lq ls lt mi mp mj mq mk mr mh ms mt mu mv bi translated">使用V的梯度分量，V是动量中梯度的指数移动平均值。</li><li id="41af" class="mn mo it lo b lp mw ls mx mi my mj mz mk na mh ms mt mu mv bi translated">学习率部分，通过将学习率α除以S的平方根，即RMS prop中的平方梯度的指数移动平均值。</li></ul><p id="cce8" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">— — — <strong class="lo jd"> W = W — lr* dW </strong> — — — — —</p><p id="e312" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">———<strong class="lo jd">VdW =βx VdW+(1—β)x dW</strong>———</p><p id="b415" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">———<strong class="lo jd">Vdb =βx VD b+(1—β)x db</strong>———</p><blockquote class="li lj lk"><p id="6767" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd"> <em class="it">结论</em> </strong></p></blockquote><p id="0907" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">本文吸引人的中心是深度学习优化器，根据它们的用法以及一个优化器的限制如何被下一个克服。“Adam”优化器是处理最复杂和庞大数据集的每个人的最爱，因为它结合了高效处理任何类型输入的动力。</p><p id="575f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu mi lw lx ly mj ma mb mc mk me mf mg mh im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae lh" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="0f36" class="os nc it bd nd ot ou ov ng ow ox oy nj ki oz kj nm kl pa km np ko pb kp ns pc bi translated">推荐文章</h1><p id="c961" class="pw-post-body-paragraph ll lm it lo b lp nt kd lr ls nu kg lu mi nv lx ly mj nw mb mc mk nx mf mg mh im bi translated"><a class="ae lh" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄与Python </a> <br/> 2。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a>T5】3 .<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/deep-learning-88e218b74a14?source=friends_link&amp;sk=540bf9088d31859d50dbddab7524ba35">为什么LSTM在深度学习方面比RNN更有用？</a> <br/> 5。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88?source=friends_link&amp;sk=6844935e3de14e478ce00f0b22e419eb">神经网络:递归神经网络的兴起</a> <br/> 6。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/differences-between-concat-merge-and-join-with-python-1a6541abc08d?source=friends_link&amp;sk=3b37b694fb90db16275059ea752fc16a">concat()、merge()和join()与Python </a> <br/>的区别9。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a> <br/> 10。<a class="ae lh" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>