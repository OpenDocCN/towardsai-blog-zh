<html>
<head>
<title>NLP News Cypher | 09.13.20</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP新闻密码| 09.13.20</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/nlp-news-cypher-09-13-20-a898b9988376?source=collection_archive---------2-----------------------#2020-09-14">https://pub.towardsai.net/nlp-news-cypher-09-13-20-a898b9988376?source=collection_archive---------2-----------------------#2020-09-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/476d7a8478f80825a1dc269ca1f860d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z1L8Kggf3s3tgY-q.jpg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/The_Ninth_Wave" rel="noopener ugc nofollow" target="_blank"> <em class="je">第九波</em> </a> (1850)伊凡·艾瓦佐夫斯基</figcaption></figure><h2 id="056c" class="jf jg jh bd b dl ji jj jk jl jm jn dk jo translated" aria-label="kicker paragraph">自然语言处理每周时事通讯</h2><div class=""/><div class=""><h2 id="1dfd" class="pw-subtitle-paragraph kn jq jh bd b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le dk translated">Aere Perrenius</h2></div></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="59ca" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi mi translated">欢迎回来。希望你喜欢你的一周！我们有另一个更新是在这个星期五。增加了另外11个<a class="ae jd" href="https://datasets.quantumstat.com" rel="noopener ugc nofollow" target="_blank">数据集</a>和5个新的<a class="ae jd" href="https://notebooks.quantumstat.com" rel="noopener ugc nofollow" target="_blank">笔记本</a>。感谢所有为上周更新做出贡献的人！Won Ik Cho，Gayatri Venugopal，Moin Nadeem Himanshu Chaudhary，胡炜昇，Prafull Sharma，Yeshwanth Reddy &amp; Manu Romero！</p><p id="0e4e" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr">如果你想冒险的话</strong>:Lex frid man的真相就在那里，他采访了F/A-18战斗机飞行员Fravor中校，他于2004年在南加州海岸遭遇了一架UFO，俗称“尼米兹事件”。👽</p><figure class="mr ms mt mu gt is"><div class="bz fp l di"><div class="mv mw l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">解密的</figcaption></figure><p id="f580" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr">过去的爆炸</strong>:查看谷歌介绍变形金刚模型的旧博文(2017)。很想看看在过去的几年里，自然语言处理领域取得了多大的进步。😊</p><div class="ip iq gp gr ir mx"><a href="https://ai.googleblog.com/2017/08/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">谷歌人工智能博客</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">机器学习(ML)是谷歌的一个关键战略重点，高度活跃的团队几乎在所有领域都从事研究</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">ai.googleblog.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl ix mx"/></div></div></a></div><p id="4e9a" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr">匿名信:</strong>前Salesforce首席安全官理查德·索彻(Richard Socher)最近离开公司，开始了自己的创业，在撰写本文时，他仍在秘密进行。虽然对这家初创公司知之甚少，但他似乎希望通过纠正错误信息，让互联网成为一个更安全的地方。他的公司正在招聘精选职位，如有兴趣，请申请:</p><div class="ip iq gp gr ir mx"><a href="https://su-sea.github.io/jobs/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">关于我们</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">互联网坏了。是的，我们获得的信息比以往任何时候都多，但太多时候，仇恨和…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">su-sea.github.io</p></div></div></div></a></div><p id="e53e" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr">信息安全新闻:</strong>使用USB保持移动和便携。点击此处了解更多关于Tails操作系统的信息:</p><div class="ip iq gp gr ir mx"><a href="https://tails.boum.org/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">尾巴-回家</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">Tails使用Tor网络来保护您的在线隐私，并帮助您避免审查。像它一样享受互联网…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">tails.boum.org</p></div></div><div class="ng l"><div class="nm l ni nj nk ng nl ix mx"/></div></div></a></div><p id="7d25" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr">借壳</strong>:<a class="ae jd" href="https://littlealchemy2.com/" rel="noopener ugc nofollow" target="_blank">https://littlealchemy2.com/</a></p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="4440" class="nn no jh bd np nq nr ns nt nu nv nw nx kw ny kx nz kz oa la ob lc oc ld od oe bi translated">本周</h1><blockquote class="of og oh"><p id="4b46" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">卷起</p><p id="a121" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">删除零</p><p id="7a01" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">韩国ASR图书馆</p><p id="e533" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">应用NLPers的数据准备情况</p><p id="a2b6" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">PyTorch和KGEs</p><p id="e2c4" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">荣誉奖<em class="jh">🙉</em></p><p id="f14d" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">本周数据集:立体数据集</p></blockquote></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="8236" class="nn no jh bd np nq nr ns nt nu nv nw nx kw ny kx nz kz oa la ob lc oc ld od oe bi translated">卷起</h1><p id="bd72" class="pw-post-body-paragraph lm ln jh lo b lp om kr lr ls on ku lu lv oo lx ly lz op mb mc md oq mf mg mh ij bi translated">你可能已经经历过了，你的下一个NLP项目可能需要你从事知识密集型的工作，比如开放领域的问答或者事实核查。对这些知识密集型任务进行基准测试可能很困难，因为这些任务需要大量的知识来源(当您有各种知识来源时，事情会变得更加困难)。因此，脸书人工智能的一个新基准为研究人员提供了一个集中的基线，以开始他们的研究，并为这些艰巨的任务测试模型性能，它被称为KILT。它利用了基于单一知识来源的跨任务界面:包含590万篇文章的2019/08/01维基百科快照。以下是您将在KILT中处理的任务:事实检查、开放域问答、槽填充、实体链接和对话。</p><p id="1249" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">下面是每个维基记录的样子:</p><pre class="mr ms mt mu gt or os ot ou aw ov bi"><span id="84e3" class="ow no jh os b gy ox oy l oz pa">{<br/> 'wikipedia_title': 'Email marketing',<br/> 'wikipedia_id': 1101759, <br/> 'text': ['p1', 'p2',...., 'pn'], <strong class="os jr"># list of paragraph text</strong><br/> 'anchors': [{"text":,"href":,"paragraph_id":,"start":,"end":} ]  , <br/> 'categories': 'comma separated list of categories'<br/> 'history': <strong class="os jr"># some info from wikipedia, including original url</strong><br/> 'wikidata_info': <strong class="os jr"># wikidata info</strong><br/> }</span></pre><p id="95d7" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr"> GitHub </strong>:</p><div class="ip iq gp gr ir mx"><a href="https://github.com/facebookresearch/KILT" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">Facebook研究/苏格兰裙</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">在下面的文章中描述了短裙基准:https://arxiv.org/abs/2009.02252康达创建-n短裙37 -y…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="pb l ni nj nk ng nl ix mx"/></div></div></a></div><p id="2bf0" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">【https://arxiv.org/pdf/2009.02252.pdf】纸 : <a class="ae jd" href="https://arxiv.org/pdf/2009.02252.pdf" rel="noopener ugc nofollow" target="_blank">纸</a></p><h1 id="2a30" class="nn no jh bd np nq pc ns nt nu pd nw nx kw pe kx nz kz pf la ob lc pg ld od oe bi translated">删除零</h1><p id="655a" class="pw-post-body-paragraph lm ln jh lo b lp om kr lr ls on ku lu lv oo lx ly lz op mb mc md oq mf mg mh ij bi translated">我们的神经网络模型是密集的野兽，热爱线性代数(又名矩阵乘法)。但是这种密度不是很有效。我们实际上可以通过去掉这些零来删除矩阵中的空间，而不会损失性能。在这个问题上，我不想赘述我的天真，以下是弗朗索瓦·拉古纳斯从他之前的<a class="ae jd" href="https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70" rel="noopener">博客</a>帖子中的直观感受:</p><blockquote class="of og oh"><p id="86ef" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">矩阵的<strong class="lo jr"> <em class="jh">稀疏度</em> </strong>是零相对于矩阵大小的分数</p><p id="e55d" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated"><strong class="lo jr">利弊？</strong>如果你有很多零，你就不用计算一些乘法，也不用存储。所以你<strong class="lo jr"> <em class="jh">可能</em> </strong>在体型和速度上有所增益，用于训练和推断。</p><p id="ea17" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated"><strong class="lo jr">弊？</strong>当然，拥有所有这些零可能会对网络精度/性能产生影响。但是到什么程度呢？你可能会感到惊讶。"</p></blockquote><p id="59d3" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">仅供参考，关于稀疏性的更深入的讨论/历史，你可以在这里查看HF的新博客<a class="ae jd" href="https://huggingface.co/blog/pytorch_block_sparse" rel="noopener ugc nofollow" target="_blank">。最酷的是，你可以用一个新的拥抱脸笔记本开始你的稀疏冒险，它有助于用稀疏块取代线性块！执行起来相当简单。更多的直觉，请查看下面他们的笔记本。</a></p><div class="ip iq gp gr ir mx"><a href="https://github.com/huggingface/pytorch_block_sparse" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">hugging face/py torch _ block _ sparse</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">这个PyTorch扩展使用块稀疏矩阵代替密集矩阵，为torch.nn.Linear提供了一个替代方案</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="ph l ni nj nk ng nl ix mx"/></div></div></a></div><p id="acb5" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr">笔记本(6隐藏层RoBERTa) </strong>:</p><blockquote class="of og oh"><p id="fe29" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">请记住，这是在培训之前配置模型，(请注意，他们没有调用任何检查点)</p><p id="a81c" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">哦，你还需要一个图形处理器<em class="jh">🤭</em></p></blockquote><div class="ip iq gp gr ir mx"><a href="https://github.com/huggingface/pytorch_block_sparse/blob/master/doc/notebooks/ModelSparsification.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">hugging face/py torch _ block _ sparse</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="pi l ni nj nk ng nl ix mx"/></div></div></a></div><p id="e489" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr">笔记本</strong>:</p><blockquote class="of og oh"><p id="9a25" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated">用于初始化数据集、标记器和训练稀疏语言模型的实现。</p></blockquote><div class="ip iq gp gr ir mx"><a href="https://github.com/huggingface/pytorch_block_sparse/blob/master/doc/notebooks/01_how_to_train_sparse/01_how_to_train_sparse.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">hugging face/py torch _ block _ sparse</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="pj l ni nj nk ng nl ix mx"/></div></div></a></div><h1 id="5544" class="nn no jh bd np nq pc ns nt nu pd nw nx kw pe kx nz kz pf la ob lc pg ld od oe bi translated">韩国ASR图书馆</h1><p id="8630" class="pw-post-body-paragraph lm ln jh lo b lp om kr lr ls on ku lu lv oo lx ly lz op mb mc md oq mf mg mh ij bi translated">一个新的朝鲜语语音识别库出来了，它叫做KoSpeech，基于PyTorch。它们还包括KsponSpeech语料库的预处理方法和基线模型。😎</p><p id="71f9" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr"> GitHub ( </strong>年度候选人简介图<strong class="lo jr"> ) </strong>:</p><div class="ip iq gp gr ir mx"><a href="https://github.com/sooftware/KoSpeech/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">软件/KoSpeech</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">1Kakao Brain 2光光电子信息技术大学3光光信息大学…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="pk l ni nj nk ng nl ix mx"/></div></div></a></div><h1 id="1ca1" class="nn no jh bd np nq pc ns nt nu pd nw nx kw pe kx nz kz pf la ob lc pg ld od oe bi translated">应用NLPers的数据准备情况</h1><p id="a654" class="pw-post-body-paragraph lm ln jh lo b lp om kr lr ls on ku lu lv oo lx ly lz op mb mc md oq mf mg mh ij bi translated">在NLP中认真对待数据？我们经常会忽略在项目开始前处理数据时可能出现的棘手问题。因此，瑞典RISE公司的人们为那些在企业/机构中应用NLP的人写了一份有趣的关于数据准备的白皮书。这里有一个关于利益相关者应该关注可访问性的简明例子:</p><blockquote class="of og oh"><p id="3cf3" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated"><strong class="lo jr">数据存在吗？</strong>是否记录了完成任务所需的数据？</p><p id="ae12" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated"><strong class="lo jr">数据转换和编码。</strong>NLP面临的主要挑战之一是将文档从源格式(如PDF、Word或Excel)转换为适合处理手头任务的格式。为了超越C带，必须进行数据转换和编码。</p><p id="2e3f" class="lm ln oi lo b lp lq kr lr ls lt ku lu oj lw lx ly ok ma mb mc ol me mf mg mh ij bi translated"><strong class="lo jr">无障碍的法律方面</strong>。不仅数据应该对预期的团队可用，而且团队和他们为手头的任务产生解决方案的努力结果也应该在访问和处理数据的法律方面得到澄清。例如，这包括个人身份信息的处理和版权问题。</p></blockquote><figure class="mr ms mt mu gt is"><div class="bz fp l di"><div class="pl mw l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://arxiv.org/pdf/2009.02043.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="71e2" class="nn no jh bd np nq pc ns nt nu pd nw nx kw pe kx nz kz pf la ob lc pg ld od oe bi translated">PyTorch和KGEs</h1><p id="f8e5" class="pw-post-body-paragraph lm ln jh lo b lp om kr lr ls on ku lu lv oo lx ly lz op mb mc md oq mf mg mh ij bi translated">今年将出版的第一百万个PyTorch图书馆😭😭。如果你对链接预测感兴趣，你可以在这里查看他们的库:</p><p id="8254" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr"> GitHub </strong></p><div class="ip iq gp gr ir mx"><a href="https://github.com/torchkge-team/torchkge" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">火炬队/火炬队</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">TorchKGE:嵌入Python和Pytorch的知识图。TorchKGE是一个用于知识图(KG)的Python模块…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="pm l ni nj nk ng nl ix mx"/></div></div></a></div><h1 id="3e98" class="nn no jh bd np nq pc ns nt nu pd nw nx kw pe kx nz kz pf la ob lc pg ld od oe bi translated">荣誉奖🙉</h1><h2 id="379d" class="ow no jh bd np pn po dn nt pp pq dp nx lv pr ps nz lz pt pu ob md pv pw od jn bi translated">多模态机器翻译；</h2><div class="ip iq gp gr ir mx"><a href="https://github.com/DeepLearnXMU/MM-DCCN" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">DeepLearnXMU/MM-DCCN</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">Pytorch中“用于多模态机器翻译的动态上下文引导胶囊网络”的代码。这个项目是…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="px l ni nj nk ng nl ix mx"/></div></div></a></div><p id="c359" class="pw-post-body-paragraph lm ln jh lo b lp lq kr lr ls lt ku lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><strong class="lo jr">论文</strong>:【https://arxiv.org/pdf/2009.02016.pdf】T2</p><h2 id="3469" class="ow no jh bd np pn po dn nt pp pq dp nx lv pr ps nz lz pt pu ob md pv pw od jn bi translated">填写空白的LM</h2><div class="ip iq gp gr ir mx"><a href="https://ai.stanford.edu/blog/infilling-by-language-modeling/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">如何用语言模型填空</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">当编辑或修改时，我们经常以非线性的方式写作。写一封电子邮件现有的系统可能会建议…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">ai.stanford.edu</p></div></div><div class="ng l"><div class="py l ni nj nk ng nl ix mx"/></div></div></a></div><h2 id="c718" class="ow no jh bd np pn po dn nt pp pq dp nx lv pr ps nz lz pt pu ob md pv pw od jn bi translated">如果您有内存问题，请参阅批量大小故障排除教程</h2><div class="ip iq gp gr ir mx"><a href="https://medium.com/ai2-blog/tutorial-training-on-larger-batches-with-less-memory-in-allennlp-1cd2047d92ad" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">教程:在AllenNLP中用较少的内存进行较大批量的训练</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">这是一系列迷你教程的一部分，帮助您了解AllenNLP库的各个方面。</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">medium.com</p></div></div><div class="ng l"><div class="pz l ni nj nk ng nl ix mx"/></div></div></a></div><h1 id="663d" class="nn no jh bd np nq pc ns nt nu pd nw nx kw pe kx nz kz pf la ob lc pg ld od oe bi translated">本周数据集:立体数据集</h1><h1 id="2dbf" class="nn no jh bd np nq pc ns nt nu pd nw nx kw pe kx nz kz pf la ob lc pg ld od oe bi translated">这是什么？</h1><p id="682b" class="pw-post-body-paragraph lm ln jh lo b lp om kr lr ls on ku lu lv oo lx ly lz op mb mc md oq mf mg mh ij bi translated">StereoSet是一个测量语言模型中刻板印象偏差的数据集。它由17，000个句子组成，衡量不同性别、种族、宗教和职业的模型偏好。</p><h1 id="4e96" class="nn no jh bd np nq pc ns nt nu pd nw nx kw pe kx nz kz pf la ob lc pg ld od oe bi translated">样本:</h1><div class="ip iq gp gr ir mx"><a href="https://stereoset.mit.edu/explore/dev/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">立体声系统</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">探索模型如何与StereoSet交互。</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">stereoset.mit.edu</p></div></div><div class="ng l"><div class="qa l ni nj nk ng nl ix mx"/></div></div></a></div><h1 id="c1d9" class="nn no jh bd np nq pc ns nt nu pd nw nx kw pe kx nz kz pf la ob lc pg ld od oe bi translated">它在哪里？</h1><div class="ip iq gp gr ir mx"><a href="https://stereoset.mit.edu/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jr gy z fp nc fr fs nd fu fw jq bi translated">立体声系统</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">StereoSet是一个测量语言模型中刻板印象偏差的数据集。StereoSet由17，000个句子组成…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">stereoset.mit.edu</p></div></div><div class="ng l"><div class="qb l ni nj nk ng nl ix mx"/></div></div></a></div></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><blockquote class="qc"><p id="7668" class="qd qe jh bd qf qg qh qi qj qk ql mh dk translated"><em class="je">每周日，我们都会对来自世界各地的研究人员的NLP新闻和代码进行每周综述。</em></p><p id="e8a1" class="qd qe jh bd qf qg qh qi qj qk ql mh dk translated">如果您喜欢这篇文章，请帮助我们并与朋友分享！</p><p id="b5f4" class="qd qe jh bd qf qg qh qi qj qk ql mh dk translated"><em class="je">如需完整报道，请关注我们的Twitter:</em><a class="ae jd" href="http://twitter.com/Quantum_Stat" rel="noopener ugc nofollow" target="_blank"><em class="je">@ Quantum _ Stat</em></a></p></blockquote><figure class="qn qo qp qq qr is gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/d155ba0ea7fdb2367849a05b820e07ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:108/0*xfDDysnL64WgNeoH"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="http://www.quantumstat.com/" rel="noopener ugc nofollow" target="_blank">www.quantumstat.com</a></figcaption></figure></div></div>    
</body>
</html>