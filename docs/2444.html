<html>
<head>
<title>Collaboration and Competition in Reinforcement Learning Applications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习应用中的合作与竞争</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/collaboration-and-competition-in-reinforcement-learning-applications-9e4e1c29ac66?source=collection_archive---------1-----------------------#2021-12-28">https://pub.towardsai.net/collaboration-and-competition-in-reinforcement-learning-applications-9e4e1c29ac66?source=collection_archive---------1-----------------------#2021-12-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1361" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="a920" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">回顾OpenAI的多主体演员-评论家技术。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1221e7a0e11ee875357e9bf813d6f522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QU5kR86xeZRAnzbQ.jpg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://lafayettetimes.org/2130/opinion/competition-or-collaboration/" rel="noopener ugc nofollow" target="_blank">https://lafayettetimes . org/2130/opinion/competition-or-collaboration/</a></figcaption></figure><blockquote class="li lj lk"><p id="6f4e" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过10万名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="http://thesequence.ai/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列|子堆栈</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到110，000+的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.ai</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="ba8f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">合作竞争是一个新词，通常用来描述合作与竞争之间的平衡关系。竞争是进化的标志之一，也是社会环境中最成熟的动力之一，因为人类走到一起实现一个特定的目标，同时保持对其他目标的竞争力。多代理强化学习(MARL)是深度学习空间的学科，它类似于我们的社会环境，因为代理需要交互来完成特定的任务。学会合作和竞争似乎是MARL进化的一个明显步骤。然而，大多数MARL方法集中于孤立地训练代理，这限制了协作行为的出现。这一领域最有趣的研究之一来自OpenAI <a class="ae lh" href="https://arxiv.org/abs/1706.02275" rel="noopener ugc nofollow" target="_blank">在一篇研究论文中提出了一种MARL算法，允许代理学习在群体环境中相互合作和竞争</a>。</p><h1 id="4957" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">合作竞争的挑战</h1><p id="9186" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">MARL环境对代理之间的竞争策略的创建提出了重大挑战。首先，多智能体环境很少有稳定的纳什均衡，这使得智能体不得不不断调整它们的策略。因此，代理人有一种内在的压力，要求他们总是变得更聪明，而不一定要合作。不足为奇的是，我们看到MARL模型专注于激发竞争或合作，但很少两者都有。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/fedcb6a89f5c71fbafb00bec79baf8fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U3IbiqFnae4AlAA1f4TZxQ.png"/></div></div></figure><p id="2b94" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在多代理环境中学习的最简单的方法是使用独立的学习代理。这是Q学习或策略梯度等流行的强化学习算法所遵循的方法，但它们已被证明不太适合多代理环境。传统的强化学习方法在多主体场景中面临的挑战与集中的训练和政策评估方法有关。在多智能体环境中，每个智能体的策略随着训练的进行而变化，这使得从任何单个智能体的角度来看，环境变得不稳定，这种不稳定是无法用智能体自身策略的变化来解释的。这提出了学习稳定性的挑战，并阻止了直接使用过去的经验重放，这对于稳定深度Q学习是至关重要的。另一方面，当需要多个代理的协调时，策略梯度方法通常表现出非常高的方差。</p><h1 id="d890" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">多主体演员-评论家</h1><p id="5952" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">为了克服传统强化学习技术的一些挑战，OpenAI引入了一种将集中训练与分散执行相结合的方法，允许策略使用额外的信息来简化训练。称为MADDPG(因为它将另一种称为<a class="ae lh" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank"> DDPG </a>的强化学习算法的原理扩展到多智能体设置)，OpenAI算法允许智能体从自己的动作以及环境中其他智能体的动作中学习。</p><p id="b9ca" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在MADDPG模型中，每个代理被视为一个“行动者”,它从“评论家”那里获得建议，帮助行动者决定在训练期间加强哪些行动。批评家的目标是试图预测一个行动在特定状态下的<em class="ln">值</em>(即我们期望在未来得到的回报)，这被代理人<em class="ln">和行动者</em>用来更新其政策。与传统的强化学习方法相比，通过使用对未来奖励的预测，MADDPG随着时间的推移注入了一些稳定性，因为实际奖励在多代理环境中可能会有很大不同。为了使训练多个能够以全球协调的方式行动的代理变得可行，MADDPG允许批评家访问所有代理的观察和行动。下图说明了MADDPG模型的基本结构。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/88dadeaff314f1527c3f7849951b2d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*aotOIzOaYrHxgh-_hPZ-Ow.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:OpenAI</figcaption></figure><p id="6a39" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">MADDPG方法的主要贡献在于，代理不需要在测试时访问中心评论家，而是根据他们的观察并结合他们对其他代理行为的预测来采取行动。由于每个代理独立地学习一个集中的批评家，这种方法也可以用于建模代理之间的任意奖励结构，包括奖励相反的对立情况。</p><h1 id="6998" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">MADDPG在运行</h1><p id="f19f" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">要了解MADDPG的价值，我们来看一个简单的游戏，在这个游戏中，一些代理(红点)试图在到达水边(蓝点)之前追逐其他代理(绿点)。使用MADDPG，红色的代理人学会互相合作，追逐一个绿色的代理人，获得更高的奖励。与此同时，绿色特工学会了分头行动，当一个被追赶时，另一个试图靠近水源。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/522b72dfb152306fcd5a2f878060e4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*N9_iErtL87eH7XeE8zQxwg.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:OpenAI</figcaption></figure><p id="3cb9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">OpenAI团队通过一系列实验测试了MADDPG，这些实验评估了代理的合作和竞争行为。</p><p id="ffd5" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">a) <strong class="lo jd">合作交流:</strong>这个任务由两个合作的智能体组成，一个说话者和一个听者，他们被放置在一个有三个不同颜色地标的环境中。在每一集，收听者必须导航到特定颜色的地标，并根据其到正确地标的距离获得奖励。</p><p id="babe" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">b) <strong class="lo jd">捕食者-被捕食者:</strong>在这个经典的捕食者-被捕食者游戏的变体中，N个较慢的合作代理人必须在随机生成的环境中追逐较快的对手，其中L个大型地标挡住了道路。</p><p id="489f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">c) <strong class="lo jd">协作导航:</strong>在这种环境下，智能体必须通过物理动作协作才能到达一组L个地标。代理观察其他代理和地标的相对位置，并且基于任何代理与每个地标的接近度来集体奖励。</p><p id="ac9b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">d) <strong class="lo jd">物理欺骗:</strong>这里，N个智能体合作，从总共N个地标中到达单个目标地标。他们的奖励基于任何代理到目标的最小距离(因此只需要一个代理到达目标地标)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/24e84dd152b081c0715eea2adcd68565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dcppWtiVnqTcha3VTi9Rkg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:OpenAI</figcaption></figure><p id="4657" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在所有情况下，MADDPG都优于传统的强化学习方法，如下图所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/1a13ce8bebd9d323563df48e31d81291.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*kpm70oOIHcILJ20mTP_0kQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:OpenAI</figcaption></figure><p id="33dc" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">最近，我们看到竞争成为MARL场景中更重要的组成部分。OpenAI和DeepMind分别在Dota2或Quake III等多人游戏中取得的成绩清楚地表明，合作竞争在MARL环境中是一个非常可实现的目标。像MADDPG这样的技术可以帮助简化竞争性多代理技术的采用。OpenAI团队在GitHub<a class="ae lh" href="https://github.com/openai/multiagent-particle-envs" rel="noopener ugc nofollow" target="_blank">上开源了MADDPG的初始版本。</a></p></div></div>    
</body>
</html>