<html>
<head>
<title>Breakthrough: Can Giving Memory to Entire Neural Nets be Revolutionary?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">突破:给整个神经网络赋予记忆会是革命性的吗？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/breakthrough-can-giving-memory-to-entire-neural-nets-be-revolutionary-8e98e119b13a?source=collection_archive---------5-----------------------#2021-03-03">https://pub.towardsai.net/breakthrough-can-giving-memory-to-entire-neural-nets-be-revolutionary-8e98e119b13a?source=collection_archive---------5-----------------------#2021-03-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f3a9" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/b497347bcf6d7f5990c9e39a2f60d8ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1PZYG-vyImUB_j8wwon7rA.png"/></div></div></figure><h1 id="9df4" class="kh ki iq bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">这是什么？</h1><p id="51b3" class="pw-post-body-paragraph lf lg iq lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">这可以用一个非常简单明了的例子来解释。</p><p id="f282" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">想象一下，如果人工智能是一个学生在翻阅一本书(数据集)并准备考试。我在这里所做的是，我试着给它一个笔记本，记下它认为重要的东西。</p><p id="f519" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">在学习AI时困扰我的一个问题是，如果有人给整个神经网络赋予记忆，会发生什么？听起来这很明显是可行的，对吗？我就是这样做的，我将与你分享，详细地，我是如何做到的，它是如何工作的，以及最重要的问题，<strong class="lh ja">它工作吗</strong>？</p><h1 id="e4a0" class="kh ki iq bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">有用吗？</h1><p id="3ef8" class="pw-post-body-paragraph lf lg iq lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">简短的回答:<strong class="lh ja">是的</strong></p><p id="b4af" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">我决定在斯坦福狗数据集上训练一个标准的ResNet50作为我的基础，然后给完全相同的架构添加一些内存，并以与ResNet50完全相同的方式训练它。</p><figure class="mj mk ml mm gt ka gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/1466ec1813925fc971eca57c14b01006.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*7-ccUoEMgrBxAw3iCo2eeA.png"/></div></figure><p id="1d58" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">这就是<strong class="lh ja">标准ResNet50 </strong>的表现，该模型在测试数据上达到的最高精度为<strong class="lh ja"> 15.14% </strong>。训练在第40个时期停止，因为验证准确性在7个时期中没有增加。</p><figure class="mj mk ml mm gt ka gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/bd93dfc46df10547b380f587946356fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*XVR8A2BpPIaIgsP1e6p3cA.png"/></div></figure><p id="54b2" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">而同样的<strong class="lh ja">AiM</strong>T14】resnet 50就是这样执行的(AiM来自于人工智能的记忆)。我只让它运行到第50纪元。在它的第五十个纪元，它在测试数据上获得了20.06%的准确度，它仍然有潜力继续前进。</p><p id="36e6" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">这只是一个猜测，但看看AiM ResNet50的验证精度如何波动不大，只是稳步上升，看起来它不仅可以学习更多，而且在从数据集学习东西时不会犯太多错误。</p><p id="6c8e" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">一个ResNet50大约有<strong class="lh ja">2380万个参数</strong>。</p><p id="e216" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">一个内存大小约为50000个浮点的aim-ResNet50(上面实验中使用的)，大约有<strong class="lh ja">2410万个参数</strong>。</p><h1 id="994f" class="kh ki iq bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">它是如何工作的？</h1><p id="619c" class="pw-post-body-paragraph lf lg iq lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">我瞄准时的目标是-</p><ul class=""><li id="2fe1" class="mn mo iq lh b li md lm me lq mp lu mq ly mr mc ms mt mu mv bi translated">每一层/神经元/单元都应该有访问存储器的方法。</li><li id="bba3" class="mn mo iq lh b li mw lm mx lq my lu mz ly na mc ms mt mu mv bi translated">神经网络应该有办法更新记忆。</li><li id="346a" class="mn mo iq lh b li mw lm mx lq my lu mz ly na mc ms mt mu mv bi translated">新更新的内存应传递给下一批。</li></ul><p id="9aa3" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">我实现它的方法是，我决定有一个“记忆”层，并将该层与前一层的输出一起传递到下一层，用于模型中的每一层。我还为“内存层”制作了更新块，以便满足第二个条件。这层的重量和偏见给了我一个传递它的方法。</p><h1 id="16fe" class="kh ki iq bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">我是怎么做到的？</h1><p id="897f" class="pw-post-body-paragraph lf lg iq lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">对于CNN，我使用conv2D层作为我的内存。通过线性激活将1的数组作为输入传递给该层，向前传递的值将只是权重和偏差。因此，它可以作为存储层。</p><p id="1ddf" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">如果该存储层与架构中最小的卷积层具有相同的行数和列数，则它可以被重新整形并连接到输入层，并且还可以从其他层输出，而没有问题。</p><p id="7478" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">当其他卷积层的n_rows * n_columns超过存储器层的总存储器(n_rows * n_columns *信道数)时，存储器可以被整形并用零填充到一个信道中，然后，它可以被连接到该层已经在接收的输入。</p><p id="18c0" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">就像通过整形和填充一样，这个记忆层可以连接到几乎所有的输入。</p><p id="83fd" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">将该存储层与一些其他数据/特征一起通过卷积块可以更新该存储层。</p><p id="8a57" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">对于完全连接的神经网络，这甚至更容易做到，因为我们只是使用密集层作为记忆层。同样，通过传递它，一个1的数组作为输入并使用线性激活，我们可以使用weight + bias作为一个内存单元。然后，可以将该记忆层与每一层的输入连接起来，以获得具有记忆的神经网络。</p><h1 id="7979" class="kh ki iq bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">有多百搭？</h1><p id="5503" class="pw-post-body-paragraph lf lg iq lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">只要看一下实现它的方法，就很容易得出结论，存储器可以添加到大多数CNN和完全连接的神经网络架构中，只需很少的改变。我已经测试了向ResNet50和VGG16添加内存，如果这提高了它们的性能，<strong class="lh ja">这很有可能实际上改善几乎所有深度神经网络架构</strong>。</p><h1 id="984b" class="kh ki iq bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">结论</h1><p id="d3df" class="pw-post-body-paragraph lf lg iq lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">我对这个看似简单却有效的想法寄予厚望。虽然这个实验很粗糙，有很多改进的空间，但我想通过这篇文章，让你注意到这个概念的证明。</p><p id="2a6b" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">我喜欢深度学习社区如此支持和帮助。如此多的论文、代码、有帮助的博客文章和笔记本都是免费的，我认为最好的回报方式是将其公之于众。</p><p id="00a8" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated">我很乐意听到你对我的工作的意见，我迫不及待地想看看深度学习社区如何进一步发展这个想法。</p><p id="d711" class="pw-post-body-paragraph lf lg iq lh b li md lk ll lm me lo lp lq mf ls lt lu mg lw lx ly mh ma mb mc ij bi translated"><a class="ae nb" href="https://www.notion.so/Contact-Me-8e6512526e8b4d4fa29e8b573992c370" rel="noopener ugc nofollow" target="_blank">了解我并联系我</a></p></div></div>    
</body>
</html>