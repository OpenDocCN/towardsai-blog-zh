<html>
<head>
<title>Building a Basic Web Text Scraper with Scrapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Scrapy构建一个基本的Web文本刮刀</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/building-a-basic-web-text-scraper-with-scrapy-3f6f2702b1d9?source=collection_archive---------0-----------------------#2020-11-07">https://pub.towardsai.net/building-a-basic-web-text-scraper-with-scrapy-3f6f2702b1d9?source=collection_archive---------0-----------------------#2020-11-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/0f72f4a9e3ac3d50a333af037749fbb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_AtSOlTCnn9wcncW5cP1UQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">在<a class="ae jd" href="https://unsplash.com/s/photos/html?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae jd" href="https://unsplash.com/@_imkiran?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Sai Kiran Anagani </a>拍摄的照片</figcaption></figure><h2 id="fe04" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a>，<a class="ae ep" href="https://towardsai.net/p/category/web-scraping" rel="noopener ugc nofollow" target="_blank">网页抓取</a></h2><div class=""/><div class=""><h2 id="7b68" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">如何从web中提取文本数据？</h2></div><p id="eb4f" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq"> <em class="ma">免责声明:</em> </strong> <em class="ma">本文仅出于教育目的。我们不鼓励任何人抓取网站，尤其是那些可能有条款和条件反对此类行为的网站。</em></p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="89e7" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mi translated">crapy 是一个开源库，旨在实现快速简单的网络抓取和抓取。我将用它来提取哥伦比亚宪法法院的所有判决，大约20000份。这样做的目的是能够使用NLP技术分析大量的非结构化数据。但是现在，这里是如何使用Scrapy设置一个刮刀。</p><h2 id="bd19" class="mr ms jg bd mt mu mv dn mw mx my dp mz ln na nb nc lr nd ne nf lv ng nh ni jm bi translated">装置</h2><p id="7887" class="pw-post-body-paragraph le lf jg lg b lh nj kq lj lk nk kt lm ln nl lp lq lr nm lt lu lv nn lx ly lz ij bi translated">使用pip就像使用pip一样简单(尽管我更喜欢使用<a class="ae jd" href="https://pipenv.pypa.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> pipenv </a>，这是一个非常酷的包管理器，通过虚拟环境来避免令人头疼的问题)。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="747f" class="mr ms jg nt b gy nx ny l nz oa">pip install scrapy</span><span id="7056" class="mr ms jg nt b gy ob ny l nz oa">#or</span><span id="129e" class="mr ms jg nt b gy ob ny l nz oa">pipenv install scrapy</span></pre><h2 id="fff9" class="mr ms jg bd mt mu mv dn mw mx my dp mz ln na nb nc lr nd ne nf lv ng nh ni jm bi translated">目录</h2><p id="d003" class="pw-post-body-paragraph le lf jg lg b lh nj kq lj lk nk kt lm ln nl lp lq lr nm lt lu lv nn lx ly lz ij bi translated">首先，你需要进入你的终端，导航到你选择的工作目录(wd ),输入以下内容，创建Scrapy需要的所有必要的实例:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="8ab6" class="mr ms jg nt b gy nx ny l nz oa"># let's call it 'tutorial' for the next cell to make sense</span><span id="0ea5" class="mr ms jg nt b gy ob ny l nz oa">scrapy startproject &lt;project name&gt; </span></pre><p id="f88d" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这将在你的wd中创建一系列文件，一个新的Scrapy工作目录。来自Scrapy文档:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="c71b" class="mr ms jg nt b gy nx ny l nz oa">"<br/>tutorial/<br/>    scrapy.cfg            <em class="ma"># deploy configuration file</em><br/><br/>    tutorial/             <em class="ma"># project's Python module, you'll import your code from here</em><br/>        __init__.py<br/><br/>        items.py          <em class="ma"># project items definition file</em><br/><br/>        middlewares.py    <em class="ma"># project middlewares file</em><br/><br/>        pipelines.py      <em class="ma"># project pipelines file</em><br/><br/>        settings.py       <em class="ma"># project settings file</em><br/><br/>        spiders/          <em class="ma"># a directory where you'll later put your spiders</em><br/>            __init__.py<br/>"</span></pre><p id="a9fc" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在那里，你会看到一个名为<em class="ma">蜘蛛的目录。</em>那是你的蜘蛛将要生活的地方。从技术上讲，蜘蛛是您创建的类，但它继承了蜘蛛子类。在其中，您可以设置蜘蛛要遵循的各种规则，比如从哪些链接开始，允许哪些域，等等。</p><h2 id="5589" class="mr ms jg bd mt mu mv dn mw mx my dp mz ln na nb nc lr nd ne nf lv ng nh ni jm bi translated">剧本</h2><p id="4169" class="pw-post-body-paragraph le lf jg lg b lh nj kq lj lk nk kt lm ln nl lp lq lr nm lt lu lv nn lx ly lz ij bi translated">现在，转到您选择的IDE并创建一个空白python脚本(any_name.py)，在这里您将创建第一个蜘蛛。首先，导入Scrapy和LinkExtractor，因为我们将处理一系列链接:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="f015" class="mr ms jg nt b gy nx ny l nz oa">import scrapy</span><span id="fa1e" class="mr ms jg nt b gy ob ny l nz oa">from scrapy.linkextractors import LinkExtractor</span></pre><p id="ed07" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，创建一个存储每个URL及其内容的项目类:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="5e79" class="mr ms jg nt b gy nx ny l nz oa">class VeredictItem(scrapy.Item):</span><span id="3f4e" class="mr ms jg nt b gy ob ny l nz oa">    url= scrapy.Field()</span><span id="5cec" class="mr ms jg nt b gy ob ny l nz oa">    content = scrapy.Field()</span></pre><p id="dcba" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，创建Spider类(继承自scrapy。蜘蛛，如下):</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="8a9c" class="mr ms jg nt b gy nx ny l nz oa">class SentspiderSpider(scrapy.Spider):</span><span id="787c" class="mr ms jg nt b gy ob ny l nz oa">    name = 'verdictspider'</span><span id="dd34" class="mr ms jg nt b gy ob ny l nz oa">    allowed_domains = ['corteconstitucional.gov.co']</span><span id="d324" class="mr ms jg nt b gy ob ny l nz oa">    start_urls =<br/>       ['https://www.corteconstitucional.gov.co/relatoria/radicador/buscar.php?vs=26184&amp;pg=0&amp;ponente=&amp;demandado=&amp;Sentencia=&amp;Tipo=Sentencias&amp;busqueda=&amp;conector=AND&amp;segundotema=&amp;anios=Todos&amp;proceso=']</span><span id="bd80" class="mr ms jg nt b gy ob ny l nz oa">    base_url = "https://www.corteconstitucional.gov.co"</span></pre><p id="afe6" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">请注意，我设置了四个不同的变量:蜘蛛的名称、允许蜘蛛爬行到哪些域、应该从哪里开始以及基本URL是什么，依次排列。接下来，我们需要定义方法来告诉蜘蛛到底要做什么。让我们一个接一个地创建三个不同的方法。</p><p id="92ae" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">第一种方法将允许您的蜘蛛解析页面(读取组成页面的HTML，更多信息请参见此处的<a class="ae jd" href="https://www.w3schools.com/html/" rel="noopener ugc nofollow" target="_blank"/>):</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="329b" class="mr ms jg nt b gy nx ny l nz oa">def parse(self, response):</span><span id="1a8f" class="mr ms jg nt b gy ob ny l nz oa">    """Method to parse the HTML table and extract the link to each              verdict """</span><span id="bc68" class="mr ms jg nt b gy ob ny l nz oa">    links =       LinkExtractor(allow_domains=self.allowed_domains).extract_links(response)</span><span id="426d" class="mr ms jg nt b gy ob ny l nz oa">    for link in links:</span><span id="15dc" class="mr ms jg nt b gy ob ny l nz oa">        if not self.use_link(link.url):</span><span id="6008" class="mr ms jg nt b gy ob ny l nz oa">        continue</span><span id="a341" class="mr ms jg nt b gy ob ny l nz oa">        request = response.follow(link, callback=self.parse_links)</span><span id="3f83" class="mr ms jg nt b gy ob ny l nz oa">        yield request</span></pre><p id="726b" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这里是您将使用LinkExtractor的地方。因为您正在搜索一个主页，并且您所寻找的信息在各个链接中被引用，所以您需要识别它们并提取它们。一旦你有了它们，就建立一个检查点(下一个单元格中的方法)，用它来评估你拥有的链接是否是感兴趣的。如果不是，蜘蛛会继续它的路线，不会看它。如果是，蜘蛛将接受服务器的响应，并按照链接，产生请求，即链接包含的内容。</p><p id="7699" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如上所述，第二种方法是健全性检查:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="6c0f" class="mr ms jg nt b gy nx ny l nz oa">def use_link(self, url):</span><span id="ddbf" class="mr ms jg nt b gy ob ny l nz oa">    return '/relatoria/' in url</span></pre><p id="e857" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">它所做的就是创建一个方法，通过这个方法你可以评估一个关键词是否出现在你的蜘蛛提取的链接中。在他的例子中，如果链接中包含模式<em class="ma"> '/relatoria/' </em>，那么这个链接将是感兴趣的，这个方法将返回一个逻辑断言(<em class="ma"> True </em>)。</p><p id="e47b" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最后，最后一个方法将使用解析方法提取每个链接的响应中包含的所有非结构化数据。它将获取在相应的HTML标签中找到的所有文本，并将其提取出来，以便保存在“content”变量中。提取将使用XPath(更多关于XPath做什么<a class="ae jd" href="https://www.w3schools.com/xml/xpath_intro.asp" rel="noopener ugc nofollow" target="_blank">这里</a>)来指导，它将告诉蜘蛛哪些标签是目标和读取的。在这种情况下，<em class="ma">'//div/p[@ class = ' MsoNormal ':</em></p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="2746" class="mr ms jg nt b gy nx ny l nz oa">def parse_links(self, response):</span><span id="acd0" class="mr ms jg nt b gy ob ny l nz oa">    """Method to request a new HTML link to the server, extracted     with the parse method"""</span><span id="aa9b" class="mr ms jg nt b gy ob ny l nz oa">    content = ' '.join(response.xpath('//div/p[@class = 'MsoNormal'][not(contains(normalize-space(), '\u00a0'))]//text()").extract())</span><span id="51ac" class="mr ms jg nt b gy ob ny l nz oa">    content = content.replace('\r\n', ' ')</span><span id="0902" class="mr ms jg nt b gy ob ny l nz oa">    yield VeredictItem(url= response.url, content= content)</span></pre><p id="cf6e" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">提取之后，该方法将删除换行符(' \n ')，并用空格替换它们，最终生成我们之前在VerdictItem类中创建的变量的总体结果:<em class="ma"> URL </em>和<em class="ma">内容</em>。</p><h2 id="b972" class="mr ms jg bd mt mu mv dn mw mx my dp mz ln na nb nc lr nd ne nf lv ng nh ni jm bi translated">运行脚本</h2><p id="8af1" class="pw-post-body-paragraph le lf jg lg b lh nj kq lj lk nk kt lm ln nl lp lq lr nm lt lu lv nn lx ly lz ij bi translated">用您选择的文件名(当然，扩展名是. py)保存整个文件，您就可以开始了。</p><p id="33d9" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">要运行蜘蛛脚本，请再次进入终端，键入以下内容:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="5d17" class="mr ms jg nt b gy nx ny l nz oa">scrapy crawl &lt;spider_name&gt; # verdictspider in this case</span></pre><p id="177a" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最后，如果您想要设置所有数据将存放的文件，请像这样运行脚本:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="ac5a" class="mr ms jg nt b gy nx ny l nz oa">scrapy crawl verdictspider -O all_verdicts.json</span></pre><p id="d3e5" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">给你。该蜘蛛将抓取20多个链接，提取20000多个司法判决的相关文本，并将其全部保存到一个JSON文件中。一个真正的NLP天堂！</p></div></div>    
</body>
</html>