<html>
<head>
<title>Signal Modeling Using Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用递归神经网络的信号建模</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/signal-modelling-using-recurrent-neural-networks-d832f0c50503?source=collection_archive---------0-----------------------#2019-06-11">https://pub.towardsai.net/signal-modelling-using-recurrent-neural-networks-d832f0c50503?source=collection_archive---------0-----------------------#2019-06-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="81fe" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">用RNNs | <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">构建面向AI </a>的信号建模器</h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/58698e7012003daa720db8feeecb5fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Hnq9D3q_CoYWr2aLsh294w.gif"/></div></div></figure><p id="7fd9" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在我之前的<a class="ae lf" href="https://medium.com/@vsadhak/disassembling-recurrent-neural-networks-695ce75dddf6" rel="noopener">文章</a>中，我提到了理解事物的最好方法之一是把它拆开，看看它是如何工作的，然后用它来建造一些东西。在那篇文章中，我们剖析了一个递归神经网络(RNN ),并理解了它的工作原理。本文将集中于用RNNs构建一些东西，从而更深入地理解递归神经网络(RNNs)。我们将在TensorFlow中构建一个递归神经网络来模拟傅立叶级数。</p><h1 id="1aff" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">基础工作</h1><p id="f9c6" class="pw-post-body-paragraph kh ki iq kj b kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le ij bi translated">基本项目在<a class="ae lf" href="https://github.com/veda-s4dhak/RNN_Signal_Modeler" rel="noopener ugc nofollow" target="_blank">这里</a>有安装说明。我不会重复TensorFlow的基础知识，因为在其他地方<a class="ae lf" href="https://www.tensorflow.org/tutorials/" rel="noopener ugc nofollow" target="_blank">有广泛的介绍。然而，我将介绍RNN在TensorFlow中是如何构造的。</a></p><p id="6965" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在我们进入代码的细节之前，让我们先对信号建模器如何工作有一个高层次的了解。我们正在建模的信号只是一组坐标(x，y)。我们希望使用先前的y值通过RNN来建模/预测即将到来的y值。当我们将y值输入RNN时，RNN将创建一个概率分布，将y的可能未来值映射到y的先前值</p><p id="5d48" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这里有一个重要的概念需要考虑。如果y是实值呢？在这种情况下，y的未来值有无限多种可能的结果。例如，如果我们试图对函数y=sin(x)建模。我们知道y被约束在[1，-1]之间。然而，y可以在[1，-1]之间有无限个值。<strong class="kj ja">因此，我们如何创建一个具有无限多个结果的概率空间的概率分布呢？为了简单起见，我们不会。相反，我们将人为地将概率空间(即y的值)离散化为我们需要的分辨率。</strong></p><p id="80eb" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在y = sin(x)的情况下，我们将把[1，-1]分成几组区域。每个区域的大小将决定RNN模拟信号的分辨率。例如，如果我们有两个大小为1的区域。第一个区域包含[1，0]之间的所有值，另一个区域包含[0，-1]之间的所有值。我们模型的分辨率将是1。如果我们使用分辨率为1的RNN对y=sin(x)建模，结果将不会非常准确，因为模型将仅预测2个值。相反，我们需要更高的分辨率，这样模型才会准确。但是，请记住，如果分辨率太高，RNN的大小将会增加，因此正向/反向传播的计算时间将会增加。我们希望在分辨率上取得平衡，我们的RNN生成一个精确的模型，而不需要太多的计算量。</p><p id="82cc" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">要理解的核心思想是，实值函数可以通过离散化用RNN来建模。这个特定于神经网络的离散化过程被称为<em class="mj">一键编码</em> 。一旦实值函数被离散化，RNN就可以创建一个概率分布，将以前的y值建模为新的y值。现在，我们对项目有了一个高层次的了解，让我们进入代码。</p><h1 id="8a67" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">源头</h1><p id="9df1" class="pw-post-body-paragraph kh ki iq kj b kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le ij bi translated">RNN在文件RNN _信号_建模器. py中实现。其余文件用于支持功能:读取CSV、绘制数据、记录。在RNN _信号_建模器. py文件中，有一个名为RNN _信号_建模器的类，它包含三个方法:初始化(缩写为init)、训练和运行。由于RNN是在init()中构造的，我们将主要研究该方法。</p><p id="afaf" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">要定义RNN，必须指定几个关键参数。它们是x_m、x_n、y_m、w_m、w_n、b_m、x_r、x_s、rnn_cell、输出、状态、预测、成本和优化器。注意，所有这些参数在源代码中都带有前缀“self”。理解这些参数非常重要，因为它们实际上代表了RNN。</p><p id="99b3" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">该网络由一组RNN单元构成。RNN细胞的数量等于批量大小。批量大小定义为x_m(批量大小将在下一节解释)。RNN单元的这种分组在每次迭代中运行。下图进一步解释了输入参数x_m和x_n。</p><figure class="ml mm mn mo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mk"><img src="../Images/a9d791a4527b6d3dcc2f9ed5590bef7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nv9RJLT0qu2FI8gMCyffTg.jpeg"/></div></div></figure><p id="89d9" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">大多数神经网络的标准标签矩阵定义为y_m。标签矩阵是用来比较预测值和实际值的矩阵。权重矩阵定义为(w_m，w_n)，偏差矩阵定义为(b_m，1)。偏差矩阵不是多维矩阵。如上所述，输入矩阵的列数(x_n)会影响你的权重和偏差矩阵。具体来说，它会影响权重矩阵中的列数。条件是w_n = x_n = b_m. <strong class="kj ja">参数w_m很重要。它决定了你的RNN的大小。您可以将w_m设置得非常高，这将通过增加RNN细胞内的神经元数量来增加RNN的复杂性。注意细胞不是神经元，细胞内有大量的神经元。<strong class="kj ja">更高复杂度的RNN将能够模拟更复杂的信号，然而，它将需要更多的内存和时间来训练和运行。</strong></strong></p><p id="0620" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">参数x_r和x_s用于将输入矩阵x分割成许多小矩阵，每个小矩阵将被单独馈入RNN。顾名思义，输出参数包含当前迭代中RNN的输出。状态是代表RNN单元内部状态的参数。它将被传递到下一个RNN细胞。这是完成数据的临时保留的地方，这是RNNs的核心功能(这里<a class="ae lf" href="https://medium.com/@vsadhak/disassembling-recurrent-neural-networks-695ce75dddf6" rel="noopener">解释</a>)。其余的参数，预解码，成本，优化等。是大多数其他网络(卷积神经网络等)的标准。)在TensorFlow。</p><h1 id="1261" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">细节决定一切，校准</h1><p id="294c" class="pw-post-body-paragraph kh ki iq kj b kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le ij bi translated">在运行RNN之前，校准网络的超参数非常重要。RNN对一段数据建模的有效程度取决于超参数。你总是可以有超功率的超参数，以便RNN可以模拟几乎任何信号，然而，这是内存和时间的代价。为了达到事半功倍的效果，即在低内存资源和低计算量的情况下精确建模，我们必须对RNN进行校准。</p><p id="0c0f" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ja">这些超参数是什么？这些是批量大小、一键编码、RNN大小、学习速率、次数和下采样因子。</strong>我们将尝试超参数的许多不同组合，并观察神经网络如何响应数据。</p><p id="fcbc" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">批量大小指的是RNN在每次迭代中生成输出所需的数据点数。<strong class="kj ja">我们输入到网络中的数据点越多(批量越大)，网络就越能有效地进行预测。</strong>批量越大，每次迭代网络越慢。这是因为批量大小与每次迭代运行的RNN池数量成正比。更多的RNN单元相当于更多的计算和内存。</p><p id="88eb" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">如上所述的独热编码指的是如上所述被建模的信号的离散化。<strong class="kj ja">有效独热编码的大小指的是RNN的分辨率。</strong>更高的分辨率将创建更精确的模型，但需要模式记忆和训练时间。</p><p id="50df" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">RNN大小是指每个单元内的复杂度。如上所述，RNN，我所说的复杂性指的是RNN细胞内的神经元。<strong class="kj ja">更高的复杂度将允许RNN模拟更复杂的信号，但也会导致训练和运行需要更多的内存和时间。</strong> RNN大小可以被视为网络架构的一部分，但是，我更喜欢将其作为超参数的一部分，因为它比其他超参数更显著地影响精度、计算时间和所需内存。因此，根据您的数据校准RNN大小非常重要。</p><p id="7cc0" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ja">学习率指的是每当RNN出错时，它将做出的修正的大小。</strong>如果学习率太高，RNN将总是过校正，并且永远不会得到正确的预测。另一方面，如果学习率太低，那么RNN将需要很长时间(可能永远不会)才能得到正确的预测。在我们的代码中，我们将从一个高的学习率开始，然后迅速降低到原来的十分之一。这允许RNN在训练的初始阶段过度校正，然后当它在一个良好的值范围内预测时，学习率将降低，以允许RNN学习信号的细微差别。</p><p id="9f56" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">次数是指RNN将重复数据训练的次数。更多的纪元会给RNN更多的机会去尝试、失败和修正它的预测。更高的历元数也意味着需要更长的训练时间。</p><p id="69bd" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">下采样因子实际上与数据集相关，而不是与RNN直接相关。降采样是指降低数据集的分辨率。如果我们以因子10对数据集进行下采样，那么我们将取数据集中每第10个值，并忽略其余的9个值。降采样允许RNN从更一般的角度来看待数据。下采样和批量大小密切相关。<strong class="kj ja">较高的下采样因子和较高的批量，允许RNN回溯到很远的时间，但会损失分辨率(时间序列数据的时间分辨率损失)。</strong></p><p id="d499" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">至此，我们已经设计了一个RNN。那是什么意思？嗯，这意味着你已经有效地创建了一个可以看透时间的人工神经网络。<strong class="kj ja">现在让我们输入我们的神经网络数据，看看它是如何实现的！</strong></p><h1 id="f6bd" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">数据生成</h1><p id="00ff" class="pw-post-body-paragraph kh ki iq kj b kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le ij bi translated">我们将提供什么样的数据？任意信号。我们从哪里得到这些任意的信号呢？我们会自己做的！使用傅立叶级数，我们将生成我们自己的信号，这些信号具有不同的幅度、频率和复杂性。然后，我们将使用这些生成的信号来训练和测试我们的RNN。为了您的方便，下面是傅立叶级数的快速入门。</p><p id="6cf2" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">一个名叫约瑟夫·傅立叶的法国数学家和物理学家首先将傅立叶级数理论化。傅立叶级数的基本思想是，时间的函数可以用各种谐波的无限和来近似，谐波是正弦和余弦，如下所示。</p><figure class="ml mm mn mo gt ka gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/3f2607626081262b3e4742eb5bf1ae8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*uxQnCXu0qR1KeOOEZBq4-g.gif"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk translated">傅立叶级数(<a class="ae lf" href="https://en.wikipedia.org/wiki/Fourier_series" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="df83" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">本质上，我们将通过傅里叶级数产生信号，然后通过RNN对其建模。我们可以增加复杂性和规模来挑战RNNs的极限！谢谢，<a class="ae lf" href="https://en.wikipedia.org/wiki/Joseph_Fourier" rel="noopener ugc nofollow" target="_blank">傅立叶先生</a>！</p><h1 id="2583" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">这里什么都没有…</h1><p id="55fb" class="pw-post-body-paragraph kh ki iq kj b kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le ij bi translated">我选择了由下面的傅立叶级数定义的任意信号来用RNN建模。它有一个变化的频率和幅度范围，足以证明我们的系统。</p><p id="30e6" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">y = 2s in(0.08 x)+0.02 cos(2x)+sin(0.02 x+2)+sin(0.001 x)</p><p id="c3a6" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">从批量大小为10、RNN大小为50、下采样因子为2以及独热编码分辨率为0.01开始，经过45个时期的训练后，我们得到以下结果。</p><figure class="ml mm mn mo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mu"><img src="../Images/b67610ab68ef59cfb1e3ba2c6e13788f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HpRtcHE2X94Qnl6Lz51tEg.gif"/></div></div></figure><p id="c884" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">正如所观察到的，网络对数据的建模不够精确。可能是因为数据的分辨率太高，RNN需要查看更早的数据。我们可以增加批量大小或增加下采样因子。<strong class="kj ja">为了便于实验，我们将下采样因子增加到10。</strong></p><figure class="ml mm mn mo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mu"><img src="../Images/bd31c809de2ff8a968dc62b555972498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*8KOfD37rytMdI7Z7AUto2A.gif"/></div></div></figure><p id="cf5d" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">预测略有改善，网络仍可进一步完善。它仍然不能精确地模拟较高频率的分量。在任何特定的学习速度下，我们只运行5个时期。让我们给网络更多的时间来训练每个学习速率。<strong class="kj ja">让我们将每个学习率的时期数从5增加到50 </strong>。</p><figure class="ml mm mn mo gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/58698e7012003daa720db8feeecb5fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Hnq9D3q_CoYWr2aLsh294w.gif"/></div></div></figure><p id="36c8" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">看起来不错！增加纪元的数量给了RNN更多的机会去学习信号的细微差别，事实也的确如此！</p><p id="fa1d" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">因此，通过简单地改变下采样因子、批量大小和周期数，我们能够校准网络来模拟该信号。</p><h1 id="f440" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">瞧啊。</h1><p id="16cf" class="pw-post-body-paragraph kh ki iq kj b kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le ij bi translated">好了，现在你知道了，我们已经知道了如何使用RNNs来构建一个信号建模器！我希望这能让RNNs更容易理解。这可能是一个很好的练习，产生更复杂的信号，并尝试用RNN来模拟它们。修改本文中提供的RNN代码来模拟多变量信号可能会更好(<strong class="kj ja">提示:</strong>rnn _ signal _ modeller . py中的第66行)。</p><p id="c01a" class="pw-post-body-paragraph kh ki iq kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">RNNs是神经网络领域的核心构件。由于CNN在建模空间方面是有效的，rnn在建模时间方面是有效的。CNN能够创建高效且有效的数据模型，这些数据在每次迭代中都是固有的大数据(例如视频)。另一方面，rnn在对迭代之间存在潜在关系的数据进行建模时是有效的。观察RNNs将如何发展并与人工神经网络领域中的其他基本构件一起应用将是有趣的。我很高兴看到RNNs的未来发展。</p></div></div>    
</body>
</html>