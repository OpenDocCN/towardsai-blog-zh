<html>
<head>
<title>A New BLOOM in AI? Why the BLOOM Model Can Be a Gamechanger</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AI新绽放？为什么布鲁姆模式可以改变游戏规则</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-new-bloom-in-ai-why-the-bloom-model-can-be-a-gamechanger-380a15b1fba7?source=collection_archive---------1-----------------------#2022-07-05">https://pub.towardsai.net/a-new-bloom-in-ai-why-the-bloom-model-can-be-a-gamechanger-380a15b1fba7?source=collection_archive---------1-----------------------#2022-07-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e078" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们现在已经习惯了大型语言模型，为什么这个如此特别呢？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b77909230fd5f87b830d1b39974d091f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bWlFpuGywbwT1vSaylD6sg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">图片由作者使用OpenAI <a class="ae kv" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank"> DALL-E 2 </a>模型生成</figcaption></figure><p id="cd11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">越来越大</strong></p><p id="b301" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当<a class="ae kv" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> BERT </a>问世时，业界为自然语言处理领域的未来选择的道路已经非常清楚了。伯特是第一个真正得到关注的变形金刚，但不是最后一个(可悲的是，我们可以说电影系列也是如此)。</p><p id="9831" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">伯特开启了<a class="ae kv" href="https://arxiv.org/abs/1910.13461" rel="noopener ugc nofollow" target="_blank">巴特</a>、<a class="ae kv" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">罗伯塔</a>等大型变形金刚模型。它表明，自我关注层和更多参数的堆栈对于许多任务(命名实体识别、翻译、问题回答等)来说是惊人的好。然后到了2020年<a class="ae kv" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>强势进入与<a class="ae kv" href="https://openai.com/api/" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>(约1750亿参数的巨型模型)的竞争。它令人印象深刻，但它在宝座上停留了一段时间，谷歌和其他几家公司发布了一系列更大的模型。我们看到了<a class="ae kv" href="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval" rel="noopener ugc nofollow" target="_blank">Gopher</a>(2800亿)<a class="ae kv" href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" rel="noopener ugc nofollow" target="_blank">PALM</a>(540 b)<a class="ae kv" href="https://blog.google/technology/ai/lamda/" rel="noopener ugc nofollow" target="_blank">LaMDA</a>(137 b)。除了龙猫(700亿，反正不是很小)之外，原则是一样的，收集更多的数据，增加参数的数量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/e2bfc6098bef92d1b4fa3a87ba206f7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Icb3dzVs_DjtJSy3.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">语言模型的参数数量呈指数增长。图片来源:<a class="ae kv" href="https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="12de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一场参与者很少的比赛。事实是，伯特向世界表明，只有技术蓝筹公司才能在这场游戏中竞争。如果我们只考虑电费(想象一下购买所有用于培训的GPU需要多少钱)，GPT3估计只需要花费<a class="ae kv" href="https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game" rel="noopener ugc nofollow" target="_blank">1000-2000万美元来培训</a>。</p><p id="0518" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">科技痘:困扰科技公司的开源过敏症。</strong></p><p id="3bc0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个人都兴奋地尝试GPT-3、<a class="ae kv" href="https://www.deepmind.com/publications/a-generalist-agent" rel="noopener ugc nofollow" target="_blank"> GATO </a>、<a class="ae kv" href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model" rel="noopener ugc nofollow" target="_blank">火烈鸟</a>、<a class="ae kv" href="https://openai.com/blog/dall-e/" rel="noopener ugc nofollow" target="_blank">达尔-E </a>和<a class="ae kv" href="https://imagen.research.google/" rel="noopener ugc nofollow" target="_blank"> Imagen </a>，然而，在最好的情况下也有限制(以及这么长的等待名单)。OpenAI、Meta、Google和微软开源了他们的一些模型(例子有<a class="ae kv" href="https://arxiv.org/abs/2205.01068" rel="noopener ugc nofollow" target="_blank"> OPT </a>、<a class="ae kv" href="https://openai.com/blog/vpt/" rel="noopener ugc nofollow" target="_blank"> VPT </a>和<a class="ae kv" href="https://arxiv.org/abs/2101.03961" rel="noopener ugc nofollow" target="_blank"> Switch Transformers </a>)，但是他们对此并不满意。事实是，如果你有班上最好的，你想留给自己。谷歌很容易地使用BERT和following模型来改进谷歌搜索，但失去对它的独家报道是另一回事。</p><p id="16b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们都知道开源的重要性(如果你忘了，还有Linus Torvalds记得)。我们每天都在使用它，程序中的任何人都知道使用开源组件有多重要。然而，开源正在成为一个只想赚钱的搭便车的公司。</p><p id="153f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">平心而论，<a class="ae kv" href="https://www.eleuther.ai/" rel="noopener ugc nofollow" target="_blank"> EleutherAI </a>，<a class="ae kv" href="https://bigscience.huggingface.co/" rel="noopener ugc nofollow" target="_blank"> BigScience </a>，<a class="ae kv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">抱脸</a>试图打破垄断，开源很多很棒的模型。亚当·斯密(Adam Smith)说过类似“市场调节自我购买”的话，所以你看，现在我们有公司会开源，我们会解决这个问题。当然，我们有两个世纪的经验证明亚当·斯密说的不是真的。因此，在这一点上，我们有一个问题，机构在哪里？</p><p id="6df6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">好的、坏的和偏见</strong></p><p id="1ff4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练这些庞大的模型并不环保。不同的文章指出了人工智能对环境的影响(甚至<a class="ae kv" href="https://www.forbes.com/sites/glenngow/2020/08/21/environmental-sustainability-and-ai/?sh=68775f0e7db3" rel="noopener ugc nofollow" target="_blank">福布斯都注意到了</a>，所以你可以想象)。根据马萨诸塞大学的一项研究，训练一个大型<a class="ae kv" href="https://datascientest.com/introduction-au-nlp-natural-language-processing" rel="noopener ugc nofollow" target="_blank"> NLP模型</a>的碳足迹令人印象深刻。此外，我们还必须处理生产硬件(所有GPU)所需的成本和稀有元素。</p><p id="8fc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，自从2013年<a class="ae kv" href="https://www.tensorflow.org/tutorials/text/word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>发表以来，有人注意到有些奇怪的事情，语言模型可能是种族主义者和厌恶女性者。为什么？因为带着收集尽可能多的数据的目的，他们经常收集充满<a class="ae kv" href="https://venturebeat.com/2020/08/07/researchers-quantify-bias-in-reddit-content-sometimes-used-to-train-ai/" rel="noopener ugc nofollow" target="_blank">刻板印象</a>的数据。例如，<a class="ae kv" href="https://www.reddit.com/" rel="noopener ugc nofollow" target="_blank"> Reddit </a>是收集数据最常用的来源之一，国王学院的研究人员发表了一篇<a class="ae kv" href="https://arxiv.org/pdf/2008.02754.pdf" rel="noopener ugc nofollow" target="_blank">文章</a>显示了Reddit社区中性别和宗教偏见的证据。这引发了批评，OpenAI声称已经减轻了GPT 3号的偏见。我们知道公司控制自己是不够的(还记得脸书对假新闻的承诺吗？).事实上，如果这些模型将停止生产，我们需要尽可能地消除伤害。</p><p id="52ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">什么是BLOOM？！为什么我应该关心另一个基于变压器的模型？</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/b40873fdbe80b1ec7c11b521dd1ae13e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*qThT909jTkdUa9WZ.jpg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">BigScience标志。图片来源:<a class="ae kv" href="https://twitter.com/BigscienceW" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="35f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4" rel="noopener ugc nofollow" target="_blank">BLOOM</a>(big science Language Open-science Open-access Multilingual)拥有1760亿个参数，并在1.5万亿字节的文本上进行了训练。在引擎盖下看，该网站报道它有70层，并使用了<a class="ae kv" href="https://paperswithcode.com/method/multi-head-attention" rel="noopener ugc nofollow" target="_blank">多头注意力</a>。好了，技术术语说够了，它是什么意思？这是另一个变压器。为什么这么特别？</p><p id="fdfd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，布鲁姆背后有一个大约1000人的国际团队，主要是学术志愿者(来自50多个国家，使用20多种语言)。该项目涵盖了从法国到加拿大的机构，但也包括像拥抱脸这样的公司。</p><p id="e137" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，他们发布了一个<a class="ae kv" href="https://bigscience.huggingface.co/blog/bigscience-ethical-charter" rel="noopener ugc nofollow" target="_blank">道德宪章</a>，描述了激励这个项目的核心价值。他们决定区分两类内在和外在价值。值得花一点时间来简要描述激发这个项目的价值观。</p><p id="5602" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">内在价值:</strong></p><ul class=""><li id="8bb4" class="lu lv iq ky b kz la lc ld lf lw lj lx ln ly lr lz ma mb mc bi translated">包容性。该项目旨在避免任何歧视。</li><li id="8f7c" class="lu lv iq ky b kz md lc me lf mf lj mg ln mh lr lz ma mb mc bi translated">多样性。大科学项目被定义为一种多样性的手段，涵盖了来自不同国家和背景的许多研究人员。</li><li id="6dfc" class="lu lv iq ky b kz md lc me lf mf lj mg ln mh lr lz ma mb mc bi translated">再现性。作为核心价值观，他们决定开放科学</li><li id="f159" class="lu lv iq ky b kz md lc me lf mf lj mg ln mh lr lz ma mb mc bi translated">开放性。他们进一步细分为一个关注过程的和一个与结果相关的</li><li id="a40c" class="lu lv iq ky b kz md lc me lf mf lj mg ln mh lr lz ma mb mc bi translated">责任。他们称之为个人和集体的责任，也是社会和环境的责任。</li></ul><p id="c7dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">外在价值:</strong></p><ul class=""><li id="c0c3" class="lu lv iq ky b kz la lc ld lf lw lj lx ln ly lr lz ma mb mc bi translated">可达性。它们被描述为与公开性相关联，但扩展的目的是向更广泛的公众开放和解释。</li><li id="37f1" class="lu lv iq ky b kz md lc me lf mf lj mg ln mh lr lz ma mb mc bi translated">透明度。与开放性相关，大科学鼓励项目的泄露和扩散</li><li id="26b4" class="lu lv iq ky b kz md lc me lf mf lj mg ln mh lr lz ma mb mc bi translated">跨学科。重点是从一开始就将不同的学科(计算机科学、语言学、社会学、哲学等等)联系起来。</li><li id="b56f" class="lu lv iq ky b kz md lc me lf mf lj mg ln mh lr lz ma mb mc bi translated">多语制。与不同的价值观相联系，自项目构思以来，他们的目标是涵盖不同的语言，作为一种包容性的手段</li></ul><p id="cf2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从这个事实开始，这个宪章不是一个模糊的承诺。首先，该模型在由核能(一种低碳能源)驱动的Jean Zay公共超级计算机上进行训练。此外，他们还将硬件产生的热量用于吃掉校园内的建筑。</p><p id="45c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">意识到过去的教训，他们试图限制种族主义或性别歧视协会的危害。怎么会？包括学者(包括伦理学家、法律学者和哲学家)，甚至包括脸书或谷歌的雇主。此外，他们不只是浏览网页，而是选择500个来源(在研讨会上讨论，包括社区团体，如<a class="ae kv" href="https://www.masakhane.io/" rel="noopener ugc nofollow" target="_blank"> Masakhane </a>、<a class="ae kv" href="https://www.latinxinai.org/" rel="noopener ugc nofollow" target="_blank"> LatinX in AI </a>和<a class="ae kv" href="https://machinelearningtokyo.com/" rel="noopener ugc nofollow" target="_blank">机器学习东京</a>)。研究人员告诉《自然》( Nature)杂志，即使有这些警告，这个模型也不可能没有偏差。但由于代码和数据集是开放的，他们可以了解有害行为的根源并加以改进。</p><p id="3a62" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型将免费使用，并将很快通过HuggingFace推出(他们还计划推出一个更小、硬件更少的版本以及一个服务器分布式版本)。</p><p id="b8a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="3c6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NLP模型可能会改变世界，人工智能将会渗透到我们未来生活的方方面面。然而，我们知道当有突破性技术时垄断是一个问题(<a class="ae kv" href="https://www.cablefax.com/technology/how-at-amp-t-became-a-monopoly-2" rel="noopener ugc nofollow" target="_blank">还记得电话</a>？没有打破垄断的互联网将会完全不同)。迄今为止，语言模型一直是富裕科技公司的一个小俱乐部的爱好。BLOOM是第一个让所有人都能从AI中受益的努力。在未来，我们可能到处都有聊天机器人，我们需要为公众提供大型模型。</p><p id="7681" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">附加资源</strong></p><ul class=""><li id="027f" class="lu lv iq ky b kz la lc ld lf lw lj lx ln ly lr lz ma mb mc bi translated">好奇想知道更多关于技术部分的内容，看一下这里的<a class="ae kv" href="https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours" rel="noopener ugc nofollow" target="_blank">和这里的</a>和<a class="ae kv" href="https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml" rel="noopener ugc nofollow" target="_blank">和</a></li><li id="0e57" class="lu lv iq ky b kz md lc me lf mf lj mg ln mh lr lz ma mb mc bi translated">你想知道更多他们的方法吗？<a class="ae kv" href="https://montrealethics.ai/category/columns/social-context-in-llm-research/" rel="noopener ugc nofollow" target="_blank">此处</a></li></ul><h1 id="fc06" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">如果你觉得有趣:</h1><p id="a3cd" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">你可以寻找我的其他文章，你也可以<a class="ae kv" href="https://salvatore-raieli.medium.com/subscribe" rel="noopener"> <strong class="ky ir">订阅</strong> </a>在我发表文章时得到通知，你也可以在<strong class="ky ir"/><a class="ae kv" href="https://www.linkedin.com/in/salvatore-raieli/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">LinkedIn</strong></a><strong class="ky ir">上连接或联系我。</strong>感谢您的支持！</p><p id="81e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是我的Github资源库的链接，我计划在那里收集代码，以及许多与机器学习、人工智能等相关的资源。</p><div class="nf ng gp gr nh ni"><a href="https://github.com/SalvatoreRa/tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">GitHub - SalvatoreRa/tutorial:关于机器学习、人工智能、数据科学的教程…</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">关于机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码(python…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">github.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw kp ni"/></div></div></a></div></div></div>    
</body>
</html>