<html>
<head>
<title>Accelerate: Democratizing Deep Learning Distributed Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加速:民主化深度学习分布式培训</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/accelerate-democratizing-deep-learning-distributed-training-70cd1695e6a7?source=collection_archive---------0-----------------------#2021-10-30">https://pub.towardsai.net/accelerate-democratizing-deep-learning-distributed-training-70cd1695e6a7?source=collection_archive---------0-----------------------#2021-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="18b0" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="9faa" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">扩大模型培训的需求比以往任何时候都高；别担心，你不需要改变太多</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/95c1440340de6f8a79fc6fac4e586461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CwT-uU0KHnZKoUIPV26vYg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@claybanks?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">粘土银行</a>在<a class="ae lh" href="https://unsplash.com/s/photos/accelerate?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="fe83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">机器学习和深度学习领域在过去十年中取得了巨大的发展。这主要是由于硬件的进步和我们现在能够产生和收集的大量数据。</p><p id="96e5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，将模型训练扩展到更多计算资源的需求比以往任何时候都高。但这对你来说意味着什么？除了获得或租用更多的设备，你需要学习一个新的API或放弃你目前的习惯吗？分布式培训需要高级软件工程技能吗？</p><blockquote class="me"><p id="d7b7" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">如今，将模型训练扩展到更多计算资源的需求比以往任何时候都高。</p></blockquote><p id="f724" class="pw-post-body-paragraph li lj it lk b ll mo kd ln lo mp kg lq lr mq lt lu lv mr lx ly lz ms mb mc md im bi translated">幸运的是，如果你是PyTorch用户，你很幸运；你只需要添加/修改四行代码。事实上，最终，你的代码看起来会更简单，更容易理解！</p><blockquote class="mt mu mv"><p id="d700" class="li lj mw lk b ll lm kd ln lo lp kg lq mx ls lt lu my lw lx ly mz ma mb mc md im bi translated"><a class="ae lh" href="https://www.dimpo.me/newsletter?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=accelerate" rel="noopener ugc nofollow" target="_blank">学习率</a>是为那些对AI和MLOps的世界感到好奇的人准备的时事通讯。你会在每周五收到我关于最新人工智能新闻和文章的更新和想法。在这里订阅<a class="ae lh" href="https://www.dimpo.me/newsletter?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=accelerate" rel="noopener ugc nofollow" target="_blank"/>！</p></blockquote></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="5313" class="nh ni it bd nj nk nl nm nn no np nq nr ki ns kj nt kl nu km nv ko nw kp nx ny bi translated">DNN训练的基础</h1><p id="5953" class="pw-post-body-paragraph li lj it lk b ll nz kd ln lo oa kg lq lr ob lt lu lv oc lx ly lz od mb mc md im bi translated">首先，让我们看看如何在PyTorch中实现一个标准的训练循环。我们将使用它作为一个运行示例。这个脚本的目的是强调PyTorch训练循环的步骤，这样我们就可以直接看到我们应该改变什么以及如何改变。</p><p id="7455" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通常，为了训练DNN，我们遵循以下算法:</p><ol class=""><li id="4fcc" class="oe of it lk b ll lm lo lp lr og lv oh lz oi md oj ok ol om bi translated">我们在训练数据集上获得预测，并计算损失(即向前传递)</li><li id="9782" class="oe of it lk b ll on lo oo lr op lv oq lz or md oj ok ol om bi translated">我们使用反向传播算法(即反向传递)来计算梯度</li><li id="e2de" class="oe of it lk b ll on lo oo lr op lv oq lz or md oj ok ol om bi translated">我们更新模型的参数(即优化步骤)</li></ol><p id="1758" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们看看PyTorch代码中的这些步骤:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="2e20" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们实例化我们的模型，一个转换器，并将其参数传递给我们选择的设备。然后，我们实例化优化器并加载数据集。</p><p id="8977" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我用代码中的注释标记了上面的三个步骤。查找第20、23和25行。</p><h1 id="9ae8" class="nh ni it bd nj nk ou nm nn no ov nq nr ki ow kj nt kl ox km nv ko oy kp nx ny bi translated">分发一切</h1><p id="f37b" class="pw-post-body-paragraph li lj it lk b ll nz kd ln lo oa kg lq lr ob lt lu lv oc lx ly lz od mb mc md im bi translated">基于PyTorch构建的几个框架提供了一种简单的方法来分发培训过程。比如这里可以看到fastai如何处理分布式训练<a class="ae lh" href="https://docs.fast.ai/distributed.html#DistributedTrainer" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="1435" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然他们试图通过增加一个抽象层次来简化这个过程，但是你应该熟悉他们的API来创建和控制你自己的训练循环。</p><p id="3176" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一方面，Hugging Face的新库旨在实现相同程度的简化，而不中断您的代码。再说说<code class="fe oz pa pb pc b">Accelerate</code>！</p><p id="0980" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">安装完这个库之后，通过一个简单的<code class="fe oz pa pb pc b">pip install accelerate</code>调用，在任何设备上分发PyTorch代码都需要做以下事情:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="57d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您需要做的第一件事是导入并实例化一个<code class="fe oz pa pb pc b">Accelerator</code>对象。您不需要为<code class="fe oz pa pb pc b">model</code>或<code class="fe oz pa pb pc b">data</code>指定设备，因为<code class="fe oz pa pb pc b">Accelerate</code>可以为您处理设备放置。</p><p id="7552" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，在第15行，你准备好你的<code class="fe oz pa pb pc b">model</code>、<code class="fe oz pa pb pc b">data</code>和<code class="fe oz pa pb pc b">optim</code>物品，开始你习惯的训练。最后，在第24行，您应该用<code class="fe oz pa pb pc b">accelerator.backward(loss)</code>替换<code class="fe oz pa pb pc b">loss.backward()</code>调用。你完了！</p><p id="744c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，您的培训循环更加简单，没有设备放置，也没有分发培训样板代码！</p><h1 id="cb12" class="nh ni it bd nj nk ou nm nn no ov nq nr ki ow kj nt kl ox km nv ko oy kp nx ny bi translated">它是如何工作的</h1><p id="cbd1" class="pw-post-body-paragraph li lj it lk b ll nz kd ln lo oa kg lq lr ob lt lu lv oc lx ly lz od mb mc md im bi translated">首先，实例化<code class="fe oz pa pb pc b">Accelerator</code>对象的代码行不仅仅做这些；它还决定分布式训练运行的类型，并执行必要的初始化。我们可以通过传递一个参数<code class="fe oz pa pb pc b">cpu=True</code>来强迫它使用<code class="fe oz pa pb pc b">CPU</code>，或者利用<code class="fe oz pa pb pc b">fp16=True.</code>进行混合精度训练</p><p id="4f6a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，<code class="fe oz pa pb pc b">accelerator.prepare</code>方法准备<code class="fe oz pa pb pc b">model</code>、<code class="fe oz pa pb pc b">DataLoaders</code>和<code class="fe oz pa pb pc b">optimizer</code>。它用适当的分发器包装模型，例如,<code class="fe oz pa pb pc b">DistributedDataParallel</code>容器，它将数据批次放置到相关设备，并准备好<code class="fe oz pa pb pc b">optimizer</code>来处理混合精度训练。</p><p id="738d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，<code class="fe oz pa pb pc b">accelerator.backward</code>方法处理向后传递的必要步骤。</p><h1 id="d1b6" class="nh ni it bd nj nk ou nm nn no ov nq nr ki ow kj nt kl ox km nv ko oy kp nx ny bi translated">结论</h1><p id="e2cf" class="pw-post-body-paragraph li lj it lk b ll nz kd ln lo oa kg lq lr ob lt lu lv oc lx ly lz od mb mc md im bi translated">深度神经网络(DNNs)一直是机器学习领域大多数最新进展背后的主要力量。然而，这些进步在很大程度上依赖于我们可支配的数据量，这是保持深度学习引擎运行的燃料。因此，将模型训练扩展到更多计算资源的需求比以往任何时候都高。</p><p id="c454" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有两种方法可以实现这一点:(I)数据并行化或(ii)模型并行化，前者是最常用的方法。我们从一个简单的例子开始我们的旅程，展示PyTorch Distributed是如何工作的:</p><div class="pd pe gp gr pf pg"><a href="https://towardsdatascience.com/distributed-deep-learning-101-introduction-ebfc1bcd59d9" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd jd gy z fp pl fr fs pm fu fw jc bi translated">分布式深度学习101:简介</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">如何用PyTorch编写分布式深度学习应用的指南</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu lb pg"/></div></div></a></div><p id="588e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们更深入一点，用一个真实的例子构建了一个教程:</p><div class="pd pe gp gr pf pg"><a href="https://towardsdatascience.com/pytorch-distributed-all-you-need-to-know-a0b6cf9301be" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd jd gy z fp pl fr fs pm fu fw jc bi translated">PyTorch分发:所有你需要知道的</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">用PyTorch编写分布式应用程序:一个真实的例子</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pv l pr ps pt pp pu lb pg"/></div></div></a></div><p id="b14c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们看到了如何使用Kubeflow和PyTorch操作符在Kubernetes上正确地完成这项工作:</p><div class="pd pe gp gr pf pg"><a href="https://towardsdatascience.com/pytorch-distributed-on-kubernetes-71ed8b50a7ee" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd jd gy z fp pl fr fs pm fu fw jc bi translated">停止在一个GPU上训练模型</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">今天，没有什么可以阻止你在多个GPU上扩展你的深度学习训练过程</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pw l pr ps pt pp pu lb pg"/></div></div></a></div><p id="a3f1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文研究了如何在不中断代码的情况下扩展我们的训练循环。<code class="fe oz pa pb pc b"><a class="ae lh" href="https://huggingface.co/docs/accelerate/" rel="noopener ugc nofollow" target="_blank">Accelerate</a></code>通过<a class="ae lh" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">抱紧脸</a>是目前为止最简单的实现方式。你提议用什么方法来并行化你的训练循环？你打算用<code class="fe oz pa pb pc b">Accelerate</code>吗？让我们在评论区开始讨论吧！</p><h1 id="9fa0" class="nh ni it bd nj nk ou nm nn no ov nq nr ki ow kj nt kl ox km nv ko oy kp nx ny bi translated">关于作者</h1><p id="a3b5" class="pw-post-body-paragraph li lj it lk b ll nz kd ln lo oa kg lq lr ob lt lu lv oc lx ly lz od mb mc md im bi translated">我的名字是<a class="ae lh" href="https://www.dimpo.me/?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=accelerate" rel="noopener ugc nofollow" target="_blank">迪米特里斯·波罗普洛斯</a>，我是一名为<a class="ae lh" href="https://www.arrikto.com/" rel="noopener ugc nofollow" target="_blank">阿里克托</a>工作的机器学习工程师。我曾为欧洲委员会、欧盟统计局、国际货币基金组织、欧洲央行、经合组织和宜家等主要客户设计和实施过人工智能和软件解决方案。</p><p id="4fb7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据操作的帖子，请关注我的<a class="ae lh" href="https://towardsdatascience.com/medium.com/@dpoulopoulos/follow" rel="noopener" target="_blank"> Medium </a>、<a class="ae lh" href="https://www.linkedin.com/in/dpoulopoulos/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或Twitter上的<a class="ae lh" href="https://twitter.com/james2pl" rel="noopener ugc nofollow" target="_blank"> @james2pl </a>。</p><p id="8d84" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所表达的观点仅代表我个人，并不代表我的雇主的观点或意见。</p></div></div>    
</body>
</html>