<html>
<head>
<title>Linear Regression using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch进行线性回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/linear-regression-using-pytorch-5c016e69be7b?source=collection_archive---------0-----------------------#2020-10-30">https://pub.towardsai.net/linear-regression-using-pytorch-5c016e69be7b?source=collection_archive---------0-----------------------#2020-10-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c4e7" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/38f29cde7d593879b32c72555e4654db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BO8R8HIaTMVP65G66s-IYA.jpeg"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">来源:<a class="ae kl" href="https://www.pexels.com/photo/gray-industrial-machine-during-golden-hour-162568/" rel="noopener ugc nofollow" target="_blank"> Pexels </a></figcaption></figure><p id="0cb2" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们知道，<strong class="ko ja">‘数据是新的石油。’</strong>意思是就像油一样。如果一个人知道数据的价值，我们就能学会提取和使用，它能解决许多问题。</p><p id="7eb0" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，数据可以用两个东西来解释，模型和误差。</p><p id="9f56" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在本文中，我们将深入线性模型。我们的目标是使用数据，通过线性模型来预测输出。在这里，模型是一条线，它最大程度地缩小了预测值(从模型输出)和目标值之间的距离。</p><p id="ef4f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> L </strong> et的潜入:</p><p id="97b5" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们想学习一个线性模型(yhat ),给定x来模拟y。</p><p id="ec42" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以把它写成，<strong class="ko ja"><em class="lk">yhat</em></strong>=<strong class="ko ja">XW+b .</strong></p><p id="fc17" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> <em class="lk"> yhat </em> =预测</strong></p><p id="24ca" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> <em class="lk"> X </em> =输入</strong></p><p id="1090" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> <em class="lk"> W </em> =重量</strong></p><p id="5be8" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> <em class="lk"> b </em> =偏置</strong></p><h1 id="1bde" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">数据集</strong></h1><p id="62cb" class="pw-post-body-paragraph km kn iq ko b kp mj kr ks kt mk kv kw kx ml kz la lb mm ld le lf mn lh li lj ij bi translated">我们采用了一个小型创业数据集，其中包括4个输入特征(<strong class="ko ja">营销支出、R &amp; D支出、管理和状态</strong>)以及一个目标变量<strong class="ko ja">利润。</strong></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="375f" class="mx lm iq mt b gy my mz l na nb">import pandas as pd<br/>data = pd.read_csv('startup.csv')<br/>data.head()</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/4bb5af2a60d1add95e33073f316d8335.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*v3QvRcglIKWcZrVEW-oIuw.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">数据负责人</figcaption></figure><p id="be36" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们用散点图来找出变量之间的关系。</p><p id="4107" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae kl" href="https://matplotlib.org/3.3.2/api/_as_gen/matplotlib.pyplot.scatter.html" rel="noopener ugc nofollow" target="_blank">https://matplotlib . org/3 . 3 . 2/API/_ as _ gen/matplotlib . py plot . scatter . html</a></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="f3cb" class="mx lm iq mt b gy my mz l na nb">import matplotlib.pyplot as plt<br/>plt.title('R&amp;D vs Profit')<br/>plt.scatter(x = data['R&amp;D Spend'], y = data['Profit'])<br/>plt.show()</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/c2e59cab2e209d57374773997926aa7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*Nb0_IH12Ohy8GIYvJZ1qBw.png"/></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="a85c" class="mx lm iq mt b gy my mz l na nb">plt.title('Administration vs Profit')<br/>plt.scatter(x = data['Administration'], y = data['Profit'], c = 'green')<br/>plt.show()</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3528da10d61da41724415f195e7bc27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*kZyo-p_CZsrDsBpTvQsIaQ.png"/></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="eb98" class="mx lm iq mt b gy my mz l na nb">plt.title('Marketing Spend vs Profit')<br/>plt.scatter(x = data['Marketing Spend'], y = data['Profit'], c = 'red')<br/>plt.show()</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/9d51b5b63f7790ffd3729c0d7fa285e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*YEGyL52q_HwZQGRiKJ-2cQ.png"/></div></figure><p id="386d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们选择我们的X和y</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="b49c" class="mx lm iq mt b gy my mz l na nb"># First, we are selecting our X which consists our input features, for that we are using drop method which takes the column which you want to drop and an axis</span><span id="f4d8" class="mx lm iq mt b gy ng mz l na nb">X = data.drop(['Profit'], axis = 1)</span><span id="e7b0" class="mx lm iq mt b gy ng mz l na nb"># This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.<br/># <a class="ae kl" href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html</a></span><span id="85b9" class="mx lm iq mt b gy ng mz l na nb">from sklearn.compose import ColumnTransformer<br/>from sklearn.preprocessing import OneHotEncoder<br/>ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')</span><span id="4dcb" class="mx lm iq mt b gy ng mz l na nb">X = np.array(ct.fit_transform(X))</span><span id="d99c" class="mx lm iq mt b gy ng mz l na nb"># What we did here ?<br/># We used OneHotEncoder to transform the column(State, which is a categorical feature)<br/># After transforming the categorical feature we are combining it back to our X.<br/></span><span id="d586" class="mx lm iq mt b gy ng mz l na nb"># Now, defining our y<br/>y = data[['Profit']].values<br/># Notice we used double bracket here as it will give the shape of(n, 1) and we will not the issue of 1-D array.</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/72e65dc445f7ce49a164cf7dee7efebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*7rzU0fjU9kLRuterk5HgJg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">一个热编码</figcaption></figure><p id="7967" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">拆分数据</strong></p><p id="7d4c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们现在将数据随机分成3组:训练、验证和测试。</p><ul class=""><li id="45eb" class="ni nj iq ko b kp kq kt ku kx nk lb nl lf nm lj nn no np nq bi translated">Train:用于训练我们的模型。</li><li id="00f9" class="ni nj iq ko b kp nr kt ns kx nt lb nu lf nv lj nn no np nq bi translated">Val:用于在培训期间验证我们模型的性能。</li><li id="2a5f" class="ni nj iq ko b kp nr kt ns kx nt lb nu lf nv lj nn no np nq bi translated">测试:用于评估我们完全训练好的模型。</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="a7f4" class="mx lm iq mt b gy my mz l na nb">TRAIN_SIZE = 0.7<br/>VAL_SIZE = 0.15<br/>TEST_SIZE = 0.15<br/>SHUFFLE = True</span><span id="7198" class="mx lm iq mt b gy ng mz l na nb">def split(X, y, val_size, test_size, shuffle):<br/>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, shuffle = shuffle)<br/>    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = val_size, shuffle = shuffle)<br/>    return X_train, X_val, X_test, y_train, y_val, y_test</span><span id="d813" class="mx lm iq mt b gy ng mz l na nb">X_train, X_val, X_test, y_train, y_val, y_test = split(X, y, VAL_SIZE, TEST_SIZE, SHUFFLE)</span></pre><p id="2416" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">标准化数据</strong></p><p id="56c2" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们需要标准化我们的数据(零均值和单位方差)</p><p id="8c57" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja"> z(标准化值)</strong>=<em class="lk">Xi</em>-<em class="lk">μ</em>/<em class="lk">σ</em></p><ul class=""><li id="fc49" class="ni nj iq ko b kp kq kt ku kx nk lb nl lf nm lj nn no np nq bi translated"><em class="lk"> xi </em> =输入</li><li id="ad26" class="ni nj iq ko b kp nr kt ns kx nt lb nu lf nv lj nn no np nq bi translated"><em class="lk"> μ </em> =平均值</li><li id="8e8d" class="ni nj iq ko b kp nr kt ns kx nt lb nu lf nv lj nn no np nq bi translated"><em class="lk"> σ </em> =标准差</li></ul><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="63ec" class="mx lm iq mt b gy my mz l na nb">from sklearn.preprocessing import StandardScaler<br/>x_scaler = StandardScaler().fit(X_train)<br/>y_scaler = StandardScaler().fit(y_train)</span></pre><p id="8f12" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将把拟合的数据应用于训练和测试数据</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="9ce4" class="mx lm iq mt b gy my mz l na nb">X_train = x_scaler.transform(X_train)<br/>y_train = y_scaler.transform(y_train).reshape(-1, 1)<br/>X_val = x_scaler.transform(X_val)<br/>y_val = y_scaler.transform(y_val).reshape(-1, 1)<br/>X_test = x_scaler.transform(X_test)<br/>y_test = y_scaler.transform(y_test).reshape(-1, 1)</span></pre><p id="e84b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将定义输入维度和输出维度</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="dba0" class="mx lm iq mt b gy my mz l na nb">INPUT_DIM = X_train.shape[1] # size of input <br/>OUTPUT_DIM = y_train.shape[1] # size of output sample</span></pre><p id="4eb5" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将使用torch.nn。这是一个PyTorch类，包含了许多我们将要使用的有用方法。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7dcf" class="mx lm iq mt b gy my mz l na nb">from torch import nn</span><span id="b85b" class="mx lm iq mt b gy ng mz l na nb">class LinearRegression(nn.Module):<br/>    def __init__(self, input_dim, output_dim):<br/>        super(LinearRegression, self).__init__()<br/>        self.fc1 = nn.Linear(input_dim, output_dim)<br/>        <br/>    def forward(self, x_in):<br/>        y_pred = self.fc1(x_in)<br/>        return y_pred</span></pre><p id="a3bc" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这里，我们创建了一个简单的线性回归类，它使用了一个神经网络模块<br/> <strong class="ko ja">。神经网络模块</strong>——当我们想要创建一个定制模型时，我们使用它。这意味着我们正在以我们的方式定义我们的模型。它是用输入和输出维度初始化的，这是我们在上面定义的。<br/> <strong class="ko ja"> nn。线性</strong> -这将线性变换应用到我们的输入和输出样本。这是一个全连接层，有输入和输出。现在，它的输入会有一个权重。它的输出将是<em class="lk"/>【XW+b】，因为它完全是线性的，没有激活函数。</p><p id="b121" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将初始化模型。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4bf2" class="mx lm iq mt b gy my mz l na nb">model = LinearRegression(input_dim = INPUT_DIM, output_dim = OUTPUT_DIM)</span></pre><p id="d133" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将定义我们的损失函数。我们将预测值(y_hat)与实际值(y)进行比较，以确定损失，线性回归的常见损失函数是<strong class="ko ja"> MSE(均方损失)。这个</strong>函数只是计算预测值和实际值之间的差值，并对其求平方。</p><p id="4d03" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">为什么要平方误差？</strong></p><p id="e220" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">将“你错过了多少”平方的原因是它迫使输出为正。</p><p id="cf76" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">为什么只有阳性？</strong></p><p id="8578" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">想象一下，如果您试图让模型正确预测2个点、2个输入→目标预测对。如果第一个有10的误差，第二个有–10的误差，那么平均误差将是零，当你每次错过10时，你将认为你预测完美。因此，您希望每个预测的误差总是正的，这样当您平均它们时，它们不会意外地相互抵消。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="26e4" class="mx lm iq mt b gy my mz l na nb">loss_fn = nn.MSELoss()</span></pre><p id="dac0" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">MSE =(1/N)* SUM((y _ train—y _ pred)* * 2)，其中N = y _ train的长度。</strong></p><p id="1091" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们将前进到我们的优化器。</p><p id="6ff6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当我们试图最小化或最大化某样东西时，那就叫做优化。优化器就像一种方法，通过改变我们使用的参数(权重和学习率)来帮助我们减少损失。</p><p id="a5fc" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这里，我们使用的是来自<strong class="ko ja"> torch.optim </strong>包的<strong class="ko ja"> Adam </strong>优化器，有很多优化器，但是我们将在这里使用Adam。https://pytorch.org/docs/stable/optim.html<a class="ae kl" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="6a61" class="mx lm iq mt b gy my mz l na nb">optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)</span></pre><p id="d17a" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们需要将数据转换成张量。为此，我们使用<strong class="ko ja">火炬。张量</strong>。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="e66c" class="mx lm iq mt b gy my mz l na nb">X_train = torch.Tensor(X_train)<br/>y_train = torch.Tensor(y_train)<br/>X_val = torch.Tensor(X_val)<br/>y_val = torch.Tensor(y_val)<br/>X_test = torch.Tensor(X_test)<br/>y_test = torch.Tensor(y_test)</span></pre><p id="0b04" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们训练我们的模型</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="27d1" class="mx lm iq mt b gy my mz l na nb">NUM_EPOCHS = 100<br/>for epoch in range(NUM_EPOCHS):<br/>    # Forward pass<br/>    y_pred = model(X_train)</span><span id="624b" class="mx lm iq mt b gy ng mz l na nb">    # Loss<br/>    loss = loss_fn(y_pred, y_train)</span><span id="810e" class="mx lm iq mt b gy ng mz l na nb">    # zero all gradients<br/>    optimizer.zero_grad()</span><span id="7556" class="mx lm iq mt b gy ng mz l na nb">    # backward pass<br/>    loss.backward()</span><span id="8497" class="mx lm iq mt b gy ng mz l na nb">    # Update weights<br/>    optimizer.step()<br/>    <br/>    if epoch%20==0: <br/>        print (f"Epoch: {epoch} | loss: {loss:.2f}")</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/001d74e130c67d48e9a57b576019c9d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*58sbMiUuwWnKvl9H2kRw7A.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">每历元损失</figcaption></figure><p id="c1ab" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们来计算预测</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="a0aa" class="mx lm iq mt b gy my mz l na nb">tr_pred = model(X_train)<br/>te_pred = model(X_test)</span></pre><p id="dd15" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们在训练和测试数据上检查我们的模型的性能</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="678d" class="mx lm iq mt b gy my mz l na nb">tr_error = loss_fn(tr_pred, y_train)<br/>te_error = loss_fn(te_pred, y_test)<br/>print(f'train_error: {tr_error:.2f}')<br/>print(f'test_error: {te_error:.2f}')</span></pre><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f5db386eadaffa19a9308bc0463b9b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*nQ4PAyF49Vjg7Ze97ZALBg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">错误</figcaption></figure><p id="ead0" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，我们正在使用度量标准(这将告诉我们线性模型的性能)。我们在这里使用R2分数，也称为决定系数。</p><p id="8f82" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">r2_score = 1 —残差平方和/总误差平方和</p><p id="833b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们试着理解这个公式:</p><p id="695c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">残差的平方和——这只是我们的误差，上面已经计算过了(yi-yi_hat)**2。这告诉我们，我们的实际值和预测值有多大的差异。</p><p id="440c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总误差的平方和—这是总误差(yi-yi的平均值)**2</p><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ny"><img src="../Images/3e9c202b6bb4bf4ab9033a7c19b17cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PHqWmW6WCmkpzKurrgEvIg.jpeg"/></div></div></figure><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="6eb9" class="mx lm iq mt b gy my mz l na nb">from sklearn.metrics import r2_score<br/># r2 = r2_score(y_test, pred_test)<br/># print(f'r2 square for the model is : {r2}')<br/># This gives the below error.<br/># Note : RuntimeError --&gt; Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.<br/>y_test = y_test.detach().numpy()<br/>pred_test = pred_test.detach().numpy()</span></pre><p id="7796" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">R2 _ score的解释</strong></p><p id="a7cc" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果分数是100%，则意味着根本没有方差(模型能够解释总方差)</p><p id="46a5" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们得到一个低分，比如说30%，那么这意味着我们的模型只能解释30%的方差。</p><p id="1c7d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我们的例子中，我们得到了95.66%的分数，这意味着我们的模型可以解释95.66%的方差。</p><p id="fba6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">渐变</strong></p><p id="6c93" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">梯度很重要，因为它帮助我们选择公式中的任何变量，并告诉我们它们是如何变化的，以及变化了多少，这就是为什么我想用一个例子来深入解释它。</p><p id="9f5f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们用一个场景来理解梯度，</p><p id="f078" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设有一位老人想穿过马路到马路的另一边，但是他不能，因为他是盲人，他不知道他应该走多少步，应该朝哪个方向走。你去那里帮助那位老人过马路。</p><p id="5cb7" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这是梯度，老人是重量，因为梯度和重量有关。步数是我们的学习速度。</p><p id="77c3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以，梯度就像向量，它会指出我们可以将损失最小化的方向，而学习率是步骤，它会告诉我们在这个方向上应该走多少步。</p><p id="fca9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用步骤很重要。如果你不指定，那么就有可能达不到目标。想象一下，你从A站乘火车，你想到达C站，但是，你睡着了，到达了d站。</p><p id="b9ce" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">权重渐变</strong></p><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ny"><img src="../Images/65707d05d8081d87efee6a90e6d5c6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cJmK58QZ049x1k48Tq4pTg.jpeg"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">重量梯度</figcaption></figure><p id="3450" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">偏差梯度</strong></p><figure class="mo mp mq mr gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi nz"><img src="../Images/76ab70c23b82b71d7ecd305f5538fd27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pt3jHNTLpbjR38l5WqYlQA.jpeg"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">梯度相对湿度偏差</figcaption></figure><p id="b64a" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我只是一个初学者，希望得到一些反馈，并希望不要对我严厉，如果有错误。谢谢大家！！</p><p id="301d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">code-<a class="ae kl" href="https://github.com/pratikraut1/Linear-Regression-with-PyTorch" rel="noopener ugc nofollow" target="_blank">https://github . com/prati kraut 1/Linear-Regression-with-py torch</a></p><p id="60bc" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">免责声明-标题中没有链接的图片是作者的；否则，会提供一个链接。</p></div></div>    
</body>
</html>