<html>
<head>
<title>Logistic Regression — An Overview with an Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归——示例概述</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/logistic-regression-an-overview-with-example-2661ee075d13?source=collection_archive---------2-----------------------#2020-08-25">https://pub.towardsai.net/logistic-regression-an-overview-with-example-2661ee075d13?source=collection_archive---------2-----------------------#2020-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9e5f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a></h2><div class=""/><div class=""><h2 id="407a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">逻辑回归简介及其在Python中的实现</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/270a1b063a0a2841a672870a59244303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vTYhayQtZmqhmtsF"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@chrisliverani?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克里斯·利维拉尼</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="687d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">众所周知，逻辑回归算法简单易懂，非常可靠而且非常有用，这就是为什么当涉及到二元分类问题时，逻辑回归是任何工程师的首选。</p><p id="7e91" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">逻辑回归使用sigmoid函数输出<strong class="lk jd">连续的概率值</strong>，该值为自变量的任意值，介于0-1之间，然后将这些<strong class="lk jd">概率值</strong>与阈值0.5进行比较。</p><p id="c4b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">任何大于0.5的值被归入"<strong class="lk jd"> 1类，"</strong>"类，任何小于0.5的值被归入"<strong class="lk jd"> 0类"</strong>或"<strong class="lk jd">类，其中特定事件不发生。"</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi me"><img src="../Images/e460e37fc1774fbbd6572f2f2dc92756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5IxgCMTQlym0Q9zk1PtmcQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">s形曲线</figcaption></figure><h1 id="75c9" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">为什么叫Logistic“回归”？</h1><p id="022f" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">人们问的一个常见问题是，如果逻辑回归用于分类问题，为什么它会有“回归”一词？而分类问题为什么不能用线性回归代替Logistic回归呢？</p><p id="b143" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第一个问题的答案是，即使逻辑回归用于二元分类问题，sigmoid方程的输出仍然是一个<strong class="lk jd">连续数值。</strong>我们只是将这些值与阈值0.5进行比较，并将结果分类为0或1。</p><p id="6545" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">线性回归不能用于分类问题的主要原因是，我们的线性回归模型的输出不能限制在"<strong class="lk jd">0–1 "</strong>之间，对于概率值，我们总是需要0–1之间的值。</p><p id="cb68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">更好的解释方式是看下面的图片:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/58e27db7e4e64bda338fa49f6f08fe7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*e3RA1kX6kKb0L6F01hiASg.jpeg"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">例子</figcaption></figure><p id="abd7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">比方说。我们使用线性回归来根据诊断的肿瘤大小对特定患者是否患有癌症进行分类。我们追踪阈值0.5，发现任何大于75的肿瘤大小导致癌症的阳性病例，任何小于0.5的值导致阴性病例。<br/>这似乎很好，但问题出在哪里？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/bf7d8baab522d2012a6ab5a3dc162d0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*jbzNZcyn-eiq1SS1cZdIgw.jpeg"/></div></figure><p id="e134" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从上图中可以看出，<strong class="lk jd">右侧的新肿瘤尺寸</strong>导致最佳拟合线出现较大偏差，导致一些数据点被错误分类。</p><h1 id="b077" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">逻辑回归的假设</h1><p id="0522" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">逻辑回归没有做出线性回归做出的许多关键假设，如<strong class="lk jd">线性</strong>、<strong class="lk jd">同方差、</strong>或<strong class="lk jd">正态性</strong>。但是，有些假设确实适用，例如:</p><ol class=""><li id="08e7" class="ne nf it lk b ll lm lo lp lr ng lv nh lz ni md nj nk nl nm bi translated"><strong class="lk jd">二元因变量</strong> <br/>二元分类的因变量必须是二元的</li><li id="e56b" class="ne nf it lk b ll nn lo no lr np lv nq lz nr md nj nk nl nm bi translated"><strong class="lk jd">无多重共线性</strong> <br/>多重共线性<a class="ae lh" href="https://medium.com/analytics-vidhya/how-multicollinearity-is-a-problem-in-linear-regression-dbb76e25cd80" rel="noopener">在我以前的一篇文章</a>中讨论过，会导致线性回归中的问题，也会导致逻辑回归中的问题</li><li id="3b3d" class="ne nf it lk b ll nn lo no lr np lv nq lz nr md nj nk nl nm bi translated"><strong class="lk jd">独立观测值<br/> </strong>观测值应相互独立，不应来自重复数据。</li></ol><p id="6d01" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果上述假设都考虑到了，我们可以期待我们的逻辑模型表现良好。</p><h1 id="5ef9" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">逻辑回归背后的数学</h1><p id="5a3f" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">如上所述，逻辑回归使用Sigmoid方程来输出连续的概率值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/14bcf765886ed69adee333f3c2dac28a.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*GdTEWSZtcY9TG5K3LUL2Uw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">Sigmoid方程</figcaption></figure><p id="e8a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Sigmoid方程或<strong class="lk jd">假设方程</strong>在分母中有一个“<strong class="lk jd">z的负指数”</strong>项，其中<strong class="lk jd"> z </strong>只不过是“系数转置”和“独立变量”的<strong class="lk jd">点积</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1ccb33768787aba59c7e9540840fa7e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*gzl2P_kw-EtOndsNcbMcrA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">简化的Sigmoid方程</figcaption></figure><p id="05bd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们知道这里的X，它是我们数据的独立变量，但是我们如何找到系数的值呢？</p><p id="d40b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们转向<strong class="lk jd">成本函数</strong>和<strong class="lk jd">梯度下降法。</strong></p><p id="494b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当试图找到我们的系数时，我们总是以能够最小化我们的成本函数的值为目标，这就是<strong class="lk jd">梯度下降法的用武之地。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/90b4cf5110ff16310a36c4ffbbf5bfc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rBf45uv4ahrs6I2bBwcSDg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">逻辑回归的成本函数</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/18d2d898c94be1d415f8ae004cabbbfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*4l6RZr4yChG6r3SUh0-Xpg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">梯度下降方程</figcaption></figure><p id="1f3b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用我们的梯度下降方程，我们通过多次迭代找到我们的系数值，直到我们的系数值不再收敛。</p><h1 id="f122" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">实际实施</h1><p id="1bfb" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">对于这一部分，我将处理来自Andrew ng的机器学习入门课程的数据集，该数据集接受学生在两次考试中的分数，并以二进制值<strong class="lk jd"> 0或1 </strong>的形式给出输出。<br/> 0表示学生未被录取，1表示学生未被录取。</p><p id="d7e2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将数据集导入Jupyter笔记本:</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="2e47" class="ob mg it nx b gy oc od l oe of">data = pd.read_csv('ex2data1.txt')<br/>data.columns = ['Exam1','Exam2','Outcome']<br/>sns.scatterplot(data['Exam1'],data['Exam2'],hue=data['Outcome'])</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6cc6153ba4fc213174c938f14cf7ed3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*AvUISGY_--fiVChiKpgv-Q.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">散点图</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/b6623c8610abff8edefe2d668fbe95ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*5JP_bZhHxcx0Eio-bskoag.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料组</figcaption></figure><p id="2e91" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自变量将是考试1和考试2的分数以及一个常数值。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="40e8" class="ob mg it nx b gy oc od l oe of">X = data[['Exam1','Exam2','ones']]</span></pre><p id="50a6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了找到我们的最佳系数，我们将实施梯度下降法，在这种方法中，我们将对θ值进行初始猜测，并从那里开始工作。</p><p id="fdf9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将我们最初的θ猜测值作为0，定义X和y变量:</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="ba09" class="ob mg it nx b gy oc od l oe of">Theta = np.array([0,0,0])<br/>Theta = Theta.reshape(3,1)<br/>X_array = np.array(X)<br/>X_array.shape<br/>y_array = np.array(y)<br/>y_array = y_array.reshape(99,1)</span></pre><p id="e60f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将运行梯度下降法进行大量迭代，直到我们的系数不再收敛。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="8b67" class="ob mg it nx b gy oc od l oe of">T = []<br/>I = []<br/>i=0<br/>while i &lt;10000000:<br/>    hypothesis = 1/(1+np.exp(-1*(np.dot(X_array,Theta))))<br/>    submission = hypothesis - y_array<br/>    transpose = np.transpose(submission)<br/>    grad = (1/99) * np.dot(transpose,X)<br/>    grad = grad.reshape(3,1)<br/>    grad<br/>    Theta = Theta - (0.001*grad)<br/>    Theta<br/>    i = i +1<br/>    T.append(Theta[0])<br/>    I.append(i)</span></pre><p id="f683" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在继续之前，我想稍微解释一下代码。</p><p id="fe0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">“假设”术语只不过是sigmoid方程。</p><p id="6a06" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我已经使用了NumPy的<strong class="lk jd">点和转置函数</strong>来执行我的矩阵操作。</p><p id="76e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">得到假设函数后，我找到假设和因变量之间的差异，然后在下一步中进行转置。</p><p id="f479" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后我找到梯度，其中99是观察次数。</p><p id="ff36" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下一步，我将我的渐变值乘以我的alpha值0.001，然后从我的初始θ猜测值中减去它。</p><p id="55da" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">整个过程重复几次，直到θ值不再进一步收敛。</p><p id="c207" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我得到的θ值是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/bfc8c4d4278d43b8fc34476c178d3170.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*XQjb5llyEveD8Tqg3IEP6Q.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">θ值</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/cc96d537ba2d51a80a51cf6d0d427630.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*jw7KntzBcmM2vc1_OgzGYQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">θ值接近0.20</figcaption></figure><h1 id="75d1" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">接下来呢？</h1><p id="97ff" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">我们已经找到了θ值。让我们测试一下我们的发现有多准确。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6cc6153ba4fc213174c938f14cf7ed3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*AvUISGY_--fiVChiKpgv-Q.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">散点图</figcaption></figure><p id="f033" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">看着散点图，我可以猜测一个考试1得80分，考试2得30分的学生不太可能被录取。<br/>让我们用新发现的系数来检验这些。<br/>将考试分数乘以系数并求和，我得到</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="8ddd" class="ob mg it nx b gy oc od l oe of">product = (0.2011*80) + (0.1975*30)-24.58</span></pre><p id="5198" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">把这个值代入我的Sigmoid方程。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="027f" class="ob mg it nx b gy oc od l oe of">1/ ( 1+ np.exp(-1*product))</span></pre><p id="a4c1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我得到的值是0.071292，小于阈值0.5。<br/>因此，该学生将无法获得入学许可。</p><h1 id="90d3" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">结论</h1><p id="a83f" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在本文中，我回顾了逻辑回归的基础知识，比如逻辑回归的假设。</p><p id="7101" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我还看了一个基本的例子，在这个例子中，梯度下降法被用来寻找系数的最优值，然后使用sigmoid函数来预测一个学生是否会被录取。</p><p id="7701" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这只是对逻辑回归的简单介绍。在接下来的几篇文章中，我将介绍与分类算法相关的其他主题，比如<strong class="lk jd">性能指标、超参数调优、算法选择、</strong>等等。</p><p id="2a9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[1]:安德鲁·吴..<em class="ok">机器学习-简介<br/></em><a class="ae lh" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/machine-learning</a></p></div></div>    
</body>
</html>