<html>
<head>
<title>Spoken Language Recognition Using Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用卷积神经网络的口语识别</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/spoken-language-recognition-using-convolutional-neural-networks-6aec5963eb18?source=collection_archive---------0-----------------------#2020-12-17">https://pub.towardsai.net/spoken-language-recognition-using-convolutional-neural-networks-6aec5963eb18?source=collection_archive---------0-----------------------#2020-12-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="a080" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="f8e0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">用张量流从语音音频信号中自动识别口语</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/0df9f6e7778cb048030ca9add039c650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-74a3VaurSpav7cl"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@blackodc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">苏三·李</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="578e" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">介绍</h1><h2 id="1fe1" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">应用程序</h2><p id="54d2" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">在弗劳恩霍夫IAIS公司，我们致力于各种语音技术，如自动语音识别、说话人识别等。在最近的工作中，我开发了一项新的服务，直接从音频流中预测口语。如果语言是已知的，可以自动选择用于语音识别步骤的合适模型。这方面的潜在应用可以是<a class="ae le" href="https://www.iais.fraunhofer.de/en/business-areas/speech-technologies/audio-mining-ard.html" rel="noopener ugc nofollow" target="_blank"> <em class="ne">自动转录软件</em> </a>和<a class="ae le" href="https://www.speaker.fraunhofer.de/en.html" rel="noopener ugc nofollow" target="_blank"> <em class="ne">对话式AI </em> </a>。</p><h2 id="8a59" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">开源代码</h2><p id="d80b" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">我准备了一个<a class="ae le" href="https://github.com/fraunhofer-iais/language-recognition" rel="noopener ugc nofollow" target="_blank"> <em class="ne"> GitHub知识库</em> </a>包括Jupyter笔记本，可以用来实验和训练自己的口语识别器。在当前状态下，它能够区分德语和英语。您可以将此视为实现自己的语言识别器的起点。我们主要用的是Python 3，TensorFlow 2，和librosa。</p><h2 id="2978" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">方法</h2><p id="5457" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">这个概念是基于Bartz等人的出版物。al [1]和Sarthak等人。al [2]，他们正在利用<em class="ne">卷积神经网络</em> (CNN)实现口语识别系统。其思想是用巨大的CNN分析包含语音信号的短音频段的声谱图，用于分类。我们使用的<em class="ne">是梅尔标度的声谱图</em>类似于prve【4】。</p><h2 id="c73c" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">结果</h2><p id="b44c" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">基于普通语音数据的my语言识别的开源变体产生了93.8 %的英语和德语分类准确率。在我们的研究所，我们使用自己的媒体数据集，并在152小时的增强测试数据上测试了相同的代码，获得了98.2 %的总体准确性。我们的算法可以区分英语、德语和“其他”(未知类别)。</p><h1 id="6b1c" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">资料组</h1><p id="e1f0" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">笔记本中使用的数据集基于Mozilla的常用语音<a class="ae le" href="https://commonvoice.mozilla.org" rel="noopener ugc nofollow" target="_blank"><em class="ne"/></a>。您需要下载英语和德语数据集，然后运行<a class="ae le" href="https://github.com/fraunhofer-iais/language-recognition/blob/master/src/0_prepare-dataset.ipynb" rel="noopener ugc nofollow" target="_blank">第一个笔记本</a>中的脚本，以提取包含持续时间在7.5到10秒之间的语音信号的训练和评估数据集。</p><h1 id="79ec" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">数据扩充</h1><p id="6c5e" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">可以通过添加噪声来扩充训练数据集。这将有助于提高最终模型对受噪声影响的记录的鲁棒性。对于正态(高斯)分布，我们使用Numpy函数random.normal，这会产生白噪声。</p><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="5a86" class="lx lg iq ng b gy nk nl l nm nn"><strong class="ng ja">def</strong> add_noise(audio_segment, gain):<br/>    num_samples = audio_segment.shape[0]<br/>    noise = gain * <a class="ae le" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html" rel="noopener ugc nofollow" target="_blank">numpy.random.normal</a>(size=num_samples)<br/>    <strong class="ng ja">return</strong> audio_segment + noise</span></pre><h1 id="3180" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">数据预处理</h1><p id="a8f2" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">对所有音频文件进行预处理以提取Mel缩放的声谱图。这是通过以下步骤完成的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e4c18e7c0d89f8a7a9e99e31eb7d113e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*w7NzBzruVYcEgqu1e1igFg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图1:从语音音频到声谱图的数据预处理</figcaption></figure><h2 id="ffaa" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">加载音频文件</h2><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="244c" class="lx lg iq ng b gy nk nl l nm nn"><strong class="ng ja">def</strong> load_audio_file(audio_file_path):<br/>    audio_segment, _ = <a class="ae le" href="https://librosa.org/doc/main/generated/librosa.load.html?highlight=load" rel="noopener ugc nofollow" target="_blank">librosa.load</a>(audio_file_path, sr=sample_rate)<br/>    <strong class="ng ja">return</strong> audio_segment</span></pre><p id="12c4" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">在此步骤中，音频被载入并下采样至8 kHz，以将带宽限制在4 kHz。这有助于使算法对较高频率的噪声具有鲁棒性。如[1]所述，英语中的大多数音素不超过3 kHz。</p><h2 id="d0aa" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">将持续时间固定为10秒</h2><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="c380" class="lx lg iq ng b gy nk nl l nm nn"><strong class="ng ja">def</strong> fix_audio_segment_to_10_seconds(audio_segment):<br/>    target_len = 10 * sample_rate<br/>    audio_segment = numpy.concatenate([audio_segment]*2, axis=0)<br/>    audio_segment = audio_segment[0:target_len]<br/>    <br/>    <strong class="ng ja">return</strong> audio_segment</span></pre><p id="b7db" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">我们复制7.5到10秒长的信号，并将其缩短到10秒。</p><h2 id="c45e" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">梅尔标度谱图</h2><p id="421d" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">Mel缩放的声谱图是根据音频计算的。它有助于用神经网络分析语音频率。Mel-scaling以比较高频率更高的分辨率表示较低频率，并考虑了人类感知频率的方式。</p><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="7af9" class="lx lg iq ng b gy nk nl l nm nn"><strong class="ng ja">def</strong> spectrogram(audio_segment):<br/>    <em class="ne"># Compute Mel-scaled spectrogram image</em><br/>    hl = audio_segment.shape[0] // image_width<br/>    spec = <a class="ae le" href="https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html" rel="noopener ugc nofollow" target="_blank">librosa.feature.melspectrogram</a>(audio_segment,<br/>                                     n_mels=image_height, <br/>                                     hop_length=int(hl))<br/><br/>    <em class="ne"># Logarithmic amplitudes</em><br/>    image = <a class="ae le" href="https://librosa.org/doc/main/generated/librosa.power_to_db.html?highlight=power_to_db#librosa.power_to_db" rel="noopener ugc nofollow" target="_blank">librosa.core.power_to_db</a>(spec)<br/><br/>    <em class="ne"># Convert to numpy matrix</em><br/>    image_np = numpy.asmatrix(image)<br/><br/>    <em class="ne"># Normalize and scale</em><br/>    image_np_scaled_temp = (image_np - numpy.min(image_np))<br/>    <br/>    image_np_scaled = image_np_scaled_temp / <br/>                      numpy.max(image_np_scaled_temp)<br/><br/>    <strong class="ng ja">return</strong> image_np_scaled[:, 0:image_width]</span></pre><p id="f63f" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">进行归一化以在0和1之间调整谱图中的值。此步骤将安静和大声录音均衡到一个共同的水平。</p><h2 id="5a5d" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">转换为PNG</h2><p id="8174" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">所有文件最终都存储为大小为500 x 128的PNG文件，但首先，归一化光谱图需要从0到1之间的十进制值转换为0到255之间的整数值。</p><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="ee06" class="lx lg iq ng b gy nk nl l nm nn"><strong class="ng ja">def</strong> to_integer(image_float):<br/>    <em class="ne"># range (0,1) -&gt; (0,255)</em><br/>    image_float_255 = image_float * 255.0<br/>    <br/>    <em class="ne"># Convert to uint8 in range [0:255]</em><br/>    image_int = image_float_255.astype(numpy.uint8)<br/>    <br/>    <strong class="ng ja">return</strong> image_int</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/70956478554717b17db29d0f5b663330.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*3cJ7-DsIEYZVgJcvvMfyZQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图2:作者的语音|图像的Mel比例谱图</figcaption></figure><h1 id="bd23" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">模特培训</h1><h2 id="7c5c" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">访问数据集</h2><p id="071b" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">为了使模型训练算法可以访问数据集，我们首先需要实例化生成器，这些生成器以内存高效的方式迭代数据集。这一点很重要，因为我们有60，000张图像用于训练和评估数据，我们不能只将其加载到内存中。幸运的是，作为TensorFlow的一部分，Keras具有处理图像数据集的惊人功能。我使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator" rel="noopener ugc nofollow" target="_blank"> ImageDataGenerator </a>作为模型训练的数据输入源。</p><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="2e07" class="lx lg iq ng b gy nk nl l nm nn">image_data_generator = ImageDataGenerator(<br/>                                  rescale=1./255,      <br/>                                  validation_split=validation_split)</span><span id="d6fe" class="lx lg iq ng b gy nv nl l nm nn">train_generator = image_data_generator.flow_from_directory(<br/>                        train_path,<br/>                        batch_size=batch_size,<br/>                        class_mode='categorical',<br/>                        target_size=(image_height, image_width), <br/>                        color_mode='grayscale',<br/>                        subset='training')</span><span id="b478" class="lx lg iq ng b gy nv nl l nm nn">validation_generator = image_data_generator.flow_from_directory(<br/>                            train_path,<br/>                            batch_size=batch_size,<br/>                            class_mode='categorical',<br/>                            target_size=(image_height, image_width), <br/>                            color_mode='grayscale',<br/>                            subset='validation')</span></pre><p id="ddc7" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">合适的批量大小是128，这足够大以帮助减少过拟合并保持模型训练的效率。</p><h2 id="133d" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">模型定义初始版本3</h2><p id="96f4" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">正如[1]中所建议的，建议使用庞大的CNN(如Inception V3)来解决这一任务。口语识别是一项非常复杂的任务，需要一个大容量的模型。较小的网络性能较差，因为它们无法处理复杂的数据。</p><p id="0223" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">最初的Inception V3模型 [3】，已经可以从Keras应用程序中获得，必须稍加修改以处理我们的数据集。它预计处理3个不同颜色层的图像，但我们的图像是灰度。以下代码行将单个图像颜色层复制到Inception V3的输入张量的所有三个通道。</p><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="1314" class="lx lg iq ng b gy nk nl l nm nn">img_input = <a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/Input" rel="noopener ugc nofollow" target="_blank">Input</a>(shape=(image_height, image_width, 1))<br/><br/>img_conc = <a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate" rel="noopener ugc nofollow" target="_blank">Concatenate</a>(axis=3, name='input_concat')([img_input, img_input, img_input])<br/><br/>model = <a class="ae le" href="https://keras.io/api/applications/inceptionv3/" rel="noopener ugc nofollow" target="_blank">InceptionV3</a>(input_tensor=img_conc, weights=<strong class="ng ja">None</strong>, include_top=<strong class="ng ja">True</strong>, classes=2)</span></pre><p id="f262" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">该模型有近22个mio参数。</p><h2 id="8efe" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">编译模型</h2><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="42eb" class="lx lg iq ng b gy nk nl l nm nn">model.compile(<br/>         optimizer=<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop" rel="noopener ugc nofollow" target="_blank">RMSprop</a>(lr=initial_learning_rate, clipvalue=2.0),       <br/>         loss='categorical_crossentropy',<br/>         metrics=['accuracy'])</span></pre><p id="00d0" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">在我的评估中，<a class="ae le" href="https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a" rel="noopener" target="_blank"> RMS-Prop优化器</a>产生了良好的结果。<a class="ae le" href="https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218" rel="noopener">亚当</a>是在【1】中暗示的。</p><h2 id="fe25" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">提前停止</h2><p id="6f06" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">学习结束时自动停止可以加快训练速度。我使用Keras的<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping" rel="noopener ugc nofollow" target="_blank"> EarlyStopping </a>方法来实现。</p><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="5156" class="lx lg iq ng b gy nk nl l nm nn">early_stopping = <a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping" rel="noopener ugc nofollow" target="_blank">EarlyStopping</a>(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=<strong class="ng ja">True</strong>)</span></pre><h2 id="ce01" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">学习率衰减</h2><p id="c61d" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">正如[1]中所建议的，我实现了一个指数学习率衰减<a class="ae le" href="https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1" rel="noopener" target="_blank">。</a></p><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="ac93" class="lx lg iq ng b gy nk nl l nm nn"><strong class="ng ja">def</strong> step_decay(epoch, lr):<br/>    drop = 0.94<br/>    epochs_drop = 2.0<br/>    lrate = lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))<br/>    <strong class="ng ja">return</strong> lrate<br/><br/>learning_rate_decay = <a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler" rel="noopener ugc nofollow" target="_blank">LearningRateScheduler</a>(step_decay, verbose=1)</span></pre><h2 id="7fb4" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">培养</h2><p id="57c1" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">现在，使用下面的命令进行实际的训练。</p><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="0a4a" class="lx lg iq ng b gy nk nl l nm nn">model.fit(train_generator,<br/>          validation_data=validation_generator,<br/>          epochs=60,<br/>          steps_per_epoch=steps_per_epoch,<br/>          validation_steps=validation_steps,<br/>          callbacks=[early_stopping, learning_rate_decay])</span></pre><p id="0e25" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">历元数非常多，但由于我使用了提前停止，幸运的是，算法在36个历元后停止。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/93f6c9ae5d0d4cad3d7580838b3d92e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*MhQUCW-VO2RrnQfrT_mKYQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">图3:训练和评估准确度|作者图片</figcaption></figure><p id="fd20" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">当查看36个时期的训练和评估精度时，我们可以看到存在过度拟合，这意味着该模型在分类训练数据集方面比评估数据集更准确。这是因为Inception V3模型是巨大的，并且具有巨大的容量。使用如此大的网络意味着多样化的训练数据量也应该非常大。因此，为了克服这个问题，您可以增加训练数据的数量，直到不再出现过度拟合。在图中，你还可以看到学习率衰减的影响。在前10个时期可以看到最陡的学习曲线。</p><h2 id="edb9" class="lx lg iq bd lh ly lz dn ll ma mb dp lp mc md me lr mf mg mh lt mi mj mk lv iw bi translated">估价</h2><p id="71a6" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">最后，我为测试数据实例化了一个新的ImageDataGenerator，并使用</p><pre class="kp kq kr ks gt nf ng nh ni aw nj bi"><span id="44d6" class="lx lg iq ng b gy nk nl l nm nn">_, test_accuracy = model.evaluate(evaluation_generator, <br/>                                  steps=evaluation_steps)</span></pre><p id="af02" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">来评估这个模型。测试结果的准确率为93.8 %。</p><h1 id="dd8a" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">调整和改进模型</h1><p id="c2db" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">请随意使用代码来改进模型。例如，您可以添加更多的类，并从其他语言中获取更多的数据。此外，建议根据您的应用调整数据和增强。我们弗劳恩霍夫有一个针对媒体数据的优化版本。我们正在使用两个额外的增强步骤来改善双向通话(overdub)和背景音乐的效果。您还可以<a class="ae le" href="https://medium.com/@makcedward/data-augmentation-for-audio-76912b01fdf6" rel="noopener">改变音调和速度来执行数据扩充</a>。此外，建议使用更多的数据。如果训练数据集的数量和多样性不足，可能会出现过度拟合。</p><h1 id="51b0" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">摘要</h1><p id="aaf0" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">我在GitHub上介绍了我的开源代码，以及直接从语音音频中识别口语的实现方法。在给定的基于普通语音的数据集上，模型的准确率为93.8 %。这个想法是使用高容量的CNN来分析10秒长的音频片段的Mel标度的频谱图。由于语言识别是一项复杂的任务，网络需要足够大，以便在训练期间捕捉复杂性并获得有意义的特征。我建议通过扩展数据集和增强技术来进一步改进模型。然后，在不改变训练实现本身的情况下，测试准确率超过98 %是可能的。</p><h1 id="f566" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参考</h1><p id="c7c6" class="pw-post-body-paragraph ml mm iq mn b mo mp ka mq mr ms kd mt mc mu mv mw mf mx my mz mi na nb nc nd ij bi translated">[1] C. Bartz，T. Herold，H. Yang和C. Meinel，<a class="ae le" href="https://www.springerprofessional.de/language-identification-using-deep-convolutional-recurrent-neura/15202392" rel="noopener ugc nofollow" target="_blank"/>(2015)，Proc .国际神经信息处理会议</p><p id="5405" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">[2] S. S. Sarthak，G. Mittal，<a class="ae le" href="https://deeplearn.org/arxiv/97126/spoken-language-identification-using-convnets" rel="noopener ugc nofollow" target="_blank">使用ConvNets进行口语识别</a> (2019)，Ambient Intelligence，vol. 11912，Springer Nature 2019，第252页</p><p id="4baf" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">[3] C. Szegedy，V. Vanhoucke，S. Ioffe，J. Shlens和Z. Wojna，<a class="ae le" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">重新思考计算机视觉的初始架构</a> (2015)，CoRR，abs/1512.00567</p><p id="0d30" class="pw-post-body-paragraph ml mm iq mn b mo np ka mq mr nq kd mt mc nr mv mw mf ns my mz mi nt nb nc nd ij bi translated">[4]p .-l . prve，<a class="ae le" href="https://github.com/pietz/language-recognition" rel="noopener ugc nofollow" target="_blank">口语识别</a> (2017)，GitHub知识库</p></div></div>    
</body>
</html>