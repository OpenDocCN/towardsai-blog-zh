<html>
<head>
<title>Breaking it Down: Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分解它:梯度下降</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/breaking-it-down-gradient-descent-b94c124f1dfd?source=collection_archive---------2-----------------------#2022-07-25">https://pub.towardsai.net/breaking-it-down-gradient-descent-b94c124f1dfd?source=collection_archive---------2-----------------------#2022-07-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1391" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用<a class="ae kf" href="https://github.com/JacobBumgarner/grad-descent-visualizer" rel="noopener ugc nofollow" target="_blank"> Grad-Descent-Visualizer </a>从头开始探索和可视化梯度下降的数学基础。</h2></div><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kl km l"/></div></figure><pre class="kg kh ki kj gt kn ko kp kq aw kr bi"><span id="c3a9" class="ks kt iq ko b gy ku kv l kw kx"><strong class="ko ir">Outline</strong><em class="ky"><br/></em>1. <a class="ae kf" href="#d9c4" rel="noopener ugc nofollow">What is Gradient Descent?</a><br/>2. <a class="ae kf" href="#e501" rel="noopener ugc nofollow">Breaking Down Gradient Descent</a><br/>  2.1 <a class="ae kf" href="#f7ef" rel="noopener ugc nofollow">Computing the Gradient</a><br/>  2.2 <a class="ae kf" href="#eec4" rel="noopener ugc nofollow">Descending the Gradient</a><br/>3. <a class="ae kf" href="#e4a7" rel="noopener ugc nofollow">Visualizing Multivariate Descents with Grad-Descent-Visualizer</a><br/>  3.1 <a class="ae kf" href="#bfec" rel="noopener ugc nofollow">Descent Montage</a><br/>4. <a class="ae kf" href="#991f" rel="noopener ugc nofollow">Conclusion: Contextualizing Gradient Descent<br/></a>5. <a class="ae kf" href="#2918" rel="noopener ugc nofollow">Resources</a></span></pre></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h2 id="d9c4" class="ks kt iq bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">1.什么是梯度下降？</h2><p id="3f0d" class="pw-post-body-paragraph ma mb iq mc b md me jr mf mg mh ju mi ln mj mk ml lr mm mn mo lv mp mq mr ms ij bi translated">梯度下降是一种优化算法，用于提高深度/机器学习模型的性能。在一系列重复的训练步骤中，梯度下降识别最小化成本函数输出的最佳参数值。</p><p id="ef41" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">在这篇文章的下两个部分，我们将从这个卫星视图描述中走出来，把梯度下降分解成更容易理解的东西。我们还将使用我的Python包<a class="ae kf" href="https://github.com/JacobBumgarner/grad-descent-visualizer" rel="noopener ugc nofollow" target="_blank"> grad-descent-visualizer </a>可视化各种测试函数的梯度下降。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kl km l"/></div></figure><h2 id="e501" class="ks kt iq bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">2.分解梯度下降</h2><p id="5818" class="pw-post-body-paragraph ma mb iq mc b md me jr mf mg mh ju mi ln mj mk ml lr mm mn mo lv mp mq mr ms ij bi translated">要对梯度下降有一个直观的认识，我们先忽略机器学习和深度学习。让我们从一个简单的函数开始:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/11b12959308d38171e575d8f6e279d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*v2_KNtzOml4VYs0bR1-6SA@2x.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">简单的一元函数</figcaption></figure><p id="b2e7" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">梯度下降的目标是找到函数的<em class="ky">最小值</em>或该函数的最低可能输出值。这意味着给定我们上面的函数<strong class="mc ir"> <em class="ky"> f(x) </em> </strong>，梯度下降的目标将是找到导致<strong class="mc ir"><em class="ky">【f(x)</em></strong>的输出接近<strong class="mc ir"> <em class="ky"> 0 </em> </strong>的<strong class="mc ir"> <em class="ky"> x </em> </strong>的值。通过形象化这个函数(下图)，可以很明显的看到<strong class="mc ir"> <em class="ky"> x = 0 </em> </strong>产生<strong class="mc ir"> <em class="ky"> f(x) </em> </strong>的最小值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nf km l"/></div></figure><p id="87ee" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">梯度下降的重要部分是:如果我们将<strong class="mc ir"> <em class="ky"> x </em> </strong>初始化为某个随机数，比如说<strong class="mc ir"> <em class="ky"> x = 1.8 </em> </strong>，是否有办法让<em class="ky">自动</em>更新<strong class="mc ir"> <em class="ky"> x </em> </strong>使其最终产生函数的最小输出？事实上，我们可以通过两步过程自动找到这个最小输出:</p><ol class=""><li id="84af" class="ng nh iq mc b md mt mg mu ln ni lr nj lv nk ms nl nm nn no bi translated">在我们的输入参数<strong class="mc ir"> <em class="ky"> x </em> </strong>所在的点找到函数的<em class="ky">斜率</em>。</li><li id="130d" class="ng nh iq mc b md np mg nq ln nr lr ns lv nt ms nl nm nn no bi translated">更新我们的输入参数<strong class="mc ir"> <em class="ky"> x </em> </strong>，使其<em class="ky">下降</em>梯度。</li></ol><p id="e40c" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">在我们的简单梯度下降算法中，这个两步过程被反复重复，直到我们的函数的输出稳定在最小值，或者达到定义的梯度容差水平。值得注意的是，其他更有效的下降算法采用不同的方法(例如，<a class="ae kf" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp" rel="noopener ugc nofollow" target="_blank"> RMSProp </a>、<a class="ae kf" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad" rel="noopener ugc nofollow" target="_blank"> AdaGrad </a>、<a class="ae kf" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam" rel="noopener ugc nofollow" target="_blank"> Adam </a>)。</p><h2 id="f7ef" class="ks kt iq bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">2.1.计算梯度</h2><p id="a04c" class="pw-post-body-paragraph ma mb iq mc b md me jr mf mg mh ju mi ln mj mk ml lr mm mn mo lv mp mq mr ms ij bi translated">为了找到函数<strong class="mc ir"><em class="ky">【f(x)</em></strong>在<strong class="mc ir"><em class="ky"/></strong>任意值处的斜率(或<em class="ky">梯度，</em>因此梯度下降)，我们可以对函数进行微分。微分简单例子函数简单用幂法则(下图)，为我们提供:<strong class="mc ir"><em class="ky">f’(x)= 2x</em></strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/563cb64c79370e16fca29303cff92401.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*VZ6zfiMt9PEVGtVWrjuysQ@2x.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">权力规则</figcaption></figure><p id="74e4" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">利用我们的起点<strong class="mc ir"> <em class="ky"> x = 1.8 </em> </strong>，我们发现我们的起点梯度<strong class="mc ir"><em class="ky">x</em></strong>(<strong class="mc ir"><em class="ky">dx</em></strong>)为<strong class="mc ir"> <em class="ky"> dx = 3.6 </em> </strong>。</p><p id="d9e8" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">让我们用python写一个简单的函数，自动计算任意输入变量对<strong class="mc ir"> <em class="ky"> f(x) = x </em> </strong>的导数。</p><blockquote class="nv nw nx"><p id="88ec" class="ma mb ky mc b md mt jr mf mg mu ju mi ny mv mk ml nz mw mn mo oa mx mq mr ms ij bi translated">*我强烈建议查看<a class="ae kf" href="https://www.youtube.com/watch?v=9vKqVkMQHKk&amp;list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&amp;index=2&amp;t=2s" rel="noopener ugc nofollow" target="_blank"> 3Blue1Brown的视频</a>，直观地了解差异化。这个样本函数与第一原理的区别可以在<a class="ae kf" href="https://socratic.org/questions/how-you-you-find-the-derivative-f-x-x-2-using-first-principles" rel="noopener ugc nofollow" target="_blank">这里</a>看到。</p></blockquote><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob km l"/></div></figure><pre class="kg kh ki kj gt kn ko kp kq aw kr bi"><span id="024f" class="ks kt iq ko b gy ku kv l kw kx">Gradient at x = 1.8: dx = 3.6</span></pre><h2 id="eec4" class="ks kt iq bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">2.2.下降梯度</h2><p id="181e" class="pw-post-body-paragraph ma mb iq mc b md me jr mf mg mh ju mi ln mj mk ml lr mm mn mo lv mp mq mr ms ij bi translated">一旦我们找到了起点的梯度，我们想要更新我们的输入参数，这样它就使<em class="ky">下降</em>这个梯度。这样做将最小化函数的输出。</p><p id="4a69" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">要向下移动变量的梯度，我们可以简单地从输入参数中减去梯度。然而，如果你仔细观察，你可能会注意到从输入参数<strong class="mc ir"> <em class="ky"> x=1.8 </em> </strong>中减去整个渐变会导致它在<strong class="mc ir"> <em class="ky"> 1.8 </em> </strong>和<strong class="mc ir"> <em class="ky"> -1.8 </em> </strong>之间无限地来回跳动，阻止它接近<strong class="mc ir"> <em class="ky"> 0 </em> </strong>。</p><p id="c37d" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">相反，我们可以定义一个<strong class="mc ir"> <em class="ky">学习率= 0.1个</em> </strong>。在从<strong class="mc ir"> <em class="ky"> x </em> </strong>中减去之前，我们将用这个学习率来缩放<strong class="mc ir"> <em class="ky"> dx </em> </strong>。通过调整学习率，我们可以创造“更平滑”的下降。大的学习率沿着函数产生大的跳跃，小的学习率沿着函数产生小的步长。</p><p id="0c08" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">最后，我们最终将不得不停止梯度下降。否则，当梯度接近0时，算法将无休止地继续。对于这个例子，一旦<strong class="mc ir"> <em class="ky"> dx </em> </strong>小于<strong class="mc ir"> <em class="ky"> 0.01 </em> </strong>，我们就简单地停止下降。在您自己的IDE中，您可以更改<code class="fe oc od oe ko b">learning_rate</code>和<code class="fe oc od oe ko b">tolerance</code>参数，以查看迭代和<strong class="mc ir"> <em class="ky"> x </em> </strong>的输出如何变化。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob km l"/></div></figure><pre class="kg kh ki kj gt kn ko kp kq aw kr bi"><span id="3fc0" class="ks kt iq ko b gy ku kv l kw kx">Function minimum found in 27 iterations. X = 0.00</span></pre><p id="b57f" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">从上面的视频中可以看出，我们的起始值<strong class="mc ir"> <em class="ky"> x = 1.8 </em> </strong>能够通过梯度下降的迭代过程自动更新为<strong class="mc ir"> <em class="ky"> x = 0.0 </em> </strong>。</p><h1 id="e4a7" class="of kt iq bd lg og oh oi lj oj ok ol lm jw om jx lq jz on ka lu kc oo kd ly op bi translated">3.使用Grad-Descent-Visualizer可视化多元下降</h1><p id="70c4" class="pw-post-body-paragraph ma mb iq mc b md me jr mf mg mh ju mi ln mj mk ml lr mm mn mo lv mp mq mr ms ij bi translated">希望这个单变量的例子为梯度下降实际上做什么提供了一些基础的见解。现在让我们扩展到多元函数的上下文。</p><p id="2d00" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">我们将首先想象Himmelblau函数的梯度下降。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="or os di ot bf ou"><div class="gh gi oq"><img src="../Images/349deddb6d764e5efac1f3fbd16dbe54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dBriX5IO0vhJvzvgUzN7eA@2x.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">希梅尔布劳函数</figcaption></figure><p id="4571" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">多元函数的下降有几个关键的区别。</p><p id="4ca4" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">首先，我们需要计算<em class="ky">偏导数</em>来更新每个参数。在Himmelblau的函数中，<strong class="mc ir"> <em class="ky"> x </em> </strong>的梯度依赖于<strong class="mc ir"> <em class="ky"> y </em> </strong>(它们的和是平方的，需要<a class="ae kf" href="https://g.co/kgs/8bwVeF" rel="noopener ugc nofollow" target="_blank">链式法则</a>)。这意味着用于区分<strong class="mc ir"> <em class="ky"> x </em> </strong>的公式将包含<strong class="mc ir"> <em class="ky"> y </em> </strong>，反之亦然。</p><p id="e8e6" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">第二，你可能已经注意到在第二节的简单函数中只有一个最小值。在现实中，在我们的成本函数中可能有许多未知的局部极小值。这意味着我们的参数找到的局部最小值将取决于它们的起始位置和梯度下降算法的行为。</p><p id="429d" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">为了可视化这种景观的下降，我们将初始化我们的起始参数为<strong class="mc ir"> <em class="ky"> x = -0.4 </em> </strong>和<strong class="mc ir"> <em class="ky"> y = -0.65 </em> </strong>。然后，我们可以观察每个参数在其自身维度上的下降，以及被相反参数的位置分割的2D下降。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nf km l"/></div></figure><p id="efd3" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">对于更大的背景，让我们使用我的<a class="ae kf" href="https://github.com/JacobBumgarner/grad-descent-visualizer" rel="noopener ugc nofollow" target="_blank"> grad-descent-visualizer </a>包在<a class="ae kf" href="https://github.com/pyvista/pyvista" rel="noopener ugc nofollow" target="_blank"> PyVista </a>的帮助下创建的3D可视化同一点的下降。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kl km l"/></div></figure><h2 id="bfec" class="ks kt iq bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">3.1下降蒙太奇</h2><p id="2b56" class="pw-post-body-paragraph ma mb iq mc b md me jr mf mg mh ju mi ln mj mk ml lr mm mn mo lv mp mq mr ms ij bi translated">现在让我们想象更多的<a class="ae kf" href="https://www.sfu.ca/~ssurjano/optimization.html" rel="noopener ugc nofollow" target="_blank">测试函数</a>的下降！我们将在每个函数上放置一个网格点，观察这些点在下降时是如何移动的。</p><p id="6694" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated"><a class="ae kf" href="https://www.sfu.ca/~ssurjano/spheref.html" rel="noopener ugc nofollow" target="_blank">球体功能</a>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kl km l"/></div></figure><p id="3353" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated"><a class="ae kf" href="https://www.sfu.ca/~ssurjano/griewank.html" rel="noopener ugc nofollow" target="_blank">格里万克函数</a>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kl km l"/></div></figure><p id="3b54" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated"><a class="ae kf" href="https://www.sfu.ca/~ssurjano/camel6.html" rel="noopener ugc nofollow" target="_blank">六峰骆驼功能</a>。注意函数的许多局部极小值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kl km l"/></div></figure><p id="8e90" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">让我们重新想象一下<a class="ae kf" href="https://en.wikipedia.org/wiki/Himmelblau%27s_function" rel="noopener ugc nofollow" target="_blank"> Himmelblau函数</a>的网格下降。注意不同的参数初始化如何导致不同的最小值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kl km l"/></div></figure><p id="1991" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">最后是<a class="ae kf" href="https://www.sfu.ca/~ssurjano/easom.html" rel="noopener ugc nofollow" target="_blank"> Easom功能</a>。注意有多少点是静止的，因为它们是在平坦的梯度上初始化的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kl km l"/></div></figure><h2 id="991f" class="ks kt iq bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">4.结论:语境化梯度下降</h2><p id="f9de" class="pw-post-body-paragraph ma mb iq mc b md me jr mf mg mh ju mi ln mj mk ml lr mm mn mo lv mp mq mr ms ij bi translated">到目前为止，我们已经完成了一元函数的梯度下降，并可视化了几个多元函数的下降。事实上，现代深度学习模型的参数比我们研究的函数多得多。</p><p id="1927" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">比如拥抱脸最新的自然语言处理模型Bloom，拥有<em class="ky">1750亿</em>个参数。这个模型中使用的链式函数也比我们的测试函数更复杂。</p><p id="6616" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">然而，重要的是要认识到我们所学的梯度下降的基础仍然适用。在任何深度学习模型的训练的每次迭代期间，计算每个参数的梯度。然后，该梯度将在训练示例中进行平均，然后从参数中减去，以便它们“逐步降低”其梯度，推动它们从模型的成本函数中产生最小输出。</p><p id="e820" class="pw-post-body-paragraph ma mb iq mc b md mt jr mf mg mu ju mi ln mv mk ml lr mw mn mo lv mx mq mr ms ij bi translated">感谢阅读！</p><h2 id="2918" class="ks kt iq bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">5.资源</h2><pre class="kg kh ki kj gt kn ko kp kq aw kr bi"><span id="0eef" class="ks kt iq ko b gy ku kv l kw kx">- <a class="ae kf" href="https://github.com/JacobBumgarner/grad-descent-visualizer" rel="noopener ugc nofollow" target="_blank">Grad-Descent-Visualizer</a><br/>- <a class="ae kf" href="https://www.youtube.com/c/3blue1brown" rel="noopener ugc nofollow" target="_blank">3Blue1Brown</a><br/>  - <a class="ae kf" href="https://www.youtube.com/watch?v=IHZwWFHWa-w" rel="noopener ugc nofollow" target="_blank">Gradient Descent</a><br/>  - <a class="ae kf" href="https://www.youtube.com/watch?v=9vKqVkMQHKk&amp;t=10s" rel="noopener ugc nofollow" target="_blank">Derivatives</a><br/>- <a class="ae kf" href="https://www.sfu.ca/~ssurjano/optimization.html" rel="noopener ugc nofollow" target="_blank">Simon Fraser University: Test Functions for Optimization</a><br/>- <a class="ae kf" href="https://docs.pyvista.org" rel="noopener ugc nofollow" target="_blank">PyVista</a><br/>- <a class="ae kf" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">Michael Nielsen's Neural Networks and Deep Learning</a></span></pre></div></div>    
</body>
</html>