<html>
<head>
<title>Outline a Smaller Class With the Custom Loss Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用自定义损失函数概述一个较小的类</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/outline-a-smaller-class-with-the-custom-loss-function-94ff00359698?source=collection_archive---------2-----------------------#2022-10-13">https://pub.towardsai.net/outline-a-smaller-class-with-the-custom-loss-function-94ff00359698?source=collection_archive---------2-----------------------#2022-10-13</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="1d26" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">处理不平衡数据集时充分利用分类的简短指南</h2></div><div class="kj kk kl km gu ab cb"><figure class="kn ko kp kq kr ks kt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/b61a81ddf4f39b5e73ef93a8d269363a.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*ir-xJiOx7pcwg02tOXvZwg.png"/></div></figure><figure class="kn ko la kq kr ks kt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/10316f57bd1d8b6692bbe3aa03093d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*Mh1h97Y9FSiEapp0Xgb6JQ.jpeg"/></div></figure><figure class="kn ko lb kq kr ks kt paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/b80cb72ebd20f73f2bebf7bc3a91d683.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*xckIYT3Q1jkqJ9A7GtOIRg.jpeg"/></div><figcaption class="lc ld gk gi gj le lf bd b be z dk lg di lh li translated">(<strong class="bd lj">左</strong>)照片由<a class="ae lk" href="https://www.flickr.com/photos/157270154@N05/" rel="noopener ugc nofollow" target="_blank">麦克·劳伦斯</a>在<a class="ae lk" href="https://www.flickr.com" rel="noopener ugc nofollow" target="_blank"> Flickr </a> | ( <strong class="bd lj">中</strong>)照片由<a class="ae lk" href="https://www.flickr.com/people/91261194@N06/" rel="noopener ugc nofollow" target="_blank">杰奈杰·弗曼</a>在<a class="ae lk" href="https://www.flickr.com" rel="noopener ugc nofollow" target="_blank"> Flickr </a> | ( <strong class="bd lj">右</strong>)照片由<a class="ae lk" href="https://www.flickr.com/people/mythoto/" rel="noopener ugc nofollow" target="_blank">伦纳德·J·马修斯</a>在<a class="ae lk" href="https://www.flickr.com" rel="noopener ugc nofollow" target="_blank"> Flickr </a></figcaption></figure></div><h2 id="4c61" class="ll lm iu bd lj ln lo dn lp lq lr dp ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">为什么要使用自定义损失函数？</h2><p id="a611" class="pw-post-body-paragraph mg mh iu mi b mj mk jv ml mm mn jy mo lt mp mq mr lx ms mt mu mb mv mw mx my in bi translated">可能会出现准确性度量不足以获得预期结果的情况。我们可能希望降低假阴性(FN)或假阳性(FP)率。当数据集不平衡并且我们寻求的结果属于较小的类时，这可能是有益的。</p><p id="768e" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated"><strong class="mi iv">哪里适用</strong></p><p id="6ef9" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">1.欺诈检测。我们希望减少假阴性样本(例如，当交易可疑时，模型会照常处理交易)。目标是最小化不被注意的欺诈交易的风险(即使假阳性率会增加)。</p><p id="32d6" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">2.<strong class="mi iv">疾病识别</strong>。逻辑是一样的。我们希望增加感染这种疾病的机会，即使我们将来会有更多的FP病例需要处理。</p><p id="647c" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">3.<strong class="mi iv">在体育比赛中挑一个局外人</strong>。让我们假设一个有两种结果的游戏(赢/输一个最喜欢的)。但是我们想知道一个局外人是否有机会。一个平衡的模型将会在大多数时候执行“樱桃采摘”,因为这是一个预测最喜欢的获胜的简单方法。解决的办法是惩罚假阴性(当一个模型预测一个更强的团队的胜利，但准确的标签是一个最喜欢的损失)。</p><p id="3af0" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">我们不希望在前两种情况下出现高FP率，因为这将增加资源使用量，以确认样本不是欺诈/疾病。因此，模型优化仍然是必要的。</p><h2 id="e75c" class="ll lm iu bd lj ln lo dn lp lq lr dp ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">原始日志和定制日志之间的桥梁损耗</strong></h2><p id="ba20" class="pw-post-body-paragraph mg mh iu mi b mj mk jv ml mm mn jy mo lt mp mq mr lx ms mt mu mb mv mw mx my in bi translated">普通的梯度增强算法(该算法)进行以下步骤。它获得预测值(让我们称之为“predt”，根据我们使用的库，以logit或logitraw的形式)和相应的真实标签(让我们称之为“y”)，然后它估计logloss函数的值。它逐行执行这些操作来计算累积损失函数值。</p><p id="df48" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">logloss具有以下公式:</p><figure class="kj kk kl km gu ko gi gj paragraph-image"><div class="gi gj ne"><img src="../Images/011c161c6fc1fb5803391fdc3dd6b615.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*H7pefgUz1fZ64blbFUvmJw.png"/></div></figure><p id="1452" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">其中<code class="fe nf ng nh ni b">N</code> —样本数，<code class="fe nf ng nh ni b">y</code> —真标签，log —自然对数。</p><p id="24e8" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated"><code class="fe nf ng nh ni b">p_i</code> —第I个样本为<code class="fe nf ng nh ni b">1</code>的概率(sigmoid函数):</p><figure class="kj kk kl km gu ko gi gj paragraph-image"><div class="gi gj nj"><img src="../Images/fb92d09308f52facd2247e18979f3ff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/format:webp/1*sR8Ym_g7xSWn4fEMbstv8g.png"/></div></figure><figure class="kj kk kl km gu ko gi gj paragraph-image"><div class="gi gj nk"><img src="../Images/43bdd79e566d72d00aea8ed6a1b4ae16.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*Od0cENPU3a0RWsq091ectw.png"/></div></figure><p id="da31" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">其中m是特征的数量，<code class="fe nf ng nh ni b">z</code>是线性回归，其中<code class="fe nf ng nh ni b">w</code>是特征的权重，<code class="fe nf ng nh ni b">x</code>是特征值。</p><p id="41e5" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">下一步是更新要素的权重以最小化损失。该算法计算梯度和hessian来选择改变权重的方向和功率。</p><p id="173b" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">梯度的公式为:</p><figure class="kj kk kl km gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gi gj nl"><img src="../Images/1eb35814108dc24c7a4021af0ac659b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*TlcNmYIGSMiYXHnqZjYd2A.png"/></div></div></figure><p id="7f43" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">黑森的公式:</p><figure class="kj kk kl km gu ko gi gj paragraph-image"><div class="gi gj nm"><img src="../Images/2ee604ff91f69d185c0a6ff9afe940bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*PvBJlVNQy1uROR9oAZfGYw.png"/></div></figure><blockquote class="nn no np"><p id="b474" class="mg mh nq mi b mj mz jv ml mm na jy mo nr nb mq mr ns nc mt mu nt nd mw mx my in bi translated">我们<!-- -->通过将链式法则应用于对数损失函数<code class="fe nf ng nh ni b">l</code>来计算关于自变量<code class="fe nf ng nh ni b">z</code>变化的导数。</p></blockquote><p id="3167" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">我们可以把这个过程想象成一场高尔夫球赛。一个高尔夫球手(一种算法)试图把球打入洞内(一个正确的预测，使损失最小化)；要做到这一点，他可以选择击球的方向和力量(梯度)，并采取各种球杆来控制击球(黑森)。</p><figure class="kj kk kl km gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gi gj nu"><img src="../Images/1cfe3ce7c8482ad645b39f4f9cf330ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*scAMyQYKvHNYkKy-Jc7l2A.jpeg"/></div></div><figcaption class="lc ld gk gi gj le lf bd b be z dk translated">照片由<a class="ae lk" href="https://www.flickr.com/photos/deapeajay/" rel="noopener ugc nofollow" target="_blank">大卫·乔伊斯</a>在<a class="ae lk" href="https://www.flickr.com" rel="noopener ugc nofollow" target="_blank"> Flickr </a>上拍摄</figcaption></figure><p id="ea93" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">算法的主要目标是使累积logloss尽可能的小(一个奇怪的高尔夫球手，他追逐的是进洞球的数量，而不是用最少的击球数得出结果)。如果我们有一个不平衡的数据集，并且特征没有提供明确的分类，那么它倾向于预测一个样本属于一个更大的组(某种类型的挑选)。</p><p id="a083" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">让我们想象一场改变了规则的高尔夫球赛；大多数孔都有一个标准值，但也有比普通孔更有意义的额外值。高尔夫球手事先不了解目标的价格。如果一个玩家多打了一洞，他会得到更高的奖励。如果他错误地将球洞识别为标准球洞，并拒绝接近球洞，他会损失时间并得到较少的补偿。</p><p id="b7e0" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">基本算法无法意识到规则的变化，所以它继续进行更简单的规则，并且无法识别和评分额外的规则。让我们引入“beta”(一个没有上限的正数)来扭转局面。它的目标是，如果算法做出不正确的预测，就会大幅增加累积损失。基本算法也有“beta”，等于1。</p><p id="17ea" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">β-log loss有以下公式:</p><figure class="kj kk kl km gu ko gi gj paragraph-image"><div class="gi gj nv"><img src="../Images/1da168bc7f9b8b50c24fb1c99a06d006.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*09cwoo7Eh80z8Nvge8Pnsg.png"/></div></figure><p id="0323" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">梯度和hessian公式:</p><figure class="kj kk kl km gu ko gi gj paragraph-image"><div class="gi gj nw"><img src="../Images/b6aae7cdac464d23990625d4c9c6e761.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*2YE5AGAYw7n3VVkLUmEXIw.png"/></div></figure><figure class="kj kk kl km gu ko gi gj paragraph-image"><div class="gi gj nx"><img src="../Images/7cf6f6877139aa961825c974f00c5d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*WZXxMuou8JMRB4MSBdFiPw.png"/></div></figure><p id="aad8" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">自定义损失是通过将算法“beta”更改为小于1以惩罚FN或大于1以使FP花费更多来实现的。我们应该用新的“beta”重新计算梯度和hessian，迫使algo改变损失计算策略。</p></div><div class="ab cl ny nz hy oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="in io ip iq ir"><h2 id="1364" class="ll lm iu bd lj ln lo dn lp lq lr dp ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">咱们码自定义损耗</strong></h2><p id="48a7" class="pw-post-body-paragraph mg mh iu mi b mj mk jv ml mm mn jy mo lt mp mq mr lx ms mt mu mb mv mw mx my in bi translated">我用Python实现了scikit-learn API for XGboost中的代码(版本1.0.2)。我们可以在梯度提升包(XGBoost，LightGBM，Catboost)或深度学习包(如TensorFlow)中使用自定义损失函数。</p><p id="4f89" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">有四个用户定义的函数来实现自定义损失函数。第一步是计算修正的logloss函数的一阶导数(梯度)，第二步是计算二阶导数，第三步是获得目标函数，第四步是计算修正的评估度量。</p><p id="a411" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">此后，<code class="fe nf ng nh ni b">y</code>参数包含一个实际类别值，<code class="fe nf ng nh ni b">predt</code>获取要评估的预测值。</p><figure class="kj kk kl km gu ko"><div class="bz fq l di"><div class="of og l"/></div></figure><p id="798b" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated"><code class="fe nf ng nh ni b">logreg_err_beta_sklearn</code>函数根据数据集计算logloss以进行评估。它根据看不见的数据检查性能。即使我们应用sklearn API，这个自定义函数接收的是<code class="fe nf ng nh ni b">dmat</code>参数，是DMatrix数据类型(XGBoost的内部数据结构)。</p></div><div class="ab cl ny nz hy oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="in io ip iq ir"><h2 id="5fd9" class="ll lm iu bd lj ln lo dn lp lq lr dp ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">基于泰坦尼克号数据集的示例</strong></h2><p id="d2fb" class="pw-post-body-paragraph mg mh iu mi b mj mk jv ml mm mn jy mo lt mp mq mr lx ms mt mu mb mv mw mx my in bi translated">泰坦尼克号数据集是一个不平衡数据集的例子，存活率只有38%。</p><p id="3334" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">让我们跳过建模前的预处理步骤，直接进入分类任务。您可以在本文末尾引用的我的GitHub资源库中找到一个整体。</p><figure class="kj kk kl km gu ko"><div class="bz fq l di"><div class="of og l"/></div></figure><p id="1a01" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">基本算法的结果是下面的混淆矩阵:</p><figure class="kj kk kl km gu ko gi gj paragraph-image"><div class="gi gj oh"><img src="../Images/cf005a3df884979f38ffa1fc4d58ea56.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*WmzWzFVQCf2eQz0YyaGYBw.png"/></div><figcaption class="lc ld gk gi gj le lf bd b be z dk translated">作者图片</figcaption></figure><p id="6cb9" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">基本算法给出了38个FN案例。假设我们的目标是减少FN的数量。让我们将目标函数更改为自定义函数，相应地修改评估指标，并将“beta”设置为. 4。</p><figure class="kj kk kl km gu ko"><div class="bz fq l di"><div class="of og l"/></div></figure><p id="5425" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">对基本算法的更改包括禁用默认评估指标、明确引入自定义logloss函数以及相应的评估指标。</p><p id="619d" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">贝塔对数损失算法的混淆矩阵；</p><figure class="kj kk kl km gu ko gi gj paragraph-image"><div class="gi gj oh"><img src="../Images/86ccc2164539b9ce7df331998fa6f4f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*rEyEwKfflMUVJPhyoXhgWA.png"/></div><figcaption class="lc ld gk gi gj le lf bd b be z dk translated">作者图片</figcaption></figure><p id="f61d" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">beta算法将FN病例减少到25 (-34%)，真阳性(TP)增加到112 (+13%)，FP增加到53 (+103%)，真阴性(TN)减少到167 (-14%)。FN下降导致TP和FP都上升。整体准确率下降。</p><p id="7be3" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">我们可以很容易地应用同样的逻辑，通过选择大于1的“beta”来惩罚FP。</p></div><div class="ab cl ny nz hy oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="in io ip iq ir"><h2 id="80b9" class="ll lm iu bd lj ln lo dn lp lq lr dp ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">结论</strong></h2><p id="26b0" class="pw-post-body-paragraph mg mh iu mi b mj mk jv ml mm mn jy mo lt mp mq mr lx ms mt mu mb mv mw mx my in bi translated">贝塔对数损失有一些独特的优点，也有缺点。我已经列出了最明显的几个。</p><p id="d129" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated"><strong class="mi iv">优点</strong></p><ul class=""><li id="ffe5" class="oi oj iu mi b mj mz mm na lt ok lx ol mb om my on oo op oq bi translated">易于快速应用(使用四个用户定义的函数和测试版，仅此而已)。</li><li id="d0c9" class="oi oj iu mi b mj or mm os lt ot lx ou mb ov my on oo op oq bi translated">在建模之前，不需要对底层数据进行操作(如果数据集不是高度不平衡的话)</li><li id="9455" class="oi oj iu mi b mj or mm os lt ot lx ou mb ov my on oo op oq bi translated">它可以作为数据探索的一部分或模型叠加的一部分来应用。</li><li id="50b9" class="oi oj iu mi b mj or mm os lt ot lx ou mb ov my on oo op oq bi translated">我们可以将它添加到最流行的机器学习包中。</li></ul><p id="d3b5" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated"><strong class="mi iv">快捷键</strong></p><ul class=""><li id="e6d5" class="oi oj iu mi b mj mz mm na lt ok lx ol mb om my on oo op oq bi translated">我们应该调整“β”,以获得最佳的FN与FP的平衡。</li><li id="4e98" class="oi oj iu mi b mj or mm os lt ot lx ou mb ov my on oo op oq bi translated">当数据集高度不平衡时(次要类别少于所有样本的10%的数据集)，它可能不会提供有意义的结果。探索性数据分析对于模型的运行至关重要。</li><li id="bd24" class="oi oj iu mi b mj or mm os lt ot lx ou mb ov my on oo op oq bi translated">如果我们惩罚FN，通常会导致FP大幅增长，反之亦然。您可能需要额外的资源来弥补这种增长。</li></ul><p id="a150" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated"><strong class="mi iv">参考文献</strong></p><ol class=""><li id="c28f" class="oi oj iu mi b mj mz mm na lt ok lx ol mb om my ow oo op oq bi translated">二元交叉熵(logloss)概述:<a class="ae lk" href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" rel="noopener" target="_blank">https://towards data science . com/understanding-binary-cross-entropy-log-loss-a-visual-explain-a3ac 6025181</a></li><li id="f3e0" class="oi oj iu mi b mj or mm os lt ot lx ou mb ov my ow oo op oq bi translated">回归任务自定义损失函数概述:<a class="ae lk" href="https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d" rel="noopener" target="_blank">https://towards data science . com/custom-loss-functions-for-gradient-boosting-f 79 C1 b 40466d</a></li><li id="37bd" class="oi oj iu mi b mj or mm os lt ot lx ou mb ov my ow oo op oq bi translated">自定义损耗函数相关的XGBoost文档:<a class="ae lk" href="https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html" rel="noopener ugc nofollow" target="_blank">https://XGBoost . readthedocs . io/en/latest/tutorials/custom _ metric _ obj . html</a></li><li id="54fc" class="oi oj iu mi b mj or mm os lt ot lx ou mb ov my ow oo op oq bi translated"><code class="fe nf ng nh ni b">Plot_confusion_matrix </code>功能来源:<a class="ae lk" href="https://www.kaggle.com/grfiv4/plot-a-confusion-matrix" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/grfiv4/plot-a-confusion-matrix</a></li><li id="1e46" class="oi oj iu mi b mj or mm os lt ot lx ou mb ov my ow oo op oq bi translated">链接到我的GitHub资源库:<a class="ae lk" href="https://github.com/kpluzhnikov/binary_classification_custom_loss" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/kpluzhnikov/binary _ class ification _ custom _ loss</a></li></ol></div><div class="ab cl ny nz hy oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="in io ip iq ir"><p id="5ab8" class="pw-post-body-paragraph mg mh iu mi b mj mz jv ml mm na jy mo lt nb mq mr lx nc mt mu mb nd mw mx my in bi translated">如果你喜欢这篇文章，请毫不犹豫地喜欢、评论并分享它。或者甚至:</p><div class="ox oy gq gs oz pa"><a href="https://medium.com/@kplz/membership" rel="noopener follow" target="_blank"><div class="pb ab fp"><div class="pc ab pd cl cj pe"><h2 class="bd iv gz z fq pf fs ft pg fv fx it bi translated">通过我的推荐链接-康斯坦丁·普鲁申尼科夫加入媒体</h2><div class="ph l"><h3 class="bd b gz z fq pf fs ft pg fv fx dk translated">阅读康斯坦丁·普鲁申尼科夫的每一个故事(以及媒体上成千上万的其他作家)。您的会员费直接…</h3></div><div class="pi l"><p class="bd b dl z fq pf fs ft pg fv fx dk translated">medium.com</p></div></div><div class="pj l"><div class="pk l pl pm pn pj po ky pa"/></div></div></a></div></div></div>    
</body>
</html>