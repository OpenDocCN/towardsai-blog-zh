<html>
<head>
<title>Lasso (l1) and Ridge (l2) Regularization Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">套索(l1)和脊(l2)正则化技术</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/lasso-l1-and-ridge-l2-regularization-techniques-33b7f12ac0b?source=collection_archive---------2-----------------------#2021-06-07">https://pub.towardsai.net/lasso-l1-and-ridge-l2-regularization-techniques-33b7f12ac0b?source=collection_archive---------2-----------------------#2021-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8998" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><div class=""><h2 id="3398" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用于减少过度拟合的技术</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/6829c958c4f255ba5641685f3ed86e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*zuyjF8wKPxCC7QSoidpwxA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><h2 id="3cfc" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated"><strong class="ak">山脊和套索回归简介</strong></h2><p id="c258" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">脊和套索回归的必要性是什么？</p><p id="d9ce" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">当我们用最佳拟合线创建线性模型并进入测试阶段时，由于变化增加，我们的模型过度拟合，因此它在未来不会很好地工作，也不会提供适当的精度。因此，为了减少过度拟合，脊和套索回归就出现了。两者都是强大的技术，但略有不同，用于创建这样的模型，这些模型是有效的并且在计算上适合以减少过拟合。</p><h2 id="619d" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated"><strong class="ak">正规化</strong></h2><p id="cae4" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">这是一个对类别进行分类并提供附加信息以防止过度拟合的过程。线性回归是一种众所周知的标准回归方法，它假设输入变量和目标变量之间存在线性关系。它是线性回归的附加部分，包括在训练期间向损失函数添加惩罚。因此简称为<strong class="me jd"> <em class="na">正则化线性回归</em> </strong>。</p><p id="23ef" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">换句话说，它是一种用于减少过度拟合的方法或技术，以便我们可以适当地进行模型预测。</p><p id="437c" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">正则化给出了两种技术L1(套索回归)和L2(岭回归)。</p><h2 id="be1d" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated"><strong class="ak">岭回归:</strong></h2><p id="d3cc" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">线性回归的正则化版本称为岭回归。它用于拟合数据，并将权重保持在较小的范围内，以便训练过程轻松进行。</p><p id="df26" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">它执行L2正则化，这意味着它添加等价于系数大小的平方的罚值，该罚值可以由下面给出的公式给出:</p><p id="6abc" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">岭回归方程由下式给出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/227190c8cf65c9ee18f044cc4569dc1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*5lZk1BOoavGj5xGP48VQ3w.png"/></div></figure><p id="2d46" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">其中y是目标变量，x1，x2…xk是预测变量。</p><p id="cc8c" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">λ(斜率)是罚项，其中λ是偏转度。</p><p id="31b4" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">通过限制预测变量的系数来偏离简单回归拟合线，但这绝不会使它们为零。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c3173b4a279a7e9df0326c4b14fb64e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*euqWilbhCy55-uvyc5cMuA.png"/></div></figure><p id="04b1" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">假设一个非常流行的数据集工资-经验的例子，使用λ= 100显示岭回归图。</p><p id="1f6a" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">注意:随着α的增加，系数的大小减少到0，但不是0。</p><p id="de9d" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">自变量向0移动。</p><p id="9ce7" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated"><strong class="me jd">岭回归的优势</strong></p><ul class=""><li id="961a" class="nd ne it me b mf mv mi mw lq nf lu ng ly nh mu ni nj nk nl bi translated">降低了具有大量系数的模型的复杂性</li><li id="9284" class="nd ne it me b mf nm mi nn lq no lu np ly nq mu ni nj nk nl bi translated">计算费用也降低了。</li><li id="c319" class="nd ne it me b mf nm mi nn lq no lu np ly nq mu ni nj nk nl bi translated">它在存在高度相关的特征时工作良好。</li></ul><p id="d698" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated"><strong class="me jd">缺点</strong></p><p id="1108" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">模型解释能力意味着它会将系数缩小到非常接近零，但不完全是零。</p><h2 id="da00" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated"><strong class="ak">拉索回归(L1) </strong></h2><p id="50ca" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">套索回归是线性回归的收缩型版本，其中数据点向中心点收缩。它用于显示高度多重共线性的模型。Lasso回归的工作原理是自动化模型选择的某些部分，即特征提取。</p><p id="36c1" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">套索完全形式是“最小绝对收缩和选择操作符”。</p><p id="ab79" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">lasso回归方程为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/3fac1c73c0898ddb10bed293859d5fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*24rpv8VyaFLKEdsiwK0Z4A.png"/></div></figure><p id="3928" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">其中y是目标变量，x1，x2…xk是预测变量。</p><p id="f6ce" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">λ(斜率)是罚项，其中λ是偏转度。</p><p id="5f9c" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">Lasso回归也可以通过使变量的系数为零来消除变量，从而消除与其他预测变量具有高协方差的变量。<strong class="me jd">套索回归</strong>与岭回归的区别仅在于惩罚项。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f315e5e98cc85b9ea25d92d98fdc83d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*yQxZzuMaEWTudSU3dATnVw.png"/></div></figure><p id="ff3a" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">假设一个非常流行的数据集salary-experience使用λ= 10000显示岭回归图。</p><p id="a158" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated"><strong class="me jd">拉索回归的优势</strong></p><ul class=""><li id="469a" class="nd ne it me b mf mv mi mw lq nf lu ng ly nh mu ni nj nk nl bi translated">降低模型复杂性，即过度拟合。</li><li id="d35e" class="nd ne it me b mf nm mi nn lq no lu np ly nq mu ni nj nk nl bi translated">还通过使高度相关的特征为零来充当特征选择。</li><li id="a2f0" class="nd ne it me b mf nm mi nn lq no lu np ly nq mu ni nj nk nl bi translated">因此，计算能力也被降低，以使其成为合适的模型。</li></ul><p id="53fe" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated"><strong class="me jd">劣势</strong></p><p id="7d93" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">(一)不能进行分组选择。</p><p id="31b6" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">(ii)小的α值给出了显著的稀疏性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6d0e5191622ca069e929fbd82f6ef0db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*4WmQdBK8CRdK73mQOzuZvQ.png"/></div></figure><p id="2950" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">上图显示了所有三个回归的比较。</p><p id="854e" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated"><strong class="me jd">这是两个回归的代码，在Google colab中的可用住房数据集上实现。</strong></p><pre class="ks kt ku kv gt nu nv nw nx aw ny bi"><span id="d3d2" class="lh li it nv b gy nz oa l ob oc">import pandas as pd</span><span id="bc64" class="lh li it nv b gy od oa l ob oc">BHNames= [‘crim’,’zn’,’indus’,’chas’,’nox’,’rm’,</span><span id="ad45" class="lh li it nv b gy od oa l ob oc">‘age’,’dis’,’rad’,’tax’,’ptratio’,’black’,’lstat’,’medv’]</span><span id="05d8" class="lh li it nv b gy od oa l ob oc">url=’https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'</span><span id="e0a0" class="lh li it nv b gy od oa l ob oc">data = pd.read_csv(url, delim_whitespace=True, names=BHNames)</span><span id="2b55" class="lh li it nv b gy od oa l ob oc">print(data.head(20))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/48355a66eb8a4b80100644e1db91bcc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0N4S45lwFrR06x7vYr4b6g.png"/></div></div></figure><pre class="ks kt ku kv gt nu nv nw nx aw ny bi"><span id="8d86" class="lh li it nv b gy nz oa l ob oc">from sklearn.model_selection import train_test_split</span><span id="a3f5" class="lh li it nv b gy od oa l ob oc">X = data.drop(‘medv’, axis = 1)</span><span id="c6d8" class="lh li it nv b gy od oa l ob oc">print(‘X shape = ‘,X.shape)</span><span id="5f02" class="lh li it nv b gy od oa l ob oc">#output:<br/>X shape =  (506, 13)</span><span id="7e2e" class="lh li it nv b gy od oa l ob oc">Y = data[‘medv’]</span><span id="d82c" class="lh li it nv b gy od oa l ob oc">print(‘Y shape = ‘,Y.shape)</span><span id="4002" class="lh li it nv b gy od oa l ob oc">#output:<br/>Y shape =  (506,)</span><span id="5256" class="lh li it nv b gy od oa l ob oc">from sklearn import linear_model<br/>import matplotlib.pyplot as plt</span><span id="d613" class="lh li it nv b gy od oa l ob oc">names = data.drop(‘medv’, axis =1).columns</span></pre><p id="bc20" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated"><strong class="me jd">拉索回归将从本节</strong>开始工作</p><pre class="ks kt ku kv gt nu nv nw nx aw ny bi"><span id="2e16" class="lh li it nv b gy nz oa l ob oc">lasso = linear_model.Lasso(alpha=0.2)</span><span id="a1e0" class="lh li it nv b gy od oa l ob oc">lasso_coef = lasso.fit(X,Y).coef_</span><span id="2821" class="lh li it nv b gy od oa l ob oc">plt.plot(range(len(names)),lasso_coef )</span><span id="b26d" class="lh li it nv b gy od oa l ob oc">plt.xticks(range(len(names)), names, rotation=60)</span><span id="b659" class="lh li it nv b gy od oa l ob oc">plt.ylabel(“coefficient”)</span><span id="5dc8" class="lh li it nv b gy od oa l ob oc">plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/adc6a44a960fa2ed2665b1271ad49fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*DNZjKiXpRRmei_eCscgCGQ.png"/></div></figure><p id="c717" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated"><strong class="me jd">岭回归将从这里开始。</strong></p><pre class="ks kt ku kv gt nu nv nw nx aw ny bi"><span id="02a9" class="lh li it nv b gy nz oa l ob oc">#For Ridge</span><span id="0576" class="lh li it nv b gy od oa l ob oc">from sklearn.preprocessing import StandardScaler</span><span id="bcd2" class="lh li it nv b gy od oa l ob oc">scaler = StandardScaler()</span><span id="9041" class="lh li it nv b gy od oa l ob oc">x_std = scaler.fit_transform(X)</span><span id="e266" class="lh li it nv b gy od oa l ob oc">from sklearn.linear_model import Ridge</span><span id="d6aa" class="lh li it nv b gy od oa l ob oc">ridge = Ridge(alpha=0.2)</span><span id="1904" class="lh li it nv b gy od oa l ob oc">model = ridge.fit(x_std,Y).coef_</span><span id="531a" class="lh li it nv b gy od oa l ob oc">model</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7c8dbf11aa1582b4b0f29425b073e471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*VE1V3uqkI097uL6uDySwqw.png"/></div></figure><h2 id="25b9" class="lh li it bd lj lk ll dn lm ln lo dp lp lq lr ls lt lu lv lw lx ly lz ma mb iz bi translated"><strong class="ak">结论</strong></h2><p id="9754" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated">本文介绍了岭回归和套索回归这两种强有力的正则化技术，它们使我们的模型具有适当的预测能力。</p><p id="4b5c" class="pw-post-body-paragraph mc md it me b mf mv kd mh mi mw kg mk lq mx mm mn lu my mp mq ly mz ms mt mu im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae oh" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae oh" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="aea9" class="oi li it bd lj oj ok ol lm om on oo lp ki op kj lt kl oq km lx ko or kp mb os bi translated">推荐文章</h1><p id="1eeb" class="pw-post-body-paragraph mc md it me b mf mg kd mh mi mj kg mk lq ml mm mn lu mo mp mq ly mr ms mt mu im bi translated"><a class="ae oh" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄与Python </a> <br/> 2。<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a>T5】3 .<a class="ae oh" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae oh" rel="noopener ugc nofollow" target="_blank" href="/deep-learning-88e218b74a14?source=friends_link&amp;sk=540bf9088d31859d50dbddab7524ba35">为什么LSTM在深度学习方面比RNN更有用？</a> <br/> 5。<a class="ae oh" rel="noopener ugc nofollow" target="_blank" href="/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88?source=friends_link&amp;sk=6844935e3de14e478ce00f0b22e419eb">神经网络:递归神经网络的兴起</a> <br/> 6。<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae oh" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae oh" rel="noopener ugc nofollow" target="_blank" href="/differences-between-concat-merge-and-join-with-python-1a6541abc08d?source=friends_link&amp;sk=3b37b694fb90db16275059ea752fc16a">concat()、merge()和join()与Python </a> <br/>的区别9。<a class="ae oh" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a> <br/> 10。<a class="ae oh" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>