<html>
<head>
<title>Deep Insights Into K-nearest Neighbors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入了解K近邻</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/deep-insights-into-k-nearest-neighbors-2c95c04728d9?source=collection_archive---------2-----------------------#2020-07-25">https://pub.towardsai.net/deep-insights-into-k-nearest-neighbors-2c95c04728d9?source=collection_archive---------2-----------------------#2020-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4fa8" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/b2af7dd3d2f07e2bd20187fa12b28056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_3pXqLRdVSsFG6y9"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">在<a class="ae ko" href="https://unsplash.com/s/photos/two-owls?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ko" href="https://unsplash.com/@zmachacek?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">zdenk macha ek</a>拍摄的照片</figcaption></figure><h1 id="0ade" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">介绍</h1><p id="a572" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">k-最近邻(简称KNNs)是一种健壮而简单的监督机器学习算法。虽然它的回归变量也存在，但我们只讨论它的分类形式。</p><p id="afcc" class="pw-post-body-paragraph ln lo it lp b lq ml ls lt lu mm lw lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">KNN是一个<strong class="lp jd"> <em class="mq">无参</em> </strong> <em class="mq">懒学</em>的算法。让我们来看看每个术语的含义—</p><ul class=""><li id="c62a" class="mr ms it lp b lq ml lu mm ly mt mc mu mg mv mk mw mx my mz bi translated"><strong class="lp jd">非参数</strong>:与参数模型不同，非参数模型更加灵活。他们对参数的数量或假设的函数形式没有任何先入为主的概念。因此，它们将我们从任何关于数据分布的错误假设的麻烦中解救出来。例如，在线性回归(一种参数算法)中，我们已经有了一种假设形式，在训练阶段，我们会寻找其正确的系数值。</li><li id="d023" class="mr ms it lp b lq na lu nb ly nc mc nd mg ne mk mw mx my mz bi translated"><strong class="lp jd">懒惰学习</strong>:又称<em class="mq">基于实例的</em>学习。这里，该算法不明确地学习模型，而是选择记忆训练集，以便在预测阶段使用。这导致快速训练阶段，但是昂贵的测试阶段，因为它需要存储和遍历整个数据集。</li></ul><h2 id="caaa" class="nf kq it bd kr ng nh dn kv ni nj dp kz ly nk nl ld mc nm nn lh mg no np ll iz bi translated">直觉方法</h2><p id="c6f3" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">这种算法背后的基本思想是——<strong class="lp jd"><em class="mq">【物以类聚】，</em> </strong>即类似的事情发生在附近，想知道为什么这样的事情竟然成立？好吧，试着想象一个D维空间中的大量数据点。得出子空间将被密集填充的结论并没有错，这意味着两个相当接近的点更有可能具有相同的标签。事实上，这个算法存在一个严格的误差范围—</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b77bce4beda42ca49161647d5af07df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/0*1FNtrykrX6m9BSH1"/></div></figure><p id="b9d4" class="pw-post-body-paragraph ln lo it lp b lq ml ls lt lu mm lw lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">其中P*是贝叶斯错误率，这是任何分类器的最低可能错误率，c是类的数量，P是我们算法的错误率。根据上面的等式，如果点的数量相当大，那么算法的错误率小于Bayes错误率的两倍。相当酷！不要误会我的意思。我说的大量数据点，不是指大量特征。相反，该算法特别容易受到<strong class="lp jd">维度诅咒</strong>的影响，这意味着维度(特征数量)的增加将导致积累固定数量的邻居所需的超空间体积的增加。而且随着体积的增大，邻居越来越不相似，因为他们之间的距离增大了。因此，具有相同标签的点的概率降低。在这些情况下，功能选择可能会派上用场。</p><h1 id="de14" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">KNN是做什么的？</h1><p id="a222" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">KNN算法获取测试输入，将其与每个单个训练样本进行比较，并将目标标签预测为k个最相似训练样本中最常出现的标签。两个数据点之间的相似性通常使用欧几里德距离来度量。</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ce2d6bbc9a1b583e6ea8f1950ca0f085.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/0*DAoKf_15b3-SE4uV.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://mccormickml.com/2013/08/15/the-gaussian-kernel/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="b7f6" class="pw-post-body-paragraph ln lo it lp b lq ml ls lt lu mm lw lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">当k = 1时，目标标签被预测为最近的训练样本的标签。<br/>因为我们存储并遍历每个测试示例的整个训练集，所以测试阶段在内存和时间方面都很昂贵。</p><h1 id="2566" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">kNN中的k是什么？</h1><p id="ed53" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">k是超参数，其值控制学习过程。它是预测测试样本标签时考虑的邻居数量。</p><h1 id="6e1c" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">k的值对我们的分类器有影响吗？</h1><p id="39bb" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">在回答上述问题之前，让我们先了解一下过度拟合和欠拟合的含义。在监督学习中，我们希望建立一个从我们的训练集中学习的模型，并对看不见的数据做出合理准确的预测。我们模型的这种<em class="mq">学习到足够应用</em>的能力叫做泛化。直觉上，我们期望简单的模型能够更好地概括。复杂的模型<a class="ae ko" href="https://methodsblog.com/2012/04/23/simple-models-ftw/" rel="noopener ugc nofollow" target="_blank">符合数据的特性，而不是总结潜在的生物学。</a>它们可能在训练数据上表现准确，但通常不能概括测试数据。<br/><br/>而当模型过于简单，甚至在训练集上失败时，就会出现欠拟合。</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/8b1f009149ed6c5d2667ab293aabbd89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PHQjkaxnVCkFyoY7.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://mc.ai/overfitting-and-underfitting-bug-in-ml-models/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="f1c4" class="pw-post-body-paragraph ln lo it lp b lq ml ls lt lu mm lw lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">让我们看看考虑一个最近的邻居和20个最近的邻居是什么样子的—</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nw"><img src="../Images/f108b497b059f1412595da5ecd22e377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WRfhwiTg7iIB_0TVXTNkjw.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="682f" class="pw-post-body-paragraph ln lo it lp b lq ml ls lt lu mm lw lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">从上图可以观察到，k = 1导致了锯齿状的边界，而k = 20给了我们一个平滑的边界。<br/>平滑的边界对应于简单的模型，而锯齿状的边界对应于复杂的模型。因此，如果我们保持k值较低，我们有过度拟合的风险，而如果我们保持k值较高，我们有欠拟合的风险。<br/>因此，一个显而易见的解决方案是保持k，使其既不太小也不太大。也许可以尝试用不同的k值进行评估，然后选择效果最好的。<br/>好吧，这就是我们要做的，但是要巧妙一点。</p><h1 id="7d89" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">超参数调谐的验证</h1><p id="3283" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">如前所述，我们将尝试不同的k值，但不是在测试集上。直到最后一次才使用测试设备。以前使用它会给我们带来过度适应测试集的麻烦，因此无法对看不见的数据进行归纳。<br/>解决这个问题的一种方法是保留训练集的一个子集，用于调整超参数。这个集合称为验证集。在训练集很小的情况下，一种更复杂的技术是交叉验证。这里我们将训练集分成x个组，比如5个。我们将使用其中的4个进行训练，一个进行验证。对于每次迭代，我们使用另一个组作为验证集。最后，平均性能用于计算k的最佳值。</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nx"><img src="../Images/10ba047aed693107ce09daad0b7786e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Am__tpXTvneKjx5u.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://cs231n.github.io/classification/#nn" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kr">来源</strong> </a> <strong class="bd kr"> : </strong>给出一个训练和测试集。训练集被分成多个折叠(例如，这里是五个折叠)。折叠1–4成为训练集。一个折叠(例如，此处黄色的折叠五)被表示为验证折叠，并用于调整超参数。交叉验证更进了一步，它迭代选择哪个折叠是验证折叠，独立于1-5。这将被称为5重交叉验证。最后，一旦模型被训练并且所有的最佳超参数被确定，模型在测试数据(红色)上被评估一次。</figcaption></figure><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ny"><img src="../Images/d38b9516973a64ea46296fb6533cce2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4U3FvNvqwLLOJMNI.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="64ae" class="pw-post-body-paragraph ln lo it lp b lq ml ls lt lu mm lw lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">在上图中，k = 7导致最低的验证误差。</p><h1 id="dfbc" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">KNNs的优势</h1><ul class=""><li id="43de" class="mr ms it lp b lq lr lu lv ly nz mc oa mg ob mk mw mx my mz bi translated">KNN易于实现和理解。</li><li id="3fc1" class="mr ms it lp b lq na lu nb ly nc mc nd mg ne mk mw mx my mz bi translated">不需要或只需要很少的培训时间。</li><li id="082c" class="mr ms it lp b lq na lu nb ly nc mc nd mg ne mk mw mx my mz bi translated">它适用于多类数据集。</li><li id="25f3" class="mr ms it lp b lq na lu nb ly nc mc nd mg ne mk mw mx my mz bi translated">KNN是非参数的，所以它可以很好地处理非常不寻常的数据，因为不需要关于函数形式的假设。</li></ul><h1 id="8331" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">KNNs的缺点</h1><ul class=""><li id="ad7a" class="mr ms it lp b lq lr lu lv ly nz mc oa mg ob mk mw mx my mz bi translated">它有一个计算昂贵的测试阶段。</li><li id="3e5b" class="mr ms it lp b lq na lu nb ly nc mc nd mg ne mk mw mx my mz bi translated">KNN饱受阶级分布不均衡之苦。更频繁的类倾向于支配投票过程。</li><li id="7ab8" class="mr ms it lp b lq na lu nb ly nc mc nd mg ne mk mw mx my mz bi translated">KNN也遭受了维数灾难。</li></ul><p id="8d81" class="pw-post-body-paragraph ln lo it lp b lq ml ls lt lu mm lw lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">我已经尝试对K-最近邻算法给出了深入的见解。我希望一切都有意义。</p><p id="5b17" class="pw-post-body-paragraph ln lo it lp b lq ml ls lt lu mm lw lx ly mn ma mb mc mo me mf mg mp mi mj mk im bi translated">感谢阅读！</p><h1 id="77bf" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">参考资料:</h1><div class="oc od gp gr oe of"><a href="https://cs231n.github.io/" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd jd gy z fp ok fr fs ol fu fw jc bi translated">用于视觉识别的CS231n卷积神经网络</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">斯坦福CS231n课程材料和笔记:视觉识别的卷积神经网络。</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">cs231n.github.io</p></div></div></div></a></div><div class="oc od gp gr oe of"><a href="https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd jd gy z fp ok fr fs ol fu fw jc bi translated">K近邻(KNN)算法的详细介绍</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">k最近邻(从现在开始是KNN)是一种非常容易理解但是非常有效的算法…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">saravananthirumuruganathan.wordpress.com。</p></div></div><div class="oo l"><div class="op l oq or os oo ot ki of"/></div></div></a></div><div class="oc od gp gr oe of"><a href="https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd jd gy z fp ok fr fs ol fu fw jc bi translated">Python和R语言K近邻应用完全指南</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">这是一个深入的教程，旨在向您介绍一个简单而强大的分类算法，称为…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">kevinzakka.github.io</p></div></div><div class="oo l"><div class="ou l oq or os oo ot ki of"/></div></div></a></div></div></div>    
</body>
</html>