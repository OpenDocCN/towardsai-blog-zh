<html>
<head>
<title>ML Algorithms from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中从头开始的ML算法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/ml-algorithms-from-scratch-in-python-5caac512eabc?source=collection_archive---------1-----------------------#2022-03-12">https://pub.towardsai.net/ml-algorithms-from-scratch-in-python-5caac512eabc?source=collection_archive---------1-----------------------#2022-03-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5482" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">幕后数学理解的自我注释</h2></div><p id="f286" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">目标:</strong>在不使用任何SKLearn或Tensorflow之类的现成ML库的情况下，用python构建机器学习算法。打算涵盖线性回归、逻辑回归、KNN、K均值聚类、决策树、随机森林、SVM、XGBoost、感知器、带反向传播的神经网络、DNN、RNN、LSTM、TF-IDF、单词袋、LDA和Word2Vec。</p><h1 id="f147" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">线性回归</h1><p id="7b99" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">梯度下降是通过遵循成本函数的梯度来最小化函数的过程。这包括知道成本和导数的形式，以便从给定点开始，你知道梯度并可以向那个方向移动，例如向最小值的下坡。</p><p id="bfde" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们遵循梯度下降法来寻找B0和B1，这将使这种情况下的损失函数(均方误差)最小化。<br/>初始化:<br/> B0=B1=0，学习率(lr)=0.01，次数=500</p><blockquote class="mb mc md"><p id="6689" class="ki kj me kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">伪代码:<br/> 1。输入Xs和Ys通过初始值<br/> 2输入系统。用B0*X+B1 <br/> 3进行Y的预测。返回误差值用于系数修正<br/> 4。B0 = B0-lr *错误和B1 = B1-lr * X *错误<br/> 5。在一个点之后，比如说300个历元，误差函数将开始收敛，即没有变化，这将是早期停止的点。</p></blockquote><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/33899e6c6122afd440f4e41763568d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*qrPHxn1Q0sVAuBoV5BCcCA.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">线性回归结果</figcaption></figure></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="c12b" class="le lf it bd lg lh nh lj lk ll ni ln lo jz nj ka lq kc nk kd ls kf nl kg lu lv bi translated">逻辑回归(类似于线性回归)</h1><div class="nm nn gp gr no np"><a href="https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">如何在Python中从头实现逻辑回归——机器学习精通</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">逻辑回归是两类问题的首选线性分类算法。这很容易实现，很容易…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">machinelearningmastery.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od mu np"/></div></div></a></div></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="e746" class="le lf it bd lg lh nh lj lk ll ni ln lo jz nj ka lq kc nk kd ls kf nl kg lu lv bi translated">KNN</h1><div class="nm nn gp gr no np"><a href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">从零开始用Python开发k-最近邻-机器学习精通</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">在本教程中，您将学习k-最近邻算法，包括它是如何工作的，以及如何…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">machinelearningmastery.com</p></div></div><div class="ny l"><div class="oe l oa ob oc ny od mu np"/></div></div></a></div><p id="82e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于分类问题，我们通过使用成对欧几里德距离和采取多数投票来找到最近的k个邻居。<br/>伪代码:<br/> <strong class="kk iu">第一步</strong>:计算欧氏距离。<br/> <strong class="kk iu">第二步</strong>:获取最近邻居。<br/> <strong class="kk iu">第三步</strong>:做预测。<br/> <strong class="kk iu">第四步:</strong>通过交叉验证评估预测精度，通过改变k或距离度量优化度量</p><p id="8844" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于分类问题，我们使用成对欧几里德距离找到最近的k个邻居，并采取多数投票。<br/>伪代码:<br/> <strong class="kk iu">第一步</strong>:计算欧氏距离。<br/> <strong class="kk iu">第二步</strong>:获取最近邻居。<br/> <strong class="kk iu">第三步</strong>:做预测。<br/> <strong class="kk iu">步骤4: </strong>通过交叉验证评估预测精度，并通过改变k或距离度量来优化度量</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div></figure></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="c220" class="le lf it bd lg lh nh lj lk ll ni ln lo jz nj ka lq kc nk kd ls kf nl kg lu lv bi translated">k-表示:</h1><h2 id="205c" class="of lf it bd lg og oh dn lk oi oj dp lo kr ok ol lq kv om on ls kz oo op lu oq bi translated">最佳K用肘法计算</h2><p id="7a55" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">在肘方法中，我们实际上是在1到10之间改变聚类的数量(K)。对于K的每个值，我们正在计算<strong class="kk iu"> WCSS(组内平方和)。WCSS是聚类中每个点和质心之间距离的平方之和。</strong>当我们用K值绘制WCSS时，该图看起来像一个弯头。随着集群数量的增加，WCSS值将开始降低。当K = 1时，WCSS值最大。当我们分析该图时，我们可以看到该图在某一点会快速变化，从而形成一个肘形。从这一点，图形开始几乎平行于X轴移动。对应于该点的K值是最佳K值或最佳聚类数。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi or"><img src="../Images/fb89d7b9a1d137484077902a4d1168e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/0*Hnru5j6DY633jDbc.png"/></div></figure><p id="ae06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过在可能值的范围内改变聚类的数量来挑选聚类的数量，并在每次迭代中计算轮廓分数和失真。最佳数字是最大化第一个数字并最小化第二个数字。<br/>下面介绍两种测量聚类质量的方法:</p><p id="65b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">惯性:</strong>直觉上，惯性告诉我们一个星团内的点有多远。因此，小惯性是目标。惯性值的范围从零开始上升。</p><p id="6973" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">轮廓分数:</strong>轮廓分数表示一个聚类中的数据点与另一个聚类中的数据点之间的距离。轮廓分数的范围是从-1到1。比分应该更接近1而不是-1。<br/><a class="ae os" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">肘方法</a>将方差百分比解释为聚类数的函数:人们应该选择多个聚类，以便添加另一个聚类不会给出更好的数据建模。更准确地说，如果绘制由聚类相对于聚类数量解释的方差的百分比，第一个聚类将增加很多信息(解释很多方差)，但是在某个点，边际增益将下降，在图中给出一个角度。在这一点上选择簇的数量，因此称为“肘形标准”。</p><p id="35ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Kmeans将文档(NLP)分成不相交的簇。假设每个聚类属于单个主题。然而，描述实际上可能以主题的“混合”为特征。例如，让我们看一篇关于扎克伯格在国会前举行的听证会的文章:显然，你会根据关键词出现不同的话题:隐私、技术、脸书应用程序、数据等。<strong class="kk iu"> LDA有助于处理这个问题。</strong></p><p id="61f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输入是具有<strong class="kk iu"> k </strong>簇的<strong class="kk iu"> n </strong>个数据点，小的容差极限，例如0.01之后用于提前停止，n _迭代次数为500。输出是一组“<em class="me"> k </em>聚类质心和数据集的标签，将每个数据点映射到一个唯一的聚类。</p><p id="1231" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第一步。随机选取k个数据点作为我们的初始质心。<br/> <strong class="kk iu">第二步。</strong>用k个质心找出训练集中每个数据点之间的距离(我们的目的是欧几里德距离)。<br/> <strong class="kk iu">第三步。</strong>现在根据找到的距离将每个数据点分配到最近的质心。<br/> <strong class="kk iu">第四步。</strong>通过取每个聚类组中的点的平均值来更新质心位置。<br/> <strong class="kk iu">第五步。重复步骤2到4，直到我们的质心不变。</strong></strong></p><pre class="mi mj mk ml gt ot ou ov ow aw ox bi"><span id="1c10" class="of lf it ou b gy oy oz l pa pb">if np.sum((curr - original_centroid)/original_centroid * 100.0) &gt; self.tolerance:<br/>	isOptimal = False<br/>#break out of the main loop if the results are optimal, ie. the centroids don't change their positions much(more than our tolerance)<br/>if isOptimal:<br/>		break</span></pre><div class="nm nn gp gr no np"><a href="https://en.wikipedia.org/wiki/File:K-means_convergence.gif" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">K-means收敛. gif</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">从维基百科，免费的百科全书点击一个日期/时间来查看当时出现的文件。日期/时间…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">en.wikipedia.org</p></div></div><div class="ny l"><div class="pc l oa ob oc ny od mu np"/></div></div></a></div></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="c616" class="le lf it bd lg lh nh lj lk ll ni ln lo jz nj ka lq kc nk kd ls kf nl kg lu lv bi translated">感知器</h1><p id="d15b" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">感知器从训练数据的例子中接收输入信号，我们对这些数据进行加权并组合成一个称为激活的线性方程。它与以类似方式进行预测的线性回归和逻辑回归密切相关(例如，输入的加权和)。感知器算法的权重必须用随机梯度下降法从你的训练数据中估计出来。</p><p id="bf2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">权重和偏差被初始化为0，并通过<em class="me">激活= sum(weight_i * x_i) + bias根据预测值与输出值的误差进行学习。<br/> </em>权重矩阵等于输入矩阵或num_neurons</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">感知器学习权重</figcaption></figure></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="afc1" class="le lf it bd lg lh nh lj lk ll ni ln lo jz nj ka lq kc nk kd ls kf nl kg lu lv bi translated">反向传播神经网络</h1><div class="nm nn gp gr no np"><a href="https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">如何用Python编写一个带反向传播的神经网络(从头开始)——机器学习…</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">经典的前向人工神经网络采用反向传播算法。这是技术…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">machinelearningmastery.com</p></div></div><div class="ny l"><div class="pd l oa ob oc ny od mu np"/></div></div></a></div><p id="f3c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定输入训练数据集以及随机权重和偏差，您能否构建一个具有隐藏图层和反向传播的DNN？</p><blockquote class="mb mc md"><p id="0488" class="ki kj me kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">伪代码<br/> 1。用#层(输入+输出+隐藏)和每个神经元的权重和偏差矩阵初始化网络<br/> 2。构建激活函数，该函数将通过(Wi*Xi)+Bias计算输入的神经元激活。<br/> 3。通过sigmoid、Relu等激活函数传递激活的神经元<br/> 4。向前传播预测并计算总误差<br/> 5。将误差反向传播并将个别误差归咎于神经元并通过导数函数和wi=wj+lr*ei <br/> 6调整权重。继续训练网络以最小化误差，直到n个时期具有指定的学习速率(如果不使用动量)</p></blockquote><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">反向传播神经网络</figcaption></figure><h2 id="fb97" class="of lf it bd lg og oh dn lk oi oj dp lo kr ok ol lq kv om on ls kz oo op lu oq bi translated">误差反向传播</h2><p id="28e3" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">第一步是计算每个输出神经元的误差，这将使我们的误差信号(输入)通过网络反向传播。给定神经元的误差可以计算如下</p><blockquote class="mb mc md"><p id="40a9" class="ki kj me kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">误差=(输出-预期)*传递导数(输出)</p></blockquote><p id="9bf1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="kk iu">预期</strong>是神经元的预期输出值，<strong class="kk iu">输出</strong>是神经元的输出值，<strong class="kk iu">传递导数()</strong>计算神经元输出值的斜率<br/> <strong class="kk iu">消失梯度</strong>意味着靠近输入层的层中的权重不会响应于在训练数据集上计算的误差而更新。具有许多层的训练网络(例如深度神经网络)的一个问题是，当梯度通过网络向后传播时，梯度急剧减小。当误差到达靠近模型输入的层时，误差可能非常小，以至于几乎没有影响。因此，这个问题被称为“<em class="me">消失梯度</em>问题。消失梯度是递归神经网络的一个特殊问题，因为网络的更新包括为每个输入时间步长展开网络，实际上创建了一个非常深的网络，需要权重更新。<br/>解决方案:<em class="me">预训练、更好的随机初始缩放、更好的优化方法、特定架构、正交初始化等。</em></p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="f3c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> TF-IDF(词频-逆文档频率)</strong></p><p id="e46a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">词频:</strong>是单词在文档中出现的次数占该文档总字数的比率。<br/> <code class="fe pe pf pg ou b">tf(t,d) = count of t in d / number of words in d<br/></code>为文集——“拉维来了。罗汉在那里”，<br/> tf(拉维)=1/6 <br/> tf(is)=2/6</p><p id="948e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">逆文档频率:文档数与包含该词的文档数之比的Log<br/>语料库=[“Ravi在这里”、“Rohan在那里”、“好读ML”、“坏读ML”]<br/><em class="me">IDF(t)= Log(N/(df+1))<br/>IDF(Ravi)= Log(4/2)<br/>IDF(ML)= Log(4/3)</em></p><p id="cc7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果模型遇到除vocab之外的未知单词，它会给我们一个关键错误，因为我们没有考虑任何未知的标记。为这样的用例使用word2vec嵌入。</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">给定语料库的TF-IDF计算</figcaption></figure><p id="ee75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其他技术，如计数矢量器、单词包(工作对生活和生活对工作是一样的)和散列矢量器是TF-IDF的前身，可用于其他功能。<br/>TF-IDF的缺点:<br/> - TF-IDF基于词袋(BoW)模型，因此它不能捕捉文本中的位置、语义、在不同文档中的共现等。<br/> -无法捕获语义(例如，与主题模型、单词嵌入相比)</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="3368" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词嵌入表示单词在受限空间中的矢量表示，例如100维空间，而不是字典中数百万个可能的世界(维度)。Skip-Gram将输出作为输入层和隐藏层之间的权重，而对于CBOW，它是隐藏层和输出层。<br/>在监督学习中，权重可以如下重新训练:</p><blockquote class="mb mc md"><p id="f06d" class="ki kj me kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">1.用随机分数初始化一个一键编码矩阵<br/> 2。将它乘以代表单词的一维向量，例如“Nice”有[1，0，0，0]表示法<br/> 3。将单个样品组合成统一的重量矩阵，例如12个重量，每个重量有4个维度。小型评论使用填充。<br/> 4。将矩阵通过一个神经元，根据标记数据计算误差(伪问题)，并反向传播以更新输入权重。<br/> 5。使用x LR执行n个时期的迭代，以提高二进制交叉熵<br/> 6。将单个关键字的向量权重存储为嵌入权重。</p></blockquote><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ph"><img src="../Images/c7786338b5311c39f6663302075ed50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*prgFUzf0TzhLTHHBHUrfRw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">监督学习中的嵌入计算</figcaption></figure><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div></figure></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="2d21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">递归神经网络(RNN)——</strong>用于序列问题，如下一个单词预测、时间序列、语音识别等，其输出依赖于前一个序列。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi pi"><img src="../Images/fb74cb46aebf2b397763d359133da03e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aBI8RYDnclweMXbm.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">RNN</figcaption></figure><p id="f3d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RNN白手起家的步骤:<br/> 1。以序列作为输入的数据准备，例如股票价格的50天跟踪<br/> 2。RNN的建筑创作:输入为LR、#Epochs、隐藏层、输入层和输出层，类似于DNN，附加参数为BPPT(通过时间的反向传播)、消失/爆炸渐变的Min_Clip和Max_Clip。BPTT与反向传播的核心区别在于，反向传播步骤是针对RNN层中的所有时间步长进行的。因此，如果我们的序列长度为50，我们将反向传播当前时间步长之前的所有时间步长。<br/>除了输入和输出权重矩阵之外，我们还有一个用于RNN层中共享权重的附加矩阵。<br/> 3。训练模型:正向传递、计算损失、开始训练、反向传递误差和更新权重<br/> 4。生成预测</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi pj"><img src="../Images/d12c037f1aba15267c73a604f2c8d4e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-xInZQ7_BIyuz3dMSpvVg.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">RNN的优点和缺点</figcaption></figure><p id="7a2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">梯度削波</strong>是一种技术，用于处理执行反向传播时有时会遇到的爆炸梯度问题。通过限制梯度的最大值，这种现象在实践中得到控制。<br/>梯度更新规则，新权重=weight-LR*Gradient，<br/>如果梯度较小，则没有消息到达更早的层。<br/>如果梯度较大，网络不会收敛，出现NaN误差。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="acbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LSTM:他们有门来消除不必要的噪音。它有一个单元状态和门:遗忘门、输入门和输出门。<br/>门包含sigmoid激活，即更新或遗忘[1，0]，不同于Tanh[-1，1]。<br/> <strong class="kk iu">忘记门:</strong>来自先前隐藏状态和当前状态的信息通过sigmoid函数传递。更接近0表示忘记，更接近1表示保留。<br/> <strong class="kk iu">输入门:</strong>先前隐藏和当前输入通过Tanh功能。下一个状态的Tanh输出和Sigmoid输出相乘。<br/> <strong class="kk iu">输出门:</strong>决定下一个隐藏状态应该是什么。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi pk"><img src="../Images/3b675e838c062230452b73e4a763e595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wg38XZlhji3G_4ZiEkpJyA.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">LSTM的伪代码</figcaption></figure><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi pl"><img src="../Images/d6e643656737184416e6da0c86de0470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FH8LBecs1jQptz50h7Dyw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">LSTM如何看待评论</figcaption></figure><p id="02fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">来源</strong></p><div class="nm nn gp gr no np"><a href="https://machinelearningmastery.com/implement-linear-regression-stochastic-gradient-descent-scratch-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">如何在Python中从头实现线性回归—机器学习精通</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">很多机器学习算法的核心是优化。机器学习使用优化算法…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">machinelearningmastery.com</p></div></div><div class="ny l"><div class="pm l oa ob oc ny od mu np"/></div></div></a></div><div class="nm nn gp gr no np"><a href="https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">如何用Python编写一个带反向传播的神经网络(从头开始)——机器学习…</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">经典的前向人工神经网络采用反向传播算法。这是技术…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">machinelearningmastery.com</p></div></div><div class="ny l"><div class="pd l oa ob oc ny od mu np"/></div></div></a></div><div class="nm nn gp gr no np"><a href="https://www.askpython.com/python/examples/tf-idf-model-from-scratch" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">用Python-ask Python从头开始创建TF-IDF模型</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">TF-IDF模型是一种用数值表示单词的方法。“你好，最近怎么样？”，您可以轻松地…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">www.askpython.com</p></div></div><div class="ny l"><div class="pn l oa ob oc ny od mu np"/></div></div></a></div><div class="nm nn gp gr no np"><a href="https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">RNN从零开始|用Python构建RNN模型</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">每当我们听到一个句子时，人类不会重新启动他们对语言的理解。给定一篇文章，我们掌握…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="ny l"><div class="po l oa ob oc ny od mu np"/></div></div></a></div></div></div>    
</body>
</html>