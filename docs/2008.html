<html>
<head>
<title>How Machines Beat Humans at Everything</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器如何在所有方面打败人类</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-machines-beat-humans-at-everything-9d25eafdf256?source=collection_archive---------0-----------------------#2021-07-22">https://pub.towardsai.net/how-machines-beat-humans-at-everything-9d25eafdf256?source=collection_archive---------0-----------------------#2021-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1625" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="bac2" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">发现机器如何在它们使用强化学习攻击的大多数领域中变成超人。</h2></div><blockquote class="kr ks kt"><p id="d494" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最初发表于<a class="ae lr" href="https://www.louisbouchard.ai/reinforcement-learning-icjai-21/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/reinforcement-learning-icjai-21/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/ba8849cffcdb267e3eb757804e2bb8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QDAqKiK7_pdAXt-SXczOzw.png"/></div></div></figure><p id="3e57" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">听听这个故事…</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="69e3" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">你大概听说过国际象棋、围棋，甚至一些像Dota这样的电子游戏的世界冠军都是机器。人工智能的最新进展使研究人员能够在这些游戏中击败世界上最好的人类玩家，这要归功于一种叫做<strong class="kx jd">强化学习</strong>的技术。同样的技术也允许机器人行走、开门，甚至踢足球。但是这种技术到底是什么？这篇短文旨在介绍这项技术的基础知识，并概述其工作原理。</p><h2 id="fa5d" class="mj mk it bd ml mm mn dn mo mp mq dp mr me ms mt mu mf mv mw mx mg my mz na iz bi translated">它是如何工作的</h2><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5ea4d651499605b61b2d152fc964f66c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*rpUAt6RQRpyycVtimMOWsA.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">强化学习受到生物的启发:优化正负奖励。</figcaption></figure><p id="de9b" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">强化学习是一种受生物启发的技术。一般来说，生物正在学习某些行为以获得奖励或避免惩罚。如果你正在吃好吃的东西，你可能会想再吃一次。如果你正在触摸一个热炉子，很可能你不想再做一次。强化学习就是做同样的事情:教会机器如何获得积极的回报，避免消极的回报。我们称这些机器为“代理”</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/390fd4d8b47a60365d907ed884ae31b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*ctVyvjhS_axoMpIsNeSJ3A.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">在其环境(足球场)中进化的智能体。</figcaption></figure><p id="8ebc" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这些代理在一个环境中进化。他们将观察这个环境，并根据这些观察采取行动。根据他们行动的结果，他们将得到奖励，无论是正面的还是负面的。一开始代理会随机表现，但是通过试错会越来越好。换句话说，他们正在学习最大化他们一生中得到的奖励。</p><h2 id="19d2" class="mj mk it bd ml mm mn dn mo mp mq dp mr me ms mt mu mf mv mw mx mg my mz na iz bi translated">一个简单的例子…</h2><p id="b5e8" class="pw-post-body-paragraph ku kv it kx b ky ng kd la lb nh kg ld me ni lg lh mf nj lk ll mg nk lo lp lq im bi translated">让我们看一个简单的例子。你在一条假想的线上，一边是一个可以吃的蛋糕，另一边是燃烧的火堆。在这种情况下你会怎么做？通常，你的答案会是径直走向蛋糕。否则在火堆里走会受伤的。但是计算机如何知道这些并学习同样的决策过程呢？通过反复试验！</p><p id="f507" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">正如我们所讨论的，首先，代理人会随机行动。一半的时间，它会去左边，另一半去右边。但在某个时刻，它会达到其中一个回报，无论是积极的还是消极的。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/0b04fcd79b612a3d2fb3a19d1f29fff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*4TA99y9EzDw59OcIEJSwQQ.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">一个简单的强化学习的例子，代理探索它的环境以获得最大的回报。</figcaption></figure><p id="2db3" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这时，代理人知道往左会痛，或者反过来，如果足够幸运，它会知道蛋糕有多好吃。就是这样！一旦它了解到这些奖励，它就可以在这种环境中有最佳的行为，每次都直接去吃蛋糕。</p><p id="01f3" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这是一个简单的例子，因为代理的唯一可能性是向右或向左。然而，通常它会有更多可能的路径。即使它已经在如此复杂的环境中找到了好的回报，它还需要继续寻找更好的回报。换句话说，也许下一个拐角处有一个更大的蛋糕在等着我们，所以我们需要时不时地抓住机会看一看。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi nl"><img src="../Images/7f6ab7330af4633413d726a5b8c932c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iKAXcmkBnZ7MXqTC.png"/></div></a></figure><p id="3b71" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">我们可以和现实世界做一个比较。如果你像我一样，习惯了定期在同一家披萨店点同样的披萨，但如果你偶尔尝试一家新的呢？你可能会更加欣赏它，并认为它是你的新宠。如果不尝试新的东西，你永远不会发现这种改进，即使你已经享受了第一次的味道。</p><h2 id="076e" class="mj mk it bd ml mm mn dn mo mp mq dp mr me ms mt mu mf mv mw mx mg my mz na iz bi translated">结论</h2><p id="c0a5" class="pw-post-body-paragraph ku kv it kx b ky ng kd la lb nh kg ld me ni lg lh mf nj lk ll mg nk lo lp lq im bi translated">当然，不是每个场景都这么简单，但是在强化学习中，每个问题都可以这么看。对于每个新的挑战，代理人将面临的唯一变化是它将进化的环境类型。无论是棋盘、电子游戏，甚至是学习如何行走的机器人的马达状态，逻辑都是一样的:代理人尝试事物，观察环境对他的行动的反应，并适应未来做得更好。你可以把强化学习看作是机器以达尔文主义的方式学习。</p><p id="af28" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">如果这对你有意义，那么恭喜你！您现在了解了什么是强化学习以及它是如何工作的。当然，还有更多技术细节，但这是这一惊人技术的要点，具有惊人的能力</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="878d" class="mj mk it bd ml mm mn dn mo mp mq dp mr me ms mt mu mf mv mw mx mg my mz na iz bi translated">文章作者</h2><p id="9438" class="pw-post-body-paragraph ku kv it kx b ky ng kd la lb nh kg ld me ni lg lh mf nj lk ll mg nk lo lp lq im bi translated">——马利克·科斯坦蒂尼，<a class="ae lr" href="https://www.linkedin.com/in/malrick-costantini-25316687/" rel="noopener ugc nofollow" target="_blank">领英</a>，<a class="ae lr" href="http://www.malrickcostantini.com/" rel="noopener ugc nofollow" target="_blank">网站</a></p><p id="cb5c" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">—伊莱亚斯·伊尔马里，<a class="ae lr" href="https://www.linkedin.com/in/eliasliinamaa/" rel="noopener ugc nofollow" target="_blank">领英</a>，<a class="ae lr" href="https://nordicgrit.com/" rel="noopener ugc nofollow" target="_blank">网站</a></p><p id="8c1f" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">—路易-弗朗索瓦·布沙尔，<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank">领英</a>，<a class="ae lr" href="https://www.louisbouchard.ai/" rel="noopener ugc nofollow" target="_blank">网站</a></p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="615a" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank">T5】简讯 </a>！</p><h2 id="3e2b" class="mj mk it bd ml mm mn dn mo mp mq dp mr me ms mt mu mf mv mw mx mg my mz na iz bi translated">支持我:</h2><ul class=""><li id="85fc" class="nt nu it kx b ky ng lb nh me nv mf nw mg nx lq ny nz oa ob bi translated">支持我的最好方式是成为这个网站<strong class="kx jd"> </strong>的成员，或者如果你喜欢视频格式，在<a class="ae lr" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"><strong class="kx jd">YouTube</strong></a><strong class="kx jd"/>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="e780" class="nt nu it kx b ky oc lb od me oe mf of mg og lq ny nz oa ob bi translated">在经济上支持我在T21的工作</li><li id="ffc4" class="nt nu it kx b ky oc lb od me oe mf of mg og lq ny nz oa ob bi translated">跟我来这里上<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a></li></ul></div></div>    
</body>
</html>