<html>
<head>
<title>Training Faster R-CNN Using TensorFlow’s Object Detection API with a Custom Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow的对象检测API和自定义数据集训练更快的R-CNN</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/training-faster-r-cnn-using-tensorflow-object-detection-api-with-a-custom-dataset-88dd525666fd?source=collection_archive---------0-----------------------#2021-04-20">https://pub.towardsai.net/training-faster-r-cnn-using-tensorflow-object-detection-api-with-a-custom-dataset-88dd525666fd?source=collection_archive---------0-----------------------#2021-04-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c821681e151af62a0d8fef0ec5a84242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wtM8sTfh2KcodPSZzuf4Tw.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">来源:马里乌斯·马萨拉尔在<a class="ae jg" href="https://unsplash.com/photos/CyFBmFEsytU" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="10b1" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>，<a class="ae ep" href="https://towardsai.net/p/category/editorial" rel="noopener ugc nofollow" target="_blank">社论</a></h2><div class=""/><div class=""><h2 id="54ea" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">使用自定义数据集通过TensorFlow训练更快的R-CNN进行对象检测的分步教程</h2></div><p id="f16c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">作者:</strong>布斯·任亚·泰京</p><div class="is it gp gr iu md"><a href="https://members.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">向着AI加入。通过成为会员，你不仅将支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><p id="0c2c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi ms translated"><span class="l mt mu mv bm mw mx my mz na di"> R </span>最近，对象检测从其当前状态继续发展，并且由于其技术，几乎可以在每个技术平台上找到它。无论是通过图像分类、识别，还是定位，这些都是基于<strong class="lj jt">物体检测</strong>。</p><p id="b966" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://news.towardsai.net/cnn" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">卷积神经网络</strong></a>(CNN)通过融入深度学习和计算机视觉方法，可以将许多物体识别和分类技术汇集在一起。在计算机视觉中，<strong class="lj jt">卷积神经网络，</strong>顾名思义，在数据集中的每个像素图像中应用一个卷积层。</p><p id="792a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于计算机视觉和其初级结构中的<a class="ae jg" href="https://news.towardsai.net/dl" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">深度学习</strong> </a>基础，CNN通过将我们指定的过滤器移动到图像上来逐步获得不同的输出层。</p><blockquote class="nb"><p id="0d90" class="nc nd jj bd ne nf ng nh ni nj nk mc dk translated">“我们可以建立一个更加光明的未来，在这个未来，人类可以利用人工智能的能力摆脱卑微的工作。”</p><p id="1cd5" class="nc nd jj bd ne nf ng nh ni nj nk mc dk translated">~吴恩达</p></blockquote><h1 id="618a" class="nl nm jj bd nn no np nq nr ns nt nu nv ky nw kz nx lb ny lc nz le oa lf ob oc bi translated">什么是更快的R-CNN？</h1><p id="2a66" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">基本上，卷积神经网络对样本图像执行基于像素的卷积过程。<strong class="lj jt">更快R-CNN </strong>，物体识别算法之一，是R-CNN <a class="ae jg" href="https://news.towardsai.net/dnn" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">网络类型</strong> </a>之一。R-CNN是基于区域的<a class="ae jg" href="https://news.towardsai.net/cnn" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> CNN </strong> </a>网络类型。</p><p id="e44a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">随着时间的推移，已经看到，即使是R-CNN或者甚至是快速R-CNN在性能和准确性方面也是不够的。为了忽略可能负面影响性能的原因，需要并获得执行更快的神经网络。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/519a01b22a6470fe8edff769a00df622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*cusH3c_WpvJKLe5v14pWEw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:快速R-CNN的架构。</figcaption></figure><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/5d8eee6e594644a5e80bfe94ef5c31a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8vU0XkZQpCTmM592YrD5kw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:更快的R-CNN的架构。</figcaption></figure><p id="adf2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">卷积层没有显著变化。在快速R-CNN网络中，区域建议值变成了具有更快R-CNN的区域建议网络。</p><p id="12e1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">快速RCNN是最常用于对象识别和图像识别的CNN网络之一，比RCNN和快速RCNN工作得更好。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/181b88b06918bac28a721eb6164fa956.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*Uz8sPx13AUs3dl4EAsB-AA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3:更快的R-CNN架构。</figcaption></figure><blockquote class="op oq or"><p id="3ee0" class="lh li os lj b lk ll kt lm ln lo kw lp ot lr ls lt ou lv lw lx ov lz ma mb mc im bi translated">更快的R-CNN是一种通过提取图像特征和最小化图像分析的噪声来实现比当前对象检测算法更好的准确性的方法。在各种各样的学习模型中，学习模型曾经是更快的RCNN Inception v3——Google开发的一种架构。</p></blockquote><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/7cfaa552771b5b119a7ed64959d59ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ka62yvUJviiIgaYpRZO0-g.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:典型的卷积神经网络(CNN)架构|来源:Wikimedia Commons，图片根据<a class="ae jg" href="https://en.wikipedia.org/wiki/en:Creative_Commons" rel="noopener ugc nofollow" target="_blank">Creative Commons</a><a class="ae jg" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" rel="noopener ugc nofollow" target="_blank">Attribution-Share like 4.0 International</a>许可进行许可。</figcaption></figure><h1 id="13f0" class="nl nm jj bd nn no np nq nr ns nt nu nv ky ow kz nx lb ox lc nz le oy lf ob oc bi translated">什么是TensorFlow对象检测API？</h1><p id="434c" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">TensorFlow的对象检测应用程序接口(API)用作创建深度学习神经网络的框架，该网络旨在解决对象检测问题。</p><p id="9dd7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于TensorFlow对象检测API，特定的数据集可以使用它在现成状态下包含的模型进行训练。此外，TensorFlow的工具是它所使用的COCO数据集的权重。可以使用现有模型，而不需要通过预先训练的模型重新学习。</p><p id="72ad" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">纵观其神经网络模型，在数据科学方面进行了很多研究，解决了<a class="ae jg" href="https://mld.ai/mldcmu" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">机器学习</strong> </a>中遇到的很多问题。对于计算机视觉专业的数据科学家和从事人工智能工作的人来说，这是一个宝藏。</p><h2 id="641d" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated"><strong class="ak">我们安装所需的库📣</strong></h2><ul class=""><li id="90a8" class="pk pl jj lj b lk od ln oe lq pm lu pn ly po mc pp pq pr ps bi translated">Protobuf 3.0.0</li><li id="0cfd" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">Python-tk</li><li id="4e8e" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">枕头1.0</li><li id="c55f" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">Lxml</li><li id="f93b" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">tf Slim(包含在<code class="fe py pz qa qb b">tensorflow/models/research/</code>结账中)</li><li id="fad2" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">Jupyter笔记本</li><li id="df3b" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">Matplotlib</li><li id="390a" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">张量流(&gt; = 1.12.0)</li><li id="67e5" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">Cython</li><li id="4153" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">上下文库2</li><li id="25be" class="pk pl jj lj b lk pt ln pu lq pv lu pw ly px mc pp pq pr ps bi translated">可可API</li></ul><p id="b44d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">📌TensorFlow提供的模型链接和用于物体检测的物体检测API文件可以在<a class="ae jg" href="https://github.com/tensorflow/models" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> Github </strong> </a>上找到。</p><h1 id="9936" class="nl nm jj bd nn no np nq nr ns nt nu nv ky ow kz nx lb ox lc nz le oy lf ob oc bi translated">用更快的R-CNN逐步训练</h1><figure class="oj ok ol om gt iv"><div class="bz fp l di"><div class="qc qd l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图5:执行一个更快的R-CNN。</figcaption></figure><h2 id="7e06" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤1:创建虚拟环境并在Anaconda中激活</h2><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="34e5" class="oz nm jj qb b gy qi qj l qk ql">(base) C:\Users\PCName&gt; conda create -n myenv python=3.6<br/>(base) C:\Users\PCName&gt; conda activate myenv<br/>(myenv)C:\Users\PCName&gt;</span></pre><p id="e871" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果在没有安装新的虚拟环境的情况下进行交易，将会造成困难，因为在基地可能会有不正确的下载。</p><p id="10a2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">步骤1.1:安装TensorFlow GPU </strong></p><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="334f" class="oz nm jj qb b gy qi qj l qk ql">conda install tensorflow-gpu==1.15.0</span></pre><p id="086a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">步骤1.2:在Anaconda提示符下安装协议缓冲编译器</strong></p><p id="8f53" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Protobuf文件是数据传输所需的协议文件。</p><blockquote class="op oq or"><p id="566a" class="lh li os lj b lk ll kt lm ln lo kw lp ot lr ls lt ou lv lw lx ov lz ma mb mc im bi translated">协议缓冲区是一种语言中立、平台中立的可扩展机制，用于序列化结构化数据。<br/> —谷歌开发者[7]</p></blockquote><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="8841" class="oz nm jj qb b gy qi qj l qk ql">conda install -c anaconda protobuf</span></pre><h2 id="d4ca" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤1.3:在安装了一个新的虚拟环境之后，从requirements.txt文件中加载它</h2><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="bb65" class="oz nm jj qb b gy qi qj l qk ql">#requirements.txt sample</span><span id="7f64" class="oz nm jj qb b gy qm qj l qk ql">pillow <br/>lxml <br/>jupyter <br/>matplotlib <br/>pandas <br/>opencv-python <br/>cython==0.28.1 <br/>tf_slim <br/>scipy</span></pre><p id="6345" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">可以使用pip install命令从终端安装所需的软件包，也可以使用以下命令下载并运行该文件。</p><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="6a18" class="oz nm jj qb b gy qi qj l qk ql">pip install -r requirements.txt</span></pre><h2 id="12b2" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤2:上传张量流模型文件</h2><p id="06d5" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">没有TensorFlow模型文件，就不可能使用对象检测API。因此，请确保文件已经从<a class="ae jg" href="https://github.com/tensorflow/models." rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">正确的位置</strong> </a>下载。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/a3001c1c57f18dc2b1dada30b39bddda.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*CgdQDzspzDlMErrsHAAnYw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7: TensorFlow模型目录。</figcaption></figure><p id="90f2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当文件夹中的数据从索引中取出时，它应该看起来像上面这样。</p><h2 id="bc0f" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤3:将更快的R-CNN盗梦V2模型放入对象检测文件夹</h2><p id="6294" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">在这一步中，我们将使用<strong class="lj jt"> Inception v2模型</strong>和更快的R-CNN神经网络，应该从存储库中取出并放入文件夹中。因为要激活的文件夹将是object_detection，所以它必须在此目录中。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/8cd9c3472bcf83eac4895b636a97ce6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*ONyIDllxNL4u_KGyrxvJuw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:将R-CNN inception v2模型放置在对象检测目录中。</figcaption></figure><p id="92d3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">注意:</strong>与物体检测相关的文件位于<strong class="lj jt"> object_detection </strong>文件夹中。</p><h2 id="8650" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤4:将PYTHON_PATH指定为系统环境变量</h2><p id="ea7f" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">model、research和object_detection目录的地址，包括下载的Protobuf文件，必须添加到现有的PYTHON_PATH中。</p><p id="e8b5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">添加路径有两种选择。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/870804fb0f88215167617b3c3558bd11.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*RVVhpAiOulIkFbz4BLKNnA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:手动添加到路径</figcaption></figure><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qq"><img src="../Images/b08ce3832f73a9df9089cf479b51bf66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-NTkgRmDeHchbj72YNxpA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:每次使用Set命令添加</figcaption></figure><h2 id="c738" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤5:运行研究文件夹中的协议文件</h2><p id="810a" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">请在这里完整地键入原型文件来运行。确保终端中的代码写在研究文件夹中。</p><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="e8da" class="oz nm jj qb b gy qi qj l qk ql">protoc --python_out=. .\object_detection\protos\anchor_generator.proto .\object_detection\protos\argmax_matcher.proto .\object_detection\protos\bipartite_matcher.proto .\object_detection\protos\box_coder.proto .\object_detection\protos\box_predictor.proto .\object_detection\protos\eval.proto .\object_detection\protos\faster_rcnn.proto  .\object_detection\protos\faster_rcnn_box_coder.proto .\object_detection\protos\flexible_grid_anchor_generator.proto .\object_detection\protos\calibration.proto .\object_detection\protos\hyperparams.proto .\object_detection\protos\image_resizer.proto .\object_detection\protos\input_reader.proto .\object_detection\protos\losses.proto .\object_detection\protos\matcher.proto .\object_detection\protos\mean_stddev_box_coder.proto .\object_detection\protos\model.proto .\object_detection\protos\optimizer.proto .\object_detection\protos\pipeline.proto .\object_detection\protos\post_processing.proto .\object_detection\protos\preprocessor.proto .\object_detection\protos\region_similarity_calculator.proto .\object_detection\protos\square_box_coder.proto .\object_detection\protos\ssd.proto .\object_detection\protos\ssd_anchor_generator.proto .\object_detection\protos\string_int_label_map.proto .\object_detection\protos\train.proto .\object_detection\protos\keypoint_box_coder.proto .\object_detection\protos\multiscale_anchor_generator.proto .\object_detection\protos\graph_rewriter.proto .\object_detection\protos\center_net.proto .\object_detection\protos\grid_anchor_generator.proto .\object_detection\protos\fpn.proto .\object_detection\protos\target_assigner.proto</span></pre><h2 id="6dc8" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤6:运行研究文件夹中的setup.py文件</h2><p id="33af" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">确保研究文件夹中的setup.py文件已编译并安装。</p><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="cb0c" class="oz nm jj qb b gy qi qj l qk ql">python setup.py build</span><span id="b86c" class="oz nm jj qb b gy qm qj l qk ql">python setup.py install</span></pre><h2 id="bb15" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤7:将准备好的数据集的XML文件转换为CSV</h2><p id="a4c5" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">可与TensorFlow对象检测API一起使用的数据集属于有限类型。由于PASCAL VOC的使用很普遍，所以将主要使用XML数据。图像和图像的位置标签同时存储在Pascal VOC数据中。</p><h2 id="d9a1" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">PASCAL VOC (PASCAL可视对象类挑战)</h2><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/8ebc9e2742f42f964b07c3a5e71bc8c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/0*DJNtW1ml0q-V42Ov.jpg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:来自PASCAL VOC公共数据集的对象检测|来源:Dong等人在代码为[ <a class="ae jg" href="https://paperswithcode.com/dataset/pascal-voc" rel="noopener ugc nofollow" target="_blank"> 8 </a>的论文中的公共数据集。</figcaption></figure><blockquote class="op oq or"><p id="d39d" class="lh li os lj b lk ll kt lm ln lo kw lp ot lr ls lt ou lv lw lx ov lz ma mb mc im bi translated">PASCAL视觉对象类(VOC) 2012数据集包含20个对象类别，包括车辆、家庭、动物和其他:飞机、自行车、船、公共汽车、汽车、摩托车、火车、瓶子、椅子、餐桌、盆栽植物、沙发、电视/显示器、鸟、猫、牛、狗、马、羊和人。该数据集中的每个图像都有像素级分割注释、边界框注释和对象类注释。<br/> —帕斯卡VOC [8]</p></blockquote><p id="3745" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在此步骤中，用于测试和训练文件夹的数据必须移动到<code class="fe py pz qa qb b">models/research/object_detection/</code> <strong class="lj jt"> </strong>图像目录。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/9aca99e0bd440d2225b558a5e2d83e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*Qyp54pqNNgVHwL2c0LhlDA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12: Object_detection文件夹。</figcaption></figure><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qt"><img src="../Images/0ad66efbdc31ca61ed5c44124c7bd8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*d3eaZtKlNHFS7nM3nnqHkQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图13:训练和测试CSV文件。</figcaption></figure><p id="6982" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">运行<code class="fe py pz qa qb b">models/research/object_detection directory</code>中显示的代码。在images文件夹中，将创建test_labels.csv和train_labels.csv CSV文件。</p><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="b2ba" class="oz nm jj qb b gy qi qj l qk ql">python xml_to_csv.py</span></pre><h2 id="dae2" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤8:在generate_tfrecord.py文件中输入类</h2><p id="e4d5" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">我们要使用的类名必须在这一步中指定。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qu"><img src="../Images/a328dc00d5aff47d06bab2b5506eebc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*9NMthGswCUNKVRN2DBCKrA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图14:为我们的TensorFlow记录输入类。</figcaption></figure><h2 id="4a28" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤9:为训练集和测试集运行Generate_tf_record.py文件</h2><p id="ff77" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">在<code class="fe py pz qa qb b">models/research/object_detection</code>目录下运行下面的代码。要创建Tensorflow记录，需要使用以下代码。</p><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="d075" class="oz nm jj qb b gy qi qj l qk ql">python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record</span><span id="f8a3" class="oz nm jj qb b gy qm qj l qk ql">python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record</span></pre><h2 id="8b2a" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤10:在models/research/object _ detection/training目录下创建一个名为labelmap.pbtxt的文件</h2><ul class=""><li id="c9da" class="pk pl jj lj b lk od ln oe lq pm lu pn ly po mc pp pq pr ps bi translated">这一步首先要做的是将文件<strong class="lj jt">faster _ rcnn _ inception _ v2 _ pets . config和graph.pbtxt </strong>移动到<code class="fe py pz qa qb b">models/research/object_detection/training</code>目录中。</li></ul><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/3b5c2433754f04538abe845852f04266.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*lZvQ-1SgCixn7d6TeBXpHg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图15:将文件移动到正确的目录中。</figcaption></figure><ul class=""><li id="60ba" class="pk pl jj lj b lk ll ln lo lq qw lu qx ly qy mc pp pq pr ps bi translated">在执行generate_tf_record文件后，将自动创建一个标签映射。可以通过转到如下文件夹中的标签图来控制。</li></ul><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qz"><img src="../Images/31d508809c67639e8b3d4af1b22ebb4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*kOwNAIly3iQKSwKWOa0VbA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图TensorFlow记录文件的输出。</figcaption></figure><p id="047c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<code class="fe py pz qa qb b">models/research/object_detection/training</code>目录下，更改fast _ rcnn _ inception _ v2 _ pets . config中的行，如下所示。</p><p id="8e36" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">(5类样本变化)</strong></p><ul class=""><li id="779f" class="pk pl jj lj b lk ll ln lo lq qw lu qx ly qy mc pp pq pr ps bi translated"><strong class="lj jt">第9行:</strong> #写班级人数</li></ul><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi ra"><img src="../Images/51c5482d61dab53fcbb1365cc31dc829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*7xqEFRvJ1VAoxcNrrzdoWw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图17:用我们更快的R-CNN写类计数。</figcaption></figure><ul class=""><li id="5475" class="pk pl jj lj b lk ll ln lo lq qw lu qx ly qy mc pp pq pr ps bi translated"><strong class="lj jt">第106行:</strong>写微调检查点</li></ul><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi rb"><img src="../Images/7b870c072fae03267b32095c98fd8f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0TZ5m6APQg1YUzVsSpGcg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图18:编写我们的检查点。</figcaption></figure><ul class=""><li id="8ca5" class="pk pl jj lj b lk ll ln lo lq qw lu qx ly qy mc pp pq pr ps bi translated"><strong class="lj jt">第123行:</strong>写入train.record路径的输入路径</li></ul><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi gj"><img src="../Images/0be4e2941595be8792afd863dbdf8bae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-uxI2gxzoDmosF47v0xTA.png"/></div></div></figure><ul class=""><li id="8f8e" class="pk pl jj lj b lk ll ln lo lq qw lu qx ly qy mc pp pq pr ps bi translated"><strong class="lj jt">第130行:</strong>将测试图像的数量写入<code class="fe py pz qa qb b">models/research/images/test</code>文件夹</li></ul><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi rc"><img src="../Images/fa4dfa5933c2a5377832ff41c8284ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*6qWojFRDYn0JNFMBgT4_xw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图19:统计我们的图像。</figcaption></figure><ul class=""><li id="469a" class="pk pl jj lj b lk ll ln lo lq qw lu qx ly qy mc pp pq pr ps bi translated"><strong class="lj jt">第135行:</strong>写test.record路径的输入路径</li></ul><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi gj"><img src="../Images/16c28b89ef9b2390c546218fac7bc384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yUwmYYmW0znPPmxwDBp9Vg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图20:编写测试记录的输入路径。</figcaption></figure><ul class=""><li id="b33b" class="pk pl jj lj b lk ll ln lo lq qw lu qx ly qy mc pp pq pr ps bi translated"><strong class="lj jt">第137行:</strong>编写我们的标签映射路径</li></ul><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi rd"><img src="../Images/227c64a99703af3920571677ea50e4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RdOOOawAJG9lZKUSu1xu0g.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图21:编写我们的标签映射路径。</figcaption></figure><h2 id="7515" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">第十一步。使用train.py文件训练模型</h2><p id="3d7a" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">训练前的<code class="fe py pz qa qb b">models/research/object_detection/inference_graph</code>文件夹必须是空的。从开始训练开始，在<code class="fe py pz qa qb b">models/research/object_detection</code>目录下运行下面的命令。</p><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="3daa" class="oz nm jj qb b gy qi qj l qk ql">python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config</span></pre><p id="d6b7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="os">当培训开始时，我们将在终端看到以下内容；</em></p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi re"><img src="../Images/663ff51a21ce976a5d0c629d4f9b09d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJAfJqjQ--lM-Y1j1e95Uw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图22:我们模型训练的开始。</figcaption></figure><h2 id="9fcb" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated">步骤12:使用export_inference_graph.py测试结果的推理图阶段</h2><p id="c1b1" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">在命令的XXXX部分写下在推理图目录中创建的最后一个<code class="fe py pz qa qb b">model.ckpt</code>模型的编号。例如，由于训练而创建的模型的编号可以被分配为“0”，如下所示。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi rf"><img src="../Images/9b09720420e4e5258209bb6abeeca168.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*XyNBau0dtyKiE0CXQbBoVA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图23:推理图阶段。</figcaption></figure><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="f961" class="oz nm jj qb b gy qi qj l qk ql">python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_inception_v2_pets.config --trained_checkpoint_prefix training/model.ckpt-XXXX --output_directory inference_graph</span></pre><p id="ff8d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在编写Python代码时，当在终端中键入idle时，会出现idle屏幕。</p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi rg"><img src="../Images/b105aa071b0c9fcf9cf3daac6ed73e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-HpNX3gBWHkA4R2zsILfEA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图24:空闲屏幕。</figcaption></figure><p id="0456" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">选择文件/打开…从左上角的闲置。从打开的屏幕中选择目录<code class="fe py pz qa qb b">models/research/object_detection</code>中的Object_detection_image.py文件。</p><p id="7a73" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里有两个选择。我们可以在object_detection_image.py文件中设置测试文件夹的路径，也可以把我们要测试的图像移到<code class="fe py pz qa qb b">models/research/object_detection</code>目录下。</p><h2 id="32ff" class="oz nm jj bd nn pa pb dn nr pc pd dp nv lq pe pf nx lu pg ph nz ly pi pj ob jp bi translated"><strong class="ak">提示🗝️ </strong></h2><p id="7aac" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">这里，通过将所需图像移动到测试<code class="fe py pz qa qb b">models/research/object_detection</code>目录来执行操作。</p><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="5e06" class="oz nm jj qb b gy qi qj l qk ql">IMAGE_NAME = ‘yourtestimagename.JPG’</span></pre><p id="9215" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">写下物体探测器可以识别的类别数量。</p><pre class="oj ok ol om gt qe qb qf qg aw qh bi"><span id="96a3" class="oz nm jj qb b gy qi qj l qk ql">NUM_CLASSES = 1</span></pre><p id="195b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">按F5运行模块:</strong></p><figure class="oj ok ol om gt iv gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/2e4333bebfae187d48c3ee0058f30ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*0TfeNsYgGKajxm2eRsUhDw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图25:运行模块后。来源:PASCAL视觉对象类挑战，公共数据集[ <a class="ae jg" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2010/" rel="noopener ugc nofollow" target="_blank"> 9 </a> ]</figcaption></figure></div><div class="ab cl rh ri hx rj" role="separator"><span class="rk bw bk rl rm rn"/><span class="rk bw bk rl rm rn"/><span class="rk bw bk rl rm"/></div><div class="im in io ip iq"><p id="af4a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">免责声明:</strong>本文所表达的观点均为作者个人观点，不代表与作者(直接或间接)相关的任何公司的观点。这项工作并不打算成为最终产品，而是当前思想的反映，同时也是讨论和改进的催化剂。</p><p id="a724" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">除非另有说明，所有图片均来自作者。</strong></p><p id="57f9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">经由<strong class="lj jt">发表走向</strong>艾</a></p><h1 id="e390" class="nl nm jj bd nn no np nq nr ns nt nu nv ky ow kz nx lb ox lc nz le oy lf ob oc bi translated">进一步阅读</h1><div class="is it gp gr iu md"><a rel="noopener  ugc nofollow" target="_blank" href="/support-vector-machine-svm-introduction-machine-learning-8c56b7da63f1"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">支持向量机(SVM)简介—机器学习</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">介绍机器学习中的支持向量机(SVM ),以及它们在有监督的最大似然学习中的作用，并举例说明</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">pub.towardsai.net</p></div></div><div class="mm l"><div class="ro l mo mp mq mm mr ja md"/></div></div></a></div><div class="is it gp gr iu md"><a rel="noopener  ugc nofollow" target="_blank" href="/diving-into-data-pipelines-b2eb1b8a4923"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">深入数据管道——数据工程的基础</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">数据管道介绍。如何，什么，什么时候，为什么？</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">pub.towardsai.net</p></div></div><div class="mm l"><div class="rp l mo mp mq mm mr ja md"/></div></div></a></div><div class="is it gp gr iu md"><a rel="noopener  ugc nofollow" target="_blank" href="/k-nearest-neighbors-knn-algorithm-tutorial-machine-learning-basics-ml-ec6756d3e0ac"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">k近邻(KNN)算法教程—机器学习基础</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">潜入K-最近邻，一个基本的经典机器学习(ML)算法</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">pub.towardsai.net</p></div></div><div class="mm l"><div class="rq l mo mp mq mm mr ja md"/></div></div></a></div><h1 id="f22c" class="nl nm jj bd nn no np nq nr ns nt nu nv ky ow kz nx lb ox lc nz le oy lf ob oc bi translated">参考</h1><p id="291f" class="pw-post-body-paragraph lh li jj lj b lk od kt lm ln oe kw lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">[1]“更快的R-CNN:下现代物体探测的兔子洞| Tryolabs博客”。2021.Tryolabs.Com。<a class="ae jg" href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/." rel="noopener ugc nofollow" target="_blank">https://tryo labs . com/blog/2018/01/18/faster-r-CNN-down-the-rabbit-hole-of-modern-object-detection/。</a></p><p id="b6be" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]任、、、何、罗斯·吉希克和。2015.“更快的R-CNN:用区域提议网络实现实时目标检测”。Arxiv.Org。https://arxiv.org/abs/1506.01497.<a class="ae jg" href="https://arxiv.org/abs/1506.01497." rel="noopener ugc nofollow" target="_blank"/></p><p id="5387" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3]《少清人/更快_Rcnn》。2021.Github。【https://github.com/ShaoqingRen/faster_rcnn. T4】</p><p id="cba7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4]“在自己的数据集上训练一个Tensorflow更快的R-CNN物体检测模型”。2020.Roboflow博客。<a class="ae jg" href="https://blog.roboflow.com/training-a-tensorflow-faster-r-cnn-object-detection-model-on-your-own-dataset/." rel="noopener ugc nofollow" target="_blank">https://blog . robo flow . com/training-a-tensor flow-faster-r-CNN-object-detection-model-on-your-own-dataset/。</a></p><p id="bb6c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]《张量流导论》。2021.张量流。<a class="ae jg" href="https://www.tensorflow.org/learn." rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/learn.</a></p><p id="764f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6]“tensor flow—维基百科”。2021.En.Wikipedia.Org。<a class="ae jg" href="https://en.wikipedia.org/wiki/TensorFlow." rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/TensorFlow.</a></p><p id="92f2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[7]“协议缓冲区|谷歌开发者”。2021.谷歌开发者。<a class="ae jg" href="https://developers.google.com/protocol-buffers." rel="noopener ugc nofollow" target="_blank">https://developers.google.com/protocol-buffers.</a></p><p id="e3b9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[8]《带代码的论文——PASCAL VOC数据集》。2021.Paperswithcode.Com。<a class="ae jg" href="https://paperswithcode.com/dataset/pascal-voc." rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com/dataset/pascal-voc.</a></p><p id="3081" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[9]“2010年帕斯卡视觉对象类挑战赛(VOC2010)”。2021.host . robots . ox . AC . uk<a class="ae jg" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2010/." rel="noopener ugc nofollow" target="_blank">http://host.robots.ox.ac.uk/pascal/VOC/voc2010/.</a></p></div><div class="ab cl rh ri hx rj" role="separator"><span class="rk bw bk rl rm rn"/><span class="rk bw bk rl rm rn"/><span class="rk bw bk rl rm"/></div><div class="im in io ip iq"><div class="oj ok ol om gt md"><a href="https://ws.towardsai.net/shop" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">店铺↓ |走向AI</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">发布最好的技术、科学和工程|社论→https://towardsai.net/p/editorial |订阅→…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">ws.towardsai.net</p></div></div><div class="mm l"><div class="rr l mo mp mq mm mr ja md"/></div></div></a></div><div class="is it gp gr iu md"><a href="https://members.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">向着AI加入。通过成为会员，你不仅将支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><div class="is it gp gr iu md"><a href="https://sponsors.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">赞助商|了解如何成为面向人工智能的赞助商</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">无论你是想以一种吸引读者的方式突出你的产品，吸引高度相关的利基受众，还是…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">sponsors.towardsai.net</p></div></div><div class="mm l"><div class="rs l mo mp mq mm mr ja md"/></div></div></a></div></div></div>    
</body>
</html>