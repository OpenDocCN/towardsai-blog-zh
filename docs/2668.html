<html>
<head>
<title>Random Forest for Binary Classification: Hands-On with Scikit-Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于二进制分类的随机森林:Scikit-Learn实践</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/random-forest-for-binary-classification-hands-on-with-scikit-learn-4909e29c29f5?source=collection_archive---------0-----------------------#2022-04-08">https://pub.towardsai.net/random-forest-for-binary-classification-hands-on-with-scikit-learn-4909e29c29f5?source=collection_archive---------0-----------------------#2022-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ca08" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python和Google Colab</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6ebfbf69829636d3d104973f5b745849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3xWftHfm4aXmUSK2FUAHw.png"/></div></div></figure><p id="783e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">随机森林算法属于集成决策树的一个子组。如果你想了解更多关于决策树的知识，可以在这里阅读我之前的文章<a class="ae lq" rel="noopener ugc nofollow" target="_blank" href="/decision-and-classification-tree-cart-for-binary-classification-hands-on-with-scikit-learn-b59474b2c039">。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lr"><img src="../Images/facabe5f89be73eeaab7ffb42ce4025a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cokhywfrwh2WE-m5gGbNxw.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk translated"><a class="ae lq" rel="noopener ugc nofollow" target="_blank" href="/decision-and-classification-tree-cart-for-binary-classification-hands-on-with-scikit-learn-b59474b2c039">https://pub . toward sai . net/decision-and-class ification-tree-cart-for-binary-class ification-hands-on-with-sci kit-learn-b 59474 B2C 039</a></figcaption></figure><p id="d2e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在一个随机森林模型中，多个D <a class="ae lq" rel="noopener ugc nofollow" target="_blank" href="/decision-and-classification-tree-cart-for-binary-classification-hands-on-with-scikit-learn-b59474b2c039">决策树</a>被构建和组合，这产生了一个随机森林树(通常是一个更加精确的决策树)。在生长树时，随机森林方法以随机方式搜索下一个节点(或要素)，这增加了创建的不同树的数量。当树被组合时，它导致模型不会过度拟合(因为特征是随机搜索的，而不仅仅是包括最佳特征)，同时保持分类精度。</p><p id="993c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最好的理解方式就是做自己的项目。如果您有一个数据库，您可以按照本教程进行操作，只需更改必要的代码以包含您的数据集名称和要素名称。如果您想尝试使用样本数据，可以从本文使用的<a class="ae lq" href="https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载<a class="ae lq" href="https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset" rel="noopener ugc nofollow" target="_blank"> breast-cancer.csv </a>数据集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/2b8260b3688f23ff785ad3955b5b9c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Str_y-vF_np77pfdF2edeQ.png"/></div></div></figure><p id="c8be" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该数据集由569个细胞损伤的观察结果组成，其特征为30个特征。结果就是诊断，其中0代表良性，1代表恶性。在我们的项目中，我们将使用随机森林算法来预测基于30个可用特征的结果。</p><p id="ff58" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了启动项目，我们将导入必要的库:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="1f52" class="mc md it ly b gy me mf l mg mh">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.tree import export_graphviz<br/>import pydot<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span></pre><p id="94d1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们需要将数据库上传并加载到我们的Google Colab会话中:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="86a0" class="mc md it ly b gy me mf l mg mh">df = pd.read_csv('/content/breast-cancer.csv')<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/03ccc34f096caf42c75ee99dd74034ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-kaGTXPSOlmEtBv5mIC_g.png"/></div></div></figure><p id="95b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们有一个大表，有31列，1个输出分类器(良性0，恶性1)和30个特征。这30个变量使得很难一眼就解释和可视化数据集。所以我们将进行一个非常简单的探索性数据分析:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="d0d0" class="mc md it ly b gy me mf l mg mh">df.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/6e2417d19ce8d728c96e15f984ed62ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*juqKVotPTLYZOjsJtOnVEA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/6465051102adf0e54cefeb599351d15e.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*sdA_jn3hS86amskXNsB4tQ.png"/></div></figure><p id="2b2c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了直观地观察这两个结果在我们的数据集中是如何分布的，我们可以建立一个配对图。这将提示我们有多少数据被清楚地分为两类。请注意，为包含30列的数据集构建配对图可能需要一些时间。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="aaaf" class="mc md it ly b gy me mf l mg mh"><strong class="ly iu">#Exploring dataset:</strong><br/>sns.pairplot(df, kind="scatter", hue="diagnosis")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/dc5ac79d53e8d94f02ade8d65ddb93f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zEk7hVi9EIsxix87G39dFw.png"/></div></div></figure><p id="d896" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在30个特征中，发现它们之间的多重共线性是可以预期的。我们将检查变量之间是如何相互关联的，但是，我们不会在本文中探索任何降维技术。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="b118" class="mc md it ly b gy me mf l mg mh"><strong class="ly iu">#Drop the outcome:<br/></strong>df_corr = df.drop(["diagnosis"], axis=1)</span><span id="2773" class="mc md it ly b gy mm mf l mg mh"><strong class="ly iu">#Build heatmap:<br/></strong>corr_matrix=df_corr.corr()<br/>mask = np.zeros_like(corr_matrix)<br/>mask[np.triu_indices_from(mask)] = True<br/>sns.heatmap(corr_matrix, mask=mask, square=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/980a9dc5d3867f1d9f38522e39b08e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*isXkCGGugk9flFys74fj0w.png"/></div></figure><p id="70f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下一步是探索这两个类中的变量分布。因为我们有30个变量要研究，所以我们将打印一个包含所有列名的列表，以便于下一步构建分布图。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="7ce8" class="mc md it ly b gy me mf l mg mh"><strong class="ly iu">#Get the full list of column names:<br/></strong>for col in df.columns:<br/>    print(col)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/39880b49229cfbd20a0ff0866348ca45.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*7kJM7cB_sYiLc4I3UMMUvw.png"/></div></div></figure><p id="2af4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们可以构建直方图。我们将使用6行5列在同一幅图像中绘制30个直方图:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="806d" class="mc md it ly b gy me mf l mg mh"><strong class="ly iu">#Define an output image with 6 lines and 5 columns, with inside image size of 20x25:</strong> <br/>fig, axs = plt.subplots(6, 5, figsize=(20, 25))</span><span id="4324" class="mc md it ly b gy mm mf l mg mh"><strong class="ly iu">#Build the 30 histograms:<br/></strong>sns.histplot(data=df, x="radius_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[0, 0])<br/>sns.histplot(data=df, x="texture_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[0, 1])<br/>sns.histplot(data=df, x="perimeter_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[0, 2])<br/>sns.histplot(data=df, x="area_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[0, 3])<br/>sns.histplot(data=df, x="smoothness_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[0, 4])<br/>sns.histplot(data=df, x="compactness_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[1, 0])<br/>sns.histplot(data=df, x="concavity_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[1, 1])<br/>sns.histplot(data=df, x="concave points_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[1, 2])<br/>sns.histplot(data=df, x="symmetry_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[1, 3])<br/>sns.histplot(data=df, x="fractal_dimension_mean", hue="diagnosis", kde=True, color="skyblue", ax=axs[1, 4])<br/>sns.histplot(data=df, x="radius_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[2, 0])<br/>sns.histplot(data=df, x="texture_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[2, 1])<br/>sns.histplot(data=df, x="perimeter_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[2, 2])<br/>sns.histplot(data=df, x="area_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[2, 3])<br/>sns.histplot(data=df, x="smoothness_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[2, 4])<br/>sns.histplot(data=df, x="compactness_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[3, 0])<br/>sns.histplot(data=df, x="concavity_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[3, 1])<br/>sns.histplot(data=df, x="concave points_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[3, 2])<br/>sns.histplot(data=df, x="symmetry_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[3, 3])<br/>sns.histplot(data=df, x="fractal_dimension_se", hue="diagnosis", kde=True, color="skyblue", ax=axs[3, 4])<br/>sns.histplot(data=df, x="radius_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[4, 0])<br/>sns.histplot(data=df, x="texture_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[4, 1])<br/>sns.histplot(data=df, x="perimeter_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[4, 2])<br/>sns.histplot(data=df, x="area_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[4, 3])<br/>sns.histplot(data=df, x="smoothness_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[4, 4])<br/>sns.histplot(data=df, x="compactness_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[5, 0])<br/>sns.histplot(data=df, x="concavity_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[5, 1])<br/>sns.histplot(data=df, x="concave points_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[5, 2])<br/>sns.histplot(data=df, x="symmetry_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[5, 3])<br/>sns.histplot(data=df, x="fractal_dimension_worst", hue="diagnosis", kde=True, color="skyblue", ax=axs[5, 4])</span><span id="f542" class="mc md it ly b gy mm mf l mg mh"><strong class="ly iu">#Print the final result:<br/></strong>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/b79c5b29173731bb7727e66a7e0b4b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSs1MrvoWTAPkBwTt_d12w.png"/></div></div></figure><p id="a8f5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以将相同的技术用于箱线图:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="4a5d" class="mc md it ly b gy me mf l mg mh">fig, axs = plt.subplots(6, 5, figsize=(20, 25))</span><span id="03d5" class="mc md it ly b gy mm mf l mg mh">sns.boxplot(x=df["diagnosis"], y=df["radius_mean"], ax=axs[0, 0])<br/>sns.boxplot(x=df["diagnosis"], y=df["texture_mean"], ax=axs[0, 1])<br/>sns.boxplot(x=df["diagnosis"], y=df["perimeter_mean"], ax=axs[0, 2])<br/>sns.boxplot(x=df["diagnosis"], y=df["area_mean"],ax=axs[0, 3])<br/>sns.boxplot(x=df["diagnosis"], y=df["smoothness_mean"], ax=axs[0, 4])<br/>sns.boxplot(x=df["diagnosis"], y=df["compactness_mean"], ax=axs[1, 0])<br/>sns.boxplot(x=df["diagnosis"], y=df["concavity_mean"], ax=axs[1, 1])<br/>sns.boxplot(x=df["diagnosis"], y=df["concave points_mean"], ax=axs[1, 2])<br/>sns.boxplot(x=df["diagnosis"], y=df["symmetry_mean"], ax=axs[1, 3])<br/>sns.boxplot(x=df["diagnosis"], y=df["fractal_dimension_mean"],ax=axs[1, 4])<br/>sns.boxplot(x=df["diagnosis"], y=df["radius_se"], ax=axs[2, 0])<br/>sns.boxplot(x=df["diagnosis"], y=df["texture_se"], ax=axs[2, 1])<br/>sns.boxplot(x=df["diagnosis"], y=df["perimeter_se"], ax=axs[2, 2])<br/>sns.boxplot(x=df["diagnosis"], y=df["area_se"], ax=axs[2, 3])<br/>sns.boxplot(x=df["diagnosis"], y=df["smoothness_se"], ax=axs[2, 4])<br/>sns.boxplot(x=df["diagnosis"], y=df["compactness_se"],ax=axs[3, 0])<br/>sns.boxplot(x=df["diagnosis"], y=df["concavity_se"], ax=axs[3, 1])<br/>sns.boxplot(x=df["diagnosis"], y=df["concave points_se"], ax=axs[3, 2])<br/>sns.boxplot(x=df["diagnosis"], y=df["symmetry_se"], ax=axs[3, 3])<br/>sns.boxplot(x=df["diagnosis"], y=df["fractal_dimension_se"], ax=axs[3, 4])<br/>sns.boxplot(x=df["diagnosis"], y=df["radius_worst"], ax=axs[4, 0])<br/>sns.boxplot(x=df["diagnosis"], y=df["texture_worst"],ax=axs[4, 1])<br/>sns.boxplot(x=df["diagnosis"], y=df["perimeter_worst"], ax=axs[4, 2])<br/>sns.boxplot(x=df["diagnosis"], y=df["area_worst"], ax=axs[4, 3])<br/>sns.boxplot(x=df["diagnosis"], y=df["smoothness_worst"], ax=axs[4, 4])<br/>sns.boxplot(x=df["diagnosis"], y=df["compactness_worst"], ax=axs[5, 0])<br/>sns.boxplot(x=df["diagnosis"], y=df["concavity_worst"], ax=axs[5, 1])<br/>sns.boxplot(x=df["diagnosis"], y=df["concave points_worst"],ax=axs[5, 2])<br/>sns.boxplot(x=df["diagnosis"], y=df["symmetry_worst"], ax=axs[5, 3])<br/>sns.boxplot(x=df["diagnosis"], y=df["fractal_dimension_worst"], ax=axs[5, 4])</span><span id="a064" class="mc md it ly b gy mm mf l mg mh">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/6147861a70e407a62022c479727c5dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cr9RpI_ECEeN3cpxzbKDpw.png"/></div></div></figure><p id="c2a8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们所看到的，一些变量在区分这两个类方面表现很好，因此我们可以期待一个具有良好准确性的模型。现在是时候建立我们的模型了:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="2b38" class="mc md it ly b gy me mf l mg mh"><strong class="ly iu">#Define the features and output:</strong><br/>y = np.array(df['diagnosis'])<br/>X = df.drop('diagnosis', axis=1)</span><span id="1e35" class="mc md it ly b gy mm mf l mg mh"><strong class="ly iu">#Split data into train an test, with test size of 20%:</strong><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)</span><span id="ffa1" class="mc md it ly b gy mm mf l mg mh"><strong class="ly iu">#Build the model:</strong><br/>rf = RandomForestClassifier()<br/>rf.fit(X_train, y_train)</span><span id="7c95" class="mc md it ly b gy mm mf l mg mh"><strong class="ly iu">#Evaluate the model:<br/></strong>print("accuracy on training set: %f" % rf.score(X_train, y_train))<br/>print("accuracy on test set: %f" % rf.score(X_test, y_test))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/23f934d48ba35e5a2f70ca51ef2649b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*dEnXc1IwFFdy66wXLDnsVw.png"/></div></figure><p id="4516" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当我们最终有了我们的模型，类似于决策树，就有可能评估特征的重要性，并因此选择要包含的特征的数量来构建保持准确性的简单模型。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="e45b" class="mc md it ly b gy me mf l mg mh"><strong class="ly iu">#Print features importance:</strong><br/>rf.feature_importances_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/bc2868078cc0cd6df43d5a64addce07b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*qWB3-cE_K7gHYJYIRQNzLA.png"/></div></figure><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="13ae" class="mc md it ly b gy me mf l mg mh"><strong class="ly iu">#Build a graphical visualisation of feature importance:<br/></strong>import matplotlib.pyplot as plt<br/>plt.plot(rf.feature_importances_, 'o')<br/>plt.xticks(range(X.shape[1]), X.columns, rotation=90)<br/>plt.ylim(0, 1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/79cbe59d1e29abd66e8e7a1599d49c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*KnU7PoGlwedMToxz_40hfA.png"/></div></figure><p id="0d7f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们所看到的，大约5个特性比所有其他特性更重要，因此我们将构建一个只有5个节点的树:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="2850" class="mc md it ly b gy me mf l mg mh"><strong class="ly iu">#Using only 5 nodes:<br/></strong>rf_5 = RandomForestClassifier(max_depth=5)<br/>rf_5.fit(X_train, y_train)</span><span id="1a56" class="mc md it ly b gy mm mf l mg mh">print("accuracy on training set: %f" % rf_5.score(X_train, y_train))print("accuracy on test set: %f" % rf_5.score(X_test, y_test))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/dd08e5882137f6ccaff4ac9a2ea0d6fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*Zp6Lyxx2W23AOkFeGfs50Q.png"/></div></figure><p id="beb4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了构建具有5个节点的树的图形可视化，我们使用Graphviz函数:</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="ad28" class="mc md it ly b gy me mf l mg mh"><strong class="ly iu"># Pull out one tree from the forest:</strong><br/>tree = rf_5.estimators_[5]</span><span id="d4ac" class="mc md it ly b gy mm mf l mg mh"><strong class="ly iu"># Export the image to a dot file:<br/></strong>export_graphviz(tree, out_file = 'tree.dot', feature_names = X.columns, rounded = True, precision = 1)</span><span id="b2d3" class="mc md it ly b gy mm mf l mg mh"><strong class="ly iu"># Use dot file to create a graph:<br/></strong>(graph, ) = pydot.graph_from_dot_file('tree.dot')</span><span id="291f" class="mc md it ly b gy mm mf l mg mh"><strong class="ly iu"># Write graph to a png file:<br/></strong>graph.write_png('tree.png')</span></pre><p id="e795" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">运行代码后，您会发现<strong class="kw iu">。png </strong>文件放在你的数据库的同一个目录下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6ebfbf69829636d3d104973f5b745849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3xWftHfm4aXmUSK2FUAHw.png"/></div></div></figure><p id="aa86" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在构建随机森林时要记住的一件事是准确性的可变性，因为树是随机构建的，并且预期可变性很小(通常小于2%)。</p><p id="5acf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感谢您的阅读！如果您对本教程有任何建议，请告诉我，不要忘记订阅以接收关于我未来出版物的通知。</p><p id="b0ab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">如果:</strong>你喜欢这篇文章，别忘了关注我，这样你就能收到关于新出版物的所有更新。</p><p id="262e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">其他如果:</strong>你想在Medium上阅读更多，可以通过<a class="ae lq" href="https://cdanielaam.medium.com/membership" rel="noopener"> <strong class="kw iu">我的推荐链接</strong> </a>订阅Medium会员。它不会花你更多的钱，但会支付我一杯咖啡。</p><p id="595b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">其他:</strong>谢谢！</p></div></div>    
</body>
</html>