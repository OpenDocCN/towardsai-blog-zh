<html>
<head>
<title>So retrieval is what we needed?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所以我们需要的是恢复？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/so-retrieval-is-what-we-needed-9485e4f9e939?source=collection_archive---------0-----------------------#2022-01-09">https://pub.towardsai.net/so-retrieval-is-what-we-needed-9485e4f9e939?source=collection_archive---------0-----------------------#2022-01-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="63f5" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><p id="d0ba" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">上个月，DeepMind发布了他们的新NLP模型，名为<a class="ae ku" href="https://arxiv.org/pdf/2112.04426.pdf" rel="noopener ugc nofollow" target="_blank"> RETRO </a>(检索增强变压器)，根据该论文，这是NLP世界在多个方面的一次飞跃。一个值得注意的是，虽然这个模型取得了与SOTA建筑相当的结果(例如，<a class="ae ku" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>)，但它比<a class="ae ku" href="https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf" rel="noopener ugc nofollow" target="_blank"> AI21侏罗纪-1 </a>的178B参数小25倍，只有7.5B参数。</p><p id="11ba" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这打破了越大的模型意味着越准确的假设。</p><p id="0528" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">较小模型的主要优势在于，它们意味着更短的训练时间，因此计算时间和能源更少，总体碳足迹更小。客观地说，在一个NVIDIA Tesla V100 GPU上训练GPT-3需要355年，估计计算时间成本约为500万美元(训练GPT-3 OpenAI使用1024个A100 GPU，仍然达到约34天的天文训练时间)。就碳而言，它在训练期间产生了相当于85，000公斤的二氧化碳！这相当于驾驶120辆汽车一整年产生的二氧化碳量！除此之外，由于其高昂的训练成本，定期重新训练大型语言模型以纳入新的数据、语言、规范，或者在公平和偏见方面更新模型是极其昂贵的。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi kv"><img src="../Images/ea703dec495880ed0a49695cb5ca6985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*7oePLzdXEb9LiEsz"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源: S &amp; P全球</figcaption></figure><p id="57b9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">较小模型的另一个优势是，虽然大型语言模型在与从网络或其他来源收集的大型训练数据集结合时可以完美地记住它们的部分训练数据，从而带来隐私和安全问题，但较小的模型不太容易出现这种情况，<em class="li">如</em> <a class="ae ku" href="https://aclanthology.org/2020.findings-emnlp.301/" rel="noopener ugc nofollow" target="_blank"> <em class="li"> Gehman等人(2020) </em> </a> <em class="li">所示。</em> <a class="ae ku" href="https://dl.acm.org/doi/10.1145/3442188.3445922" rel="noopener ugc nofollow" target="_blank"> <em class="li">本德等人(2021) </em> </a></p><h2 id="6795" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kh ls lt lu kl lv lw lx kp ly lz ma iw bi translated">那么，为什么近年来模特的尺寸增加了这么多呢？</h2><p id="a1e0" class="pw-post-body-paragraph jw jx iq jy b jz mb kb kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt ij bi translated">直到最近，当我们谈到NLU模型时，我们分析了文本背后的语言逻辑，并做出了相应的回应。近年来，我们正在尝试(并希望)解决更多知识密集型任务，如问答，为此我们需要将更多关于世界的知识集成到模型中。问答任务的一个例子可能是— <em class="li">电影《黑客帝国》是什么时候发行的？</em></p><p id="2082" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">利用语言学，我们只能理解我们正在搜索一个日期而不是一个名字，但是如果我们对得到正确的答案感兴趣，模型需要在某个地方存储这些知识。增长的另一个原因是计算能力的可用性越来越高，GPU处理器的进步明确允许模型和所需训练周期的快速增长。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h2 id="6449" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kh ls lt lu kl lv lw lx kp ly lz ma iw bi translated"><strong class="ak">检索哲学</strong></h2><p id="27f3" class="pw-post-body-paragraph jw jx iq jy b jz mb kb kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt ij bi translated">不用增加模型的大小和训练更多的数据，而是让模型能够在执行预测时直接访问大型数据库，这是一种半参数方法。</p><p id="dbd5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">与检索方法最相似的是开卷考试和闭卷考试的区别。</p><p id="9db4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">而在闭卷考试中，人们需要学习和记忆所有的考试材料。在开卷考试中，你只需要理解材料的本质，在哪里寻找正确的答案，以及如何从给定的材料中提取正确的答案。</p><p id="ee70" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">检索的概念(在NLP模型中)并不新鲜，在几篇论文中已经对NLU模型提出了建议，包括<a class="ae ku" href="https://openreview.net/forum?id=B184E5qee" rel="noopener ugc nofollow" target="_blank"> Grave等人</a>的“使用连续缓存改进神经语言模型”和从<a class="ae ku" href="https://arxiv.org/abs/2005.11401" rel="noopener ugc nofollow" target="_blank"> Meta-AI </a>或机器翻译工作中基于源句子之间的编辑距离检索翻译对并使用最接近的检索目标句子指导翻译输出的检索增强生成，如<a class="ae ku" href="https://arxiv.org/abs/1804.02559" rel="noopener ugc nofollow" target="_blank">张等人(2018) </a>和<a class="ae ku" href="https://www.aaai.org/GuideBook2018/17282-74380-GB.pdf" rel="noopener ugc nofollow" target="_blank">顾等人(2018) </a></p><p id="c88d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">下面我重点介绍DeepMind实现的检索机制，据我(和他们)所知，这是第一个展示将检索数据库扩展到数万亿个大型参数语言模型的好处的工作。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/2f308f685e35c95eee38889231fef5b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*j3arAUSnJZXe0mps"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><em class="lh">来源:通过从数万亿个标记中检索来改进语言模型</em></figcaption></figure><h2 id="f484" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kh ls lt lu kl lv lw lx kp ly lz ma iw bi translated"><strong class="ak">那么这个神奇的检索方法是什么呢？</strong></h2><p id="8d8b" class="pw-post-body-paragraph jw jx iq jy b jz mb kb kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt ij bi translated">在高层次上，检索模型获取输入，并从给定的数据库中检索与输入相似的信息，以将其合并到模型中，从而改进网络的预测。</p><p id="4cbd" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">DeepMind复古模型的检索增强架构的新颖性在于，它被设计为能够从拥有数万亿项的数据库中检索数据，并能够使用分块交叉注意力方案将其纳入模型。这实现了与计算时间和存储需求的线性相关。</p><p id="53af" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">首先，为了实现检索机制，我们需要构建一个键-值数据库，其中值是原始的文本标记块，键是冻结的嵌入。追溯模型中的关键字由冻结的预训练BERT模型生成，以避免在训练期间必须周期性地计算整个数据库的嵌入。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ms"><img src="../Images/2e6ad1f9602a6b495d2f7e052c961cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VwAqGrGjulRXlh7V"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><em class="lh">来源:</em>作者</figcaption></figure><p id="b5a8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">根据任务的不同，该值可以是各种各样的东西，例如在问答任务中，该值可以是邻居(问题)的元组— continuation(答案)。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mt"><img src="../Images/ed72596f2ff620e13f43d78ed253d870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kb5hvqRDDG957WnA"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><em class="lh">来源:</em>作者</figcaption></figure><p id="6f78" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在训练阶段，每个训练序列被分成组块，然后使用冻结的BERTs嵌入这些组块。从数据库中，检索每个块的k个最近邻，以便稍后在追溯块中进行扩充。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mu"><img src="../Images/5d636426ccff14a3152889f77ba64078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tt1pw1969Vr7GJH8"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><em class="lh">来源:</em>作者</figcaption></figure><p id="5880" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">然后，编码器-解码器架构将检索组块集成到模型的预测中。</p><p id="17b5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">检索到的信息通过变换器编码块，以便能够被合并到复古模型的解码器部分。实际上，在复古模型中，从第9个复古模块开始，每隔三个模块(即9、12、15…32)就要加入一个</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mv"><img src="../Images/724945d8e7424f19c1964b79b108365d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b91yCCAfVxAuC7Yu"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><em class="lh">来源:通过从万亿令牌中检索来改进语言模型:复古架构—复古架构。左图:简化版本，其中长度为𝑛 = 12的序列被分成𝑙 = 3的大小为𝑚 = 4的块。对于每个块，我们检索𝑘=𝑟的2个邻居=每个5个令牌。检索路径显示在顶部。右图:Cca运算符中交互的详细信息。因为第一组块的邻居仅影响第一组块的最后一个记号和来自第二组块的记号，所以因果关系得以保持。</em></figcaption></figure><h2 id="fa76" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kh ls lt lu kl lv lw lx kp ly lz ma iw bi translated"><strong class="ak">检索的优点</strong></h2><p id="10d3" class="pw-post-body-paragraph jw jx iq jy b jz mb kb kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt ij bi translated">检索的明显优势是可以使用更小的模型而不损失准确性。在实践中，这对训练(或为模型更新而重新训练)和推理时间有有益的影响。更重要的是，保持检索模型是最新的，可能只需要更新检索数据库就足够了，这比从头开始重新训练模型要便宜几个数量级。</p><p id="04c5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">此外，如果在训练后发现部分训练数据是有偏差的或有害的输出，则检索允许一些校正，因为违规的检索数据可以被追溯性地过滤。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h2 id="df87" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kh ls lt lu kl lv lw lx kp ly lz ma iw bi translated"><strong class="ak">总结</strong></h2><p id="1758" class="pw-post-body-paragraph jw jx iq jy b jz mb kb kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt ij bi translated">我故意不想太深入模型的机制。该模型已经在许多文章中进行了分析，当然，您可以随时阅读Deepmind ( <a class="ae ku" href="https://arxiv.org/pdf/2112.04426.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>)的完整文章。</p><p id="0bc4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在我看来，检索的概念是革命性的，值得额外关注(双关语)。范式的转变是，与其收集和标记大量的数据来教我们的模型，不如让我们看看如何教模型分析信息，只给模型我们认为可能与他相关的信息来提取这些数据。</p><p id="e590" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如今，对信息的访问是无止境的，对我们人类来说，搜索网络和分析给定的信息比记住所有东西更直观，所以为什么模型不应该这样做呢？</p><p id="7529" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">就像“注意力”让我们看到句子中的上下文一样，检索等方法可以帮助我们将模型与相关信息联系起来。</p><p id="258e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这可能是迈向通用人工智能模型的第一步，通过在模型中嵌入从庞大的数据库中提取特定任务信息的能力，以提高广泛任务的准确性。</p><p id="ded0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">此外，由于我们可以直接可视化或修改检索机制提供的邻居，我们可以获得对模型输出的更多洞察，从而提高模型的可解释性。</p><p id="16e5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">展望未来，接下来要问的问题可能是:我们今天如何改进现有的语言模型？有没有可能把这种检索方式推广到其他不太直观的地方？我们的数据库大小限制是多少？我们能改进我们的数据库查询吗？还有很多。</p><p id="b677" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">日子会证明一切</p><p id="e15b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">顺便说一下，黑客帝国于1999年3月24日发布:)</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h2 id="3337" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kh ls lt lu kl lv lw lx kp ly lz ma iw bi translated">参考</h2><p id="221c" class="pw-post-body-paragraph jw jx iq jy b jz mb kb kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt ij bi translated"><a class="ae ku" href="https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens" rel="noopener ugc nofollow" target="_blank">https://deep mind . com/research/publications/2021/improving-language-models-by-retrieving-from-万亿令牌</a></p><p id="0f77" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Borgeaud，s .，Mensch，a .，Hoffmann，j .，Cai，t .，Rutherford，e .，Millican，k .，… &amp; Sifre，L. (2021)。通过从数万亿个标记中检索来改进语言模型。https://arxiv.org/pdf/2112.04426.pdf<em class="li">arXiv预印本arXiv:2112.04426 </em> - <a class="ae ku" href="https://arxiv.org/pdf/2112.04426.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="bdad" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Lewis，p .，Perez，e .，Piktus，a .，Petroni，f .，Karpukhin，v .，Goyal，n .，… &amp; Kiela，D. (2020)。知识密集型自然语言处理任务的检索增强生成。arXiv预印本arXiv:2005.11401 。<a class="ae ku" href="https://arxiv.org/abs/2005.11401" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2005.11401</a></p><p id="bf03" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">格雷夫，e .，乔林，a .，&amp;乌苏尼尔，N. (2016)。用连续缓存改进神经语言模型。<em class="li"> arXiv预印本arXiv:1612.04426 </em>。——【https://openreview.net/forum?id=B184E5qee T2】</p><p id="8ff1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae ku" href="http://jalammar.github.io/illustrated-retrieval-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar . github . io/illustrated-retrieval-transformer/</a></p><p id="c3c6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Gehman，s .，Gururangan，s .，Sap，m .，Choi，y .，&amp; Smith，N. A. (2020)。真实毒性提示:评估语言模型中的神经毒性退化。<em class="li"> arXiv预印本arXiv:2009.11462</em>-<a class="ae ku" href="https://aclanthology.org/2020.findings-emnlp.301/" rel="noopener ugc nofollow" target="_blank">https://aclanthology.org/2020.findings-emnlp.301/</a></p><p id="e8e9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">张，j .，Utiyama，m .，Sumita，e .，Neubig，g .，&amp; Nakamura，S. (2018)。用检索到的翻译片段指导神经机器翻译。<em class="li"> arXiv预印本arXiv:1804.02559</em>-<a class="ae ku" href="https://arxiv.org/abs/1804.02559" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1804.02559</a></p><p id="40ff" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">顾，王，周，李(2018年4月)。搜索引擎引导的神经机器翻译。在<em class="li">https://www.aaai.org/GuideBook2018/17282-74380-GB.pdfAAAI人工智能会议论文集</em>(第32卷，№1)<a class="ae ku" href="https://www.aaai.org/GuideBook2018/17282-74380-GB.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="fb16" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">本德，E. M .，t .格布鲁，麦克米兰-梅杰，a .，&amp; Shmitchell，S. (2021年3月)。论随机鹦鹉的危险:语言模型会不会太大？🦜.在<em class="li">2021年ACM公平、问责和透明会议记录</em>(第610-623页)<a class="ae ku" href="https://dl.acm.org/doi/10.1145/3442188.3445922" rel="noopener ugc nofollow" target="_blank">https://dl.acm.org/doi/10.1145/3442188.3445922</a></p><p id="f0c5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Lieber，o .，Sharir，o .，Lenz，b .，&amp; Shoham，Y. (2021)。侏罗纪-1:技术细节和评估。<em class="li">白皮书。AI21实验室</em>。—<a class="ae ku" href="https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://uploads-SSL . webflow . com/60fd 4503684 b 466578 c0d 307/61138924626 a 6981 ee 09 caf 6 _ Jurassic _ tech _ paper . pdf</a></p><p id="4b94" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae ku" href="https://fortune.com/2021/04/21/ai-carbon-footprint-reduce-environmental-impact-of-tech-google-research-study/" rel="noopener ugc nofollow" target="_blank">https://fortune . com/2021/04/21/ai-carbon-footprint-reduce-environmental-impact-of-tech-Google-research-study/</a></p><p id="fea8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae ku" href="https://www.spglobal.com/marketintelligence/en/news-insights/trending/HyvwuXMO9YgqHfj7J6tGlA2" rel="noopener ugc nofollow" target="_blank">https://www . sp global . com/market intelligence/en/news-insights/trending/hyvwuxmo 9 ygqhfj 7j 6 tgla 2</a></p><p id="fa6f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae ku" href="https://www.theregister.com/2020/11/04/gpt3_carbon_footprint_estimate/" rel="noopener ugc nofollow" target="_blank">https://www . the register . com/2020/11/04/gp T3 _ carbon _ footprint _ estimate/</a></p><p id="7f1b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae ku" href="https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/" rel="noopener ugc nofollow" target="_blank">https://ai . Facebook . com/blog/retrieval-augmented-generation-streaming-the-creation-of-intelligent-natural language-processing-models/</a></p></div></div>    
</body>
</html>