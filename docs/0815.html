<html>
<head>
<title>REALM: Retrieval-Augmented Language Model Pre-Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">领域:检索增强语言模型预训练</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/realm-retrieval-augmented-language-model-pre-training-534feae7ab98?source=collection_archive---------3-----------------------#2020-08-17">https://pub.towardsai.net/realm-retrieval-augmented-language-model-pre-training-534feae7ab98?source=collection_archive---------3-----------------------#2020-08-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e9be" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="7cf5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">检索增强语言模型预训练导论</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e233d8a8389bf9b03b3b316051351c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dferpNZL64LHVIcs"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="8d0c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自2018年以来，基于transformer的语言模型已被证明在开放领域问答(Open-QA)等许多NLP下游任务中取得了良好的性能。为了获得更好的结果，模型打算增加模型参数(例如，更多的头、更大的尺寸)，以便在神经网络中存储世界知识。</p><p id="e6d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Google Research的Guu等人(2020年)发布了最先进的模型(检索增强语言模型预训练，aks REALM)，该模型利用了来自其他大型语料库(如维基百科)的<code class="fe me mf mg mh b">knowledge retriever</code>增强数据。给定一个额外的信号，它有助于模型提供一个更好的结果。在这个故事中，我们将讲述这个模型是如何达到最先进的效果的。</p><h1 id="a221" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">领域概述</h1><p id="a854" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">总体思想是利用额外的文档向模型提供更多的信号，以便它可以准确地预测屏蔽令牌。将这种方法命名为<code class="fe me mf mg mh b">retrieve-then-predict</code>方法。下图显示了预训练工作流程。</p><ol class=""><li id="be24" class="nf ng it lk b ll lm lo lp lr nh lv ni lz nj md nk nl nm nn bi translated">给出一个带面具的句子(金字塔顶端的面具)</li><li id="778b" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">向神经知识检索器输入屏蔽语句。它将返回一个与输入相关的文档(不一定是整篇文章)。</li><li id="b61e" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md nk nl nm nn bi translated">将原始句子和增强文档传递给知识增强编码器。它将预测屏蔽令牌(金字塔)。</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/96d75098c63800b06e563744fd4e2345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*nINwg5YmIaBPgQ9OmLmmnw.png"/></div></figure><p id="f934" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于微调阶段，它使用非屏蔽语句而不是包含屏蔽标记的语句。</p><h1 id="2a56" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">模型架构</h1><p id="5850" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">从前面的概述中，你可能知道REALM (Guu et al .，2020)包含两个模型，即知识检索器和知识扩充编码器。我们将一个接一个地检查它。</p><h2 id="c26c" class="nu mj it bd mk nv nw dn mo nx ny dp ms lr nz oa mu lv ob oc mw lz od oe my iz bi translated">知识检索器</h2><p id="b5fb" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">首先，知识检索器的目标是为下一步输出一个有用的文档。对于输入，它使用<strong class="lk jd"> BERT-style将句子转换为一个令牌</strong>，分别以【CLS】和【SEP】作为前缀和前缀。对于外部文档，它还包括文档的标题和正文。因此，我们需要通过[SEP]来连接它，这是遵循BERT风格的。你可以访问这个<a class="ae lh" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">故事</a>来获得更多关于伯特风格格式的信息。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f1a670b22fd380e44397243fc23cec24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*Yeizhz2R46PJSBOcItOTXA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">伯特风格格式。x、x1和x2指的是句子(Guu等人，2020年)</figcaption></figure><p id="56c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，它使用向量嵌入(来自知识语料库的输入和文档)的内积。Softmax将应用于内部产品结果，以便挑选最相关的文档。</p><h2 id="bf05" class="nu mj it bd mk nv nw dn mo nx ny dp ms lr nz oa mu lv ob oc mw lz od oe my iz bi translated">知识扩充编码器</h2><p id="f6a1" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">与knowledge retreiver相同，Guu等人遵循BERT机制来训练和微调该编码器。</p><p id="df27" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在预训练阶段，它使用掩蔽语言建模(Devlin et al .，2018)。基本上，训练目标是通过非屏蔽令牌预测屏蔽令牌。您可以访问<a class="ae lh" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">故事</a>来更好地了解MLM机制(Devlin et al .，2018)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/e6e8a064fae51a5c834aa4bc2a1bbda6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*CS30oRCYZ8NoDoMUUPVZtw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">预训练阶段示例(无监督学习)(Guu等人，2020年)</figcaption></figure><p id="f1dc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在Open-QA微调阶段，没有屏蔽令牌，Guu等人<strong class="lk jd">假设答案可以从文档</strong>(知识检索器的输出)中找到。它遵循BERT风格来构造矢量嵌入，并将其传递给transformer模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/6082ff211cdf1507d944ebc7866602e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*0n6hV5v3IqfemUa6YRXZsg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">开放式质量保证微调阶段(监督学习)示例(Guu等人，2020年)</figcaption></figure><h2 id="a483" class="nu mj it bd mk nv nw dn mo nx ny dp ms lr nz oa mu lv ob oc mw lz od oe my iz bi translated">最大内积搜索</h2><p id="91c0" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">这种<code class="fe me mf mg mh b">retrieve-then-predict</code>架构的主要挑战是从更大的外部语料库中选择一个好的文档。Guu等人提出使用MIPS来缩短检索时间。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/25b53837cd0f05c877105bbcbeab2a4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*GwIgdlO6pDj_-gmnpcBE5w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">上公式:给定输入句子，从语料库中选择文档。下式:给出输入句子和选择的文档，选择答案(Guu et al .，2020)</figcaption></figure><p id="a6c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了减少计算时间，Guu等人提出了两步计算。首先，通过提供输入句子x. <strong class="lk jd">利用</strong><a class="ae lh" href="https://arxiv.org/pdf/1202.6101v1.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">MIPS</strong></a><strong class="lk jd">(Ram and Gary，2012)从更大的语料库中计算文档的可能性，以挑选前k个概率文档</strong>作为下一步的输入。MIPS使用构建球树来将数据点(即向量)分成不同的簇。数据点将被分割成集群，并且它将只属于一个集群(同一级别的集群)。因此，Guu等人可以使用少得多的运行时间来找到顶部k个文档。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3b57f178a83d905fa5629635f65f8011.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*_xVRnUkKTkto44q6eJeBxA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">球树示例(Ram和Gray，2012年)</figcaption></figure><h1 id="dcf9" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">培训前的数据处理</h1><p id="8535" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">除了使用MIPS来选择最相关的文档之外，Guu等人还在预训练中注入额外的信息来辅助模型训练。</p><h2 id="b101" class="nu mj it bd mk nv nw dn mo nx ny dp ms lr nz oa mu lv ob oc mw lz od oe my iz bi translated">显著跨度</h2><p id="0a88" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">由于领域聚焦于开放问答领域，他们倾向于强调命名实体和日期。那些命名的实体和日期将被屏蔽为显著的跨度。为了不费力地找出命名实体，训练基于BERT的标记器来识别命名实体和日期。</p><h2 id="520a" class="nu mj it bd mk nv nw dn mo nx ny dp ms lr nz oa mu lv ob oc mw lz od oe my iz bi translated">空文档</h2><p id="f55d" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">Guu等人认为，并不是所有的屏蔽记号都需要额外的知识来预测。空文档被注入到前k个检索文档来类似这种情况。</p><h2 id="32fc" class="nu mj it bd mk nv nw dn mo nx ny dp ms lr nz oa mu lv ob oc mw lz od oe my iz bi translated">丢弃琐碎的检索</h2><p id="803b" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">前k个文档可能包括相同的输入句子。为了防止编码器通过关注未屏蔽的标记来预测结果，这种琐碎的训练数据将在预训练阶段被排除。</p><h2 id="d728" class="nu mj it bd mk nv nw dn mo nx ny dp ms lr nz oa mu lv ob oc mw lz od oe my iz bi translated">向量初始化</h2><p id="1945" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">好的向量导致更好的预测结果。为了简单起见，我们可以使用随机初始化，但它会引入冷启动问题。因此Guu等人使用<a class="ae lh" href="https://arxiv.org/pdf/1906.00300.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"/></a><strong class="lk jd">逆向完形填空任务进行预训练</strong>。简而言之，它是屏蔽令牌预测的逆版本。给出一个查询(下图的左侧)，目标是从候选项中挑选一个真实的上下文(下图的右侧)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/012b55b0426edad7ed646f8a58c047d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QAlSvwlHvwyo10gDujKk0Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">逆向完形填空任务示例(Lee et al .，2019)</figcaption></figure><h1 id="d86e" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">外卖</h1><ul class=""><li id="1bf8" class="nf ng it lk b ll na lo nb lr ol lv om lz on md oo nl nm nn bi translated">命名实体和日期的显著跨度很重要。由于这个模型着眼于OpenQA。让模型关注那些命名的实体和日期是很重要的。</li><li id="02ee" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md oo nl nm nn bi translated">从更大的语料库中选择文档是重要的。假设最终结果存在于额外的文档中。挑选k个相关文档也很重要。</li></ul><h1 id="0c0f" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">关于我</h1><p id="6f41" class="pw-post-body-paragraph li lj it lk b ll na kd ln lo nb kg lq lr nc lt lu lv nd lx ly lz ne mb mc md im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。你可以通过<a class="ae lh" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae lh" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae lh" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="022b" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">参考</h1><ul class=""><li id="cf64" class="nf ng it lk b ll na lo nb lr ol lv om lz on md oo nl nm nn bi translated">页（page的缩写）拉姆和A. G .格雷。<a class="ae lh" href="https://arxiv.org/pdf/1202.6101v1.pdf" rel="noopener ugc nofollow" target="_blank">使用树形数据结构的最大内积搜索</a>。2012</li><li id="029a" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md oo nl nm nn bi translated">放大图片作者:Jeffrey j ...<a class="ae lh" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a>。2018</li><li id="7b2a" class="nf ng it lk b ll no lo np lr nq lv nr lz ns md oo nl nm nn bi translated">K.古、李刚、董子珍、帕苏帕特和张明威。<a class="ae lh" href="https://arxiv.org/pdf/2002.08909.pdf" rel="noopener ugc nofollow" target="_blank">领域:检索-增强语言模型预训练</a>。2020</li></ul></div></div>    
</body>
</html>