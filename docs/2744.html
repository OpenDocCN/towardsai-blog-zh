<html>
<head>
<title>LASSO (L1) Vs Ridge (L2) Vs Elastic Net Regularization For Classification Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类模型的套索(L1)与岭(L2)与弹性网正则化</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/lasso-l1-vs-ridge-l2-vs-elastic-net-regularization-for-classification-model-409c3d86f6e9?source=collection_archive---------0-----------------------#2022-05-08">https://pub.towardsai.net/lasso-l1-vs-ridge-l2-vs-elastic-net-regularization-for-classification-model-409c3d86f6e9?source=collection_archive---------0-----------------------#2022-05-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="aec1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过比较套索正则化、脊正则化和弹性网正则化的性能在它们之间进行选择</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0b97f8198ffb198285f5424de146deaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MJ45PoZxTrd1O-6r"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@nordwood?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">诺德伍德主题</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="230b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LASSO(最小绝对收缩和选择算子)也称为L1正则化，Ridge也称为L2正则化。弹性网是套索和脊的结合。这三种都是机器学习中常用的纠正过度拟合的技术。</p><p id="9f57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将介绍</p><ul class=""><li id="bcb9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">套索(L1)，山脊(L2)，弹力网有什么区别？</li><li id="ab4c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如何使用Python <code class="fe mj mk ml mm b">sklearn</code>为分类模型运行LASSO？</li><li id="1b4f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">分类模型如何运行Ridge？</li><li id="4748" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">分类模型如何运行弹性网？</li><li id="2aad" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如何比较套索、脊、弹力网的性能？</li></ul><p id="1abf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">本岗位资源:</strong></p><ul class=""><li id="caad" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">YouTube<a class="ae ky" href="https://www.youtube.com/watch?v=PAOkp9CEn58&amp;list=PLVppujud2yJrTGSjtN7j8gqCthOVyyk4W&amp;index=1" rel="noopener ugc nofollow" target="_blank">上的视频教程</a></li><li id="9e01" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Python代码在帖子最后。点击<a class="ae ky" href="https://mailchi.mp/a30206d20b9a/in5ymnmlkv" rel="noopener ugc nofollow" target="_blank">此处</a>为笔记本。</li><li id="3b28" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">关于<a class="ae ky" href="https://www.youtube.com/playlist?list=PLVppujud2yJrTGSjtN7j8gqCthOVyyk4W" rel="noopener ugc nofollow" target="_blank">过拟合校正的更多视频教程</a></li><li id="aa7f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">更多关于<a class="ae ky" href="https://medium.com/@AmyGrabNGoInfo/list/overfitting-correction-2fb2052285d6" rel="noopener">过度拟合修正</a>的博文</li></ul><p id="dae1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧！</p><h1 id="2599" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">第0步:套索(L1) vs山脊(L2) vs弹力网</h1><p id="ad07" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在第0步中，我们将讨论套索、脊和弹性网的区别。</p><p id="cb0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">套索和脊正则化通过缩小模型的系数来校正过度拟合。在模型训练过程中，不是最小化模型训练误差，而是最小化模型训练误差加上惩罚项。LASSO和Ridge对于惩罚项有不同的计算算法。弹性网的惩罚项是LASSO和Ridge算法的组合。</p><ul class=""><li id="2163" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">惩罚项有一个参数叫做λ。它控制着惩罚的力度。</li></ul><blockquote class="nk nl nm"><p id="c8d2" class="kz la nn lb b lc ld ju le lf lg jx lh no lj lk ll np ln lo lp nq lr ls lt lu im bi translated">当λ等于0时，罚项等于0。所以这个模型是一个没有正则化的模型。</p><p id="09c5" class="kz la nn lb b lc ld ju le lf lg jx lh no lj lk ll np ln lo lp nq lr ls lt lu im bi translated">当λ增加时，罚项值增加，模型系数值减少。</p><p id="029e" class="kz la nn lb b lc ld ju le lf lg jx lh no lj lk ll np ln lo lp nq lr ls lt lu im bi translated">当λ趋于无穷大时，模型系数收缩到接近0。模型只剩下截距，它预测每个数据点的平均值。</p></blockquote><ul class=""><li id="9a45" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">LASSO的罚项是罚参数λ乘以系数的绝对值之和。因为LASSO的系数可能会缩小到零，所以它可用于自动特征选择。</li><li id="8ca5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Ridge根据系数的平方和缩小模型系数。Ridge不会将模型系数缩小到零。</li><li id="194b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">弹性网的惩罚项是LASSO和岭回归的惩罚项的组合。它将一些系数设置为零，但数量小于LASSO。</li></ul><h1 id="3537" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">步骤1:导入库</h1><p id="c763" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">第一步，让我们导入本教程所需的Python库。</p><p id="6587" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将在本教程中使用乳腺癌数据集，因此需要导入来自<code class="fe mj mk ml mm b">sklearn</code>的<code class="fe mj mk ml mm b">datasets</code>。<code class="fe mj mk ml mm b">pandas</code>和<code class="fe mj mk ml mm b">numpy</code>被导入进行数据处理。<code class="fe mj mk ml mm b">matplotlib</code>是为了可视化，<code class="fe mj mk ml mm b">StandardScaler</code>是为了数据标准化。</p><p id="ce83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于模型训练，我们导入了train_test_split和LogisticRegression。</p><p id="7850" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于模型性能评估，我们导入了<code class="fe mj mk ml mm b">plot_confusion_matrix</code>、<code class="fe mj mk ml mm b">classification_report</code>、<code class="fe mj mk ml mm b">log_loss</code>、<code class="fe mj mk ml mm b">roc_curve</code>、<code class="fe mj mk ml mm b">roc_auc_score</code>。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="c71e" class="nv mo it mm b gy nw nx l ny nz"># Dataset<br/>from sklearn import datasets</span><span id="b9ce" class="nv mo it mm b gy oa nx l ny nz"># Data processing<br/>import pandas as pd<br/>import numpy as np</span><span id="b8fb" class="nv mo it mm b gy oa nx l ny nz"># Visualization<br/>import matplotlib.pyplot as plt</span><span id="92ab" class="nv mo it mm b gy oa nx l ny nz"># Standardize the data<br/>from sklearn.preprocessing import StandardScaler</span><span id="0b1b" class="nv mo it mm b gy oa nx l ny nz"># Model and performance evaluation<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import plot_confusion_matrix, classification_report, log_loss, roc_curve, roc_auc_score</span></pre><h1 id="758b" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">第二步:读入数据</h1><p id="0c21" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在第二步中，来自<code class="fe mj mk ml mm b">sklearn</code>库的乳腺癌数据被加载并转换成熊猫数据帧。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="a5eb" class="nv mo it mm b gy nw nx l ny nz"># Load the breast cancer dataset<br/>data = datasets.load_breast_cancer()</span><span id="6858" class="nv mo it mm b gy oa nx l ny nz"># Put the data in pandas dataframe format<br/>df = pd.DataFrame(data=data.data, columns=data.feature_names)<br/>df['target']=data.target</span><span id="9802" class="nv mo it mm b gy oa nx l ny nz"># Check the data information<br/>df.info()</span></pre><p id="2b3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信息摘要显示数据集有569条记录和31列。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="cbba" class="nv mo it mm b gy nw nx l ny nz">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 569 entries, 0 to 568<br/>Data columns (total 31 columns):<br/> #   Column                   Non-Null Count  Dtype  <br/>---  ------                   --------------  -----  <br/> 0   mean radius              569 non-null    float64<br/> 1   mean texture             569 non-null    float64<br/> 2   mean perimeter           569 non-null    float64<br/> 3   mean area                569 non-null    float64<br/> 4   mean smoothness          569 non-null    float64<br/> 5   mean compactness         569 non-null    float64<br/> 6   mean concavity           569 non-null    float64<br/> 7   mean concave points      569 non-null    float64<br/> 8   mean symmetry            569 non-null    float64<br/> 9   mean fractal dimension   569 non-null    float64<br/> 10  radius error             569 non-null    float64<br/> 11  texture error            569 non-null    float64<br/> 12  perimeter error          569 non-null    float64<br/> 13  area error               569 non-null    float64<br/> 14  smoothness error         569 non-null    float64<br/> 15  compactness error        569 non-null    float64<br/> 16  concavity error          569 non-null    float64<br/> 17  concave points error     569 non-null    float64<br/> 18  symmetry error           569 non-null    float64<br/> 19  fractal dimension error  569 non-null    float64<br/> 20  worst radius             569 non-null    float64<br/> 21  worst texture            569 non-null    float64<br/> 22  worst perimeter          569 non-null    float64<br/> 23  worst area               569 non-null    float64<br/> 24  worst smoothness         569 non-null    float64<br/> 25  worst compactness        569 non-null    float64<br/> 26  worst concavity          569 non-null    float64<br/> 27  worst concave points     569 non-null    float64<br/> 28  worst symmetry           569 non-null    float64<br/> 29  worst fractal dimension  569 non-null    float64<br/> 30  target                   569 non-null    int64  <br/>dtypes: float64(30), int64(1)<br/>memory usage: 137.9 KB</span></pre><p id="47a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标变量分布显示数据集中63%的1和37%的0。因此，1表示患者患有乳腺癌，0表示患者没有乳腺癌。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="3c84" class="nv mo it mm b gy nw nx l ny nz"># Check the target value distribution<br/>df['target'].value_counts(normalize=True)</span><span id="83c3" class="nv mo it mm b gy oa nx l ny nz">1    0.627417<br/>0    0.372583<br/>Name: target, dtype: float64</span></pre><h1 id="db91" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">步骤3:训练测试分割</h1><p id="392b" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在步骤3中，我们将数据集分成80%的训练数据集和20%的测试数据集。<code class="fe mj mk ml mm b">random_state</code>使随机分割结果可重复。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="7223" class="nv mo it mm b gy nw nx l ny nz"># Train test split<br/>X_train, X_test, y_train, y_test = train_test_split(df[df.columns.difference(['target'])], df['target'], test_size=0.2, random_state=42)</span><span id="4d04" class="nv mo it mm b gy oa nx l ny nz"># Check the number of records in training and testing dataset.<br/>print(f'The training dataset has {len(X_train)} records.')<br/>print(f'The testing dataset has {len(X_test)} records.')</span></pre><p id="4c6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练数据集有455条记录，测试数据集有114条记录。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="dd78" class="nv mo it mm b gy nw nx l ny nz">The training dataset has 455 records.<br/>The testing dataset has 114 records.</span></pre><h1 id="ad43" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">第四步:标准化</h1><p id="6f0e" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">标准化是将要素重新调整到相同的比例。它通过提取平均值并除以标准差来计算。标准化后，每个特征的均值和单位标准差为零。</p><p id="a79c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标准化应该只适用于训练数据集，以防止测试数据集信息泄露到训练过程中。然后，使用来自训练数据集的拟合结果来标准化测试数据集。</p><p id="c8be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有不同类型的定标器。<code class="fe mj mk ml mm b">StandardScaler</code>和<code class="fe mj mk ml mm b">MinMaxScaler</code>是最常用的。对于有离群值的数据集，我们可以使用<code class="fe mj mk ml mm b">RobustScaler</code>。</p><p id="25d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将使用<code class="fe mj mk ml mm b">StandardScaler</code>。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="3b26" class="nv mo it mm b gy nw nx l ny nz"># Initiate scaler<br/>sc = StandardScaler()</span><span id="aa67" class="nv mo it mm b gy oa nx l ny nz"># Standardize the training dataset<br/>X_train_transformed = pd.DataFrame(sc.fit_transform(X_train),index=X_train.index, columns=X_train.columns)</span><span id="05d0" class="nv mo it mm b gy oa nx l ny nz"># Standardized the testing dataset<br/>X_test_transformed = pd.DataFrame(sc.transform(X_test),index=X_test.index, columns=X_test.columns)</span><span id="1fe7" class="nv mo it mm b gy oa nx l ny nz"># Summary statistics after standardization<br/>X_train_transformed.describe().T</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/fa890b3b52fc4986abdcd438af3314fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bkyzQneFrt7Yqqxq.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">数据标准化—图片来自GrabNGoInfo.com</figcaption></figure><p id="8853" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，使用<code class="fe mj mk ml mm b">StandardScaler</code>后，所有特征的均值和单位标准差都为零。</p><p id="6ed8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们也获得标准化之前的训练数据的汇总统计，我们可以看到，均值和标准差在尺度上可以有很大的不同。例如，面积误差的平均值为40，标准差为47。另一方面，紧密度误差具有大约0.023的平均值和0.019的标准偏差。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="e4d4" class="nv mo it mm b gy nw nx l ny nz"># Summary statistics before standardization<br/>X_train.describe().T</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/cea6b7e2ff7bf7e46625ae0f6cda2aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Yzte1nkHUK-wuDyV.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">数据标准化之前—图片来自GrabNGoInfo.com</figcaption></figure><h1 id="70c8" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">步骤5:无正则化的逻辑回归</h1><p id="f40e" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在第5步中，我们将创建一个没有正则化的逻辑回归作为基线模型。</p><p id="03f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mj mk ml mm b">sklearn</code>中的逻辑回归默认使用岭正则化。当检查LogisticRegression()的默认超参数值时，我们看到了<code class="fe mj mk ml mm b">penalty='l2'</code>，这意味着使用了L2正则化。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="f10f" class="nv mo it mm b gy nw nx l ny nz"># Check default values<br/>LogisticRegression()</span><span id="31a4" class="nv mo it mm b gy oa nx l ny nz">LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,<br/>                   intercept_scaling=1, l1_ratio=None, max_iter=100,<br/>                   multi_class='auto', n_jobs=None, penalty='l2',<br/>                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,<br/>                   warm_start=False)</span></pre><p id="9d3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要将<code class="fe mj mk ml mm b">penalty</code>从<code class="fe mj mk ml mm b">l2</code>改为‘无’,以获得没有正则化的模型。运行基线逻辑回归模型后，我们还使用<code class="fe mj mk ml mm b">.predict</code>预测了测试数据集，并使用<code class="fe mj mk ml mm b">.predict_proba</code>计算了预测概率。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="489c" class="nv mo it mm b gy nw nx l ny nz"># Run model<br/>logistic = LogisticRegression(penalty='none', random_state=0).fit(X_train_transformed, y_train)</span><span id="97c6" class="nv mo it mm b gy oa nx l ny nz"># Make prediction<br/>logistic_prediction = logistic.predict(X_test_transformed)</span><span id="6d12" class="nv mo it mm b gy oa nx l ny nz"># Get predicted probability<br/>logistic_pred_Prob = logistic.predict_proba(X_test_transformed)[:,1]</span></pre><p id="1be4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">得到预测值和预测概率后，就准备检查模型性能了！</p><p id="320b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们来看看ROC曲线。我们得到的曲线下面积值为0.9818。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="609f" class="nv mo it mm b gy nw nx l ny nz"># Get the false positive rate and true positive rate<br/>fpr,tpr, _=roc_curve(y_test,logistic_pred_Prob)</span><span id="ddeb" class="nv mo it mm b gy oa nx l ny nz"># Get auc value<br/>auc=roc_auc_score(y_test,logistic_pred_Prob)</span><span id="cd03" class="nv mo it mm b gy oa nx l ny nz"># Plot the chart<br/>plt.plot(fpr,tpr,label="area="+str(auc))<br/>plt.legend(loc=4)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/4c665350acd39f4f76276c27f30f4343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YMGs-i0amSz_DHQM.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">无正则化ROC曲线的Logistic回归——来自GrabNGoInfo.com的图像</figcaption></figure><p id="676e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型的对数损失值为2.02。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="6db7" class="nv mo it mm b gy nw nx l ny nz"># Caclulate log loss<br/>log_loss(y_test,logistic_pred_Prob)</span><span id="4ec1" class="nv mo it mm b gy oa nx l ny nz">2.0150517046435854</span></pre><p id="b3e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">混淆矩阵显示1个假阳性和6个假阴性。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="fab5" class="nv mo it mm b gy nw nx l ny nz"># Confusion matrix<br/>plot_confusion_matrix(logistic, X_test_transformed, y_test)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/45e394d23ab5fe8ee7f43a9c679bf79d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*ReMVj0sYYXNK60ZZ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">无正则化混淆矩阵的逻辑回归——来自GrabNGoInfo.com的图像</figcaption></figure><p id="5f51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总共7个不正确的预测对应于0.939的准确度。</p><p id="391a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不希望在乳腺癌预测中遗漏任何实际的癌症患者，也不介意出现一些假阳性，因此召回率将是最需要关注的指标。召回率才是真正的阳性率。它测量模型捕获的实际癌症患者的百分比。我们可以看到，逻辑回归给出了0.915的召回值，这意味着91.5%的实际癌症患者被捕获。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="93e5" class="nv mo it mm b gy nw nx l ny nz"># Performance report<br/>print(classification_report(y_test, logistic_prediction, digits=3))</span><span id="ecba" class="nv mo it mm b gy oa nx l ny nz">precision    recall  f1-score   support</span><span id="937e" class="nv mo it mm b gy oa nx l ny nz">           0      0.875     0.977     0.923        43<br/>           1      0.985     0.915     0.949        71</span><span id="2ab6" class="nv mo it mm b gy oa nx l ny nz">    accuracy                          0.939       114<br/>   macro avg      0.930     0.946     0.936       114<br/>weighted avg      0.943     0.939     0.939       114</span></pre><p id="c085" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们检查模型的系数。根据它们的绝对值，我将模型系数从高到低排序，我们可以看到顶部变量的系数在几百。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="664f" class="nv mo it mm b gy nw nx l ny nz"># Model coefficients<br/>LogisticCoeff = pd.concat([pd.DataFrame(X_test_transformed.columns),<br/>                           pd.DataFrame(np.transpose(logistic.coef_))], axis = 1)<br/>LogisticCoeff.columns=['Variable','Coefficient']<br/>LogisticCoeff['Coefficient_Abs']=LogisticCoeff['Coefficient'].apply(abs)<br/>LogisticCoeff.sort_values(by='Coefficient_Abs', ascending=False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6297353a1a089c2c4815b4f0b1cb2072.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*39QAqCAFrxqwWr3w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">无正则化系数的逻辑回归-来自GrabNGoInfo.com的图像</figcaption></figure><h1 id="b828" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">第六步:套索</h1><p id="adea" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在第6步，套索模型用于运行相同的分析。</p><p id="c74b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mj mk ml mm b">penalty='l1'</code>表示应用套索正则化。</p><p id="a108" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mj mk ml mm b">solver</code>是优化问题中使用的算法。有不同类型的解算器。对于小型数据集，“liblinear”是一个不错的选择，而“sag”和“saga”对于大型数据集更快。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="5284" class="nv mo it mm b gy nw nx l ny nz"># Run model<br/>lasso = LogisticRegression(penalty='l1', solver='liblinear', <br/>                           random_state=0).fit(X_train_transformed, y_train)</span><span id="9fc1" class="nv mo it mm b gy oa nx l ny nz"># Make prediction<br/>lasso_prediction = lasso.predict(X_test_transformed)</span><span id="7282" class="nv mo it mm b gy oa nx l ny nz"># Get predicted probability<br/>lasso_pred_Prob = lasso.predict_proba(X_test_transformed)[:,1]</span></pre><p id="0396" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LASSO的ROC/AUC值为0.9967，高于逻辑回归值0.9818。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="da4a" class="nv mo it mm b gy nw nx l ny nz"># Get the false positive rate and true positive rate<br/>fpr,tpr, _= roc_curve(y_test,lasso_pred_Prob)</span><span id="5173" class="nv mo it mm b gy oa nx l ny nz"># Get auc value<br/>auc = roc_auc_score(y_test,lasso_pred_Prob)</span><span id="5c7e" class="nv mo it mm b gy oa nx l ny nz"># Plot the chart<br/>plt.plot(fpr,tpr,label="area="+str(auc))<br/>plt.legend(loc=4)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/45525460db381c2e4aea21c69b0a1f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*k0LMpuZaJxEUmHre.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">使用套索正则化ROC曲线的逻辑回归—来自GrabNGoInfo.com的图像</figcaption></figure><p id="f69e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对数损失从基线逻辑回归的2.015下降到0.0685。这是一个显著的进步！</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="1324" class="nv mo it mm b gy nw nx l ny nz"># Calculate log loss<br/>log_loss(y_test,lasso_pred_Prob)</span><span id="eab2" class="nv mo it mm b gy oa nx l ny nz">0.06846705785516008</span></pre><p id="4716" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">混淆矩阵显示相同的假阳性计数1，但是假阴性计数从6减少到2。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="f14f" class="nv mo it mm b gy nw nx l ny nz"># Confusion matrix<br/>plot_confusion_matrix(lasso, X_test_transformed, y_test)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a3c68c192623994b4a42c7ffe24c2c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*S9tcpTJLbxh6K4dT.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">使用LASSO正则化混淆矩阵的逻辑回归-来自GrabNGoInfo.com的图像</figcaption></figure><p id="ef16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于假阴性计数的减少，准确率从0.939提高到0.974。召回值从0.915增加到0.972。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="7684" class="nv mo it mm b gy nw nx l ny nz"># Performance report<br/>print(classification_report(y_test, lasso_prediction, digits=3))</span><span id="6361" class="nv mo it mm b gy oa nx l ny nz">precision    recall  f1-score   support</span><span id="9824" class="nv mo it mm b gy oa nx l ny nz">           0      0.955     0.977     0.966        43<br/>           1      0.986     0.972     0.979        71</span><span id="95ae" class="nv mo it mm b gy oa nx l ny nz">    accuracy                          0.974       114<br/>   macro avg      0.970     0.974     0.972       114<br/>weighted avg      0.974     0.974     0.974       114</span></pre><p id="7115" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与逻辑回归系数相比，拉索系数下降了很多。因此，大约一半的特征的系数为零。</p><p id="d598" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以LASSO给了我们一个性能更好的更简单的模型。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="13ec" class="nv mo it mm b gy nw nx l ny nz"># Model coefficients<br/>lassoCoeff = pd.concat([pd.DataFrame(X_test_transformed.columns),<br/>                       pd.DataFrame(np.transpose(lasso.coef_))], axis = 1)<br/>lassoCoeff.columns=['Variable','Coefficient']<br/>lassoCoeff['Coefficient_Abs']=lassoCoeff['Coefficient'].apply(abs)<br/>lassoCoeff.sort_values(by='Coefficient_Abs', ascending=False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/bbe7486d2757a610081b5be3937fe1da.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/0*_IRjQnnp7AgvOCLC.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">使用套索正则化系数的逻辑回归-来自GrabNGoInfo.com的图像</figcaption></figure><h1 id="98c6" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">山脊</h1><p id="c0d4" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在第7步中，我们将通过将惩罚改为l2来运行岭回归。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="bb89" class="nv mo it mm b gy nw nx l ny nz"># Run model<br/>ridge = LogisticRegression(penalty='l2', random_state=0).fit(X_train_transformed, y_train)</span><span id="d674" class="nv mo it mm b gy oa nx l ny nz"># Make prediction<br/>ridge_prediction = ridge.predict(X_test_transformed)</span><span id="3b03" class="nv mo it mm b gy oa nx l ny nz"># Get predicted probability<br/>ridge_pred_Prob = ridge.predict_proba(X_test_transformed)[:,1]</span><span id="13f1" class="nv mo it mm b gy oa nx l ny nz"># Get the false positive rate and true positive rate<br/>fpr,tpr, _= roc_curve(y_test,ridge_pred_Prob)</span><span id="619d" class="nv mo it mm b gy oa nx l ny nz"># Get auc value<br/>auc = roc_auc_score(y_test,ridge_pred_Prob)</span><span id="e126" class="nv mo it mm b gy oa nx l ny nz"># Plot the chart<br/>plt.plot(fpr,tpr,label="area="+str(auc))<br/>plt.legend(loc=4)</span></pre><p id="9fde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ROC/AUC值为0.9974，略高于LASSO的0.9967。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/22d7822b25570dc0089ba3a71add0afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pVYDQyMFl8onqA5L.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">基于岭正则化ROC曲线的Logistic回归——来自GrabNGoInfo.com的图像</figcaption></figure><p id="f52f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对数损失从LASSO回归的0.0685下降到0.0601。这也是一个进步。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="5541" class="nv mo it mm b gy nw nx l ny nz"># Calculate log loss<br/>log_loss(y_test,ridge_pred_Prob)</span><span id="64dc" class="nv mo it mm b gy oa nx l ny nz">0.06014109569918717</span></pre><p id="e4f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">混淆矩阵显示相同的假阳性计数2和假阴性计数1。</p><p id="e4e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然错误预测的总数是3，与LASSO回归相同，但岭回归具有更好的性能，因为假阴性减少了1。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="ef31" class="nv mo it mm b gy nw nx l ny nz"># Confusion matrix<br/>plot_confusion_matrix(ridge, X_test_transformed, y_test)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8e1ef245ddbee2a952a0eef05ce334bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*1h_kHtP8c4mae44R.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">基于岭正则化混淆矩阵的Logistic回归——来自GrabNGoInfo.com的图像</figcaption></figure><p id="f169" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为LASSO和Ridge之间不正确预测的数量相同，所以精度相同，为0.974。因为假阴性计数从2减少到1，召回值从0.972增加到0.986。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="8aae" class="nv mo it mm b gy nw nx l ny nz"># Performance matrix<br/>print(classification_report(y_test, ridge_prediction, digits=3))</span><span id="b79f" class="nv mo it mm b gy oa nx l ny nz">precision    recall  f1-score   support</span><span id="c109" class="nv mo it mm b gy oa nx l ny nz">           0      0.976     0.953     0.965        43<br/>           1      0.972     0.986     0.979        71</span><span id="5300" class="nv mo it mm b gy oa nx l ny nz">    accuracy                          0.974       114<br/>   macro avg      0.974     0.970     0.972       114<br/>weighted avg      0.974     0.974     0.974       114</span></pre><p id="6aa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与LASSO回归系数相比，Ridge系数略有下降。有些特征的系数接近于零，但没有一个等于零。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="6ea7" class="nv mo it mm b gy nw nx l ny nz"># Model coefficients<br/>ridgeCoeff = pd.concat([pd.DataFrame(X_test_transformed.columns),<br/>                       pd.DataFrame(np.transpose(ridge.coef_))], axis = 1)<br/>ridgeCoeff.columns=['Variable','Coefficient']<br/>ridgeCoeff['Coefficient_Abs']=ridgeCoeff['Coefficient'].apply(abs)<br/>ridgeCoeff.sort_values(by='Coefficient_Abs', ascending=False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b7254e2527eac7b3b293d7fd428f1d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/0*6n4Rzx5KFl04lWPN.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">具有岭正则化系数的Logistic回归——来自GrabNGoInfo.com的图像</figcaption></figure><p id="575f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于性能对比，我们可以看到，Ridge的性能比LASSO略好，但LASSO的模型比Ridge更简单。</p><h1 id="3da0" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">弹性网</h1><p id="8507" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在第8步中，我们将通过将惩罚改为<code class="fe mj mk ml mm b">'elasticnet'</code>来运行弹性净回归。<code class="fe mj mk ml mm b">l1_ratio=0.5</code>意味着弹性网使用50%的套索和50%的脊。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="7784" class="nv mo it mm b gy nw nx l ny nz"># Run model<br/>elasticNet = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, random_state=0).fit(X_train_transformed, y_train)</span><span id="cdbc" class="nv mo it mm b gy oa nx l ny nz"># Make prediction<br/>elasticNet_prediction = elasticNet.predict(X_test_transformed)</span><span id="488b" class="nv mo it mm b gy oa nx l ny nz"># Get predicted probability<br/>elasticNet_pred_Prob = elasticNet.predict_proba(X_test_transformed)[:,1]</span><span id="8733" class="nv mo it mm b gy oa nx l ny nz"># Get the false positive rate and true positive rate<br/>fpr,tpr, _ = roc_curve(y_test,elasticNet_pred_Prob)</span><span id="a719" class="nv mo it mm b gy oa nx l ny nz"># Get auc value<br/>auc = roc_auc_score(y_test,elasticNet_pred_Prob)</span><span id="55bf" class="nv mo it mm b gy oa nx l ny nz"># Plot the chart<br/>plt.plot(fpr,tpr,label="area="+str(auc))<br/>plt.legend(loc=4)</span></pre><p id="3c53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">弹性网的ROC/AUC值为0.9974，与岭回归大致相同。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/df5873a0ab2d13deb1eb8354fd190c2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7k1pRwqusF3NVcFw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">弹性网正则化ROC曲线的Logistic回归——来自GrabNGoInfo.com的图像</figcaption></figure><p id="189d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对数损失从岭回归的0.0601下降到0.0597。这是一个微小的进步。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="ca90" class="nv mo it mm b gy nw nx l ny nz"># Calculate log loss<br/>log_loss(y_test,elasticNet_pred_Prob)</span><span id="a0b1" class="nv mo it mm b gy oa nx l ny nz">0.05970787798591165</span></pre><p id="04f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">混淆矩阵显示同样的假阴性计数为1，假阳性计数从2减少到1。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="0c5f" class="nv mo it mm b gy nw nx l ny nz"># Confusion matrix<br/>plot_confusion_matrix(elasticNet, X_test_transformed, y_test)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/87128b69cfa796b6bd5ee9684dcbba59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/0*2iernTNiBwack2wG.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">使用弹性网正则化混淆矩阵的逻辑回归——来自GrabNGoInfo.com的图像</figcaption></figure><p id="1caf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为与LASSO和Ridge相比，错误预测的数量减少了，所以准确性从0.974增加到0.982。召回值相同，为0.986。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="3c54" class="nv mo it mm b gy nw nx l ny nz"># Performance report<br/>print(classification_report(y_test, elasticNet_prediction, digits=3))</span><span id="3571" class="nv mo it mm b gy oa nx l ny nz">precision    recall  f1-score   support</span><span id="449a" class="nv mo it mm b gy oa nx l ny nz">           0      0.977     0.977     0.977        43<br/>           1      0.986     0.986     0.986        71</span><span id="8b7a" class="nv mo it mm b gy oa nx l ny nz">    accuracy                          0.982       114<br/>   macro avg      0.981     0.981     0.981       114<br/>weighted avg      0.982     0.982     0.982       114</span></pre><p id="9884" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于某些要素，山脊的系数被设置为零，但系数为0的要素数量小于LASSO。</p><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="1300" class="nv mo it mm b gy nw nx l ny nz"># Model coefficients<br/>elasticNetCoeff = pd.concat([pd.DataFrame(X_test_transformed.columns),<br/>                             pd.DataFrame(np.transpose(elasticNet.coef_))], axis = 1)<br/>elasticNetCoeff.columns=['Variable','Coefficient']<br/>elasticNetCoeff['Coefficient_Abs']=elasticNetCoeff['Coefficient'].apply(abs)<br/>elasticNetCoeff.sort_values(by='Coefficient_Abs', ascending=False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/199ee3f4ddd511fda9688e369d7a394a.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/0*3TMKKXjkFpU6uCD2.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">具有弹性网络正则化系数的逻辑回归——来自GrabNGoInfo.com的图像</figcaption></figure><h1 id="7df1" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">步骤9:将所有代码放在一起</h1><pre class="kj kk kl km gt nr mm ns nt aw nu bi"><span id="0712" class="nv mo it mm b gy nw nx l ny nz">###### Step 1: Import Libraries</span><span id="1b91" class="nv mo it mm b gy oa nx l ny nz"># Dataset<br/>from sklearn import datasets</span><span id="a0ec" class="nv mo it mm b gy oa nx l ny nz"># Data processing<br/>import pandas as pd<br/>import numpy as np</span><span id="15ba" class="nv mo it mm b gy oa nx l ny nz"># Visualization<br/>import matplotlib.pyplot as plt</span><span id="ab42" class="nv mo it mm b gy oa nx l ny nz"># Standardize the data<br/>from sklearn.preprocessing import StandardScaler</span><span id="2376" class="nv mo it mm b gy oa nx l ny nz"># Model and performance evaluation<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import plot_confusion_matrix, classification_report, log_loss, roc_curve, roc_auc_score<br/></span><span id="a0ea" class="nv mo it mm b gy oa nx l ny nz">###### Step 2: Read In Data</span><span id="22fc" class="nv mo it mm b gy oa nx l ny nz"># Load the breast cancer dataset<br/>data = datasets.load_breast_cancer()</span><span id="8a87" class="nv mo it mm b gy oa nx l ny nz"># Put the data in pandas dataframe format<br/>df = pd.DataFrame(data=data.data, columns=data.feature_names)<br/>df['target']=data.target</span><span id="c180" class="nv mo it mm b gy oa nx l ny nz"># Check the data information<br/>df.info()</span><span id="2c9f" class="nv mo it mm b gy oa nx l ny nz"># Check the target value distribution<br/>df['target'].value_counts(normalize=True)<br/></span><span id="ed69" class="nv mo it mm b gy oa nx l ny nz">###### Step 3: Train Test Split</span><span id="1e1a" class="nv mo it mm b gy oa nx l ny nz"># Train test split<br/>X_train, X_test, y_train, y_test = train_test_split(df[df.columns.difference(['target'])], df['target'], test_size=0.2, random_state=42)</span><span id="4fd1" class="nv mo it mm b gy oa nx l ny nz"># Check the number of records in training and testing dataset.<br/>print(f'The training dataset has {len(X_train)} records.')<br/>print(f'The testing dataset has {len(X_test)} records.')<br/></span><span id="df95" class="nv mo it mm b gy oa nx l ny nz">###### Step 4: Standardization</span><span id="c7ae" class="nv mo it mm b gy oa nx l ny nz"># Initiate scaler<br/>sc = StandardScaler()</span><span id="675b" class="nv mo it mm b gy oa nx l ny nz"># Standardize the training dataset<br/>X_train_transformed = pd.DataFrame(sc.fit_transform(X_train),index=X_train.index, columns=X_train.columns)</span><span id="5671" class="nv mo it mm b gy oa nx l ny nz"># Standardized the testing dataset<br/>X_test_transformed = pd.DataFrame(sc.transform(X_test),index=X_test.index, columns=X_test.columns)</span><span id="2846" class="nv mo it mm b gy oa nx l ny nz"># Summary statistics after standardization<br/>X_train_transformed.describe().T</span><span id="1587" class="nv mo it mm b gy oa nx l ny nz"># Summary statistics before standardization<br/>X_train.describe().T<br/></span><span id="1955" class="nv mo it mm b gy oa nx l ny nz">###### Step 5: Logistic Regression With No Regularization</span><span id="cdbf" class="nv mo it mm b gy oa nx l ny nz"># Check default values<br/>LogisticRegression()</span><span id="ea8d" class="nv mo it mm b gy oa nx l ny nz"># Run model<br/>logistic = LogisticRegression(penalty='none', random_state=0).fit(X_train_transformed, y_train)</span><span id="e3c9" class="nv mo it mm b gy oa nx l ny nz"># Make prediction<br/>logistic_prediction = logistic.predict(X_test_transformed)</span><span id="5c19" class="nv mo it mm b gy oa nx l ny nz"># Get predicted probability<br/>logistic_pred_Prob = logistic.predict_proba(X_test_transformed)[:,1]</span><span id="e142" class="nv mo it mm b gy oa nx l ny nz"># Get the false positive rate and true positive rate<br/>fpr,tpr, _=roc_curve(y_test,logistic_pred_Prob)</span><span id="8511" class="nv mo it mm b gy oa nx l ny nz"># Get auc value<br/>auc=roc_auc_score(y_test,logistic_pred_Prob)</span><span id="6ce1" class="nv mo it mm b gy oa nx l ny nz"># Plot the chart<br/>plt.plot(fpr,tpr,label="area="+str(auc))<br/>plt.legend(loc=4)</span><span id="ce93" class="nv mo it mm b gy oa nx l ny nz"># Caclulate log loss<br/>log_loss(y_test,logistic_pred_Prob)</span><span id="0fa1" class="nv mo it mm b gy oa nx l ny nz"># Confusion matrix<br/>plot_confusion_matrix(logistic, X_test_transformed, y_test)</span><span id="8ca6" class="nv mo it mm b gy oa nx l ny nz"># Performance report<br/>print(classification_report(y_test, logistic_prediction, digits=3))</span><span id="ea0f" class="nv mo it mm b gy oa nx l ny nz"># Model coefficients<br/>LogisticCoeff = pd.concat([pd.DataFrame(X_test_transformed.columns),pd.DataFrame(np.transpose(logistic.coef_))], axis = 1)<br/>LogisticCoeff.columns=['Variable','Coefficient']<br/>LogisticCoeff['Coefficient_Abs']=LogisticCoeff['Coefficient'].apply(abs)<br/>LogisticCoeff.sort_values(by='Coefficient_Abs', ascending=False)<br/></span><span id="016d" class="nv mo it mm b gy oa nx l ny nz">###### Step 6: LASSO</span><span id="1412" class="nv mo it mm b gy oa nx l ny nz"># Run model<br/>lasso = LogisticRegression(penalty='l1', solver='liblinear', random_state=0).fit(X_train_transformed, y_train)</span><span id="750f" class="nv mo it mm b gy oa nx l ny nz"># Make prediction<br/>lasso_prediction = lasso.predict(X_test_transformed)</span><span id="6c3e" class="nv mo it mm b gy oa nx l ny nz"># Get predicted probability<br/>lasso_pred_Prob = lasso.predict_proba(X_test_transformed)[:,1]</span><span id="442e" class="nv mo it mm b gy oa nx l ny nz"># Get the false positive rate and true positive rate<br/>fpr,tpr, _= roc_curve(y_test,lasso_pred_Prob)</span><span id="bc4f" class="nv mo it mm b gy oa nx l ny nz"># Get auc value<br/>auc = roc_auc_score(y_test,lasso_pred_Prob)</span><span id="e46e" class="nv mo it mm b gy oa nx l ny nz"># Plot the chart<br/>plt.plot(fpr,tpr,label="area="+str(auc))<br/>plt.legend(loc=4)</span><span id="f413" class="nv mo it mm b gy oa nx l ny nz"># Calculate log loss<br/>log_loss(y_test,lasso_pred_Prob)</span><span id="6bca" class="nv mo it mm b gy oa nx l ny nz"># Confusion matrix<br/>plot_confusion_matrix(lasso, X_test_transformed, y_test)</span><span id="c953" class="nv mo it mm b gy oa nx l ny nz"># Performance report<br/>print(classification_report(y_test, lasso_prediction, digits=3))</span><span id="1c8b" class="nv mo it mm b gy oa nx l ny nz"># Model coefficients<br/>lassoCoeff = pd.concat([pd.DataFrame(X_test_transformed.columns),pd.DataFrame(np.transpose(lasso.coef_))], axis = 1)<br/>lassoCoeff.columns=['Variable','Coefficient']<br/>lassoCoeff['Coefficient_Abs']=lassoCoeff['Coefficient'].apply(abs)<br/>lassoCoeff.sort_values(by='Coefficient_Abs', ascending=False)<br/></span><span id="feb0" class="nv mo it mm b gy oa nx l ny nz">###### Step 7: Ridge</span><span id="1d94" class="nv mo it mm b gy oa nx l ny nz"># Run model<br/>ridge = LogisticRegression(penalty='l2', random_state=0).fit(X_train_transformed, y_train)</span><span id="3023" class="nv mo it mm b gy oa nx l ny nz"># Make prediction<br/>ridge_prediction = ridge.predict(X_test_transformed)</span><span id="665c" class="nv mo it mm b gy oa nx l ny nz"># Get predicted probability<br/>ridge_pred_Prob = ridge.predict_proba(X_test_transformed)[:,1]</span><span id="5307" class="nv mo it mm b gy oa nx l ny nz"># Get the false positive rate and true positive rate<br/>fpr,tpr, _= roc_curve(y_test,ridge_pred_Prob)</span><span id="a70d" class="nv mo it mm b gy oa nx l ny nz"># Get auc value<br/>auc = roc_auc_score(y_test,ridge_pred_Prob)</span><span id="a3ad" class="nv mo it mm b gy oa nx l ny nz"># Plot the chart<br/>plt.plot(fpr,tpr,label="area="+str(auc))<br/>plt.legend(loc=4)</span><span id="ed49" class="nv mo it mm b gy oa nx l ny nz"># Calculate log loss<br/>log_loss(y_test,ridge_pred_Prob)</span><span id="364c" class="nv mo it mm b gy oa nx l ny nz"># Confusion matrix<br/>plot_confusion_matrix(ridge, X_test_transformed, y_test)</span><span id="1a70" class="nv mo it mm b gy oa nx l ny nz"># Performance matrix<br/>print(classification_report(y_test, ridge_prediction, digits=3))</span><span id="c214" class="nv mo it mm b gy oa nx l ny nz"># Model coefficients<br/>ridgeCoeff = pd.concat([pd.DataFrame(X_test_transformed.columns),pd.DataFrame(np.transpose(ridge.coef_))], axis = 1)<br/>ridgeCoeff.columns=['Variable','Coefficient']<br/>ridgeCoeff['Coefficient_Abs']=ridgeCoeff['Coefficient'].apply(abs)<br/>ridgeCoeff.sort_values(by='Coefficient_Abs', ascending=False)<br/></span><span id="38e2" class="nv mo it mm b gy oa nx l ny nz">###### Step 8: Elastic Net</span><span id="f5a7" class="nv mo it mm b gy oa nx l ny nz"># Run model<br/>elasticNet = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, random_state=0).fit(X_train_transformed, y_train)</span><span id="8253" class="nv mo it mm b gy oa nx l ny nz"># Make prediction<br/>elasticNet_prediction = elasticNet.predict(X_test_transformed)</span><span id="ee6b" class="nv mo it mm b gy oa nx l ny nz"># Get predicted probability<br/>elasticNet_pred_Prob = elasticNet.predict_proba(X_test_transformed)[:,1]</span><span id="da1b" class="nv mo it mm b gy oa nx l ny nz"># Get the false positive rate and true positive rate<br/>fpr,tpr, _ = roc_curve(y_test,elasticNet_pred_Prob)</span><span id="80fc" class="nv mo it mm b gy oa nx l ny nz"># Get auc value<br/>auc = roc_auc_score(y_test,elasticNet_pred_Prob)</span><span id="e219" class="nv mo it mm b gy oa nx l ny nz"># Plot the chart<br/>plt.plot(fpr,tpr,label="area="+str(auc))<br/>plt.legend(loc=4)</span><span id="3228" class="nv mo it mm b gy oa nx l ny nz"># Calculate log loss<br/>log_loss(y_test,elasticNet_pred_Prob)</span><span id="145a" class="nv mo it mm b gy oa nx l ny nz"># Confusion matrix<br/>plot_confusion_matrix(elasticNet, X_test_transformed, y_test)</span><span id="9514" class="nv mo it mm b gy oa nx l ny nz"># Performance report<br/>print(classification_report(y_test, elasticNet_prediction, digits=3))</span><span id="2f58" class="nv mo it mm b gy oa nx l ny nz"># Model coefficients<br/>elasticNetCoeff = pd.concat([pd.DataFrame(X_test_transformed.columns),pd.DataFrame(np.transpose(elasticNet.coef_))], axis = 1)<br/>elasticNetCoeff.columns=['Variable','Coefficient']<br/>elasticNetCoeff['Coefficient_Abs']=elasticNetCoeff['Coefficient'].apply(abs)<br/>elasticNetCoeff.sort_values(by='Coefficient_Abs', ascending=False)</span></pre></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><h1 id="bc3d" class="mn mo it bd mp mq ou ms mt mu ov mw mx jz ow ka mz kc ox kd nb kf oy kg nd ne bi translated">摘要</h1><p id="801e" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在本教程中，我们介绍了</p><ul class=""><li id="83f9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">套索(L1)，山脊(L2)，弹力网有什么区别？</li><li id="a300" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如何为分类模型运行LASSO？</li><li id="70a9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">分类模型如何运行Ridge？</li><li id="6498" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">分类模型如何运行弹性网？</li><li id="8584" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">如何比较套索、脊、弹力网的性能？</li></ul><p id="3999" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，正则化比不使用正则化显著提高了模型性能。</p><p id="58c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在三种正则化算法中，弹性网的性能最好，其次是岭和套索回归。然而，并非所有数据集都是如此。因此，我建议为您的项目尝试所有三种算法，进行超参数调优，并选择最适合您的数据集的算法。</p><p id="5772" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更多教程可在GrabNGoInfo <a class="ae ky" href="https://www.youtube.com/channel/UCmbA7XB6Wb7bLwJw9ARPcYg" rel="noopener ugc nofollow" target="_blank"> YouTube频道</a>和GrabNGoInfo.com<a class="ae ky" href="https://grabngoinfo.com/tutorials/" rel="noopener ugc nofollow" target="_blank">网站</a>上获得</p><h1 id="e9c2" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">推荐教程</h1><ul class=""><li id="f095" class="lv lw it lb b lc nf lf ng li oz lm pa lq pb lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/grabngoinfo/grabngoinfo-machine-learning-tutorials-inventory-9b9d78ebdd67" rel="noopener"> GrabNGoInfo机器学习教程盘点</a></li><li id="85ff" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/one-class-svm-for-anomaly-detection-6c97fdd6d8af" rel="noopener">用于异常检测的单级SVM</a></li><li id="a932" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/3-ways-for-multiple-time-series-forecasting-using-prophet-in-python-7a0709a117f9" rel="noopener">使用Python中的Prophet进行多时间序列预测的3种方法</a></li><li id="1113" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python-7304aedf9037" rel="noopener">使用Python实现不平衡分类的四种过采样和欠采样方法</a></li><li id="4f91" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/multivariate-time-series-forecasting-with-seasonality-and-holiday-effect-using-prophet-in-python-d5d4150eeb57" rel="noopener">利用Python中的Prophet进行具有季节性和假日效应的多元时间序列预测</a></li><li id="c98c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/how-to-detect-outliers-data-science-interview-questions-and-answers-1e400284f6b4" rel="noopener">如何检测离群值|数据科学面试问答</a></li><li id="e048" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/time-series-anomaly-detection-using-prophet-in-python-877d2b7b14b4" rel="noopener">利用Python中的Prophet进行时间序列异常检测</a></li><li id="0983" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/how-to-use-r-with-google-colab-notebook-610c3a2f0eab" rel="noopener">如何用谷歌Colab笔记本使用R</a></li></ul><h1 id="a76f" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">参考</h1><ul class=""><li id="2bd2" class="lv lw it lb b lc nf lf ng li oz lm pa lq pb lu ma mb mc md bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=NGf0voTMlcs" rel="noopener ugc nofollow" target="_blank">套索回归解释</a></li><li id="b702" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=Q81RR3yKn30" rel="noopener ugc nofollow" target="_blank">岭回归解释</a></li><li id="a4e4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank"> sklearn关于逻辑回归的文档</a></li><li id="004b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">标准定标器上的sklearn文档</a></li></ul><div class="pc pd gp gr pe pf"><a href="https://medium.com/@AmyGrabNGoInfo/membership" rel="noopener follow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">通过我的推荐链接加入媒体-艾米GrabNGoInfo</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">medium.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt ks pf"/></div></div></a></div></div></div>    
</body>
</html>