<html>
<head>
<title>Hyper-parameters: RandomSeachCV and GridSearchCV in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数:机器学习中的RandomSeachCV和GridSearchCV</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/hyper-parameters-randomseachcv-and-gridsearchcv-in-machine-learning-b7d091cf56f4?source=collection_archive---------1-----------------------#2021-07-26">https://pub.towardsai.net/hyper-parameters-randomseachcv-and-gridsearchcv-in-machine-learning-b7d091cf56f4?source=collection_archive---------1-----------------------#2021-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d62f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="66d4" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">提高算法精确度的技术</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f06db87a85aef3769fc88fcfbd50a212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3I2U_IOA3FH_J5ENMTMyag.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><blockquote class="lh li lj"><p id="8e40" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd"> <em class="it">超参数调谐</em> </strong></p></blockquote><p id="0196" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">在本文中，我们将讨论超参数调优。当我们谈论提高机器和深度学习模型的准确性时，我们首先想到的是调整参数。</p><blockquote class="lh li lj"><p id="5612" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd"> <em class="it">要涵盖的主题</em> </strong></p></blockquote><pre class="ks kt ku kv gt mk ml mm mn aw mo bi"><span id="7dd1" class="mp mq it ml b gy mr ms l mt mu">1. What is hyper-parameter tuning?<br/>2. Why do we need hyper-parameter tuning?<br/>3. Hyper-parameter Types<br/>4. Techniques of hyper-parameter tuning<br/>   a. GridSearchCV<br/>   b. RandomizedSearchCV<br/>5. Bayesian Optimization -Automate Hyper-parameter Tuning(Hyper-<br/>   optimization)</span></pre><blockquote class="lh li lj"><p id="d764" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd"> <em class="it">什么是超参数调谐？</em>T13】</strong></p></blockquote><p id="1f3b" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">“超参数调整就是为学习算法选择一组最优的超参数”。我们对不同的机器学习模型使用不同的输入参数。这些输入参数被命名为<strong class="ln jd">超参数</strong>或超参数，其值用于控制学习过程。超参数是可以在模型中更改以获得最合适值的参数。例如，我们决定选择隐藏层和每层中节点的数量。模型性能在很大程度上取决于超参数。<strong class="ln jd">超参数整定</strong>也称为<strong class="ln jd">超参数优化。</strong></p><p id="2eb9" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">不同模型中不同类型的超参数技术</p><ul class=""><li id="1279" class="mv mw it ln b lo lp lr ls mh mx mi my mj mz mg na nb nc nd bi translated">决策树的最大深度</li><li id="b5c4" class="mv mw it ln b lo ne lr nf mh ng mi nh mj ni mg na nb nc nd bi translated">随机森林中的树木数量</li><li id="60f6" class="mv mw it ln b lo ne lr nf mh ng mi nh mj ni mg na nb nc nd bi translated">K中的K-最近邻</li><li id="e5d4" class="mv mw it ln b lo ne lr nf mh ng mi nh mj ni mg na nb nc nd bi translated">梯度下降中的学习速率</li><li id="4740" class="mv mw it ln b lo ne lr nf mh ng mi nh mj ni mg na nb nc nd bi translated">支持向量机中的c和sigma</li><li id="e4a7" class="mv mw it ln b lo ne lr nf mh ng mi nh mj ni mg na nb nc nd bi translated">逻辑回归分类器中的惩罚，即L1或L2正则化</li><li id="015f" class="mv mw it ln b lo ne lr nf mh ng mi nh mj ni mg na nb nc nd bi translated">训练神经网络的学习速率</li></ul><div class="nj nk gp gr nl nm"><a rel="noopener  ugc nofollow" target="_blank" href="/fully-explained-k-nearest-neighbors-with-python-ebbe27f93ba9"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jd gy z fp nr fr fs ns fu fw jc bi translated">用Python完整解释K近邻</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">数据科学中解决真实案例的机器学习分类算法研究。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">pub.towardsai.net</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa lb nm"/></div></div></a></div><div class="nj nk gp gr nl nm"><a rel="noopener  ugc nofollow" target="_blank" href="/regression-and-classification-metrics-in-machine-learning-with-python-6d9fcd8b73aa"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jd gy z fp nr fr fs ns fu fw jc bi translated">Python机器学习中的回归和分类度量</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">使用度量API进行回归和分类的模型评估</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">pub.towardsai.net</p></div></div><div class="nv l"><div class="ob l nx ny nz nv oa lb nm"/></div></div></a></div><blockquote class="lh li lj"><p id="eba3" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd"> <em class="it">我们为什么需要超参数调优？</em> </strong></p></blockquote><p id="e4a1" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">超参数调整的主要目的是获得建模中使用的所有参数的优化值，以获得更高的精度。它用于防止过度配合。</p><p id="44e2" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd"> <em class="lm">过度拟合:</em> </strong>当我们训练模型时，训练数据的准确性变高，但当我们在新点上测试模型时，准确性变得非常低，这种情况称为过度拟合。为了防止这种情况，我们使用<strong class="ln jd">超参数调谐。</strong></p><blockquote class="lh li lj"><p id="4951" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd"> <em class="it">超参数类型</em> </strong></p></blockquote><p id="f715" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">超参数有一种类型的数据值，我们试图将它们固定在如下所示的范围内:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ae74298f0cd41d8da207599fad2ff36a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*PTg7e2Q7TWCjFYXvj6j9WQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><blockquote class="lh li lj"><p id="84ff" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd"> <em class="it">超参数整定技术</em> </strong></p></blockquote><h2 id="0327" class="mp mq it bd od oe of dn og oh oi dp oj mh ok ol om mi on oo op mj oq or os iz bi translated"><strong class="ak"> GridSearchCV </strong></h2><p id="cc8b" class="pw-post-body-paragraph lk ll it ln b lo ot kd lq lr ou kg lt mh ov lw lx mi ow ma mb mj ox me mf mg im bi translated">该库用于为我们的模型寻找最佳超参数，以获得更好的精度。这种方法被称为GridSearchCV，因为它从超参数值网格中搜索最佳超参数集。如上所述，这是执行超参数调整以确定给定模型的最佳值的过程。交叉验证方法用于找到目标估计器(模型)的训练和测试集。</p><p id="2739" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">当我们需要为目标模型和数据集调整参数时，GridSearchCV非常有用。在该方法中，通过交叉验证测试多个参数，并且可以提取最佳参数以应用于预测模型。在模型中实现调谐参数后，我们得到精度和损失值，因此，我们试图找到具有更高精度和最小损失值的最佳参数。</p><p id="d7af" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">GridSearchCV是Scikit-learn模型选择包中的一个函数</p><pre class="ks kt ku kv gt mk ml mm mn aw mo bi"><span id="c7a6" class="mp mq it ml b gy mr ms l mt mu">(<strong class="ml jd">from</strong> <strong class="ml jd">sklearn.model_selection</strong> <strong class="ml jd">import</strong> GridSearchCV)</span></pre><p id="06a6" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">GridSearchCV方法中使用的参数数量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/1c21688b07ac014adf91cebc33413389.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*ZBsUomt62G-iSAUHIASZ7w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="48bc" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">在这个过程中，我们将hyper-parameter的预定义值传递给GridSearchCV函数。我们制作一个字典，并提及参数及其值。</p><p id="6236" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">下面是一个例子:</p><p id="aa7a" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">在这项技术中，我们将学习如何使用sklearn的GridSearchCV为以下数据集找出支持向量机的最佳参数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/19238cc3a0f5b17dec055b414cae229a.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*L1DsRnqlC5zW18j-x0oaug.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="a8a4" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">支持向量机的可调参数:{'c '，' gamma '，' kernel'}</p><p id="c278" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">GridSearchCV的例子</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/32c17dc393e30fb9cea92bbc0a24cde1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*oYtrbm-6SukiJhrolBR6NA.png"/></div></figure><p id="3fe7" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">{ 'C' : 1，' gamma' : 0.7，' kernel' : 'rbf' }是上述数据集的SVM模型的最佳超参数值。对于其他数据集，SVM估计器的调整参数具有不同的值。</p><div class="nj nk gp gr nl nm"><a rel="noopener  ugc nofollow" target="_blank" href="/fully-explained-svm-classification-with-python-eda124997bcd"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jd gy z fp nr fr fs ns fu fw jc bi translated">用Python全面解释了SVM分类</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">如何用一个真实的例子解决分类问题。</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">pub.towardsai.net</p></div></div><div class="nv l"><div class="pb l nx ny nz nv oa lb nm"/></div></div></a></div><h2 id="3e97" class="mp mq it bd od oe of dn og oh oi dp oj mh ok ol om mi on oo op mj oq or os iz bi translated"><strong class="ak"> RandomizedSearchCV </strong></h2><p id="0d5f" class="pw-post-body-paragraph lk ll it ln b lo ot kd lq lr ou kg lt mh ov lw lx mi ow ma mb mj ox me mf mg im bi translated">随机搜索CV是寻找最佳超参数的另一个例子。此方法用于实现“fit”和“score”方法。除了这两个之外，它还实现了“预测”、“预测概率”、“决策函数”、“变换”和“逆变换”，如果它们被使用的话。这种方法与网格搜索CV的不同之处在于，它不尝试调整所有参数，而是调整分布中的一些指定参数。当我们调整所有参数时，我们会列出一个列表，如果我们只选择一个参数，那么它也来自分布。</p><p id="99b7" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">在这种方法中，当我们使用连续参数时，我们也应该在调整中选择连续分布。</p><p id="f2e8" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">RandomSearchCV改进了GridSearchCV的缺点，因为它也适用于有限数量的超参数。这种搜索方法在网格中以随机方式设置最佳参数。这种方法降低了不必要的计算复杂度</p><p id="31e0" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">RandomizedSearchCV是Scikit-learn模型选择包中的一个函数</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/30cd67e1434e9d8c6d26ea96d7ebcbe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*MF8rpzNAfeoaX1YRyps2Bg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="1c1d" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">让我们检查一下RandomSearchCV中使用的所有参数，以及需要调优的参数有哪些。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/cd29048e77b881ec8e114f5b09b8b327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*CBcTXhP18hW4V3jI-U9NjA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="9f91" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">随机搜索的示例</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/24d689438aef9558ebb97d2dbde85567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*c4DUxSryNHRYjXzZ9_JvtQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><blockquote class="lh li lj"><p id="b33b" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated"><strong class="ln jd"> <em class="it">【贝叶斯优化-自动化超参数调优(Hyper-optimization) </em> </strong></p></blockquote><p id="c847" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">这种优化方法用于自动调整参数，并使用贝叶斯方法寻找目标函数的最小值和最大值。在这种情况下，我们找到了一个函数的最合适的输入值，它可以给出最低可能的输出值。这种技术在评估复杂和有噪声的数据时很有用。</p><p id="3bd1" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd"> <em class="lm">贝叶斯优化中的一些常用术语</em> </strong></p><p id="1a03" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">样本:</strong>样本通常被定义为n维空间中预定义范围的变量向量。</p><p id="f1b8" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">目标函数:</strong>定义损失函数以最小化</p><p id="c1b4" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">域空间:</strong>定义函数输入值的范围</p><p id="f952" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">成本:</strong>通过目标函数计算的样本数值</p><p id="5eb0" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">库</strong> — <em class="lm">远视</em></p><p id="4d0d" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">这个开源库用于大规模优化。该库用于通过串行和并行技术优化真实、条件和离散维度。Hyper-opt使用贝叶斯优化的形式进行参数调整，从中我们可以获得给定模型的最佳参数。</p><div class="nj nk gp gr nl nm"><a rel="noopener  ugc nofollow" target="_blank" href="/data-preprocessing-concepts-with-python-b93c63f14bb6"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jd gy z fp nr fr fs ns fu fw jc bi translated">Python中的数据预处理概念</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">一种为机器学习估值器准备数据的稳健方法</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">pub.towardsai.net</p></div></div><div class="nv l"><div class="pd l nx ny nz nv oa lb nm"/></div></div></a></div><p id="6cb8" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">用于贝叶斯优化的hyper-opt中的一些函数:- </strong></p><p id="4e5a" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">搜索空间:</strong> hyper-opt具有不同的功能来指定输入参数的范围。</p><pre class="ks kt ku kv gt mk ml mm mn aw mo bi"><span id="8710" class="mp mq it ml b gy mr ms l mt mu">hp.choice (label,options)<br/>hp.randint(label,upper)<br/>hp.uniform(label,low,high)</span></pre><p id="125e" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd"> fmin: </strong>该函数用于最小化目标函数，并通过迭代不同的估值器及其调整参数进行优化。</p><pre class="ks kt ku kv gt mk ml mm mn aw mo bi"><span id="542f" class="mp mq it ml b gy mr ms l mt mu">parameters :- fmin<strong class="ml jd">(</strong>fn<strong class="ml jd">,<br/>                   </strong>space<strong class="ml jd">,<br/>                   </strong>algo<strong class="ml jd">,<br/>                   </strong>max_evals<strong class="ml jd">=9223372036854775807,<br/>                   </strong>trials<strong class="ml jd">=None)</strong></span></pre><p id="d9a0" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">试验:</strong>用于保存所有关于超参数、损耗和其他信息的记录。</p><pre class="ks kt ku kv gt mk ml mm mn aw mo bi"><span id="ea84" class="mp mq it ml b gy mr ms l mt mu">from hyperopt import Trials<br/>trials = Trials()</span></pre><p id="6cd1" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">要遵循的步骤</strong></p><ul class=""><li id="6933" class="mv mw it ln b lo lp lr ls mh mx mi my mj mz mg na nb nc nd bi translated"><strong class="ln jd">导入库及其功能</strong></li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/896bf7ff8a2797130b2ab013e8041fce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*3A_34giQdoMwVZt3oOh-QA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><ul class=""><li id="f3ed" class="mv mw it ln b lo lp lr ls mh mx mi my mj mz mg na nb nc nd bi translated"><strong class="ln jd">定义优化的参数空间</strong></li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/d069e61d6a4b504f54cc5e357096f149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*4fMs57_tIZ4GL4a8DThjrQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><ul class=""><li id="bf07" class="mv mw it ln b lo lp lr ls mh mx mi my mj mz mg na nb nc nd bi translated"><strong class="ln jd">定义一个函数来最小化</strong></li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/65bde5bc7892acfec15fb4a527b6ccd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Ggl8ZDqN4X8FlzgRGOnapQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><ul class=""><li id="17ee" class="mv mw it ln b lo lp lr ls mh mx mi my mj mz mg na nb nc nd bi translated"><strong class="ln jd">微调模型</strong></li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/8cbaac5592d8055aed2a9bde64fca4c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*NlpPuWX8uPR-n8UFuEujeA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><ul class=""><li id="d37a" class="mv mw it ln b lo lp lr ls mh mx mi my mj mz mg na nb nc nd bi translated"><strong class="ln jd"> Trail对象正在分析结果</strong></li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/d10ea911c0e98cd32a1a6dcd017824fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*OBSs7ASvbzy342PcgdVxxw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="3b5a" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated"><strong class="ln jd">结论</strong>:这是一篇关于超参数调整的文章，用于提高机器学习模型的准确性。</p><p id="d4e3" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mh lv lw lx mi lz ma mb mj md me mf mg im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae ph" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae ph" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="711a" class="pi mq it bd od pj pk pl og pm pn po oj ki pp kj om kl pq km op ko pr kp os ps bi translated">推荐文章</h1><p id="ebcd" class="pw-post-body-paragraph lk ll it ln b lo ot kd lq lr ou kg lt mh ov lw lx mi ow ma mb mj ox me mf mg im bi translated"><a class="ae ph" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄用Python </a> <br/> 2。<a class="ae ph" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a> <br/> 3。<a class="ae ph" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae ph" rel="noopener ugc nofollow" target="_blank" href="/deep-learning-88e218b74a14?source=friends_link&amp;sk=540bf9088d31859d50dbddab7524ba35">为什么LSTM在深度学习方面比RNN更有用？</a> <br/> 5。<a class="ae ph" rel="noopener ugc nofollow" target="_blank" href="/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88?source=friends_link&amp;sk=6844935e3de14e478ce00f0b22e419eb">神经网络:递归神经网络的兴起</a> <br/> 6。<a class="ae ph" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python充分解释了线性回归</a> <br/> 7。<a class="ae ph" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae ph" rel="noopener ugc nofollow" target="_blank" href="/differences-between-concat-merge-and-join-with-python-1a6541abc08d?source=friends_link&amp;sk=3b37b694fb90db16275059ea752fc16a">concat()、merge()和join()与Python </a> <br/> 9的区别。<a class="ae ph" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a>T30】10。<a class="ae ph" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>