<html>
<head>
<title>Google Colab: Performance Analysis in a real Deep Learning Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Google Colab:真实深度学习项目中的性能分析</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/google-colab-performance-analysis-in-a-real-deep-learning-project-9a0d4f8f489d?source=collection_archive---------1-----------------------#2022-01-18">https://pub.towardsai.net/google-colab-performance-analysis-in-a-real-deep-learning-project-9a0d4f8f489d?source=collection_archive---------1-----------------------#2022-01-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="72e9" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="de76" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用TensorFlow和Keras APIs</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/6ec884b8364826a20a22e84e9b770548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*ueWsHxKCEOpOmlQKFKYxKg.png"/></div></figure><h2 id="f9f9" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt iz bi translated">0.介绍</h2><p id="b10f" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc li md me mf lm mg mh mi lq mj mk ml mm im bi translated">Google Colab项目旨在为每个拥有电脑(甚至只是平板电脑或智能手机)和互联网连接的人提供免费的GPU访问。不是每个人都负担得起数据科学项目的强大GPU，因此Google Colab是一个可能的解决方案。</p><p id="1350" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">但是Google Colab的优势并不仅限于GPU访问，免费的在线接口提供了一个功能齐全的Jupyter笔记本环境，并且易于分享(可以通过Google Drive分享，或者用GitHub连接账号)。这意味着您将需要0(零！)配置使用GPU开始你的机器学习项目。你觉得这听起来不错吗？我觉得听起来很神奇！</p><blockquote class="ms"><p id="cb25" class="mt mu it bd mv mw mx my mz na nb mm dk translated">所以我决定尝试一下！</p></blockquote><h2 id="dd8c" class="kz la it bd lb lc nc dn le lf nd dp lh li ne lk ll lm nf lo lp lq ng ls lt iz bi translated"><strong class="ak"> 1。配置</strong></h2><p id="bfe6" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc li md me mf lm mg mh mi lq mj mk ml mm im bi translated">你唯一需要做的就是创建一个谷歌账户。让我猜猜，你已经有一个了…只要登陆<a class="ae nh" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank"> Google Colab项目页面</a>并选择“新笔记本”。新页面将看起来像一个Jupyter笔记本环境。你将能够编写可执行代码和markdown文本来描述你的项目。您可以更改为深色主题，但默认主题是浅色。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/5b86a9dd60c7cd1344dc19fbc2d36fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*M1K27R11g7JJdZmnEPwQRw.png"/></div></figure><h2 id="848a" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt iz bi translated"><strong class="ak"> 2。项目描述</strong></h2><p id="f93c" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc li md me mf lm mg mh mi lq mj mk ml mm im bi translated">如标题所示，这是一个Google Colab性能分析，带有一个用于医学研究目的的真实数据科学项目。简而言之，该项目的目的是为医学图像的质量控制(QC)提供帮助。使用迁移学习方法，这意味着将使用不同的Keras模型API。将使用相同的超参数测试六个模型，并比较每个模型的性能。</p><p id="5308" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated"><strong class="lw jd"> <em class="nj">可能的图像类型:</em> </strong></p><p id="9d1b" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated"><strong class="lw jd"> <em class="nj"> (0)质量好</em> </strong>，所有参数都有用；</p><p id="7e10" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated"><strong class="lw jd"> <em class="nj"> (1)质量一般</em> </strong>，部分参数仍可使用，但重复测试为宜；</p><p id="fdf4" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated"><strong class="lw jd"><em class="nj">②</em></strong>质量差，拒绝数据，测试必须重复。</p><p id="c840" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">再说一次，这个项目并不打算提供一个医疗诊断，或将图像分类为健康/疾病，只有质量控制工具。这样，在旨在将图像分类为健康/疾病的后续项目中，可以包括更多“质量好”的图像，并避免“质量差”的图像，从而减少由于数据质量差而导致的偏差。</p><h2 id="c90a" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt iz bi translated"><strong class="ak"> 3。数据库上传</strong></h2><p id="ff5f" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc li md me mf lm mg mh mi lq mj mk ml mm im bi translated">有两种简单的方法可以上传你的数据用于Google Colab。一个是永久的，另一个是临时的，只在你的疗程结束时有效(最多12小时)。</p><p id="d899" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">您可以使用左侧菜单直接将您的数据上传到您的Google Colab会话。只需点击类似文件夹的图片，并选择上传选项。这只会以一种临时的方式上传你的数据库，这意味着每次你启动一个新的会话，你都需要上传数据。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/58227480185af09cf2f4be6620a4b3ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*Xw8dXE_Kgv1nzZOfk9KweA.png"/></div></figure><p id="6459" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">另一个选择(至少对我来说是更有效的选择)是将数据库上传到Google Drive，然后将Google Drive与Colab连接起来。你可以在同一个菜单中完成，或者你可以在你的笔记本的开头使用一个小的代码片段，永远不要忘记连接驱动器。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="dee6" class="kz la it nm b gy nq nr l ns nt">from google.colab import drive<br/>drive.mount('/content/drive')</span></pre><p id="4c91" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">用于本项目的数据库样本包含3236幅图像，大小为75x75，分布在三个类别中，如下图所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/16e11fb5c6d50e0c703c22b595e5a014.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*KRdNnTxo65yAgnUvR0LB4w.png"/></div></figure><p id="cd8e" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">数据被分成训练、测试和验证子集，训练子集中有2200幅图像，验证子集中有550幅图像，测试子集中有486幅图像。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/56c3675cec029d6aedec643873c17f03.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*J6v9AkfUiqxMe-EQ5Lr_Mg.png"/></div></figure><h2 id="ecda" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt iz bi translated"><strong class="ak"> 4。代码和模型API</strong></h2><p id="9e51" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc li md me mf lm mg mh mi lq mj mk ml mm im bi translated">在这项分析中，六个Keras应用程序用于迁移学习项目，旨在识别项目描述中解释的三种图像类别。选择的模型有:</p><p id="690a" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated"><a class="ae nh" href="https://keras.io/api/applications/xception/" rel="noopener ugc nofollow" target="_blank">例外</a><br/><a class="ae nh" href="https://keras.io/api/applications/vgg/#vgg16-function" rel="noopener ugc nofollow" target="_blank">vgg 16</a><br/><a class="ae nh" href="https://keras.io/api/applications/inceptionv3/" rel="noopener ugc nofollow" target="_blank">InceptionV3</a><br/><a class="ae nh" href="https://keras.io/api/applications/densenet/#densenet121-function" rel="noopener ugc nofollow" target="_blank">dense net 121</a><br/><a class="ae nh" href="https://keras.io/api/applications/inceptionresnetv2/" rel="noopener ugc nofollow" target="_blank">inceptionresnet v2</a><br/><a class="ae nh" href="https://keras.io/api/applications/resnet/#resnet50v2-function" rel="noopener ugc nofollow" target="_blank">resnet 50 v2</a></p><p id="8ab9" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">相同的超参数用于所有模型，批量大小为32，时期数= 100，学习率为0.0001。选择最初的高次数和低学习率是为了确定每个模型的理想次数。所有模型都使用“<em class="nj"> imagenet </em>”和“<em class="nj">soft max”</em>激活的权重。</p><h2 id="5790" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt iz bi translated"><strong class="ak"> 5。结果:Google Colab性能</strong></h2><p id="7856" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc li md me mf lm mg mh mi lq mj mk ml mm im bi translated">不得不说，Google Colab免费提供的GPU能力让我印象深刻。在启用GPU的情况下运行深度学习项目比使用CPU快10倍。这是一个整体即时满足的体验。不过，也有一些方面我没有享受到那么多，比如免费账号的GPU可用性低，或者不是所有机型都有TensorFlow版本的GPU原生支持。</p><p id="71fe" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">我将分享的每个模型的衡量标准是:</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="9853" class="kz la it nm b gy nq nr l ns nt">1. Type of processor used;<br/>2. Time per step;<br/>3. Time per epoch;<br/>4. Trainable parameters;<br/>5. Total time execution.</span></pre><p id="7a79" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">我将从我不喜欢Google Colab的方面开始。为了运行这个测试，我需要大约一周的时间才能用GPU运行所有模型。谷歌限制免费账户中的GPU访问，根据我的经验，这意味着我每天只能访问大约2小时。过了这段时间，我的会话自动断开，大约24小时后我无法再次连接GPU。如果您的会话在代码运行过程中断开，这种情况会特别令人沮丧。</p><p id="cdf4" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">这意味着我每天只能运行一个模型。这对于学习目的或小型研究项目来说已经足够了，尤其是如果您已经准备好了所有的代码。但对于更大的研究项目或商业来说，这不是最好的选择。</p><p id="4a81" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">我发现了另一个我在运行Xception模型时无法解决的问题，用Google Colab检索en error:<br/>"<em class="nj">DepthwiseConvBackpropFilter的确定性GPU实现目前不可用。</em><br/>这很奇怪，尤其是考虑到Xception是谷歌开发的一个模型。所以，我决定只使用CPU运行Xception，并取结果进行比较。GPU用于所有其他模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/26c89f00fd5e14bcaf55d1bf5130492e.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*a8B2R0P7jNYbi9Sw89QTWA.png"/></div></figure><p id="eba4" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">每个模型的可训练参数的数量是不同的，这是影响执行时间的一个因素。因此，向您展示每个模型中的参数数量是公平的。InceptionResNetV2是迄今为止具有较高数量或可训练参数的模型，DenseNet121是就要训练的参数数量而言较轻的模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/15793a9af01f3ab2e5c124f8ecc5e171.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*M2BSJsrhjU2XE9flM5b8uA.png"/></div></figure><p id="fc51" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">下图总结了每个时期的时间。正如我们所见，GPU对减少每个时期的时间需求有着巨大的影响。另一个有趣的事实是，DenseNet121是可训练参数较少的模型，但它不是训练最快的模型。VGG16是纪元时间更快的型号。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d7eb6c1899ec11aa963fe53152f8c4f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*Ggx_KO6_1mlgrkJe_gcrVQ.png"/></div></figure><p id="6d70" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">总执行时间反映了每个时期的时间以及每个步骤的时间。显示了相关图并总结了数据。总执行时间主要受每步时间的影响，在较小程度上受可训练参数数量的负面影响。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/8537e772f5df1462a50d3037e7f0fb08.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*eQFFPWvcQrEc_ewhuh5XjA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/67df3e6b7f607f8defe01c1b44f3272e.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*1CM-SKjProSnRPyrwS7z1Q.png"/></div></figure><p id="60f3" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">再一次，我们可以在图“<em class="nj">总时间(秒)</em>中看到，VGG16是训练最快的模型，GPU可以带来巨大的差异，因为所有模型都是使用GPU训练的，除了例外。所有模型最多花费5000秒或更少(大约1小时20分钟)，然而，在没有GPU的情况下，训练时间高达35000，这几乎是10小时。</p><p id="4525" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">也就是说，如果你没有GPU访问，也买不起为深度学习准备的带有GPU的笔记本电脑，谷歌Colab是一个免费的绝佳替代选择。您不仅可以避免笔记本电脑中需要的所有配置，还可以通过使用GPU来改善您的工作流程。然而，由于免费版本的使用限制，我不认为Google Colab可以成为商业的重要替代产品。</p><h2 id="8825" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt iz bi translated"><strong class="ak"> 6。结果:模型性能</strong></h2><p id="c3ae" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc li md me mf lm mg mh mi lq mj mk ml mm im bi translated">在尝试Google Colab和不同的Keras模型API时，也可以得出一些关于所研究任务的模型性能的结论。总的来说，没有一个模型表现得足够好，可以在生产环境中测试，但是我会分享我的结果。所有指标都是针对验证子集的。这些结果中不包括例外模型。</p><pre class="ks kt ku kv gt nl nm nn no aw np bi"><span id="615b" class="kz la it nm b gy nq nr l ns nt">Validation:<br/>1. Overall accuracy;<br/>2. Three category precision;<br/>3. Three category recall;<br/>4. Three category f1-score.</span></pre><p id="5d60" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">五个模型的总体精度非常相似，但是VGG16的精度稍好，为0.70。DenseNet121性能最差，精度为0.64。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/406f6091d469411d2f332e5d64aa9c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*hmekuX5sLlkRRMfqPR7JQQ.png"/></div></figure><p id="fd28" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">对于每个模型，在不同的历元数下实现了更高的精度，具有测试和验证损失的图形以及训练期间的测试和验证精度如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ac591d0fa69f675ee73ba30e711663e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*iVooi8dbu3rlDTqfDILh-A.png"/></div><figcaption class="od oe gj gh gi of og bd b be z dk translated">InceptionV3</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi oh"><img src="../Images/29033b69b7aa66bdf42e111378a373ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vp8PsyS65mHuzOuQBd3nGQ.png"/></div></div><figcaption class="od oe gj gh gi of og bd b be z dk translated">DenseNet121</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi oh"><img src="../Images/289fc4395d3919caf8e2f6bc95960acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hh4DM-PiAG_K_x40uQPQ4w.png"/></div></div><figcaption class="od oe gj gh gi of og bd b be z dk translated">InceptionResNetV2</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c576ba71b2f22fe7488115ebebd6da3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*pmXtIRy2BHhF2X5SvKbVHA.png"/></div><figcaption class="od oe gj gh gi of og bd b be z dk translated">ResNet50</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi oh"><img src="../Images/ee51dd9adbfa17ba96140cd0a8a02a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rcbpsQjK9di9KfEUFCSUFg.png"/></div></div><figcaption class="od oe gj gh gi of og bd b be z dk translated">VGG16</figcaption></figure><p id="0d3a" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">所有模型的混淆矩阵也以同样的顺序显示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi om"><img src="../Images/0dce6dc4173b82664c46bd1e0382f151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Itzwngxl5vgsaq6YGP_xlA.png"/></div></div></figure><p id="98df" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">精度因图像类别而异。对于几乎所有模型，更容易分类的图像是第2组中的图像(质量差)。组1(平均质量)中的图像是最难预测的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/71f2ffeed461b2be2455a7514ba46ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*LLuqL8uxU9EMs8uMIJXBmg.png"/></div></figure><p id="6651" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">显示召回数据。同样，VGG16可能具有更好的性能。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/9060f6650db0c94676eb3dd0d40530a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*VFm9PqA1qa5Th6FX9YcSYA.png"/></div></figure><p id="1ad7" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">最后，我们可以看到F1的分数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/3163466e6a3ee601179225bfc6b3f9c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*ExSqIxbjuwKcwQ_almSACg.png"/></div></figure><p id="71bf" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">这些结果用于初步分析，将有助于为将来的训练微调hyper参数。</p><h2 id="9cef" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt iz bi translated"><strong class="ak">结论</strong></h2><p id="eef3" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc li md me mf lm mg mh mi lq mj mk ml mm im bi translated">在Google Colab免费版的性能分析中，可以看出Google提供的GPU能力可以用于小型研究项目或学习目的。这有可能加快我们的工作，将培训时间缩短近10倍。然而，对于在更大的项目中使用这个工具来说，使用限制是一种倒退。</p><p id="7f2a" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">在经过训练的模型中，VGG16是训练速度最快的模型，而且重要的是，它在整体准确度和精确度方面也是结果最好的模型。</p><p id="6095" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated">感谢您的阅读。</p><p id="81c9" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated"><strong class="lw jd">如果:</strong>你喜欢这篇文章，别忘了关注我，这样你就能收到关于新出版物的所有更新。</p><p id="a041" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated"><strong class="lw jd">其他如果:</strong>你想了解更多，你可以通过<a class="ae nh" href="https://cdanielaam.medium.com/membership" rel="noopener">我的推荐链接</a>订阅媒体会员。它不会花你更多的钱，但会支付我一杯咖啡。</p><p id="f712" class="pw-post-body-paragraph lu lv it lw b lx mn kd lz ma mo kg mc li mp me mf lm mq mh mi lq mr mk ml mm im bi translated"><strong class="lw jd">其他:</strong>谢谢！</p></div></div>    
</body>
</html>