<html>
<head>
<title>The Best 2021 AI Breakthroughs - A Review</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年最佳人工智能突破-综述</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-best-2021-ai-breakthroughs-d92def157d99?source=collection_archive---------0-----------------------#2021-12-31">https://pub.towardsai.net/the-best-2021-ai-breakthroughs-d92def157d99?source=collection_archive---------0-----------------------#2021-12-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7ee5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/technology" rel="noopener ugc nofollow" target="_blank">技术</a></h2><div class=""/><div class=""><h2 id="6fae" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">按发布日期排列的人工智能最新突破的精选列表，带有清晰的视频解释、更深入文章的链接和代码。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/16f57054f477109db85edb407e7ba659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SLSV4KqTdn0_-69i"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯利·西克玛</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="7fd8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然世界仍在复苏，但研究并没有放缓其疯狂的步伐，尤其是在人工智能领域。此外，今年还强调了许多重要方面，如道德方面、重要偏见、治理、透明度等等。人工智能和我们对人脑及其与人工智能的联系的理解正在不断发展，显示出在不久的将来改善我们生活质量的有前途的应用。然而，我们应该小心选择应用哪种技术。</p><blockquote class="me mf mg"><p id="5854" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">科学不能告诉我们应该做什么，只能告诉我们能做什么—让-保罗·萨特，《存在与虚无》</p></blockquote><p id="5548" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里是今年最有趣的研究论文，以防你错过其中的任何一篇。简而言之，它是人工智能和数据科学领域最新突破的精选列表，按发布日期排列，带有清晰的视频解释、更深入文章的链接和代码(如果适用)。享受阅读吧！</p><p id="e45a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">每篇论文的完整参考资料都列在该存储库的末尾。</strong></p><p id="b285" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">订阅我的<a class="ae lh" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank">时事通讯</a>—AI的最新更新每周都有讲解。</p><p id="371a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mh">随时给我发</em> <a class="ae lh" href="https://www.louisbouchard.ai/contact/" rel="noopener ugc nofollow" target="_blank"> <em class="mh">消息</em> </a> <em class="mh">我可能错过的任何有趣的论文都可以添加到这个库中。</em></p><p id="9ff0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mh">在</em> <strong class="lk jd"> <em class="mh">上给我加标签Twitter</em></strong><em class="mh"/><a class="ae lh" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"><em class="mh">@ Whats _ AI</em></a><em class="mh">或</em><strong class="lk jd"><em class="mh">LinkedIn</em></strong><em class="mh"/><a class="ae lh" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"><em class="mh">@ Louis(什么是AI) Bouchard </em> </a> <em class="mh">如果分享一下名单！</em></p><p id="96db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">去年错过了？看看这个:<a class="ae lh" href="https://github.com/louisfb01/Best_AI_paper_2020" rel="noopener ugc nofollow" target="_blank"> 2020年:充满令人惊叹的人工智能论文的一年——一篇评论</a></p><p id="d2cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">👀<strong class="lk jd">如果你想支持我的工作</strong>并使用W &amp; B(免费)来跟踪你的ML实验并使你的工作可复制或与团队合作，你可以通过遵循<a class="ae lh" href="https://colab.research.google.com/github/louisfb01/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb" rel="noopener ugc nofollow" target="_blank">这个指南</a>来尝试一下！由于这里的大部分代码都是基于PyTorch的，我们认为分享一个在PyTorch上使用W &amp; B的<a class="ae lh" href="https://colab.research.google.com/github/louisfb01/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb" rel="noopener ugc nofollow" target="_blank">快速入门指南</a>会很有趣。</p><p id="044d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">👉遵循<a class="ae lh" href="https://colab.research.google.com/github/louisfb01/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb" rel="noopener ugc nofollow" target="_blank">这个快速指南</a>，在你的代码或下面的任何回复中使用相同的W &amp; B行，让你的所有实验在你的w &amp; b账户中自动跟踪！它不需要超过5分钟的时间来设置，并将改变你的生活，就像我改变你的生活一样！<a class="ae lh" href="https://colab.research.google.com/github/louisfb01/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb" rel="noopener ugc nofollow" target="_blank">这里有一个更高级的使用超参数扫描的指南</a>，如果你有兴趣的话:)</p><p id="15d9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🙌感谢<a class="ae lh" href="https://wandb.ai/" rel="noopener ugc nofollow" target="_blank">Weights&amp;bias</a>赞助这个库和我一直在做的工作，感谢你们中的任何人使用这个链接并尝试W &amp; B！</p><p id="90d9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://github.com/louisfb01/best_AI_papers_2021" rel="noopener ugc nofollow" target="_blank">访问GitHub存储库中的完整列表</a></p><h2 id="9149" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">在15分钟内观看完整的2021年倒带</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h2 id="d973" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">目录</h2><ul class=""><li id="116f" class="nf ng it lk b ll nh lo ni lr nj lv nk lz nl md nm nn no np bi translated">DALL E:open ai的零镜头文本到图像生成[1]</li><li id="822d" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">时尚:StyleGAN插值优化的尝试[2]</li><li id="666c" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">驯服高分辨率图像合成的变压器[3]</li><li id="88ce" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">人工智能中的快速思考和慢速思考[4]</li><li id="94d3" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">航空图像中漂浮海洋大垃圾的自动检测和量化[5]</li><li id="165b" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">ShaRF:来自单个视图的形状调节辐射场[6]</li><li id="98d9" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">生成对抗性变形金刚〔7〕</li><li id="fc1d" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">我们要求人工智能创建约会档案。你能向右滑动吗？[8]</li><li id="8a69" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">Swin转换器:使用移位窗口的分层视觉转换器[9]</li><li id="a1ee" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">图像GANS满足反向图形和可解释的3D神经渲染的可区分渲染[10]</li><li id="6d15" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">深网:他们为视觉做过什么？[11]</li><li id="773a" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">无限自然:从单一图像生成自然场景的永久视图[12]</li><li id="f8cf" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">具有基于深度学习的手指控制的便携式独立神经假体手[13]</li><li id="19d8" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">完全重新照明:学习为背景替换重新照明肖像[14]</li><li id="cb6f" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">LASR:从单目视频学习关节形状重建[15]</li><li id="471d" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">增强照片真实感增强[16]</li><li id="772b" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">DefakeHop:一种轻型高性能Deepfake检测器[17]</li><li id="e243" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">实时高分辨率照片真实感图像翻译:拉普拉斯金字塔翻译网络[18]</li><li id="cbbc" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">理发店:基于GAN的图像合成使用分段掩模[19]</li><li id="5b11" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">TextStyleBrush:从一个例子看文本美学的转移[20]</li><li id="d73c" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">用欧拉运动场制作动画[21]</li><li id="f06b" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">CVPR 2021最佳论文奖:长颈鹿——可控图像生成[22]</li><li id="b2b0" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">GitHub Copilot &amp; Codex:评估代码训练的大型语言模型[23]</li><li id="d43c" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">苹果:通过私人设备上的机器学习识别照片中的人[24]</li><li id="0847" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">用随机微分方程进行图像合成和编辑[25]</li><li id="8b87" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">勾画自己的GAN [26]</li><li id="ce7b" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">特斯拉的自动驾驶仪解释[27]</li><li id="b97a" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">Styleclip:文本驱动的StyleGAN图像操作[28]</li><li id="6d61" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">TimeLens:基于事件的视频帧插值[29]</li><li id="048f" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">单一视频的多样化生成成为可能[30]</li><li id="a2fa" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">使用雷达的深度生成模式的巧妙降水临近预报[31]</li><li id="5e57" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">鸡尾酒叉问题:现实世界音轨的三干音频分离[32]</li><li id="415f" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">ADOP:近似可微分单像素点渲染[33]</li><li id="1f58" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">(Style)CLIPDraw:文本到绘图合成中内容和样式的耦合[34]</li><li id="dd9e" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">SwinIR:使用swin transformer进行图像恢复[35]</li><li id="039d" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">EditGAN:高精度语义图像编辑[36]</li><li id="59a1" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">城市NeRF:在城市尺度上建造NeRF[37]</li><li id="4c87" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">ClipCap:图像字幕的剪辑前缀[38]</li><li id="10e3" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">论文参考</li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="2a7b" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">DALL E:open ai的零镜头文本到图像生成[1]</h2><p id="0625" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">OpenAI成功训练了一个能够从文本字幕生成图像的网络。它与GPT 3号和GPT图像非常相似，产生了惊人的效果。</p><p id="1894" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">短视频讲解:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="1c94" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">简短阅读:</p><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/openais-dall-e-text-to-image-generation-explained-1f6fb4bb5a0a"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">OpenAI的DALL E:解释文本到图像的生成</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">OpenAI刚刚发布了解释DALL-E如何工作的论文！它被称为“零镜头文本到图像的生成”。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="os l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="265b" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2102.12092.pdf" rel="noopener ugc nofollow" target="_blank">零投文本转图像生成</a></li><li id="9daf" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">代码:<a class="ae lh" href="https://github.com/openai/DALL-E" rel="noopener ugc nofollow" target="_blank">代码&amp;用于DALL E </a>的离散VAE的更多信息</li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="ca29" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">时尚:StyleGAN插值优化的尝试[2]</h2><p id="2a34" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">谷歌使用修改后的StyleGAN2架构创建了一个在线试衣间，在这里你可以只使用自己的图像自动试穿任何你想要的裤子或衬衫。</p><ul class=""><li id="4603" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="7971" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">简短阅读:</p><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/the-ai-powered-online-fitting-room-vogue-5f77c599832"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">人工智能驱动的在线试衣间:VOGUE</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">谷歌使用了一种经过修改的StyleGAN2架构来创建一个在线试衣间，在这里你可以自动试穿任何…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pa l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="292a" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://vogue-try-on.github.io/static_files/resources/VOGUE-virtual-try-on.pdf" rel="noopener ugc nofollow" target="_blank">时尚:试戴方式</a>插值优化</li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="7abb" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">驯服高分辨率图像合成的变压器[3]</h2><p id="1b60" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">TL；DR:他们将GANs和卷积方法的效率与transformers的表达能力结合起来，为语义指导的高质量图像合成提供了一种强大而省时的方法。</p><ul class=""><li id="21c4" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="cb2d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">简短阅读:</p><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/combining-the-transformers-expressivity-with-the-cnns-efficiency-for-high-resolution-image-synthesis-31c6767547da"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">结合变形金刚的表现力和CNN的高分辨率图像的效率…</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">TL；DR:他们将GANs和卷积方法的效率与变压器的表达能力结合起来…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pb l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="df7f" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://compvis.github.io/taming-transformers/" rel="noopener ugc nofollow" target="_blank">驯服高分辨率图像合成的变形金刚</a></li><li id="39b2" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">代号:<a class="ae lh" href="https://github.com/CompVis/taming-transformers" rel="noopener ugc nofollow" target="_blank">驯服变形金刚</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="3e76" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">人工智能中的快速思考和慢速思考[4]</h2><p id="9474" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">从人类能力中汲取灵感，走向更通用、更可信的人工智能&amp;给人工智能研究界的10个问题。</p><ul class=""><li id="1bac" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="5148" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">简短阅读:</p><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/thinking-fast-and-slow-and-the-third-wave-of-ai-79156b5545e8"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">思维的快慢与第三次人工智能浪潮</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">从人类能力中汲取灵感走向更通用和更可信的人工智能&amp;人工智能的10个问题…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pc l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="1e99" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2010.06002" rel="noopener ugc nofollow" target="_blank">思考中的快与慢艾</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="f8d1" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">航空图像中漂浮海洋大垃圾的自动检测和量化[5]</h2><p id="0d4b" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">来自巴塞罗纳大学的奥代·加西亚-加林等人开发了一种基于深度学习的算法，能够从航空图像中检测和量化漂浮的垃圾。他们还开发了一个面向网络的应用程序，允许用户在海面图像中识别这些垃圾，称为浮动海洋宏观垃圾，或FMML。</p><ul class=""><li id="5fb6" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="300d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">简短阅读:</p><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/an-ai-software-able-to-detect-and-count-plastic-waste-in-the-ocean-7211aa0baf89"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">一个人工智能软件能够检测和计算海洋中的塑料垃圾</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">它既聪明又简单，你可以在许多图像分类应用中使用相同的模型。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pd l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="09a1" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://doi.org/10.1016/j.envpol.2021.116490" rel="noopener ugc nofollow" target="_blank">航拍图像中漂浮海洋大型垃圾的自动检测和量化:介绍一种新的深度学习方法，连接到R，环境污染中的web应用</a></li><li id="5829" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/amonleong/MARLIT" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="4526" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">ShaRF:来自单个视图的形状调节辐射场[6]</h2><p id="a9d0" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">想象一下，拍一张物体的照片，然后以3D形式插入到您正在制作的电影或视频游戏中，或者插入到3D场景中作为插图，这将是多么酷的事情。</p><ul class=""><li id="cf97" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="29cb" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/sharf-take-a-picture-from-a-real-life-object-and-create-a-3d-model-of-it-c6809806b32"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">ShaRF:从现实生活中的物体上拍一张照片，然后创建一个它的3D模型</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">想象一下，拍一张物体的照片，然后用3D方式插入到电影或视频中，那该有多酷…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pe l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="ee72" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2102.08860" rel="noopener ugc nofollow" target="_blank"> ShaRF:来自单一视图的形状调节辐射场</a></li><li id="46dc" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="http://www.krematas.com/sharf/index.html" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="5c0f" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">生成对抗性变形金刚〔7〕</h2><p id="9e85" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">他们基本上利用了强大的StyleGAN2架构中变形金刚的注意力机制，使其更加强大！</p><ul class=""><li id="d1e0" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="27ff" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/generative-adversarial-transformers-gansformers-explained-bf1fa76ef58d"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">生成对抗性变形:GANsformers解释</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">他们基本上利用了强大的StyleGAN2架构中变形金刚的注意力机制，使其更加…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pf l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="4874" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2103.01209.pdf" rel="noopener ugc nofollow" target="_blank">生成对立的变形金刚</a></li><li id="977e" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/dorarad/gansformer" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul><blockquote class="me mf mg"><p id="530e" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated"><em class="it">订阅我的每周</em> <a class="ae lh" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <em class="it">时事通讯</em> </a> <em class="it">及时了解AI 2022年最新出版物！</em></p></blockquote></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="c8b7" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">我们要求人工智能创建约会档案。你能向右滑动吗？[8]</h2><p id="f019" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">你会在人工智能档案上向右滑动吗？你能区分真人和机器吗？这就是这项研究揭示的在约会应用程序上使用人工智能化妆的人。</p><ul class=""><li id="d186" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="14bb" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/would-you-swipe-right-on-an-ai-profile-98dc8a4451ec"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">你会在人工智能档案上向右滑动吗？</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">你能区分真人和机器吗？这就是这项研究揭示的在约会中使用人工智能制造的人…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pg l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="0c46" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://studyonline.unsw.edu.au/blog/ai-generated-dating-profile" rel="noopener ugc nofollow" target="_blank">我们让人工智能创建约会档案。你能向右滑动吗？</a></li><li id="01b2" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#forceEdit=true&amp;sandboxMode=true&amp;scrollTo=aeXshJM-Cuaf" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="d09b" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">Swin转换器:使用移位窗口的分层视觉转换器[9]</h2><p id="2b97" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">变形金刚会取代计算机视觉中的CNN吗？在不到5分钟的时间内，您将通过一篇名为Swin transformer的新论文了解如何将Transformer架构应用于计算机视觉。</p><ul class=""><li id="6670" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="d098" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/will-transformers-replace-cnns-in-computer-vision-55657a196833"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">变形金刚会取代计算机视觉中的CNN吗？</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">几分钟后，您将了解如何通过一种新的方式将transformer架构应用于计算机视觉</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="ph l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="6100" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2103.14030v1" rel="noopener ugc nofollow" target="_blank"> Swin Transformer:使用移位窗口的分层视觉转换器</a></li><li id="e3d4" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/microsoft/Swin-Transformer" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="a28b" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">图像GANS满足反向图形和可解释的3D神经渲染的可区分渲染[10]</h2><p id="deb7" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">这个被称为GANverse3D的有前途的模型只需要一个图像就可以创建一个可以定制和动画的3D人物！</p><ul class=""><li id="f2bc" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="c21f" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/create-3d-models-from-images-ai-and-game-development-design-1835785b8563"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">从图像创建三维模型！人工智能和游戏开发，设计…</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">这个被称为GANverse3D的有前途的模型只需要一个图像就可以创建一个可以定制和动画的3D人物！</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pi l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="ff4f" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2010.09125.pdf" rel="noopener ugc nofollow" target="_blank">图像GANS满足逆向图形的可微分渲染和可解释的3D神经渲染</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="6317" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">深网:他们为视觉做过什么？[11]</h2><p id="3cec" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">“我将公开分享关于视觉应用深度网络的一切，它们的成功，以及我们必须解决的局限性。”</p><ul class=""><li id="dc27" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="528d" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/what-has-ai-done-for-computer-vision-3748f5958e07"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">AI为计算机视觉做了什么？</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">关于当前深度网络的一切，他们为视觉应用做了什么。他们的成功和局限。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pj l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="3ac7" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/1805.04025" rel="noopener ugc nofollow" target="_blank">深网:他们为视觉做过什么？</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="4d93" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">无限自然:从单一图像生成自然场景的永久视图[12]</h2><p id="33b7" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">视图合成的下一步:永久视图生成，目标是拍摄一幅图像，然后飞进去探索风景！</p><ul class=""><li id="560e" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="cfe6" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/infinite-nature-fly-into-an-image-and-explore-it-like-controlling-a-drone-541cab44b8f5"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">无限自然:飞入一个影像，像控制无人机一样探索它！</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">视图合成的下一步:永久视图生成，其目标是获取一幅图像，然后…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pk l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="f190" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2012.09855.pdf" rel="noopener ugc nofollow" target="_blank">无限自然:从单一图像生成自然场景的永久视图</a></li><li id="c61c" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/google-research/google-research/tree/master/infinite_nature" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li><li id="a8b6" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://colab.research.google.com/github/google-research/google-research/blob/master/infinite_nature/infinite_nature_demo.ipynb#scrollTo=sCuRX1liUEVM" rel="noopener ugc nofollow" target="_blank"> Colab演示</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="15dc" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">具有基于深度学习的手指控制的便携式独立神经假体手[13]</h2><p id="17eb" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">有了这个人工智能驱动的神经接口，截肢者可以像生活一样灵活和直观地控制神经假体手。</p><ul class=""><li id="ce74" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="8806" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/from-amputee-to-cyborg-with-this-ai-powered-hand-bdeb40f9b0d8"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">用这只人工智能手从截肢者变成机器人！🦾</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">通过这种人工智能驱动的神经接口，截肢者可以像生活一样灵活地控制神经假体手</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pl l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="65e4" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2103.13452" rel="noopener ugc nofollow" target="_blank">便携式独立神经修复手，具有基于深度学习的手指控制</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="5b7e" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">完全重新照明:学习为背景替换重新照明肖像[14]</h2><p id="c9c7" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">根据您添加的新背景的照明，正确地重新照亮任何肖像。你是否曾经想改变一张图片的背景，但却让它看起来很真实？如果你已经尝试过，你就会知道这并不简单。你不能在家里给自己拍张照片，然后给海滩换个背景。只是看起来很糟糕，不现实。任何人都会马上说“那是PS过的”。对于电影和专业视频，你需要完美的灯光和艺术家来再现高质量的图像，而这是超级昂贵的。你不可能用自己的照片做到这一点。还是可以？</p><ul class=""><li id="5496" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="0549" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/change-your-portraits-backgrounds-with-realistic-lighting-b6f2ebeb1a85"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">用真实的灯光改变你的肖像背景</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">根据您添加的新背景的照明，正确地重新照亮任何肖像。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pm l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="7107" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">纸张:<a class="ae lh" href="https://augmentedperception.github.io/total_relighting/total_relighting_paper.pdf" rel="noopener ugc nofollow" target="_blank">整体重新照明:学习为背景替换重新照亮肖像</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="25ee" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">LASR:从单目视频学习关节形状重建[15]</h2><p id="2d93" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">仅通过短视频作为输入，生成人类或动物运动的3D模型。这是一种仅从短视频作为输入来生成人类或动物运动的3D模型的新方法。事实上，它知道这是一个奇怪的形状，它可以移动，但仍然需要保持连接，因为这仍然是一个“对象”，而不只是许多对象在一起…</p><ul class=""><li id="88f0" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="e051" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/3d-reconstruction-from-videos-lasr-c29459399aed"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">视频三维重建:LASR</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">仅通过短视频作为输入，生成人类或动物运动的3D模型。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pn l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="b5f8" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_LASR_Learning_Articulated_Shape_Reconstruction_From_a_Monocular_Video_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank"> LASR:从单目视频学习关节形状重建</a></li><li id="449c" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/google/lasr" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="b943" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">增强照片真实感增强[16]</h2><p id="1446" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">这种人工智能可以应用到视频游戏中，并使每一帧看起来更加自然。来自英特尔实验室的研究人员刚刚发表了一篇名为“增强真实感增强”的论文。如果你认为这可能是“另一个甘”，把视频游戏的照片作为输入，并按照自然世界的风格改变它，让我改变你的想法。他们花了两年时间研究这个模型，让它变得非常健壮。它可以现场应用到视频游戏中，并使每一帧看起来更加自然。想象一下这样的可能性，你可以在游戏图形中投入更少的努力，使它超级稳定和完整，然后使用这种模式改进风格…</p><ul class=""><li id="67d0" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="a51b" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/the-future-of-video-game-design-234cbeba7ab"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">电子游戏设计的未来？</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">这种人工智能可以应用到视频游戏中，并使每一帧看起来更加自然。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="po l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="5179" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="http://vladlen.info/papers/EPE.pdf" rel="noopener ugc nofollow" target="_blank">增强真实感增强</a></li><li id="bf72" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/isl-org/PhotorealismEnhancement" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="f6ec" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">DefakeHop:一种轻型高性能Deepfake检测器[17]</h2><p id="1b99" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">2021年如何识破深度假？突破性的美国陆军技术使用人工智能寻找deepfakes。</p><p id="885a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然它们似乎一直都在那里，但第一个现实的deepfake直到2017年才出现。它从有史以来第一个自动生成的相似的假图像发展到今天的带有声音的视频中的一模一样的某人的复制品。</p><p id="1563" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">事实是，我们再也看不出真正的视频或图片与假的之间的区别了。我们如何区分什么是真实的，什么不是？如果一个人工智能可以完全生成音频文件或视频文件，它们如何在法庭上用作证据？那么，这篇新论文可能会提供这些问题的答案。这里的答案可能再次是人工智能的使用。“当我看到它时我会相信它”这句话可能很快就会变成“当人工智能告诉我相信它时我会相信它…”</p><ul class=""><li id="c80d" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="9efb" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/how-to-spot-a-deep-fake-in-2021-3067ebb218fe"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">2021年如何识破深度假</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">突破性的美国陆军技术使用人工智能寻找deepfakes。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pp l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="a6df" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2103.06929" rel="noopener ugc nofollow" target="_blank"> DefakeHop:一款轻量级高性能深度打假检测器</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="3c19" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">实时高分辨率照片真实感图像翻译:拉普拉斯金字塔翻译网络[18]</h2><p id="fb85" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">使用这种基于机器学习的新方法，实时将任何风格应用到您的4K图像中！</p><ul class=""><li id="92de" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="0a8e" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/computer-vision-fe47a52e76a5"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">实时高分辨率真实感图像翻译</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">使用这种基于机器学习的新方法，实时将任何风格应用到您的4K图像中！</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pq l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="9c95" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2105.09188.pdf" rel="noopener ugc nofollow" target="_blank">实时高分辨率真实感图像翻译:拉普拉斯金字塔翻译网络</a></li><li id="da74" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/csjliang/LPTN" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="c6e3" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">理发店:基于GAN的图像合成使用分段掩模[19]</h2><p id="ada3" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">这篇文章本身并不是关于一项新技术。相反，它是关于GANs的一个令人兴奋的新应用。的确，你看到了标题，它不是clickbait。这个人工智能可以转移你的头发，看看它会是什么样子，然后再做出改变…</p><ul class=""><li id="09c5" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="998f" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/barbershop-try-different-hairstyles-and-hair-colors-from-pictures-gans-e5138a8ee5f4"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">理发店:尝试图片中不同的发型和发色</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">这个人工智能可以在改变之前转移你的头发，看看它会是什么样子。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pr l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="2f57" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2106.01505.pdf" rel="noopener ugc nofollow" target="_blank">理发店:使用分割遮罩的基于GAN的图像合成</a></li><li id="0616" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/ZPdesu/Barbershop" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="6028" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">TextStyleBrush:从一个例子看文本美学的转移[20]</h2><p id="1f4d" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">这个新的脸书人工智能模型可以用你自己的语言直接翻译或编辑图像中的文本，遵循同样的风格！</p><p id="595a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">想象你在另一个你不会说该语言的国家度假。你想试试当地的餐馆，但是他们的菜单用的是你不会说的语言。我认为这不会太难想象，因为我们大多数人已经面临这种情况，无论你看到菜单项或方向，你不能理解写的是什么。嗯，在2020年，你会拿出你的手机，谷歌翻译你看到的东西。2021年你甚至不需要再打开谷歌翻译，试着把你看到的一个一个写下来翻译。相反，你可以简单地使用脸书人工智能的这个新模型，用你自己的语言翻译图像中的每一个文本…</p><ul class=""><li id="1a8f" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="e789" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/translate-text-from-images-emulating-the-style-textstylebrush-1b73af3d0ac9"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">从模拟样式的图像翻译文本:TextStyleBrush</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">这个新的脸书人工智能模型可以用你自己的语言翻译或编辑图像中的每一个文本，遵循相同的…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="ps l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="187b" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2106.08385" rel="noopener ugc nofollow" target="_blank"> TextStyleBrush:从单个例子看文本美学的转移</a></li><li id="b81c" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/facebookresearch/IMGUR5K-Handwriting-Dataset?fbclid=IwAR0pRAxhf8Vg-5H3fA0BEaRrMeD21HfoCJ-so8V0qmWK7Ub21dvy_jqgiVo" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul><blockquote class="me mf mg"><p id="7303" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated"><em class="it">如果你也想阅读更多的研究论文，我推荐你阅读</em> <a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/how-to-read-more-research-papers-7737e3770d7f"> <em class="it">我的文章</em> </a> <em class="it">，在那里我分享了寻找和阅读更多研究论文的最佳技巧。</em></p></blockquote></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="c103" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">用欧拉运动场制作动画[21]</h2><p id="b103" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">该模型拍摄一张照片，了解哪些粒子应该在移动，并在无限循环中逼真地动画化它们，同时完全保留照片的其余部分，仍然创建像这样看起来令人惊叹的视频…</p><ul class=""><li id="4d02" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="7631" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/create-realistic-animated-looping-videos-from-pictures-58debf6f139"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">从图片创建逼真的动画循环视频</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">这个模型拍一张照片，了解哪些粒子应该在移动，并逼真地将它们动画化…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pt l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="a8f3" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2011.15128" rel="noopener ugc nofollow" target="_blank">用欧拉运动场制作图片动画</a></li><li id="3fed" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://eulerian.cs.washington.edu/" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="773b" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">CVPR 2021最佳论文奖:长颈鹿——可控图像生成[22]</h2><p id="7821" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">使用改进的GAN架构，他们可以移动图像中的对象，而不会影响背景或其他对象！</p><ul class=""><li id="7209" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="8491" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/cvpr-2021-best-paper-award-giraffe-controllable-image-generation-24eac0001ca4"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">CVPR 2021年最佳论文奖:长颈鹿——可控图像生成</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">使用改良的氮化镓架构，他们甚至可以移动图像中的物体，而不影响背景或…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pu l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="58f9" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf" rel="noopener ugc nofollow" target="_blank">长颈鹿:将场景表示为合成生成神经特征场</a></li><li id="877b" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="9956" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">GitHub Copilot &amp; Codex:评估代码训练的大型语言模型[23]</h2><p id="1b37" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">了解OpenAI的这个新模型如何从单词生成代码！</p><ul class=""><li id="dc3f" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="e5c2" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/openais-new-code-generator-github-copilot-and-codex-6031d65e47c1"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">OpenAI的新代码生成器:GitHub Copilot(和Codex)</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">了解这个人工智能如何从单词中生成代码</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pv l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="af72" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2107.03374.pdf" rel="noopener ugc nofollow" target="_blank">评估基于代码</a>训练的大型语言模型</li><li id="f193" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://copilot.github.com/" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="d08e" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">苹果:通过私人设备上的机器学习识别照片中的人[24]</h2><p id="637e" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">使用多种基于机器学习的算法在你的设备上私下运行，苹果允许你在iOS 15上准确地管理和组织你的图像和视频。</p><ul class=""><li id="f442" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="7751" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/how-apple-photos-recognizes-people-in-private-photos-using-machine-learning-8c7f4d60f914"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">Apple Photos如何使用机器学习识别私人照片中的人</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">使用多种基于机器学习的算法在您的设备上秘密运行，Apple允许您准确地…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pw l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="f710" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://machinelearning.apple.com/research/recognizing-people-photos" rel="noopener ugc nofollow" target="_blank">通过私有设备上的机器学习识别照片中的人</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="e73b" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">用随机微分方程进行图像合成和编辑[25]</h2><p id="05ae" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">告别复杂的GAN和变压器架构，实现图像生成！斯坦福大学和卡耐基梅隆大学的孟等人提出的这种新方法可以从任何基于用户的输入中生成新图像。即使像我这样没有艺术技巧的人，现在也可以从快速草图中生成美丽的图像或修改…</p><ul class=""><li id="8e52" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="2ada" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/image-synthesis-and-editing-from-hand-drawn-color-strokes-sdedit-8da8592182cb"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">手绘彩色笔画的图像合成和编辑。</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">告别复杂的GAN和变压器架构，实现图像生成。这种新方法可以生成新的图像…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="px l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="06da" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2108.01073.pdf" rel="noopener ugc nofollow" target="_blank">用随机微分方程进行图像合成与编辑</a></li><li id="64f8" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/ermongroup/SDEdit" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li><li id="5151" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://colab.research.google.com/drive/1KkLS53PndXKQpPlS1iK-k1nRQYmlb4aO?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab演示</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="1096" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">勾画自己的GAN [26]</h2><p id="49f4" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">通过根据草图生成图像，使GANs训练对每个人来说都更容易！事实上，有了这种新方法，你可以根据你能提供的最简单的知识类型来控制你的GAN的输出:手绘草图。</p><ul class=""><li id="7989" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="a529" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/make-gans-training-easier-for-everyone-generate-images-following-a-sketch-a2bc4fd6ec8c"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">让GANs训练对每个人都更容易:根据草图生成图像</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">根据你能提供的最简单的知识类型控制GANs输出:手绘草图。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="py l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="2ad5" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2108.02774" rel="noopener ugc nofollow" target="_blank">勾画自己的甘</a></li><li id="f60d" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/PeterWang512/GANSketching" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="58e7" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">特斯拉的自动驾驶仪解释[27]</h2><p id="17d7" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">如果你想知道特斯拉汽车如何不仅能看到其他车辆，还能在道路上导航，这就是你一直在等待的视频。几天前是第一个特斯拉人工智能日，特斯拉人工智能总监Andrej Karpathy和其他人展示了特斯拉的自动驾驶仪如何通过他们的八个摄像头进行图像采集，以及在道路上的导航过程。</p><ul class=""><li id="55e6" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="977f" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/tesla-ai-day-in-10-minute-show-does-teslas-autopilot-work-3990252082dc"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">特斯拉人工智能日亮点&amp;自动驾驶仪如何工作</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">安德烈·卡帕西(Andrej Karpathy)关于特斯拉自动驾驶仪的演讲在不到10分钟的时间里就解释清楚了</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="pz l ot ou ov or ow lb oi"/></div></div></a></div></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="2a04" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">风格剪辑:文本驱动的风格处理[28]</h2><p id="bbb3" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">人工智能可以生成图像，然后，使用大量的脑力和反复试验，研究人员可以按照特定的风格控制结果。现在，有了这个新模型，你可以只用文本就能做到！</p><ul class=""><li id="e415" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="1328" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/manipulate-real-images-with-text-25b9f583e292"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">用文本处理真实图像</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">创意艺术家的人工智能！StyleCLIP解释</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qa l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="30f2" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">Paper: <a class="ae lh" href="https://arxiv.org/abs/2103.17249" rel="noopener ugc nofollow" target="_blank"> Styleclip:文本驱动的StyleGAN图像操作。</a></li><li id="b66e" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/orpatashnik/StyleCLIP" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li><li id="f490" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab演示</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="e3ea" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">TimeLens:基于事件的视频帧插值[29]</h2><p id="b736" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">TimeLens可以理解视频帧之间的粒子运动，以我们肉眼无法看到的速度重建真实发生的事情。事实上，它实现了我们的智能手机和其他型号之前无法达到的效果！</p><ul class=""><li id="98f5" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="fdeb" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/change-video-into-slow-motion-with-ai-timelens-explained-4281d97c9b9d"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">用AI把视频换成慢动作！TimeLens解释道</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">时间镜头可以理解视频帧之间的粒子运动，以重建真正的…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qb l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="54a7" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="http://rpg.ifi.uzh.ch/docs/CVPR21_Gehrig.pdf" rel="noopener ugc nofollow" target="_blank"> TimeLens:基于事件的视频帧插值</a></li><li id="9044" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/uzh-rpg/rpg_timelens" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul><blockquote class="me mf mg"><p id="0ac6" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated"><em class="it">订阅我的每周</em> <a class="ae lh" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <em class="it">时事通讯</em> </a> <em class="it">及时了解AI 2022年最新出版物！</em></p></blockquote></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="d4bf" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">单一视频的多样化生成成为可能[30]</h2><p id="8a4d" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">你有没有想过剪辑一个视频？删除或添加某人，更改背景，使其持续更长时间，或者更改分辨率以适应特定的纵横比，而不压缩或拉伸它。对于那些已经开展过广告活动的人来说，你肯定希望有不同的视频来进行AB测试，看看什么效果最好。Niv Haim等人的这项新研究可以帮助你在一个高清视频中完成所有这些事情！</p><p id="7ee0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">的确，使用一个简单的视频，你可以在几秒钟或几分钟内完成我刚才提到的任何高质量视频的任务。你基本上可以把它用于任何你想到的视频操作或视频生成应用。它甚至在所有方面都优于GANs，并且不使用任何深度学习的花哨研究，也不需要庞大而不切实际的数据集！最棒的是，这项技术可以扩展到高分辨率视频。</p><ul class=""><li id="119b" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="4bee" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/diverse-generation-from-a-single-video-made-possible-no-dataset-or-deep-learning-required-f4377c0c56bf"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">从单个视频生成不同的内容成为可能—不需要数据集或深度学习！</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">这种模式可以做任何视频操作或视频生成应用程序，你记住了！</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qc l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="ffc7" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2109.08591" rel="noopener ugc nofollow" target="_blank">单一视频的多样化生成成为可能</a></li><li id="f65b" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://nivha.github.io/vgpnn/" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="b115" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">使用雷达的深度生成模式的巧妙降水临近预报[31]</h2><p id="422d" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">DeepMind刚刚发布了一个生成模型，该模型在89%的情况下能够胜过广泛使用的临近预报方法，其准确性和实用性由50多位专家气象学家评估！他们的模型侧重于预测未来2小时的降水量，并取得了令人惊讶的好成绩。这是一个生成模型，这意味着它将生成预测，而不是简单地预测它们。它基本上利用过去的雷达数据来创建未来的雷达数据。因此，利用过去的时间和空间成分，他们可以生成它在不久的将来的样子。</p><p id="925a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以看到这与Snapchat滤镜一样，获取您的面部，并生成一个带有修改的新面部。为了训练这样一个生成模型，你需要来自人脸和你想要生成的人脸类型的大量数据。然后，使用经过许多小时训练的非常相似的模型，你将拥有一个强大的生成模型。这种模型通常使用GANs架构进行训练，然后独立使用生成器模型。</p><ul class=""><li id="769e" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="4e4b" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/artificial-intelligence-4d7b12727ac4"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">DeepMind使用人工智能来预测更准确的天气预报</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">50多名专家气象学家评估DeepMind的新模型在89%的情况下击败了当前的临近预报方法，因为它…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qd l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="ee48" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://www.nature.com/articles/s41586-021-03854-z" rel="noopener ugc nofollow" target="_blank">巧用雷达深度生成模式进行降水临近预报</a></li><li id="660c" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/deepmind/deepmind-research/tree/master/nowcasting" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="dbf3" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">鸡尾酒叉问题:现实世界音轨的三干音频分离[32]</h2><p id="2c39" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">你有没有收听过一个视频或电视节目，而演员完全听不见，或者音乐太大声？嗯，这个问题，也叫鸡尾酒会问题，可能再也不会发生了。三菱和印第安纳大学刚刚发布了一个新的模型和一个新的数据集来处理识别正确的配乐的任务。例如，如果我们把刚才播放的同一个音频片段的音乐音量调得太大，你可以简单地调高或调低音轨，让语音比音乐更重要。</p><p id="1723" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里的问题是从复杂的声学场景中分离出任何独立的声源，比如电影场景或youtube视频，其中一些声音不平衡。有时你根本听不到一些演员的声音，因为背景中有音乐、爆炸声或其他环境声音。嗯，如果你成功地隔离了一个音轨中的不同类别，这意味着你也可以只调高或调低其中一个类别，就像把音乐调低一点，以便正确地听到所有其他演员的声音。这正是研究人员所取得的成果。</p><ul class=""><li id="570b" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="d2aa" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/separate-voice-music-and-sound-effects-with-ai-9a652f4f9cfc"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">用人工智能分离语音、音乐和音效</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">如果我们在播放音乐的时候声音太大，你只需要调高音量，调低音乐就可以了！</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qe l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="90dc" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2110.09958.pdf" rel="noopener ugc nofollow" target="_blank">鸡尾酒叉问题:现实世界配乐的三茎音频分离</a></li><li id="4e74" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://cocktail-fork.github.io/" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="d28d" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">ADOP:近似可微分单像素点渲染[33]</h2><p id="0ec9" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">想象一下，你想从你拍的一堆照片中生成一个3D模型或者简单的一个流畅的视频。嗯，现在有可能了！我不想给出太多，但结果简直是惊人的，你需要自己检查一下！</p><ul class=""><li id="10f5" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="9e58" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/ai-synthesizes-smooth-videos-from-a-couple-of-images-aeb288493b3d"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">人工智能从一对夫妇的图像合成流畅的视频！</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">让我们从几张照片中构建3D模型…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qf l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="402a" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2110.06635.pdf" rel="noopener ugc nofollow" target="_blank"> ADOP:近似可微单像素点渲染</a></li><li id="2d62" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/darglein/ADOP" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="77c5" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">(Style)CLIPDraw:文本到绘图合成中内容和样式的耦合[34]</h2><p id="2f25" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">你有没有梦想过采用图片的风格，比如左边这个很酷的抖音绘画风格，并将其应用到你选择的新图片中？是的，我做到了，而且从来没有这么容易做到。事实上，你甚至可以只通过文本来实现，现在就可以用这种新方法和他们为每个人提供的Google Colab笔记本来尝试。只需拍下你想要复制的样式的图片，输入你想要生成的文本，这个算法就会从中生成一张新的图片！回头看看上面的结果就知道了，这么大的进步！结果非常令人印象深刻，尤其是如果您考虑到它们是由单行文本构成的！</p><ul class=""><li id="9cdf" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="8ce6" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/text-to-drawing-synthesis-with-artistic-control-clipdraw-styleclipdraw-dd56fa208bea"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">具有艺术控制的文本到绘图合成| CLIPDraw &amp; StyleCLIPDraw</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">给你想要复制的风格拍张照，输入文字，算法会从中生成一张新的图片！</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qg l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="8031" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">Paper (CLIPDraw): <a class="ae lh" href="https://arxiv.org/abs/2106.14843" rel="noopener ugc nofollow" target="_blank"> CLIPDraw:通过语言图像编码器探索文本到绘图的合成</a></li><li id="d71d" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated">paper(StyleCLIPDraw):<a class="ae lh" href="https://arxiv.org/abs/2111.03133" rel="noopener ugc nofollow" target="_blank">StyleCLIPDraw:文本到绘图合成中内容和样式的耦合</a></li><li id="b4bc" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb" rel="noopener ugc nofollow" target="_blank"> CLIPDraw Colab演示</a></li><li id="084a" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://colab.research.google.com/github/pschaldenbrand/StyleCLIPDraw/blob/master/Style_ClipDraw.ipynb" rel="noopener ugc nofollow" target="_blank"> StyleCLIPDraw Colab演示</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="7e7d" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">SwinIR:使用swin transformer进行图像恢复[35]</h2><p id="3eec" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">你是否曾经有过一个你非常喜欢的图片，但只能找到一个小版本，看起来像下面左边的这张图片？如果你能把这张图片放大两倍，那该有多酷？这很棒，但是如果你能把它的清晰度提高四到八倍呢？现在我们在谈话，看看那个。</p><p id="33a5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这里，我们将图像的分辨率提高了四倍，这意味着我们有四倍多的高度和宽度像素来获得更多的细节，使它看起来更加平滑。最棒的是，这是在几秒钟内完成的，完全自动，几乎可以处理任何图像。哦，你甚至可以通过他们提供的演示自己使用它…</p><ul class=""><li id="ebc9" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="84ab" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/this-ai-makes-blurry-faces-look-8-times-sharper-swinir-photo-upsampling-b41d394b41e9"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">这个人工智能使模糊的脸看起来清晰8倍！SwinIR:照片上采样</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">用AI把你的小512像素大图转换成4k！</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qh l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="7cc4" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2108.10257" rel="noopener ugc nofollow" target="_blank"> SwinIR:使用swin transformer进行图像恢复</a></li><li id="5c0c" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/JingyunLiang/SwinIR" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li><li id="9545" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://replicate.ai/jingyunliang/swinir" rel="noopener ugc nofollow" target="_blank">演示</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="2bf9" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">EditGAN:高精度语义图像编辑[36]</h2><p id="3394" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">控制快速草稿中的任何功能，它将只编辑你想要的，保持图像的其余部分不变！NVIDIA，MIT和UofT基于GANs的草图模型的SOTA图像编辑。</p><ul class=""><li id="44a8" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="9c1f" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/image-editing-from-sketches-editgan-4cacca609e2d"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">从草图编辑图像:EditGAN</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">控制快速草稿中的任何功能，它将只编辑你想要的，保持图像的其余部分不变！SOTA…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qi l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="a9c3" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/abs/2111.03186" rel="noopener ugc nofollow" target="_blank"> EditGAN:高精度语义图像编辑</a></li><li id="5ab0" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://nv-tlabs.github.io/editGAN/" rel="noopener ugc nofollow" target="_blank">点击此处获取代码(即将发布)</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="9c0d" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">城市NeRF:在城市尺度上建造NeRF[37]</h2><p id="27ca" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">这个模型被称为CityNeRF，它是从NeRF发展而来的，我之前在我的频道中介绍过。NeRF是首批使用辐射场和机器学习从图像中构建3D模型的模型之一。但是NeRF并不是那么有效，而且只适用于单一规模。在这里，CityNeRF同时应用于卫星和地面图像，为任何视点生成各种3D模型比例。简而言之，他们将NeRF带到了城市规模。但是怎么做呢？</p><ul class=""><li id="0f3d" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="04cf" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/technology-fcb0fbfa9c00"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">CityNeRF:城市比例的3D渲染！</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">以任何比例生成具有高质量细节的城市级3D场景！</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qj l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="1992" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">论文:<a class="ae lh" href="https://arxiv.org/pdf/2112.05504.pdf" rel="noopener ugc nofollow" target="_blank">城市NeRF:城市规模的建筑NeRF</a></li><li id="32e4" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://city-super.github.io/citynerf/" rel="noopener ugc nofollow" target="_blank">点击此处获取代码(即将发布)</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="61e8" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">ClipCap:图像字幕的剪辑前缀[38]</h2><p id="59d3" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">我们已经看到人工智能使用GANs从其他图像生成图像。然后，有模型能够使用文本生成有问题的图像。2021年初，DALL-E发表，击败了之前所有使用CLIP从文本输入中生成图像的尝试，CLIP是一种将图像与文本链接起来的模型。一个非常相似的任务叫做图像字幕，听起来可能很简单，但实际上也很复杂。它是机器生成图像的自然描述的能力。简单地标记你在图像中看到的物体是很容易的，但要理解在一张二维图像中发生了什么却是另一个挑战，这个新模型做得非常好…</p><ul class=""><li id="9344" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">短视频讲解:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><ul class=""><li id="2c0e" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">简短阅读:</li></ul><div class="of og gp gr oh oi"><a rel="noopener  ugc nofollow" target="_blank" href="/image-captioning-with-clip-and-gpt-d0cb3f3fddda"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">带剪辑和GPT的图像字幕</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">使用剪辑和GPT模型轻松生成图像的文本描述！</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">pub.towardsai.net</p></div></div><div class="or l"><div class="qk l ot ou ov or ow lb oi"/></div></div></a></div><ul class=""><li id="7849" class="nf ng it lk b ll lm lo lp lr ox lv oy lz oz md nm nn no np bi translated">Paper: <a class="ae lh" href="https://arxiv.org/abs/2111.09734" rel="noopener ugc nofollow" target="_blank"> ClipCap:图像字幕的剪辑前缀</a></li><li id="b436" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://github.com/rmokady/CLIP_prefix_caption" rel="noopener ugc nofollow" target="_blank">点击此处获取代码</a></li><li id="79eb" class="nf ng it lk b ll nq lo nr lr ns lv nt lz nu md nm nn no np bi translated"><a class="ae lh" href="https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing" rel="noopener ugc nofollow" target="_blank">点击这里观看Colab演示</a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><blockquote class="me mf mg"><p id="fc42" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated"><em class="it">如果你想阅读更多的论文并有更广阔的视野，这里有另一个涵盖2020年的伟大知识库:</em> <a class="ae lh" href="https://github.com/louisfb01/Best_AI_paper_2020" rel="noopener ugc nofollow" target="_blank"> <em class="it"> 2020:充满令人惊叹的人工智能论文的一年——回顾</em> </a> <em class="it">，并随时订阅我的每周</em> <a class="ae lh" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <em class="it">时事通讯</em> </a> <em class="it">，了解2022年人工智能的最新出版物！</em></p></blockquote><p id="ac31" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mh">在</em> <strong class="lk jd"> <em class="mh">上给我加标签推特</em></strong><em class="mh"/><a class="ae lh" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"><em class="mh">@ Whats _ AI</em></a><em class="mh">或</em><strong class="lk jd"><em class="mh">LinkedIn</em></strong><em class="mh"/><a class="ae lh" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"><em class="mh">@ Louis(什么是AI) Bouchard </em> </a> <em class="mh">如果分享名单！</em></p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="e51d" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">论文参考</h2><p id="44fd" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">[1] A. Ramesh等，零拍文本到图像的生成，2021。arXiv:2102.12092</p><p id="c3ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2] Lewis，Kathleen M等人，(2021)，《时尚:通过StyleGAN插值优化试穿》。</p><p id="4422" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3]驯服高分辨率图像合成的变压器，Esser等人，2020年。</p><p id="fa1a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4]《思考的快与慢》载艾、布奇等人，(2020)，<a class="ae lh" href="https://arxiv.org/abs/2010.06002" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="e723" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[5]https://doi.org/10.1016/j.envpol.2021.116490奥代·加西亚-加林等，航拍图像中漂浮海洋大型垃圾的自动检测和量化:介绍一种连接到网络应用的新型深度学习方法，载于R，环境污染，<a class="ae lh" href="https://doi.org/10.1016/j.envpol.2021.116490" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="ac3d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[6] Rematas，k .，Martin-Brualla，r .，和Ferrari，v .，“ShaRF:来自单一视图的形状调节辐射场”，(2021)，【https://arxiv.org/abs/2102.08860<a class="ae lh" href="https://arxiv.org/abs/2102.08860" rel="noopener ugc nofollow" target="_blank"/></p><p id="5249" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[7]德鲁·a·哈德森和c·劳伦斯·兹尼克，《生成性对抗性变形金刚》，(2021)</p><p id="9413" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[8]桑德拉·布莱恩特(Sandra Bryant)等人，“我们要求人工智能创建约会档案。你会向右滑动吗？”，(2021)，UNSW悉尼博客。</p><p id="650b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[9]刘，z .等，2021，“Swin变压器:使用移位窗口的分层视觉变压器”，arXiv预印本【https://arxiv.org/abs/2103.14030v1】</p><p id="719a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[10]张，y，陈，w，凌，h，高，j，张，y，Torralba，a .和Fidler，s .，2020。图像甘满足逆向图形和可解释的3d神经渲染的可区分渲染。arXiv预印本arXiv:2010.09125</p><p id="e5e4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[11]尤耶和刘，2021年。深网:他们为视觉做过什么？。《国际计算机视觉杂志》，129(3)，第781–802页，<a class="ae lh" href="https://arxiv.org/abs/1805.04025" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1805.04025</a>。</p><p id="918c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[12]刘，a .，塔克，r .，贾帕尼，v .，马卡迪亚，a .，斯内夫利，n .，金泽，a .，2020年。无限自然:从单一图像生成自然场景的永久视图，<a class="ae lh" href="https://arxiv.org/pdf/2012.09855.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2012.09855.pdf</a></p><p id="0efd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[13] Nguyen &amp; Drealan等人(2021)一种具有基于深度学习的手指控制的便携式独立神经假体手:<a class="ae lh" href="https://arxiv.org/abs/2103.13452" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2103.13452</a></p><p id="9ace" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[14] Pandey等人，2021，Total Relighting:学习为背景替换重新照亮人像，doi: 10.1145/3450626.3459872，<a class="ae lh" href="https://augmentedperception.github.io/total_relighting/total_relighting_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://augmented perception . github . io/Total _ re lighting/Total _ re lighting _ paper . pdf</a>。</p><p id="b308" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[15]耿山阳等，(2021)，:从单目视频学习关节形状重建，，<a class="ae lh" href="https://lasr-google.github.io/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="0a23" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[16]里克特，阿布·阿尔海贾，科尔敦，(2021)，“增强照片真实感增强”，<a class="ae lh" href="https://intel-isl.github.io/PhotorealismEnhancement/" rel="noopener ugc nofollow" target="_blank">https://intel-isl.github.io/PhotorealismEnhancement/</a>。</p><p id="d34e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[17] DeepFakeHop:陈，洪硕，等，(2021)，“DeepFakeHop:一种轻量级高性能Deepfake检测器。”ArXiv abs/2103.06929。</p><p id="b853" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[18]梁，解，曾，惠，张，雷，(2021)，“高分辨率真实感图像实时翻译:拉普拉斯金字塔翻译网络”，【https://export.arxiv.org/pdf/2105.09188.pdf】。</p><p id="b453" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[19]朱佩豪等，(2021)，理发店，【https://arxiv.org/pdf/2106.01505.pdf】。</p><p id="f3b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[20] Praveen Krishnan，Rama Kovvuri，Guan Pang，Boris Vassilev，and Tal Hassner，脸书·艾(2021)，“文本风格刷:从单个例子转移文本美学”。</p><p id="185b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Holynski，Aleksander等人，“用欧拉运动场制作动画”IEEE/CVF计算机视觉和模式识别会议录。2021.</p><p id="2d88" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[22] Michael Niemeyer和Andreas Geiger，(2021)，“长颈鹿:将场景表示为合成生成神经特征场”，发表于CVPR 2021年。</p><p id="495b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[23] Chen，m .，Tworek，j .，Jun，h .，Yuan，q .，Pinto，H.P.D.O .，Kaplan，j .，Edwards，h .，y .，Joseph，n .，Brockman，g .和Ray，a .，2021年。评估基于代码训练的大型语言模型。arXiv预印本arXiv:2107.03374。</p><p id="0867" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[24]苹果，“通过私人设备上的机器学习识别照片中的人”，(2021年)，<a class="ae lh" href="https://machinelearning.apple.com/research/recognizing-people-photos" rel="noopener ugc nofollow" target="_blank">https://Machine Learning . Apple . com/research/recognized-People-Photos</a></p><p id="927b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[25]孟，c，宋，杨，宋，吴，朱，朱和埃蒙，s，2021。Sdedit:用随机微分方程进行图像合成和编辑。arXiv预印本arXiv:2108.01073。</p><p id="5b86" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[26]王士友、鲍、朱，2021。画出你自己的GAN。IEEE/CVF国际计算机视觉会议论文集(第14050-14060页)。</p><p id="c9d7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[27]“特斯拉AI日”，特斯拉，2021年8月19日，<a class="ae lh" href="https://youtu.be/j0z4FweCy4M" rel="noopener ugc nofollow" target="_blank">https://youtu.be/j0z4FweCy4M</a></p><p id="a83b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[28] Patashnik，Or等人(2021)，“样式剪辑:样式图像的文本驱动操作。”，<a class="ae lh" href="https://arxiv.org/abs/2103.17249" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2103.17249</a></p><p id="50f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[29]斯捷潘·图利亚科夫*、丹尼尔·格赫里希*、斯塔马蒂亚斯·乔戈里斯、朱利叶斯·埃尔巴赫、马蒂亚斯·格赫里希、元佑·李、大卫·斯卡拉穆扎，TimeLens:基于事件的视频帧内插，IEEE计算机视觉和模式识别会议(CVPR)，纳什维尔，2021年，<a class="ae lh" href="http://rpg.ifi.uzh.ch/docs/CVPR21_Gehrig.pdf" rel="noopener ugc nofollow" target="_blank">http://rpg.ifi.uzh.ch/docs/CVPR21_Gehrig.pdf</a></p><p id="ed01" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[30]哈伊姆、范斯坦、格拉诺特、肖彻、巴贡、戴克尔和伊拉尼(2021年)。多样化的一代从一个单一的视频成为可能，【https://arxiv.org/abs/2109.08591】T2。</p><p id="5768" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[31]拉武里、伦克、王绍博、坎金、拉姆、米罗夫斯基、菲茨西蒙斯、阿萨纳夏杜、卡舍姆、马奇和普鲁登，2021年。使用雷达深度生成模式的巧用降水临近预报，【https://www.nature.com/articles/s41586-021-03854-z T4】</p><p id="c41d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[32]彼得曼、威彻恩、王和鲁(2021年)。鸡尾酒叉问题:现实世界音轨的三干音频分离。<a class="ae lh" href="https://arxiv.org/pdf/2110.09958.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2110.09958.pdf</a>。</p><p id="c418" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[33]吕克特博士、弗兰克博士和斯塔明格博士，2021年。https://arxiv.org/pdf/2110.06635.pdf<a class="ae lh" href="https://arxiv.org/pdf/2110.06635.pdf" rel="noopener ugc nofollow" target="_blank">ADOP:近似可微分单像素点绘制。</a></p><p id="25b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[34] a) CLIPDraw:通过语言-图像编码器探索文本到绘图的合成。StyleCLIPDraw:文本到绘图合成中内容和样式的耦合。</p><p id="2b31" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[35]梁军、曹军、孙、张、范古尔和，2021年。SwinIR:使用swin transformer进行图像恢复。IEEE/CVF国际计算机视觉会议论文集(第1833-1844页)。</p><p id="512d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[36] Ling，h .，Kreis，k .，Li，d .，Kim，S.W .，Torralba，a .和Fidler，s .，2021年5月。EditGAN:高精度语义图像编辑。在第三十五届神经信息处理系统会议上。</p><p id="59c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[37]杨，，徐，李，潘，x，赵，n，饶，a，西奥多，c，戴，b和林，2021。城市NeRF:在城市尺度上建造NeRF。</p><p id="1d8b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[38]莫凯迪、赫兹和伯尔曼诺，2021年。ClipCap:图像字幕的剪辑前缀。<a class="ae lh" href="https://arxiv.org/abs/2111.09734" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2111.09734</a></p></div></div>    
</body>
</html>