<html>
<head>
<title>Why Should Adam Optimizer Not Be the Default Learning Algorithm?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么Adam Optimizer不应该是默认的学习算法？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/why-adam-optimizer-should-not-be-the-default-learning-algorithm-a2b8d019eaa0?source=collection_archive---------0-----------------------#2022-08-20">https://pub.towardsai.net/why-adam-optimizer-should-not-be-the-default-learning-algorithm-a2b8d019eaa0?source=collection_archive---------0-----------------------#2022-08-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a531" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于训练时间快，越来越多的深度学习实践者正在用自适应梯度方法训练他们的模型。特别是Adam，已经成为许多深度学习框架中使用的默认算法。尽管训练结果较好，但众所周知，与随机梯度下降(SGD)相比，Adam和其他自适应优化方法的泛化能力较差。这些方法往往在训练数据上表现良好，但在测试数据上却优于SGD。</p><p id="e66d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近，许多研究人员写下了实证研究，以思考自适应梯度方法的边际价值。让我们试着理解这些研究发现。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/dada50fab56379c3f1c1de7d29d80a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*8GtvbC59a3rZIqltLGbxHA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk translated">来源:自适应梯度方法的有界调度方法</figcaption></figure></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h2 id="b80b" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">亚当可能收敛得更快，但归纳能力很差！</h2><p id="d440" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">为了充分理解这种说法，有必要简要观察一下流行的优化算法Adam和SGD的优缺点。</p><p id="1dcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">梯度下降(vanilla) </strong>是优化深度学习网络最常用的方法。这项技术于20世纪50年代首次提出，它可以<em class="mc">更新模型的每个参数，观察变化如何影响目标函数，选择降低错误率的方向，并继续迭代，直到目标函数收敛到最小值。</em>要了解梯度下降的数学和功能，您可以阅读:</p><div class="md me gp gr mf mg"><a href="https://harjot-dadhwal.medium.com/math-behind-the-gradient-descent-algorithm-8d6137d92e9" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ir gy z fp ml fr fs mm fu fw ip bi translated">梯度下降算法背后的数学</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">注意:这篇文章是写给那些对ML设置有基本了解，并且对……</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">harjot-dadhwal.medium.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu kr mg"/></div></div></a></div><p id="56c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SGD是梯度下降的变体。SGD不是对整个数据集执行计算(这是多余且低效的),而是只对随机选择的数据示例的一个小子集进行计算。当学习速率较低时，SGD产生与常规梯度下降相同的性能。</p><p id="13c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">亚当的优化</strong> <em class="mc">方法根据梯度的一阶和二阶矩的估计值计算不同参数的个体自适应学习率。</em>它结合了均方根传播(RMSProp)和自适应梯度算法(AdaGrad)的优点，可以计算不同参数的个体自适应学习率。与RMSProp中基于平均一阶矩(平均值)调整参数学习率不同，Adam还利用了梯度二阶矩的平均值(无中心方差)。要了解更多关于Adam的信息，请阅读<a class="ae mv" href="https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c" rel="noopener" target="_blank"><em class="mc">Adam——深度学习优化的最新趋势。</em>T13】</a></p><blockquote class="mw mx my"><p id="b925" class="jn jo mc jp b jq jr js jt ju jv jw jx mz jz ka kb na kd ke kf nb kh ki kj kk ij bi translated">总的来说，Adam肯定快速收敛到“尖锐的最小值”,而SGD计算量大，收敛到“平坦的最小值”,但在测试数据上表现良好。</p></blockquote></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h2 id="dd68" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated"><strong class="ak">为什么ADAM不应该是默认算法？</strong></h2><p id="74bf" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">文章发表于2019年9月，<a class="ae mv" href="https://pdfs.semanticscholar.org/255b/6f582b91838d6ad56b305dc47bdfc5b8075e.pdf?_ga=2.98574494.1480037159.1661030524-754637628.1661030524" rel="noopener ugc nofollow" target="_blank"> <em class="mc">《自适应梯度方法的有界调度方法》</em> </a> <em class="mc"> </em>调查了导致Adam在训练复杂神经网络时性能不佳的因素。导致Adam经验泛化能力弱的关键因素总结如下:</p><ul class=""><li id="3070" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nh ni nj nk bi translated"><strong class="jp ir"> <em class="mc">梯度</em> </strong>的非均匀缩放会导致自适应梯度方法的泛化性能差。SGD是统一缩放的，低训练误差将很好地概括</li><li id="1aba" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated"><strong class="jp ir"><em class="mc">Adam中使用的指数移动平均</em> </strong>不能使学习率单调下降，会导致其无法收敛到最优解，出现泛化性能差的问题。</li><li id="d6e4" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">Adam学习到的学习率可能<strong class="jp ir"> <em class="mc">在某些情况下太小</em> </strong>而无法有效收敛，这将使其无法找到正确的路径并收敛到次优点。</li><li id="70de" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">亚当可能<strong class="jp ir"> <em class="mc">激进地增加学习率</em> </strong>，这对算法的整体性能是不利的。</li></ul><h2 id="866c" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">到目前为止的故事…</h2><p id="2bcd" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">尽管收敛速度更快，但自适应梯度算法的泛化性能通常比SGD差。具体来说，自适应梯度算法通常在训练阶段表现出较快的进展，但是它们的性能在测试数据上很快达到平稳状态。不同的是，SGD通常会缓慢地提高模型性能，但可以获得更高的测试性能。对这种推广差距的一个经验解释是，自适应梯度算法倾向于收敛到尖锐的最小值，其局部流域具有大的曲率，并且通常推广较差，而SGD更喜欢找到平坦的最小值，因此推广得更好。</p><p id="0cea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请记住，这并不否定自适应梯度方法在神经网络框架中学习参数的贡献。相反，它保证了SGD和其他非自适应梯度方法的实验。通过这篇文章，我试图在ML实验设置中探索非适应性梯度方法。盲目地将Adam设置为默认算法可能不是最好的方法。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><p id="ddef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你读到这里，我感谢你的耐心，希望这篇文章是你知识的引子，你正在找回一些有价值的东西。让我知道你对此的想法。</p><h2 id="a84d" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated"><strong class="ak">参考文献:</strong></h2><p id="0548" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated"><em class="mc"> a)一种用于自适应梯度方法的有界调度方法</em></p><p id="2ec5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mc"> b)机器学习中自适应梯度方法的边际价值</em></p><p id="8523" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mc"> c)通过从Adam切换到SGD来提高泛化性能</em></p><p id="c61e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mc"> d)从理论上理解为什么SGD在深度学习方面比ADAM概括得更好</em></p></div></div>    
</body>
</html>