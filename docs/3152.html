<html>
<head>
<title>This Is How I Utilize AI To Create One-of-A-kind Fairytales</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这就是我如何利用人工智能创造独一无二的童话故事</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/this-is-how-i-utilize-ai-to-create-one-of-a-kind-fairytales-d84bde347b5?source=collection_archive---------1-----------------------#2022-09-25">https://pub.towardsai.net/this-is-how-i-utilize-ai-to-create-one-of-a-kind-fairytales-d84bde347b5?source=collection_archive---------1-----------------------#2022-09-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b34e1f25a533c4dbfc77f7755a64aea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pEvUaChXhFBh8Op_hF0_gg.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="ef97" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">探索使用递归神经网络生成文本的新方法。</h2></div><p id="b14a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一个年轻的女孩曾经喜欢阅读童话。每天晚上睡觉前，她都会和妈妈一起读。然而，随着年龄的增长，她意识到这些故事并不总是有意义的。他们非常容易预测，性格也没有太大变化。</p><p id="fbc3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">“我们可以做得更好，”她心想。因此，她与她的人工智能专家父亲交谈，他们合作开始撰写她的童话，这将是新的，比她读过的任何东西都更令人激动。</p><p id="cc3b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">于是，在人工智能的辅助下，他们开始开发自己的童话故事！</p><p id="03d4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">正如我上面提到的，故事已经大致开始了，自从我开始利用人工智能编写我们的童话以来，我的女儿们现在每天晚上都有无限的新故事可以讲。</p><p id="e3e1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">人工智能生成的童话是我们对自己讲述的故事进行新思考的结果，而不仅仅是讲述它们的新方法。</p><p id="916b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我的人工智能生成的童话是以经典儿童故事的风格创作的。总的来说，它们提供了一个有着快乐结局的简单故事。</p><p id="9fdc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">它们由文本的逐字符生成模型生成，该模型使用Python中的LSTM递归神经网络和Keras。</p><p id="54ce" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">LTSM模型调查字符之间的关系和序列中字符的条件概率，允许您构建全新的和唯一的字符序列。一旦经过训练，该模型可以产生新的字符序列。</p><p id="2c1b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我用书中的文本训练我的模型，这些文本包括讲故事的例子。</p><p id="2f6e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然后，模型应用这些信息来创建新的任务。</p><p id="c22b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">除了用它们来帮助我的女孩们晚上睡觉，我还在纯人工智能博客上发布了一些最棒的故事，这些故事总是与我的人工智能生成的仙女配对。</p><p id="e806" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我计划很快为家长提供购买印刷版或下载电子书的选择，供他们的孩子在平板电脑和智能手机上阅读。</p><h2 id="1dc0" class="lp lq je bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">向经典学习</h2><p id="5785" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">正如我之前所说的，我的人工智能模型是用教科书上的童话例子创建的。因为古登堡计划是下载不再受版权保护的免费书籍的最佳地点之一，所以我这么做了。</p><p id="eb7b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我利用古腾堡计划中的数百个童话故事创建了一个策划数据集，其中包括刘易斯·卡罗尔和格林兄弟的作品。</p><p id="5f17" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当我开发这个模型的基本版本时，我创建了一个包含超过1，000，000个字符的数据集，当转换为小写时，神经网络学习的词典中有56个不同的字符——比字母表的26个字符多得多，对吗？</p><p id="0667" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在为我的神经网络定义训练数据时，我可以自由选择如何在训练期间分解文本并将其暴露给网络。</p><p id="0a95" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">所以我决定将数据集中的文本分成100个字符或任意长度的子序列。我可以把数据分成句子，填充较短的序列，截断较长的序列。我会在未来的版本中包括这一点。</p><p id="d221" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">每个网络训练模式由一个字符(X)后跟一个字符输出(y)的100次步进组成。当设计这些序列时，我在整本书上移动一个虚拟窗口，一次一个字符，允许每个字符从之前的100个字符中学习(当然，除了前100个字符)。</p><p id="4d06" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当然，因为神经网络处理的是数字而不是字符，所以当我将这本书分成这些序列时，我使用之前创建的查找表将字符转换为整数。</p><p id="38ed" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">到目前为止，运行我的代码表明，当我将数据集划分为供网络学习的训练数据时，我收到了不到1，000，000个训练模式。这是有意义的，因为除了前100个字符，我只有一个训练模式可以预测剩余的每个字符。</p><p id="97c6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一旦我准备好使用Keras，我就必须转换我的训练数据。</p><p id="ad95" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，我必须将输入序列列表转换成LSTM网络期望的[样本、时间步长、特征]格式。</p><p id="245c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">接下来，我必须将整数重新调整到0到1的范围内，以使LSTM网络更容易学习这些模式，该网络默认使用sigmoid激活函数。</p><p id="5167" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最后，我使用一键编码来转换输出模式(单个字符到整数)。这就是为什么我可以设置网络来预测词汇表中56个可能字符中每个字符的概率(一种更简单的表示)，而不是推动它准确地预测下一个字符。</p><p id="a1db" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">每个y值都被转换成一个稀疏向量，长度为56，一列1表示模式所代表的字母(整数)。</p><h2 id="6ace" class="lp lq je bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">创造LTSM模式。</h2><p id="abf5" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">现在必须定义LSTM模型，所以我定义了一个有256个内存单元的隐藏LSTM层。还有，我设置退学的概率是20。</p><p id="b0c9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">输出图层是一个密集图层，它使用softmax激活函数为每56个字符生成一个概率预测，范围从0到1。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="f53e" class="lp lq je ms b gy mw mx l my mz"><strong class="ms jf"># define my LSTM model</strong><br/>model = Sequential()<br/>model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))<br/>model.add(Dropout(0.2))<br/>model.add(LSTM(256))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(y.shape[1], activation=’softmax’))<br/>model.compile(loss=’categorical_crossentropy’, optimizer=’adam’)</span></pre><p id="b6ec" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">作为具有56个类的单字符分类问题，该任务被表征为使用ADAM优化算法来优化日志损失(交叉熵)以提高速度。</p><p id="b193" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">您可能已经注意到，我没有定义测试数据集。相反，我正在对完整的训练数据集进行建模，以计算出序列中每个字符的可能性。</p><p id="2d85" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我对训练数据集的最佳精度(分类精度)模型不感兴趣。</p><p id="defa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这是一个正确预测训练数据集中每个字符的模型。相反，我对概化最小化所选损失函数的数据集很感兴趣。</p><p id="9121" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我在努力平衡泛化和过拟合，但是在记忆方面有所欠缺。</p><p id="b734" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">该网络需要很长时间来训练(在特斯拉P100-PCIe-16GB上每个时期大约4分钟)。</p><p id="4fe1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">由于延迟和优化的需要，我采用了模型检查点来记录所有的网络权重，以便在epoch之后的丢失有所改善时归档。</p><p id="018a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然后，我使用最佳权重(最低损失)实例化了我的童话式生成模型。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi na"><img src="../Images/3c7e8a73c3a472bcbbe52d871f5e5516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M42dJ8svUf-ck3b79RbOLQ.png"/></div></div></figure><p id="a06f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我用这个模型的增强版得到了最好的结果，它使用了我从古腾堡项目下载的所有453本书，这些书都是英文的，有纯文本。</p><p id="6de0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个增强版本允许我在超过500次交互(epoch)后拥有可信的童话，这意味着大约43小时的GPU训练，在epoch 479时实现了0.2219的损失。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/64f995659c318cd333c8d2e6a24ed773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iLSdDdvV_t0p71mD8rsDaA.png"/></div></div></figure><h2 id="4b58" class="lp lq je bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">用我的LTSM模型创造一个童话</h2><p id="e4bd" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">使用训练有素的LSTM网络创建一个寓言极其简单。</p><p id="3f56" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，我加载了数据，并以与之前相同的方式定义了网络，只是网络权重是从一个检查点文件中加载的，并且网络不需要训练。</p><p id="49ac" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了理解这些预测，我需要开发一个反向映射，允许我将独特的字符转换成整数。</p><p id="ef72" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你想试试，这里有完整的代码:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="79e7" class="lp lq je ms b gy mw mx l my mz"><strong class="ms jf"># LSTM Network to Generate Fairytales</strong></span><span id="57bc" class="lp lq je ms b gy nc mx l my mz">import numpy as np</span><span id="7070" class="lp lq je ms b gy nc mx l my mz">from tensorflow.keras.models import Sequential</span><span id="adec" class="lp lq je ms b gy nc mx l my mz">from tensorflow.keras.layers import Dense</span><span id="064d" class="lp lq je ms b gy nc mx l my mz">from tensorflow.keras.layers import Dropout</span><span id="d4e3" class="lp lq je ms b gy nc mx l my mz">from tensorflow.keras.layers import LSTM</span><span id="beb2" class="lp lq je ms b gy nc mx l my mz">from tensorflow.keras.callbacks import ModelCheckpoint</span><span id="6388" class="lp lq je ms b gy nc mx l my mz">from tensorflow.keras.utils import to_categorical</span><span id="0760" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># load ascii text and covert to lowercase</strong></span><span id="f2c0" class="lp lq je ms b gy nc mx l my mz">filename = “wonderland.txt”</span><span id="c5fe" class="lp lq je ms b gy nc mx l my mz">raw_text = open(filename, ‘r’, encoding=’utf-8').read()</span><span id="10f3" class="lp lq je ms b gy nc mx l my mz">raw_text = raw_text.lower()</span><span id="2d0d" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># create mapping of unique chars to integers</strong></span><span id="69ab" class="lp lq je ms b gy nc mx l my mz">chars = sorted(list(set(raw_text)))</span><span id="075f" class="lp lq je ms b gy nc mx l my mz">char_to_int = dict((c, i) for i, c in enumerate(chars))</span><span id="f0bf" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># summarize the loaded data</strong></span><span id="047d" class="lp lq je ms b gy nc mx l my mz">n_chars = len(raw_text)</span><span id="f18f" class="lp lq je ms b gy nc mx l my mz">n_vocab = len(chars)</span><span id="789b" class="lp lq je ms b gy nc mx l my mz">print(“Total Characters: “, n_chars)</span><span id="9900" class="lp lq je ms b gy nc mx l my mz">print(“Total Vocab: “, n_vocab)</span><span id="cb36" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># prepare the dataset of input to output pairs encoded as integers</strong></span><span id="563d" class="lp lq je ms b gy nc mx l my mz">seq_length = 100</span><span id="d01c" class="lp lq je ms b gy nc mx l my mz">dataX = []</span><span id="7a7e" class="lp lq je ms b gy nc mx l my mz">dataY = []</span><span id="ecd7" class="lp lq je ms b gy nc mx l my mz">for i in range(0, n_chars — seq_length, 1):</span><span id="ecc0" class="lp lq je ms b gy nc mx l my mz">seq_in = raw_text[i:i + seq_length]</span><span id="1cd9" class="lp lq je ms b gy nc mx l my mz">seq_out = raw_text[i + seq_length]</span><span id="70f9" class="lp lq je ms b gy nc mx l my mz">dataX.append([char_to_int[char] for char in seq_in])</span><span id="142c" class="lp lq je ms b gy nc mx l my mz">dataY.append(char_to_int[seq_out])</span><span id="cf06" class="lp lq je ms b gy nc mx l my mz">n_patterns = len(dataX)</span><span id="854f" class="lp lq je ms b gy nc mx l my mz">print(“Total Patterns: “, n_patterns)</span><span id="1e65" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># reshape X to be [samples, time steps, features]</strong></span><span id="cc42" class="lp lq je ms b gy nc mx l my mz">X = np.reshape(dataX, (n_patterns, seq_length, 1))</span><span id="b3e7" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># normalize</strong></span><span id="1145" class="lp lq je ms b gy nc mx l my mz">X = X / float(n_vocab)</span><span id="01e9" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># one hot encode the output variable</strong></span><span id="9ab2" class="lp lq je ms b gy nc mx l my mz">y = to_categorical(dataY)</span><span id="f82e" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># define the LSTM model</strong></span><span id="561c" class="lp lq je ms b gy nc mx l my mz">model = Sequential()</span><span id="ab03" class="lp lq je ms b gy nc mx l my mz">model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))</span><span id="05d0" class="lp lq je ms b gy nc mx l my mz">model.add(Dropout(0.2))</span><span id="6229" class="lp lq je ms b gy nc mx l my mz">model.add(LSTM(256))</span><span id="1548" class="lp lq je ms b gy nc mx l my mz">model.add(Dropout(0.2))</span><span id="49d3" class="lp lq je ms b gy nc mx l my mz">model.add(Dense(y.shape[1], activation=’softmax’))</span><span id="fd14" class="lp lq je ms b gy nc mx l my mz">model.compile(loss=’categorical_crossentropy’, optimizer=’adam’)</span><span id="6bf7" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># define the checkpoint</strong></span><span id="89d4" class="lp lq je ms b gy nc mx l my mz">filepath = “weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5”</span><span id="1ad2" class="lp lq je ms b gy nc mx l my mz">checkpoint = ModelCheckpoint(filepath, monitor=’loss’, verbose=1, save_best_only=True, mode=’min’)</span><span id="105a" class="lp lq je ms b gy nc mx l my mz">callbacks_list = [checkpoint]</span><span id="2842" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># fit the model</strong></span><span id="64b1" class="lp lq je ms b gy nc mx l my mz">model.fit(X, y, epochs=650, batch_size=64, callbacks=callbacks_list)</span></pre><p id="30de" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在建立模型并获得检查点之后，我准备进行文本预测。</p><p id="4f5a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了使用Keras LSTM模型进行预测，我从一个种子序列作为输入开始，生成下一个字符，然后更新种子序列，在末尾添加生成的字符，并删除初始字符。</p><p id="c190" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">每当我需要预测新字符(例如，长度为1，000个字符的序列)时，就使用这种方法。</p><p id="4a8e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我可以使用一个随机的输入模式作为我的种子序列，然后完全按照生成的样子发布结果字符。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="44dc" class="lp lq je ms b gy mw mx l my mz"><strong class="ms jf"># pick a random seed</strong></span><span id="36f4" class="lp lq je ms b gy nc mx l my mz">start = np.random.randint(0, len(dataX)-1)</span><span id="c238" class="lp lq je ms b gy nc mx l my mz">pattern = dataX[start]</span><span id="e28c" class="lp lq je ms b gy nc mx l my mz">print(“Seed:”)</span><span id="0845" class="lp lq je ms b gy nc mx l my mz">print(“\””, ‘’.join([int_to_char[value] for value in pattern]), “\””)</span><span id="9eea" class="lp lq je ms b gy nc mx l my mz"><strong class="ms jf"># generate characters</strong></span><span id="d34e" class="lp lq je ms b gy nc mx l my mz">for i in range(1000):</span><span id="df75" class="lp lq je ms b gy nc mx l my mz">x = np.reshape(pattern, (1, len(pattern), 1))</span><span id="7102" class="lp lq je ms b gy nc mx l my mz">x = x / float(n_vocab)</span><span id="eacb" class="lp lq je ms b gy nc mx l my mz">prediction = model.predict(x, verbose=0)</span><span id="f615" class="lp lq je ms b gy nc mx l my mz">index = np.argmax(prediction)</span><span id="79a2" class="lp lq je ms b gy nc mx l my mz">result = int_to_char[index]</span><span id="26ea" class="lp lq je ms b gy nc mx l my mz">seq_in = [int_to_char[value] for value in pattern]</span><span id="8673" class="lp lq je ms b gy nc mx l my mz">sys.stdout.write(result)</span><span id="c0de" class="lp lq je ms b gy nc mx l my mz">pattern.append(index)</span><span id="adb0" class="lp lq je ms b gy nc mx l my mz">pattern = pattern[1:len(pattern)]</span><span id="6645" class="lp lq je ms b gy nc mx l my mz">print(“\nThe End.”)</span></pre><h2 id="97e9" class="lp lq je bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">结果</h2><p id="e8f3" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">我对我从这个模型的增强版本中获得的结果非常满意，它帮助我在任何我想要的时候获得一个新的鼓舞人心的童话。</p><p id="fef8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我正致力于这个模型的疯狂优化版本，为我的博客写故事…你可以在下面看到一些结果:</p><ul class=""><li id="8f6b" class="nd ne je kv b kw kx kz la lc nf lg ng lk nh lo ni nj nk nl bi translated">温柔的耶泽贝塔·梭罗</li><li id="53c8" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" href="https://merelyaiart.shop/2022/09/21/tinkerellis-the-elf/?v=9b7d173b068d" rel="noopener ugc nofollow" target="_blank">小精灵tinker Ellis</a></li><li id="456d" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" href="https://merelyaiart.shop/2022/09/22/stella-oriebir-the-fairy-goblin/?v=9b7d173b068d" rel="noopener ugc nofollow" target="_blank">斯特拉·奥里比尔，仙女和妖精</a></li></ul><p id="a1ed" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当然，仍然需要一些语法修正和角色名字和地点的丰富多彩的添加，但我正在努力使童话制作完全自动化。</p><p id="4eb5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我可以使用绝对强大且随时可用的GPT-2模型，但我想自己试验并学习做一个相对较好的模型…对于某些任务，我显然更喜欢使用随时可用的模型…但对于特定的任务，如帮助我的女儿做甜蜜和鼓舞人心的梦，我更喜欢对正在发生的事情有一些控制…</p><h2 id="069b" class="lp lq je bd lr ls lt dn lu lv lw dp lx lc ly lz ma lg mb mc md lk me mf mg mh bi translated">结论</h2><p id="341c" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">如果我们告诉自己的故事创造了我们的现实，如果我们用人工智能对故事有了新的思考方式，它就有可能塑造现实。</p><p id="f6ac" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">通过这个模型，我正在尝试使用递归神经网络生成文本的新方法。</p><p id="05cd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你有兴趣深入了解，下面我会分享更多关于这个主题的资源和教程。</p><p id="081e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我还推荐从最受欢迎的教程开始，作者是Andrej Karpathy，题目是“<a class="ae nm" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络的不合理有效性</a>”</p><h1 id="1753" class="ns lq je bd lr nt nu nv lu nw nx ny lx kk nz kl ma kn oa ko md kq ob kr mg oc bi translated">你可能想读的其他文章。</h1><ul class=""><li id="dfbf" class="nd ne je kv b kw mi kz mj lc od lg oe lk of lo ni nj nk nl bi translated"><a class="ae nm" href="https://medium.com/codex/a-quick-look-under-the-hood-of-stable-diffusion-open-source-architecture-2f07fc1e729" rel="noopener"> <strong class="kv jf">快速浏览稳定扩散开源架构的引擎盖下。</strong> </a></li><li id="8188" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" rel="noopener ugc nofollow" target="_blank" href="/are-we-witnessing-the-next-evolution-of-artificial-intelligence-264f251ea06d"> <strong class="kv jf">我们正在见证人工智能的下一次进化吗？</strong>T9】</a></li><li id="4bc4" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" rel="noopener ugc nofollow" target="_blank" href="/these-10-algorithms-can-change-your-life-if-you-work-with-data-ff544657922d"> <strong class="kv jf">这10种算法可以改变你的生活——如果你与数据打交道</strong> </a></li><li id="7105" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" href="https://medium.com/illumination/these-9-research-papers-are-changing-how-i-see-artificial-intelligence-this-year-cd8ba548f785" rel="noopener"> <strong class="kv jf">这9篇研究论文正在改变我今年对人工智能的看法。</strong> </a></li><li id="83c1" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" rel="noopener ugc nofollow" target="_blank" href="/5-very-practical-ways-ai-can-help-to-improve-your-companys-productivity-f4d5dcd0b30c"> <strong class="kv jf"> 5个非常实用的方法人工智能可以帮助提高你公司的生产力</strong> </a></li></ul><h1 id="be3b" class="ns lq je bd lr nt nu nv lu nw nx ny lx kk nz kl ma kn oa ko md kq ob kr mg oc bi translated">链接、参考和资源</h1><ul class=""><li id="b5b3" class="nd ne je kv b kw mi kz mj lc od lg oe lk of lo ni nj nk nl bi translated"><a class="ae nm" href="https://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf" rel="noopener ugc nofollow" target="_blank">用递归神经网络生成文本</a> [pdf]，2011</li><li id="17ce" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" href="https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py" rel="noopener ugc nofollow" target="_blank">用于文本生成的LSTM Keras代码示例</a></li><li id="b7b1" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" href="https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py" rel="noopener ugc nofollow" target="_blank">用于文本生成的LSTM千层面代码示例</a></li><li id="8083" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" href="https://mxnetjl.readthedocs.io/en/latest/tutorial/char-lstm.html" rel="noopener ugc nofollow" target="_blank">使用LSTM生成文本的MXNet教程</a></li><li id="1e6b" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" href="https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">用递归神经网络自动生成点击诱饵</a></li><li id="7e2f" class="nd ne je kv b kw nn kz no lc np lg nq lk nr lo ni nj nk nl bi translated"><a class="ae nm" href="https://blog.csdn.net/xiewenbo/article/details/70834376" rel="noopener ugc nofollow" target="_blank">用Python中的LSTM递归神经网络生成文本</a></li></ul><h1 id="8749" class="ns lq je bd lr nt nu nv lu nw nx ny lx kk nz kl ma kn oa ko md kq ob kr mg oc bi translated">你愿意支持我吗？</h1><p id="5306" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated"><em class="og">为了获得无限的故事，你也可以考虑</em> <a class="ae nm" href="https://medium.com/@jairribeiro/membership" rel="noopener"> <em class="og">注册</em> </a> <em class="og">成为中等会员，只需5美元。此外，如果您使用我的链接注册</em>  <em class="og">，我会收到一小笔佣金(无需额外费用)。</em></p><div class="is it gp gr iu oh"><a href="https://medium.com/@jairribeiro/membership" rel="noopener follow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd jf gy z fp om fr fs on fu fw jd bi translated">通过我的推荐链接加入媒体- Jair Ribeiro</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">阅读我分享的每一个故事(以及媒体上成千上万的其他作者)。你的会员费直接支持其他…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">medium.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov ja oh"/></div></div></a></div></div></div>    
</body>
</html>