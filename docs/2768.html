<html>
<head>
<title>No Training Data? No Problem! Weak Supervision to the Rescue!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">没有训练数据？没问题！对救援的监管不力！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/no-training-data-no-problem-weak-supervision-to-the-rescue-bd38b475a412?source=collection_archive---------0-----------------------#2022-05-17">https://pub.towardsai.net/no-training-data-no-problem-weak-supervision-to-the-rescue-bd38b475a412?source=collection_archive---------0-----------------------#2022-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="db53" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用领域知识生成大型带标签的数据集，具有最先进的NLP弱监督。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c7d29d29bd535f514c888b070f423bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IOo1yogZoegVq0_5mnpslg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">在机器学习模型中利用来自主题专家的丰富领域知识！作者使用的图片来自Nikita Golubev创建的科学家图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/scientist" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>，Freepik创建的人物图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/people" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>，Freepik创建的十字架图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/cross" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>，img flip<a class="ae ky" href="http://imgflip.com" rel="noopener ugc nofollow" target="_blank">创建的Drake Meme</a>。</figcaption></figure><h1 id="667e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">🚧当代机器学习的挑战</h1><p id="c223" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在现实世界应用中开发现代机器学习(ML)模型的主要瓶颈之一是需要大量人工标记的训练数据。例如，<a class="ae ky" href="https://image-net.org/index.php" rel="noopener ugc nofollow" target="_blank"> ImageNet数据集</a>由超过1400万张各种现实世界物体的手动标记图像组成。<a class="ae ky" href="https://en.wikipedia.org/wiki/Transfer_learning#:~:text=Transfer%20learning%20(TL)%20is%20a,when%20trying%20to%20recognize%20trucks." rel="noopener ugc nofollow" target="_blank">迁移学习</a>的出现极大地缓解了这一要求，但我们仍然需要数百个(如果不是数千个)带标签的例子来微调最新的艺术(SOTA)语言模型，如BERT。</p><p id="83cf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">不幸的是，获得这种手动注释通常是耗时且费力的，容易出现人为错误和偏差，并且很难根据不断变化的条件保持更新[ <a class="ae ky" href="https://arxiv.org/abs/1707.02968" rel="noopener ugc nofollow" target="_blank">扳手纸</a> ]。</p><p id="d55d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">那么我们能做些什么来改善这种情况呢？进入弱监管！大多数组织在布尔查询、启发式规则或传统上在ML模型中不被使用的部落知识方面具有巨大的领域知识深度。</p><p id="a33a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最近的弱监督(WS)框架可以减少手动标记的工作，同时通过利用各种较弱的、通常是程序性的监督来源来释放领域主题专家(SME)的大量知识。</p><blockquote class="ms"><p id="1c45" class="mt mu it bd mv mw mx my mz na nb mm dk translated">在爱德曼，这导致了爱德曼研究信任的历史中的最佳领域知识与最先进的NLP ML模型的有力结合。</p></blockquote><figure class="nc nd ne nf ng kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ac61608ba82c9b8f582e619e30dfe80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMQ495arUYoJI0hatbMrNw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">在机器学习模型中利用来自主题专家的丰富领域知识！作者使用的图片来自Nikita Golubev创建的科学家图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/scientist" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>，Freepik创建的人物图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/people" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>，Freepik创建的十字架图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/cross" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>，img flip<a class="ae ky" href="http://imgflip.com" rel="noopener ugc nofollow" target="_blank">创建的Drake Meme</a>。</figcaption></figure><p id="b550" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这篇文章将介绍在弱监管领域的一些最新进展，爱德曼DxI的数据科学团队为解决我们的NLP问题而感到兴奋不已！</p><h1 id="71e3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">💡监管不力</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/e3ee364244480d17f604c2db58417a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XkF9LfllxxeuoEmWsnWorw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">弱监管概述。作者使用的图片来自Nikita Golubev创建的科学家图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/scientist" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>，平面图标创建的布尔图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/boolean" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>，surang创建的层次图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/hierarchy" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>，Freepik创建的知识图标— <a class="ae ky" href="https://www.flaticon.com/free-icons/knowledge" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>。</figcaption></figure><p id="6931" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">数据编程是使用标签模型结合<a class="ae ky" href="https://arxiv.org/abs/1605.07723" rel="noopener ugc nofollow" target="_blank">启发式标签功能</a>编程创建标签数据集的名称。弱监督使用由标签模型创建的标签数据集来训练下游终端模型，该下游终端模型在标签模型的输出之外进行概括。对数据集实施弱监管有三个步骤，如<a class="ae ky" href="http://arxiv.org/abs/1711.10160" rel="noopener ugc nofollow" target="_blank">浮潜论文</a>中所述。</p><ol class=""><li id="8f7f" class="ni nj it lt b lu mn lx mo ma nk me nl mi nm mm nn no np nq bi translated"><strong class="lt iu">编写标签函数(LFs): </strong>标签函数是任何Python函数，它可以将一行数据作为输入，并使用一些规则输出该行的标签。例如，如果我们的任务是“垃圾邮件检测”，可以构建一个标记函数，如下面的代码片段所示。每个标记功能独立运行，以标记每一行数据。在二元分类问题的情况下，标签或者是0(没有标签)或者是1(有标签)或者是-1(信息不足，所以不加标签)。</li><li id="9b31" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated"><strong class="lt iu">将弱标签与标签模型(LM)相结合:</strong>如果我们有<code class="fe nw nx ny nz b">m</code>行数据和<code class="fe nw nx ny nz b">n</code>个LFs，那么运行所有LFs将产生总共<code class="fe nw nx ny nz b">m X n</code>个标签。我们需要聚集<code class="fe nw nx ny nz b">n</code>个独立LFs的输出，这样每一行只有一个标签。多数投票标签模型是将多个LFs聚合为每行一个标签的最简单方法。尽管如此，通过了解整个<code class="fe nw nx ny nz b">m</code>行中各种LFs之间的协议和分歧，仍有更好的方法进行汇总。我们在一些花哨的数学的帮助下做到这一点，它不需要任何地面事实数据[ <a class="ae ky" href="http://arxiv.org/abs/1605.07723" rel="noopener ugc nofollow" target="_blank">数据编程纸</a> ][ <a class="ae ky" href="http://arxiv.org/abs/1810.02840" rel="noopener ugc nofollow" target="_blank">金属纸</a> ][ <a class="ae ky" href="http://arxiv.org/abs/2002.11955" rel="noopener ugc nofollow" target="_blank">飞行乌贼纸</a> ]！</li><li id="6757" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated"><strong class="lt iu">训练下游端模型(EM): </strong>标签模型的输出被用作训练数据，以微调诸如BERT的下游模型，从而推广到LM的标签之外。由于LFs是编程标记源，我们可以在整个未标记语料库上运行步骤1和2来生成许多标签。步骤3中训练的模型可以受益于步骤1和2中创建的更广泛的训练数据集。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/0f4790614dbd5af1d8b8e7b3cd69a547.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AfogUgQiboK7UZNRHUpq4A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://www.snorkel.org/use-cases/01-spam-tutorial" rel="noopener ugc nofollow" target="_blank">官方通气管文件</a>的标签功能示例。图片作者。</figcaption></figure><p id="d09e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://github.com/snorkel-team/snorkel" rel="noopener ugc nofollow" target="_blank">scupk</a>是事实上的Python库，使用数据编程范式从弱监督开始。它提供了易于使用的API来实现和评估步骤1和2。我们可以使用高级的ML APIs比如<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> Huggingface的变形金刚</a>或者<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Sklearn </a>来实现步骤3。</p><p id="a34c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在某些方法中，我们可以将步骤2和3合并成一个步骤。然而，在我们的实验和文献T23扳手论文T24中发表的那些实验中，两阶段方法似乎优于一阶段方法，因为它们允许我们选择任何LM和em组合，直到我们获得最佳性能。因此，我们仅限于两阶段的方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/19c22fda0d5f7b18c89648fddeca40a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*nU4H1TAu-4_fO74kG8-FVw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">监管不力的类型。图片来自<a class="ae ky" href="https://arxiv.org/abs/2109.11377" rel="noopener ugc nofollow" target="_blank"> arxiv论文</a>。</figcaption></figure><h1 id="d1a1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">🧰监管框架薄弱</h1><p id="e726" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在弱监管基准[ <a class="ae ky" href="https://arxiv.org/abs/2109.11377" rel="noopener ugc nofollow" target="_blank">扳手论文</a> ][ <a class="ae ky" href="https://github.com/jieyuz2/wrench" rel="noopener ugc nofollow" target="_blank"> Github </a> ]中，作者对各种弱监管框架进行了基准测试，并与全监管黄金基准进行了比较，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/57058a5b13ce5ccc7a9c76aef8ec2a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*kIkXvKR7jVcNrkzjY6O1XQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">扳手论文的作者测试了各种弱监管框架——图片来自<a class="ae ky" href="https://arxiv.org/abs/2109.11377" rel="noopener ugc nofollow" target="_blank"> arxiv论文</a>。</figcaption></figure><p id="558e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于NLP任务，余弦RoBERTa (RC)始终优于其他终端模型(EM)，包括vanilla RoBERTa (R)，因此我们可以在两阶段方法中安全地选择RC作为我们的终端模型！另一方面，没有一个单一标签模型(LM)框架始终表现出色，这表明我们必须在我们的数据集上试验不同的LM，以选择最佳的一个。下面简单讨论一下这些方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/2095d910e7b361a9ffd397e866e7e656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*RMgqMPcxss0aAsmg5kzpAg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">扳手论文中几个数据集的最佳弱监管框架总结？图片来自<a class="ae ky" href="https://arxiv.org/abs/2109.11377" rel="noopener ugc nofollow" target="_blank"> arxiv论文</a>，由作者标注</figcaption></figure><h2 id="5240" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated"><strong class="ak">余弦(RC，BC)【</strong><a class="ae ky" href="https://aclanthology.org/2021.naacl-main.84/" rel="noopener ugc nofollow" target="_blank"><strong class="ak">论文</strong></a><strong class="ak">】</strong><a class="ae ky" href="https://github.com/yueyu1030/COSINE" rel="noopener ugc nofollow" target="_blank"><strong class="ak">Github</strong></a><strong class="ak">】</strong><a class="ae ky" href="https://github.com/jieyuz2/wrench" rel="noopener ugc nofollow" target="_blank"><strong class="ak">扳手实现</strong></a><strong class="ak"/></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/a3a187a5b5b7557e4d071b4599797195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LhTdaBC-FltFN6DU.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">余弦模型架构。图片来自<a class="ae ky" href="https://aclanthology.org/2021.naacl-main.84/" rel="noopener ugc nofollow" target="_blank"> ACL论文</a>。</figcaption></figure><p id="d11c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">余弦是用于微调预训练语言模型的对比自训练的缩写，是近年来弱监督领域最有前途的发展之一。余弦算法有五个步骤。</p><ol class=""><li id="0f99" class="ni nj it lt b lu mn lx mo ma nk me nl mi nm mm nn no np nq bi translated"><strong class="lt iu">初始化:</strong>我们使用来自标签模型的弱标签，在初始化步骤中使用交叉熵损失来微调诸如BERT的语言模型。然后，我们将这个微调的BERT模型在整个数据集上的概率预测作为软伪标签。我们使用软伪标签，使用复合损失迭代地继续BERT模型的微调，如下式所示。</li><li id="d771" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated"><strong class="lt iu">样本重新加权:</strong>每个样本然后基于其预测概率被重新加权，使得具有高预测概率的样本具有高权重，而具有低预测概率的样本具有相应的低权重。</li><li id="7167" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated"><strong class="lt iu">高置信度样本的分类损失:</strong>我们使用高于预定义权重阈值ξ的样本来计算基于<a class="ae ky" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> Kullback-Leibler散度</a>的加权分类损失，因为我们使用的是软伪标签。</li><li id="92f5" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated"><strong class="lt iu">高置信度样本的对比损失:</strong>我们使用上一步中的相同样本来计算对比损失，使得具有相似伪标签的样本靠近在一起。相反，具有不同伪标签的样本在向量空间中被推开。阳性和阴性样本之间的差值是一个超参数。</li><li id="bd47" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated"><strong class="lt iu">所有样本上的置信度正则化:</strong>上述整个方法仅在我们的置信度(预测概率)是正确的并且错误标记的样本具有低置信度时才有效。因此，最终损失是基于置信度的正则化器，其阻止错误标记的样本获得过高的置信度(过置信度)。可以调整超参数λ来调整正则化强度。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/08ffe7753eb589929235cd89730ff639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*L4Dy6aURIkb8AhjQ.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">余弦复合损失函数。图片作者。</figcaption></figure><p id="21e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为这些步骤的结果，余弦方法对于弱标签中的噪声是鲁棒的。在我们的内部基准测试中，这是性能最好的方法之一，尤其是如果您有一个小的带标签的数据集来执行初始化步骤。<a class="ae ky" href="https://github.com/jieyuz2/wrench" rel="noopener ugc nofollow" target="_blank">扳手报告</a>中的实现简单易用。</p><h2 id="0692" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">通气管[ <a class="ae ky" href="http://arxiv.org/abs/1605.07723" rel="noopener ugc nofollow" target="_blank">数据编程(DP)纸</a> ][ <a class="ae ky" href="http://arxiv.org/abs/1810.02840" rel="noopener ugc nofollow" target="_blank">金属纸</a> ][ <a class="ae ky" href="https://github.com/snorkel-team/snorkel" rel="noopener ugc nofollow" target="_blank"> Github </a> ]</h2><p id="3435" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">通气管是所有弱监督标签模型方法之母！创造了“数据编程”一词的斯坦福大学研究人员也发明了潜泳。浮潜的前提很简单:给定一组启发式标记函数(LFs)，将来自这些函数中每一个的弱标签组合成每个样本的单个标签[ <a class="ae ky" href="https://www.youtube.com/watch?v=RUPbYvzSrg0" rel="noopener ugc nofollow" target="_blank"> Youtube视频</a> ]。通气管提供了一个易于使用的框架来聚合多个嘈杂、重叠、微弱的LFs。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/cf04bb7e0f2be0f481f021d52df4703b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m5vboHVR5NPf98uIwTokpA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">使用通气管将每行多个LFs合并为每行一个弱标签。作者图片</figcaption></figure><p id="82ea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">组合多个弱标签的一种方法是使用多数投票(MV ),事实上，在扳手论文的基准测试结果中，MV确实显示为少数数据集的最佳LM。然而，LFs可能是相关的，导致MV模型中的特定信号过多。相反，浮潜实现了一个更复杂的LM，使用一些简洁的数学矩阵逆魔法来组合单个LFs的输出，如下图所示，并在本<a class="ae ky" href="https://www.datacouncil.ai/hubfs/DataEngConf/Data%20Council/Slides%20SF%2019/Accelerating%20Machine%20Learning%20with%20Training%20Data%20Management.pdf" rel="noopener ugc nofollow" target="_blank">演示</a>和[ <a class="ae ky" href="http://arxiv.org/abs/1810.02840" rel="noopener ugc nofollow" target="_blank">金属纸</a>中进行了详细描述。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/108c18fa38c4309eb21230deb39aadc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-29eUsIZ2Br2yQYx.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">浮潜背后的数学。图片来自<a class="ae ky" href="https://ajratner.github.io/assets/papers/MTS_AAAI_2019_poster_portrait.pdf" rel="noopener ugc nofollow" target="_blank">海报</a>和<a class="ae ky" href="https://www.youtube.com/watch?v=RUPbYvzSrg0" rel="noopener ugc nofollow" target="_blank"> Youtube视频</a>。</figcaption></figure><p id="9317" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通气管文档提供了优秀的代码示例，帮助您开始使用通气管[ <a class="ae ky" href="https://www.snorkel.org/use-cases/" rel="noopener ugc nofollow" target="_blank">链接</a> ]构建LFs和LM。</p><h2 id="7d64" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">启发式LF选择</h2><p id="a63a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在我们的实验中，通气管易于使用，但根据通气管标签训练的末端模型(EM)的准确性可能会因LFs的质量而有很大差异。因此，我们实现了一个启发式的LF选择过程，它只使用LF Zoo中的一个LF子集，在一个小的手工标记的验证集上具有最好的准确性。</p><p id="034b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">启发式LF选择的迭代性质是特别可取的，因为它允许我们从很少数量的LF开始工作，并随着时间的推移添加或改进它们。通过分析哪些逻辑框架在每次迭代中表现不佳，我们可以确定逻辑框架中的差距，为下一轮逻辑框架的创建或更新提供信息。该分析还可能暴露我们对问题领域理解的差距！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/722c298370951156cfa6d18004621e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vB04-4ijdhEw0-4BY143cg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">我们在爱德曼实施的启发式LF选择。作者使用的图片来自Nikita Golubev—<a class="ae ky" href="https://www.flaticon.com/free-icons/scientist" rel="noopener ugc nofollow" target="_blank">flat icon</a>创建的科学家图标和Freepik — <a class="ae ky" href="https://www.flaticon.com/free-icons/filter" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>创建的过滤器图标。</figcaption></figure><h1 id="a875" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">🏗️结论和未来工作</h1><p id="39f4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我介绍了弱监督的概念，以及我们如何使用它将主题专家的领域知识编码到机器学习模型中。我还讨论了最终模型的余弦和标签模型的通气管。在两步弱监督方法中结合这两个框架可以实现与完全监督ML模型相媲美的准确性，而无需收集大量手动标记的训练数据集！爱德曼DxI的严格内部实验证实了这些结果，并且是数据科学团队的核心关注领域。</p><p id="2ab4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇文章中，我主要关注扳手、余弦和浮潜论文。弱监管领域有几种更有趣的方法，看起来很有前途。Edelman DxI团队计划在未来关注的一些问题包括:</p><ol class=""><li id="dd52" class="ni nj it lt b lu mn lx mo ma nk me nl mi nm mm nn no np nq bi translated">想降低贴标成本？GPT-3可以帮助[ <a class="ae ky" href="http://arxiv.org/abs/2108.13487" rel="noopener ugc nofollow" target="_blank">纸</a> ][ <a class="ae ky" href="https://github.com/rafaelsandroni/gpt3-data-labeling" rel="noopener ugc nofollow" target="_blank"> Github </a></li><li id="0ac0" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated">x级:监督极弱的文本分类[ <a class="ae ky" href="https://arxiv.org/abs/2010.12794v1" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae ky" href="https://github.com/ZihanWangKi/XClass" rel="noopener ugc nofollow" target="_blank"> Github </a> ]</li><li id="3b81" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated">OptimSeed:无监督误差估计的弱监督文本分类的种子词选择:[ <a class="ae ky" href="https://arxiv.org/abs/2104.09765v1" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae ky" href="https://github.com/YipingNUS/OptimSeed" rel="noopener ugc nofollow" target="_blank"> Github </a> ]</li><li id="eb7d" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated">ASTRA:弱监督下的自我训练[ <a class="ae ky" href="http://arxiv.org/abs/2104.05514" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae ky" href="https://github.com/microsoft/ASTRA" rel="noopener ugc nofollow" target="_blank"> Github </a> ]</li><li id="99cf" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated">SPEAR:带有子集选择的半监督数据编程[ <a class="ae ky" href="http://arxiv.org/abs/2008.09887" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae ky" href="https://github.com/decile-team/spear" rel="noopener ugc nofollow" target="_blank"> Github </a> ]</li></ol><p id="d78f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">感谢阅读！</p></div><div class="ab cl ow ox hx oy" role="separator"><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb"/></div><div class="im in io ip iq"><p id="6fcb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个帖子是2022年5月19日Quantum Black在新加坡组织的一次Meetup的话题。幻灯片的链接在这里:<a class="ae ky" href="https://www.slideshare.net/StephenLeo7/weak-supervisionpdf" rel="noopener ugc nofollow" target="_blank">链接</a>。视频记录在下面</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure></div></div>    
</body>
</html>