<html>
<head>
<title>A Fundamental Principle of Neuroscience that is Inspiring Optimizations in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经科学的一个基本原则是激发神经网络的优化</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-fundamental-principle-of-neuroscience-that-is-inspiring-optimizations-in-neural-networks-1a5850951089?source=collection_archive---------1-----------------------#2021-10-20">https://pub.towardsai.net/a-fundamental-principle-of-neuroscience-that-is-inspiring-optimizations-in-neural-networks-1a5850951089?source=collection_archive---------1-----------------------#2021-10-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f4a4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="8266" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">IBM的研究希望在神经网络的设计中结合Hebb规则的原则。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ed4546ae48fcdb250ad9fe7a9d38b124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Ur4h5AUP-Sk1ZvZ0W5uuA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://www.samwoolfe.com/2014/01/hebbian-theory-personality-change-over-time.html" rel="noopener ugc nofollow" target="_blank">https://www . samwoolfe . com/2014/01/heb bian-theory-personality-change-over-time . html</a></figcaption></figure><blockquote class="li lj lk"><p id="0a15" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过10万名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mi mj gp gr mk ml"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab fo"><div class="mn ab mo cl cj mp"><h2 class="bd jd gy z fp mq fr fs mr fu fw jc bi translated">序列|克塞尼亚·塞梅诺娃|子堆栈</h2><div class="ms l"><h3 class="bd b gy z fp mq fr fs mr fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到102，000多人的信任…</h3></div><div class="mt l"><p class="bd b dl z fp mq fr fs mr fu fw dk translated">thesequence.substack.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz lb ml"/></div></div></a></div><p id="19d1" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">连接主义是认知科学的一个流派，旨在建立受人脑启发的人工智能(AI)系统。作为市场上最活跃的技术趋势之一，连接主义学派一直是最近深度学习和深度神经网络出现的背后原因。尽管最近取得了技术进步，但神经网络体系结构仅适用于高度专业化的任务，并且没有表现出与人类如何随着时间的推移建立知识的兼容性。几个月前，来自IBM <a class="ae lh" href="https://www.pnas.org/content/early/2019/03/27/1820458116" rel="noopener ugc nofollow" target="_blank">的一组研究人员发表了一篇新论文，提出了一种从神经科学模式中汲取灵感的学习方法，以改善深度神经网络</a>的学习过程。</p><p id="9fd8" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">人脑仍然是整个人工智能领域的最大灵感。帮助人类获取知识的神经生物学机制和认知模式在很大程度上仍然未知，但神经科学领域在过去十年中一直在这一领域取得稳步进展。今天，我们清楚地知道，知识是由不同组神经元之间的连接形成的，这些连接在其他认知模式中发挥作用，如记忆、直觉、规划和许多其他模式。虽然在概念上很简单，但这些模式不可能在神经网络中有效地重现。挑战的部分根源在于神经网络架构和人脑的基本学习模式之一之间的不匹配。</p><h1 id="cf0a" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">赫布规则与反向传播</h1><p id="4cb5" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">加拿大神经心理学家唐纳德·赫布在他1949年的著作《组织行为》中介绍了一种新理论，这种理论后来被称为赫布法则或细胞组装理论。赫布法则的官方假设如下:</p><p id="2dfa" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated"><em class="ln">“让我们假设一个回响活动(或“痕迹”)的持续或重复往往会诱发持久的细胞变化，增加其稳定性。……当细胞A的一个</em> <a class="ae lh" href="https://en.wikipedia.org/wiki/Axon" rel="noopener ugc nofollow" target="_blank"> <em class="ln">轴突</em> </a> <em class="ln">足够接近并反复或持续地参与激发细胞B时，一个或两个细胞中会发生一些生长过程或代谢变化，从而使A作为激发B的细胞之一的效率增加。”</em></p><p id="a506" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">神经科学界经常使用Hebb规则的一个更简单的版本:“一起放电的细胞连接在一起”。赫布规则是另一种重要的大脑模式的基础，这种模式被称为突触可塑性，本质上是指突触的强度根据它们的活动随着时间的推移而减弱。换句话说，经常活跃的神经连接往往会随着时间的推移变得比不活跃的神经连接更强。根据Hebbian理论，更强的突触连接构成了长期记忆和其他学习机制的基础。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/db6561df979c6433c201d772e0e3c947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T5eiGqfT8n_NOiGxStZBlw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:IBM Research</figcaption></figure><p id="571e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">赫布法则告诉我们，一些突触连接会随着时间的推移而加强，而另一些则不会。这个原理与深度神经网络架构的核心组件之一完全矛盾。<a class="ae lh" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播算法</a>通常用于计算神经网络中的梯度和权重，以改进学习模型。为了对给定的神经元执行有效的更新，反向传播不仅需要目标神经元及其连接的知识，还需要关于结构的更高层的知识，这些知识不能从特定神经元的活动中直接得知。从这个意义上说，反向传播依赖于自上而下的知识分布模型，这种模型与大脑的工作方式无关。其次，反向传播依赖于大量的标记数据来构建网络的初始组成，这也与大脑在任何学习活动中使用的无监督的、主要是观察的模型形成对比。如果人脑使用类似反向传播的算法，我们就不会有我们所知的记忆概念，因为为了形成联系，大脑必须预测未来的事件😉</p><p id="6385" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">将反向传播算法的数学模型与Hebb规则的生物学本质联系起来是深度神经网络中最近的研究领域。这是IBM太空工作的基本灵感。</p><h1 id="15b3" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">神经网络的赫布规则</h1><p id="d979" class="pw-post-body-paragraph ll lm it lo b lp nv kd lr ls nw kg lu na nx lx ly nb ny mb mc nc nz mf mg mh im bi translated">IBM研究的主要原则是设计一种深度神经网络架构，其中包括遵循类似Hebb规则的类似突触的机制。更具体地说，IBM将来自心理学家的想法作为基础，即突触功效的改变是学习的核心，并且生物学习的最重要方面是突触前细胞I和突触后细胞j的协调活动将产生它们之间突触Wij的突触功效的改变。利用这些想法，IBM提出了一种受生物启发的神经网络架构，其中较低层神经元的权重是通过其连接的活动来推断的。这种学习模型允许最初使用非监督技术训练神经网络，然后使用监督模型完成训练。</p><p id="9f3d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">在IBM的生物启发神经网络中，学习过程中突触强度的变化与突触前细胞的活动和突触后细胞活动的函数成比例。最初，使用该思想训练模型，直到计算出较低层的权重。此时，这些层被用作全连接感知器模型的输入，然后该模型与随机梯度下降(SGD)相结合来计算更高层的权重。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/5135099b948c249e079f5dda792d2e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQjpzHPrWbHqX5PD5ODcnw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:IBM Research</figcaption></figure><p id="6c99" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">将前面的数学模型放在神经网络的环境中，我们得到一个分成三个主要阶段的流水线。给定图像输入，无监督训练模型生成训练电流向量<em class="ln"> I{I1，I2，…，Iu} </em>。在这个阶段产生的激活被用于使用类似突触的机制来更新较低层的权重。此时，使用传统的监督学习和基于SGD的优化来完成神经网络的训练。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/5e2d9e2efe5ef83591282460041b0f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*18gale5xoSd1Scgdoi-8bg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:IBM Research</figcaption></figure><p id="41bc" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">IBM的生物启发神经网络与传统模型相比有两个关键优势:</p><p id="b88d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">1)训练的第一部分是完全无人监督的，并且不需要大量的标记数据。</p><p id="eb03" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">2)仅基于局部活动来推断较低隐藏层的权重，而不需要昂贵的反向传播技术。</p><p id="ec5f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">提出的模型的一个主要缺点是执行。首先，它是一个在线算法，因此一次只显示一个训练示例，这与SGD不同，SGD可以小批量显示训练示例。第二，对于任何训练示例，必须等到隐藏单元集达到稳定状态。</p><p id="ecda" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">为了评估新模型的性能，IBM创建了两个神经网络。第一个是使用两阶段过程训练的:第一层的建议的“生物”训练，随后是顶层中分类器的标准梯度下降训练。第二个是在监督任务上用反向传播算法进行端到端训练。这两个神经网络都已经在MNIST和CIFAR-10数据集上进行了训练。在这两种情况下，受生物启发的模型用一小部分训练数据集实现了与传统反向传播神经网络相当的性能水平。在MNIST的例子中，用反向传播算法端到端训练的网络证明了众所周知的基准:训练误差= 0%，测试误差= 1.5%。以“生物”方式训练的网络在训练集上达到0.4%的误差。因此，它永远不会完美地符合训练数据。同时，在保留测试集上的误差为1.46%，与端到端训练的网络的误差相同。这是令人惊讶的，因为生物启发的模型学习第一层的权重，而不知道这些权重将用于什么任务，这与端到端训练的网络不同。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a2f572dbab492997dcf0c0ebee6ed10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zf1W62yySX3SsXyXfWg9mg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:IBM Research</figcaption></figure><p id="91d9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu na lw lx ly nb ma mb mc nc me mf mg mh im bi translated">与研究论文一起，IBM <a class="ae lh" href="https://github.com/DimaKrotov/Biological_Learning" rel="noopener ugc nofollow" target="_blank">在GitHub </a>上发表了他们生物神经网络的实现。当前的实现局限于非常具体的图像分析场景，但许多核心原则可以外推至其他深度学习模型。无监督和监督学习以及突触样权重计算的结合当然是IBM模型的一个超级创新的想法。我们真的不知道大脑的哪些认知模式可以适用于深度神经网络，但对这些想法的探索很可能是未来几年深度学习研究的前沿。</p></div></div>    
</body>
</html>