# NLP 新闻密码| 09.13.20

> 原文：<https://pub.towardsai.net/nlp-news-cypher-09-13-20-a898b9988376?source=collection_archive---------2----------------------->

![](img/476d7a8478f80825a1dc269ca1f860d6.png)

[*第九波*](https://en.wikipedia.org/wiki/The_Ninth_Wave) (1850)伊凡·艾瓦佐夫斯基

## 自然语言处理每周时事通讯

## Aere Perrenius

欢迎回来。希望你喜欢你的一周！我们有另一个更新是在这个星期五。增加了另外 11 个[数据集](https://datasets.quantumstat.com)和 5 个新的[笔记本](https://notebooks.quantumstat.com)。感谢所有为上周更新做出贡献的人！Won Ik Cho，Gayatri Venugopal，Moin Nadeem Himanshu Chaudhary，胡炜昇，Prafull Sharma，Yeshwanth Reddy & Manu Romero！

**如果你想冒险的话**:Lex frid man 的真相就在那里，他采访了 F/A-18 战斗机飞行员 Fravor 中校，他于 2004 年在南加州海岸遭遇了一架 UFO，俗称“尼米兹事件”。👽

解密的

**过去的爆炸**:查看谷歌介绍变形金刚模型的旧博文(2017)。很想看看在过去的几年里，自然语言处理领域取得了多大的进步。😊

[](https://ai.googleblog.com/2017/08/) [## 谷歌人工智能博客

### 机器学习(ML)是谷歌的一个关键战略重点，高度活跃的团队几乎在所有领域都从事研究

ai.googleblog.com](https://ai.googleblog.com/2017/08/) 

**匿名信:**前 Salesforce 首席安全官理查德·索彻(Richard Socher)最近离开公司，开始了自己的创业，在撰写本文时，他仍在秘密进行。虽然对这家初创公司知之甚少，但他似乎希望通过纠正错误信息，让互联网成为一个更安全的地方。他的公司正在招聘精选职位，如有兴趣，请申请:

 [## 关于我们

### 互联网坏了。是的，我们获得的信息比以往任何时候都多，但太多时候，仇恨和…

su-sea.github.io](https://su-sea.github.io/jobs/) 

**信息安全新闻:**使用 USB 保持移动和便携。点击此处了解更多关于 Tails 操作系统的信息:

[](https://tails.boum.org/) [## 尾巴-回家

### Tails 使用 Tor 网络来保护您的在线隐私，并帮助您避免审查。像它一样享受互联网…

tails.boum.org](https://tails.boum.org/) 

**借壳**:[https://littlealchemy2.com/](https://littlealchemy2.com/)

# 本周

> 卷起
> 
> 删除零
> 
> 韩国 ASR 图书馆
> 
> 应用 NLPers 的数据准备情况
> 
> PyTorch 和 KGEs
> 
> 荣誉奖*🙉*
> 
> 本周数据集:立体数据集

# 卷起

你可能已经经历过了，你的下一个 NLP 项目可能需要你从事知识密集型的工作，比如开放领域的问答或者事实核查。对这些知识密集型任务进行基准测试可能很困难，因为这些任务需要大量的知识来源(当您有各种知识来源时，事情会变得更加困难)。因此，脸书人工智能的一个新基准为研究人员提供了一个集中的基线，以开始他们的研究，并为这些艰巨的任务测试模型性能，它被称为 KILT。它利用了基于单一知识来源的跨任务界面:包含 590 万篇文章的 2019/08/01 维基百科快照。以下是您将在 KILT 中处理的任务:事实检查、开放域问答、槽填充、实体链接和对话。

下面是每个维基记录的样子:

```
{
 'wikipedia_title': 'Email marketing',
 'wikipedia_id': 1101759, 
 'text': ['p1', 'p2',...., 'pn'], **# list of paragraph text**
 'anchors': [{"text":,"href":,"paragraph_id":,"start":,"end":} ]  , 
 'categories': 'comma separated list of categories'
 'history': **# some info from wikipedia, including original url**
 'wikidata_info': **# wikidata info**
 }
```

**GitHub** :

[](https://github.com/facebookresearch/KILT) [## Facebook 研究/苏格兰裙

### 在下面的文章中描述了短裙基准:https://arxiv.org/abs/2009.02252 康达创建-n 短裙 37 -y…

github.com](https://github.com/facebookresearch/KILT) 

【https://arxiv.org/pdf/2009.02252.pdf】纸 : [纸](https://arxiv.org/pdf/2009.02252.pdf)

# 删除零

我们的神经网络模型是密集的野兽，热爱线性代数(又名矩阵乘法)。但是这种密度不是很有效。我们实际上可以通过去掉这些零来删除矩阵中的空间，而不会损失性能。在这个问题上，我不想赘述我的天真，以下是弗朗索瓦·拉古纳斯从他之前的[博客](https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70)帖子中的直观感受:

> 矩阵的 ***稀疏度*** 是零相对于矩阵大小的分数
> 
> **利弊？**如果你有很多零，你就不用计算一些乘法，也不用存储。所以你 ***可能*** 在体型和速度上有所增益，用于训练和推断。
> 
> **弊？**当然，拥有所有这些零可能会对网络精度/性能产生影响。但是到什么程度呢？你可能会感到惊讶。"

仅供参考，关于稀疏性的更深入的讨论/历史，你可以在这里查看 HF 的新博客[。最酷的是，你可以用一个新的拥抱脸笔记本开始你的稀疏冒险，它有助于用稀疏块取代线性块！执行起来相当简单。更多的直觉，请查看下面他们的笔记本。](https://huggingface.co/blog/pytorch_block_sparse)

[](https://github.com/huggingface/pytorch_block_sparse) [## hugging face/py torch _ block _ sparse

### 这个 PyTorch 扩展使用块稀疏矩阵代替密集矩阵，为 torch.nn.Linear 提供了一个替代方案

github.com](https://github.com/huggingface/pytorch_block_sparse) 

**笔记本(6 隐藏层 RoBERTa)** :

> 请记住，这是在培训之前配置模型，(请注意，他们没有调用任何检查点)
> 
> 哦，你还需要一个图形处理器*🤭*

[](https://github.com/huggingface/pytorch_block_sparse/blob/master/doc/notebooks/ModelSparsification.ipynb) [## hugging face/py torch _ block _ sparse

### permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…

github.com](https://github.com/huggingface/pytorch_block_sparse/blob/master/doc/notebooks/ModelSparsification.ipynb) 

**笔记本**:

> 用于初始化数据集、标记器和训练稀疏语言模型的实现。

[](https://github.com/huggingface/pytorch_block_sparse/blob/master/doc/notebooks/01_how_to_train_sparse/01_how_to_train_sparse.ipynb) [## hugging face/py torch _ block _ sparse

### permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…

github.com](https://github.com/huggingface/pytorch_block_sparse/blob/master/doc/notebooks/01_how_to_train_sparse/01_how_to_train_sparse.ipynb) 

# 韩国 ASR 图书馆

一个新的朝鲜语语音识别库出来了，它叫做 KoSpeech，基于 PyTorch。它们还包括 KsponSpeech 语料库的预处理方法和基线模型。😎

**GitHub (** 年度候选人简介图 **)** :

[](https://github.com/sooftware/KoSpeech/) [## 软件/KoSpeech

### 1Kakao Brain 2 光光电子信息技术大学 3 光光信息大学…

github.com](https://github.com/sooftware/KoSpeech/) 

# 应用 NLPers 的数据准备情况

在 NLP 中认真对待数据？我们经常会忽略在项目开始前处理数据时可能出现的棘手问题。因此，瑞典 RISE 公司的人们为那些在企业/机构中应用 NLP 的人写了一份有趣的关于数据准备的白皮书。这里有一个关于利益相关者应该关注可访问性的简明例子:

> **数据存在吗？**是否记录了完成任务所需的数据？
> 
> **数据转换和编码。**NLP 面临的主要挑战之一是将文档从源格式(如 PDF、Word 或 Excel)转换为适合处理手头任务的格式。为了超越 C 带，必须进行数据转换和编码。
> 
> **无障碍的法律方面**。不仅数据应该对预期的团队可用，而且团队和他们为手头的任务产生解决方案的努力结果也应该在访问和处理数据的法律方面得到澄清。例如，这包括个人身份信息的处理和版权问题。

[链接](https://arxiv.org/pdf/2009.02043.pdf)

# PyTorch 和 KGEs

今年将出版的第一百万个 PyTorch 图书馆😭😭。如果你对链接预测感兴趣，你可以在这里查看他们的库:

**GitHub**

[](https://github.com/torchkge-team/torchkge) [## 火炬队/火炬队

### TorchKGE:嵌入 Python 和 Pytorch 的知识图。TorchKGE 是一个用于知识图(KG)的 Python 模块…

github.com](https://github.com/torchkge-team/torchkge) 

# 荣誉奖🙉

## 多模态机器翻译；

[](https://github.com/DeepLearnXMU/MM-DCCN) [## DeepLearnXMU/MM-DCCN

### Pytorch 中“用于多模态机器翻译的动态上下文引导胶囊网络”的代码。这个项目是…

github.com](https://github.com/DeepLearnXMU/MM-DCCN) 

**论文**:【https://arxiv.org/pdf/2009.02016.pdf】T2

## 填写空白的 LM

[](https://ai.stanford.edu/blog/infilling-by-language-modeling/) [## 如何用语言模型填空

### 当编辑或修改时，我们经常以非线性的方式写作。写一封电子邮件现有的系统可能会建议…

ai.stanford.edu](https://ai.stanford.edu/blog/infilling-by-language-modeling/) 

## 如果您有内存问题，请参阅批量大小故障排除教程

[](https://medium.com/ai2-blog/tutorial-training-on-larger-batches-with-less-memory-in-allennlp-1cd2047d92ad) [## 教程:在 AllenNLP 中用较少的内存进行较大批量的训练

### 这是一系列迷你教程的一部分，帮助您了解 AllenNLP 库的各个方面。

medium.com](https://medium.com/ai2-blog/tutorial-training-on-larger-batches-with-less-memory-in-allennlp-1cd2047d92ad) 

# 本周数据集:立体数据集

# 这是什么？

StereoSet 是一个测量语言模型中刻板印象偏差的数据集。它由 17，000 个句子组成，衡量不同性别、种族、宗教和职业的模型偏好。

# 样本:

[](https://stereoset.mit.edu/explore/dev/) [## 立体声系统

### 探索模型如何与 StereoSet 交互。

stereoset.mit.edu](https://stereoset.mit.edu/explore/dev/) 

# 它在哪里？

[](https://stereoset.mit.edu/) [## 立体声系统

### StereoSet 是一个测量语言模型中刻板印象偏差的数据集。StereoSet 由 17，000 个句子组成…

stereoset.mit.edu](https://stereoset.mit.edu/) 

> *每周日，我们都会对来自世界各地的研究人员的 NLP 新闻和代码进行每周综述。*
> 
> 如果您喜欢这篇文章，请帮助我们并与朋友分享！
> 
> *如需完整报道，请关注我们的 Twitter:*[*@ Quantum _ Stat*](http://twitter.com/Quantum_Stat)

![](img/d155ba0ea7fdb2367849a05b820e07ae.png)

[www.quantumstat.com](http://www.quantumstat.com/)