# 机器学习中的特征选择

> 原文：<https://pub.towardsai.net/feature-selection-in-machine-learning-3b2902852933?source=collection_archive---------2----------------------->

## [机器学习](https://towardsai.net/p/category/machine-learning)

![](img/207f09f384a08bc8df414cc8d803bbfe.png)

照片由[路易斯·汉瑟@shotsoflouis](https://unsplash.com/@louishansel?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

在现实世界中，数据并不像人们通常认为的那样干净。这就是所有数据挖掘和争论的来源；从使用查询构建的数据中构建洞察力，这些数据现在可能包含某些缺失值，并显示肉眼看不到的可能模式。这就是**机器学习的用武之地:检查模式，并利用这些模式，通过数据中这些新理解的关系来预测结果。**

为了理解算法的深度，需要通读数据中的变量，以及这些变量代表什么。理解这一点很重要，因为你需要基于你对数据的理解来证明你的结果。**如果你的数据包含五个甚至五十个变量，假设你能够遍历所有变量。但是如果它包含 200 个变量呢？你没有时间去研究每一个变量。**最重要的是，各种算法都不能处理分类数据，因此您必须将所有分类列转换为定量变量(它们看起来是定量的，但度量标准将证明它们是分类的)，以将它们推入模型。所以，这增加了数据中变量的数量，现在你有 500 个变量。你如何处理他们？你可能会马上认为降维就是答案。**降维算法会降低维度，但是可解释性不是很好。如果我告诉你，有其他的技术，可以消除特征，并且仍然很容易理解和解释保留的特征呢？**

取决于分析是基于回归还是基于分类，特征选择技术可以不同/变化，但是如何实现它的一般思想是相同的。

**这里有一些特征选择技术来解决这个问题:**

# 1.高度相关的变量

**彼此高度相关的变量为模型**提供相同的信息，因此没有必要将它们全部包括在我们的分析中。例如:**如果一个数据集包含一个特性“浏览时间”，另一个称为“浏览时使用的数据”，那么你可以想象这两个变量在某种程度上是相关的**，即使我们选择了一个无偏的数据样本，我们也会看到这种高度的相关性。在这种情况下，我们只需要这些变量中的一个作为模型中的预测值，因为**如果我们两个都用，那么模型将会过度拟合，并偏向这个特定的特征。**

![](img/f70c7f626eccf025a0ab69875d0c151e.png)

照片由 [Akin Cakiner](https://unsplash.com/@akin?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

# 2.p 值

在线性回归等算法中，**初始统计模型始终是一个好主意，因为它有助于使用使用该模型获得的 P 值**来可视化特征的重要性。在设置显著性水平时，我们检查获得的 P 值，如果该值小于显著性水平，则表明该特征是显著的，即该值的变化可能表明目标值的变化。

![](img/660fd70f4a68a195950f4abae769baa8.png)

Joshua Eckstein 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

# 3.预选

**前向选择是一种使用逐步回归的技术。**因此，模型从零开始构建，即一个空模型，然后每次迭代增加一个变量，使得正在构建的模型有所改进。**每次迭代中添加的变量使用其显著性**确定，并且可以使用各种度量进行计算，其中一个常见的度量是从使用所有变量构建的初始统计模型中获得的 P 值。**有时，前向选择会导致过度拟合，因为它会将高度相关的变量添加到模型中，即使它们向模型提供了相同的数据**(但模型显示出了改进)。

![](img/6dae429b595af5612aa384aeb85da21f.png)

照片由[edu·格兰德](https://unsplash.com/@edgr?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

# 4.反向消除

向后消除也涉及逐步特征选择，其方式与向前选择相反。在这种情况下，**初始模型从所有的独立变量开始，如果这些变量在每次迭代中没有为新形成的回归模型提供值，则一个接一个地消除这些变量(每次迭代一个)**。这也是，**基于使用初始统计模型获得的 P 值，并且基于这些 P 值，从模型中消除特征。**同样使用这种方法，在移除高度相关的变量时存在不确定性。

![](img/2dac0e19455e6aadc7735bc49589452a.png)

照片由[马库斯·斯皮斯克](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄

# 5.递归特征消除(RFE)

RFE 是一种广泛使用的技术/算法，用于选择精确数量的重要特征，有时用于解释影响业务的特定数量的“最重要”特征，有时作为一种方法，用于将非常多的特征(比如大约 200-400 个)减少到只对模型产生一点影响的特征，并消除其余的特征。 **RFE 使用基于等级的系统来显示数据集中要素的等级，这些等级用于根据要素之间存在的共线性以及这些要素在模型中的重要性来消除递归循环中的要素。**除了对特征进行排序之外，RFE 还可以显示这些特征是否重要，甚至对于选定数量的特征也是如此(因为我们选择的选定数量很可能不代表重要特征的最佳数量，并且最佳数量的特征可能多于或少于用户选择的数量)。

![](img/bd0208301d6e9b2d52bdba3db403393b.png)

安德鲁·希曼在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

# 6.图表特征重要性

当我们谈论机器学习算法的**可解释性时，我们通常会讨论线性回归**(因为我们可以使用 P 值来分析特征重要性)**和决策树**(它实际上以树的形式显示了特征重要性，也显示了重要性的层次结构)，但另一方面，**我们经常使用变量重要性图表来绘制变量和“它们的重要性量”，在诸如随机森林分类器、光梯度增强机和 XG Boost** 等算法中。当需要向正在被分析的业务呈现特征的结构良好的重要性时，这尤其有用。

![](img/746d6af29d510d26386df11217a4e3c3.png)

罗伯特·阿纳施在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

# 7.正规化

**正则化用于监控偏差和方差之间的权衡。偏差表明模型在训练数据集上过度拟合了多少。方差告诉我们在训练和测试数据集上做出的预测有多么不同。**理想情况下，偏差和方差都需要减少。正规化来拯救这里的一天！主要有两种类型的正则化技术:

**L1 正则化- Lasso:** **Lasso 对模型的 beta 系数进行惩罚，以改变它们在模型中的重要性，甚至可能将它们接地(将它们变成零，即基本上从最终模型中移除这些变量)。**一般来说，当您观察到您的数据集有大量变量，并且您需要移除其中一些变量以更好地理解重要特征如何影响您的模型时，就使用 Lasso(即 Lasso 最终选择的特征，并分配其重要性)。

**L2 正则化-岭:** **岭的作用是维护所有的变量，即使用所有的变量来建立模型，同时给它们分配重要性，使得模型性能有所提高。**当数据集中的变量数量较少时，岭是一个很好的选择，因此所有这些变量都需要用来解释所获得的洞察力和预测目标结果。

由于 Ridge 保持所有变量不变，而 Lasso 在赋予变量重要性方面做得更好，**两者的结合，被称为弹性网**，通过结合 Ridge 和 Lasso 的最佳特性，被开发为一种开发算法的方法。弹性网成为理想的选择。

![](img/6df6646a13f35967a085651c94711e40.png)

亨特·哈里特在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

在执行机器学习时，有更多的方法来选择特征，但**的基本思想通常保持不变:展示特征重要性，然后根据获得的“重要性”消除变量。**这里的重要性是一个非常主观的术语，因为它不是一个指标，而是一组指标和图表，可用于检查最重要的功能。

感谢您的阅读！快乐学习！