<html>
<head>
<title>Linear Models for Classification, Logistic Regression, with and without sklearn library</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类的线性模型，逻辑回归，有和没有sklearn库</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/linear-models-for-classification-logistic-regression-with-without-sklearn-library-6ec9a5556023?source=collection_archive---------1-----------------------#2021-07-30">https://pub.towardsai.net/linear-models-for-classification-logistic-regression-with-without-sklearn-library-6ec9a5556023?source=collection_archive---------1-----------------------#2021-07-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0142" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><p id="69d8" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">本文推导性地分解了逻辑回归的主题，逻辑回归是用于分类的线性模型。它解释了逻辑回归算法在数学上是如何工作的，它是如何用sklearn库实现的，最后它是如何在python中用数学方程而不用sklearn库实现的。此外，解释了线性模型的多类分类。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="91ad" class="lg lh it lc b gy li lj l lk ll">Table of Contents (TOC)<br/>---- Introduction<br/>---- Linear Models for Classification without Sklearn<br/>---- Linear Models for Classification with Sklearn<br/>-----Linear Models for Multiclass Classification</span></pre><figure class="kx ky kz la gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi lm"><img src="../Images/925cb2e87c5e555c81ba004d3577801b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fOPKByUdnnDJ0pxm"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated">由<a class="ae ly" href="https://unsplash.com/@roberto_sorin?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">罗伯特·索林</a>在<a class="ae ly" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="d2d4" class="lz lh it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">介绍</h1><p id="57ab" class="pw-post-body-paragraph jz ka it kb b kc mw ke kf kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw im bi translated">线性模型用于分类和回归。其中之一，<em class="nb">逻辑回归</em>，与其名称相反，用于二元分类。二元分类意味着数据集包括2个输出(类)。此外，逻辑回归是神经网络的基础部分。它通过更新用户设置的初始值来最小化每次迭代中的误差(成本)。图1显示了如何使用逻辑回归对具有4个特征和2个类的数据集进行分类的流程图。</p><figure class="kx ky kz la gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi nc"><img src="../Images/fc6dfd7038b423699fb2e0b3d92a0a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8lJ4Sb510MqviPmgEp72Zg.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated">图一。作者的逻辑回归图像流程图</figcaption></figure><ol class=""><li id="0051" class="nd ne it kb b kc kd kg kh kk nf ko ng ks nh kw ni nj nk nl bi translated">在逻辑回归的训练部分，为每个特征分配1个权重，并向系统添加1个偏差值。</li><li id="9bd3" class="nd ne it kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">这些权重和偏差值由用户选择的值初始化。</li><li id="a6d3" class="nd ne it kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">每个权重乘以自己的特征值，并通过添加偏差值来相加。</li><li id="59c3" class="nd ne it kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">用户选择的激活功能(图2)应用于和值。</li><li id="6eec" class="nd ne it kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">当我们假设它是一个sigmoid函数时，激活函数(sum)将在0–1之间，误差由该值计算。</li><li id="c95b" class="nd ne it kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">根据计算出的误差值，以学习率的速率用梯度下降法(点击<a class="ae ly" href="https://medium.com/geekculture/linear-regression-gradient-descent-model-regularization-fba0e2c1e4a7" rel="noopener">此处</a>)更新权重和偏差值。</li><li id="64f3" class="nd ne it kb b kc nm kg nn kk no ko np ks nq kw ni nj nk nl bi translated">这个过程重复迭代的次数。现在，让我们用代码在乳腺癌数据集上实现上述过程。</li></ol><figure class="kx ky kz la gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi nr"><img src="../Images/52af0a74cd7085654ce0341b2df4da12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LGWG1wvZf2c-5yOb76I8AQ.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated">图二。激活功能，<a class="ae ly" href="https://www.linkedin.com/pulse/activation-functions-neural-networks-leonardo-calderon-j-/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="5855" class="lz lh it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">无Sklearn库的线性分类模型</h1><p id="e5bc" class="pw-post-body-paragraph jz ka it kb b kc mw ke kf kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw im bi translated">导入数据集，用MinMaxScaler归一化特征值，用train_test_split分离数据。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="9caa" class="lg lh it lc b gy li lj l lk ll">IN[1]<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import load_breast_cancer<br/>data = load_breast_cancer()<br/>x=data.data<br/>y=data.target<br/>print("shape of data:",x.shape)<br/>from sklearn.preprocessing import MinMaxScaler<br/>scaler=MinMaxScaler()<br/>x_new=scaler.fit_transform(x)<br/><strong class="lc jd">OUT[1]<br/>shape of data: (569, 30)</strong></span><span id="ef6c" class="lg lh it lc b gy ns lj l lk ll">IN[2]<br/>from sklearn.model_selection import train_test_split<br/>x_train, x_test, y_train, y_test = train_test_split(x_new,y,test_size = 0.15,random_state=2021)<br/>x_train = x_train.T<br/>x_test = x_test.T<br/>y_train = y_train.T<br/>y_test = y_test.T</span></pre><p id="3dfc" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">初始值设定为权重为0.1，初始偏移为1。应用sigmoid函数的方程。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="d05a" class="lg lh it lc b gy li lj l lk ll">IN[3]<br/>def weights_bias(shape):<br/>    weights = np.full((shape,1),0.1)<br/>    bias = 1<br/>    return weights,bias<br/>IN[4]<br/>def sigmoid(z):  <br/>    y_predict = 1/(1+ np.exp(-z))<br/>    return y_predict</span></pre><p id="0565" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">正向传播和反向传播设计如下。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="5656" class="lg lh it lc b gy li lj l lk ll">IN[5]<br/>def forward_backward(w,b,x_train,y_train):<br/>    z = np.dot(w.T,x_train) + b<br/>    y_predict = sigmoid(z)<br/>    derivative_weight = (np.dot(x_train,((y_predict-y_train).T)))/x_train.shape[1]<br/>    derivative_bias = np.sum(y_predict-y_train)/x_train.shape[1]                 <br/>    gradients = {"derivative_weight": derivative_weight, "derivative_bias": derivative_bias}<br/>    return gradients</span></pre><p id="0197" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">初始值更新了学习率和迭代次数。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="de44" class="lg lh it lc b gy li lj l lk ll">IN[6]<br/>def update_parameters(w, b, x_train, y_train, learning_rate,iterations):<br/>    index = []<br/>    for i in range(iterations):<br/>        gradients = forward_backward(w,b,x_train,y_train)<br/>        w = w - learning_rate * gradients["derivative_weight"]<br/>        b = b - learning_rate * gradients["derivative_bias"]<br/>    parameters = {"weight": w,"bias": b}<br/>    return parameters, gradients</span></pre><p id="c027" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">该架构的最后一步是预测电流输入，如果sigmoid函数(sum)≤0.5，则属于0类，如果sigmoid函数(sum)&gt;0.5，则属于1类。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="9adb" class="lg lh it lc b gy li lj l lk ll">IN[7]<br/>def predict(w,b,x_test):<br/>    z = sigmoid(np.dot(w.T,x_test)+b)<br/>    y_prediction = np.zeros((1,x_test.shape[1]))<br/>    for i in range(z.shape[1]):<br/>        if z[0,i]&lt;= 0.5:<br/>            y_prediction[0,i] = 0<br/>        else:<br/>            y_prediction[0,i] = 1<br/>    return y_prediction</span></pre><p id="3dd9" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">现在，让我们将它们结合起来，用Learning_rate=0.1和迭代次数=100进行测试:</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="c363" class="lg lh it lc b gy li lj l lk ll">IN[8]<br/>def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  iterations):<br/>    shape =  x_train.shape[0]<br/>    w,b = weights_bias(shape)<br/>    parameters, gradients = update_parameters(w, b, x_train, y_train, learning_rate,iterations)<br/>    y_prediction_test = predict(parameters["weight"],parameters["bias"],x_test)<br/>    print("test accuracy: {}% ".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))<br/>    <br/>logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.1, iterations = 100)<br/><strong class="lc jd">OUT[8]<br/>test accuracy: 91.86046511627907%</strong></span></pre><blockquote class="nt nu nv"><p id="9a4e" class="jz ka nb kb b kc kd ke kf kg kh ki kj nw kl km kn nx kp kq kr ny kt ku kv kw im bi translated">初始值、学习率、迭代次数是项目的超参数。可以尝试其他配置，并且可以获得更高的精度。</p></blockquote><h1 id="8e27" class="lz lh it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">使用Sklearn库进行分类的线性模型</h1><p id="e746" class="pw-post-body-paragraph jz ka it kb b kc mw ke kf kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw im bi translated">现在让我们使用sklearn库来处理同一个数据集。当我们改变超参数C值时，可以看到它对模型精度的影响。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="1430" class="lg lh it lc b gy li lj l lk ll">IN[9]<br/>from sklearn.linear_model import LogisticRegression<br/>c_list=[0.001,0.01,0.1,1,10]<br/>for i in c_list:<br/>    lrc = LogisticRegression(C=i).fit(x_train.T,y_train.T)<br/>    lrc_test=lrc.score(x_test.T,y_test.T)<br/>    lrc_test=round(lrc_test*100,2)<br/>    print("C=",i,"test acc: ", lrc_test,"%")<br/><strong class="lc jd">OUT[9]<br/>C= 0.001 test acc:  63.95 %<br/>C= 0.01 test acc:  77.91 %<br/>C= 0.1 test acc:  93.02 %<br/>C= 1 test acc:  95.35 %<br/>C= 10 test acc:  98.84 %</strong></span></pre><p id="18f1" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">如图所示，当控制C值增加时，测试精度也增加。Sklearn中有许多用于逻辑回归的超参数，点击此<a class="ae ly" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">链接</a>即可访问所有超参数。</p><h1 id="327b" class="lz lh it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">多类分类</h1><p id="9902" class="pw-post-body-paragraph jz ka it kb b kc mw ke kf kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw im bi translated">提到了逻辑回归用于二元分类。那么超过2类的话怎么用logistic回归呢？在这里，我们遇到了基于一对其余原则的多类分类。对于每个类别，系数和偏差是通过将所有其他类别与它相对比而产生的。在预测阶段，它被放在最合适的类中。有了Sklearn库，我们可以很容易地实现它，如下所示。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="8d47" class="lg lh it lc b gy li lj l lk ll">IN[10]<br/>from sklearn.datasets import load_digits<br/>digits = load_digits()<br/>x_digit=digits.data<br/>y_digit=digits.target<br/>print("shape of data:",x_digit.shape)</span><span id="6fb6" class="lg lh it lc b gy ns lj l lk ll">from sklearn.preprocessing import MinMaxScaler<br/>scaler=MinMaxScaler()<br/>x_digit_new=scaler.fit_transform(x_digit)</span><span id="e782" class="lg lh it lc b gy ns lj l lk ll">from sklearn.model_selection import train_test_split<br/>x_digit_train, x_digit_test, y_digit_train, y_digit_test = train_test_split(x_digit_new,y_digit,test_size = 0.20,random_state=2021)<br/><strong class="lc jd">OUT[10]<br/>shape of data: (1797, 64)</strong></span><span id="4b74" class="lg lh it lc b gy ns lj l lk ll">IN[11]<br/>from sklearn.linear_model import LogisticRegression<br/>multiclass = LogisticRegression(multi_class='multinomial')<br/>multiclass.fit(x_digit_train,y_digit_train)<br/>multiclass_test=multiclass.score(x_digit_test,y_digit_test)<br/>multiclass_test=round(multiclass_test*100,2)<br/>print("test acc: ", multiclass_test,"%")<br/><strong class="lc jd">OUT[11]<br/>test acc:  95.28 %</strong></span></pre><h2 id="f61b" class="lg lh it bd ma nz oa dn me ob oc dp mi kk od oe mm ko of og mq ks oh oi mu iz bi translated">回到指引点击<a class="ae ly" href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener">这里</a>。</h2><div class="oj ok gp gr ol om"><a href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener follow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">机器学习指南</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">本文旨在准备一个机器学习数据库，以便在一个视图中显示所有的机器学习标题。这个…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">ibrahimkovan.medium.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa ls om"/></div></div></a></div></div></div>    
</body>
</html>