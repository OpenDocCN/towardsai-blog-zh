<html>
<head>
<title>The AI-Powered Online Fitting Room: VOGUE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能驱动的在线试衣间:VOGUE</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-ai-powered-online-fitting-room-vogue-5f77c599832?source=collection_archive---------1-----------------------#2021-01-23">https://pub.towardsai.net/the-ai-powered-online-fitting-room-vogue-5f77c599832?source=collection_archive---------1-----------------------#2021-01-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="dcd6" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="47c5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">谷歌使用修改后的StyleGAN2架构创建了一个在线试衣间，在这里你可以只使用自己的图像自动试穿任何你想要的裤子或衬衫。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/54661aed448df66c456600fa3601ef04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y7jShQVd74PBinR6lAO-2A.png"/></div></div></figure><h2 id="e82a" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">时尚:StyleGAN插值优化的尝试[1]</h2><p id="8eca" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">来自谷歌、麻省理工学院和华盛顿大学的一组研究人员最近发表了一篇名为“VOGUE: Try-On by StyleGAN插值优化”的论文。他们使用GAN架构创建了一个在线试衣间，在这里你可以只使用自己的图像自动试穿任何你想要的裤子或衬衫。也称为服装转移，目标是将照片中一个人的衣服转移到另一个人身上，同时保留正确的体形、头发和肤色。这是一项复杂的任务，因为需要从一幅图像中提取输出图像的某些部分，如衣服，而从另一幅图像中提取真实人物的其他部分，以保持我们想要试穿衣服的人的“身份”。</p><p id="495c" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">嗯，他们能够使用基于GAN的架构做到这一点。更准确地说，姿势控制的StyleGAN2是其架构的核心。我不会深入讨论这个StyleGAN2和GAN架构的细节，因为我已经在许多视频中解释过了，比如在这个视频中，我解释了Toonify，它也使用了基于StyleGAN2的架构。如果你不熟悉GANs或StyleGAN2，我肯定会邀请你在继续这个视频之前观看这个视频。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h2 id="556c" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">模型</h2><div class="ks kt ku kv gt ab cb"><figure class="my kw mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/98aa1651e222e470bb29a06b04a03e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*wscnS_Mo4n8MCreouNha4A.png"/></div></figure><figure class="my kw ne na nb nc nd paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/6ba1dab8a4cd5d889d9db64d756e192d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*f3-fnTM110MsAc4MVHGfJw.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk nj di nk nl translated">StyleGAN2和VOGUE版StyleGAN2的区别。经Kathleen Lewis许可使用[1]</figcaption></figure></div><p id="8003" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">因此，为了用不同的服装创作出逼真的照片,《VOGUE》需要训练这种受姿势限制的风格架构。但这比简单地实现StyleGAN2更难，因为它主要是为面部图像开发的，这也是它受欢迎的原因。他们必须做两个关键的修改:首先，他们必须用一个编码器修改生成器的开头，该编码器将图像的姿态关键点作为输入。这用作StyleGAN2的第一个“4x4样式块”的输入，而不是实现该姿势条件的常量输入。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/b310c77c75a8963b4e0b71cead0ca33a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oc0NQCzyndSHAxsIrVX-yA.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk translated">VOGUE的StyleGAN2模型生成的细分图[1]</figcaption></figure><p id="395e" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">然后，他们训练他们的StyleGAN2输出除RGB图像之外的每个分辨率的分割，正如你在这里看到的。使用这个网络，他们能够生成许多图像，并以期望的姿势对它们进行分割。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/50af03db7704850e0c32d473315245cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KRsxAf_WmTdnE6EilwYpGA.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk translated">VOGUE的模特。经Kathleen Lewis许可使用[1]</figcaption></figure><p id="97ee" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">接下来，给定一对输入图像，他们可以将图像“投影”到生成器的潜在空间中，以计算潜在代码，该潜在代码将最好地区分这对输入图像的特征。使用优化器来找到第二图像中的服装和第一图像中的人所处的组合空间。他们必须最大化感兴趣区域内的变化，同时最小化感兴趣区域外的变化。为此，他们使用两个潜在空间，代表两个输入图像，第一个来自要生成的人的图像，第二个来自要转移的衣服的图像。正如我们看到的，他们还需要姿势热图作为StyleGAN2生成器的输入，这里再次以灰色显示。然后，他们可以访问由经过训练的GAN架构生成的分割和图像。接下来，他们使用了一个由三个独立项组成的损失函数，每个项都优化了生成图像的一部分。</p><p id="18be" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">存在编辑定位损失项，该损失项鼓励网络使用分割输出仅在感兴趣的区域(这里定义为M)内插入样式。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/673647f1ddc5b45ae3e5f6dd2aeaedbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*nJkD2dAdExpq9ETv5BkD7Q.png"/></div></figure><p id="311f" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">然后，有服装损失用于转移服装的正确形状和纹理。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e7f773170894c6ccbea2472f2aa76937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*izDTC9phfVS-X19nMnf4aA.png"/></div></figure><p id="1d49" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">使用一种非常流行的卷积神经网络架构VGG-16的嵌入，他们再次使用分割标签来计算两幅图像的服装区域之间的距离。然后，将创建的蒙版应用于生成的RGB图像。</p><p id="b530" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">最后，身份丢失引导网络，正如它所说的，保存人的身份。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c32b6a87be4e6814d8e332c61621a3e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*rf1_OlAX5T2ydsC_Wix6xw.png"/></div></figure><p id="a01f" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">这也是使用分段标签按照与服装损失相同的程序来完成的。</p><p id="2ceb" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">请花点时间看看这些损耗如何影响输出图像。你可以清楚地看到什么时候本土化缺失或者身份缺失，以及它们的重要性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/3aa4b59ebab9ac4de95a8db0aaa495fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8faMWems7TT0EQTGkG7qtw.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk translated">损耗对服装转移效果的影响。经Kathleen Lewis许可使用[1]</figcaption></figure><h2 id="3fb0" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">更多结果</h2><p id="db7e" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">正如他们所说:“我们的方法可以通过固定风格向量来合成不同姿势和体型的同一风格的衬衫。我们以多种姿势呈现几种不同的风格。”[1]</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/74c9c4e31a7a1870aab5309103be6318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*C8qbLbJAKuLxtYvjbRHv6w.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk translated">在这个图中，每一行都是固定的样式，每一列都是固定的姿势和身体形状。经Kathleen Lewis许可使用[1]</figcaption></figure><p id="7692" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">看看这种新方法的效果有多好:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/512966fbc72e3fab32a9f6a38e9c4951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*twiLo0SmEH2tUILmG38WLw.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk translated">VOGUE的定性比较。经Kathleen Lewis许可使用[1]</figcaption></figure><h2 id="ff3f" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">甚至更多的结果！</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="f732" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">当然，这只是这篇新论文的概述。我强烈邀请您阅读下面参考资料中他们的论文，以获得更好的技术理解。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="b9a1" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">如果你喜欢我的工作，并想了解最新的人工智能技术，你绝对应该在我的社交媒体频道上关注我。</p><ul class=""><li id="4c1e" class="ob oc it ma b mb mr me ms lm od lq oe lu of mq og oh oi oj bi translated">订阅我的<a class="ae ok" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd"> YouTube频道</strong> </a>。</li><li id="4139" class="ob oc it ma b mb ol me om lm on lq oo lu op mq og oh oi oj bi translated">关注我的项目上<a class="ae ok" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd"> LinkedIn </strong> </a> <strong class="ma jd"> </strong>和这里上<strong class="ma jd"> </strong> <a class="ae ok" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="ma jd">中</strong> </a> <strong class="ma jd">。</strong></li><li id="10a6" class="ob oc it ma b mb ol me om lm on lq oo lu op mq og oh oi oj bi translated">一起学习AI，加入我们的<a class="ae ok" href="https://discord.gg/learnaitogether" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd">不和谐社区</strong> </a>，<em class="oq">分享你的项目、论文、最佳课程，寻找Kaggle队友，等等！</em></li></ul></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h2 id="e825" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">参考</h2><p id="16ee" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">[1] Lewis，Kathleen M等，(2021)，VOGUE:通过StyleGAN插值优化试穿，<a class="ae ok" href="https://vogue-try-on.github.io/" rel="noopener ugc nofollow" target="_blank">https://vogue-try-on.github.io/</a></p><p id="8235" class="pw-post-body-paragraph ly lz it ma b mb mr kd md me ms kg mg lm mt mi mj lq mu ml mm lu mv mo mp mq im bi translated">互动示例:<a class="ae ok" href="https://vogue-try-on.github.io/demo_rewrite.html" rel="noopener ugc nofollow" target="_blank">https://vogue-try-on.github.io/demo_rewrite.html</a></p></div></div>    
</body>
</html>