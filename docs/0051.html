<html>
<head>
<title>Naive Bayes(NB)-Support Vector Machine(SVM): Art Of State Result Hands-On Guide using Fast.ai</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯(NB)-支持向量机(SVM):使用Fast.ai的最新结果实践指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/naive-bayes-support-vector-machine-svm-art-of-state-results-hands-on-guide-using-fast-ai-13b5d9bea3b2?source=collection_archive---------0-----------------------#2019-05-21">https://pub.towardsai.net/naive-bayes-support-vector-machine-svm-art-of-state-results-hands-on-guide-using-fast-ai-13b5d9bea3b2?source=collection_archive---------0-----------------------#2019-05-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="24dc" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">融合NB和SVM | <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向AI </a></h2><div class=""/><div class=""><h2 id="51b9" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">具有NB特征的SVM:最先进的性能模型差异</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/8441eab540655654e6cedfa1e40849db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*2kivDB1ZzCg61Gy_7YyMiw.png"/></div></figure><p id="8679" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">在进入NB和SVM的模型变体之前，将讨论NB什么时候表现得比SVM更好，这解释了使用NBSVM的原因</strong></p><p id="5edd" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NB和SVM有不同的选项，包括为每一个选择核函数。<strong class="ky ja">它们都对参数优化敏感(即不同的参数选择会显著改变它们的输出)。</strong>所以，如果你有一个结果显示NB比SVM表现更好。这只适用于选定的参数。然而，对于另一个参数选择，你可能会发现SVM表现更好。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/a5c450d54753eb9cfcd9c24f8cd17bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*X7Dtrbk1dgZYl2ao5_nDRg.png"/></div></figure><p id="eb00" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NB和SVM的变体通常被用作文本分类的基线方法，但是它们的性能根据模型变体、所使用的特征和任务/数据集而有很大不同。基于这些观察，您可以确定简单的NB和SVM变体，它们在文本数据集上的表现优于大多数已发布的结果，有时会提供新的最先进的性能水平。</p><h1 id="8893" class="lt lu iq bd lv lw lx ly lz ma mb mc md kf me kg mf ki mg kj mh kl mi km mj mk bi translated"><strong class="ak">现在，讨论NBSVM背后的数学原理，它是如何推导出来的。</strong></h1><p id="f234" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">设f (i) ∈ R |V |是标签为y(I)∈{-1，1}的训练情况I的特征计数向量。v是特征的集合，f (i) j表示特征Vj在训练案例I中的出现次数</p><p id="4fe1" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计数向量公式为P = α +总和(i:y (i))=1。</p><p id="cec7" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">f (i)和Q = α +总和(I:y(I))= 1。这里f (i)用于平滑参数α。</p><p id="a426" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对数计数比是:r = log (p/||p||1 ) /(q/||q||1)</p><p id="81d7" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">多项朴素贝叶斯(MNB) </strong></p><p id="5bb8" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在MNB，x (k) = f (k)，w = r，b = log(N+/N)。N+和N-是正训练案例和负训练案例的数量。</p><p id="6ac1" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，发现二值化f (k)更好。让我们取x(k)=ˇf(k)= 1 { f(k)&gt; 0 }，其中1是指示函数。ˇp，ˇq，ˇr是用ˇf (i)而不是f(I)计算的</p><p id="abf2" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">支持向量机(SVM) </strong></p><p id="7c05" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于SVM，x(k)= f(k)和w，b通过最小化得到</p><p id="1e63" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">wT w + C总和i ( max(0，1y(I)(wTˇf(I)+b))2)</p><p id="c973" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里L2正则化的L2损失SVM是用来工作最好的和L1损失SVM不太稳定。</p><h1 id="edd0" class="lt lu iq bd lv lw lx ly lz ma mb mc md kf me kg mf ki mg kj mh kl mi km mj mk bi translated"><strong class="ak">具有NB特征的SVM(nbs VM)</strong></h1><p id="822c" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">除了使用x(k)= &gt; f(k)之外，其他方面与SVM相同，</p><p id="01f3" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中f(k)= r * f(k)是元素间的乘积。虽然这对于长文档非常有效，但现在您可以发现，MNB和SVM之间的插值对于所有文档都表现出色，并使用此模型找到结果:</p><p id="7395" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">w0 =(1β)w+βw(4)其中w = ||w||1/|V |</p><p id="7433" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以上是w的平均星等，β ∈ [0，1]是插值参数。这种插值可以看作是正则化的一种形式:信任NB，除非SVM非常有信心。</p><p id="00b7" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们开始实现，这里我使用IMDB数据集fast.ai NVSVM将电影评论分为正面和负面类别</p><p id="dd0f" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">第一步:</strong></p><p id="2ef2" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">标记化和术语文档矩阵创建</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="d843" class="mv lu iq mr b gy mw mx l my mz">PATH='data/aclImdb/'<br/>names = ['neg','pos']</span><span id="2cbf" class="mv lu iq mr b gy na mx l my mz">%ls {PATH}</span><span id="0a40" class="mv lu iq mr b gy na mx l my mz">aclImdb_v1.tar.gz  imdbEr.txt  imdb.vocab  models/  README  test/  tmp/  train/</span><span id="208d" class="mv lu iq mr b gy na mx l my mz">%ls {PATH}train</span><span id="955a" class="mv lu iq mr b gy na mx l my mz">aclImdb/  all_val/         neg/  tmp/    unsupBow.feat  urls_pos.txt<br/>all/      labeledBow.feat  pos/  unsup/  urls_neg.txt   urls_unsup.txt</span><span id="ac00" class="mv lu iq mr b gy na mx l my mz">%ls {PATH}train/pos | head</span><span id="601c" class="mv lu iq mr b gy na mx l my mz">trn,trn_y = texts_labels_from_folders(f'<strong class="mr ja">{PATH}</strong>train',names)<br/>val,val_y = texts_labels_from_folders(f'<strong class="mr ja">{PATH}</strong>test',names)</span></pre><p id="9d83" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是第一次审查的文本</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="82b4" class="mv lu iq mr b gy mw mx l my mz">trn[0]</span><span id="7b84" class="mv lu iq mr b gy na mx l my mz">" A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly."</span><span id="d7f8" class="mv lu iq mr b gy na mx l my mz">trn_y[0]</span><span id="954b" class="mv lu iq mr b gy na mx l my mz">0</span></pre><p id="429b" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">第二步:</strong></p><p id="9599" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nb nc nd mr b"><a class="ae ne" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank"><strong class="ky ja">CountVectorizer</strong></a></code>将一组文本文档转换成一个令牌计数矩阵(是<code class="fe nb nc nd mr b"><strong class="ky ja">sklearn.feature_extraction.text</strong></code>的一部分)。</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="b2e7" class="mv lu iq mr b gy mw mx l my mz">veczr = CountVectorizer(tokenizer=tokenize)</span></pre><p id="bd28" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nb nc nd mr b">fit_transform(trn)</code>在训练集中查找词汇。它还将训练集转换为术语-文档矩阵。现在让我们将<em class="nf">相同的转换</em>应用到验证集，第二行只使用了方法<code class="fe nb nc nd mr b">transform(val)</code>。<code class="fe nb nc nd mr b">trn_term_doc</code>和<code class="fe nb nc nd mr b">val_term_doc</code>是稀疏矩阵。<code class="fe nb nc nd mr b">trn_term_doc[i]</code>表示训练文档I，并且它包含词汇表中每个单词的每个文档的单词计数。</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="63b8" class="mv lu iq mr b gy mw mx l my mz">trn_term_doc = veczr.fit_transform(trn)<br/>val_term_doc = veczr.transform(val)<br/>trn_term_doc</span><span id="5316" class="mv lu iq mr b gy na mx l my mz">&lt;25000x75132 sparse matrix of type '&lt;class 'numpy.int64'&gt;'<br/>	with 3749745 stored elements in Compressed Sparse Row format&gt;</span><span id="5c6a" class="mv lu iq mr b gy na mx l my mz">trn_term_doc[0]</span><span id="7703" class="mv lu iq mr b gy na mx l my mz">&lt;1x75132 sparse matrix of type '&lt;class 'numpy.int64'&gt;'<br/>	with 93 stored elements in Compressed Sparse Row format&gt;</span><span id="22b1" class="mv lu iq mr b gy na mx l my mz">vocab = veczr.get_feature_names(); vocab[5000:5005]</span><span id="cb5c" class="mv lu iq mr b gy na mx l my mz">['aussie', 'aussies', 'austen', 'austeniana', 'austens']</span><span id="cfbb" class="mv lu iq mr b gy na mx l my mz">w0 = set([o.lower() <strong class="mr ja">for</strong> o <strong class="mr ja">in</strong> trn[0].split(' ')]); w0</span><span id="1a10" class="mv lu iq mr b gy na mx l my mz">len(w0)</span></pre><p id="7c6b" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">92</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="53c5" class="mv lu iq mr b gy mw mx l my mz">veczr.vocabulary_['absurd']</span></pre><p id="fe0d" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">1297</p><p id="da11" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">第三步:</strong></p><p id="a6d2" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">现在定义朴素贝叶斯</strong></p><p id="d34d" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在此定义每个单词的<strong class="ky ja">对数计数率</strong>，</p><p id="302d" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">$ r = \ log \ frac { \ text { feature $ f $在正文档中的比率} } { \ text { feature $ f $在负文档中的比率}}$</p><p id="0247" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中肯定文档中特征的比率是肯定文档具有特征的次数除以肯定文档的数量。</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="f0ff" class="mv lu iq mr b gy mw mx l my mz"><strong class="mr ja">def</strong> pr(y_i):<br/>    p = x[y==y_i].sum(0)<br/>    <strong class="mr ja">return</strong> (p+1) / ((y==y_i).sum()+1)</span><span id="1d57" class="mv lu iq mr b gy na mx l my mz">x=trn_term_doc<br/>y=trn_y</span><span id="9851" class="mv lu iq mr b gy na mx l my mz">r = np.log(pr(1)/pr(0))<br/>b = np.log((y==1).mean() / (y==0).mean())</span></pre><p id="b14d" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是朴素贝叶斯的公式。</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="11d4" class="mv lu iq mr b gy mw mx l my mz">pre_preds = val_term_doc @ r.T + b<br/>preds = pre_preds.T&gt;0<br/>(preds==val_y).mean()</span><span id="2d8d" class="mv lu iq mr b gy na mx l my mz">0.80691999999999997</span></pre><p id="66e8" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">二值化的朴素贝叶斯。</strong></p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="5e4c" class="mv lu iq mr b gy mw mx l my mz">x=trn_term_doc.sign()<br/>r = np.log(pr(1)/pr(0))</span><span id="4e59" class="mv lu iq mr b gy na mx l my mz">pre_preds = val_term_doc.sign() @ r.T + b<br/>preds = pre_preds.T&gt;0<br/>(preds==val_y).mean()</span><span id="2411" class="mv lu iq mr b gy na mx l my mz">0.83016000000000001</span></pre><h1 id="12fb" class="lt lu iq bd lv lw lx ly lz ma mb mc md kf me kg mf ki mg kj mh kl mi km mj mk bi translated">第四步:逻辑回归</h1><p id="66af" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">此处拟合逻辑回归，其中特征是单字。</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="4ce5" class="mv lu iq mr b gy mw mx l my mz">m = LogisticRegression(C=1e8, dual=<strong class="mr ja">True</strong>)<br/>m.fit(x, y)<br/>preds = m.predict(val_term_doc)<br/>(preds==val_y).mean()</span><span id="749a" class="mv lu iq mr b gy na mx l my mz">0.85504000000000002</span><span id="b6be" class="mv lu iq mr b gy na mx l my mz">m = LogisticRegression(C=1e8, dual=<strong class="mr ja">True</strong>)<br/>m.fit(trn_term_doc.sign(), y)<br/>preds = m.predict(val_term_doc.sign())<br/>(preds==val_y).mean()</span><span id="3969" class="mv lu iq mr b gy na mx l my mz">0.85487999999999997</span></pre><p id="866c" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">规则化版本</strong></p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="33cc" class="mv lu iq mr b gy mw mx l my mz">m = LogisticRegression(C=0.1, dual=<strong class="mr ja">True</strong>)<br/>m.fit(x, y)<br/>preds = m.predict(val_term_doc)<br/>(preds==val_y).mean()</span><span id="522e" class="mv lu iq mr b gy na mx l my mz">0.88275999999999999</span><span id="c46e" class="mv lu iq mr b gy na mx l my mz">m = LogisticRegression(C=0.1, dual=<strong class="mr ja">True</strong>)<br/>m.fit(trn_term_doc.sign(), y)<br/>preds = m.predict(val_term_doc.sign())<br/>(preds==val_y).mean()</span><span id="da5b" class="mv lu iq mr b gy na mx l my mz">0.88404000000000005</span></pre><h1 id="0c28" class="lt lu iq bd lv lw lx ly lz ma mb mc md kf me kg mf ki mg kj mh kl mi km mj mk bi translated">步骤5:具有NB特征的三元模型</h1><p id="c0e1" class="pw-post-body-paragraph kw kx iq ky b kz ml ka lb lc mm kd le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们的下一个模型是带有朴素贝叶斯特征的逻辑回归版本。对于每个文档，现在如上所述计算二值化特征，但是这次也使用二元模型和三元模型。每个特征都是对数计数比。然后训练逻辑回归模型来预测情绪。</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="e9e2" class="mv lu iq mr b gy mw mx l my mz">veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)<br/>trn_term_doc = veczr.fit_transform(trn)<br/>val_term_doc = veczr.transform(val)</span><span id="63a3" class="mv lu iq mr b gy na mx l my mz">trn_term_doc.shape</span><span id="7924" class="mv lu iq mr b gy na mx l my mz">(25000, 800000)</span><span id="21e4" class="mv lu iq mr b gy na mx l my mz">vocab = veczr.get_feature_names()</span><span id="2ce3" class="mv lu iq mr b gy na mx l my mz">vocab[200000:200005]</span><span id="b699" class="mv lu iq mr b gy na mx l my mz">['by vast', 'by vengeance', 'by vengeance .', 'by vera', 'by vera miles']</span><span id="e869" class="mv lu iq mr b gy na mx l my mz">y=trn_y<br/>x=trn_term_doc.sign()<br/>val_x = val_term_doc.sign()</span><span id="386c" class="mv lu iq mr b gy na mx l my mz">r = np.log(pr(1) / pr(0))<br/>b = np.log((y==1).mean() / (y==0).mean())</span></pre><p id="ecc3" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">步骤6:这里拟合正则化逻辑回归，其中特征是三元模型。</strong></p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="c4a8" class="mv lu iq mr b gy mw mx l my mz">m = LogisticRegression(C=0.1, dual=<strong class="mr ja">True</strong>)<br/>m.fit(x, y);</span><span id="9956" class="mv lu iq mr b gy na mx l my mz">preds = m.predict(val_x)<br/>(preds.T==val_y).mean()</span><span id="2172" class="mv lu iq mr b gy na mx l my mz">0.90500000000000003</span><span id="8046" class="mv lu iq mr b gy na mx l my mz">np.exp(r)</span><span id="99da" class="mv lu iq mr b gy na mx l my mz">matrix([[ 0.94678,  0.85129,  0.78049, ...,  3.     ,  0.5    ,  0.5    ]])</span></pre><p id="29c4" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后拟合正则化的逻辑回归，其中特征是三元模型的对数比。</p><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="d850" class="mv lu iq mr b gy mw mx l my mz">x_nb = x.multiply(r)<br/>m = LogisticRegression(dual=<strong class="mr ja">True</strong>, C=0.1)<br/>m.fit(x_nb, y);</span><span id="e364" class="mv lu iq mr b gy na mx l my mz">val_x_nb = val_x.multiply(r)<br/>preds = m.predict(val_x_nb)<br/>(preds.T==val_y).mean()</span></pre><p id="9038" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">0.91768000000000005</p><h1 id="f01c" class="lt lu iq bd lv lw lx ly lz ma mb mc md kf me kg mf ki mg kj mh kl mi km mj mk bi translated">第七步:fast.ai NBSVM</h1><pre class="kp kq kr ks gt mq mr ms mt aw mu bi"><span id="2b42" class="mv lu iq mr b gy mw mx l my mz">sl=2000</span><span id="1892" class="mv lu iq mr b gy na mx l my mz"><em class="nf"># Here is how the model looks from a bag of words</em><br/>md = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)</span><span id="1c33" class="mv lu iq mr b gy na mx l my mz">learner = md.dotprod_nb_learner()<br/>learner.fit(0.02, 1, wds=1e-6, cycle_len=1)</span><span id="ce41" class="mv lu iq mr b gy na mx l my mz">A Jupyter Widget</span><span id="3aab" class="mv lu iq mr b gy na mx l my mz">[ 0.       0.0251   0.12003  0.91552]</span><span id="6bdd" class="mv lu iq mr b gy na mx l my mz">learner.fit(0.02, 2, wds=1e-6, cycle_len=1)</span><span id="200e" class="mv lu iq mr b gy na mx l my mz">A Jupyter Widget</span><span id="3b29" class="mv lu iq mr b gy na mx l my mz">[ 0.       0.02014  0.11387  0.92012]                         <br/>[ 1.       0.01275  0.11149  0.92124]</span><span id="43d4" class="mv lu iq mr b gy na mx l my mz">learner.fit(0.02, 2, wds=1e-6, cycle_len=1)</span><span id="284a" class="mv lu iq mr b gy na mx l my mz">A Jupyter Widget</span><span id="e991" class="mv lu iq mr b gy na mx l my mz">[ 0.       0.01681  0.11089  0.92129]                           <br/>[ 1.       0.00949  0.10951  0.92223]</span></pre><p id="52a5" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该NBSVM实现了0.92129的精度。</p><p id="7ce9" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相同的NBSVM可以用来解决其他基于NLP的问题，但不同的特征工程需要进行实验，以获得最先进的结果，我在Kaggle毒性评论分类中获得了前8%。</p><p id="a3e5" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">非常感谢你的阅读，希望你喜欢。</p></div></div>    
</body>
</html>