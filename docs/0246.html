<html>
<head>
<title>Show Me The Black Box</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">给我看看黑盒</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/show-me-the-black-box-3495dd6ff52c?source=collection_archive---------0-----------------------#2019-12-18">https://pub.towardsai.net/show-me-the-black-box-3495dd6ff52c?source=collection_archive---------0-----------------------#2019-12-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d231" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">向AI解释机器学习模型|<a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank"/></h2><div class=""/><div class=""><h2 id="6c58" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">解释和诠释黑盒模型；基于树的算法的用例</h2></div><p id="3c0a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi ln translated"><span class="l lo lp lq bm lr ls lt lu lv di"> H </span>您是否曾经面临过业务(高管或用户)挑战模型所做决策的情况？</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi lw"><img src="../Images/d94d327325da31833661f4c793142fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1wy_l-q16tmNbVkLOaCwOQ.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图1 </strong>:“黑箱”模型简图</figcaption></figure><p id="2a33" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">几乎所有的行业和公司都意识到了机器学习在改进产品、流程和战略方面的优势和潜力。然而，<strong class="kt jd">模型通常不会解释它们的预测</strong>，这导致我们作为人类提出质疑，有时会成为采用机器学习的障碍。</p><p id="cb6b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">可解释性</strong>和<strong class="kt jd">可解释性</strong>已经成为现实世界的热门话题之一(见下面一些新闻)。我们知道<strong class="kt jd"><em class="mn"/></strong>预测的是什么，但现在我们想知道<strong class="kt jd"> <em class="mn">为什么</em> </strong>会做出预测。知道了原因，我们就能从本质上理解问题的根源(为什么我们首先要建立模型),并帮助我们从一开始就防止问题的发生。</p><div class="mo mp gp gr mq mr"><a href="https://www.newscientist.com/article/2225186-companies-could-be-fined-if-they-fail-to-explain-decisions-made-by-ai/" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd jd gy z fp mw fr fs mx fu fw jc bi translated">如果公司未能解释AI做出的决定，可能会被罚款</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">如果企业和其他组织不能解释由…做出的决定，他们可能面临数百万英镑的罚款</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">www.newscientist.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf mg mr"/></div></div></a></div><div class="mo mp gp gr mq mr"><a href="https://www.nasdaq.com/articles/goldman-sachs-ceo-denies-gender-bias-in-credit-limits-for-apple-card-2019-11-21" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd jd gy z fp mw fr fs mx fu fw jc bi translated">高盛首席执行官否认苹果信用卡信用额度存在性别偏见</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">(RTTNews) -高盛公司首席执行官大卫·所罗门周四否认了性别歧视的指控</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">www.nasdaq.com</p></div></div><div class="na l"><div class="ng l nc nd ne na nf mg mr"/></div></div></a></div><p id="a6a1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本帖中，我们将探讨一些解释该模型的方法。通过使用轻GBM分类，但是该概念可以应用于任何算法和回归问题)。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="17a6" class="no np it bd mm nq nr ns nt nu nv nw nx ki ny kj nz kl oa km ob ko oc kp od oe bi translated">模型解释</h1><p id="4b8d" class="pw-post-body-paragraph kr ks it kt b ku of kd kw kx og kg kz la oh lc ld le oi lg lh li oj lk ll lm im bi translated">在本节中，我们将使用银行客户流失数据作为练习数据，并使用轻型GBM算法进行预测和解释。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/93241f1c2a0cf48ffb63c9dba13dd818.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*TP_5B6WcmlGsUrLhDtxNMg.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图二</strong> : ML解释了另一个ML模因</figcaption></figure><p id="b37b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通常，我们会将解释视为两个主要类:<em class="mn">局部</em>和<em class="mn">全局</em>。</p><ol class=""><li id="d14c" class="ol om it kt b ku kv kx ky la on le oo li op lm oq or os ot bi translated">本地可解释性:提供每个预测是如何做出的详细解释。这将有助于用户信任我们的模型，并了解模型/建议是如何制定的。</li><li id="30e6" class="ol om it kt b ku ou kx ov la ow le ox li oy lm oq or os ot bi translated"><strong class="kt jd">全局可解释性</strong>:提供对模型结构的整体理解，以及模型的一般工作方式。这对于需要在高层次上理解模型的资深人士或发起人来说更为重要。</li></ol><p id="cf18" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们来看看每一种方法。</p><h1 id="1db5" class="no np it bd mm nq oz ns nt nu pa nw nx ki pb kj nz kl pc km ob ko pd kp od oe bi translated"><em class="pe"> 1。特征重要性</em></h1><p id="b818" class="pw-post-body-paragraph kr ks it kt b ku of kd kw kx og kg kz la oh lc ld le oi lg lh li oj lk ll lm im bi translated">特征重要性是一种全局解释方法，它提供了一个分数，表明数据中的每个特征在构建基于树的模型时的价值(在本例中为Light GBM)。一个属性在每棵树上被用来做关键决策的次数越多，它的相对重要性就越高。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi pf"><img src="../Images/0d24fb6ec7a9286dd45ec28cf3043842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6C9XKsK_OGWOqXOrql0v4g.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图3: </strong>使用增益作为度量，从轻型GBM生成的特征重要性</figcaption></figure><p id="bc85" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，有一个关键是我们需要理解它所代表的价值是什么。有几种计算/类型:</p><ul class=""><li id="4b60" class="ol om it kt b ku kv kx ky la on le oo li op lm pg or os ot bi translated"><strong class="kt jd"> <em class="mn">增益</em> </strong>表示通过使用每个特征对模型中每棵树的贡献来计算对模型的相对贡献。该值越高，该特征对于预测结果越重要</li><li id="26f9" class="ol om it kt b ku ou kx ov la ow le ox li oy lm pg or os ot bi translated"><strong class="kt jd"> <em class="mn">覆盖率</em> </strong>是与每个特性相关的相对观察数量，以所有特性覆盖率的百分比表示。让我们来看一些例子:<code class="fe ph pi pj pk b">Feature x</code>用来决定叶节点<code class="fe ph pi pj pk b">a</code>、<code class="fe ph pi pj pk b">b</code>和<code class="fe ph pi pj pk b">c</code>观察，因此覆盖范围是<code class="fe ph pi pj pk b">a+b+c</code>。</li><li id="2bb5" class="ol om it kt b ku ou kx ov la ow le ox li oy lm pg or os ot bi translated"><strong class="kt jd"> <em class="mn">权重</em> </strong>是表示该特征在模型的树中出现的相对次数的百分比，并表示为其占所有特征权重的百分比。</li></ul><p id="4458" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一般来说，我们将重点使用每个特征的<strong class="kt jd">增益</strong>。</p><h1 id="edc1" class="no np it bd mm nq oz ns nt nu pa nw nx ki pb kj nz kl pc km ob ko pd kp od oe bi translated">2.ELI5</h1><p id="ff09" class="pw-post-body-paragraph kr ks it kt b ku of kd kw kx og kg kz la oh lc ld le oi lg lh li oj lk ll lm im bi translated">Eli5(或解释为i5)是一个帮助进入机器学习模型并解释其预测的包。</p><p id="1c30" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">看待分类或回归模型有两种主要方式:</p><ol class=""><li id="c401" class="ol om it kt b ku kv kx ky la on le oo li op lm oq or os ot bi translated">检查模型参数并尝试弄清楚模型如何全局工作(<code class="fe ph pi pj pk b">eli5.show_weights()</code>)</li><li id="825a" class="ol om it kt b ku ou kx ov la ow le ox li oy lm oq or os ot bi translated">检查一个模型的单个预测，试图找出为什么模型会做出这样的决定(<code class="fe ph pi pj pk b">eli5.show_prediction()</code>)</li></ol><pre class="lx ly lz ma gt pl pk pm pn aw po bi"><span id="8bf5" class="pp np it pk b gy pq pr l ps pt">eli5.show_weights(lgb_model)</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/f7e058b6a29e5f62f4ccc8810da80386.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*wPWF4py79roWfziEfVTQnw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图4: </strong>特征的权重</figcaption></figure><p id="11ce" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上面的分数与特征重要性类似。</p><p id="4a0e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以用<code class="fe ph pi pj pk b">eli5</code>做的另一件事是<strong class="kt jd">排列重要性</strong>，或均值降低准确度(MDA)。其思想是，当<em class="mn">特征不可用</em>时，可以通过查看分数(可以是准确度F1)降低多少来测量特征重要性。</p><p id="f37f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了避免重新训练机器学习模型，它不会删除该特征，而是通过从与原始特征值相同的分布中提取来用随机噪声替换它。这可以简单地通过改变一个特性的值来实现(如图5所示)。请注意，如果列的数量很大，这个过程可能会占用大量资源。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi pv"><img src="../Images/c57ded10a7ea1cb1b8e3a4930c4cbcc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ETwAKIhpzfJ5Nr7FN0mTPA.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图5: </strong>如何进行洗牌的例子</figcaption></figure><p id="6eb0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">排列重要性的结果如下表所示，<code class="fe ph pi pj pk b">x±y</code>，越靠前的值是最重要的特征。第一个数字显示随机洗牌后模型性能下降了多少。该符号显示了由于特征上的混洗而导致的确切性能变化的一些随机性。通过重复洗牌过程来测量这种随机性，从而得出第二个数字。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/43f35753ca9339e7b2fb42ed7fb90da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*a5VLkOssJnUYrdd5QdK-bw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图6: </strong>排列重要性结果</figcaption></figure><p id="aa94" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">有时，排列重要性会导致负值。这意味着由混洗数据产生的<strong class="kt jd">预测最终比真实数据更加准确</strong>。</p><p id="29b7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，它并没有告诉你每个特性的重要性。如果一个特性具有中等排列重要性，这可能意味着它具有中等排列重要性；</p><ul class=""><li id="7586" class="ol om it kt b ku kv kx ky la on le oo li op lm pg or os ot bi translated">对一些预测有较大影响，但总体上没有影响，或</li><li id="0694" class="ol om it kt b ku ou kx ov la ow le ox li oy lm pg or os ot bi translated">所有预测的中等效果</li></ul><p id="0c58" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">ELI5还可以用来做局部解释，解释来自机器学习模型的个体预测。它显示了特征权重，该权重是通过遵循集成树中每棵树的决策路径来计算的。决策路径上的每个特征的贡献是期望分数从父代到子代改变了多少。所有特征的权重加起来就是模型的输出分数。</p><pre class="lx ly lz ma gt pl pk pm pn aw po bi"><span id="15b8" class="pp np it pk b gy pq pr l ps pt">eli5.explain_prediction_lightgbm(lgb_model, X_test.iloc[1])</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi px"><img src="../Images/fee3d0c75e2443bff84408e516581c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*fQGugH-pE4oKi6eLLC_Sow.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图7 </strong>:使用eli5的个别说明</figcaption></figure><p id="f3e0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请注意<code class="fe ph pi pj pk b">&lt;BIAS&gt;</code>值，这是因为<code class="fe ph pi pj pk b">eli5</code>中的解释器实现开始为所有树中的每个节点重建伪叶分数。该算法也适用于树的根节点，其被类似地分配伪叶分数。对所有树求和，所有根节点分数的总和就是你可能得到的所有树的平均分数。这就是<code class="fe ph pi pj pk b">eli5</code>里的<code class="fe ph pi pj pk b">&lt;BIAS&gt;</code>。</p><h1 id="38d9" class="no np it bd mm nq oz ns nt nu pa nw nx ki pb kj nz kl pc km ob ko pd kp od oe bi translated">3.并行分布处理</h1><p id="63b5" class="pw-post-body-paragraph kr ks it kt b ku of kd kw kx og kg kz la oh lc ld le oi lg lh li oj lk ll lm im bi translated">部分相关图(PDP)是一种<strong class="kt jd">全局解释方法</strong>；该方法考虑了所有实例，并给出了关于特征与预测结果的全局<strong class="kt jd">关系的陈述。它可用于显示目标和特征之间的关系是线性的、单调的还是更复杂的关系。</strong></p><p id="bd6a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于分类问题，在模型输出概率的情况下，PDP显示给定某个特性的不同值时某个类别的概率。</p><p id="6cb8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请注意特征重要性和部分相关的区别；</p><ul class=""><li id="5112" class="ol om it kt b ku kv kx ky la on le oo li op lm pg or os ot bi translated">特征重要性显示了哪些变量对预测影响最大</li><li id="2390" class="ol om it kt b ku ou kx ov la ow le ox li oy lm pg or os ot bi translated">部分相关性显示了特征如何影响预测</li></ul><p id="e262" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">PDP非常有用，可以为以下问题提供额外的见解；</p><ol class=""><li id="1e16" class="ol om it kt b ku kv kx ky la on le oo li op lm oq or os ot bi translated">在不同的地区，类似大小的房子如何定价？</li><li id="7278" class="ol om it kt b ku ou kx ov la ow le ox li oy lm oq or os ot bi translated">两组之间预测结果的差异是由于具体特征的不同吗？</li></ol><p id="7e8f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们来看看用法和结果图的例子。</p><pre class="lx ly lz ma gt pl pk pm pn aw po bi"><span id="6669" class="pp np it pk b gy pq pr l ps pt">pdp_goals = pdp.pdp_isolate(model=lgb_model, dataset=X_test, <br/>                            model_features=X_test.columns,  <br/>                            feature='age')<br/>pdp.pdp_plot(pdp_goals, 'Ages')<br/>plt.show()</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi py"><img src="../Images/4e8d1cc04f6dd3daa74c89e184ea0440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOd1OvnSa_AdsdrYw4kCrg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图8 </strong>:特性的PDP</figcaption></figure><pre class="lx ly lz ma gt pl pk pm pn aw po bi"><span id="ec59" class="pp np it pk b gy pq pr l ps pt">fig, axes = pdp.pdp_plot(pdp_goals, 'Ages', center=True,                                                           <br/>                         plot_lines= True, x_quantile=True, <br/>                         show_percentile=True, frac_to_plot=100, <br/>                         plot_pts_dist=True)<br/>plt.show()</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi pz"><img src="../Images/f627fcdbc80f49b075040cc0cbdf206f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qpF73r_41WSEDaETnfZY5Q.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图9: </strong>另一个包含更多细节的PDP</figcaption></figure><p id="6c75" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里我们可以看到,<code class="fe ph pi pj pk b">Ages</code>特征从40增加到50，结局(退出)的概率增加。然而，这一特征的有效性降低了。</p><p id="d7d5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请注意，当我们解释该图时，当数据点不可用时，该区域的PD估计值不太可靠。</p><p id="1a1f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">另一种用法是同时查看两个特征的部分相关性:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/80b1b3b8355fc5b5c228152e3c7a0340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*9-73aZ9-34XL7uO-D3Jwkw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图10 </strong>:两个特征的PD交互图</figcaption></figure><p id="e4fe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在该图中，我们可以看到客户退出银行的概率以及年龄和产品特性数量的相互作用。同样，得出结论时要小心，任何增加/减少的概率可能只是一种相关性，而不是因果关系。</p><h1 id="6727" class="no np it bd mm nq oz ns nt nu pa nw nx ki pb kj nz kl pc km ob ko pd kp od oe bi translated">4.石灰</h1><p id="8bb0" class="pw-post-body-paragraph kr ks it kt b ku of kd kw kx og kg kz la oh lc ld le oi lg lh li oj lk ll lm im bi translated">LIME，本地可解释的模型不可知解释，是实现<strong class="kt jd">本地代理模型</strong>的方法。LIME不是训练一个全局的代理模型，而是专注于训练一个局部代理模型，通过围绕每个预测建立稀疏线性模型来解释黑盒模型如何工作，从而解释单个预测。</p><p id="38a8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们可以使用<strong class="kt jd">保真度测量</strong>，可解释模型逼近黑盒预测的程度，来查看可解释模型在解释预测方面的可靠性。</p><p id="7c4a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在撰写本文时，LIME是为数不多的能够处理表格数据、文本和图像的方法之一。</p><p id="2672" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们来看看如何用Python来编写代码。首先，我们需要使用训练数据集来训练解释器，就像我们对其他机器学习算法所做的那样。</p><p id="4f64" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，我们可以为解释者提供每个单独的数据集来解释预测。</p><pre class="lx ly lz ma gt pl pk pm pn aw po bi"><span id="003b" class="pp np it pk b gy pq pr l ps pt">lime_explainer = lime.lime_tabular.LimeTabularExplainer(<br/>   X_train.values,<br/>   feature_names=X_train.columns.values.tolist(),<br/>   class_names=['non_exited', 'exited'],<br/>   verbose=True, mode='classification')</span><span id="ef69" class="pp np it pk b gy qb pr l ps pt">expLgb = lime_explainer.explain_instance(<br/>   data_row=X_test.values[1], <br/>   predict_fn=lgb_model.predict_proba, <br/>   num_features=3)</span><span id="cce9" class="pp np it pk b gy qb pr l ps pt">expLgb.show_in_notebook(show_table=True)</span></pre><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/bef7627fcc8dc0d13f8a6526e57148ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*R1vYgvH8im4wJA3J_ldhyw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图11 </strong>:个人讲解员的输出</figcaption></figure><p id="cd3c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们最终可以看到哪些特征正在影响预测以及预测的最终概率。请注意，在这种情况下，我只展示了前3个特性。</p><h1 id="aa10" class="no np it bd mm nq oz ns nt nu pa nw nx ki pb kj nz kl pc km ob ko pd kp od oe bi translated">5.SHAP</h1><p id="8e51" class="pw-post-body-paragraph kr ks it kt b ku of kd kw kx og kg kz la oh lc ld le oi lg lh li oj lk ll lm im bi translated">SHAP是一种通过计算每个特征对预测的贡献来解释单个预测的方法。然而，SHAP也带来了许多基于Shapley值聚合的全局解释方法。<strong class="kt jd"> SHAP方法是基于博弈理论上的最优沙普利值</strong>。</p><p id="b6a5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Shapley值的技术定义是一个特征值在所有可能的联合中的平均边际贡献(T4)。或者，Shapley值使用所有可能的输入组合来考虑实例的所有可能的预测。因此，Shapley值是唯一具有可靠理论的解释方法，不像LIME，它假设机器学习模型在局部具有线性行为。</p><p id="56f6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SHAP方法是将石灰与沙普利值联系在一起。让我们看看如何使用SHAP来解释和说明这个模型(全球和本地)。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/3e4273dca9793291ac7cd08d7bb63891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*edntZ3NLR8uvO43UysxFDg.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图12 </strong> : SHAP版特征重要性</figcaption></figure><p id="b505" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SHAP使用Shapley值来表示特征的重要性，或者说它是基于特征属性的大小，这与排列特征重要性是不同的。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/1961b9a8f7b80a00524179787a0c7e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*52SeoPYwhOMfIz-J7RF50g.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图十三</strong> : SHAP概要图</figcaption></figure><p id="f647" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该图结合了特征重要性和特征效果。图上的每个点都是特征和目标的Shapley值。这些特征根据其重要性进行排序(类似于<em class="mn">图12 </em>)。颜色表示特征的值，x轴是Shapley值。</p><p id="d338" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SHAP还支持<strong class="kt jd">依赖图</strong>，通过使用<code class="fe ph pi pj pk b">dependence_plot()</code>函数，它可以显示单个特征如何影响模型的输出。</p><p id="2145" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下图显示了<code class="fe ph pi pj pk b">age</code>特征如何与预测相互作用。该功能将选择另一个可视化功能(颜色)，我们可以使用<code class="fe ph pi pj pk b">interaction_index=None</code>关闭它。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/b917b608ff848a3fe7612aef7c498a0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*BvexV4nel9nca5Z644bBxg.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图14 </strong>:年龄特征与预测交互</figcaption></figure><p id="8f81" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SHAP为每个单独的预测提供了可视化，在这种情况下，我们可以了解每个SHAP值如何影响预测。图中的基值是所有预测的所有Shapley值的平均值。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi qf"><img src="../Images/e447b818b60462d53506fd20ac434453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AvA9SVGZahIz5BryTPYR6A.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated"><strong class="bd mm">图十五</strong> : SHAP个人解释</figcaption></figure><p id="64e5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SHAP也有如下几个缺点:</p><ul class=""><li id="9735" class="ol om it kt b ku kv kx ky la on le oo li op lm pg or os ot bi translated">Shapley值<strong class="kt jd">需要大量的计算时间</strong>，基于树的算法除外</li><li id="5b5e" class="ol om it kt b ku ou kx ov la ow le ox li oy lm pg or os ot bi translated">可能是<strong class="kt jd">曲解了</strong>，不是从模型训练中去除特征后的预测值的差异。而是，给定特征值的当前集合，特征值对实际预测和平均预测之间的差异的贡献是估计的Shapley值。</li><li id="71ff" class="ol om it kt b ku ou kx ov la ow le ox li oy lm pg or os ot bi translated">Shapley值总是<strong class="kt jd">使用所有特征</strong>来计算</li></ul></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="d0d4" class="no np it bd mm nq nr ns nt nu nv nw nx ki ny kj nz kl oa km ob ko oc kp od oe bi translated">结论</h1><p id="84b9" class="pw-post-body-paragraph kr ks it kt b ku of kd kw kx og kg kz la oh lc ld le oi lg lh li oj lk ll lm im bi translated">我已经为我们的黑盒模型提供了基本的解释方法，尽管这个例子是基于轻量级GBM的，但是大多数方法可以与其他算法一起重用。您可以选择使用哪种方法最适合您的用例，以及您将向谁进行演示(即，选择<strong class="kt jd">本地解释</strong>供实际操作用户进行交互并使用模型结果来做出决策，选择<strong class="kt jd">全局解释</strong>供业务主管查看哪些因素(特征)影响模型或我们试图解决的问题)。</p><p id="994a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所有的情节和代码以及解释都可以在下面我的GitHub库中找到。</p><div class="mo mp gp gr mq mr"><a href="https://github.com/netsatsawat/model-interpretation" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd jd gy z fp mw fr fs mx fu fw jc bi translated">netsatsawat/模型-解释</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">这个库存储Jupyter笔记本和HTML文件，笔记本解释了方法和演练…</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">github.com</p></div></div><div class="na l"><div class="qg l nc nd ne na nf mg mr"/></div></div></a></div></div></div>    
</body>
</html>