<html>
<head>
<title>For The Win: An AI Agent Achieves Human-Level Performance in a 3D Video Game</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">获奖理由:人工智能代理在3D视频游戏中实现了人类水平的性能</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/for-the-win-an-ai-agent-achieves-human-level-performance-in-a-3d-video-game-3971277ea3d4?source=collection_archive---------0-----------------------#2020-01-18">https://pub.towardsai.net/for-the-win-an-ai-agent-achieves-human-level-performance-in-a-3d-video-game-3971277ea3d4?source=collection_archive---------0-----------------------#2020-01-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4d9336ba07dd2add4f643b26015574cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIWN_CuMLDsH6fMLu6qa_w.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">来源:https://deepmind.com/blog/article/capture-the-flag-science</figcaption></figure><div class=""/><div class=""><h2 id="4ac1" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">DeepMind对FTW代理的详细解释</h2></div><h1 id="54c8" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">介绍</h1><p id="b0bf" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在本文中，我们将讨论来自DeepMind的Win(FTW)代理，它在一款流行的基于团队的多人第一人称视频游戏中实现了人类级别的性能。FTW代理使用一种新颖的两层优化过程，在该过程中，一群独立的RL代理从成千上万的平行比赛中同时被训练，代理在随机生成的环境中一起组队比赛并彼此对抗。群体中的每个代理学习其内部奖励信号，以补充来自获胜的稀疏延迟奖励，并使用新颖的时间分层表示来选择动作，这使得代理能够在多个时间尺度上进行推理。</p><h1 id="a1d3" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">任务描述</h1><p id="8f53" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">FTW代理在“捕捉旗帜”( CTF)环境中接受训练，在该环境中，由多个单独玩家组成的两个对立团队(他们只进行2对2游戏的训练，但发现代理可以推广到不同的团队规模)通过战略性导航、标记和躲避对手来竞争捕捉对方的旗帜。五分钟后获得最多旗帜的队伍获胜。</p><h2 id="39fb" class="mm kz jj bd la mn mo dn le mp mq dp li lz mr ms lk md mt mu lm mh mv mw lo mx bi translated">环境观察</h2><p id="aa2c" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">该观察由<em class="my"> 84个⨉ 84个</em>像素组成。每个像素由三个字节组成的三元组表示，我们通过<em class="my"> 1/255 </em>对其进行缩放，以产生一个观察值<em class="my"> x ∈ [0，1]^{84 ⨉ 84 ⨉ 3} </em>，就像我们在雅达利游戏上所做的那样。此外，某些游戏点信号<em class="my"> 𝜌_t </em>，如“我捡起了旗子”，也是可用的。</p><h2 id="d0e5" class="mm kz jj bd la mn mo dn le mp mq dp li lz mr ms lk md mt mu lm mh mv mw lo mx bi translated">行为空间</h2><p id="bb6d" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">动作空间由六种离散的部分动作组成:</p><ul class=""><li id="1508" class="mz na jj ls b lt nb lw nc lz nd md ne mh nf ml ng nh ni nj bi translated">五个值的偏航旋转变化<em class="my"> (-60，-10，0，10，60) </em></li><li id="5ca7" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated">三个值的音高变化<em class="my"> (-5，0，5) </em></li><li id="1e9d" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated">向左或向右扫射(三元)</li><li id="f6f3" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated">向前或向后移动(三元)</li><li id="b562" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated">标记与否(二进制)</li><li id="8e96" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated">跳跃与否(二进制)</li></ul><p id="51ca" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">这给了我们一个代理可以产生的<em class="my"> 5⨉3⨉3⨉3⨉2⨉2=540 </em>复合动作的总数。</p><h2 id="b7df" class="mm kz jj bd la mn mo dn le mp mq dp li lz mr ms lk md mt mu lm mh mv mw lo mx bi translated">记号</h2><p id="4c3f" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为了便于参考，我们在这里列出了一些稍后使用的符号</p><ul class=""><li id="3a3b" class="mz na jj ls b lt nb lw nc lz nd md ne mh nf ml ng nh ni nj bi translated">𝜙:超参数</li><li id="66c4" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated">𝜋:代理政策</li><li id="22d0" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated">𝛺: CTF地图空间</li><li id="5d6e" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated"><em class="my">r =</em><strong class="ls jk"><em class="my">w</em></strong><em class="my">【𝜌_t】</em>:内在奖励</li><li id="0cb1" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated"><em class="my"> p </em>:玩家<em class="my"> p </em></li><li id="9e8e" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated"><em class="my"> m_p(𝜋) </em>:一种随机配对方案，使合作者偏向于与玩家<em class="my"> p </em>具有相似技能，请参见补充材料中的Elo分数，了解评分代理的表现详情</li><li id="6755" class="mz na jj ls b lt nk lw nl lz nm md nn mh no ml ng nh ni nj bi translated"><em class="my">𝜄≁m_p(𝜋)</em>:p<em class="my">的合作者</em></li></ul><h1 id="d199" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">FTW代理</h1><h2 id="8013" class="mm kz jj bd la mn mo dn le mp mq dp li lz mr ms lk md mt mu lm mh mv mw lo mx bi translated">全部的</h2><p id="c8f8" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">夺旗(CTF)提出了三个挑战:</p><p id="8f00" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">1.CTF游戏需要高水平策略的记忆和长期时间推理。</p><p id="bebd" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">2.奖励很少——只有在游戏结束时才会给予奖励。因此，信用分配问题是一个难题。</p><p id="c99a" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">3.环境设置因比赛而异。除了不同的地图，可能还有不同数量的玩家，不同等级的队友等等。</p><p id="ab05" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">FTW代理通过引入一种架构来满足第一个要求，该架构具有多时间尺度表示，这让人想起在灵长类动物大脑皮层中观察到的情况，以及外部工作记忆模块，该模块主要受人类情景记忆的启发。通过使代理能够基于游戏点数信号<em class="my"> 𝜌_t </em>发展内部奖励信号，信用分配问题得以缓解。最后，为了开发对不同环境设置具有鲁棒性的多样化概括技能，我们同时培训了大量多样化的代理人，他们通过在不同的地图中相互游戏来学习。这种多样化的代理群体也为执行基于群体的内部奖励和超参数优化培训铺平了道路。</p><p id="2e89" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">在本节的其余部分，我们首先讨论FTW使用的体系结构和目标。然后，我们讨论内在奖励和基于群体的训练。</p><h2 id="2e7b" class="mm kz jj bd la mn mo dn le mp mq dp li lz mr ms lk md mt mu lm mh mv mw lo mx bi translated">时间分层强化学习</h2><p id="3a7f" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls jk">建筑</strong></p><p id="2bb0" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">FTW代理使用分层RNN，其中两个LSTMs在不同的时间尺度上运行(尽管该架构可以扩展到更多层，但作者发现实际上两层以上对任务几乎没有影响)。在每个时间步长上进化的快速跳动的LSTM输出变化的后验概率</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c6f27cdd9b823f338c4cac2a78b387bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*78Xlf0__XMUvu6Zx01piJQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">从快速跳动的LSTM变分后验概率</figcaption></figure><p id="ecd9" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">从<em class="my"> N( </em> 𝜇 <em class="my"> _t^q,𝛴_t^q) </em>采样的<em class="my"> z_t </em>然后被用作策略、价值函数和辅助任务的潜在变量。慢速时标LSTM，更新每个𝜏时间步长，输出潜在先验</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nx"><img src="../Images/2212214b3fc4eadadd1ec2dcf899996b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nL-NhO-lM8ppDeQ_jTnmeg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">来自慢滴答的LSTM的潜在先验</figcaption></figure><p id="e883" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">这个先验分布，正如我们将很快解释的，然后作为变分后验的正则化。直观地说，慢速LSTM在<em class="my"> z，</em>上生成先验，该先验为后续的𝜏步骤预测<em class="my"> z </em>的演变，而快速LSTM在<em class="my"> z </em>上生成变分后验，该变分后验结合了新的观察结果，但坚持了先验做出的预测。下图总结了在时间步<em class="my"> t </em>的这一过程，我们将在补充材料中附加一个来自论文的更详细的图。</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/b582023a585eb94add4bfe70c33c3f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8875MTLaHa3F9yDjGVKzKw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">等级RNN结构</figcaption></figure><p id="f3b5" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">我们还将快速和慢速rnn的隐藏状态的演化数学表达如下</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/c6846a52b4606db0a5da24dbb6b2050c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ne20QLvKcloqqGCZe5TBOA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">慢和快LSTM。其中g_p、g_q分别是慢速和快速时间尺度的LSTM核。</figcaption></figure><p id="6266" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated"><strong class="ls jk">目标</strong></p><p id="7af9" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">FTW使用与<a class="ae jg" href="https://arxiv.org/abs/1611.05397" rel="noopener ugc nofollow" target="_blank"> UNREAL </a>几乎相同的目标，V-trace用于偏离策略校正，附加KL项用于正则化。</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/5c8d0f6423deead6775101bc1291960f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRROzmqVyjgvVI7A6JWSnA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">等式1。FTW的损失函数</figcaption></figure><p id="8ae7" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">物体的大部分先前已经被<a class="ae jg" href="https://arxiv.org/abs/1611.05397" rel="noopener ugc nofollow" target="_blank">虚幻</a>和<a class="ae jg" href="https://arxiv.org/abs/1802.01561" rel="noopener ugc nofollow" target="_blank">黑斑羚</a>覆盖。第一个新引入的KL项按照RL作为概率推理的思想，对照先前的策略<em class="my"> P </em>来正则化策略<em class="my"> Q </em>。与直接正则化策略的传统概率RL方法不同，等式1引入了中间潜在变量<em class="my"> z </em>，其对过去观察的依赖性进行建模。通过调整潜在变量，策略和先验现在的区别仅在于对过去观察的依赖性的建模方式。第二个KL惩罚针对具有平均值<em class="my"> 0 </em>和标准偏差<em class="my"> 0.1 </em>的多变量高斯调整先前的策略<em class="my"> P </em>(参见论文中的<em class="my">第5.4节</em></p><p id="4396" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">与先前方法的另一个细微差别在于像素控制策略，在等式1中表示为<em class="my"> L_{PC} </em>。鉴于动作空间的复合性质，作者建议为六个动作组中的每一个训练独立的像素控制策略(详见补充材料中的<em class="my">图S10(i) </em>)。</p><p id="2c94" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">等式1中使用的所有系数𝝀s首先从某个范围采样，然后通过基于群体的训练进行优化。</p><h2 id="e2eb" class="mm kz jj bd la mn mo dn le mp mq dp li lz mr ms lk md mt mu lm mh mv mw lo mx bi translated">内在奖励</h2><p id="ce27" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">外在奖励只在比赛结束时给出，表示赢(+1)、输(-1)或平(0)。这种延迟的奖励给学习带来了极其困难的学分分配问题。为了缓解这个问题，基于游戏点信号<em class="my"> 𝜌_t </em>定义了密集的内在奖励。具体来说，对于每个游戏点信号，代理的内在报酬映射<strong class="ls jk"><em class="my">w</em></strong><em class="my">(𝜌_t)</em>最初独立于<em class="my"> Uniform(-1，1) </em>进行采样。然后，使用基于群体的训练(PBT)过程以及其他超参数(如等式1中的𝝀s和学习率)来进化这些内部奖励。</p><h2 id="f933" class="mm kz jj bd la mn mo dn le mp mq dp li lz mr ms lk md mt mu lm mh mv mw lo mx bi translated">基于人口的培训</h2><p id="b7d2" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">基于群体的训练(PBT)是一种进化方法，它并行训练模型群体，并不断用更好的模型加上微小的修改来替换更差的模型。对于我们的FTW代理，PBT可以通过重复以下步骤来总结</p><p id="c5e5" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">1.<strong class="ls jk">步骤</strong>:在<em class="my"> P=30 </em>的群体中的每个模型都用其超参数进行一些步骤的训练(例如<em class="my"> 1K </em>游戏)。对于每场比赛，我们首先随机抽取一名代理人<em class="my"> 𝜋_p </em>，然后我们根据他们的Elo分数选择其队友和对手——关于Elo分数的简要介绍，请参见补充材料。</p><p id="e316" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">2.<strong class="ls jk"> Eval </strong>:满足step要求后，我们对每个模型的性能进行评估。在FTW的情况下，我们让就绪代理与另一个随机抽样的代理竞争，并估计Elo分数。</p><p id="8f64" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">3.<strong class="ls jk">利用</strong>:如果发现代理的估计获胜概率小于<em class="my"> 70% </em>，那么失败的代理复制策略、内部奖励转换和更好代理的超参数。</p><p id="47ef" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">4.<strong class="ls jk">探索</strong>:我们用<em class="my"> 20 </em>以<em class="my"> 5% </em>的概率扰动继承的内部报酬和超参数，除了慢速LSTM时标𝜏，从整数范围<em class="my">【5，20】</em>均匀采样。</p><h1 id="eb6a" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">摘要</h1><p id="cfe5" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们可以将政策培训和PBT总结为以下目标的联合优化</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/2359fc7ea4a934b799818a885e853e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h3uryvgY2lXdGkDDjtpodw.png"/></div></div></figure><p id="7bab" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">这可以看作是一个两层的强化学习问题。用时间分层RL求解的内部优化最大化<em class="my"> J_{inner} </em>，代理的预期未来贴现内部报酬。用PBT求解的<em class="my"> J_{outer} </em>的外部优化可以看作是一个元博弈，其中获胜的元奖励通过内部奖励变换<strong class="ls jk"> <em class="my"> w </em> </strong>和超参数𝜙最大化，内部优化提供元转换动力学。</p><h1 id="f881" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">结束</h1><p id="2d05" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">就是这样。这是一次长途旅行；希望你喜欢。如果你遇到一些错误或者有一些担心，欢迎在下面留言或者评论。感谢阅读:-)</p><h1 id="4048" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">参考</h1><p id="5538" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">Max Jaderberg，Wojciech M. Czarnecki，Iain Dunning，Luke Marris，Guy Lever，Antonio Garcia Castañ eda，Charles Beattie，等人2019。"基于群体强化学习的3D多人游戏中人类水平的表现."<em class="my">理科</em>364(6443):859–65。<a class="ae jg" href="https://doi.org/10.1126/science.aau6249." rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1126/science.aau6249.</a></p><p id="e859" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">Espeholt，Lasse，Hubert Soyer，Remi Munos，卡伦·西蒙扬，Volodymyr Mnih，汤姆·沃德，Boron Yotam等人，2018年。" IMPALA:具有重要性加权的行动者-学习者架构的可扩展分布式深度学习."在<em class="my">2018 ICML第35届国际机器学习大会上</em>。</p><p id="d39b" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">Max Jaderberg，Volodymyr Mnih等人，2018年。“无监督辅助任务的强化学习”，1–9。<a class="ae jg" href="https://doi.org/10.1051/0004-6361/201527329." rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1051/0004-6361/201527329.</a></p><h1 id="de36" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">补充材料</h1><h2 id="f951" class="mm kz jj bd la mn mo dn le mp mq dp li lz mr ms lk md mt mu lm mh mv mw lo mx bi translated">网络体系结构</h2><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oc"><img src="../Images/cdca6a9ff43e7f7a78326416302fb613.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yLUxLHpXJvVTvM3DiEcWPw.png"/></div></div></figure><p id="c715" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">其中<a class="ae jg" href="https://medium.com/towards-artificial-intelligence/dnc-differential-neural-network-3cfd82d0d99e?source=friends_link&amp;sk=b0aee4af647f583267676b9171c6cbc7" rel="noopener"> DNC </a>已经在我们之前的帖子中解释过了。</p><h2 id="8d25" class="mm kz jj bd la mn mo dn le mp mq dp li lz mr ms lk md mt mu lm mh mv mw lo mx bi translated">Elo分数</h2><p id="f629" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">给定一群<em class="my"> M </em>代理，让可训练变量<em class="my"> 𝜓_i∈R </em>成为代理<em class="my"> i </em>的等级。我们用向量<strong class="ls jk"><em class="my">m</em></strong><em class="my">∈z^m</em>来描述两个玩家【T10(I，j)】在蓝色和红色上的给定匹配，其中<em class="my"> m_i </em>是代理人I出现在蓝色团队中的次数减去代理人出现在红色团队中的次数——在PBT的评估步骤中，我们使用两个在蓝色团队中有<em class="my"> 𝜋_i </em>的玩家和两个有<em class="my"> 𝜋_j的玩家标准的Elo公式是</em></p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/cef4c9ccde737d979bd3593f4ea3520f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ie3xnFmWyIVHTy-avu8zmQ.png"/></div></div></figure><p id="a226" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">其中，我们优化评级<strong class="ls jk"><em class="my"/></strong>以最大化数据的可能性(我们将数据标记为<em class="my"> y_i=1 </em>表示‘蓝击败红’，<em class="my"> y_i=1/2 </em>表示平局，而<em class="my"> y_i=0 </em>表示‘红击败蓝’)。由于获胜概率仅取决于评分的绝对差异，为了便于解释，我们通常将特定代理的评分固定在<em class="my"> 1000 </em>的水平。</p><p id="c1d4" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">特定代理的队友和对手抽样分布定义为</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oe"><img src="../Images/2e1f91bc0f339ba32cbf15aadd89d649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPS6w0eBGYBH7AjT8WkjHA.png"/></div></div></figure><p id="f3de" class="pw-post-body-paragraph lq lr jj ls b lt nb kk lv lw nc kn ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">这是基于Elo的获胜概率的正态分布，以具有相同skill(𝜇 <em class="my"> =0.5 </em>的代理为中心。</p></div></div>    
</body>
</html>