<html>
<head>
<title>How Regularization Helps in Data Overfitting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化如何帮助数据过度拟合</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-regularization-can-help-in-overfitting-the-data-ad9ff80f9ccc?source=collection_archive---------2-----------------------#2019-05-08">https://pub.towardsai.net/how-regularization-can-help-in-overfitting-the-data-ad9ff80f9ccc?source=collection_archive---------2-----------------------#2019-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/2304578098ec266270d49af172a5c332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJ0tDypBHIJUkzNU3InLfQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">资料来源:联合国人类住区规划署</figcaption></figure><div class=""/><div class=""><h2 id="c9ec" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">无疑有助于减少寻找完美模特的痛苦</h2></div><blockquote class="kv"><p id="830c" class="kw kx jf bd ky kz la lb lc ld le lf dk translated"><em class="ku">“知识是财富，实践是打开财富的钥匙”</em></p></blockquote><p id="86e4" class="pw-post-body-paragraph lg lh jf li b lj lk kg ll lm ln kj lo lp lq lr ls lt lu lv lw lx ly lz ma lf ij bi translated">让我从一个简单的事实开始这篇文章。对于<em class="mb">回归来说，大多数时候，我们通过尝试拟合曲线</em>来开始建立模型。</p><p id="08ed" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">当我们朝着数据的方向前进时，我们可能会遇到一个问题，这个问题可能会导致数据的过度拟合，这会给我们一个错误的印象和数据的错误含义。</p><p id="c8f9" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">嗯……怎么会？？</p><p id="52f1" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">我们一会儿将会看到它是如何发生的，但是首先，让我们了解它为什么会发生？  <em class="mb"> </em>我们先从一些定义开始。</p><blockquote class="mh mi mj"><p id="c613" class="lg lh mb li b lj mc kg ll lm md kj lo mk me lr ls ml mf lv lw mm mg lz ma lf ij bi translated">当我们的模型<em class="jf">太复杂</em>以至于不能概括新数据时，就会发生过度拟合。当我们的模型完美地拟合数据时，它不太可能很好地拟合新数据。</p><p id="739f" class="lg lh mb li b lj mc kg ll lm md kj lo mk me lr ls ml mf lv lw mm mg lz ma lf ij bi translated">当我们的模型不够复杂时，就会出现欠拟合。这在模型中引入了偏差，从而与真正的潜在估计量存在系统偏差。</p></blockquote><p id="de8f" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">没问题的..！！现在，让我们更详细地了解这一点。它如何影响我们的模型性能？我们将通过一个简单但最常见的图表对此有更多的了解。</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mn"><img src="../Images/2141e7487926ce5eb214d984be82f30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPqLQrYCYD3104zjvBntUA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">显示欠拟合、恰到好处和过拟合曲线之间基本区别的图表</figcaption></figure><blockquote class="mh mi mj"><p id="1535" class="lg lh mb li b lj mc kg ll lm md kj lo mk me lr ls ml mf lv lw mm mg lz ma lf ij bi translated">这里的第一种情况称为欠拟合，第二种是最佳拟合，最后一种是过拟合。</p></blockquote><p id="25f0" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">我们可以从第一个图表中看到，拟合线未能正确区分类别，这导致了拟合不足，这意味着拟合曲线未能正确解释数据的方差。</p><p id="de2c" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">最后一个已经很好地拟合了数据，这对于我们的情况应该是理想的。但是这个力拟合曲线有一个问题。这也许能够解释我们模型的潜在差异，但是当我们用它来处理看不见的数据时，这将会彻底失败。简而言之，它将无法在看不见的数据上区分类别，从而给我们错误的预测。</p><blockquote class="mh mi mj"><p id="962f" class="lg lh mb li b lj mc kg ll lm md kj lo mk me lr ls ml mf lv lw mm mg lz ma lf ij bi translated"><em class="jf">我们认为第二张图是被选中的图，尽管它不能正确预测所有类别，因为当它需要根据看不见的测试数据对类别进行正确分类时，这张图会失败。</em></p></blockquote><p id="b6e9" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">现在，所有这些都可以归结为我们在机器学习领域经常听到的术语，即<a class="ae ms" href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229" rel="noopener" target="_blank"> <strong class="li jg"> <em class="mb">偏差与方差权衡。</em> </strong> </a></p><p id="ad45" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">为了建立一个好的模型，我们需要在偏差和方差之间找到一个好的平衡，使总误差最小化。</p><h2 id="8806" class="mt mu jf bd mv mw mx dn my mz na dp nb lp nc nd ne lt nf ng nh lx ni nj nk nl bi translated"><strong class="ak">该如何……？</strong></h2><p id="7056" class="pw-post-body-paragraph lg lh jf li b lj nm kg ll lm nn kj lo lp no lr ls lt np lv lw lx nq lz ma lf ij bi translated">让我们看看它的数学形式。</p><p id="c4b4" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">最小二乘法的等式可以表示为:</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/38dfd2bc1155634232cfc88e0fe31175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LtekSwArEbAzYqMLiwiF9w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">普通最小二乘方程</figcaption></figure><p id="3a6a" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">其中<strong class="li jg"> <em class="mb"> y </em> </strong>为因变量，<strong class="li jg">T5】yhat</strong>为模型预测变量。<strong class="li jg"> <em class="mb"> w </em> </strong>代表我们渐变中的权重。现在，最小二乘法会给我们一个模型，它是我们总体均值的预期方差和预测方差之差。如果差异很大，就会导致我们模型中的高方差，我们的目标是减少这种差异。这里需要选择一个模型来满足<em class="mb">偏差与方差问题。</em></p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/c00e78c1baba385005ccf77ad1e19259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQqy1iKkVgVnTYV_VaX3zQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">偏差-方差的影响</figcaption></figure><blockquote class="mh mi mj"><p id="637e" class="lg lh mb li b lj mc kg ll lm md kj lo mk me lr ls ml mf lv lw mm mg lz ma lf ij bi translated">为了减少这种折衷的影响，我们使用了<strong class="li jg"> <em class="jf">正则化的概念。</em> </strong></p></blockquote><p id="9d17" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">正则化试图通过简化估计量来减少估计量的方差，这将增加偏差，以这种方式减少期望误差。</p><blockquote class="kv"><p id="d197" class="kw kx jf bd ky kz nt nu nv nw nx lf dk translated">根据维基百科，正则化“指的是引入额外信息以解决不适定问题或防止过度拟合的过程”。</p></blockquote><p id="dd8b" class="pw-post-body-paragraph lg lh jf li b lj lk kg ll lm ln kj lo lp lq lr ls lt lu lv lw lx ly lz ma lf ij bi translated">我们知道，一个线性回归的一般方程可以表示如下:-</p><p id="43f5" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><strong class="li jg"><em class="mb">Y≈β0+β1 x1+β2 x2+…+βpXp</em></strong></p><p id="a5d8" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">其中β是独立变量x的系数。现在，我们的目标是减少系数中的误差。这是通过使用残差平方和(RSS)来实现的。RSS的等式可以定义为:-</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/bd0155f36590e7945eab986d4d8ea564.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*1k-5RT48P-hn0bz7XXqXPw.png"/></div></figure><p id="25a9" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">这将有助于调整数据的残差。如果有噪声，就不能很好地概括数据。这就是<strong class="li jg"> <em class="mb">正则化</em> </strong>开始实施的地方，我们试图通过引入收缩因子来最小化误差。</p><p id="a8a9" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><strong class="li jg">岭回归(L2范数):</strong></p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/91039c92d4619de1a84ee0c3d2346f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*iBn8qy31OpJGi3E4oo9tJQ.png"/></div></figure><p id="0fa7" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi oa translated">这个因素是一个λ，它是一个调整参数，用来降低我们模型的灵活性，并找到最佳参数。如果λ是零，它不会影响结果，我们会得到相同的结果。然而，如果λ趋向无穷大，我们将得到完全不适合我们需要的结果。</p><p id="ef67" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">因此，选择一个好的λ值对于我们找到正确的系数是至关重要的。这就是<strong class="li jg"><em class="mb"/></strong>交叉验证<em class="mb"/><strong class="li jg"><em class="mb">派上用场的地方，它有助于估计测试集的误差，并决定哪些参数最适合模型</em>。</strong>用这种方法产生的系数估计值也被称为<strong class="li jg"><em class="mb"/></strong>。</p><p id="411f" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><strong class="li jg">拉索回归(L1常模)</strong>:</p><p id="8814" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">让我们看看另一种有助于减少方差的回归。</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e3cb8bcaec257685e6900ef4fa5d27ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*CrX3lZtwZxUe3LH-e9DCzQ.png"/></div></figure><p id="75b2" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi oa translated">asso只惩罚高系数。它使用|βj|(模数)而不是β的平方作为它的惩罚。在统计学上，这种<strong class="li jg"> <em class="mb">被称为L1常模</em> </strong>。</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/94be5f63aa92c72335bb775a4cbda589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*FtO-vlXhF-7f7AVmitp6Yw.png"/></div></figure><p id="02b0" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><strong class="li jg"> <em class="mb">蓝色的形状是指正则项，椭圆面是指我们的最小二乘误差(或数据项)。</em>T9】</strong></p><blockquote class="mh mi mj"><p id="baaf" class="lg lh mb li b lj mc kg ll lm md kj lo mk me lr ls ml mf lv lw mm mg lz ma lf ij bi translated">岭回归可以看作是求解一个方程，其中系数的平方和小于或等于s，而Lasso可以看作是一个方程，其中系数的模数之和小于或等于s。</p></blockquote><p id="7620" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><em class="mb">此处</em> <strong class="li jg"> <em class="mb"> s </em> </strong> <em class="mb">为收缩因子λ的各值存在的常数。</em></p><p id="4289" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">因此，如果我们考虑有2个参数，并且如果我们查看更广泛的图片，我们可以看到<strong class="li jg"> <em class="mb">岭回归可以由β + β ≤ s </em> </strong>表示。</p><p id="300a" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">而对于套索，我们考虑模数，所以<strong class="li jg"> <em class="mb">这个方程就变成了，</em> </strong></p><p id="2a8f" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><strong class="li jg"> <em class="mb"> |β1|+|β2|≤ s. </em> </strong></p><blockquote class="mh mi mj"><p id="fdd5" class="lg lh mb li b lj mc kg ll lm md kj lo mk me lr ls ml mf lv lw mm mg lz ma lf ij bi translated">现在Ridge有一个问题，就是模型的可解释性。它将缩小最不重要的预测因子的系数，非常接近于零。但这永远不会使它们完全为零。换句话说，最终的模型将包括所有的预测值。然而，在套索的情况下，当调谐参数λ是suﬃciently大时，L1罚函数具有迫使一些coeﬃcient估计恰好等于零的eﬀect。<strong class="li jg">因此，套索法也执行变量选择，据说能产生稀疏模型。</strong></p></blockquote><h2 id="4d0c" class="mt mu jf bd mv mw mx dn my mz na dp nb lp nc nd ne lt nf ng nh lx ni nj nk nl bi translated">正规化实现了什么？</h2><p id="764a" class="pw-post-body-paragraph lg lh jf li b lj nm kg ll lm nn kj lo lp no lr ls lt np lv lw lx nq lz ma lf ij bi oa translated"><span class="l ob oc od bm oe of og oh oi di"/>标准最小二乘模型往往会有一些偏差。<strong class="li jg"> <em class="mb">正则化，显著降低了模型的方差，而没有大幅增加其偏差。</em> </strong></p><p id="e7c2" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">因此，这一切都归结于调谐参数λ，它控制着偏差和方差的影响。随着λ值的增加，系数减小，从而方差减小。λ的增加仅在某一点之前是有益的，因为在某一点之后，如果我们继续增加其值，模型将开始失去重要的属性，从而增加模型中的偏差，导致拟合不足。</p><h1 id="0f4e" class="ol mu jf bd mv om on oo my op oq or nb kl os km ne ko ot kp nh kr ou ks nk ov bi translated">典型使用案例</h1><ul class=""><li id="712c" class="ow ox jf li b lj nm lm nn lp oy lt oz lx pa lf pb pc pd pe bi translated"><strong class="li jg">脊:</strong>主要用于<em class="mb">防止过配合</em>。因为它包括所有的特性，所以在过高的#特性的情况下，比如说数百万，它不是很有用，因为它会带来计算上的挑战。</li><li id="a4d3" class="ow ox jf li b lj pf lm pg lp ph lt pi lx pj lf pb pc pd pe bi translated"><strong class="li jg"> Lasso: </strong>因为它提供了<em class="mb">稀疏解</em>，所以它通常是对#特征数以百万计或更多的情况建模的首选模型(或这一概念的某种变体)。在这种情况下，获得稀疏解具有很大的计算优势，因为可以简单地忽略具有零系数的特征。</li></ul><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pk"><img src="../Images/5235eb5b359e0af4129489384ce64c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bv7VNj3AcsdFA-sZuPrwSw.png"/></div></div></figure><p id="403f" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><strong class="li jg">弹力网</strong></p><p id="6c3a" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">L1和L2范数的结合给了我们一个更好的正则化实现。</p><p id="4f77" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><strong class="li jg">βˇ= arg minβy Xβ2+λ2 |β2 |+λ1 |β1 |</strong></p><p id="72e9" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">那么我们如何调整<strong class="li jg"> λs </strong>来控制L1和L2的罚项呢？让我们举个例子来理解。</p><p id="4d48" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">假设，我们正试图从池塘里抓鱼。我们只有一张网，那我们该怎么办？我们会随机撒网吗？不，我们实际上会等到你看到一条鱼游来游去，然后我们会向那个方向撒网，基本上收集整个群体的鱼。因此，即使他们是相关的，我们仍然想看看他们的整个群体。</p><p id="537b" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi oa translated">弹性回归以类似的方式工作。比方说，我们在一个数据集中有一堆相关的独立变量，那么弹性网络将简单地形成一个由这些相关变量组成的组。现在，如果这组变量中的任何一个是强预测变量(意味着与因变量有很强的关系)，那么我们将在模型构建中包括整个组，因为省略其他变量(就像我们在lasso中所做的那样)可能会导致在解释能力方面丢失一些信息，从而导致模型性能不佳。</p><p id="3154" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><strong class="li jg">结论</strong></p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pl"><img src="../Images/c46e8f4a7941328d2363ef4599ea2a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbpTl6Tff9mDpyXBe7GD4A.png"/></div></div></figure><p id="7804" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">正如我们所看到的，Lasso帮助我们进行偏差-方差权衡，同时帮助我们进行重要的特征选择。然而，Ridge只能收缩接近于零的系数。</p><p id="e64f" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated"><strong class="li jg">供进一步阅读:- </strong></p><div class="ip iq gp gr ir pm"><a href="https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a" rel="noopener follow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd jg gy z fp pr fr fs ps fu fw je bi translated">机器学习中的正则化</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">训练机器学习模型的一个主要方面是避免过度拟合。该模型将有一个低…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">towardsdatascience.com</p></div></div><div class="pv l"><div class="pw l px py pz pv qa ix pm"/></div></div></a></div><div class="ip iq gp gr ir pm"><a href="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd jg gy z fp pr fr fs ps fu fw je bi translated">线性、脊形和套索回归初学者综合指南</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">我和我的一个朋友聊天，他碰巧是一家超市的运营经理…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="pv l"><div class="qb l px py pz pv qa ix pm"/></div></div></a></div><p id="1d98" class="pw-post-body-paragraph lg lh jf li b lj mc kg ll lm md kj lo lp me lr ls lt mf lv lw lx mg lz ma lf ij bi translated">暂时就这些了。下次见…！！干杯..！！！</p></div></div>    
</body>
</html>