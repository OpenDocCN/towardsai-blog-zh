# 如何训练你的 AI 龙(安全，合法，不带偏见)

> 原文：<https://pub.towardsai.net/how-to-train-your-ai-dragon-safely-legally-and-without-bias-2c4efe40b6c0?source=collection_archive---------0----------------------->

## 人工智能中的安全训练原则| [走向人工智能](https://towardsai.net/)

## 如何为人工智能算法正确获取数据并减少偏差

![](img/5db3ffbadb862f358f1479679108d788.png)

就像梦工厂 2010 年的电影《驯龙高手》中的龙一样，人工智能系统通常是……[+]梦工厂动画

未经训练的龙会造成很大的伤害。同样，随着人工智能系统进一步传播并对我们的生活产生更大影响，确保它们得到适当培训变得更加重要。**偏见可以很容易地潜入人工智能的推理中**，要么通过不够多样化的数据集，要么通过附加到可行数据点的无关数据，导致有缺陷的结果，在某些情况下，会得出偏见或危险的结论。

尽管像 GDPR 这样的法规保护我们的数据隐私，但个人消费者数据正越来越多地被公司用来改善服务或获得客户洞察力。具有讽刺意味的是，这些规定也使公司更难收集足够的数据来训练一个人工智能系统，或证明他们的人工智能如何达到其决策(这对许多深度学习系统来说是一项不可能的任务)。

因此，随着人工智能的发展及其能力的增长，**在不违反数据法规的情况下收集有用的数据将是至关重要的，以确保人工智能能够做出正确的决定**，并且个人和敏感数据不会在错误的上下文中使用。

# **安全地获取数据**

随着如此多的数据在网络空间流动，公司正在使用越来越多的粒度指标来衡量我们的行为并改善他们的服务。然而，[](https://gdpr-info.eu/)*【GDPR】的一般数据保护条例允许公司仅在征得个人明确同意的情况下，或者“如果出于公司追求合法利益的目的而有必要”，数据匿名化公司 [Statice](https://www.statice.ai/) 的首席执行官塞巴斯蒂安·文耶说。因为《GDPR》第 6 条(概述了合规数据处理的要求)对“合法利益”一词的解释是开放的，这意味着公司“在获得数据主体的直接同意时是最安全的，”Weyer 说。*

*然而，由于围绕公司使用我们的数据 a 的[*的不信任气氛，怀尔指出，大多数客户“*不会同意将他们的数据用于产品测试和创新*，这限制了可用于训练人工智能和改进产品的有用数据的数量。这种“*缺乏关于数据在构建个性化产品和服务中的重要性的教育*”可能会扼杀人工智能的创新，Weyer 认为，对数据泄露和商业滥用的担忧实际上可能会限制人工智能解决紧迫社会问题的能力。*](https://www.forbes.com/sites/charlestowersclark/2019/01/23/the-ethics-of-data-governance-data-comes-with-benefits-and-liabilities/)*

> *构建人工智能产品和服务的公司也需要对其自动化数据的使用保持透明，这并不总是像听起来那么容易。*

*机器学习系统以令人难以置信的复杂方式使用数据，最先进的算法通常“*牺牲可解释性来换取性能*”Weyer 说。然而，GDPR 第 15 条要求公司必须能够向数据主体解释其算法的基本功能，以表明其数据是如何被使用的。*

*因此，从数据集中删除所有识别信息，即**数据匿名化**，在收集数据时非常重要，因为它允许从数据集中收集有用的信息，而不会损害数据隐私法规。例如，Statice 创建一个合成数据集，该数据集遵循原始数据集的相同结构和统计属性，但没有附加任何识别信息。适当的数据匿名化不仅是收集数据时 GDPR 的要求，也有助于准确训练人工智能系统。“如果数据在用于建立机器学习模型之前没有正确匿名，学习到的模式可能会涉及敏感信息，*”Weyer 说。这是因为算法通过识别数据中的模式来工作——如果数据集中有无关的数据，如一个人的年龄、种族或地址，那么模式可以在这些因素而不是相关数据之间绘制。**

# ***训练数据***

*除了适当地匿名敏感信息之外，为特定算法获取正确的训练数据是至关重要的；事实上， [*数据可以看作是一个 AI 系统*](https://towardsdatascience.com/ai-ml-practicalities-the-unreasonable-effectiveness-of-data-c0bfd44c5057) 最重要的部分。不完整的数据集，具有过度或不足表示的元素，或者具有太多不相关的信息，很容易扭曲人工智能系统的推理。这在 [*有缺陷的刑事累犯制度* s](https://www.technologyreview.com/s/612775/algorithms-criminal-justice-ai/) 中得到了明显的证明，该制度表明，由于历史上有偏见的培训数据，非裔美国人比他们的白人同行更有可能重新犯罪。但是，从数据集中消除偏见并不容易，部分原因是历史不平等等问题，或者是因为数据集中缺乏多样性。Samasource 的创始人兼首席执行官 Leila Janah 说:“你几乎总是会从一些元素的代表性过高和其他元素的代表性过低开始，”但如果没有适当的测试和审查，“不具包容性和多样性的数据集可能会导致涉及种族、性别和文化偏见的问题。”*

*然而，偏见的问题不仅仅是表面的，对于像具有不同训练的自动驾驶汽车这样的图像识别系统来说，数据是首要的安全问题。“用于训练算法的数据是一个重要的组成部分，可以确保它能够适当地从停止标志中识别行人，从树上识别停止标志，”Janah 说。例如， [*数据集由肤色较深的人*](https://www.consumeraffairs.com/amp/news/whose-lives-matter-to-self-driving-cars-043019.html) 表示不足，这可能导致自动驾驶车辆不太可能“看到”肤色较深的行人过马路。虽然这似乎是一个极端的例子，但随着人工智能在越来越多的任务关键型应用程序中的使用，思考代表性数据集的重要性是恰当的。雇用一个多元化的团队来注释训练数据(如 Samasource 所做的那样)有助于确保所有相关的指标都得到考虑，文化偏见不会无意中进入系统，并且数据集能够代表一般人群。*

> *正确注释和适当匿名的训练数据也是训练 AI 时的良好实践。*

*从模型中删除不相关的信息，并确保训练数据尽可能多样化和具有代表性，这为算法提供了做出适当决策的最佳工具。Janah 说:“对于一个给定的问题，你能捕捉的情况越多，你就越有机会建立一个全面、可靠的模型。在寻找适当的训练数据时，这也是一个知道你想从系统中得到什么，并确保不存在误导性数据的问题——例如，如果你正在训练一个人工智能去 [*寻找肺癌结节*](https://www.forbes.com/sites/charlestowersclark/2019/04/30/the-cutting-edge-of-ai-cancer-detection/) ，那么包括肝癌筛查是没有帮助的。总的来说，在选择训练数据时采用适当的偏差预防策略与收集的数据量同等重要，因为偏差会在人工智能的整个计算过程中造成损害。Janah 认为“在生产之前、之后和整个生产过程中仔细测试你的模型的偏差将有助于你的模型走向成熟。”*

# ***数据中的魔鬼***

*尽管一家人工智能公司的表现往往被归因于其算法的复杂性，但决定一个人工智能系统成败的力量在于训练它的数据。不恰当地处理敏感数据不仅会导致公关噩梦，还会通过允许算法在不相关的数据之间绘制模式，从本质上破坏算法的推理。尽管法律要求正确匿名化数据(至少在欧盟)，但确保在训练算法之前从数据集中删除识别数据也是一种良好的做法，这样就不会出现偏见。*

*我们的生活变得越来越自动化，现在大多数人每小时都在与人工智能系统互动，不管我们是否意识到这一点。在这种背景下，**我们必须保持警惕，保护个人的数据隐私权，并确保歧视性的人工智能不会因有偏见的训练数据而在世界上传播**。人工智能每天都变得越来越强大，适当的数据管理和评估将成为对训练不足的系统的致命后果的制衡。*

**原载于*[](https://www.forbes.com/sites/charlestowersclark/2019/10/12/how-to-train-your-ai-dragon-safely-legally-and-without-bias/)**。***