<html>
<head>
<title>DropBlock: A New Regularization Technique</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DropBlock:一种新的正则化技术</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/dropblock-a-new-regularization-technique-e926bbc74adb?source=collection_archive---------1-----------------------#2021-03-29">https://pub.towardsai.net/dropblock-a-new-regularization-technique-e926bbc74adb?source=collection_archive---------1-----------------------#2021-03-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f651" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="28f5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">探索DropBlock，一种新的卷积神经网络正则化技术</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/18f345af3ecea89a4b2752867cc6ab5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*2X8v21EfOTY_S2L1kXV1iA.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated"><a class="ae ld" href="https://arxiv.org/pdf/1810.12890.pdf" rel="noopener ugc nofollow" target="_blank"> DropBlock:卷积网络的正则化方法</a></figcaption></figure><p id="d616" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">在这里我们将探索</p><ul class=""><li id="b01a" class="ma mb it lg b lh li lk ll ln mc lr md lv me lz mf mg mh mi bi translated"><strong class="lg jd"> <em class="mj">一种正则化技术和不同的正则化技术，如L1、L2正则化、剔除和空间剔除。</em> </strong></li><li id="1835" class="ma mb it lg b lh mk lk ml ln mm lr mn lv mo lz mf mg mh mi bi translated"><strong class="lg jd"> <em class="mj">什么是DropBlock，和Dropout </em> </strong>有什么不同</li><li id="2a69" class="ma mb it lg b lh mk lk ml ln mm lr mn lv mo lz mf mg mh mi bi translated"><strong class="lg jd"> <em class="mj">比较CIFAR-10数据集上的Dropout和DropBlock结果</em> </strong></li></ul><blockquote class="mp"><p id="012f" class="mq mr it bd ms mt mu mv mw mx my lz dk translated">正则化是在深度神经网络中实现的策略，它将减少泛化误差而不是训练误差，以便不仅在训练数据上，而且在新的未知输入上表现良好。</p></blockquote><p id="3155" class="pw-post-body-paragraph le lf it lg b lh mz kd lj lk na kg lm ln nb lp lq lr nc lt lu lv nd lx ly lz im bi translated"><strong class="lg jd">eﬀective正则化器显著降低了方差，同时不会过度增加偏差，从而防止过拟合</strong>。</p><p id="7dc7" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">我们使用<a class="ae ld" href="https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2" rel="noopener">正则化</a>技术，如L1和L2，以减少过度拟合，惩罚损失函数，或正则化技术，如<a class="ae ld" href="https://medium.com/analytics-vidhya/neural-network-and-dropouts-b6690c869a18" rel="noopener">漏失</a>和空间漏失，这阻碍了模型的复杂性。</p><p id="b2e9" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">神经网络中正则化方法背后的原理是将噪声注入到神经网络中，以避免过拟合训练数据。</p><h2 id="0c42" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated"><strong class="ak"> L2正规化</strong></h2><p id="7ee1" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated">L2正则化通常被称为<strong class="lg jd">权重衰减或岭回归，或吉洪诺夫正则化</strong>。这种正则化策略<strong class="lg jd">驱动权重变小，但不使它们为零</strong>，并且进行非稀疏求解。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ob"><img src="../Images/73587b8d2890db56e8ffc398873bf302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fy8WSd5d_Fn_EtYq2YBXEQ.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">L2正则化</figcaption></figure><p id="97a1" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd"> L2项是一个权重矩阵的所有平方权重值的总和</strong>。</p><p id="f0a9" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd"> L2对异常值并不稳健</strong>，因为以红色突出显示的平方项放大了异常值的误差差异，正则化项试图通过惩罚权重来解决这个问题。</p><h2 id="0bba" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">L1正则化</h2><p id="0c88" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated">L1正则化也被称为<strong class="lg jd"> L1范数或拉索</strong>。<strong class="lg jd"> Lasso生成一个简单、可解释的模型，并包含输入特征的子集</strong>。</p><p id="53eb" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">在L1范数中，我们将参数收缩为零</strong>。当输入要素的权重接近零时，会导致稀疏L1范数。在大多数输入的稀疏解决方案中，要素的权重为零，极少数要素的权重不为零。<strong class="lg jd">无关紧要的输入特征权重为零，而有用的特征被赋予非零权重</strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi og"><img src="../Images/411c472969f94c9ea3c37751f4e1405e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P4A4iS8cUXrvX9ZMo_OBGA.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">L1常模</figcaption></figure><p id="1ae1" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd"> L1正则化惩罚权重的绝对值</strong>，如上所示。</p><h2 id="463b" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">拒绝传统社会的人</h2><p id="b225" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated"><strong class="lg jd"> Dropout是一种减少过度拟合的技术，我们在神经网络中随机丢弃隐藏和可见的单元。当我们在神经网络中删除一个单元时，</strong>我们暂时从网络中删除这个单元，连同它所有的输入和输出连接，如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi oh"><img src="../Images/64ce0d4fb3929f3be25827a82e00b9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wc4nyV6lXDuEKtfnTHbpHA.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">来源:<a class="ae ld" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a></figcaption></figure><p id="1009" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">当我们将退出应用于神经网络时，它是一个由所有退出后幸存的单元组成的稀疏网络。</p><h2 id="9ff2" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">空间辍学</h2><p id="63e7" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated"><strong class="lg jd">卷积神经网络使用丢弃的另一种方法是从卷积层</strong>中丢弃整个特征图，然后在汇集过程中不使用。</p><p id="3497" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">空号掉线比掉线好，但比掉线差</strong></p><h2 id="d628" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">辍学的弊端</h2><ul class=""><li id="7b2a" class="ma mb it lg b lh nw lk nx ln oi lr oj lv ok lz mf mg mh mi bi translated"><strong class="lg jd">卷积层的丢弃允许信息在不同层之间流动，因为单元被随机丢弃</strong>。特征在卷积层中是空间相关的。当我们在神经网络中随机丢弃单元时，输入信息仍然可以流向下一层，导致模型过拟合。</li><li id="1cbd" class="ma mb it lg b lh mk lk ml ln mm lr mn lv mo lz mf mg mh mi bi translated"><strong class="lg jd"> Dropout主要用于卷积网络的全连接层。</strong></li></ul><h2 id="5f7c" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">什么是DropBlock，它与Dropout有什么不同？</h2><blockquote class="mp"><p id="fb30" class="mq mr it bd ms mt ol om on oo op lz dk translated">DropBlock是一种结构化的删除形式，用于正则化卷积网络，其中要素地图的连续区域中的单元被一起删除。</p></blockquote><p id="abea" class="pw-post-body-paragraph le lf it lg b lh mz kd lj lk na kg lm ln nb lp lq lr nc lt lu lv nd lx ly lz im bi translated"><strong class="lg jd"> Drop Block受剪切数据扩充方法</strong>的启发，其中部分输入图像被置零。因此，<strong class="lg jd"> DropBlock丢弃相关区域中的特征。网络必须从别处寻找证据来配合数据。</strong></p><p id="b799" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">教孩子识别不同动物的图像时。我们隐藏不同动物图像的一小部分，从而确保孩子将注意力集中在不同的动物特征上，以识别它们，从而进行有力的学习。</p><p id="ab45" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">应用相同的前提，DropBlock删除连续的区域，以删除某些语义信息，如面部、腿部、尾部等。从而强制剩余单元学习用于分类输入图像的其他特征。</p><h2 id="fd73" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated"><strong class="ak">滑车工作</strong></h2><p id="3850" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated"><strong class="lg jd"> DropBlock有两个主要参数</strong></p><ul class=""><li id="d78a" class="ma mb it lg b lh li lk ll ln mc lr md lv me lz mf mg mh mi bi translated"><strong class="lg jd"> block_size </strong>:即将被丢弃的块的<strong class="lg jd">大小。当block_size = 1时，DropBlock类似于dropout，当block_size覆盖整个要素地图时，drop block类似于SpatialDropout。<strong class="lg jd"> Block_size应小于或等于特征图的大小。</strong></strong></li><li id="e3f4" class="ma mb it lg b lh mk lk ml ln mm lr mn lv mo lz mf mg mh mi bi translated"><strong class="lg jd"> γ:控制掉落多少激活单位</strong></li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi oq"><img src="../Images/906cf4f498dd6f3ee0f37337319f5ab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFlEmkb3sNrc-sjyUfioOA.png"/></div></div></figure><p id="2e67" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd"> keep_prob是和传统辍学一样保持一个单位的概率，其值在0.75-0.95之间</strong>。</p><p id="9bd2" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd"> feat_size是特征图的大小</strong></p><p id="32f8" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">对于我们定义了DropBlock的每个特征映射，随机抽样二进制掩码。二进制掩码M将具有值0或1。</p><p id="c2f2" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">对于二进制掩码的每个零位置M(i，j)，创建一个中心为M(i，j)的空间正方形掩码，宽度和高度将等于block_size。将正方形中M的所有值设置为零。</p><p id="0ed7" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">如果block_size设置为5，我们以神经元为中心分块一个5乘5的小块，如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/3509c9fde23d31156054e0860a14dcec.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*ZUQukcPYFr1NQCqY1GWq4g.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated"><strong class="bd ng">下拉块中的屏蔽取样</strong>。来源:<a class="ae ld" href="https://arxiv.org/pdf/1810.12890.pdf" rel="noopener ugc nofollow" target="_blank"> DropBlock:卷积网络的一种正则化方法</a></figcaption></figure><h2 id="f7a3" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">跌落滑车的强度</h2><p id="f61a" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated"><strong class="lg jd"> DropBlock从每一层中屏蔽掉部分信息，迫使每一个神经元学习不同的图像特征，帮助模型很好地泛化，避免过拟合</strong>。由DropBlock正则化的模型学习多个判别区域，而不是只关注一个判别区域。</p><p id="7df1" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">最好<strong class="lg jd">在训练过程中初始设置小的掉线率，然后在训练过程中随着时间线性增加</strong>。</p><p id="5c7b" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">与DropBlock相比，drop block在提高ImageNet分类准确性方面展示了强大的实证结果。</p><p id="77cf" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">丢失是不够的，因为卷积图层中的相邻区域具有很强的相关性，并且随机丢失一个单元仍允许信息流经相邻单元。</p><h2 id="a65e" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">DropBlock和Dropout的异同</h2><p id="12eb" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated">与dropout不同，DropBlock根据block_size和γ 随机删除层中的一个神经元。</p><p id="b4c1" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">DropBlock在丢弃语义信息方面更有效，因此比Dropout更健壮。用DropBlock训练的模型需要学习空间分布的表示，因为DropBlock有效地移除了连续区域中的语义信息。因此，用大块大小训练的模型移除了更多的语义信息，导致更强的正则化。</p><p id="238f" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd"> DropBlock可应用于卷积层和全连接层，而</strong> Dropout主要用于卷积网络的全连接层。</p><p id="f330" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">类似于dropout，我们<strong class="lg jd">在推理过程中不应用DropBlock。</strong></p><h2 id="b52f" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">CIFAR-10上的drop和DropBlock比较</h2><p id="3c02" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated">这里我们创建两个具有相似架构和超参数的CNN模型，但是一个模型使用dropout，另一个模型使用sDropBlock。最后，我们比较了两种模型的训练和验证数据集在准确率和损失方面的差异。</p><p id="4d2c" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">导入所需的库</strong></p><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="1a89" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">import tensorflow as tf<br/>from keras.datasets import cifar10<br/>from keras.preprocessing.image import ImageDataGenerator<br/>import numpy as np<br/>from tensorflow.python.keras import backend as K</strong></span></pre><p id="9670" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">创建DropBlock类</strong></p><ul class=""><li id="4255" class="ma mb it lg b lh li lk ll ln mc lr md lv me lz mf mg mh mi bi translated"><strong class="lg jd"> block_size </strong>:即将被丢弃的块的<strong class="lg jd">大小。</strong></li><li id="ded9" class="ma mb it lg b lh mk lk ml ln mm lr mn lv mo lz mf mg mh mi bi translated"><strong class="lg jd"> keep_prob是像传统辍学一样保持一个单位的概率。</strong>keep _ prob的值介于0.75和0.95之间，并且<strong class="lg jd">将与features_size和block_size一起用于计算gamma(γ)以控制丢弃多少激活单元</strong></li></ul><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="0699" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">class DropBlock2D(tf.keras.layers.Layer):</strong><br/>    """See: <a class="ae ld" href="https://arxiv.org/pdf/1810.12890.pd" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.12890.pd</a><br/>    """</span><span id="3efc" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def __init__(self,<br/>                 block_size,<br/>                 keep_prob,<br/>                 sync_channels=False,<br/>                 data_format=None,<br/>                 **kwargs):</strong><br/>        """Initialize the layer.<br/>        :param block_size: Size for each mask block.<br/>        :param keep_prob: Probability of keeping the original feature.<br/>        :param sync_channels: Whether to use the same dropout for all channels.<br/>        :param data_format: 'channels_first' or 'channels_last' (default).<br/>        :param kwargs: Arguments for parent class.<br/>        """<br/>        <strong class="ot jd">super(DropBlock2D, self).__init__(**kwargs)<br/>        self.block_size = block_size<br/>        self.keep_prob = keep_prob<br/>        self.sync_channels = sync_channels<br/>        self.data_format = data_format<br/>        self.supports_masking = True<br/>        self.height = self.width = self.ones = self.zeros = None</strong></span><span id="c9ea" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def build(self, input_shape):<br/>        if self.data_format == 'channels_first':<br/>            self.height, self.width = input_shape[2], input_shape[3]<br/>        else:<br/>            self.height, self.width = input_shape[1], input_shape[2]<br/>        self.ones = K.ones((self.height, self.width), name='ones')<br/>        self.zeros = K.zeros((self.height, self.width), name='zeros')<br/>        super().build(input_shape)</strong></span><span id="fd2f" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def get_config(self):<br/>        config = {'block_size': self.block_size,<br/>                  'keep_prob': self.keep_prob,<br/>                  'sync_channels': self.sync_channels,<br/>                  'data_format': self.data_format}<br/>        base_config = super(DropBlock2D, self).get_config()<br/>        return dict(list(base_config.items()) + list(config.items()))</strong></span><span id="ec23" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def compute_mask(self, inputs, mask=None):<br/>        return mask</strong></span><span id="bdb0" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def compute_output_shape(self, input_shape):<br/>        return input_shape</strong></span><span id="c143" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def _get_gamma(self):</strong><br/>        """Get the number of activation units to drop"""<br/>       <strong class="ot jd"> height, width = K.cast(self.height, K.floatx()), K.cast(self.width, K.floatx())<br/>        block_size = K.constant(self.block_size, dtype=K.floatx())<br/>        return ((1.0 - self.keep_prob) / (block_size ** 2)) *\<br/>               (height * width / ((height - block_size + 1.0) * (width - block_size + 1.0)))</strong></span><span id="5efc" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def _compute_valid_seed_region(self):<br/>        positions = K.concatenate([<br/>            K.expand_dims(K.tile(K.expand_dims(K.arange(self.height), axis=1), [1, self.width]), axis=-1),<br/>            K.expand_dims(K.tile(K.expand_dims(K.arange(self.width), axis=0), [self.height, 1]), axis=-1),<br/>        ], axis=-1)<br/>        half_block_size = self.block_size // 2<br/>        valid_seed_region = K.switch(<br/>            K.all(<br/>                K.stack(<br/>                    [<br/>                        positions[:, :, 0] &gt;= half_block_size,<br/>                        positions[:, :, 1] &gt;= half_block_size,<br/>                        positions[:, :, 0] &lt; self.height - half_block_size,<br/>                        positions[:, :, 1] &lt; self.width - half_block_size,<br/>                    ],<br/>                    axis=-1,<br/>                ),<br/>                axis=-1,<br/>            ),<br/>            self.ones,<br/>            self.zeros,<br/>        )<br/>        return K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)</strong></span><span id="d876" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def _compute_drop_mask(self, shape):<br/>        mask = K.random_binomial(shape, p=self._get_gamma())<br/>        mask *= self._compute_valid_seed_region()<br/>        mask = tf.keras.layers.MaxPool2D(<br/>            pool_size=(self.block_size, self.block_size),<br/>            padding='same',<br/>            strides=1,<br/>            data_format='channels_last',<br/>        )(mask)<br/>        return 1.0 - mask</strong></span><span id="01b4" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def call(self, inputs, training=None):</strong></span><span id="77f7" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">def dropped_inputs():<br/>            outputs = inputs<br/>            if self.data_format == 'channels_first':<br/>                outputs = K.permute_dimensions(outputs, [0, 2, 3, 1])<br/>            shape = K.shape(outputs)<br/>            if self.sync_channels:<br/>                mask = self._compute_drop_mask([shape[0], shape[1], shape[2], 1])<br/>            else:<br/>                mask = self._compute_drop_mask(shape)<br/>            outputs = outputs * mask *\<br/>                (K.cast(K.prod(shape), dtype=K.floatx()) / K.sum(mask))<br/>            if self.data_format == 'channels_first':<br/>                outputs = K.permute_dimensions(outputs, [0, 3, 1, 2])<br/>            return outputs</strong></span><span id="5c34" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">return K.in_train_phase(dropped_inputs, inputs, training=training)</strong></span></pre><p id="2f22" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">创建CIFAR-10训练和测试数据集</strong></p><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="a3e0" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">def normalize(X_train,X_test)</strong>:<br/>        #normalizes inputs for zero mean and unit variance<br/>        # it is used when training a model.<br/>        # Input: training set and test set<br/>        # Output: normalized training set and test set according to the training set statistics.<br/>      <strong class="ot jd">  mean = np.mean(X_train,axis=(0,1,2,3))<br/>        std = np.std(X_train, axis=(0, 1, 2, 3))<br/>        X_train = (X_train-mean)/(std+1e-7)<br/>        X_test = (X_test-mean)/(std+1e-7)<br/>        return X_train, X_test</strong></span><span id="fc05" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">(x_train, y_train), (x_test, y_test) = cifar10.load_data()<br/>x_train = x_train.astype('float32')<br/>x_test = x_test.astype('float32')<br/>x_train, x_test = normalize(x_train, x_test)<br/>y_train = tf.keras.utils.to_categorical(y_train, 10)<br/>y_test = tf.keras.utils.to_categorical(y_test, 10)</strong></span></pre><p id="1b0f" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">构建辍学CNN模型</strong></p><p id="38ad" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">这个模型有4.97米的参数</p><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="6b63" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">def build_dropout_model():</strong><br/>        # Build the CNN for 10 classes with massive dropout and weight decay</span><span id="976d" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model = tf.keras.Sequential()<br/>        weight_decay = 0.0005<br/>        x_shape = (32,32,3)<br/>        num_classes=10<br/>        <br/>        model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same',input_shape=x_shape,<br/>                         kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())<br/>        model.add(tf.keras.layers.Dropout(0.4))<br/>        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))<br/>        model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())</strong></span><span id="7f3d" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))</strong></span><span id="427e" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model.add(tf.keras.layers.Dropout(0.4))</strong></span><span id="f218" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model.add(tf.keras.layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())<br/>        model.add(tf.keras.layers.Dropout(0.4))</strong></span><span id="9639" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model.add(tf.keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())<br/>        model.add(tf.keras.layers.Dropout(0.4))<br/>        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))<br/>        model.add(tf.keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())<br/>        model.add(tf.keras.layers.Dropout(0.4))<br/>       <br/>        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))<br/>        model.add(tf.keras.layers.Flatten())<br/>        model.add(tf.keras.layers.Dense(512,kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.Dropout(0.5))<br/>        model.add(tf.keras.layers.Dense(num_classes))<br/>        model.add(tf.keras.layers.Activation('softmax'))<br/>        return model</strong></span></pre><h2 id="5f3d" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">建筑工地</h2><p id="69fb" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated">DropBlock CNN模型也有4.97M参数</p><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="3539" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">def build_droblock_model():</strong><br/>        # Build the Dropblock CNN network of vgg for 10 classes with massive dropout and weight decay </span><span id="337e" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model = tf.keras.Sequential()<br/>        weight_decay = 0.0005<br/>        x_shape = (32,32,3)<br/>        num_classes=10</strong><br/>        <br/>       <strong class="ot jd"> model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same',input_shape=x_shape,                         kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())<br/>        model.add(DropBlock2D(block_size=3, keep_prob=0.75))</strong>        <br/>       <strong class="ot jd"> model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))<br/>        model.add(DropBlock2D(block_size=3, keep_prob=0.2))<br/>        model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())<br/>        model.add(DropBlock2D(block_size=3, keep_prob=0.75))</strong></span><span id="85ef" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model.add(tf.keras.layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())<br/>        model.add(DropBlock2D(block_size=4, keep_prob=0.8))</strong></span><span id="b541" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))</strong></span><span id="5bea" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model.add(tf.keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())<br/>        model.add(DropBlock2D(block_size=3, keep_prob=0.8))<br/>model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))<br/>model.add(tf.keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())<br/>        model.add(DropBlock2D(block_size=3, keep_prob=0.8))<br/>model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))<br/>        model.add(DropBlock2D(block_size=3, keep_prob=0.9))<br/>model.add(tf.keras.layers.Flatten())<br/>model.add(tf.keras.layers.Dense(512,kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))<br/>        model.add(tf.keras.layers.Activation('relu'))<br/>        model.add(tf.keras.layers.BatchNormalization())</strong></span><span id="1a98" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">model.add(tf.keras.layers.Dense(num_classes))<br/>        model.add(tf.keras.layers.Activation('softmax'))<br/>        return model</strong></span></pre><p id="2ccb" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">创建模型</strong></p><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="ff7c" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">model_dropout = build_dropout_model()<br/>model_dropblock = build_droblock_model()</strong></span></pre><p id="0f6e" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">编译模型</strong></p><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="12c5" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">opt = tf.keras.optimizers.Adam(lr=0.001)</strong></span><span id="4179" class="ne nf it ot b gy pb oy l oz pa"># Compile Dropout Model<br/><strong class="ot jd">model_dropout.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])</strong></span><span id="bb77" class="ne nf it ot b gy pb oy l oz pa"># Compile Dropblock Model<br/><strong class="ot jd">model_dropblock.compile(optimizer=opt, loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])</strong></span></pre><p id="f134" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">创建检查点和提前停车</strong></p><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="fac2" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">checkpoint_dropout = tf.keras.callbacks.ModelCheckpoint("dropout_model.h5", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False,mode='auto', save_freq='epoch')y</strong></span><span id="187b" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">checkpoint_dropblock = tf.keras.callbacks.ModelCheckpoint("dropBlock_model.h5", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False,mode='auto', save_freq='epoch')</strong></span><span id="3eb2" class="ne nf it ot b gy pb oy l oz pa"><strong class="ot jd">early =  tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', <br/>                                          min_delta=0, <br/>                                          patience=20, <br/>                                          verbose=1, mode='auto')</strong></span></pre><p id="d13b" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">拟合辍学模型</strong></p><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="3364" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">history = model_dropout.fit(x_train, y_train, epochs=20, batch_size=64, <br/>                    validation_data=(x_test, y_test), <br/>                    verbose=1,<br/>                    validation_steps=10,<br/>                    validation_batch_size=32,<br/>                    callbacks=[checkpoint_dropout,early])</strong></span></pre><p id="1569" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg jd">安装下拉模块型号</strong></p><pre class="ks kt ku kv gt os ot ou ov aw ow bi"><span id="ec1f" class="ne nf it ot b gy ox oy l oz pa"><strong class="ot jd">history_2 = model_dropblock.fit(x_train, y_train, epochs=20, batch_size=64, <br/>                    validation_data=(x_test, y_test), <br/>                    verbose=1,<br/>                    validation_steps=10,<br/>                    validation_batch_size=32,<br/>                    callbacks=[checkpoint_dropblock,early])</strong></span></pre><p id="dfe9" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">最后，比较了drop和DropBlock模型的精度和损耗。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/74d8a2367435e0448b16e7f9d2960b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*7lZ9wRLUERLGy2bq2BusVw.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">Dropout和DropBlock模型精度和损耗比较</figcaption></figure><p id="3249" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">我们可以看到，DropBlock模型在20个时期的最佳验证精度约为84.68%，而Dropout模型的最佳验证精度为68.1%。</p><p id="c1f4" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">Dropout模型的最低验证损失为2.4066，DropBlock模型的最低验证损失为1.0977。</p><p id="1f58" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">因此，在数据集、架构和超参数相似的情况下，DropBlock优于Dropout。</p><h2 id="b20a" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated">结论:</h2><p id="ad9d" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated">DropBlock是一种结构化的退出形式，受剪切数据扩充的启发。它有助于正则化卷积网络，其中特征图的连续区域中的单元被放在一起，<strong class="lg jd"> </strong>迫使神经元学习不同的图像特征。这有助于模型更好地泛化，避免过度拟合。</p><h2 id="4840" class="ne nf it bd ng nh ni dn nj nk nl dp nm ln nn no np lr nq nr ns lv nt nu nv iz bi translated"><strong class="ak">参考文献:</strong></h2><p id="3de4" class="pw-post-body-paragraph le lf it lg b lh nw kd lj lk nx kg lm ln ny lp lq lr nz lt lu lv oa lx ly lz im bi translated"><a class="ae ld" href="https://arxiv.org/pdf/1810.12890.pdf" rel="noopener ugc nofollow" target="_blank"> DropBlock:卷积网络的正则化方法</a></p><p id="fbd7" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><a class="ae ld" href="https://www.deeplearningbook.org/contents/regularization.html" rel="noopener ugc nofollow" target="_blank">https://www . deep learning book . org/contents/regulation . html</a></p><p id="380b" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><a class="ae ld" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" rel="noopener ugc nofollow" target="_blank">退出:防止神经网络过度拟合的简单方法</a></p><p id="a95c" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><a class="ae ld" href="https://github.com/CyberZHG/keras-drop-block/blob/0edc5e3f3021c5f40722078c7c23e09e2b475a58/keras_drop_block/drop_block.py" rel="noopener ugc nofollow" target="_blank">https://github . com/CyberZHG/keras-drop-block/blob/0 edc5 e 3f 3021 C5 f 40722078 c 7 c 23 e 09 e 2 b 475 a 58/keras _ drop _ block/drop _ block . py</a></p></div></div>    
</body>
</html>