<html>
<head>
<title>Natural Language Clustering — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言聚类—第二部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/natural-language-clustering-part-2-3d2f93cc0eaa?source=collection_archive---------2-----------------------#2021-03-19">https://pub.towardsai.net/natural-language-clustering-part-2-3d2f93cc0eaa?source=collection_archive---------2-----------------------#2021-03-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="b3ed" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="2161" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">K-Means，高斯混合，DBSCAN和OPTICS比较，带Python代码示例</h2></div><p id="cfe4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们一直在研究如何聚类文本，当没有足够的数据、时间或计算能力来<a class="ae lk" href="https://huggingface.co/transformers/custom_datasets.html" rel="noopener ugc nofollow" target="_blank">微调</a>变压器模型时，这对于常见问题解答和文本分类很有用。</p><p id="dbce" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在第一部分(<a class="ae lk" rel="noopener ugc nofollow" target="_blank" href="/natural-language-clustering-part-1-32301e3125e5">此处可用</a>)我们已经看到了如何通过标记化和嵌入将文本转化为数字向量。现在我们的文本是机器可读的格式，我们可以处理我们的主要目标:聚类它们。</p><h1 id="7e47" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated">聚类方法</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/a187591a2e61ffd15aff08b2f8d702c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0mFFxjqeexVrhr_qecdHdw.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">一些可用的聚类方法—图片来自scikit-learn库<a class="ae lk" href="https://scikit-learn.org/stable/modules/clustering.html" rel="noopener ugc nofollow" target="_blank">文档</a></figcaption></figure><p id="73bf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">聚类是最有用的<a class="ae lk" href="https://en.wikipedia.org/wiki/Unsupervised_learning" rel="noopener ugc nofollow" target="_blank">无监督学习</a>任务之一，因此多年来已经开发了许多不同的方法来完成它。我们将简要说明我们尝试过的方法，比较它们的理论优缺点和我们的第一手经验。不幸的是(正如<a class="ae lk" href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" rel="noopener ugc nofollow" target="_blank">没有免费的午餐定理</a>所证明的),当在所有指标上进行比较时，没有一个算法比其他任何一个算法客观地执行得更好，因此最佳选择取决于您的使用案例和可用资源的具体情况。</p><h1 id="5e88" class="ll lm iq bd ln lo lp lq lr ls lt lu lv kf lw kg lx ki ly kj lz kl ma km mb mc bi translated">k均值</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mt"><img src="../Images/20821e9efb2186bcc98fd53cd73119f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x6DYAq6Ssg-6hpNNV4unEQ.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">k均值聚类的一个例子，质心是黑点。<a class="ae lk" href="https://www.geeksforgeeks.org/ml-k-means-algorithm/" rel="noopener ugc nofollow" target="_blank">来源:GeeksForGeeks </a></figcaption></figure><p id="0c88" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">也许是最著名的聚类算法，它是基于<a class="ae lk" href="https://en.wikipedia.org/wiki/Cluster_analysis#Centroid-based_clustering" rel="noopener ugc nofollow" target="_blank">质心的</a>，需要预先确定聚类的数量，这是它的主要缺点之一。也只有<a class="ae lk" href="https://stats.stackexchange.com/questions/48757/why-doesnt-k-means-give-the-global-minimum" rel="noopener ugc nofollow" target="_blank">保证找到</a>局部最优解(可能是也可能不是全局最优解)，因此需要多次随机初始化来找到最佳结果，并且总是聚类所有数据点，如果您希望您的算法让更多中间结果不被聚类，这是一个次优的解决方案(例如，在FAQ相关的场景中:我们宁愿让任何FAQ都无法回答的问题不被聚类，这样它们更容易识别和管理，而不是提供错误的答案)。<br/>它通常比下面引用的其他方法更快。</p><p id="35f2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">我们使用K-Means的经验:</strong>它对我们的目标没有太大作用:缺乏关于最佳聚类数的先验知识(尽管使用了<a class="ae lk" href="https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/" rel="noopener ugc nofollow" target="_blank">剪影和肘方法</a>来缓解这个问题)以及难以区分实际聚类和噪声通常会导致较差的性能。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="5e3f" class="nb lm iq bd ln nc nd dn lr ne nf dp lv kx ng nh lx lb ni nj lz lf nk nl mb iw bi translated">k-表示Python中的示例:</h2><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="e934" class="nb lm iq nn b gy nr ns l nt nu">from sklearn.cluster import KMeans<br/>import numpy as np</span><span id="21bf" class="nb lm iq nn b gy nv ns l nt nu">X = np.array([[1, 2], [2, 2], [2, 3],<br/>             [8, 7], [8, 8], [25, 80]])<br/>kmeans = KMeans(n_clusters=2, random_state=0).fit(X)</span><span id="16cf" class="nb lm iq nn b gy nv ns l nt nu">kmeans.labels_</span><span id="a984" class="nb lm iq nn b gy nv ns l nt nu"># OUTPUT:<br/># array([0, 0, 0, 0, 0, 1], dtype=int32)</span><span id="7c6f" class="nb lm iq nn b gy nv ns l nt nu"># k-means labels the first five points as cluster 0, the last one as cluster 1, as shown below</span></pre><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nw"><img src="../Images/228d54cc527d780967b4feafad3376b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5r2mZmvb-wP6u5tea8-jJg.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">紫色点:集群0；黄点:第一组</figcaption></figure></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="3ebf" class="ll lm iq bd ln lo nx lq lr ls ny lu lv kf nz kg lx ki oa kj lz kl ob km mb mc bi translated">高斯混合模型(GMM)</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/852931a9736fce979638d4fe268581ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*KMEwptvZziQZmLsBpsxgQg.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">来自两种不同高斯分布的数据。<a class="ae lk" href="https://scikit-learn.org/stable/modules/mixture.html" rel="noopener ugc nofollow" target="_blank">来源:Scikit-Learn文档</a></figcaption></figure><p id="c8dc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一个基于<a class="ae lk" href="https://en.wikipedia.org/wiki/Cluster_analysis#Distribution-based_clustering" rel="noopener ugc nofollow" target="_blank">分布的</a>算法，它基于<a class="ae lk" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">期望最大化</a>算法，该算法在高斯分布数据上运行良好。</p><blockquote class="od oe of"><p id="38f1" class="ko kp og kq b kr ks ka kt ku kv kd kw oh ky kz la oi lc ld le oj lg lh li lj ij bi translated"><a class="ae lk" href="https://scikit-learn.org/stable/modules/mixture.html#mixture" rel="noopener ugc nofollow" target="_blank">人们可以将</a>混合模型视为推广K均值聚类，以纳入有关数据协方差结构以及潜在高斯中心的信息</p></blockquote><p id="daae" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">像K-意味着它只找到局部最优解，高斯数必须预先指定，因此可能需要多次初始化来优化性能。</p><p id="6724" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">我们使用高斯混合模型的经验:</strong>通过设置相对较高数量的高斯分布，然后移除过于稀疏或过小的集群，我们能够分离出密度较高且彼此相距较远的特定集群，这对我们的任务非常有用。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="8135" class="nb lm iq bd ln nc nd dn lr ne nf dp lv kx ng nh lx lb ni nj lz lf nk nl mb iw bi translated">Python中的GMM示例:</h2><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="70db" class="nb lm iq nn b gy nr ns l nt nu">import numpy as np<br/>from sklearn.mixture import GaussianMixture</span><span id="378b" class="nb lm iq nn b gy nv ns l nt nu">X = np.array([[1, 2], [2, 2], [2, 3],<br/>             [8, 7], [8, 8], [25, 80]])</span><span id="7fec" class="nb lm iq nn b gy nv ns l nt nu">gm_labels = GaussianMixture(n_components=2, random_state=0).fit_predict(X)</span><span id="d134" class="nb lm iq nn b gy nv ns l nt nu">gm_labels</span><span id="f12e" class="nb lm iq nn b gy nv ns l nt nu"># OUTPUT:<br/># array([0, 0, 0, 0, 0, 1])</span><span id="2e3b" class="nb lm iq nn b gy nv ns l nt nu"># gm identifies the first five points as cluster 0 and the last one as cluster 1</span><span id="91bc" class="nb lm iq nn b gy nv ns l nt nu">gm = GaussianMixture(n_components=2, random_state=0).fit(X)</span><span id="0806" class="nb lm iq nn b gy nv ns l nt nu">gm.means_</span><span id="098a" class="nb lm iq nn b gy nv ns l nt nu"># OUTPUT:<br/># array([[ 4.2,  4.4],<br/>       [25. , 80. ]])</span><span id="26a9" class="nb lm iq nn b gy nv ns l nt nu"># by calling the fit method (instead of fit_predict) we obtain more information about the model, such as the means of the distributions</span></pre><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ok"><img src="../Images/95d17c79943dc6437370f893937b9611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nWxTussAIb-h6Ud4IJBppQ.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">根据预测标签着色的点(紫色= 0；黄色= 1)和高斯分布的平均值(略大，红色)</figcaption></figure></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="4f2c" class="ll lm iq bd ln lo nx lq lr ls ny lu lv kf nz kg lx ki oa kj lz kl ob km mb mc bi translated">基于密度的噪声应用空间聚类</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/b5dc93b56540a92304ec15d16a7db5d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*RQcfdnPrAI-c44PLShlkWQ.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener ugc nofollow" target="_blank">来源:维基百科</a></figcaption></figure><p id="e47e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">D</strong>en sity-<strong class="kq ja">B</strong>Based<strong class="kq ja">S</strong>partial<strong class="kq ja">C</strong>Clustering of<strong class="kq ja">A</strong>applications with<strong class="kq ja">N</strong>oise，顾名思义就是一种基于<a class="ae lk" href="https://blog.dominodatalab.com/topology-and-density-based-clustering/" rel="noopener ugc nofollow" target="_blank">密度的</a>聚类方法。它的主要参数(eps)是您设置的距离:基本上，彼此在该距离内的所有点都被识别为属于同一个聚类。所需的其他相关参数是距离度量和识别单独聚类所需的最小聚类数。<br/>它有很多优点:不像k-means和GMM，聚类的数目不需要先验确定；它可以找到任意形状的集群，并且对异常值具有鲁棒性。它的主要缺点是，鉴于其性质，它不能很好地聚类密度差异较大的数据集，这可能取决于我们正在处理的文本的质量和数量。</p><p id="d394" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">我们使用DBSCAN的经验:</strong>不同的主题可能有不同的密度，这一事实使它的表现次优:它大多找到了正确的主题，但许多文本没有正确聚类。光学(见下文)更适合我们正在处理的数据类型。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="a0d9" class="nb lm iq bd ln nc nd dn lr ne nf dp lv kx ng nh lx lb ni nj lz lf nk nl mb iw bi translated">Python中的DBSCAN示例，来自sklearn文档:</h2><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="36fd" class="nb lm iq nn b gy nr ns l nt nu">from sklearn.cluster import DBSCAN<br/>import numpy as np</span><span id="6474" class="nb lm iq nn b gy nv ns l nt nu">X = np.array([[1, 2], [2, 2], [2, 3],<br/>             [8, 7], [8, 8], [25, 80]])</span><span id="1614" class="nb lm iq nn b gy nv ns l nt nu">clustering = DBSCAN(eps=3, min_samples=2).fit(X)</span><span id="06fc" class="nb lm iq nn b gy nv ns l nt nu">clustering.labels_</span><span id="6f3f" class="nb lm iq nn b gy nv ns l nt nu"># OUTPUT:<br/># array([ 0,  0,  0,  1,  1, -1])</span><span id="a196" class="nb lm iq nn b gy nv ns l nt nu"># DBSCAN identifies the first three points belonging to cluster 0; the fourth and fifth belonging to cluster 1 and the last one alone in cluster -1</span></pre><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi om"><img src="../Images/fc61508191fdd0439a72c3655eea8b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wc7V5V_JGaEUiZLB4n-AGg.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">根据DBSCAN聚类着色的数据点</figcaption></figure></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="43ad" class="ll lm iq bd ln lo nx lq lr ls ny lu lv kf nz kg lx ki oa kj lz kl ob km mb mc bi translated">光学</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi on"><img src="../Images/674fafce21fac6866e6df3d836c0970c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b6j58_QGxQ891Jp046FFjw.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">对左上角的数据集运行光学系统的结果。下图:给出可达性距离的光学图。右上:聚类和聚类顺序(将每个点与其前一个点连接起来)。<a class="ae lk" href="https://en.wikipedia.org/wiki/OPTICS_algorithm" rel="noopener ugc nofollow" target="_blank">来源:维基百科</a></figcaption></figure><p id="74f9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">O</strong>ordering<strong class="kq ja">P</strong>points<strong class="kq ja">T</strong>O<strong class="kq ja">I</strong>identify<strong class="kq ja">C</strong>Clustering<strong class="kq ja">S</strong>structure，一种基于密度的聚类方法，类似于DBSCAN，但解决了在不同密度的数据中检测有意义的聚类的问题。鉴于其复杂性，它需要比DBSCAN更多的内存和计算能力。</p><p id="277a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">我们在光学方面的经验:</strong>超参数微调非常敏感，微小的变化会导致巨大的集群化差异。我们只能在一些数据集上得到好的结果。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="fbfa" class="nb lm iq bd ln nc nd dn lr ne nf dp lv kx ng nh lx lb ni nj lz lf nk nl mb iw bi translated">Python中的光学示例:</h2><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="6ec8" class="nb lm iq nn b gy nr ns l nt nu">from sklearn.cluster import OPTICS<br/>import numpy as np</span><span id="5aaf" class="nb lm iq nn b gy nv ns l nt nu">X = np.array([[1, 2], [2, 2], [2, 3],<br/>             [8, 7], [8, 8], [25, 80]])</span><span id="76c6" class="nb lm iq nn b gy nv ns l nt nu">clustering = OPTICS(min_samples=2).fit(X)</span><span id="742b" class="nb lm iq nn b gy nv ns l nt nu">clustering.labels_</span><span id="4141" class="nb lm iq nn b gy nv ns l nt nu"># OUTPUT:<br/># array([0, 0, 0, 1, 1, 1])</span><span id="46bd" class="nb lm iq nn b gy nv ns l nt nu"># OPTICS identifies the first three points as belonging to cluster 0, the last three to cluster 1</span></pre><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi oo"><img src="../Images/85dafef4382b4b6e76880707389ebacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxQnizvu-VHTbWlg_sqzOQ.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">根据光学聚类着色的数据点</figcaption></figure></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="4281" class="ll lm iq bd ln lo nx lq lr ls ny lu lv kf nz kg lx ki oa kj lz kl ob km mb mc bi translated">概述</h1><p id="651e" class="pw-post-body-paragraph ko kp iq kq b kr op ka kt ku oq kd kw kx or kz la lb os ld le lf ot lh li lj ij bi translated">我们已经看到了一些在文本语料库中识别聚类的可用方法。</p><p id="60d2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">文本首先被标记化，然后被嵌入，然后聚类方法被应用于嵌入，该聚类方法是根据我们的数据的大小和形状以及可用的时间和计算资源来选择的。</p><p id="a136" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这种方法比其他机器学习应用程序更少即插即用，需要投入一些时间和精力来找到最适合您特定用例的方法，但根据我们的经验，一旦正确设置，它可以在微调之前准确地聚类大约40%的文本。请注意，即使在代码片段中提供的简单示例中，只有六个点进行聚类，也只有K-Means和GMM表现相同，而DBSCAN和OPTICS尽管方法相似，却给出了不同的结果。</p><p id="266c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">虽然与人类水平的性能相差甚远，但在处理大量数据时，它可以节省大量时间。</p><p id="7357" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通过利用和改进本文中介绍的技术，我们开发了<a class="ae lk" href="https://it.digitiamo.com/aiknowyou" rel="noopener ugc nofollow" target="_blank"> AiKnowYou </a>，这是一款分析和改进聊天机器人性能的产品。如果您想更好地了解它是如何工作的，或者如果您想提高您自己的聊天机器人的性能，请随时联系我们:)</p><p id="5285" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="og">感谢您的阅读！</em></p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="ou ov l"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">想了解我们最新发布的产品吗？上面订阅！别担心，我们会谨慎使用:)</figcaption></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/265d4476ea141a0364007cf4a2a8cef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*g_28_A_8jCH-eUqPI4BBpQ.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">Digitiamo徽标</figcaption></figure><h2 id="e2e7" class="nb lm iq bd ln nc nd dn lr ne nf dp lv kx ng nh lx lb ni nj lz lf nk nl mb iw bi translated">关于Digitiamo</h2><p id="4110" class="pw-post-body-paragraph ko kp iq kq b kr op ka kt ku oq kd kw kx or kz la lb os ld le lf ot lh li lj ij bi translated">Digitiamo是一家来自意大利的初创公司，专注于使用人工智能来帮助公司管理和利用他们的知识。要了解更多信息，请访问我们的。</p><h2 id="4119" class="nb lm iq bd ln nc nd dn lr ne nf dp lv kx ng nh lx lb ni nj lz lf nk nl mb iw bi translated">关于作者</h2><p id="da31" class="pw-post-body-paragraph ko kp iq kq b kr op ka kt ku oq kd kw kx or kz la lb os ld le lf ot lh li lj ij bi translated"><a class="ae lk" href="https://medium.com/u/56f43ec01c1e?source=post_page-----576ed5f7988b--------------------------------" rel="noopener"> <em class="og">法比奥·丘萨诺</em> </a> <em class="og">是</em><a class="ae lk" href="https://www.digitiamo.com/" rel="noopener ugc nofollow" target="_blank"><em class="og">Digitiamo</em></a><em class="og">的数据科学负责人；</em> <a class="ae lk" href="https://medium.com/u/ec9f76d504e0?source=post_page-----576ed5f7988b--------------------------------" rel="noopener"> <em class="og">弗朗西斯科·福马加利</em> </a> <em class="og">是一名有抱负的数据科学家，正在进行R &amp; D实习。</em></p></div></div>    
</body>
</html>