<html>
<head>
<title>Writing Efficient Input Pipelines Using TensorFlow’s Data API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow的数据API编写高效的输入管道</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/writing-efficient-input-pipelines-using-tensorflows-data-api-2dfc3b3ce077?source=collection_archive---------1-----------------------#2020-09-26">https://pub.towardsai.net/writing-efficient-input-pipelines-using-tensorflows-data-api-2dfc3b3ce077?source=collection_archive---------1-----------------------#2020-09-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="bbc2" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="007a" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">大规模机器学习工程的数据集处理简介</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/d7772586532eadb5b9001912e607bfd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*89jq1YxNnIOrq2YN"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@claudiotesta?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克劳迪奥·泰斯塔</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="512e" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">问题陈述</h1><p id="b455" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">处理非常大的数据集是机器学习工程中的常见场景。通常，这些数据集会太大而不适合内存。这意味着数据必须在训练过程中从磁盘上实时检索。</p><p id="6546" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Memory_hierarchy" rel="noopener ugc nofollow" target="_blank">磁盘访问比内存访问慢几个数量级</a>，因此高效的检索具有高优先级。</p><p id="8054" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">好在<a class="ae le" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank"> TensorFlow的</a> <code class="fe nf ng nh ni b"><a class="ae le" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank">tf.data</a></code> <a class="ae le" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank"> API </a>提供了一个简单直观的接口来加载、预处理，甚至预取数据。</p><p id="609c" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">在这篇文章中，我们将学习如何创建一个简单而强大的输入管道来使用<code class="fe nf ng nh ni b">tf.data</code> API有效地加载和预处理数据集。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="cc74" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">使用TensorFlow数据API的输入管道</h1><blockquote class="nj nk nl"><p id="65cb" class="me mf nm mg b mh na ka mj mk nb kd mm nn nc mp mq no nd mt mu np ne mx my mz ij bi translated">声明:TensorFlow为<strong class="mg ja">提供了优秀的</strong>模块和API文档。更多详情，请参考<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/" rel="noopener ugc nofollow" target="_blank">官方网站</a>。</p></blockquote><p id="6cd3" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">本教程的主角是<code class="fe nf ng nh ni b"><a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank">tf.data.Dataset</a></code>。目标是:</p><ul class=""><li id="80c2" class="nq nr iq mg b mh na mk nb mn ns mr nt mv nu mz nv nw nx ny bi translated">使用<code class="fe nf ng nh ni b">tf.data.Dataset.list_files()</code>静态方法从数据文件创建一个<code class="fe nf ng nh ni b">tf.data.Dataset</code></li><li id="46bd" class="nq nr iq mg b mh nz mk oa mn ob mr oc mv od mz nv nw nx ny bi translated">在<code class="fe nf ng nh ni b">tf.data.Dataset</code>实例上应用一系列函数</li></ul><h2 id="c7ef" class="oe ln iq bd lo of og dn ls oh oi dp lw mn oj ok ly mr ol om ma mv on oo mc iw bi translated">创建数据集</h2><p id="e8d2" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated"><code class="fe nf ng nh ni b">tf.data.Dataset.list_files()</code>从提供的文件路径列表或文件模式(正则表达式)中返回一个<code class="fe nf ng nh ni b">tf.data.Dataset</code>。</p><blockquote class="nj nk nl"><p id="d8c0" class="me mf nm mg b mh na ka mj mk nb kd mm nn nc mp mq no nd mt mu np ne mx my mz ij bi translated">我们将假设我们的数据集是存储如下的独立文件的集合:</p></blockquote><pre class="kp kq kr ks gt op ni oq or aw os bi"><span id="9d38" class="oe ln iq ni b gy ot ou l ov ow">datadir/<br/>    file_001.csv<br/>    file_002.csv<br/>    ...<br/>    file_n.csv</span></pre><p id="a044" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">创建一个<code class="fe nf ng nh ni b">tf.data.Dataset</code>文件路径:</p><p id="719c" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated"><code class="fe nf ng nh ni b">filepath_dataset = tf.data.Dataset.list_files(‘./datadir/file_*.csv’)</code></p><p id="dcfb" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">文件路径是很好的第一步，但是我们需要在文件中找到实际的数据来学习。我们将链接一系列方法，从文件路径开始，到训练期间使用的实际数据结束。</p><h2 id="53ca" class="oe ln iq bd lo of og dn ls oh oi dp lw mn oj ok ly mr ol om ma mv on oo mc iw bi translated">转换数据</h2><p id="27cc" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">一些最常见和最有用的<code class="fe nf ng nh ni b">tf.data.Dataset</code>方法有:<code class="fe nf ng nh ni b">interleave()</code>、<code class="fe nf ng nh ni b">shuffle()</code>、<code class="fe nf ng nh ni b">batch()</code>和<code class="fe nf ng nh ni b">prefetch()</code>。在本文中，我们将简要介绍其中的每一个。</p><h2 id="6681" class="oe ln iq bd lo of og dn ls oh oi dp lw mn oj ok ly mr ol om ma mv on oo mc iw bi translated">交错</h2><p id="efd5" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">在模型的训练期间，打乱实例以避免由于训练数据的排序(例如，按字母顺序排序的文件)而学习虚假的模式可能是有益的。<code class="fe nf ng nh ni b">interleave()</code>函数提供了一种简单的方法来对包含单独的较小文件的数据集进行粗粒度的洗牌。</p><blockquote class="nj nk nl"><p id="2d75" class="me mf nm mg b mh na ka mj mk nb kd mm nn nc mp mq no nd mt mu np ne mx my mz ij bi translated"><strong class="mg ja">粗粒度</strong>在此上下文中表示在<strong class="mg ja">文件级别</strong>。更细粒度的洗牌将在<strong class="mg ja">进行。csv行级别</strong>，我们很快就会看到。</p></blockquote><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="433a" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">通过使用<code class="fe nf ng nh ni b">lambda</code>函数，我们从每个单独的文件路径生成一个<code class="fe nf ng nh ni b">tf.data.experimental.CsvDataset</code> <em class="nm">。</em></p><p id="e4aa" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">变量<code class="fe nf ng nh ni b">csv_dataset</code>现在是对由一组<code class="fe nf ng nh ni b">CsvDataset</code>数据集组成的<code class="fe nf ng nh ni b">tf.data.Dataset </code>的引用。<code class="fe nf ng nh ni b">interleave()</code>方法将所有单独的<code class="fe nf ng nh ni b">CsvDatasets</code>数据集组合(或<em class="nm">交错</em>)成一个单独的数据集(在本例中为<code class="fe nf ng nh ni b">csv_dataset</code>)。</p><p id="a4f9" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">将<code class="fe nf ng nh ni b">num_parallel_calls</code>设置为<code class="fe nf ng nh ni b">tf.data.experimental.AUTOTUNE</code>会告诉TensorFlow自动选择最佳数量的线程来读取<code class="fe nf ng nh ni b">.csv </code>文件。这个<strong class="mg ja">增加了流水线效率</strong>，只要机器有支持多线程的多核处理器(今天大部分机器都有这个)。</p><h2 id="375c" class="oe ln iq bd lo of og dn ls oh oi dp lw mn oj ok ly mr ol om ma mv on oo mc iw bi translated">洗牌</h2><p id="7f90" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">如前所述，混洗数据可以防止学习训练数据中的虚假模式。这也改善了基于梯度的方法的收敛性，例如使用<a class="ae le" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>来训练神经网络。要打乱数据，使用<code class="fe nf ng nh ni b">shuffle()</code>方法:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="03d7" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated"><code class="fe nf ng nh ni b">shuffle_buffer_size</code>值指定缓冲区的大小，用于存储来自原始(未缓冲)数据集的数据。这个值应该根据数据集的大小和可用的内存量来设置。更多详情请参考<code class="fe nf ng nh ni b">shuffle()</code> <a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><blockquote class="nj nk nl"><p id="9e61" class="me mf nm mg b mh na ka mj mk nb kd mm nn nc mp mq no nd mt mu np ne mx my mz ij bi translated">注意:验证或测试期间不需要洗牌。事实上，一些场景要求测试集上的预测按照与提供的测试数据相同的顺序发生(例如，<a class="ae le" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a> competitions)。请记住这一点，如果需要，请禁用洗牌！</p></blockquote><h2 id="024d" class="oe ln iq bd lo of og dn ls oh oi dp lw mn oj ok ly mr ol om ma mv on oo mc iw bi translated">一批</h2><p id="b4a3" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">另一种常见的数据预处理技术是<strong class="mg ja">批处理</strong>数据，一次处理一小块，而不是一次处理整个数据集。这使得在非常大的数据集上处理和训练成为可能，因为它们不需要一次全部存储在存储器中。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ox oy l"/></div></figure><h2 id="0f86" class="oe ln iq bd lo of og dn ls oh oi dp lw mn oj ok ly mr ol om ma mv on oo mc iw bi translated">预取</h2><p id="5674" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">高效的深度学习管道利用GPU计算进行模型训练，而数据获取和预处理在独立的计算模块(如CPU)上进行。</p><p id="e2bc" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">如果GPU必须等待CPU完成加载和预处理，那么它将<em class="nm">闲置</em>，无所事事地浪费宝贵的周期。<strong class="mg ja">提前预取</strong>数据有助于防止这种情况。这<strong class="mg ja">极大地提高了效率</strong>，因为它最大化了GPU的利用率(没有浪费或空闲的周期)。</p><p id="305f" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">指定预取数据就像调用<code class="fe nf ng nh ni b">prefetch()</code>方法一样简单:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ox oy l"/></div></figure><h1 id="5d1b" class="lm ln iq bd lo lp oz lr ls lt pa lv lw kf pb kg ly ki pc kj ma kl pd km mc md bi translated">把所有的放在一起</h1><p id="205c" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">将所有方法组合到一个函数中，最终的管道可能如下所示:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ox oy l"/></div></figure><h1 id="6bef" class="lm ln iq bd lo lp oz lr ls lt pa lv lw kf pb kg ly ki pc kj ma kl pd km mc md bi translated">结论</h1><p id="083d" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">在本文中，我们学习了使用<code class="fe nf ng nh ni b">tf.data </code> API创建一个简单而高效的输入管道。我们讲述了如何:</p><ul class=""><li id="cb4c" class="nq nr iq mg b mh na mk nb mn ns mr nt mv nu mz nv nw nx ny bi translated">使用<code class="fe nf ng nh ni b">tf.data.Dataset.list_files()</code>静态方法处理分成多个独立文件的非常大的数据集</li><li id="3822" class="nq nr iq mg b mh nz mk oa mn ob mr oc mv od mz nv nw nx ny bi translated">通过利用多线程执行来实现并行</li><li id="afca" class="nq nr iq mg b mh nz mk oa mn ob mr oc mv od mz nv nw nx ny bi translated">使用粗粒度数据洗牌和<code class="fe nf ng nh ni b">interleave()</code>数据集方法改善收敛</li><li id="280b" class="nq nr iq mg b mh nz mk oa mn ob mr oc mv od mz nv nw nx ny bi translated">使用<code class="fe nf ng nh ni b">shuffle()</code>方法，通过细粒度的数据混排来改善收敛</li><li id="5c6b" class="nq nr iq mg b mh nz mk oa mn ob mr oc mv od mz nv nw nx ny bi translated">使用<code class="fe nf ng nh ni b">prefetch()</code>方法最大化GPU利用率</li><li id="9f9f" class="nq nr iq mg b mh nz mk oa mn ob mr oc mv od mz nv nw nx ny bi translated">将所有这些概念合并到一个函数中，生成一个可供模型使用的数据集</li></ul></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="98da" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">这篇文章的灵感很大程度上来自于<a class="ae le" href="https://github.com/ageron/handson-ml2" rel="noopener ugc nofollow" target="_blank">用Scikit-Learn，Keras &amp; Tensorflow </a>实践机器学习中第12章的一个练习。<strong class="mg ja">我强烈推荐这本书</strong>。</p><h2 id="b6b9" class="oe ln iq bd lo of og dn ls oh oi dp lw mn oj ok ly mr ol om ma mv on oo mc iw bi translated">来源</h2><ul class=""><li id="e031" class="nq nr iq mg b mh mi mk ml mn pe mr pf mv pg mz nv nw nx ny bi translated"><a class="ae le" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习</a></li><li id="9af9" class="nq nr iq mg b mh nz mk oa mn ob mr oc mv od mz nv nw nx ny bi translated">官方<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>文档</li><li id="4472" class="nq nr iq mg b mh nz mk oa mn ob mr oc mv od mz nv nw nx ny bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Memory_hierarchy" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Memory_hierarchy</a></li></ul></div></div>    
</body>
</html>