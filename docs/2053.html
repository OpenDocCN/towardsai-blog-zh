<html>
<head>
<title>How DeepMind Train Agents that can Play Any Game Without Human Intervention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepMind如何训练可以在没有人类干预的情况下玩任何游戏的代理</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-deepmind-train-agents-that-can-play-any-game-without-human-intervention-63ef95dbdff3?source=collection_archive---------1-----------------------#2021-08-03">https://pub.towardsai.net/how-deepmind-train-agents-that-can-play-any-game-without-human-intervention-63ef95dbdff3?source=collection_archive---------1-----------------------#2021-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6036" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="97e2" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一份新的报告提出了一个新的架构和培训环境，一般有能力的代理人。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f938bb53435f7af556954adbe69c8d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LzBYvBviol_xRHfO"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:DeepMind</figcaption></figure><blockquote class="lh li lj"><p id="1334" class="lk ll lm ln b lo lp kd lq lr ls kg lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过10万名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="mh mi gp gr mj mk"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd jd gy z fp mp fr fs mq fu fw jc bi translated">序列</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到85，000多人的信任…</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">thesequence.substack.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my lb mk"/></div></div></a></div><p id="9201" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mz lv lw lx na lz ma mb nb md me mf mg im bi translated">近年来，游戏一直是一些最大的深度学习的中心。当DeepMind的强化学习代理AlphaGo击败围棋世界冠军Lee Sedol时，深度学习和游戏的sputnik时刻到来了。AlphaGo后来被AlphaZero完善，alpha zero能够掌握像国际象棋、围棋或日本象棋这样的游戏。强化学习代理在多人游戏中也取得了超人的表现，如<a class="ae nc" href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark" rel="noopener ugc nofollow" target="_blank">雅达利</a>、<a class="ae nc" href="https://deepmind.com/blog/article/capture-the-flag-science" rel="noopener ugc nofollow" target="_blank">夺旗</a>、<a class="ae nc" href="https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">星际争霸2</a>、<a class="ae nc" href="https://openai.com/projects/five/" rel="noopener ugc nofollow" target="_blank"> Dota 2 </a>和<a class="ae nc" href="https://openai.com/blog/emergent-tool-use/" rel="noopener ugc nofollow" target="_blank">捉迷藏</a>。然而，在每种情况下，强化学习代理每次都在一个游戏中训练。在深度学习领域，构建可以同时掌握多个游戏而无需大量人工干预的智能体的想法仍然是一个难以实现的目标。最近，DeepMind发表了“<a class="ae nc" href="https://deepmind.com/research/publications/open-ended-learning-leads-to-generally-capable-agents" rel="noopener ugc nofollow" target="_blank">开放式学习导致普遍有能力的代理</a>”，这是一篇研究论文，详细介绍了训练强化学习代理的方法和过程，这些代理能够在没有人类干预的情况下掌握多个同时进行的游戏。这篇论文代表了在现实世界环境中建立更通用的智能体的一个重要步骤。</p><p id="c0b8" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mz lv lw lx na lz ma mb nb md me mf mg im bi translated">本质上，DeepMind构建通用智能代理的方法基于三个直观的构建模块:</p><p id="0756" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mz lv lw lx na lz ma mb nb md me mf mg im bi translated"><em class="lm"> 1)丰富多样的训练任务。</em></p><p id="ecf4" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mz lv lw lx na lz ma mb nb md me mf mg im bi translated"><em class="lm"> 2)灵活的架构和训练方法。</em></p><p id="7183" class="pw-post-body-paragraph lk ll it ln b lo lp kd lq lr ls kg lt mz lv lw lx na lz ma mb nb md me mf mg im bi translated">衡量进展的严格流程。</p><h1 id="9b15" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">大量的训练任务</h1><p id="142b" class="pw-post-body-paragraph lk ll it ln b lo nv kd lq lr nw kg lt mz nx lw lx na ny ma mb nb nz me mf mg im bi translated">为了普遍掌握学习不同游戏的技能，DeepMind创建了一个名为XLand的环境，本质上是一个游戏星系。在XLand galaxy中，游戏是根据某些特征(如合作或竞争动态)的接近程度来放置的。每个游戏可以使用不同的复杂程度来玩，这些复杂程度可以动态地改变，以改善代理的学习行为。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/4353f2ec85e5560ae381e1a79ee93042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vXHqFWsr2EjLBhot"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:DeepMind</figcaption></figure><h1 id="454a" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">灵活的架构和培训方法</h1><p id="48ca" class="pw-post-body-paragraph lk ll it ln b lo nv kd lq lr nw kg lt mz nx lw lx na ny ma mb nb nz me mf mg im bi translated">DeepMind的代理架构基于目标注意力代理(GOAT)神经网络，该网络使用对其当前状态的注意力。这种机制有助于代理关注给定游戏中的特定子目标。训练任务的分配是使用DeepMind最喜欢的<a class="ae nc" href="https://deepmind.com/blog/article/population-based-training-neural-networks" rel="noopener ugc nofollow" target="_blank">基于人口的训练</a> (PBT)来选择的，这已经在他们的许多强化学习模型中使用过。PBT调整任务生成过程的参数，以改进代理的学习。培训过程从零开始，并根据代理的进度逐渐增加复杂性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/985658c54c58e3c8c9601d17ed14cbb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SkSLRzPxh4wx4r5q"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:DeepMind</figcaption></figure><h1 id="c0b1" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">衡量进展</h1><p id="350a" class="pw-post-body-paragraph lk ll it ln b lo nv kd lq lr nw kg lt mz nx lw lx na ny ma mb nb nz me mf mg im bi translated">量化不同任务的学习进度可能是一个重大挑战。为了解决这个问题，DeepMind将每项任务的分数标准化，并使用通过当前一组训练有素的玩家计算出的纳什均衡值。评估任务查看标准化分数的不同百分位数，这些分数可以在不同代理之间进行比较。</p><h1 id="2031" class="nd ne it bd nf ng nh ni nj nk nl nm nn ki no kj np kl nq km nr ko ns kp nt nu bi translated">结果呢</h1><p id="464f" class="pw-post-body-paragraph lk ll it ln b lo nv kd lq lr nw kg lt mz nx lw lx na ny ma mb nb nz me mf mg im bi translated">DeepMind在XLand的4000个世界的大约700，000个游戏中训练了其一般能力的代理。这相当于大约200，000，000，000个训练步骤和3，400，000个训练任务。代理人能够在几乎没有人工干预的情况下掌握几乎每一项任务。这清楚地显示了这种类型的方法在没有人类监督的情况下使用单个代理来掌握多个复杂任务的可行性。本文概述的想法可能是新一波强化学习里程碑的开始。您可以在下面的视频中看到代理的工作:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure></div></div>    
</body>
</html>