<html>
<head>
<title>4 Tips To Write Scalable Apache Spark Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编写可伸缩Apache Spark代码的4个技巧</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/4-tips-to-write-scalable-apache-spark-code-1c736e4d698e?source=collection_archive---------0-----------------------#2019-09-18">https://pub.towardsai.net/4-tips-to-write-scalable-apache-spark-code-1c736e4d698e?source=collection_archive---------0-----------------------#2019-09-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e342" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><h1 id="bf6b" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">介绍</h1><p id="794d" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将分享一些关于如何编写可伸缩的Apache Spark代码的技巧。这里给出的例子实际上是基于我在现实世界中遇到的代码。因此，通过分享这些技巧，我希望我可以帮助新人编写高性能的Spark代码，而不会不必要地增加他们集群的资源。</p><h1 id="43c6" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">集群设置</h1><p id="bed2" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在本文中用来运行代码的集群托管在Databricks上，配置如下:</p><ul class=""><li id="0c84" class="lv lw it kz b la lx le ly li lz lm ma lq mb lu mc md me mf bi translated">集群模式:标准</li><li id="fc18" class="lv lw it kz b la mg le mh li mi lm mj lq mk lu mc md me mf bi translated">Databricks运行时版本:5.5 LTS ML(包括Apache Spark 2.4.3 Scala 2.11)</li></ul><p id="c7ce" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">有8个工作线程，工作线程和驱动程序都是M4 . XL大型实例(16.0 GB，4个内核)。</p><h1 id="cc37" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">背景</h1><p id="72c5" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我最近继承了一个笔记本，用来跟踪我们的AB测试结果，以评估我们的推荐引擎的性能。从头到尾运行笔记本慢得令人难以忍受。这是执行时间最长的单元格:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/2b0603fffd10fbf01f1ff0f90ac163de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*_Uop-8O18EjtlivXM4jZVQ.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图1:非常糟糕的实现</figcaption></figure><p id="e7f9" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">在我们的AB测试实验中，用于跟踪数据的文件存储为换行符分隔的JSON文件，按照年、月和日划分到不同的文件夹中。每天可能有几百个JSON文件。如果我们给图1中的代码一个日期列表(<code class="fe na nb nc nd b">bucketPeriod</code>)，它将遍历这些日期，并在每个日期通过调用<code class="fe na nb nc nd b">getAnalytics</code>加载所有JSON文件。这将返回一个数据帧列表，在我们从列表中移除空数据帧(那些有0行的数据帧)后，我们将通过调用<code class="fe na nb nc nd b">union</code>将这些数据帧合并成一个数据帧。</p><p id="ac25" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">执行图1中的代码来检索3天的数据需要17.72分钟，计算结果也就是<code class="fe na nb nc nd b">df.count</code>需要26.71分钟。让我们看看是否可以在不增加集群规模的情况下运行得更快。</p><p id="ad3f" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">在讨论技巧之前，这里是<code class="fe na nb nc nd b">getAnalytics</code>的实现，它基本上读取一个包含一堆换行符分隔的JSON文件的文件夹，并根据这些文件中有效载荷字段的内容添加一些特性:</p><figure class="mp mq mr ms gt mt"><div class="bz fp l di"><div class="ne nf l"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图2:另一个非常糟糕的实现</figcaption></figure><h1 id="2ddb" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">技巧1:向Spark函数提供尽可能多的输入</h1><p id="4aaf" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图1中的代码本质上是一次对一个文件夹调用<code class="fe na nb nc nd b">spark.read.json</code>。考虑到<code class="fe na nb nc nd b">spark.read.json</code>可以接受一个文件名列表，这将允许驱动程序只调度任务一次，而不是多次，如果<code class="fe na nb nc nd b">spark.read.json</code>像在for循环中一样被重复调用。</p><p id="5133" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">因此，如果您需要读取分布在许多文件夹中的文件，而不是编写以下内容:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/4a8d2318b442067e7c26a226487ab3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dw0pG_JHkWIbe7Qc0ivQLw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图3:顺序读取文件</figcaption></figure><p id="deaf" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">你应该这样写:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nl"><img src="../Images/6c7272c84fdd86fa648aaf4e47a41fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2gkBeAlobzk1dpOhVluxNg.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图4:一次读取所有文件</figcaption></figure><p id="711e" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">图4中的代码比图3中的代码稍快(6.86分钟对7.30分钟)。但是，日期范围越大，差异越大。</p><h1 id="3e40" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">技巧2:尽可能跳过模式推理</h1><p id="4a95" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据<code class="fe na nb nc nd b"> spark.read.json</code> <a class="ae nm" href="http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.DataFrameReader" rel="noopener ugc nofollow" target="_blank">文档</a>:</p><blockquote class="nn no np"><p id="b7e6" class="kx ky nq kz b la lx lc ld le ly lg lh nr ml lk ll ns mm lo lp nt mn ls lt lu im bi translated">这个函数遍历输入一次，以确定输入模式。如果您事先知道模式，请使用指定模式的版本来避免额外的扫描。</p></blockquote><p id="af7b" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">因此，图4中的代码可以重写如下:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nu"><img src="../Images/53035ddce3f54724d677cdd1af1c31a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uj9ywyChfL3J9lwV4reJRw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图5:跳过模式推断读取JSON文件</figcaption></figure><p id="9539" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">在图5中，我没有明确指定模式，而是决定只读取文件列表中的一个文件，根据这一个文件推断模式，并将相同的模式应用于其余的文件。这个代码单元只用了29.82秒就完成了。</p><h1 id="04f1" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">技巧3:构建数据框操作以最小化混乱</h1><p id="f00c" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经改进了文件I/O，让我们看看是否可以改进如图2所示的<code class="fe na nb nc nd b">getAnalytics</code>中的转换。</p><p id="0970" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">图2中代码的问题是第一次调用distinct会导致洗牌。我们怎么知道这将会发生？通过调用RDD的<code class="fe na nb nc nd b">toDebugString</code>方法，定义一个数据帧操作，如下所示:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/927aa644b8df49d4611f2c33c2004614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*pSa4ZtxVu0CY4FiSonv3Lw.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图6:如何辨别不同的原因导致洗牌</figcaption></figure><p id="f10d" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated"><code class="fe na nb nc nd b">toDebugString</code>输出中的每一个缩进都表示当执行转换时将触发一次洗牌。回想一下<code class="fe na nb nc nd b">df1</code>只是在读取JSON文件，所以混乱的原因一定是对<code class="fe na nb nc nd b">distinct</code>的调用。使用同样的方法，我们也可以推断出对<code class="fe na nb nc nd b">rank().over(rankingOrderedIds)</code>(第13行)的调用将导致洗牌。</p><p id="ae50" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">在这种情况下，在读取文件后立即触发shuffle并不是一个好主意，因为整个数据集非常庞大，而且实际上有很多我们分析不需要的无关列。因此，我们不必要地在集群中移动大文件。我们的目标是给定一个操作列表，我们希望执行这些操作来实现我们的最终目标，我们能否对它们重新排序，以便尽可能晚地进行洗牌，并使用尽可能少的数据？</p><p id="cfc2" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">幸运的是，这意味着将图2中的代码重写如下:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nw"><img src="../Images/22c53c964e7ea85c58ed13b5f09b7886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTQiamZBqmLTzxiEjPTBoQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图7:图1的优化版本</figcaption></figure><p id="f34e" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">为了清楚起见，我将图1的相关部分合并到图7中，这样结果是相同的。注意，我们只需要调用它一次就可以获得相同的结果，而不是调用<code class="fe na nb nc nd b">distinct</code> 3次(图1中一次，图2中两次)。当洗牌发生时(由于第9行中的调用)，物理计划显示只有在<code class="fe na nb nc nd b">select</code>调用中指定的列被移动，这很好，因为它只是原始数据集中所有列的一个小子集。</p><p id="3085" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">图7中的代码只需要0.90秒就可以执行，并且只需要2.58分钟。这个例子表明，认真思考您正在编写的查询以避免导致不必要的混乱的冗余操作是值得的。</p><h1 id="5018" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">技巧4:手动执行明显的优化，而不是依赖Catalyst优化器</h1><p id="d4e9" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比较和对比以下两个片段:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nw"><img src="../Images/22c53c964e7ea85c58ed13b5f09b7886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTQiamZBqmLTzxiEjPTBoQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图8a:调用等级窗口函数后的过滤器(与图7相同)</figcaption></figure><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nx"><img src="../Images/c2a859534d316584c2f26dfd89a387d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cs-7eItQM5IPX7CPVtrWwg.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图8b:调用等级窗口函数之前的过滤</figcaption></figure><p id="3613" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">除了执行过滤的顺序不同(分别是第14行和第9行)，图8a和8b是相同的。但是两个片段给出了相同的结果，因为排名窗口函数不是列c3的函数，所以在排名之前或之后执行过滤并不重要。然而，<code class="fe na nb nc nd b">df2.count</code>的执行时间明显不同:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8d1f74747ca245c382485bcfe8b9293c.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*N5WRQFIKCahW9p56iOd-AA.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">表df2.count的执行时间</figcaption></figure><p id="7a0a" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">这两个查询的物理计划解释了原因:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nz"><img src="../Images/6298f2dec75d278331f522449adcbeb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Va-R0WHzTHUD1-i3UlxlQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk translated">图9:图8a和8b的物理平面图</figcaption></figure><p id="7adc" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">图9显示了在调用秩窗口函数之前进行过滤的效果是将第一阶段的行数从8.02亿减少到仅仅7百万(图8b列)。因此，当发生洗牌时，总共只有2.9 GB的数据需要在群集中移动(图8b列中标有“Exchange”的框)。相比之下，在调用等级窗口函数之前不进行筛选会导致304.9 GB的数据在集群中移动。</p><p id="bdd6" class="pw-post-body-paragraph kx ky it kz b la lx lc ld le ly lg lh li ml lk ll lm mm lo lp lq mn ls lt lu im bi translated">这里的教训是，如果您的查询导致混乱，请尝试寻找机会对数据应用一些过滤器，以减少需要传输的数据量。此外，不要害怕研究您的查询的物理计划，寻找Catalyst优化器可能错过的任何优化机会。</p><h1 id="0160" class="jz ka it bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论</h1><p id="b865" class="pw-post-body-paragraph kx ky it kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经看到，编写Spark查询的方式会对其执行时间产生重大影响。我希望这些技巧能够帮助您编写更好的Spark代码，最大限度地利用您的集群资源。如果你有任何问题，请在评论中告诉我。</p></div></div>    
</body>
</html>