<html>
<head>
<title>EMI: Exploration with Mutual Information</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EMI:探索相互信息</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/emi-exploration-with-mutual-information-16b6c7fbe800?source=collection_archive---------0-----------------------#2019-08-28">https://pub.towardsai.net/emi-exploration-with-mutual-information-16b6c7fbe800?source=collection_archive---------0-----------------------#2019-08-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d8d4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">面向人工智能的强化学习探索策略| <a class="ae ep" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank"/></h2><div class=""/><div class=""><h2 id="baf3" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一种新的基于表征学习的探索方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1b60ea94730bdff95e116f4518c466cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HGzIVS5lHwodxbzX4bv5Ew.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://unsplash.com/@andrewtneel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">安德鲁·尼尔</a>在<a class="ae lh" href="https://unsplash.com/search/photos/explore?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="fd2b" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="2605" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">当奖励信号稀疏时，强化学习可能会很难。在这些场景中，探索策略变得至关重要:一个好的探索策略不仅能帮助主体更快更好地理解世界，还能使其对环境的变化具有鲁棒性。在本文中，我们讨论了一种新颖的探索方法，即Kim等人在2019年ICML会议上提出的互信息探索(EMI)。简而言之，EMI学习观察(状态)和动作的表示，期望我们可以在这些表示上有一个线性动力学模型。EMI然后计算内在报酬作为线性动力学模型下的预测误差。内在奖励与环境奖励相结合形成最终的奖励函数，该函数可以被任何RL方法使用。</p><p id="64dc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了避免冗余，我们假设您熟悉<a class="ae lh" href="https://en.wikipedia.org/wiki/Mutual_information" rel="noopener ugc nofollow" target="_blank">互信息</a>和<a class="ae lh" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank">马尔可夫决策过程</a>的概念。</p><h1 id="4f44" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">表征学习</h1><h2 id="8bd1" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">状态和动作的表示</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/d45f75a1937afac336fb141b6715c8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SF_62DoIP3GBOtBbsklyZA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:EMI:探索与相互信息</figcaption></figure><p id="f86e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在EMI中，我们的目标是分别学习状态和动作的表示<em class="nn"> ϕ(s): S→ ℝᵈ </em>和<em class="nn"> ψ(a): A→ ℝᵈ </em>，使得学习到的表示承载关于动态的最有用的信息。这可以通过最大化两个互信息目标来实现:1)在<em class="nn">【ϕ(s】、ψ(a)】</em><em class="nn">【ϕ(s')</em>之间的互信息；2)<em class="nn">【ϕ(s】【ϕ(s'】</em><em class="nn">ψ(a)</em>之间的交互信息。数学上，我们最大化以下两个目标</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/55b82196346b2701203fe1e1a11aaa93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lo1826c_ysaj4mocXkA83A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">等式1 .相互信息目标</figcaption></figure><p id="ade5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">其中<em class="nn"> P_{SAS'}^π </em>表示独生子女经历元组<em class="nn"> (s，a，s’)</em>遵循政策<em class="nn"> π、</em>和<em class="nn"> P_{A}^π </em>、<em class="nn"> P_{SA}^π </em>、<em class="nn"> P_{SS'}^π </em>为边际分布。这些目标可以通过我们之前讨论过的<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/{{ site.baseurl }}{% post_url 2018-09-02-MINE %}"> MINE </a>或<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/{{ site.baseurl }}{% post_url 2018-09-09-DIM %}"> DIM </a>进行优化。在EMI中，我们遵循DIM提出的目标，通过詹森-香农散度(JSD)最大化<em class="nn"> X </em>和<em class="nn"> Y </em>之间的互信息(MI)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/36b26714b39eb6011bb9c0a4219b5796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Ugz052pAtq_JaFoYuP2rg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">等式2 .最大化互信息的JSD目标。该目标的一个重要性质是它受<em class="nq"> 2log2约束，解决了</em>地雷的梯度爆炸问题</figcaption></figure><h1 id="55c9" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">用误差模型嵌入线性动力学模型</h1><p id="a606" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">除了上述目标之外，EMI还在转换是线性的嵌入空间上强加了简单和方便的拓扑。具体地，我们还试图学习状态<em class="nn"> ϕ(s) </em>和动作<em class="nn"> ψ(a) </em>的表示，使得对应的下一个状态<em class="nn"> ϕ(s') </em>的表示遵循线性动态，即</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/f2ab7db2d18084ab38f5a25e6434cd34.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*zfRke-QZxu5s5O6XX1v_tw.png"/></div></figure><p id="ac2d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">直觉上，这可能允许我们将大部分建模负担转移到嵌入函数上。</p><p id="f916" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">然而，不管神经网络的表达能力如何，在线性动态模型下，总会存在一些不可约误差。例如，在线性动态模型下，在Atari环境中引导代理从一个房间到另一个房间的状态转换是非常难以解释的。为此，作者引入误差模型<em class="nn"> e(s，a): S×A→ ℝᵈ </em>，这是另一种以状态和动作为输入的神经网络，在线性模型下估计不可约误差。为了训练误差模型，我们最小化误差项的欧几里德范数，使得误差项有助于少量的不可解释的情况。以下目标示出了在线性动态下具有建模误差的嵌入学习问题:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/cbed1e471f01946e25678d50dc0eea6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*e4l2V-LzCgGPZzOg3W2wkA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">等式3 .误差模型的目标</figcaption></figure><h1 id="0df6" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">表征学习的目标</h1><p id="223f" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">现在我们把所有的目标放在一起:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/f6a868fa9f9f928c615f643b10526982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*seVRoMwPb_IT4mHx9VTcyw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">方程式4 .表征学习的最终目标。λ_{error}和λ_{info}是对误差模型的简单性和表示之间的互信息进行加权的超参数</figcaption></figure><p id="f5d7" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通过将<a class="ae lh" href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="noopener ugc nofollow" target="_blank">拉格朗日乘数</a>应用于前面小节中定义的约束优化问题，获得第一个期望项，第二个期望项是MI目标的负值。</p><p id="07c8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在实践中，作者发现当我们正则化动作嵌入表示的分布以遵循预定义的先验分布，例如标准正态分布时，优化过程更稳定。这引入了一个额外的KL惩罚<em class="nn"> D_{KL}(P_A^π‖𝒩(0，I)】</em>类似于VAEs，其中<em class="nn"> P_A^π </em>是一个经验正态分布，其参数是从一批样本中近似得到的。这里，我们给出一个示例张量流代码来演示如何从样本中计算KL散度。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/00a539f4b76cddafb66dcb44a784fad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ffz4Af0tqcP4pmZ8_jRtsg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">计算D_{ <em class="nq"> KL}(P_A^π‖𝒩(0，I))的张量流代码</em></figcaption></figure><p id="dcbe" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">作者还试图调整状态嵌入，但他们发现这会使优化过程更加不稳定。这可能是由于状态的分布比动作的分布更容易偏斜，尤其是在优化的初始阶段。</p><h1 id="5b90" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">内在奖励</h1><p id="7bfe" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们现在将内在报酬定义为嵌入空间中的预测误差:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/8acf669e29346f231b01bc4ceb7e570c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VA2-EE44YUbZWatmJvAjlQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">内在报酬函数</figcaption></figure><p id="c9e5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这个公式包含了误差项，并确保我们区分不可约误差，这种不可约误差不构成新颖性。我们将它与外在奖励相结合，得到最终的奖励函数，用于训练一个RL代理</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f0bf4bdd8398a96c50ce77e27430ed7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*mCD6jKqJ9BDgeSjScSs2MQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">最终的奖励函数。η是内在报酬系数。</figcaption></figure><h1 id="672d" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">算法</h1><p id="f90e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">既然我们已经定义了表征学习的目标和强化学习的回报函数，算法就变得简单了</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/a4eb5ffee429086b5b29bdecdae1dfb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpSx3P8H5G39S256b10YZg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">EMI伪代码</figcaption></figure><h1 id="6478" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">实验结果</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/cb16bac0d1e3dcbdfa1baa009e151061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5j2ngHo2Y7pz_UzTRj8c1Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:EMI:探索与相互信息</figcaption></figure><p id="63c2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们可以看到，尽管EMI的方差很大，但它在具有挑战性的低维运动任务中取得了更好的结果(图4)。另一方面，它与基于视觉的任务中的许多以前的方法有某种联系。总之，电磁干扰是一种广泛适用的方法，可以获得令人满意的性能。</p><h1 id="981f" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><em class="nq">参考文献</em></h1><ol class=""><li id="4695" class="nz oa it mc b md me mg mh mj ob mn oc mr od mv oe of og oh bi translated">金亨锡，金在贤，郑延宇，谢尔盖·莱文，玄武铉。EMI:探索相互信息</li></ol></div></div>    
</body>
</html>