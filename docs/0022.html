<html>
<head>
<title>Application of Synthetic Minority Over-sampling Technique (SMOTe) for Imbalanced Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综合少数过采样技术在不平衡数据集上的应用</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/application-of-synthetic-minority-over-sampling-technique-smote-for-imbalanced-data-sets-509ab55cfdaf?source=collection_archive---------1-----------------------#2019-02-15">https://pub.towardsai.net/application-of-synthetic-minority-over-sampling-technique-smote-for-imbalanced-data-sets-509ab55cfdaf?source=collection_archive---------1-----------------------#2019-02-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6cd4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-mining" rel="noopener ugc nofollow" target="_blank">数据挖掘</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/84b0e225963aede48346380de6ce23b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OnXSxzMiTJOPadQy57cahw.jpeg"/></div></div></figure><p id="30b5" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">在数据科学中，不平衡的数据集并不奇怪。如果用于分类问题的数据集，如情感分析、医学成像或其他与离散预测分析(例如航班延误预测)相关的问题，对于不同的类别具有不同数量的实例(样本或数据点)，则这些数据集被称为不平衡的。这意味着由于属于每个类的实例数量之间的巨大差异，数据集中的类之间存在不平衡。相对于具有相对大量样本的类(称为<strong class="km jd">多数</strong>)，具有相对较少实例的类被称为<strong class="km jd">少数</strong>。不平衡数据集的示例如下:</p><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div class="gh gi li"><img src="../Images/d6b0a453db6e991382ce91b0cc85891d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*1_DR8r1a8jTb6G6F-YXjng.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">这里有两个带标签的类别:<strong class="bd lr"> 0 </strong>和<strong class="bd lr"> 1 </strong>带<strong class="bd lr">总不平衡</strong></figcaption></figure><p id="97c2" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">用这种不平衡的数据集训练机器学习模型，通常会导致模型对多数类产生一定的偏见。</p><p id="6290" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">为了解决类不平衡的问题，Chawla等人[3]在2002年引入了合成少数过采样技术(SMOTe)。</p><blockquote class="ls lt lu"><p id="5859" class="kk kl lv km b kn ko kp kq kr ks kt ku lw kw kx ky lx la lb lc ly le lf lg lh im bi translated"><strong class="km jd">SMOTe的简要说明</strong></p></blockquote><ol class=""><li id="d2d9" class="lz ma it km b kn ko kr ks kv mb kz mc ld md lh me mf mg mh bi translated">SMOTe是一种基于最近邻的技术，通过特征空间中数据点之间的欧几里德距离来判断。</li><li id="8374" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated">存在过采样的百分比，其指示要创建的合成样本的数量，并且该过采样的百分比参数总是100的倍数。如果过采样的百分比是100，那么对于每个实例，将创建一个新的样本。因此，少数类实例的数量将会翻倍。类似地，如果过采样的百分比是200，那么少数类样本的总数将增加两倍。</li></ol><p id="25ec" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">在SMOTe，</p><ul class=""><li id="266b" class="lz ma it km b kn ko kr ks kv mb kz mc ld md lh mn mf mg mh bi translated">对于每个少数实例，找到k个最近邻居，使得它们也属于同一类，</li></ul><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/7cc8dcf5336f8ec61b0545a5c05c90e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*co_h5NFcZ1Rwy8QKykDZow.png"/></div></figure><ul class=""><li id="fecd" class="lz ma it km b kn ko kr ks kv mb kz mc ld md lh mn mf mg mh bi translated">找到所考虑实例的特征向量和k个最近邻居的特征向量之间的差异。因此，获得了k个差向量。</li><li id="323c" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh mn mf mg mh bi translated">k个差向量中的每一个都乘以0和1之间的随机数(不包括0和1)。</li><li id="df3d" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh mn mf mg mh bi translated">现在，差向量在乘以随机数之后，在每次迭代时被添加到所考虑的实例(原始少数实例)的特征向量。</li></ul><p id="18b1" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">下面是用Python从头开始实现SMOTe的过程</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="b031" class="mu mv it mq b gy mw mx l my mz">import numpy as np</span><span id="2a4c" class="mu mv it mq b gy na mx l my mz">def <strong class="mq jd">nearest_neighbour</strong>(X, x, K):<br/>    euclidean = np.ones(X.shape[0]-1)<br/>    k = 0<br/>    for j in range(0,X.shape[0]):<br/>        if np.array_equal(X[j], x) == False:<br/>            euclidean[k] = sqrt(sum((X[j]-x)**2))<br/>            k = k + 1<br/>    indices=list(sorted(range(len(euclidean)), key=lambda j: euclidean[j]))<br/>    difference = []<br/>    for j in range(0, K):<br/>        difference.append((abs(x-X[indices[j]])))<br/>    weight = random.random()<br/>    while(weight == 0):<br/>        weight = random.random()<br/>    additive = np.multiply(difference,weight)<br/>    return additive</span><span id="7067" class="mu mv it mq b gy na mx l my mz">def <strong class="mq jd">SMOTE</strong>(X, K):<br/>    K = int(K/100)<br/>    new = [None]*(K*X.shape[0]*X.shape[1])<br/>    new = np.array(new).reshape(K*X.shape[0],X.shape[1])<br/>    k = 0<br/>    for i in range(0,X.shape[0]):<br/>        additive = nearest_neighbour(X, X[i], K)<br/>        for j in range(0, K):<br/>            new[k] = X[i] + additive[j]<br/>            k = k + 1<br/>    return new</span></pre></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="43c0" class="ni mv it bd lr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bi translated">SMOTe在实践中的应用</h1><p id="7fe7" class="pw-post-body-paragraph kk kl it km b kn oe kp kq kr of kt ku kv og kx ky kz oh lb lc ld oi lf lg lh im bi translated">让我们考虑来自UCI的包含48842个实例和14个属性/特征的<a class="ae oj" href="https://www.kaggle.com/uciml/adult-census-income" rel="noopener ugc nofollow" target="_blank"> <strong class="km jd">成人人口普查收入预测数据集</strong> </a>。</p><blockquote class="ls lt lu"><p id="af01" class="kk kl lv km b kn ko kp kq kr ks kt ku lw kw kx ky lx la lb lc ly le lf lg lh im bi translated"><strong class="km jd">用Python实现数据预处理:</strong></p></blockquote><ol class=""><li id="0a81" class="lz ma it km b kn ko kr ks kv mb kz mc ld md lh me mf mg mh bi translated"><strong class="km jd">标签编码</strong>用于表1(下面给出)中提到的分类(非数字)特征和标签<em class="lv">收入。</em></li><li id="3626" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated"><strong class="km jd">特征选择</strong>基于额外树分类器在整个数据集上给出的特征重要性分数来完成(如表1所示)。由于<em class="lv">种族和本国</em>给出了最低的特征重要性分数，这两个特征在模型开发中被排除。</li><li id="213a" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated"><strong class="km jd">一键编码</strong>用于具有两个以上类别的分类特征。在一位热编码中，分类特征分成子特征，每个子特征对应于其类别之一(主要分类特征),假设二进制值为0/1。在这里，分类特征、<em class="lv">工作类别</em>、<em class="lv">教育程度</em>、<em class="lv">婚姻状况</em>、<em class="lv">职业、</em>和<em class="lv">关系</em>被一键编码。由于<em class="lv">性别</em>是一个只有两个子类别的特征(<em class="lv">男性</em>和<em class="lv">女性</em>)，因此不再进行一键编码以避免维数灾难。</li></ol><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ok"><img src="../Images/25395503d41f4c3eebf669a20ff091ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rLYyXZbFUY46XJo1TXqllA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr">表1 </strong></figcaption></figure><p id="77a0" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">在特性选择后用Python实现一键编码…</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="3b76" class="mu mv it mq b gy mw mx l my mz">import numpy as np<br/>import pandas as pd<br/>from sklearn.preprocessing import OneHotEncoder</span><span id="6a61" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">Label Encoding and Feature Selection is over ....</strong></span><span id="e925" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">1. Loading the modified dataset after Label Encoding</strong><br/>df = pd.read_csv('adult.csv') <br/># Loading of Selected Features into X<br/>X = df.iloc[:,[0,1,2,3,4,5,6,7,9,10,11,12]].values</span><span id="ae8c" class="mu mv it mq b gy na mx l my mz"># Loading of the Label into y<br/>y = df.iloc[:,14].values</span><span id="7316" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">2. One Hot Encoding ....</strong><br/>onehotencoder = OneHotEncoder(categorical_features = [1,3,5,6,7])<br/>X = onehotencoder.fit_transform(X).toarray()</span></pre><p id="8bec" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">现在，这个问题中的类标签是<strong class="km jd">二进制</strong>。这意味着类别标签假定有2个值，即有2个类别。所以，这是一个二元分类问题。</p><blockquote class="ls lt lu"><p id="7125" class="kk kl lv km b kn ko kp kq kr ks kt ku lw kw kx ky lx la lb lc ly le lf lg lh im bi translated"><strong class="km jd">班级分布可视化</strong></p></blockquote><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="8d0d" class="mu mv it mq b gy mw mx l my mz"># <strong class="mq jd">Getting the no. of instances with Label 0</strong><br/>n_class_0 = df[df['income']==0].shape[0]</span><span id="a526" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">Getting the no. of instances with label 1</strong><br/>n_class_1 = df[df['income']==1].shape[0]</span><span id="4f29" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">Bar Visualization of Class Distribution<br/></strong>import matplotlib.pyplot as plt # required library</span><span id="c6cf" class="mu mv it mq b gy na mx l my mz">x = ['0', '1']<br/>y = np.array([n_class_0, n_class_1])<br/>plt.bar(x, y)<br/>plt.xlabel('Labels/Classes')<br/>plt.ylabel('Number of Instances')<br/>plt.title('Distribution of Labels/Classes in the Dataset')</span></pre><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ff14936b81166ccdc9e92669c0c0d0fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*B8vjLzwehKa7_yTibeYa_A.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr">成人数据集中的类别分布</strong></figcaption></figure><p id="2712" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">因此，在给定的数据集中，在类标签为“1”的少数类和类标签为“0”的多数类之间存在严重的不平衡。</p><p id="42d4" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">现在，有两种可能的方法:</p><ol class=""><li id="7841" class="lz ma it km b kn ko kr ks kv mb kz mc ld md lh me mf mg mh bi translated">将数据集混洗并分成训练集和验证集，并对训练数据集应用SMOTe。(<strong class="km jd">第一次接近</strong>)</li><li id="901a" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated">将SMOTe作为一个整体应用于给定的数据集，然后将数据集混洗分割成训练集和验证集。(<strong class="km jd">第二种方法</strong>)</li></ol><p id="ef39" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">在Stack Overflow等许多网络资源和许多个人博客中，第二种方法被称为<strong class="km jd">错误的过采样方法</strong>。特别是，我看过尼克·贝克尔的个人博客【1】，他提到第二种方法是错误的，理由如下:</p><p id="95e9" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">"<em class="lv">SMOTe在整个数据集上的应用创建了相似的实例，因为该算法基于k-最近邻理论。由于这个原因，在给定数据集上应用SMOTe后进行分割，会导致信息从验证集泄漏到训练集，从而导致分类器或机器学习模型高估其准确性和其他性能指标</em></p><p id="7177" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">他还通过考虑数据集，借助实际生活中的例子证明了这一点。他使用了不平衡学习工具箱[2]来应用SMOTe。说实话，我自己从来没有真正弄清楚工具箱的文档。因此，我更喜欢像上面演示的那样从头开始实现SMOTe算法。在这篇文章中，我将证明第二种方法是<strong class="km jd">没有错</strong>！！！</p><p id="38b5" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">让我们遵循第一种方法，因为它已被广泛接受。</p><p id="489a" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">为了证明第二种方法没有错，我将把整个数据集分成<strong class="km jd">训练验证</strong>和<strong class="km jd">测试</strong>组。测试集将作为未知的实例集保持独立。同样的实现如下—</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="fb8c" class="mu mv it mq b gy mw mx l my mz">from sklearn.model_selection import train_test_split <br/>X_train, X_test, y_train, y_test = train_test_split(X, y,<br/>                                   test_size=0.2, random_state=1234)<br/># <strong class="mq jd">X_train and y_train is the Train-Validation Set</strong><br/># <strong class="mq jd">X_test and y_test is the Test Set separated out</strong></span></pre><ol class=""><li id="8f9c" class="lz ma it km b kn ko kr ks kv mb kz mc ld md lh me mf mg mh bi translated">现在，在<strong class="km jd">训练验证</strong>组中，第一和第二种方法将根据具体情况应用。</li><li id="4e18" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated">然后，将对两个模型(按照第一种方法和第二种方法开发)的相同独立的未知实例集(<strong class="km jd">测试集</strong>)进行性能分析</li></ol><blockquote class="ls lt lu"><p id="8984" class="kk kl lv km b kn ko kp kq kr ks kt ku lw kw kx ky lx la lb lc ly le lf lg lh im bi translated"><strong class="km jd">遵循分割后使用SMOTe的第一种方法</strong></p></blockquote><p id="e8b3" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">= &gt;将<strong class="km jd">训练验证</strong>集合拆分为<strong class="km jd">训练</strong>和<strong class="km jd">验证</strong>集合。同样的实现如下—</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="ea44" class="mu mv it mq b gy mw mx l my mz">X_train, X_v, y_train, y_v = train_test_split(X_train, y_train,<br/>                             test_size=0.2, random_state=2341)<br/># <strong class="mq jd">X_train and y_train is the Training Set<br/></strong># <strong class="mq jd">X_v and y_v is the Validation Set</strong></span></pre><p id="1e04" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">= &gt;仅对训练集应用SMOTe</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="4488" class="mu mv it mq b gy mw mx l my mz"># <strong class="mq jd">1. Getting the number of Minority Class Instances in Training Set</strong><br/>import numpy as np # required library<br/>unique, counts = np.unique(y_train, return_counts=True)<br/>minority_shape = dict(zip(unique, counts))[1]</span><span id="984b" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">2.</strong> <strong class="mq jd">Storing the minority class instances separately</strong><br/>x1 = np.ones((minority_shape, X_train.shape[1]))<br/>k=0<br/>for i in range(0,X_train.shape[0]):<br/>    if y_train[i] == 1.0:<br/>        x1[k] = X_train[i]<br/>        k = k + 1</span><span id="f082" class="mu mv it mq b gy na mx l my mz"># 3. <strong class="mq jd">Applying 100% SMOTe<br/></strong>sampled_instances = SMOTE(x1, 100)</span><span id="a337" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">Keeping the artificial instances and original instances together</strong><br/>X_f = np.concatenate((X_train,sampled_instances), axis = 0)<br/>y_sampled_instances = np.ones(minority_shape)<br/>y_f = np.concatenate((y_train,y_sampled_instances), axis=0)<br/># <strong class="mq jd">X_f and y_f are the Training Set Features and Labels respectively</strong> </span></pre><blockquote class="ls lt lu"><p id="5654" class="kk kl lv km b kn ko kp kq kr ks kt ku lw kw kx ky lx la lb lc ly le lf lg lh im bi translated"><strong class="km jd">使用梯度推进分类器的模型训练</strong></p></blockquote><p id="338e" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">梯度提升分类器用于训练机器学习模型。网格搜索用于梯度推进分类器，以获得最佳的超参数集，即估计器的数量和max_depth。</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="61e7" class="mu mv it mq b gy mw mx l my mz">from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.model_selection import GridSearchCV</span><span id="5a04" class="mu mv it mq b gy na mx l my mz">parameters = {'n_estimators':[100,150,200,250,300,350,400,450,500],<br/>              'max_depth':[3,4,5]}<br/>clf= GradientBoostingClassifier()<br/>grid_search = GridSearchCV(param_grid = parameters, estimator = clf,<br/>                           verbose = 3)</span><span id="58e7" class="mu mv it mq b gy na mx l my mz">grid_search_1 = grid_search.fit(X_f,y_f)</span></pre><blockquote class="ls lt lu"><p id="7efb" class="kk kl lv km b kn ko kp kq kr ks kt ku lw kw kx ky lx la lb lc ly le lf lg lh im bi translated"><strong class="km jd">遵循分割前使用SMOTe的第二种方法</strong></p></blockquote><p id="b593" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">= &gt;对整个<strong class="km jd">训练验证</strong>集合应用SMOTe:</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="b1b7" class="mu mv it mq b gy mw mx l my mz"># <strong class="mq jd">1. Getting the number of Minority Class Instances in Training Set</strong><br/>unique, counts = np.unique(y_train, return_counts=True)<br/>minority_shape = dict(zip(unique, counts))[1]</span><span id="677b" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">2.</strong> <strong class="mq jd">Storing the minority class instances separately</strong><br/>x1 = np.ones((minority_shape, X_train.shape[1]))<br/>k=0<br/>for i in range(0,X_train.shape[0]):<br/>    if y_train[i] == 1.0:<br/>        x1[k] = X_train[i]<br/>        k = k + 1</span><span id="a0c8" class="mu mv it mq b gy na mx l my mz"># 3. <strong class="mq jd">Applying 100% SMOTe<br/></strong>sampled_instances = SMOTE(x1, 100)</span><span id="792e" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">Keeping the artificial instances and original instances together</strong><br/>X_f = np.concatenate((X_train,sampled_instances), axis = 0)<br/>y_sampled_instances = np.ones(minority_shape)<br/>y_f = np.concatenate((y_train,y_sampled_instances), axis=0)<br/># <strong class="mq jd">X_f and y_f are the Train-Validation Set Features and Labels respectively</strong></span></pre><p id="da5f" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">= &gt;将<strong class="km jd">训练验证</strong>组拆分为<strong class="km jd">训练</strong>和<strong class="km jd">验证</strong>组。同样的实现如下—</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="02c1" class="mu mv it mq b gy mw mx l my mz">X_train, X_v, y_train, y_v = train_test_split(X_f, y_f,<br/>                             test_size=0.2, random_state=9876)</span><span id="7b2b" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">X_train and y_train is the Training Set<br/></strong># <strong class="mq jd">X_v and y_v is the Validation Set</strong></span></pre><blockquote class="ls lt lu"><p id="f6f1" class="kk kl lv km b kn ko kp kq kr ks kt ku lw kw kx ky lx la lb lc ly le lf lg lh im bi translated"><strong class="km jd">使用梯度推进分类器的模型训练</strong></p></blockquote><p id="6662" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">类似地，网格搜索应用于梯度推进分类器</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="a271" class="mu mv it mq b gy mw mx l my mz">from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.model_selection import GridSearchCV</span><span id="5af4" class="mu mv it mq b gy na mx l my mz">parameters = {'n_estimators':[100,150,200,250,300,350,400,450,500],<br/>              'max_depth':[3,4,5]}<br/>clf= GradientBoostingClassifier()<br/>grid_search = GridSearchCV(param_grid = parameters, estimator = clf,<br/>                           verbose = 3)</span><span id="676b" class="mu mv it mq b gy na mx l my mz">grid_search_2 = grid_search.fit(X_train,y_train)</span></pre><blockquote class="ls lt lu"><p id="c0ea" class="kk kl lv km b kn ko kp kq kr ks kt ku lw kw kx ky lx la lb lc ly le lf lg lh im bi translated"><strong class="km jd">性能分析和比较</strong></p></blockquote><p id="6a1f" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">用于比较和分析的性能指标有:</p><ol class=""><li id="9b05" class="lz ma it km b kn ko kr ks kv mb kz mc ld md lh me mf mg mh bi translated">测试装置上的精度<strong class="km jd">(测试精度)</strong></li><li id="3705" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated">测试装置上的精度<strong class="km jd"/></li><li id="f21b" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated">召回测试装置上的<strong class="km jd"/></li><li id="f5b3" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated">f1-测试集上的<strong class="km jd">得分</strong></li></ol><p id="d266" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">除了这些比较度量，还计算训练准确度(训练集)和验证准确度(验证集)。</p><pre class="lj lk ll lm gt mp mq mr ms aw mt bi"><span id="bdf5" class="mu mv it mq b gy mw mx l my mz"># <strong class="mq jd">MODEL 1 PERFORMANCE ANALYSIS<br/></strong>model1 = GradientBoostingClassifier(n_estimators = 250, max_depth = 5).fit(X_f, y_f) <strong class="mq jd"># best hyperparameters obtained from grid_search_1</strong></span><span id="ddb7" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">1. Training Accuracy for Model 1 (following Approach 1)</strong><br/>print(grid_search_1.score(X_f, y_f))</span><span id="a2a6" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">2. Validation Accuracy on Validation Set for Model 1</strong> <br/>print(grid_search_1.score(X_v, y_v))</span><span id="ec1f" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">3. Test Accuracy on Test Set for Model 1</strong><br/>print(grid_search_1.score(X_test, y_test))</span><span id="bd56" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">4. Precision, Recall and F1-Score on the Test Set for Model 1</strong><br/>from sklearn.metrics import classification_report<br/>predictions=grid_search_1.predict(X_test)<br/>print(classification_report(y_test,predictions))</span><span id="eb62" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">MODEL 2 PERFORMANCE ANALYSIS<br/></strong>model2 = GradientBoostingClassifier(n_estimators = 300, max_depth = 4).fit(X_train, y_train) <strong class="mq jd"># best hyperparameters obtained from grid_search_2</strong></span><span id="1130" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">5. Training Accuracy for Model 2(following Approach 2)</strong><br/>print(grid_search_2.score(X_train, y_train))</span><span id="5099" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">6. Validation Accuracy on Validation Set for Model 2</strong><br/>print(grid_search_2.score(X_v, y_v))</span><span id="cd4c" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">3. Test Accuracy on Test Set for Model 2</strong><br/>print(grid_search_2.score(X_test, y_test))</span><span id="200c" class="mu mv it mq b gy na mx l my mz"># <strong class="mq jd">4. Precision, Recall and F1-Score on the Test Set for Model 2</strong><br/>from sklearn.metrics import classification_report<br/>predictions=grid_search_2.predict(X_test)<br/>print(classification_report(y_test,predictions))</span></pre><p id="d98e" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">模型1和模型2的训练集和验证集精度为:</p><ol class=""><li id="806c" class="lz ma it km b kn ko kr ks kv mb kz mc ld md lh me mf mg mh bi translated">训练准确率(模型1):91.6586585866686</li><li id="81c2" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated">训练准确率(模型2):92.533327%</li><li id="9bb3" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated">验证的准确性(模型1):86.32%。56860.88868888661</li><li id="1caf" class="lz ma it km b kn mi kr mj kv mk kz ml ld mm lh me mf mg mh bi translated">验证的准确性(模型2):89.5864645868686</li></ol><p id="c5b8" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">因此，从这里显然<strong class="km jd">第二种方法</strong>显示了更高的验证精度，但是没有在完全未知的相同测试集上进行测试，无法得出任何结论。表2显示了两个模型在测试集上的性能对比图。</p><figure class="lj lk ll lm gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi om"><img src="../Images/dd5c30a92f619017d9e4865634368c2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zoN0cE7SqPVvgCF2DI5Blg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk translated">表2</figcaption></figure><p id="be85" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">因此，很明显，已经证明无论差异有多小<strong class="km jd">方法2显然比方法1 </strong>更成功，因此，不能说Nick Becker [1]的上述理由是错误的，因为</p><p id="856a" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><em class="lv">“虽然SMOTe创建了类似的实例，但另一方面，该属性不仅对于类别不平衡减少和数据增加是必需的，而且对于找到适合于模型训练的最佳训练集也是必需的。如果训练集不通用，如何增强模型性能？就从验证到训练集的信息流失而言，即使它发生，它也有助于使训练集变得更好，并有助于稳健的机器学习模型开发，因为已经证明，对于完全未知的实例，</em> <strong class="km jd"> <em class="lv">方法2 </em> </strong> <em class="lv">比</em> <strong class="km jd"> <em class="lv">方法1</em></strong><em class="lv"/></p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><blockquote class="ls lt lu"><p id="27f4" class="kk kl lv km b kn ko kp kq kr ks kt ku lw kw kx ky lx la lb lc ly le lf lg lh im bi translated">参考</p></blockquote><p id="9a70" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">[1]<a class="ae oj" href="https://beckernick.github.io/oversampling-modeling/" rel="noopener ugc nofollow" target="_blank">https://beckernick.github.io/oversampling-modeling/</a></p><p id="a7ee" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><a class="ae oj" href="https://imbalanced-learn.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank">https://imbalanced-learn.readthedocs.io/en/stable/</a></p><p id="c016" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">[3] Chawla，Nitesh V .等人，“SMOTE:合成少数过采样技术”人工智能研究杂志2002年第16期:321–357页。</p></div></div>    
</body>
</html>