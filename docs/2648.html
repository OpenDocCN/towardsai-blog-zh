<html>
<head>
<title>Local Outlier Factor (LOF) For Anomaly Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于异常检测的局部异常因子(LOF)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/local-outlier-factor-lof-for-anomaly-detection-b4fdaebc98fe?source=collection_archive---------0-----------------------#2022-03-30">https://pub.towardsai.net/local-outlier-factor-lof-for-anomaly-detection-b4fdaebc98fe?source=collection_archive---------0-----------------------#2022-03-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f4fd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">LOF新奇和异常检测奖</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ace606bb3f3bc1e6d8c6560a2cb3489d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q4TDGpI73Egu2qhZv5SKEQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图像归GrabNGoInfo.com所有</figcaption></figure><p id="3e39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">局部异常因子(LOF)是一种无监督的异常检测模型。它将每个数据点的局部密度与其邻居进行比较，并将密度较低的数据点识别为异常值或离群值。</p><p id="9f19" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本教程中，我们将讨论</p><ul class=""><li id="49ec" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">新奇感检测和离群点检测有什么区别？</li><li id="5501" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">什么时候使用新颖性检测和异常值检测？</li><li id="a0a0" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">如何使用局部离群因子(LOF)进行新颖性检测？</li><li id="64b4" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">如何使用局部异常因子(LOF)进行异常或异常检测？</li></ul><p id="df4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">本帖资源:</strong></p><ul class=""><li id="32a1" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">本教程的视频在<a class="ae mi" href="https://www.youtube.com/watch?v=CiJ95in4KQc&amp;list=PLVppujud2yJo0qnXjWVAa8h7fxbFJHtfJ&amp;index=8" rel="noopener ugc nofollow" target="_blank"> YouTube上</a></li><li id="e034" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">Python代码在帖子最后。点击<a class="ae mi" href="https://mailchi.mp/8b577b6aa257/wf8l4w4tlv" rel="noopener ugc nofollow" target="_blank">此处</a>查看笔记本。</li><li id="32a5" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">更多关于<a class="ae mi" href="https://www.youtube.com/playlist?list=PLVppujud2yJo0qnXjWVAa8h7fxbFJHtfJ" rel="noopener ugc nofollow" target="_blank">异常检测的视频教程</a></li><li id="ce60" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">更多关于<a class="ae mi" href="https://medium.com/@AmyGrabNGoInfo/list/imbalanced-classification-and-anomalies-detection-dc908de4382d" rel="noopener">异常检测</a>的博文</li></ul><p id="a404" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们开始吧！</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="de3b" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">步骤1:导入库</h1><p id="a35f" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">第一步是导入库。我们需要从<code class="fe nn no np nq b">sklearn</code>导入<code class="fe nn no np nq b">make_classification</code>来创建建模数据集，导入<code class="fe nn no np nq b">pandas</code>和<code class="fe nn no np nq b">numpy</code>进行数据处理，<code class="fe nn no np nq b">Counter</code>会帮助我们统计记录的数量。</p><p id="2142" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nn no np nq b">Matplotlib</code>是为了可视化。</p><p id="1c02" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还需要<code class="fe nn no np nq b">train_test_split</code>来创建一个训练和验证数据集。<code class="fe nn no np nq b">LocalOutlierFactor</code>用于建模，<code class="fe nn no np nq b">classification_report</code>用于模型性能评估。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="7514" class="nv mr it nq b gy nw nx l ny nz"># Synthetic dataset<br/>from sklearn.datasets import make_classification</span><span id="d036" class="nv mr it nq b gy oa nx l ny nz"># Data processing<br/>import pandas as pd<br/>import numpy as np<br/>from collections import Counter</span><span id="a3c9" class="nv mr it nq b gy oa nx l ny nz"># Visualization<br/>import matplotlib.pyplot as plt</span><span id="ad35" class="nv mr it nq b gy oa nx l ny nz"># Model and performance<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.neighbors import LocalOutlierFactor<br/>from sklearn.metrics import classification_report</span></pre><h1 id="8cc0" class="mq mr it bd ms mt ob mv mw mx oc mz na jz od ka nc kc oe kd ne kf of kg ng nh bi translated">步骤2:创建带有异常的数据集</h1><p id="44b6" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">使用sklearn库中的make_classification，我们创建了两个类，多数类和少数类的比率为0.995:0.005。两个信息特征被作为预测因子。我们没有在该数据集中包含任何冗余或重复的要素。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="1e33" class="nv mr it nq b gy nw nx l ny nz"># Create an imbalanced dataset<br/>X, y = make_classification<strong class="nq iu">(</strong>n_samples=100000, n_features=2, n_informative=2,n_redundant=0, n_repeated=0,  n_classes=2,n_clusters_per_class=1, weights=<strong class="nq iu">[</strong>0.995, 0.005<strong class="nq iu">]</strong>,class_sep=0.5, random_state=0<strong class="nq iu">)</strong></span><span id="b817" class="nv mr it nq b gy oa nx l ny nz"># Convert the data from numpy array to a pandas dataframe<br/>df = pd.DataFrame<strong class="nq iu">({</strong>'feature1': X<strong class="nq iu">[</strong>:, 0<strong class="nq iu">]</strong>, 'feature2': X<strong class="nq iu">[</strong>:, 1<strong class="nq iu">]</strong>, 'target': y<strong class="nq iu">})</strong></span><span id="5627" class="nv mr it nq b gy oa nx l ny nz"># Check the target distribution<br/>df<strong class="nq iu">[</strong>'target'<strong class="nq iu">]</strong>.value_counts<strong class="nq iu">(</strong>normalize = True<strong class="nq iu">)</strong></span></pre><p id="fbc7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出显示，我们有大约1%的数据属于少数类，99%属于多数类，这意味着我们有大约1%的异常。</p><h1 id="5301" class="mq mr it bd ms mt ob mv mw mx oc mz na jz od ka nc kc oe kd ne kf of kg ng nh bi translated">步骤3:训练测试分割</h1><p id="b57f" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">在这一步中，我们将数据集分成80%的训练数据和20%的验证数据。random_state确保我们每次都有相同的训练测试分割。random_state的种子号不一定是42，可以是任何数字。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="c657" class="nv mr it nq b gy nw nx l ny nz"># Train test split<br/>X_train, X_test, y_train, y_test = train_test_split<strong class="nq iu">(</strong>X, y, test_size=0.2, random_state=42<strong class="nq iu">)</strong></span><span id="3343" class="nv mr it nq b gy oa nx l ny nz"># Check the number of records<br/>print<strong class="nq iu">(</strong>'The number of records in the training dataset is', X_train.shape<strong class="nq iu">[</strong>0<strong class="nq iu">])</strong></span><span id="6d26" class="nv mr it nq b gy oa nx l ny nz">print<strong class="nq iu">(</strong>'The number of records in the test dataset is', X_test.shape<strong class="nq iu">[</strong>0<strong class="nq iu">])</strong></span><span id="b106" class="nv mr it nq b gy oa nx l ny nz">print<strong class="nq iu">(</strong>f"The training dataset has {sorted(Counter(y_train).items())[0][1]} records for the majority class and {sorted(Counter(y_train).items())[1][1]} records for the minority class."<strong class="nq iu">)</strong></span></pre><p id="a919" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练测试拆分为我们提供了80，000条训练数据集记录和20，000条验证数据集记录。因此，在训练数据集中，我们有来自多数类的79，183个数据点和来自少数类的817个数据点。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="2e3c" class="nv mr it nq b gy nw nx l ny nz">The number of records in the training dataset is 80000</span><span id="4f23" class="nv mr it nq b gy oa nx l ny nz">The number of records in the test dataset is 20000</span><span id="6873" class="nv mr it nq b gy oa nx l ny nz">The training dataset has 79183 records for the majority class and 817 records for the minority class.</span></pre><h1 id="6d36" class="mq mr it bd ms mt ob mv mw mx oc mz na jz od ka nc kc oe kd ne kf of kg ng nh bi translated">步骤4:异常值/异常检测与新颖性检测</h1><p id="72de" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">局部异常因子(LOF)算法可用于异常/异常检测和新奇检测。离群/异常检测和新奇检测之间的区别在于训练数据集。</p><p id="7693" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">异常值/异常检测包括训练数据集中的异常值。该算法适用于高密度数据区域，忽略了异常值和异常值。</p><p id="dd34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">新颖性检测只包括训练模型时的正常数据点。然后，该模型将采用具有异常值/异常值的新数据集进行预测。新颖性检测中的异常值也称为新颖性。</p><p id="ac48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">什么时候使用新颖性检测和异常值检测？这取决于有什么数据可用。如果我们有带有异常值标签的数据集，我们可以使用其中任何一个。否则，我们只能使用离群点检测，因为我们无法获得仅含正常数据的训练数据集。</p><h1 id="8a1c" class="mq mr it bd ms mt ob mv mw mx oc mz na jz od ka nc kc oe kd ne kf of kg ng nh bi translated">步骤5:使用局部异常因子(LOF)的新颖性检测</h1><p id="3fac" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">Python的<code class="fe nn no np nq b">sklearn</code>库实现了本地离群因子(LOF)。要使用新颖性检测，我们需要将超参数新颖性设置为True。<code class="fe nn no np nq b">fit_predictfit</code>不可用，因为该算法适合并预测不同的数据集。我们需要用所有正常数据拟合训练数据集，并预测包含异常值的测试数据集。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="99c0" class="nv mr it nq b gy nw nx l ny nz"># Keep only the normal data for the training dataset<br/>X_train_normal = X_train<strong class="nq iu">[</strong>np.where<strong class="nq iu">(</strong>y_train == 0<strong class="nq iu">)]</strong></span><span id="3c54" class="nv mr it nq b gy oa nx l ny nz"># Train the local outlier factor (LOF) model for novelty detection<br/>lof_novelty = LocalOutlierFactor<strong class="nq iu">(</strong>n_neighbors=5, novelty=True<strong class="nq iu">)</strong>.fit<strong class="nq iu">(</strong>X_train_normal<strong class="nq iu">)</strong></span><span id="295b" class="nv mr it nq b gy oa nx l ny nz"># Predict novelties<br/>prediction_novelty = lof_novelty.predict<strong class="nq iu">(</strong>X_test<strong class="nq iu">)</strong></span><span id="bb6b" class="nv mr it nq b gy oa nx l ny nz"># Change the anomalies' values to make it consistent with the true values<br/>prediction_novelty = <strong class="nq iu">[</strong>1 if i==-1 else 0 for i in prediction_novelty<strong class="nq iu">]</strong></span><span id="e765" class="nv mr it nq b gy oa nx l ny nz"># Check the model performance<br/>print<strong class="nq iu">(</strong>classification_report<strong class="nq iu">(</strong>y_test, prediction_novelty<strong class="nq iu">))</strong></span></pre><p id="00fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到，局部异常值因子(LOF)新颖性检测捕获了2%的异常值/异常值。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="f76b" class="nv mr it nq b gy nw nx l ny nz">          precision    recall  f1-score   support</span><span id="0135" class="nv mr it nq b gy oa nx l ny nz">      0       0.99      1.00      0.99     19787</span><span id="436a" class="nv mr it nq b gy oa nx l ny nz">      1       0.05      0.02      0.03       213</span><span id="5a5b" class="nv mr it nq b gy oa nx l ny nz">accuracy                           0.99     20000</span><span id="a09c" class="nv mr it nq b gy oa nx l ny nz">macro avg       0.52      0.51      0.51     20000</span><span id="cec4" class="nv mr it nq b gy oa nx l ny nz">weighted avg       0.98      0.99      0.98     20000</span></pre><h1 id="bf83" class="mq mr it bd ms mt ob mv mw mx oc mz na jz od ka nc kc oe kd ne kf of kg ng nh bi translated">步骤6:使用局部异常值因子(LOF)的异常值检测</h1><p id="3012" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">用于异常值检测的局部异常值因子(LOF)在同一数据集上训练和预测。因此，如果我们想要比较新奇检测和离群点检测之间的模型性能，我们需要在测试数据集上进行拟合和预测。我们还需要设置<code class="fe nn no np nq b">novelty to</code>为假，以启用异常值检测算法。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="bef2" class="nv mr it nq b gy nw nx l ny nz"># The local outlier factor (LOF) model for outlier detection<br/>lof_outlier = LocalOutlierFactor<strong class="nq iu">(</strong>n_neighbors=5, novelty=False<strong class="nq iu">)</strong></span><span id="5c47" class="nv mr it nq b gy oa nx l ny nz"># Predict novelties<br/>prediction_outlier = lof_outlier.fit_predict<strong class="nq iu">(</strong>X_test<strong class="nq iu">)</strong></span><span id="1fa0" class="nv mr it nq b gy oa nx l ny nz"># Change the anomalies' values to make it consistent with the true values<br/>prediction_outlier = <strong class="nq iu">[</strong>1 if i==-1 else 0 for i in prediction_outlier<strong class="nq iu">]</strong></span><span id="ac75" class="nv mr it nq b gy oa nx l ny nz"># Check the model performance<br/>print<strong class="nq iu">(</strong>classification_report<strong class="nq iu">(</strong>y_test, prediction_outlier<strong class="nq iu">))</strong></span></pre><p id="d0e5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到，局部异常值因子(LOF)异常值/异常值检测捕获了3%的异常值/异常值，这略好于新颖性检测结果。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="470d" class="nv mr it nq b gy nw nx l ny nz">          precision    recall  f1-score   support</span><span id="425f" class="nv mr it nq b gy oa nx l ny nz">      0       0.99      0.99      0.99     19787</span><span id="aa8e" class="nv mr it nq b gy oa nx l ny nz">      1       0.06      0.03      0.04       213</span><span id="ce08" class="nv mr it nq b gy oa nx l ny nz">accuracy                           0.98     20000</span><span id="1884" class="nv mr it nq b gy oa nx l ny nz">macro avg       0.53      0.51      0.52     20000</span><span id="a226" class="nv mr it nq b gy oa nx l ny nz">weighted avg       0.98      0.98      0.98     20000</span></pre><h1 id="35f3" class="mq mr it bd ms mt ob mv mw mx oc mz na jz od ka nc kc oe kd ne kf of kg ng nh bi translated">第七步:视觉化</h1><p id="d646" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">这一步将绘制数据点，并检查实际、LOF新奇检测和LOF异常值检测之间的差异。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="96b3" class="nv mr it nq b gy nw nx l ny nz"># Put the testing dataset and predictions in the same dataframe<br/>df_test = pd.DataFrame<strong class="nq iu">(</strong>X_test, columns=<strong class="nq iu">[</strong>'feature1', 'feature2'<strong class="nq iu">])<br/></strong>df_test<strong class="nq iu">[</strong>'y_test'<strong class="nq iu">]</strong> = y_test<br/>df_test<strong class="nq iu">[</strong>'prediction_novelty'<strong class="nq iu">]</strong> = prediction_novelty<br/>df_test<strong class="nq iu">[</strong>'prediction_outlier'<strong class="nq iu">]</strong> = prediction_outlier</span><span id="0981" class="nv mr it nq b gy oa nx l ny nz"># Visualize the actual and predicted anomalies<br/>fig, <strong class="nq iu">(</strong>ax0, ax1, ax2<strong class="nq iu">)</strong>=plt.subplots<strong class="nq iu">(</strong>1,3, sharey=True, figsize=<strong class="nq iu">(</strong>20,6<strong class="nq iu">))</strong></span><span id="70cc" class="nv mr it nq b gy oa nx l ny nz"># Ground truth<br/>ax0.set_title<strong class="nq iu">(</strong>'Original'<strong class="nq iu">)<br/></strong>ax0.scatter<strong class="nq iu">(</strong>df_test<strong class="nq iu">[</strong>'feature1'<strong class="nq iu">]</strong>, df_test<strong class="nq iu">[</strong>'feature2'<strong class="nq iu">]</strong>, c=df_test<strong class="nq iu">[</strong>'y_test'<strong class="nq iu">]</strong>, cmap='rainbow'<strong class="nq iu">)</strong></span><span id="c915" class="nv mr it nq b gy oa nx l ny nz"># Local Outlier Factor (LOF) Novelty Detection<br/>ax1.set_title<strong class="nq iu">(</strong>'LOF Novelty Detection'<strong class="nq iu">)<br/></strong>ax1.scatter<strong class="nq iu">(</strong>df_test<strong class="nq iu">[</strong>'feature1'<strong class="nq iu">]</strong>, df_test<strong class="nq iu">[</strong>'feature2'<strong class="nq iu">]</strong>, c=df_test<strong class="nq iu">[</strong>'prediction_novelty'<strong class="nq iu">]</strong>, cmap='rainbow'<strong class="nq iu">)</strong></span><span id="f44e" class="nv mr it nq b gy oa nx l ny nz"># Local Outlier Factor (LOF) Outlier / Anomaly Detection<br/>ax2.set_title<strong class="nq iu">(</strong>'LOF Outlier / Anomaly Detection'<strong class="nq iu">)</strong>ax2.scatter<strong class="nq iu">(</strong>df_test<strong class="nq iu">[</strong>'feature1'<strong class="nq iu">]</strong>, df_test<strong class="nq iu">[</strong>'feature2'<strong class="nq iu">]</strong>, c=df_test<strong class="nq iu">[</strong>'prediction_outlier'<strong class="nq iu">]</strong>, cmap='rainbow'<strong class="nq iu">)</strong></span></pre><p id="e9d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到，在这个例子中，异常值检测比新颖性检测识别出更多的异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/68c076415a306fdfc9926bbcc11acb2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AMyB3MoKN4bCm-zz.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">GrabNGoInfo.com LOF新奇检测与异常检测</figcaption></figure><h1 id="f291" class="mq mr it bd ms mt ob mv mw mx oc mz na jz od ka nc kc oe kd ne kf of kg ng nh bi translated">步骤8:将所有代码放在一起</h1><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="500f" class="nv mr it nq b gy nw nx l ny nz">###### Step 1: Import Libraries</span><span id="73e4" class="nv mr it nq b gy oa nx l ny nz"># Synthetic dataset<br/>from sklearn.datasets import make_classification</span><span id="fa1e" class="nv mr it nq b gy oa nx l ny nz"># Data processing<br/>import pandas as pd<br/>import numpy as np<br/>from collections import Counter</span><span id="4f68" class="nv mr it nq b gy oa nx l ny nz"># Visualization<br/>import matplotlib.pyplot as plt</span><span id="c1d5" class="nv mr it nq b gy oa nx l ny nz"># Model and performance<br/>from sklearn.neighbors import LocalOutlierFactor<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import classification_report</span><span id="7103" class="nv mr it nq b gy oa nx l ny nz">###### Step 2: Create Dataset With Anomalies</span><span id="8cf5" class="nv mr it nq b gy oa nx l ny nz"># Create an imbalanced dataset<br/>X, y = make_classification(n_samples=100000, n_features=2, n_informative=2,<br/>                           n_redundant=0, n_repeated=0, n_classes=2,<br/>                           n_clusters_per_class=1,<br/>                           weights=[0.995, 0.005],<br/>                           class_sep=0.5, random_state=0)</span><span id="cf58" class="nv mr it nq b gy oa nx l ny nz"># Convert the data from numpy array to a pandas dataframe<br/>df = pd.DataFrame({'feature1': X[:, 0], 'feature2': X[:, 1], 'target': y})</span><span id="4b12" class="nv mr it nq b gy oa nx l ny nz"># Check the target distribution<br/>df['target'].value_counts(normalize = True)</span><span id="f155" class="nv mr it nq b gy oa nx l ny nz">###### Step 3: Train Test Split</span><span id="8e21" class="nv mr it nq b gy oa nx l ny nz"># Train test split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span><span id="dc12" class="nv mr it nq b gy oa nx l ny nz"># Check the number of records<br/>print('The number of records in the training dataset is', X_train.shape[0])<br/>print('The number of records in the test dataset is', X_test.shape[0])<br/>print(f"The training dataset has {sorted(Counter(y_train).items())[0][1]} records for the majority class and {sorted(Counter(y_train).items())[1][1]} records for the minority class.")</span><span id="ef5b" class="nv mr it nq b gy oa nx l ny nz">###### Step 4: Local Outlier Factor (LOF) Algorithm</span><span id="7a57" class="nv mr it nq b gy oa nx l ny nz"># No code in this step</span><span id="0479" class="nv mr it nq b gy oa nx l ny nz">###### Step 5: Outlier / Anomaly Detection vs. Novelty Detection</span><span id="9bbb" class="nv mr it nq b gy oa nx l ny nz"># No code in this step</span><span id="84e3" class="nv mr it nq b gy oa nx l ny nz">###### Step 6: Novelty Detection Using Local Outlier Factor (LOF)</span><span id="b94c" class="nv mr it nq b gy oa nx l ny nz"># Keep only the normal data for the training dataset<br/>X_train_normal = X_train[np.where(y_train == 0)]</span><span id="cc85" class="nv mr it nq b gy oa nx l ny nz"># Train the local outlier factor (LOF) model for novelty detection<br/>lof_novelty = LocalOutlierFactor(n_neighbors=5, novelty=True).fit(X_train_normal)</span><span id="d4b2" class="nv mr it nq b gy oa nx l ny nz"># Predict novelties<br/>prediction_novelty = lof_novelty.predict(X_test)</span><span id="dd82" class="nv mr it nq b gy oa nx l ny nz"># Change the anomalies' values to make it consistent with the true values<br/>prediction_novelty = [1 if i==-1 else 0 for i in prediction_novelty]</span><span id="84ad" class="nv mr it nq b gy oa nx l ny nz"># Check the model performance<br/>print(classification_report(y_test, prediction_novelty))</span><span id="12d4" class="nv mr it nq b gy oa nx l ny nz">###### Step 7: Outlier Detection Using Local Outlier Factor (LOF)</span><span id="c01c" class="nv mr it nq b gy oa nx l ny nz"># The local outlier factor (LOF) model for outlier detection<br/>lof_outlier = LocalOutlierFactor(n_neighbors=5, novelty=False)</span><span id="afec" class="nv mr it nq b gy oa nx l ny nz"># Predict novelties<br/>prediction_outlier = lof_outlier.fit_predict(X_test)</span><span id="d112" class="nv mr it nq b gy oa nx l ny nz"># Change the anomalies' values to make it consistent with the true values<br/>prediction_outlier = [1 if i==-1 else 0 for i in prediction_outlier]</span><span id="c7b9" class="nv mr it nq b gy oa nx l ny nz"># Check the model performance<br/>print(classification_report(y_test, prediction_outlier))</span><span id="c014" class="nv mr it nq b gy oa nx l ny nz">###### Step 8: Visualization</span><span id="8673" class="nv mr it nq b gy oa nx l ny nz"># Put the testing dataset and predictions in the same dataframe<br/>df_test = pd.DataFrame(X_test, columns=['feature1', 'feature2'])<br/>df_test['y_test'] = y_test<br/>df_test['prediction_novelty'] = prediction_novelty<br/>df_test['prediction_outlier'] = prediction_outlier</span><span id="1b73" class="nv mr it nq b gy oa nx l ny nz"># Visualize the actual and predicted anomalies<br/>fig, (ax0, ax1, ax2)=plt.subplots(1,3, sharey=True, figsize=(20,6))</span><span id="9464" class="nv mr it nq b gy oa nx l ny nz"># Ground truth<br/>ax0.set_title('Original')<br/>ax0.scatter(df_test['feature1'], df_test['feature2'], c=df_test['y_test'], cmap='rainbow')</span><span id="13d8" class="nv mr it nq b gy oa nx l ny nz"># Local Outlier Factor (LOF) Novelty Detection<br/>ax1.set_title('LOF Novelty Detection')<br/>ax1.scatter(df_test['feature1'], df_test['feature2'], c=df_test['prediction_novelty'], cmap='rainbow')</span><span id="98fa" class="nv mr it nq b gy oa nx l ny nz"># Local Outlier Factor (LOF) Outlier / Anomaly Detection<br/>ax2.set_title('LOF Outlier / Anomaly Detection')<br/>ax2.scatter(df_test['feature1'], df_test['feature2'], c=df_test['prediction_outlier'], cmap='rainbow')</span></pre></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="dd05" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">摘要</h1><p id="6f15" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">本教程演示了如何使用局部异常因子(LOF)进行异常和新奇检测。</p><p id="35b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用Python中的<code class="fe nn no np nq b">sklearn</code>库，我们涵盖了</p><ul class=""><li id="c8b2" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">新奇感检测和离群点检测有什么区别？</li><li id="d3e4" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">什么时候使用新颖性检测和异常值检测？</li><li id="b381" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">如何使用局部离群因子(LOF)进行新颖性检测？</li><li id="e858" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">如何使用局部异常因子(LOF)进行异常或异常检测？</li></ul><p id="fc66" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更多教程可以在我的<a class="ae mi" href="https://www.youtube.com/channel/UCmbA7XB6Wb7bLwJw9ARPcYg" rel="noopener ugc nofollow" target="_blank"> YouTube频道</a>和GrabNGoInfo.com<a class="ae mi" href="https://grabngoinfo.com/tutorials/" rel="noopener ugc nofollow" target="_blank">频道</a>上找到</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="6c02" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">推荐教程</h1><ul class=""><li id="077a" class="lu lv it la b lb ni le nj lh oh ll oi lp oj lt lz ma mb mc bi translated"><a class="ae mi" href="https://medium.com/grabngoinfo/grabngoinfo-machine-learning-tutorials-inventory-9b9d78ebdd67" rel="noopener"> GrabNGoInfo机器学习教程盘点</a></li><li id="db53" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="https://medium.com/p/one-class-svm-for-anomaly-detection-6c97fdd6d8af" rel="noopener">用于异常检测的单级SVM</a></li><li id="b8e2" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="https://medium.com/p/3-ways-for-multiple-time-series-forecasting-using-prophet-in-python-7a0709a117f9" rel="noopener">使用Python中的Prophet进行多时间序列预测的3种方法</a></li><li id="11ee" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="https://medium.com/p/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python-7304aedf9037" rel="noopener">使用Python实现不平衡分类的四种过采样和欠采样方法</a></li><li id="eb85" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="https://medium.com/p/multivariate-time-series-forecasting-with-seasonality-and-holiday-effect-using-prophet-in-python-d5d4150eeb57" rel="noopener">利用Python中的Prophet进行具有季节性和假日效应的多元时间序列预测</a></li><li id="f034" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="https://medium.com/p/how-to-detect-outliers-data-science-interview-questions-and-answers-1e400284f6b4" rel="noopener">如何检测离群值|数据科学面试问答</a></li><li id="a66d" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="https://medium.com/p/time-series-anomaly-detection-using-prophet-in-python-877d2b7b14b4" rel="noopener">利用Python中的Prophet进行时间序列异常检测</a></li><li id="b4d0" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="https://medium.com/p/how-to-use-r-with-google-colab-notebook-610c3a2f0eab" rel="noopener">如何配合谷歌Colab笔记本使用R</a></li></ul><h1 id="adc1" class="mq mr it bd ms mt ob mv mw mx oc mz na jz od ka nc kc oe kd ne kf of kg ng nh bi translated">参考</h1><ul class=""><li id="d176" class="lu lv it la b lb ni le nj lh oh ll oi lp oj lt lz ma mb mc bi translated"><a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" rel="noopener ugc nofollow" target="_blank"> Sklearn本地离群因子(LOF) </a></li><li id="8145" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection" rel="noopener ugc nofollow" target="_blank"> Sklearn新颖性和异常值检测</a></li><li id="aee2" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.8948" rel="noopener ugc nofollow" target="_blank"> LOF论文</a></li><li id="6ae3" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae mi" href="https://www.youtube.com/watch?v=CePgbdVdLvg" rel="noopener ugc nofollow" target="_blank"> LOF算法</a></li></ul><div class="ok ol gp gr om on"><a href="https://medium.com/@AmyGrabNGoInfo/membership" rel="noopener follow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd iu gy z fp os fr fs ot fu fw is bi translated">通过我的推荐链接加入媒体-艾米GrabNGoInfo</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">medium.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ks on"/></div></div></a></div></div></div>    
</body>
</html>