<html>
<head>
<title>Understanding Non-Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解非线性回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/understanding-non-linear-regression-fbef9a396b71?source=collection_archive---------0-----------------------#2019-11-06">https://pub.towardsai.net/understanding-non-linear-regression-fbef9a396b71?source=collection_archive---------0-----------------------#2019-11-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f2a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当你有一个弯曲的数据集时，知道如何拟合模型…</p><blockquote class="kl km kn"><p id="4440" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated">所有的模型都是错的，但有些是有用的… <a class="ae ks" href="https://en.wikipedia.org/wiki/George_E._P._Box" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">乔治·爱德华·佩勒姆</strong> </a></p></blockquote><p id="dd64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回归的目标是建立一个模型来准确预测未知情况。</p><p id="9568" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回归通常是使用历史数据预测连续变量的过程，如房价、工人工资、降雨强度等。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi kt"><img src="../Images/d75b5ee500376a6dc013eae6d34a99c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkp6TSB2f1qeYLpkeQyNxA.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">了解非线性回归</figcaption></figure><p id="e218" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基本上，回归只有两种类型，参见IBM<strong class="jp ir"><em class="ko">链接</em></strong></p><p id="0b37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">。简单回归</strong></p><p id="621a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">。多元回归</strong></p><p id="3217" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简单回归和多元回归都可以是线性或非线性的。</p><p id="98cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回归的线性是基于自变量和因变量之间关系的性质。</p><p id="78c1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文假设读者对简单回归和多元回归的概念有初步的了解。但不要烦恼，为了复习，请查阅我以前关于简单的<a class="ae ks" href="https://medium.com/towards-artificial-intelligence/understanding-the-simple-maths-behind-simple-linear-regression-3ce4a30e7602?source=---------8------------------" rel="noopener"><strong class="jp ir"><em class="ko"/></strong></a>和<a class="ae ks" href="https://medium.com/towards-artificial-intelligence/understanding-multiple-linear-regression-1b4a5b939f5a?source=---------6------------------" rel="noopener"> <strong class="jp ir"> <em class="ko">多重</em> </strong> </a>线性回归的深入文章。</p><p id="8f8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们都喜欢看到自变量和因变量的散点图，它显示了一条几乎截然不同的直线来拟合我们的模型，但事实是，在现实中，许多数据集显示不同的模式。</p><p id="578b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，如果数据集显示曲线趋势，那么线性回归模型可能确实不合适。在这种情况下，我们需要采用非线性回归模型。我们马上会看到几个例子…</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi lj"><img src="../Images/ca83c2cdecda06ba417fb91f65bff513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LSOOc5YZasa3GKUV053cDw.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><a class="ae ks" href="http://i1.sndcdn.com/avatars-000378018311-j1p564-original.jpg" rel="noopener ugc nofollow" target="_blank"> img_credit </a></figcaption></figure><h2 id="f84d" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">非线性回归(NLR):</h2><p id="9d23" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated"><strong class="jp ir"> NLR </strong>是自变量<strong class="jp ir"> <em class="ko"> X </em> </strong>和<em class="ko"> </em>因变量<em class="ko"> </em> <strong class="jp ir"> <em class="ko"> y </em> </strong>之间的任意关系，产生非线性函数模型化数据。</p><p id="1aea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本质上，任何关系，只要是<strong class="jp ir">而不是</strong>线性的，都可以被称为非线性的，通常用多项式<strong class="jp ir"> <em class="ko"> k </em> </strong> <em class="ko"> </em>(最大幂<strong class="jp ir"> <em class="ko"> X </em> </strong> <em class="ko">)来表示。</em></p><p id="6171" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实上，存在许多不同的NLR，可以用来拟合任何数据集，并且这些可以无限继续下去。</p><p id="9c7b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以统称所有这些<strong class="jp ir"> NLR </strong> s，<strong class="jp ir"> <em class="ko">多项式回归</em> </strong>，只要将自变量<strong class="jp ir"> <em class="ko"> X </em> </strong>与因变量<strong class="jp ir"> <em class="ko"> y </em> </strong>之间的关系建模为X  中的一个<strong class="jp ir"><em class="ko">n次多项式。参见<strong class="jp ir"><em class="ko"/></strong><a class="ae ks" href="https://d3c33hcgiwev3.cloudfront.net/b-TljGpQEei5FgrpHNEYyg.processed/full/540p/index.webm?Expires=1572912000&amp;Signature=ediGaW4W6mC4QHviPdBC3WhDGi-YI6xg0CzE5Ki9fT2dQZPI8C30isc1rQ~fOTG1cuRhXGGMN7UiuDTmpN6gJ~p0M79z8ozr7SBSXo7CGmFAXNdC2KrWznqFkX7Z3gCAB6IHErjMK7J6sySN1knVDDUIAq2adLboO1wqXniYmcQ_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir"><em class="ko">链接自IBM </em> </strong> </a></em></strong></p><h2 id="08ab" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">那么什么是多项式回归或非线性回归呢？</h2><p id="25cf" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">多项式回归将曲线拟合到您的数据。三次多项式的一个简单示例如下所示</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mi"><img src="../Images/7dd8a8853e6be5221acbbc3896d55827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9AiwSIN2CHzrZ-_ZZkJhZg.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">其中b0是截距或偏差单位，b1至b3是变量x的每个独立值的斜率。</figcaption></figure><p id="20d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这看起来很像多元线性回归的特征集，对吗？就像下面这个，是的，确实是。事实上，多项式回归是多元线性回归的一个特例，其主要思想是“你如何选择你的特征？”。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mj"><img src="../Images/a6bedf56d52b2fd25a4f1ed62d20eb13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qH7nOwi6qu48BByzm8t-Q.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">其中b0是截距或偏差单位，b1至b3是每个独立变量x1至x3的斜率</figcaption></figure><h2 id="c9b8" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated"><strong class="ak">常见的非线性回归类型:</strong></h2><p id="fe4b" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">在我们继续之前，让我们简单看一下线性回归。它的方程式是:-</p><p id="e2b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"><em class="ko">y = B0+b1x 1</em>T51】</strong></p><p id="bff3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">线性回归模拟因变量<strong class="jp ir"> <em class="ko"> y </em> </strong>和自变量<strong class="jp ir"> <em class="ko"> x </em> </strong>之间的关系。这种关系的度为1。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mk"><img src="../Images/2582b714c9d0fd5878edda36819134de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CFGqhur7uQBC1wE49MbJMw.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">样本线性回归图</figcaption></figure><p id="e3b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如前所述，有许多类型的非线性回归，但最常见的可能是:-</p><p id="e0e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">。立方</strong></p><p id="3458" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">。二次型</strong></p><p id="896c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">。指数</strong></p><p id="560a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">。对数</strong></p><p id="15c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">。s形/物流</strong></p><p id="2eb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们简单看一下这些…</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/2f6b6c977f086066c58b4fa54d10fd99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*JYLdkrbsYNSu6G8WD99a2Q.jpeg"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><a class="ae ks" href="https://media.istockphoto.com/videos/ready-set-go-video-id463916396?s=640x640" rel="noopener ugc nofollow" target="_blank"> img_credit </a></figcaption></figure><h2 id="4c16" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated"><strong class="ak"> 1。立方:</strong></h2><p id="e3fc" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">三次函数的形式为:- <strong class="jp ir"> <em class="ko"> y_hat </em> </strong>等于<strong class="jp ir"> <em class="ko">截距</em> </strong>加上变量<strong class="jp ir"> <em class="ko"> x </em> </strong>的三次幂加上<strong class="jp ir"> <em class="ko"> x </em> </strong>的二次幂，依此类推。它也可以是从1次方到3次方的反向</p><p id="5b6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个函数的图形在2D平面上不是一条直线。让我们画一个，但首先，看看下面的三次方程。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mm"><img src="../Images/d6b042a1fdc90244e94eb30326b69c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rjk4gnDnSto2dsN7KIc20A.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">y_hat =截距+ x的3次幂+ x的2次幂+ x …</figcaption></figure><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mn"><img src="../Images/773d5a63f17c9b16a107345c133fc566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jU60qt5L5r4_0ebrqncjfg.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">样本三次回归图</figcaption></figure><h2 id="9449" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">2.二次:</h2><p id="c746" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">一个二次函数的等式是:- <strong class="jp ir"> <em class="ko"> y_hat </em> </strong>等于变量<strong class="jp ir"> <em class="ko"> x </em> </strong>乘以变量<strong class="jp ir"> <em class="ko"> x </em> </strong>或2的幂。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mo"><img src="../Images/f3a465edbbb0c55117a83a7823d26839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fG0-pZZ2Ez8SqmtpfK0vgg.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">y_hat = X平方</figcaption></figure><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mp"><img src="../Images/55257faf1b92ca1aeb6091546d26dcfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZEC_Trf2x0pptGZuiDhQuA.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">样本二次回归图</figcaption></figure><h2 id="6585" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">3.指数:</h2><p id="2f64" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">以<strong class="jp ir"> <em class="ko"> c </em> </strong>为底的指数函数被定义为<strong class="jp ir"><em class="ko"/></strong>y-hat等于<strong class="jp ir"> <em class="ko">截距</em> </strong>加上<strong class="jp ir"> <em class="ko">斜率</em> </strong>乘以一个常数(<strong class="jp ir"> <em class="ko"> c </em> </strong>)的变量<strong class="jp ir"> <em class="ko"> X </em> </strong>。请参见下面的表达式。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mq"><img src="../Images/9cdc6e79e9a6fd114a4f820bd95d66ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DQfohxRNN4aiMyQXfP8Neg.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">哪里b！= 0，c &gt; 0！= 1，x是变量和实数，c也是常数。</figcaption></figure><p id="ee56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">指数可能看起来有点令人困惑，但是绘制它非常简单…只需应用<strong class="jp ir"> <em class="ko"> numpy.exp() </em> </strong>函数，并传递变量<strong class="jp ir"> <em class="ko"> X </em> </strong>作为它的参数，格式如下:-<strong class="jp ir"><em class="ko">y _ hat = NP . exp(X)</em></strong><em class="ko">。</em>然后绘制X轴上的变量<strong class="jp ir"> <em class="ko"> X </em> </strong>和y轴上的变量<em class="ko"/><strong class="jp ir"><em class="ko"/></strong><em class="ko"/>y轴<em class="ko">。</em></p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mr"><img src="../Images/4b6b36608863657d9906910a4b590900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UxMjowImtQInDr50McK_FQ.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">样本指数回归图</figcaption></figure><h2 id="dc0f" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">4.对数:</h2><p id="f985" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">在对数函数中，<strong class="jp ir"> <em class="ko"> y_hat </em> </strong>是对变量<strong class="jp ir"> <em class="ko"> X </em> </strong>应用对数映射的结果。这是对数函数最简单的表达式之一。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi ms"><img src="../Images/659508b5e2e04606016c2756d12de763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BZPNNuJKL7Hz4TkhK2OJIQ.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">y _ hat =的对数</figcaption></figure><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mt"><img src="../Images/694b0d7b75d4fe6d611fd0ae7586ec1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*viJA5IeXzDEIqkzet3oCEg.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">对数回归图表示例</figcaption></figure><h2 id="0681" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">5.s形/物流:</h2><p id="d949" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">逻辑回归是线性回归的一种变体，当观察到的因变量<strong class="jp ir"> <em class="ko"> y </em> </strong>是分类变量时，逻辑回归非常有用。它通过线性回归拟合特殊的S形曲线，并使用sigmoid函数将数字估计值转换为概率得分。参见<a class="ae ks" href="https://github.com/Blackman9t/Machine_Learning/blob/master/Classification_Logistic_Reg_churn.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="ko">链接</em> </strong> </a></p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mu"><img src="../Images/37657d1189d3dfc2354ec849e67b9081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fOuRjZjToOM6JAypZXcIPQ.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><strong class="bd lm"> β1 </strong>控制曲线的陡度<strong class="bd lm">，β2 </strong>控制x轴上的曲线。</figcaption></figure><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mv"><img src="../Images/4f2aef7a5b402472642ad33235566871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhMNZKa1rApLMQ5f91GYEQ.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">样本逻辑回归图</figcaption></figure><p id="2a5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有许多回归类型可供选择，很有可能其中一个会很好地适合您的数据集。</p><blockquote class="kl km kn"><p id="f2ad" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated"><strong class="jp ir">请记住，选择最适合数据集的回归模型非常重要。</strong></p></blockquote><p id="df61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我相信你有一些问题，我会慷慨地回答我认为最明显的问题…</p><h2 id="8d43" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">问题:</h2><blockquote class="kl km kn"><p id="0bf5" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated"><strong class="jp ir">如何简单的知道问题是线性还是非线性？</strong></p></blockquote><p id="2999" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要回答上述问题，我们可以做两件事。</p><p id="1f68" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">答</strong></p><p id="a2ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">直观地判断这种关系是线性的还是非线性的。最好用每个输入变量来绘制输出变量的二元图。参见<a class="ae ks" href="https://www.kaggle.com/residentmario/bivariate-plotting-with-pandas" rel="noopener ugc nofollow" target="_blank">上的链接<strong class="jp ir">上的<em class="ko">上的</em>上的</strong>上的</a></p><p id="1975" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> B. </strong></p><p id="344f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个简单的选择是计算自变量和因变量之间的相关系数。在熊猫身上，这很容易通过调用<strong class="jp ir"> <em class="ko">来实现。</em>corr()</strong>作用于数据集。如果所有变量的系数为0.7或更高，则存在线性趋势，因此非线性模型不合适。</p><blockquote class="kl km kn"><p id="3a9f" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated"><strong class="jp ir">好了，说够了！！让我们用一些真实的实时数据来弄脏我们的手……</strong></p></blockquote><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi mw"><img src="../Images/fefe064204821ca3ea683bffeab1bef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pGLGTfonAkCzxVdIVx-tcA.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated"><a class="ae ks" href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;ved=2ahUKEwiwusuZoNPlAhWDDGMBHckECv0QjRx6BAgBEAQ&amp;url=https%3A%2F%2Fwww.azquotes.com%2Fquote%2F282125&amp;psig=AOvVaw1dNwQfz1n24n1BJ9L4xt89&amp;ust=1573049467807411" rel="noopener ugc nofollow" target="_blank"> img_credit </a></figcaption></figure><p id="e880" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将尝试对1960年至2014年中国GDP的数据点拟合一个非线性模型。我们的数据集包含两列，第一列包含从1960年到2014年的年份，第二列包含每年相应的国内生产总值(GDP)值。</p><p id="6248" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个55行2列的小数据集，但已经足够了。</p><p id="3a06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Github   这里可以看到数据集<a class="ae ks" href="https://raw.githubusercontent.com/Blackman9t/Machine_Learning/master/china_gdp.csv" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="ko">的链接。</em></strong></a></p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="9d2b" class="lk ll iq my b gy nc nd l ne nf"># Import libraries</span><span id="2663" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</strong></span><span id="5ef7" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">china_gdp = '  </strong><a class="ae ks" href="https://raw.githubusercontent.com/Blackman9t/Machine_Learning/master/china_gdp.csv" rel="noopener ugc nofollow" target="_blank"><strong class="my ir">https://raw.githubusercontent.com/Blackman9t/Machine_Learning/master/china_gdp.csv</strong></a><strong class="my ir">'</strong></span><span id="caf4" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">df = pd.read_csv(china_gdp)</strong></span><span id="fcef" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">df.head(10)</strong></span></pre><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi nh"><img src="../Images/6e77953a43546e5178e5b7997752985f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KpFI7KRq0vuO_ejtoN3YxA.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">显示前10行…</figcaption></figure><p id="5706" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们需要绘制数据点的二元图。X轴上的自变量<strong class="jp ir"> <em class="ko"> X </em> </strong>(年)，y轴上的因变量<strong class="jp ir"> <em class="ko"> y </em> </strong>(值)。</p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="f599" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">plt.figure(figsize=(8,5))<br/>X_data, y_data = (df['Year'].values, df['Value'].values)<br/>plt.plot(X_data, y_data, 'ro')<br/>plt.suptitle('Graph showing corresponding years and GDP values for China', y=1.02)</strong><br/><strong class="my ir">plt.ylabel('GDP')<br/>plt.xlabel('Year')<br/>plt.show()</strong></span></pre><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi ni"><img src="../Images/17cfbc62c149cc918cdcedff7bae04e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KZ8_io-K1LIBma5NeCIfzg.jpeg"/></div></div></figure><p id="92f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗯……这看起来有点眼熟。你能猜出我们之前研究的哪张NLR图表有与上述数据点相似的曲线吗？</p><p id="06bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你说指数或逻辑…你错了…我在开玩笑！当然，你是对的！</p><p id="1f7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它看起来确实像指数或逻辑GDP增长开始缓慢，然后从2005年开始，增长非常显著，然后在2010年代略有减速。</p><h2 id="6b69" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">选择型号:</h2><p id="130a" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">逻辑函数可以是一个很好的近似，因为它具有开始缓慢、中间增加增长、然后最后再次减少的特性。</p><h2 id="621b" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">构建模型:</h2><p id="b764" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">根据上面定义的sigmoid方程，记住Beta_1控制曲线的陡度，而Beta_2在x轴上滑动曲线。</p><p id="c3ae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们建立我们的回归模型并初始化它的参数。</p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="9254" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">def sigmoid(X, Beta_1, Beta_2):</strong><br/>    """ <em class="ko">This method performs the sigmoid function on param X and <br/>    Returns the outcome as a varible called y</em>"""<br/>    <strong class="my ir">y = 1 / (1 + np.exp(-Beta_1*(X-Beta_2)))<br/>    return y</strong></span></pre><p id="ea8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们用一些样本值来测试我们的sigmoid函数</p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="409a" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">beta_1 = 0.10<br/>beta_2 = 1990.0</strong></span><span id="6ee6" class="lk ll iq my b gy ng nd l ne nf"># logistic_function<br/><strong class="my ir">y_pred = sigmoid(X_data, beta_1, beta_2)</strong></span><span id="3171" class="lk ll iq my b gy ng nd l ne nf"># Plot initial predictions against data points.</span><span id="2147" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">plt.figure(figsize=(8,5))<br/>plt.suptitle('Sample Plot: Sigmoid Function on data points')<br/>plt.plot(X_data, y_pred*15000000000000.0)<br/>plt.plot(X_data, y_data, 'ro')<br/>plt.show()</strong></span></pre><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi nj"><img src="../Images/4670f7a6b2d64f2b8f1ce6b334711506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yF0b_SPOp4dk2BxfKXuZJQ.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">蓝线是我们的样本sigmoid模型，红点是数据点。</figcaption></figure><h2 id="4dfc" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">标准化我们的变量:</h2><p id="e888" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">在这一点上，让我们规范化我们的变量</p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="aac3" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">xdata = X_data / max(X_data)<br/>ydata = y_data / max(y_data)</strong></span></pre><h2 id="7b2d" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">寻找最佳参数:</h2><p id="4db5" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">我们的下一个任务是找到非线性或逻辑模型的最佳参数。我们将使用来自<strong class="jp ir"> <em class="ko"> scipy </em> </strong>库中的<strong class="jp ir"> <em class="ko"> curve_fit() </em> </strong>方法。该方法所做的是:-它使用非线性<a class="ae ks" href="https://stattrek.com/regression/linear-regression.aspx" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="ko">最小二乘估计</em> </strong> </a>来将我们上面定义的sigmoid函数拟合到数据点。</p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="bceb" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">from scipy.optimize import curve_fit</strong></span><span id="791f" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">popt, pcov = curve_fit(sigmoid, xdata, ydata)</strong><br/># popt are our new optimized parameters<br/># pcov represents the covariance<br/><strong class="my ir">print('beta_1 = %f, beta_2 = %f' % (popt[0],popt[1]))</strong></span><span id="ecc1" class="lk ll iq my b gy ng nd l ne nf">&gt;&gt;<br/>  <strong class="my ir">beta_1 = 690.453017, beta_2 = 0.997207</strong></span></pre><p id="75e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于curve_fit()方法，现在我们有了理想的参数，我们将使用它们来拟合我们的模型，换句话说，就是最小化每个预测与其对应的实际值之间的平方差之和。</p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="a38d" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">x = np.linspace(1960, 2015, 55)</strong><br/># Normalize x<br/><strong class="my ir">x = x / max(x)<br/>plt.figure(figsize=(8,5))<br/>y = sigmoid(x, popt[0], popt[1])</strong><br/># Plotting the original data points<br/><strong class="my ir">plt.plot(xdata, ydata, 'ro', label='data')</strong><br/># Plotting the fitted prediction line<br/><strong class="my ir">plt.plot(x, y, linewidth=3.0, label='fit')<br/>plt.legend(loc='best')<br/>plt.ylabel('GDP', color='r', fontsize=18)<br/>plt.xlabel('Year', color='r', fontsize=18)<br/>plt.xticks(color = 'y')<br/>plt.yticks(color = 'y')<br/>plt.show()</strong></span></pre><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi nk"><img src="../Images/557ca97aa844c015f2917f41335cf87c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9mjqOpV24TDTzJqh8bJ7Q.jpeg"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">曲线数据点上拟合的模型蓝线…</figcaption></figure><p id="6d70" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所看到的，它看起来非常适合，但是让我们来评估一下我们的模型…</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi nl"><img src="../Images/a9418a2ff023a09f2b5f06b474c078f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G4toi8TRuTpSARdLhL1o6w.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk translated">模型评估…</figcaption></figure><p id="320d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，让我们将数据分成训练和测试数据集。</p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="a36d" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">msk = np.random.rand(len(df)) &lt; 0.8 <br/>train_x = xdata[msk] <br/>test_x = xdata[~msk] <br/>train_y = ydata[msk]<br/>test_y = ydata[~msk]</strong></span></pre><p id="26d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们使用训练集来构建模型，以提取理想参数</p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="2d9b" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">popt, pcov = curve_fit(sigmoid, train_x, train_y)</strong><br/># Remember popt saves the ideal parameters from curve_fit method<br/># While pcov stores the covariance</span><span id="8c7d" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">print('Ideal params are: ', popt)</strong><br/>&gt;&gt;<br/><strong class="my ir">Ideal params are:  [670.91888462   0.99708276]</strong></span></pre><p id="74ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们使用测试集进行预测</p><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="f528" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">y_hat = sigmoid(test_x, *popt)</strong></span><span id="4499" class="lk ll iq my b gy ng nd l ne nf"># *popt means unpack popt into popt[0] and popt[1]</span></pre><h2 id="a249" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">评估…</h2><pre class="ku kv kw kx gt mx my mz na aw nb bi"><span id="dd5b" class="lk ll iq my b gy nc nd l ne nf"><strong class="my ir">mean_abs_error = np.mean(np.absolute(y_hat - test_y))<br/>mean_squ_error = np.mean(np.absolute((y_hat - test_y) **2))</strong></span><span id="5f55" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">print("Mean absolute error: %.2f" % mean_abs_error)<br/>print("Residual sum of squares (MSE): %.2f" % mean_squ_error)</strong></span><span id="5bd2" class="lk ll iq my b gy ng nd l ne nf"># Next let's check the R2 score, The coefficient of determination</span><span id="d253" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">from sklearn.metrics import r2_score</strong></span><span id="6b9f" class="lk ll iq my b gy ng nd l ne nf"><strong class="my ir">r_score = r2_score(y_hat, test_y)<br/>print("R2-score: %.2f" % r_score)</strong></span><span id="1cbc" class="lk ll iq my b gy ng nd l ne nf">&gt;&gt;<br/><strong class="my ir">Mean absolute error: 0.04 <br/>Residual sum of squares (MSE): 0.00 <br/>R2-score: 0.95</strong></span></pre><p id="7793" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">MAE = 0.4MSE = 0.0。；R2得分= 0.95 (95%)</p><h2 id="7cf7" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">总结:</h2><p id="be09" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">这需要大量的实践，但是很明显，正如我们在这个小数据集上看到的，通过曲线数据集拟合非线性回归线实际上是可能的。Python有丰富的模块来帮助我们拟合一个模型来预测一个连续的甚至是分类的变量。</p><p id="996d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请随意浏览Github上的笔记本<a class="ae ks" href="https://github.com/Blackman9t/Machine_Learning/blob/master/Non_LinearRegression.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir"><em class="ko"/></strong></a>了解更多细节，尤其是我们之前绘制的NLR图表。</p><p id="b92b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">干杯！</strong></p><h2 id="4828" class="lk ll iq bd lm ln lo dn lp lq lr dp ls jy lt lu lv kc lw lx ly kg lz ma mb mc bi translated">关于我:</h2><p id="2a6d" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">劳伦斯是技术层的数据专家，对公平和可解释的人工智能和数据科学充满热情。我持有IBM的 <strong class="jp ir"> <em class="ko">数据科学专业</em> </strong> <em class="ko">和</em> <strong class="jp ir"> <em class="ko">高级数据科学专业</em> </strong> <em class="ko">证书。我已经使用ML和DL库进行了几个项目，我喜欢尽可能多地编写函数代码，即使现有的库比比皆是。最后，我从未停止学习和实验，是的，我拥有几个数据科学和人工智能认证，并且我已经写了几篇强烈推荐的文章。</em></p><p id="2aed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请随时在以下网址找到我</p><p id="31dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ks" href="https://github.com/Lawrence-Krukrubo" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> Github </strong> </a></p><p id="f239" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ks" href="https://www.linkedin.com/in/lawrencekrukrubo/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">领英</strong> </a></p><p id="4257" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ks" href="https://twitter.com/LKrukrubo" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">推特</strong> </a></p></div></div>    
</body>
</html>