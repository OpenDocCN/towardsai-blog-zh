<html>
<head>
<title>Meta-Learning in NLP Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理分类中的元学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/meta-learning-in-nlp-classification-db78fbcdf15c?source=collection_archive---------1-----------------------#2020-04-19">https://pub.towardsai.net/meta-learning-in-nlp-classification-db78fbcdf15c?source=collection_archive---------1-----------------------#2020-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ee7a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学会学习</h2></div><blockquote class="ki kj kk"><p id="2e9e" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">与众所周知的数据集不同，我们现实生活中的问题域总是只有很小的带标签的数据集，而我们可能无法在这种情况下训练出一个好的模型。<a class="ae li" href="https://towardsdatascience.com/data-augmentation-library-for-text-9661736b13ff" rel="noopener" target="_blank">数据扩充</a>是生成句法数据的方法之一，而元学习是解决这个问题的另一种方法。</p><p id="67d0" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">在这一系列的故事中，我们将经历不同的元学习方法。这项任务的动机之一是，即使是孩子也可以通过举一个例子来识别一个物体。模型不学习对特定类别进行分类，而是学习区分输入的模式。这一系列的元学习将涵盖NLP中的零镜头学习、一镜头学习、少镜头学习、元学习。</p></blockquote><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/699cd40391db9420b8fac42e0d05a432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WRGXaJ3rZ9H91i06"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">照片由<a class="ae li" href="https://unsplash.com/@kylejglenn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯尔·格伦</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="80e6" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lz kw kx ky ma la lb lc mb le lf lg lh im bi translated">在几个关于元学习的故事之后，我将谈论我们如何在NLP的分类问题中使用元学习，因为分类是NLP中最常见的问题之一。</p><h1 id="e910" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">意图嵌入的零距离学习</h1><p id="998a" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku lz mw kx ky ma mx lb lc mb my lf lg lh im bi translated">Chen等人将零触发学习应用于将文本输入和标签转移到嵌入中，使得它可以处理看不见的标签。</p><p id="7ad7" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lz kw kx ky ma la lb lc mb le lf lg lh im bi translated">开发虚拟代理(例如苹果Siri、谷歌助手、亚马逊Alexa)的挑战之一是无限的意图。处理不可见标签的经典方法是为不可见标签准备大量训练数据，以便模型可以对其进行分类。但是，新的训练和标记这些数据可能需要更长的时间。因此，陈等人提出使用零短学习方法来动态地学习话语嵌入和意图嵌入。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mz"><img src="../Images/81ed15375071901f0c697cefaf5d44cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_goRixK0dFYLJmyM2aFsw.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">零射击学习的意图嵌入架构(陈等，2016)</figcaption></figure><h2 id="b9d8" class="na md it bd me nb nc dn mi nd ne dp mm lz nf ng mo ma nh ni mq mb nj nk ms nl bi translated"><code class="fe nm nn no np b">Subword</code></h2><p id="5616" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku lz mw kx ky ma mx lb lc mb my lf lg lh im bi translated">为了解决词汇外(OOV)问题，引入了子词来处理所有的文本输入。所有的单词将分裂成三元组，这样我们可以限制嵌入的大小。例如，输入单词“email”将被拆分为“#em”、“ema”、“mai”、“ail”和“il#”，而#表示单词的开始和结束。</p><h2 id="8ed7" class="na md it bd me nb nc dn mi nd ne dp mm lz nf ng mo ma nh ni mq mb nj nk ms nl bi translated">网络神经</h2><p id="fb6b" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku lz mw kx ky ma mx lb lc mb my lf lg lh im bi translated">在对单词序列进行标记后，它将进入嵌入层、CNN、最大池层和最终生成语义层。最后，使用余弦相似度来比较输入(即话语)和标签(例如，看到的标签或看不到的标签)。</p><h1 id="8462" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">跨不同自然语言分类任务少量学习</h1><p id="ab2a" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku lz mw kx ky ma mx lb lc mb my lf lg lh im bi translated">Bansal等人将元学习应用于多自然语言处理(NLP)分类任务。这种方法的目标不是解决一个分类任务，而是解决多个分类任务，例如关系分类和自然语言推理。这种模型被命名为LEOPARD(<strong class="ko iu">L</strong>earning to g<strong class="ko iu">e</strong>generate s<strong class="ko iu">o</strong>ft max<strong class="ko iu">pa</strong>parameters for<strong class="ko iu">r</strong>T10】diverse class ),结合了迁移学习和元学习。</p><h2 id="0084" class="na md it bd me nb nc dn mi nd ne dp mm lz nf ng mo ma nh ni mq mb nj nk ms nl bi translated">文本编码器</h2><p id="10f3" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku lz mw kx ky ma mx lb lc mb my lf lg lh im bi translated">利用最先进的迁移学习技能，text encoder利用<a class="ae li" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank"> BERT </a>将输入文本数据转换为嵌入数据。</p><h2 id="55ff" class="na md it bd me nb nc dn mi nd ne dp mm lz nf ng mo ma nh ni mq mb nj nk ms nl bi translated">Softmax参数</h2><p id="4aff" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku lz mw kx ky ma mx lb lc mb my lf lg lh im bi translated">支持多个分类任务的一个挑战是许多标签在任务之间是不同的。因此，引入了用于特定任务分类的softmax参数来解决这个问题。</p><p id="4ac5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lz kw kx ky ma la lb lc mb le lf lg lh im bi translated">为了初始化softmax参数，LEOPARD挑选第一个小批量数据来形成标签。换句话说，它使用BERT (Devlin et al .，2017)，多层感知器(MLP)和tanh将文本转换为嵌入来表示类。</p><h2 id="931b" class="na md it bd me nb nc dn mi nd ne dp mm lz nf ng mo ma nh ni mq mb nj nk ms nl bi translated">元学习</h2><p id="4a02" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku lz mw kx ky ma mx lb lc mb my lf lg lh im bi translated">MAML (Finn et al .，2017)是一种著名的元学习方法。它包括训练模型的内环和外环。对于内环，在相同的训练集上需要多个训练步骤来获得更好的训练损失。要了解更多的MAML，你可以访问这个网页获取更多信息。</p><p id="4e41" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku lz kw kx ky ma la lb lc mb le lf lg lh im bi translated">由于BERT中有大量的参数，Bansal等人将这些参数分成特定于任务和与任务无关的参数。BERT的前n层被认为是任务不可知的参数，并且跨任务生成良好的特征表示。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/faf691f957a403ef91d19d30619c00eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*j0oxMsK-i8mX0A36kl3vOA.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk translated">LEOPARD模型的架构(Bansal等人，2019年)</figcaption></figure><h1 id="35f0" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">拿走</h1><h1 id="9166" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">延伸阅读</h1><ul class=""><li id="fc40" class="nr ns it ko b kp mu ks mv lz nt ma nu mb nv lh nw nx ny nz bi translated">了解<a class="ae li" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">伯特</a></li><li id="8c21" class="nr ns it ko b kp oa ks ob lz oc ma od mb oe lh nw nx ny nz bi translated">了解<a class="ae li" href="https://medium.com/towards-artificial-intelligence/meta-learning-in-dialog-generation-41367e397086" rel="noopener"> MAML </a></li><li id="51b1" class="nr ns it ko b kp oa ks ob lz oc ma od mb oe lh nw nx ny nz bi translated">理解<a class="ae li" href="https://medium.com/towards-artificial-intelligence/a-gentle-introduction-to-meta-learning-8e36f3d93f61" rel="noopener">原型网络</a></li></ul><h1 id="e4e7" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">关于我</h1><p id="a05e" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku lz mw kx ky ma mx lb lc mb my lf lg lh im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。你可以通过<a class="ae li" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae li" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae li" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="688f" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><ul class=""><li id="bc16" class="nr ns it ko b kp mu ks mv lz nt ma nu mb nv lh nw nx ny nz bi translated">Y.陈，杜德辉，和何。<a class="ae li" href="https://www.csie.ntu.edu.tw/~yvchen/doc/ICASSP16_ZeroShot.pdf" rel="noopener ugc nofollow" target="_blank">通过卷积深度结构化语义模型进行扩展的意图嵌入的零次学习</a>。2016</li><li id="b26f" class="nr ns it ko b kp oa ks ob lz oc ma od mb oe lh nw nx ny nz bi translated">T.班萨尔、r .贾和a .麦卡勒姆。<a class="ae li" href="https://arxiv.org/pdf/1911.03863.pdf" rel="noopener ugc nofollow" target="_blank">学习跨不同自然语言分类任务的少量学习</a>。2019</li></ul></div></div>    
</body>
</html>