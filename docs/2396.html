<html>
<head>
<title>Image Editing from sketches: EditGAN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从草图编辑图像:EditGAN</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/image-editing-from-sketches-editgan-4cacca609e2d?source=collection_archive---------0-----------------------#2021-12-05">https://pub.towardsai.net/image-editing-from-sketches-editgan-4cacca609e2d?source=collection_archive---------0-----------------------#2021-12-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7ec4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="a8cd" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">控制快速草稿中的任何功能，它将只编辑你想要的，保持图像的其余部分不变！SOTA图像编辑草图模型由英伟达，麻省理工学院，UofT。</h2></div><blockquote class="kr ks kt"><p id="c4b4" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/editgan/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/editgan/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><h2 id="b6af" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">看视频！</h2><figure class="mn mo mp mq gt mr"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="1dfb" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">你曾经梦想过能够用快速的草图或建议来编辑图片的任何部分吗？改变特定的特征，比如眼睛、眉毛，甚至是你汽车的轮子？嗯，这不仅是可能的，但它从来没有比现在更容易与这种新模式称为EditGAN，结果真的令人印象深刻！你基本上可以非常快速地改进或模仿任何图片。事实上，你可以从快速草稿中控制你想要的任何功能，它只会编辑修改，保持图像的其余部分不变。像GANs这样的图像合成和图像编辑人工智能模型一直在寻求控制，并且获得控制极具挑战性。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c0ef18cd46bc48f1a14940bed418b448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*3ap4LbF74PT6UpqJJVNuhA.gif"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">EditGAN的结果示例。图片由<a class="ae lr" href="https://nv-tlabs.github.io/editGAN/" rel="noopener ugc nofollow" target="_blank">凌等人提供，2021，EditGAN </a>。</figcaption></figure><p id="3404" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">正如我们所说的，NVIDIA、多伦多大学和麻省理工学院的这篇出色的新论文允许您编辑任何图片，并对草图输入的特定功能进行卓越的控制。通常，控制特定要素需要庞大的数据集和专家来了解要在模型中更改哪些要素，以便在只更改所需要素的情况下获得所需的输出图像。相反，EditGAN只通过少数几个带标签的示例来学习如何将分割与图像匹配，从而允许您使用分割，或者换句话说，使用快速草图来编辑图像。它保留了完整的图像质量，同时提供了前所未有的细节和自由度。这是一个巨大的进步，但更酷的是他们是如何实现的，所以让我们更深入地了解他们的方法！</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi nb"><img src="../Images/5ee23e095211c18c37202d5f7c8bb59e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yte4qK4EyDMJGPc4.png"/></div></a></figure><p id="6225" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">首先，该模型使用StyleGAN2来生成图像，这是发表时可用的最佳图像生成模型，在研究中被广泛使用。如果你想了解更多，我不会深入这个模型的细节，因为我已经在关于不同应用的许多文章中讨论过了。相反，我将假设您对StyleGAN2的功能有一个基本的了解:获取一个图像，将其编码到一个压缩的子空间中，并使用一种称为生成器的模型将这个编码的子空间转换成另一个图像。这也可以使用直接编码的信息而不是编码图像来获得该信息。这里重要的是发电机。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/5eac79b689bd3fc1c827fcf5ae6c0c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*cis786j4TaEi0d9omHznsw.gif"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">GAN网络的发生器如何工作。</figcaption></figure><p id="7be3" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">正如我所说的，它将从一个子空间获取信息，这个子空间通常被称为潜在空间，在这个空间中，我们有很多关于我们图像及其特征的信息，但这个空间是多维的，我们很难将它可视化。挑战在于识别该子空间的哪个部分负责重建图像中的哪个特征。这就是EditGAN发挥作用的地方，它不仅告诉您子空间的哪个部分做什么，还允许您使用另一个输入来自动编辑它们:一个您可以轻松绘制的草图。事实上，它会对你的图像进行编码，或者简单地采用特定的潜在代码，生成图片的分割图和图片本身。这意味着分割和图像都在相同的子空间中，通过训练一个模型来做到这一点，并且它允许只控制期望的特征，而不需要你做任何其他事情，因为你只需要改变分割图像，其他的将跟随。训练将只针对这种新的分割生成，而StyleGAN生成器将对原始图像保持固定。这将允许模型理解并将分割链接到生成器重建图像所需的相同子空间。然后，如果训练正确，您可以简单地编辑这个分割，它会相应地改变图像！</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/fc387bd2a958462c2ee51568a109e10c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vao24IjhKn6JsoQkd1akwg.png"/></div></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">编辑概述(1)和流程(2，3，4)。图片由<a class="ae lr" href="https://nv-tlabs.github.io/editGAN/" rel="noopener ugc nofollow" target="_blank">凌等人提供，2021，编辑</a>。</figcaption></figure><p id="a922" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">EditGAN基本上会把你图像的每个像素分配到一个特定的类，比如头、耳朵、眼睛等。，并使用覆盖潜在空间内其他类别的像素的掩模来独立控制这些类别。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/af8219d0136e61a8fa88b3ea39267727.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*LxsUowNAzB4eGgbkwjxoyg.jpeg"/></div><figcaption class="mx my gj gh gi mz na bd b be z dk translated">鸟类分割图。图片由<a class="ae lr" href="https://nv-tlabs.github.io/editGAN/" rel="noopener ugc nofollow" target="_blank">凌等人提供，2021，编辑</a>。</figcaption></figure><p id="d765" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">因此，每个像素都有自己的标签，EditGAN将决定编辑哪个标签，而不是直接在潜在空间中编辑哪个像素，并重建图像，只修改编辑区域。瞧！通过将生成的图像与分割图连接，EditGAN允许您随意编辑该图，并将这些修改应用到图像，从而创建一个新版本！</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><a href="https://www.louisbouchard.ai/learnai/"><div class="gh gi ni"><img src="../Images/656a7368cd323e67277b001a82839ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*NT2-SGQjCX3M3Wsu.png"/></div></a></figure><p id="955d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">当然，在用这些例子训练之后，它对看不见的图像起作用。像所有的GANs一样，结果受限于它被训练的图像类型，所以如果你用汽车训练它，你不能把它用在猫的图像上。尽管如此，这仍然令人印象深刻，我喜欢研究人员试图提供直观地玩GANs的方法，比如使用草图而不是参数。代码目前还不可用，但很快就会可用，我很高兴能尝试一下！这只是对这个令人惊叹的新模型的概述，我强烈邀请您阅读他们的论文以获得更深入的技术理解。<br/>让我知道你的想法，我希望你喜欢这篇文章，就像我喜欢了解这种新模式一样！观看视频了解更多结果！</p><p id="3421" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">再次感谢<a class="ae lr" href="https://wandb.ai/site" rel="noopener ugc nofollow" target="_blank">重量&amp;偏见</a>赞助视频和文章，感谢你还在阅读！下周见，一个非常特别和令人兴奋的关于我喜欢的主题的视频！</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="fb18" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mb lf lg lh mf lj lk ll mj ln lo lp lq im bi translated">如果你喜欢我的工作，并想了解人工智能的最新动态，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)，并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">简讯</strong> </a>！</p><h2 id="1cf1" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">支持我:</h2><ul class=""><li id="2375" class="nq nr it kx b ky ns lb nt mb nu mf nv mj nw lq nx ny nz oa bi translated">支持我的最好方式是成为这个网站<strong class="kx jd"> </strong>的成员，或者如果你喜欢视频格式，在<strong class="kx jd"> YouTube </strong>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="2d54" class="nq nr it kx b ky ob lb oc mb od mf oe mj of lq nx ny nz oa bi translated">跟我来这里上<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a></li><li id="f5f8" class="nq nr it kx b ky ob lb oc mb od mf oe mj of lq nx ny nz oa bi translated">想进入AI或者提升技能，<a class="ae lr" href="https://www.louisbouchard.ai/learnai/" rel="noopener ugc nofollow" target="_blank">看这个</a>！</li></ul><h2 id="76ca" class="ls lt it bd lu lv lw dn lx ly lz dp ma mb mc md me mf mg mh mi mj mk ml mm iz bi translated">参考</h2><ul class=""><li id="d074" class="nq nr it kx b ky ns lb nt mb nu mf nv mj nw lq nx ny nz oa bi translated">Ling，h .，Kreis，k .，Li，d .，Kim，S.W .，Torralba，a .和Fidler，s .，2021年5月。EditGAN:高精度语义图像编辑。在第三十五届神经信息处理系统会议上。</li><li id="be1a" class="nq nr it kx b ky ob lb oc mb od mf oe mj of lq nx ny nz oa bi translated">代码和交互工具(即将推出):<a class="ae lr" href="https://nv-tlabs.github.io/editGAN/" rel="noopener ugc nofollow" target="_blank">https://nv-tlabs.github.io/editGAN/</a></li></ul></div></div>    
</body>
</html>