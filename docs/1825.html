<html>
<head>
<title>Comprehensive Guide to Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器综合指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/comprehensive-guide-to-transformers-6c73a9e6e2df?source=collection_archive---------1-----------------------#2021-05-07">https://pub.towardsai.net/comprehensive-guide-to-transformers-6c73a9e6e2df?source=collection_archive---------1-----------------------#2021-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e006" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="9d43" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">关注是你所需要的，甚至更多</h2></div><p id="86a1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你有一张写有文字的纸，你想建立一个模型，可以把这些文字翻译成另一种语言。你如何处理这个问题？</p><p id="9135" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第一个问题是文本的可变大小。没有线性代数模型可以处理不同维数的向量。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/fbb826fc26f84001452edb237c56cbf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1r0pTRDm2wopT08DBo-CQ.jpeg"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">由<a class="ae md" href="https://unsplash.com/@elenatrn?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">埃琳娜·塔拉年科</a>在<a class="ae md" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="00c4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">处理这类问题的默认方式是使用单词袋模型(<a class="ae md" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> 1 </a>)。在这个模型中，数据将是一个巨大数量的向量，与一种语言的字数一样大，并且大多数向量元素将是零，因为大多数术语在本文中没有使用。为了最小化计算向量的大小，我们只存储呈现单词的位置。</p><p id="da54" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，单词袋模型忽略了单词的顺序，而这是至关重要的。例如:“<strong class="kt jd">为生活而工作</strong>”不同于“<strong class="kt jd">为工作而生活</strong>”为了保持数据的顺序，我们将增加图的维度(<strong class="kt jd"> n-gram </strong>)以将顺序添加到我们的等式中。</p><p id="225c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在n-gram模型中，一个单词的概率取决于(<strong class="kt jd"> n-1 </strong>)条之前的评论，这意味着模型不会与早于(<strong class="kt jd"> n-1 </strong>)条的单词进行关联。为了克服这一点，我们将不得不增加n，这将成倍地增加计算复杂度(<a class="ae md" href="http://d2l.ai/chapter_recurrent-neural-networks/rnn.html" rel="noopener ugc nofollow" target="_blank"> 2 </a>)。</p><p id="884b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以，这是我们目前的问题:</p><ol class=""><li id="d613" class="me mf it kt b ku kv kx ky la mg le mh li mi lm mj mk ml mm bi translated">文本的可变长度。</li><li id="339d" class="me mf it kt b ku mn kx mo la mp le mq li mr lm mj mk ml mm bi translated">应用词袋模型后的海量数据。</li><li id="1bf8" class="me mf it kt b ku mn kx mo la mp le mq li mr lm mj mk ml mm bi translated">当我们增加维度时，计算成本增加。</li></ol><p id="10d2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">似乎我们需要一个新的模型，它不依赖于单词袋。这就是RNN模式发挥作用的地方。</p><h1 id="3092" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">递归神经网络(RNN)</h1><p id="ff53" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">RNN与n-gram模型相同，只是当前输入的输出将取决于之前所有计算的输出。RNN有其内部状态，作为一种记忆。它非常适合自然语言处理和语音识别。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi np"><img src="../Images/62eb51017df277d865471571dc02de12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*OwsuaMEPvLHQIrOcdD3CNw.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">人物级语言模型基于RNN [ </em> <a class="ae md" href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html" rel="noopener ugc nofollow" target="_blank"> <em class="nq">来源</em></a><em class="nq"/></figcaption></figure><p id="536c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该图显示了在某个时间(t+6)的输入取决于每个先前步骤的隐藏状态和当前输入。它允许网络保存以前学习的参数的历史，并使用它来预测下面的输出，这克服了单词顺序的问题，并消除了计算成本，因为我们将在我们的模型中单独传递单词。</p><p id="52fb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这个模型看起来很完美，但是在实践中，它有一些问题:</p><ol class=""><li id="67af" class="me mf it kt b ku kv kx ky la mg le mh li mi lm mj mk ml mm bi translated">消失或爆炸梯度问题。</li><li id="3316" class="me mf it kt b ku mn kx mo la mp le mq li mr lm mj mk ml mm bi translated">我们不能并行计算，因为输出取决于之前的计算。</li></ol><p id="346a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">好吧，RNN模型并不完美。因此，它被进一步修改以克服这些缺陷。</p><p id="b6af" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">了解更多:<a class="ae md" href="https://neptune.ai/blog/recurrent-neural-network-guide" rel="noopener ugc nofollow" target="_blank">递归神经网络指南——深入RNN </a></p><h1 id="0760" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">长短期记忆(LSTM)</h1><p id="de32" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">这种特殊的RNN增加了一种遗忘机制，因为LSTM单位被分成细胞。每个单元格有三个输入:</p><ul class=""><li id="56fe" class="me mf it kt b ku kv kx ky la mg le mh li mi lm nr mk ml mm bi translated">电流输入，</li><li id="4f21" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated">隐藏状态，</li><li id="1222" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated">上一步的记忆状态(<a class="ae md" href="http://d2l.ai/chapter_recurrent-modern/lstm.html#gated-memory-cell" rel="noopener ugc nofollow" target="_blank"> 6 </a>)。</li></ul><p id="53a5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些输入通过门:</p><ul class=""><li id="5070" class="me mf it kt b ku kv kx ky la mg le mh li mi lm nr mk ml mm bi translated">输入门，</li><li id="4564" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated">忘了盖特吧，</li><li id="7e47" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated">输出门。</li></ul><p id="b24e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">门调节进出细胞的数据。</p><p id="ed17" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">遗忘门决定何时记忆，何时跳过先前隐藏状态的输入。这种设计主要是为了克服消失和爆炸梯度的问题。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/cce5171715b0b71739936c0cdde8398d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*4P78VHAOzXbPVlaqnXrxXg.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">计算LSTM的隐藏状态</em> <a class="ae md" href="https://d2l.ai/chapter_recurrent-modern/lstm.html" rel="noopener ugc nofollow" target="_blank"> <em class="nq">来源</em></a><em class="nq"/></figcaption></figure><p id="a1af" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">LSTM能够克服RNN模型中的消失和爆炸梯度，但是仍然存在从RNN模型继承的问题，例如:</p><ol class=""><li id="5288" class="me mf it kt b ku kv kx ky la mg le mh li mi lm mj mk ml mm bi translated">没有并行化，我们仍然有数据的顺序路径，甚至比以前更复杂。</li><li id="53e8" class="me mf it kt b ku mn kx mo la mp le mq li mr lm mj mk ml mm bi translated">硬件资源还是个问题。</li></ol><p id="6f55" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些解决方案不足以克服内存和并行性问题。所以，是时候引入另一种模式了。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/cce5171715b0b71739936c0cdde8398d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*4P78VHAOzXbPVlaqnXrxXg.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated">LSTM模块[ <a class="ae md" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="07fa" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">变形金刚(电影名)</h1><p id="cb03" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">在前面的章节中，我们介绍了我们面临的问题，以及一些解决部分问题的建议方案。但是仍然有研究的空间。</p><p id="93df" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们提到了序列到序列翻译的变长问题，这个问题还没有解决。</p><p id="f632" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了解决这个问题，2017年推出了一种依赖于注意力机制的模型。我们不是单独处理标记，而是将文本分成段，并学习它们之间的依赖关系。</p><p id="c72d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这个模型是基于另一个架构设计的，由两个主要组件组成。</p><p id="ce41" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">输入首先通过编码器。</p><p id="4960" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该编码器将接受可变长度的输入，并将其转换为固定长度的隐藏状态。</p><p id="2ea9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，隐藏状态将通过第二个组件，这是一个解码器，将固定长度的状态转换为可变长度的输出。</p><p id="0350" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种架构被称为<strong class="kt jd"> <em class="nt">编解码架构</em> </strong> ( <a class="ae md" href="http://d2l.ai/chapter_recurrent-modern/encoder-decoder.html#sec-encoder-decoder" rel="noopener ugc nofollow" target="_blank"> 4 </a>)。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7ae4f0b52162b746a7524689b9efc8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*9-WJvQitcn3pigHLbKZ-jg.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">编解码器架构</em> <a class="ae md" href="https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html" rel="noopener ugc nofollow" target="_blank"> <em class="nq">来源</em></a><em class="nq"/></figcaption></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/6f91a5063ee3081bc2233b281c2f24e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*GlL8Mx7w7B8UOMZaYfFqkQ.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">顺序到顺序学习</em> <a class="ae md" href="https://d2l.ai/chapter_recurrent-modern/seq2seq.html" rel="noopener ugc nofollow" target="_blank"> <em class="nq">来源</em></a><em class="nq"/></figcaption></figure><p id="bc03" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在Transformers中，输入标记被转换为向量，然后我们添加一些位置信息(位置编码),以便在模型的并发处理过程中考虑标记的顺序。</p><p id="f978" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">变形金刚修改了这个模型，使其能够抵抗我们之前讨论的问题，对编码器和解码器都使用了堆叠自关注和完全连接的层。</p><p id="fc99" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">另请阅读:<a class="ae md" href="https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape" rel="noopener ugc nofollow" target="_blank">关于BERT和正在重塑人工智能格局的Transformer架构，你需要知道的10件事</a></p><p id="9c9f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">编码器:</strong>由一堆多个相同的层组成，每层包含两个子层，多头自关注机制后跟残差连接，简单的全连接前馈网络。</p><p id="6db9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">解码器:</strong>由一堆多层组成，每层三个子层；前两层与编码器层相同，第三层是对编码器堆栈输出的多头关注。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e2f35b5e91be322a8b919b73a664a374.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*WyFIVo74mLGCwES8zwi6ww.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">变压器架构</em> <a class="ae md" href="https://d2l.ai/chapter_attention-mechanisms/transformer.html" rel="noopener ugc nofollow" target="_blank"> <em class="nq">来源</em></a><em class="nq"/></figcaption></figure><h1 id="0462" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">注意机制</h1><p id="876c" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">这个模型的灵感来自于人类的视觉系统(<a class="ae md" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> 7 </a>)。当大脑从眼睛接收大量信息输入，超过大脑一次处理的能力时，眼睛感觉系统中的注意力线索使人类能够关注眼睛接收的一小部分。</p><p id="75e2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以将这种方法应用于手头的问题。如果我们知道了能够影响我们翻译的部分，我们就可以专注于那些部分，忽略其他无用的信息。</p><p id="4b77" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这将影响系统的性能。当你在读这篇文章的时候，你在关注这篇文章，而忽略了这个世界的其他部分。这伴随着一种可以被称为机会成本的成本。</p><p id="8497" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以从不同类型的注意力机制中进行选择，比如注意力集中和全连接层。</p><p id="3a16" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在注意力集中中，对注意力系统的输入可以分为三种类型:</p><ul class=""><li id="a5b5" class="me mf it kt b ku kv kx ky la mg le mh li mi lm nr mk ml mm bi translated"><strong class="kt jd">键</strong>(非暴力提示)，</li><li id="5754" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><strong class="kt jd">查询</strong>(意志提示)，</li><li id="7ac5" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><strong class="kt jd">值</strong>(感官输入)。</li></ul><p id="71f8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以将关键字和查询之间的注意力权重可视化。值和密钥是编码器的隐藏状态，查询是前一个解码器的输出。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nx"><img src="../Images/f68962e2fe5cfd420e8f39b18af0f6da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oo-NrJc2vi6uTdlqjuWqmg.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">可视化注意力权重矩阵(</em> <a class="ae md" href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" rel="noopener ugc nofollow" target="_blank"> <em class="nq">来源</em> </a> <em class="nq"> ) </em></figcaption></figure><h2 id="0fca" class="ny mt it bd mu nz oa dn my ob oc dp nc la od oe ne le of og ng li oh oi ni iz bi translated">标度点积注意力</h2><p id="7796" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">缩放的点积是评分函数的更有效的计算设计。</p><p id="db6d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们用相同的向量长度(d)计算输入查询(Q)和键(K)的点积。然后，我们对它们进行缩放，以确保方差保持不同的向量长度，然后应用softmax函数来获得值(V)的权重。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/7265b02ade250288a4c255e182c16bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/0*Re-7v8P9PyyVzH1n"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/de5bba59ae7290d2d2ffc6d478425c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/0*Fz7-OxdYk08L_TjS"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/98a45ee07638ef0a942cc97a6c57490e.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/0*RUQO7s9OypAW8Hhr"/></div></figure><p id="761d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="nt">成比例的点积注意力&amp;多头注意力</em> <a class="ae md" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nt">来源</em></a><em class="nt"/></p><h2 id="b6fa" class="ny mt it bd mu nz oa dn my ob oc dp nc la od oe ne le of og ng li oh oi ni iz bi translated">多头注意力</h2><p id="c78c" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">我们并行执行单点乘积注意<strong class="kt jd"> h </strong>时间。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi om"><img src="../Images/27c81fed8a4c243675045716ce9eeb2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/0*9thlc4VUdxpFPkpX"/></div></figure><p id="aaa9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">w是键、查询和值的权重，O是输出线性变换。</p><p id="cf2e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">多头注意力在我们的模型中用于:</p><ul class=""><li id="6823" class="me mf it kt b ku kv kx ky la mg le mh li mi lm nr mk ml mm bi translated">解码器层；Queries是前一个解码器层的输出，Keys是编码器的输出。</li><li id="c791" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated">编码器自我关注层；关键字、查询和值来自编码器的前一层。</li></ul><h2 id="2b59" class="ny mt it bd mu nz oa dn my ob oc dp nc la od oe ne le of og ng li oh oi ni iz bi translated">自我关注</h2><p id="41a9" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">一种特殊的注意机制，其中的查询、键和值都来自同一个源。当序列长度(n)小于表征维度(d)时，自我注意(内部注意)比循环层更快。</p><p id="4616" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">自我注意被用来学习一个句子中不同单词的相关性，来计算同一个句子的表示。</p><h1 id="c202" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">位置式前馈网络</h1><p id="c4ea" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">编码器和解码器的每一层都包含完全连接的前馈网络，该网络利用两个线性变换和一个ReLU激活函数的相同前馈网络来变换每个位置中的表示。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi on"><img src="../Images/2062f2f3ae1e07dfa7517b3203f2cedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/0*vpXah1kYFBkP79Pt"/></div></figure><h1 id="22ae" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">嵌入和softmax</h1><p id="ac53" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">将输入和输出记号转换成模型维度的向量，并将解码器的输出转换成预测概率。</p><p id="9c4b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">另请查看:<a class="ae md" href="https://neptune.ai/blog/word-embeddings-deep-dive-into-custom-datasets" rel="noopener ugc nofollow" target="_blank">训练、可视化和理解单词嵌入:深入定制数据集</a></p><h1 id="b679" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">位置编码</h1><p id="ce1b" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">由于该模型没有递归和卷积，因此添加了一个层来利用序列顺序。在编码器和解码器堆栈的末尾，注入的信息包含关于令牌在该序列中的相对或绝对位置的信息。</p><h1 id="5c06" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">普通变压器概述</h1><p id="64d9" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">Vanilla Transformer是克服RNN模型缺点的一个很好的模型，但是它仍然有两个问题:</p><ul class=""><li id="43e7" class="me mf it kt b ku kv kx ky la mg le mh li mi lm nr mk ml mm bi translated"><strong class="kt jd">有限的上下文依赖:</strong>对于字符级语言建模，该模型被发现优于LSTM。然而，由于模型被设计为在几百个字符的独立固定长度段上进行训练，并且没有在段之间进行关联的信息，这引入了一个问题，即没有保存超过配置的上下文长度的长期依赖信息。有限的上下文相关性也使得该模式不能与几个片段之前出现的任何单词相关联。</li><li id="f599" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><strong class="kt jd">上下文碎片:</strong>在每个片段的前几个符号中，没有存储上下文信息，因为模型是为每个片段从头开始训练的，这导致了性能问题。</li></ul><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="dd82" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，我们仍然需要另一种增强来解决这些问题并克服这些缺点。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ca3f4ac942aa510bb6696fa0f47370fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/1*Hn85hDHhPPqkB6YVyodfmg.gif"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">香草变压器带4段</em> <a class="ae md" href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html" rel="noopener ugc nofollow" target="_blank"> <em class="nq">来源</em></a><em class="nq"/></figcaption></figure><h1 id="3044" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">变压器-XL</h1><p id="2d8b" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">transformer XL是transformer的更新版本(超长)。它是从普通的Transformer派生出来的，但是引入了递归机制和相对位置编码。</p><p id="a81b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在Transformer-XL中，模型将保留以前学习的段的隐藏状态，并将其用于当前段，而不是从头开始计算每个段的隐藏状态。</p><p id="2690" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该模型解决了普通变压器模型中引入的问题，并克服了长期依赖性问题。另一个优点是，它还解决了由使用最近初始化的或空的上下文信息引起的上下文碎片问题。因此，新模型现在可以用于字符级语言建模和单词级建模。</p><h1 id="020c" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">重现机制</h1><p id="7fef" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">为了保持段之间的依赖性，Transformer-XL引入了这种机制。Transformer-XL将像普通的Transformer一样处理第一段，然后在处理下一段时保持隐藏层的输出。</p><p id="61de" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">递归也可以加快评估速度。我们可以使用先前的分段表示，而不是在评估阶段从头开始计算。</p><p id="9640" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，每个图层的输入将是以下内容的串联形式:</p><ul class=""><li id="8d79" class="me mf it kt b ku kv kx ky la mg le mh li mi lm nr mk ml mm bi translated">前一层的输出，与vanilla Transformer中的相同(下图中的灰色箭头)。</li><li id="e3fa" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated">先前处理的隐藏层输出(下图中的绿色箭头)作为扩展上下文。</li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/7ee0a40afcda13b949bc5c3f41862e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/1*uQZloFpT1Y3LQxxBtiWPEw.gif"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">分段长度为4的Transformer-XL</em><a class="ae md" href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html" rel="noopener ugc nofollow" target="_blank"><em class="nq">来源</em></a><em class="nq"/></figcaption></figure><h1 id="3cc4" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">相对位置编码</h1><p id="d28b" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">循环机制似乎可以解决我们所有的问题。然而，递归机制引入了另一个问题:存储在隐藏状态中的位置信息从前面的段中重复使用。</p><p id="5579" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如在普通的transformer中一样，位置编码步骤提供的位置信息可以使我们从不同的片段中获得一些具有相同位置编码的标记，尽管它们的位置和重要性不同。</p><p id="7514" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该模型中添加的基本概念只是对隐藏状态中的相对位置信息进行编码，足以知道每个键与其查询之间的位置偏移，并且足以进行预测。</p><h1 id="799e" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">变压器XL摘要</h1><p id="70a1" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">Transformer-XL将递归和注意力机制相结合，将受上下文碎片和有限上下文依赖影响的vanilla transformer模型转换为词级语言模型，并通过添加递归机制和相对位置编码来提高其评估速度。</p><p id="ce21" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这导致长期依赖性增强。根据Transformer-XL的原始论文，它可以学习比RNNs长80%的依赖性，比vanilla transformers长450%，并在长序列和短序列上实现比vanilla transformer快1800+倍的更好性能。</p><p id="4012" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该模型在TensorFlow和PyTorch中实现，并可通过<a class="ae md" href="https://github.com/kimiyoung/transformer-xl/" rel="noopener ugc nofollow" target="_blank">开源</a>获得。</p><h1 id="28df" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">压缩变压器</h1><p id="ed0b" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">保持所有这些隐藏状态的一个缺点是，它增加了参与每个时间步的计算成本，以及保持所有这些信息的存储成本。</p><p id="45c6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">已经创建了几种方法来减少注意力的计算成本，例如稀疏访问机制，但是这并没有解决存储成本。</p><p id="7425" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">压缩变压器是变压器的简单扩展，灵感来自睡眠的概念。众所周知，睡眠会压缩记忆，从而提高推理能力。</p><p id="6857" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">压缩变换器利用注意力从过去中选择信息，然后将其压缩到压缩记忆中。这种压缩是通过用损失函数训练的神经网络来完成的，以保持相关信息。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi or"><img src="../Images/4127be2f3de3e4d3ef2ad508cbb06453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*T-3zNJDm9tNZROItmmQ23w.gif"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">抗压变压器[ </em> <a class="ae md" href="https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory" rel="noopener ugc nofollow" target="_blank"> <em class="nq">来源</em></a><em class="nq"/></figcaption></figure><h1 id="a977" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">压缩功能</h1><p id="ce10" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">建立在变压器XL之上。XL为每个层保留过去的激活，只有当它们过时时才丢弃它们。实现压缩模型是为了压缩旧的内存，而不是丢弃它们。</p><p id="9630" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该模型使用多种压缩函数:</p><p id="0795" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">合用被认为是最快和最简单的。最常用的压缩函数受差分神经计算机中垃圾收集机制的启发，数据按其平均使用量存储。卷积压缩函数需要训练一些权重。</p><h1 id="6724" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">压缩变压器总结</h1><p id="55eb" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">压缩变换有助于长程建模。如果这不适用于您的项目，那么压缩变换不会增加任何好处。</p><p id="087a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从下面的比较中可以看出，结果非常接近transformer-XL，但是在优化内存使用方面有很大的好处。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/2a5c91ce062b1f7d09681253f30b1de9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*VV5jKt6afq_3y3sFZIOdBg.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk translated"><em class="nq">对比结果来自原论文</em> <a class="ae md" href="https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory" rel="noopener ugc nofollow" target="_blank"> <em class="nq">来源</em></a><em class="nq"/></figcaption></figure><h1 id="90d8" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">相关著作</h1><h2 id="43ed" class="ny mt it bd mu nz oa dn my ob oc dp nc la od oe ne le of og ng li oh oi ni iz bi translated">改革家</h2><p id="ed22" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">用对位置敏感的散列法代替点积注意力，这种散列法将模型的复杂度从O(L2)变为O(L log L ),并使用残差层的可逆版本而不是使用标准残差层。这些变化降低了计算成本，并使模型在更快的同时与变压器模型的状态竞争。</p><h1 id="43cc" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">结论</h1><p id="9242" class="pw-post-body-paragraph kr ks it kt b ku nk kd kw kx nl kg kz la nm lc ld le nn lg lh li no lk ll lm im bi translated">仅此而已。我们探索了三种类型的变形金刚模型，希望现在能弄清楚它们为什么会复活。</p><p id="b9a9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">受人类视觉和记忆的启发，这些模型让我们更接近真正像人脑一样工作的模型。我们离它还很远，但是变形金刚是朝着正确方向迈出的一大步。</p><p id="418b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">感谢阅读！</p><p id="7a8b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">查看我的最新文章:</p><div class="os ot gp gr ou ov"><a href="https://medium.com/coderbyte/version-control-for-ml-models-why-you-need-it-what-it-is-how-to-implement-it-neptune-ai-497ff85b2b1a" rel="noopener follow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jd gy z fp pa fr fs pb fu fw jc bi translated">ML模型的版本控制:为什么需要它，它是什么，如何实现它— neptune.ai</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">版本控制在任何软件开发环境中都很重要，在机器学习中更是如此。在ML中…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">medium.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj lx ov"/></div></div></a></div><h1 id="46f9" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">关于变形金刚的其他资源</h1><ul class=""><li id="210e" class="me mf it kt b ku nk kx nl la pk le pl li pm lm nr mk ml mm bi translated"><a class="ae md" href="https://d2l.ai/index.html" rel="noopener ugc nofollow" target="_blank">深入研究深度学习参考书</a> —一本在线书籍，包含了对大多数深度学习算法的解释，并附有良好的代码样本。</li><li id="d287" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>——最初介绍变形金刚的原文。</li><li id="6be5" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.YIDgVpAzbIX" rel="noopener ugc nofollow" target="_blank">变形金刚——你只需要关注</a>——一篇文章用大量细节和代码示例展示了变形金刚。</li><li id="5724" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图示变压器</a></li><li id="bac7" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="https://medium.com/ml2b/introduction-to-compressive-transform-53acb767361e" rel="noopener">压缩变压器与LSTM </a></li><li id="8f9d" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">可视化神经机器翻译模型</a></li><li id="59cc" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="https://arxiv.org/pdf/2001.04451.pdf" rel="noopener ugc nofollow" target="_blank">改革者:高效的变压器</a></li><li id="29a1" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="https://arxiv.org/pdf/1802.05751.pdf" rel="noopener ugc nofollow" target="_blank">图像转换器</a></li><li id="8d4e" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="https://arxiv.org/pdf/1901.02860.pdf" rel="noopener ugc nofollow" target="_blank"> Transformer-XL:专注语言模型</a></li><li id="e1e7" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="https://arxiv.org/abs/1911.05507" rel="noopener ugc nofollow" target="_blank">用于长程序列建模的压缩变压器</a></li><li id="e49d" class="me mf it kt b ku mn kx mo la mp le mq li mr lm nr mk ml mm bi translated"><a class="ae md" href="https://towardsdatascience.com/transformer-xl-explained-combining-transformers-and-rnns-into-a-state-of-the-art-language-model-c0cfe9e5a924" rel="noopener" target="_blank"> Transformer-XL由<a class="pn po ep" href="https://medium.com/u/53f9e9fdd8d8?source=post_page-----6c73a9e6e2df--------------------------------" rel="noopener" target="_blank"> Rani Horev </a>解释</a></li></ul><p id="f1bc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="nt">原载于2021年5月7日</em><a class="ae md" href="https://neptune.ai/blog/comprehensive-guide-to-transformers" rel="noopener ugc nofollow" target="_blank"><em class="nt">https://Neptune . ai</em></a><em class="nt">。</em></p></div><div class="ab cl pp pq hx pr" role="separator"><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu"/></div><div class="im in io ip iq"><div class="lo lp lq lr gt ov"><a href="https://medium.com/@ahmhashesh/membership" rel="noopener follow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jd gy z fp pa fr fs pb fu fw jc bi translated">加入我的介绍链接媒体-艾哈迈德哈希什</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">medium.com</p></div></div><div class="pe l"><div class="pw l pg ph pi pe pj lx ov"/></div></div></a></div></div></div>    
</body>
</html>