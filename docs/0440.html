<html>
<head>
<title>End to End Tree Based Algorithm I: Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于端到端树的算法I:决策树</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/tree-based-algorithm-i-decision-tree-38cfe7f762b6?source=collection_archive---------2-----------------------#2020-04-28">https://pub.towardsai.net/tree-based-algorithm-i-decision-tree-38cfe7f762b6?source=collection_archive---------2-----------------------#2020-04-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f38f166285c4f866f0eaa2ef5fa8748d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w9CmhN-2cpYn3rNz.jpg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">选择哪条路！！</figcaption></figure><h2 id="c66f" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">介绍</h2><p id="9615" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">这个博客旨在向读者介绍决策树的概念、直觉和幕后的数学。在旅程中，我们将学习如何用python构建决策树，以及与这种健壮算法相关的某些限制。</p><p id="a67a" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">这个名字可能看起来很吸引人，但是树算法只是简单的基于规则的算法，我们已经在日常生活中不知不觉地使用了。监督学习的这种变体既可以用于分类，也可以用于回归。</p><h2 id="2778" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">什么是决策树？</h2><p id="bc9c" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated">决策树是一种监督算法，它使用流程图的概念来解决问题。现在，为了理解这个概念，考虑一个场景，其中需要根据某些特征来预测明天是否会下雨。我们已经看到的一种方法是使用逻辑回归。</p><p id="09c6" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">让我们为上面的问题陈述设置一个简单的实验，根据压力和湿度对给定的一组点进行分类，是否会下雨。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/3346e95659076eb803f94922b424a495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WHwY1pGV2eyUpz1xHGLQqQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">x1，x2是湿度和压力的轴。黄点表示无雨，黑点表示下雨。</figcaption></figure><p id="33cc" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">湿度以百分比计量，压力以千帕计量。</p><p id="dbe8" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">如果一个人使用他的直觉，如果湿度高于某个阈值，那么下雨的可能性就很高。所以他所做的只是根据湿度将数据集分成两半。</p><p id="3963" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">x1 = 70(假设)是阈值。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/88c32ded36a42e36d6fd1782bbfba5ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pfSLF6ZGTQIhwDmVbZ9djA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">x1指湿度轴上的值。左边是R2，右边是R1。</figcaption></figure><p id="2dc9" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">现在，我们有了一些规则来分割数据集，并创建两个区域R1和R2。我们在点数分类方面取得了一些进展。下一个问题是我们的分成有多好。</p><p id="ab09" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">可以有多种方法来验证我们的决定。测量之一可以简单到计算每个区域中误分类点的比例。误分类点的定义是所有标签与区域标签不同的点都被错误分类。在我们的场景中，<em class="md"> R1代表降雨，因此</em>所有非降雨点都被视为分类错误。同样，<em class="md"> R2代表无雨，</em>所以所有对应于雨的点都被错误地分类了。</p><p id="694b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">让我们称这个测度为<strong class="la ir"> M </strong> ( <em class="md"> R </em>)，其中<em class="md"> R </em>表示区域。</p><p id="b39a" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">对于<em class="md"> R1 </em>，<strong class="la ir"> M </strong> ( <em class="md"> R1 </em> ) = 1/7</p><p id="5207" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">同理对于<em class="md"> R2，</em>T20】米 ( <em class="md"> R2 </em> ) = 8/15</p><p id="a329" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">总体度量可以是<strong class="la ir"> M </strong> ( <em class="md"> R1 </em>)和<strong class="la ir"> M </strong> ( <em class="md"> R2 </em>)的任意组合；一种方法是取其平均值。这种方法的唯一问题是它没有考虑该区域的体积/密度。所以一个更好的结合它们的方法是通过权重，与每个区域的大小成比例。</p><p id="f2f1" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">M的计算(<em class="md"> R </em> ) </strong></p><p id="2044" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md"> R </em> <strong class="la ir"> <em class="md">:整体区域，n个点即</em> </strong> <em class="md"> |R| = n </em></p><p id="ffc6" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md"> |R1| = n1，|R2| = n2 </em></p><p id="a701" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md"> n1 + n2 =n </em></p><p id="6b40" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md">R</em><strong class="la ir"><em class="md">=</em></strong><em class="md">R1</em><strong class="la ir"><em class="md">U</em></strong>T36】R2</p><p id="53e3" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md">w1 = R1地区的权重</em></p><p id="dc31" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md">w2 = R2地区的权重</em></p><p id="fada" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">w1 + <em class="md"> w2 =1 </em></p><p id="7af9" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">M</strong>(<em class="md">R</em>)= w1 *<strong class="la ir">M</strong>(<em class="md">R1</em>)+<em class="md">w2 *</em><strong class="la ir">M</strong>(<em class="md">R2</em>)</p><p id="59b4" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">=<em class="md">n1/(n1+N2)*</em><strong class="la ir">M</strong>(<em class="md">R1</em>)+<em class="md">N2/(n1+N2)*</em><strong class="la ir">M</strong>(<em class="md">R2</em>)</p><p id="f3d1" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">在我们的例子中n1 = 7，<em class="md"> n2 = 15，</em><strong class="la ir">M</strong>(<em class="md">R1</em>)= 1/7，<strong class="la ir"> M </strong> ( <em class="md"> R2 </em> ) = 7/15</p><p id="afc1" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">M</strong>(<em class="md">R</em>)= 7/22 *(1/7)+15/22 *(8/15)= 0.41</p><p id="4854" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">越小的M越好。</p><p id="e3eb" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">对<strong class="la ir"> M </strong> ( <em class="md"> R </em>)的另一种解释是根据聚类的类内和类间概念，其中高的类间方差和低的类内方差导致良好定义的分离的聚类。</p><p id="43ed" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">让我们继续，并根据压力值进一步细分这两个区域。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mf"><img src="../Images/f3f7ae14dabe6f7fe6f5b36bf1c04a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VNKkrQRrrrQFu3zHcv8IPg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">r分为四部分，R11左下，R21右下，R12左上，R22右上。</figcaption></figure><p id="88ef" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">现在，它看起来比之前只基于湿度的分裂要好一些。我让读者来计算M(R)值并验证上述论点。</p><p id="e6ad" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">这种将参数空间反复分割成更小空间的整个实验就是我们所说的决策树分类方法。从形式上来说，决策树是一种基于树的决策及其结果的算法。</p><p id="8da2" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">节点代表评价，每个评价的结果作为一个分支，叶节点作为一个类。从叶节点到每个内部节点并最终到根节点的整个路径作为决策/分类规则。<br/>上述实验的图形符号如下图所示。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mg"><img src="../Images/dcacab6a0221e9fa55b0003b4d2400c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFcVlRPrdVFNno-UFuF5bg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">决策树的流程图。</figcaption></figure><p id="d34b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">在深入研究之前，这是了解决策树的每个元素的好时机。</p><ul class=""><li id="696f" class="mh mi iq la b lb lt lf lu kl mj kp mk kt ml ls mm mn mo mp bi translated">根节点:位于顶部的节点，包含整个样本。</li><li id="6c22" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">分支节点/子节点:拆分后获得的节点。</li><li id="5347" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">父子:分裂的节点称为父节点，分支节点称为子节点。</li><li id="ee42" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">拆分值:评估恰好将父代拆分为子代时的值。分裂的过程被称为分裂。</li><li id="2d71" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">叶节点/终端节点:没有子节点的子节点。</li><li id="ac4f" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">子树:至少有一个叶节点的任何一组分支节点。</li><li id="c059" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">边:任意两个节点之间的路径，连接两个节点的线。</li><li id="1748" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">节点深度:从该节点到根节点的边数。</li><li id="01e4" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">节点高度:到达叶节点的最大边数。</li><li id="ec8e" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">修剪:修剪掉树中预测能力较弱的部分的过程。</li></ul><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/25bc3f542d2cc81f3f7337b775993450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jkzl3UUTk_RBoS5ye_ALSw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">描绘树木的不同组成部分。🌲</figcaption></figure><ul class=""><li id="38b5" class="mh mi iq la b lb lt lf lu kl mj kp mk kt ml ls mm mn mo mp bi translated">根的深度为0；高度是3，因为在这种情况下到达叶片8的最大步幅是3。</li><li id="c244" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">任何叶节点的高度都是0。</li><li id="d94b" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">节点3的深度为1，高度为2。</li><li id="ac12" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">节点4的深度是2。</li></ul><p id="317f" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">我们已经讨论了一个度量来决定分裂的良好性。接下来，我们讨论一些其他指标，如基尼系数、熵、卡方、方差。</p><p id="0ada" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">其他指标</strong></p><p id="bede" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir"> <em class="md">【基尼指数】</em> </strong>:</p><p id="aae5" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">一个节点的基尼系数是该节点中任意随机选择的点被错误分类的概率，如果它被分配了那个类别的标签。</p><p id="1791" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">设pi:标签为I的点的概率<br/> GI(point) = pi*(它属于除I以外的标签的概率)=pi(1-pi)</p><p id="0cf4" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">假设存在M个类别/标签。</p><p id="916c" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">GI(node)= GI(所有点)之和= pi(1-pi)之和对于所有I属于{1，2，..，M}。</p><p id="f92b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">GI(node) = 1- pi**2 — p2**2 — … -pM**2</p><p id="5f4e" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">对于二元分类；<br/> GI(node) = 1 — pi**2 — p2**2，其中p1是yes的概率，p2是no的概率。</p><p id="3e05" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">考虑一个二进制值的节点(<strong class="la ir"> <em class="md"> S </em> </strong>)说是或否。</p><p id="2791" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir"> <em class="md"> S </em> </strong> = {是，不是，不是，是，是，是，是，是，不是，不是}</p><p id="2271" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">p1 =可能是:#Yes/(#Yes + #No)</p><p id="afa8" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">p2 = No的概率:#No/(#Yes + #No)</p><p id="e68c" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">#表示基数或计数</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/dcfc1f073fc7aff80d2db93ac1da9cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*uSTKD_1bPNOfudtpy7vkRw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">x(1-x)的绘图</figcaption></figure><p id="8439" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">因为p1 + p2 = 1</p><p id="42e4" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">GI = 2p1*p2</p><p id="3f65" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">如果我们想要高p1，那么No的情况应该相当低，反之亦然。这个对称的措施符合我们的目的。</p><p id="a6c8" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">从上面的图中我们看到，对于p1 = p2，它是最佳的。</p><p id="f4f9" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">总的来说，GI是<em class="md"> w1*G1 </em>(节点1) + <em class="md"> w2*G1 </em>(节点2)，其中<em class="md"> w1 </em>和<em class="md"> w2 </em>是那个节点的权重。</p><p id="679b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">GI越低分裂越好。</p><p id="dd6d" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">地理信息的Python实现</p><p id="b721" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">让我们在我们的降雨预测问题中计算GI。</p><p id="3b49" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md">根据湿度分割</em>:</p><p id="886e" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">w1 = 7/22，w2 = 15/22 <br/>对于节点1 <br/> p1 = 6/7，p2 = 1/7 <br/> g1 = 0.24 <br/>对于节点2 <br/> p1 = 8/15，p2 = 7/15<br/>G2 = 0.5<br/>T33】GI(湿度)= 0.42 </p><p id="1c68" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md">基于压力的分割:</em></p><p id="fd6d" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">w1 = 14/22，w2 = 8/22 <br/>对于节点1 <br/> p1 = 13/14，p2 = 1/14 <br/> g1 = 0.13 <br/>对于节点2 <br/> p1 = 1/8，p2 =7/8 <br/> g2 = 0.22 <br/> <strong class="la ir"> GI(压力)= 0.16 </strong></p><p id="f811" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">因此，分割应基于压力。</p><p id="81e6" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir"> <em class="md"> 2。基于熵的信息增益</em> </strong></p><p id="b8ae" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md">熵被定义为自我信息的期望值I(x) </em></p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/e465024a522f8113385dd26d5d41accf.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*danu8AmDzfvfh1CrxekbjQ.png"/></div></figure><p id="ef04" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">考虑一个样本，我们假设有一个未知的过程(随机变量)产生这个样本。最大似然原则获得最大化样本和过程的参数。</p><p id="d73b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">如果一个人观察到一个具有不太可能的值的数据点，我们称之为自我信息，因为它给出了关于随机变量的更多信息。</p><p id="ef75" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">最真实的例子可以是抛硬币；如果你知道它偏向尾部，那么由于没有获得任何信息，自我信息将为0。类似地，对于无偏硬币，它将是最大值，即1。</p><p id="7e4e" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">信息增益= 1-熵</p><p id="8533" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">根据概率E(node) = -p*log(p) -q*log(q)，p+q = 1 <br/>这里还评估每个节点的熵，然后取加权平均值作为熵来决定分裂。</p><p id="7834" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir"> <em class="md"> 3。</em>卡方:</strong></p><p id="c779" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md">根据父子节点的统计显著性评估分裂的另一种方法</em></p><p id="57ea" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">c(节点)=(实际—预期)**2 /(预期)**2</p><p id="8579" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">这个计算在我之前关于适者生存的博客里有。</p><p id="d49d" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">4<em class="md">4。差异:</em> </strong></p><p id="0771" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">到目前为止，我们讨论的度量是针对分类变量的；在连续变量的情况下，我们使用方差。方法是一样的，计算每个节点的方差，然后用加权方差决定拆分。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/3f5c7762f444ff58c0a7883d5ac5e6da.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*ecfP6Fxbu5ahshY9wkXegw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">x =平均值</figcaption></figure><blockquote class="mz na nb"><p id="5e22" class="ky kz md la b lb lt ld le lf lu lh li nc lv lk ll nd lw ln lo ne lx lq lr ls ij bi translated">不同决策树算法使用不同的度量。</p></blockquote><p id="fd48" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">模型复杂度</strong></p><p id="c53d" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">现在，我们已经了解了如何将根节点拆分为分支节点。如果我们在最后继续做这个过程，我们最终将同质叶节点作为一个类标签。过度拟合的风险很高，为了克服这一点，复杂性也增加了；我们开始删除不包含足够信息的节点。这个过程被称为树的修剪，我们对交叉验证集进行评估。</p><p id="d143" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">克服过度拟合和模型复杂性的另一种方法是试验分类器的不同参数。</p><ul class=""><li id="8691" class="mh mi iq la b lb lt lf lu kl mj kp mk kt ml ls mm mn mo mp bi translated">树的最大深度。<strong class="la ir"> max_depth </strong> <em class="md"> int，default=None </em></li><li id="2f32" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">一片叶子上的最小点数。<strong class="la ir">min _ samples _ leaf</strong>T25】int或float，默认=1 </li><li id="7407" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">叶节点的最大数量。<strong class="la ir"> max_leaf_nodes <em class="md"> int，default = None</em>T30】</strong></li><li id="e5df" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">节点中要分割的最小点数。<strong class="la ir">min _ samples _ split</strong><em class="md">int或float，默认=2 </em></li><li id="fa89" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">最大功能。<strong class="la ir"> max_features </strong> <em class="md"> int，float or {"auto "，" sqrt "，" log2"}，默认=None </em></li><li id="a0c5" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">分割<strong class="la ir">的杂质<strong class="la ir">和</strong>的最小减少量。最小_杂质_减少<em class="md">浮动，默认=0.0 </em> </strong></li><li id="9080" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls mm mn mo mp bi translated">其他因素取决于用例。</li></ul><h2 id="50c7" class="kc kd iq bd ke kf kg dn kh ki kj dp kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">Python实现</h2><p id="04dc" class="pw-post-body-paragraph ky kz iq la b lb lc ld le lf lg lh li kl lj lk ll kp lm ln lo kt lp lq lr ls ij bi translated"><em class="md">数据集:降雨数据集快照。</em></p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/919a0399a53b5fbf448ccb0a96588795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hIbgKJW9Jh_X3YmgvFKmPQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">118个特征</figcaption></figure><p id="6360" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">目标= <em class="md">【正常降雨】</em></p><p id="d9c0" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">我们已经完成了EDA和数据预处理，你可以在之前的博客中看到我们实现了逻辑回归。因此，目前，您可以假设这些数据是干净的。</p><p id="5757" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">形象化</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/2f7b03bede8240cbeed230499257345e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8LaSVD64jCOp2EP5m3vxDQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">树4的深度。</figcaption></figure><p id="fa68" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">此数据集的基尼系数和熵的比较。</p><p id="796b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">像sklearn中的所有估计器一样，决策树分类器也有一个预测和评分方法。</p><p id="e016" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">我们知道，默认的标准是基尼系数，用一个熵值怎么样。</p><p id="f856" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">基于基尼系数的模型得分:0.834</p><p id="450f" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">基于熵的模型得分:0.836</p><p id="846d" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">在这个数据集中，对于相同的超参数，熵的表现略好于基尼系数。</p><p id="2110" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">另一个衡量分类器的标准在我之前的<a class="ae nh" href="https://medium.com/analytics-vidhya/model-validation-for-classification-5ff4a0373090" rel="noopener">博客</a>中有。</p><p id="45f1" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">特征重要性，及其计算</strong></p><p id="bcf8" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">一旦模型被训练，下一个任务是找出控制模型的基本特征。特性的重要性对于说服其他人相信我们的模型以及推理是非常有用的。来自数据的洞察力有助于任何服务的流程构建。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f07cb4157c50a936f3e4af0baae145f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*BTej_smIVjD9GMhG4-P2TQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">这是一个描述特征重要性的图</figcaption></figure><p id="f8f0" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">超参数调谐</strong></p><p id="045f" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">有不同的技术来调整超参数；它本身就是一个全新的范例。可以有一个关于如何优化你的模型超参数的独立博客。</p><p id="4b71" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">在这次演讲中，我们将使用Grid-searchCV，它只是对估计器的给定参数进行彻底的搜索，在我们的例子中是决策树分类器。它在网格搜索中使用交叉验证技术，根据提供的评分方法得出最佳参数。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/e3ac7d697ca2498e2a09f470a2ef9a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hw2Ms4TIsheUrRL-x0BrMA.png"/></div></div></figure><pre class="lz ma mb mc gt nk nl nm nn aw no bi"><span id="34bd" class="kc kd iq nl b gy np nq l nr ns">print(grid.best_score_)<br/>print(grid.best_params_)<br/>print(grid.best_estimator_)</span><span id="4f36" class="kc kd iq nl b gy nt nq l nr ns">0.8403836694950619<br/>{'max_depth': 8, 'min_samples_leaf': 10}<br/>DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',<br/>max_depth=8, max_features=None, max_leaf_nodes=None,<br/>min_impurity_decrease=0.0,   min_impurity_split=None,<br/>min_samples_leaf=10, min_samples_split=2,<br/>min_weight_fraction_leaf=0.0, presort='deprecated',<br/>random_state=None, splitter='best')</span></pre><p id="d254" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">所以我们看到最好的参数是<em class="md"> {'max_depth': 8，' min_samples_leaf': 10}，</em>和<em class="md">精度0.84，</em>比普通模型好。</p><p id="aeae" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">我让读者包括一些其他超参数，如<em class="md">标准、最大_叶_节点、最小_样本_分割、最大_特征、最小_杂质_减少、</em>，并微调模型。</p><p id="2b9e" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">修剪</strong></p><p id="a207" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">修剪树中预测能力较弱的部分的过程。</p><p id="b647" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">如上所述，修剪用于将模型推广到看不见的数据，也就是说，减少过度拟合和复杂性。理解这种现象的另一种方式是，一个人应该知道何时停止，但出于实际目的，我们从来不知道添加/删除单个节点会如何大幅改变分数。上述现象也被称为<a class="ae nh" href="https://en.wikipedia.org/wiki/Horizon_effect" rel="noopener ugc nofollow" target="_blank">视界</a>效应。</p><p id="b618" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">在我们的例子中，训练精度是0.8349，测试精度是0.8344，这是非常相似的，因此不存在过度拟合的情况。</p><p id="7a3d" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">然而，让我们看看修剪是如何工作的及其python实现。</p><p id="103c" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">有不同的方法，无论是预修剪或早期停止🛑，它有自己的挑战，如前所述，所以在这里我们将张贴修剪。</p><blockquote class="mz na nb"><p id="6dd7" class="ky kz md la b lb lt ld le lf lu lh li nc lv lk ll nd lw ln lo ne lx lq lr ls ij bi translated">我们将讨论L. Breiman、J. Friedman、R. Olshen和C. Stone的论文中提到的一种后剪枝实现。</p></blockquote><p id="bc0b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir">最小成本复杂度剪枝</strong></p><p id="b34f" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><em class="md">最小代价复杂度剪枝是</em> <strong class="la ir"> <em class="md"> </em> </strong> <em class="md">直观地为增加的拆分增加惩罚的一种方式。</em></p><p id="cd17" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">我们为树T定义一个代价C(α，T)，其中α ≥0称为复杂度参数。</p><p id="40b7" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">C(α，T) = C(T) + α|T|，</p><p id="8f9b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">C(T):叶/终端节点的总误分类</p><p id="caea" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">|T|:树T中终端节点的数量</p><p id="ae78" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">为了更进一步，让我们试着理解C(T)</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/5cec0b5debd20ff197ac7c653993bfb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cIIODiG2WDj1G8vx0XHxMw.png"/></div></div></figure><p id="f15a" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">c(t):节点t中点的误分类概率<br/>整棵树的误分类概率:使用全概率公式。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/ed49a3fa14e490180ffda407452c8604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDI26ehwBq4uTCYwl5MZTg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">亲子不平等</figcaption></figure><p id="406c" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">父母的误分类率总是大于孩子的加权和。因此，如果我们继续分裂，错误分类就会减少，最终导致过度拟合。</p><p id="52cf" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">给定一棵树T &lt; T_max,</p><p id="a932" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">T_max: max possible tree.</p><p id="aab5" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">If α=0: largest tree selected</p><p id="5e70" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">for α tending to infinity single root node tree.</p><p id="3594" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">To get optimal T(α), we need to minimize C(α, T).</p><p id="2c6b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">C(α ,T(α)) = min (C(α ,T)) for all T &lt; T_max</p><p id="8f3b" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">min exists since we will have only finitely many values of T.</p><p id="a181" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">For optimum</p><ol class=""><li id="b740" class="mh mi iq la b lb lt lf lu kl mj kp mk kt ml ls nw mn mo mp bi translated">C(α ,T(α)) = min (C(α ,T)) for all T &lt; T_max)</li><li id="d4ee" class="mh mi iq la b lb mq lf mr kl ms kp mt kt mu ls nw mn mo mp bi translated">C(α, T) = C(α ,T(α)) ===&gt; T(α) ≤ T，否则存在一棵更大的树。</li></ol><p id="5041" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">对于单个节点，我们将得到C(α，T) = C(T) + α</p><p id="0d72" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">考虑一棵树(树枝),它的根是T，用T_t表示</p><p id="72fc" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">C(T_t) ≤ C(t)，来自亲子不等式方程。</p><p id="2691" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">α_opt是等式成立时的值，即</p><p id="2613" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">C(T_t) = C(t)</p><p id="94ca" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">C(T_t) + α|T_t| = C(t) + α|t| = C(t) + α</p><p id="6902" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">α(| T T |-1)= C(T)-C(T T)</p><p id="d0f4" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">αopt = C(T)-C(T T)/(| T T |-1)</p><p id="e51f" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">最弱链路:具有最小α_opt值的任何非终端节点</p><p id="6da6" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">我们剪枝最弱的环节，直到α_ opt≥CCP _α参数。</p><p id="f502" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated"><strong class="la ir"> Python实现</strong></p><p id="989f" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">决策树分类器有方法<strong class="la ir">cost _ complexity _ pruning _ path</strong>(<em class="md">self</em>，<em class="md"> X </em>，<em class="md"> y </em>，<em class="md"> sample_weight=None </em>)计算剪枝的路径。它为每个步骤返回最佳α和相应的总叶杂质。</p><p id="5a17" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">在我们的例子中，我使用了一个sklearn可视化脚本进行修剪。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/2e668da9aec68fa62915a6989e87aa17.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*kZ0eqLoh7EyA2pYEOdpxbQ.png"/></div></figure><pre class="lz ma mb mc gt nk nl nm nn aw no bi"><span id="d7d5" class="kc kd iq nl b gy np nq l nr ns">Number of nodes in the last tree is: 3 with ccp_alpha: 0.061183857976855216</span></pre><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d5eac892052388f4e4d892b1522503c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*Kn1Cj5BXnF0R37xtiRAUUQ.png"/></div></figure><p id="4bcf" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">我们用最后一个观察来总结剪枝，它在准确性和alpha之间。</p><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/bcc419d97f287b0ecc67edf893daec15.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*iiLgWe9yU-Q_CUjxLy_2ag.png"/></div></figure><p id="6a81" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">最初，测试acc约为0.78，而系列1随着α增加而达到平衡。</p><p id="9f2f" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">在这篇博客中，我们学习了决策树的工作原理、python实现、超参数调整和修剪。</p><p id="70a6" class="pw-post-body-paragraph ky kz iq la b lb lt ld le lf lu lh li kl lv lk ll kp lw ln lo kt lx lq lr ls ij bi translated">我希望你喜欢它😊！！🥂</p></div></div>    
</body>
</html>