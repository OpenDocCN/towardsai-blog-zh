<html>
<head>
<title>Logistic Regression from Scratch with Only Python Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">仅使用Python代码从零开始进行逻辑回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/logistic-regression-from-scratch-with-only-python-code-9d3ae607e739?source=collection_archive---------1-----------------------#2020-07-11">https://pub.towardsai.net/logistic-regression-from-scratch-with-only-python-code-9d3ae607e739?source=collection_archive---------1-----------------------#2020-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c504" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="386f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">仅使用Python对多要素数据集应用逻辑回归。用Python逐步实现编码示例</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/fb7c1f734c4d2a7a59d85632c9714d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4qEfDR8AkxSIIy-uqu8zxw.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">从零开始的逻辑回归</figcaption></figure><blockquote class="le lf lg"><p id="8a67" class="lh li lj lk b ll lm ka ln lo lp kd lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在本文中，我们将建立一个<strong class="lk ja">逻辑</strong> <strong class="lk ja">回归</strong>模型，用于对患者是否患有糖尿病进行分类。这里的重点是，我们将只使用<strong class="lk ja"> python </strong>来构建读取文件、规范化数据、优化参数等功能。因此，您将深入了解从阅读文件到做出预测的一切是如何工作的。</p></blockquote><p id="dae4" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">如果你是机器学习新手，或者不熟悉<strong class="lk ja">逻辑回归</strong>或<strong class="lk ja">梯度</strong> <strong class="lk ja">下降</strong>，不要担心我会尽力用<strong class="lk ja"> <em class="lj">俗人的</em> </strong>术语解释这些。有更多的教程解释相同的概念。但是本教程的独特之处在于它的<strong class="lk ja"> <em class="lj">简短的</em> </strong>和<strong class="lk ja"> <em class="lj">初学者友好的</em> </strong>代码片段的高级描述。因此，让我们从一些重要的理论概念开始，以便理解我们模型的工作原理。</p><h2 id="b73c" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated"><strong class="ak">逻辑回归</strong></h2><p id="98c9" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">逻辑回归是入门级<strong class="lk ja">监督</strong>机器学习算法，用于<strong class="lk ja">分类</strong>目的。这是每个人都应该知道的算法之一。逻辑回归在某种程度上类似于线性回归，但它具有不同的<strong class="lk ja">成本</strong> <strong class="lk ja">函数</strong>和<strong class="lk ja">预测</strong> <strong class="lk ja">函数</strong>(假设)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ne"><img src="../Images/79c8cf01b117bc53ac6e38b44432b69e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*cezDhOKzsa4cWn98zFDPuQ.png"/></div></div></figure><h2 id="ab7a" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">Sigmoid函数</h2><p id="7a6c" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">在<strong class="lk ja"> 0 </strong>和<strong class="lk ja"> 1 </strong>之间的范围内<strong class="lk ja">挤压</strong>该函数的输出是激活函数，其中小于0.5的值表示0级，大于等于0.5的值表示1级。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7dbc78db7edd068ca34a476cdcbc569b.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*luiZ3lcz9bBw4iNY0TF6jA.png"/></div></figure><h2 id="a6b9" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">价值函数</h2><p id="281b" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated"><strong class="lk ja">成本函数</strong>找出我们算法的<strong class="lk ja"> <em class="lj">实际值</em> </strong>和<strong class="lk ja"> <em class="lj">预测值</em> </strong>之间的误差。应该尽可能少。在线性回归的情况下，公式如下</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/a872e1aa23fc72e66b0ab597349c4c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*DZaejTqdbwjqQnBnEpu89g.png"/></div></figure><p id="1028" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">但是这个公式<strong class="lk ja">不能</strong>用于<strong class="lk ja">逻辑</strong>T7】回归，因为这里的假设是一个<strong class="lk ja">非凸</strong>函数，这意味着有机会找到局部最小值，从而避免全局最小值。如果我们使用相同的公式，那么我们的绘图将如下所示</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nh"><img src="../Images/4bda41fd00e1de5c951ad413e9580aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8I3yLN0YOhg6DbbgMLv7vg.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">非凸图</figcaption></figure><p id="de48" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">因此，为了避免这种情况，我们在<strong class="lk ja">日志</strong>的帮助下<strong class="lk ja">平滑了</strong>曲线，我们的成本函数将如下所示</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ni"><img src="../Images/dcc517bbb49b712dc288ea9127ac1610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWvYunYNRvkTz7ROGR43_w.png"/></div></div></figure><p id="5811" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">其中m =数据集中的示例数或行数，iᵗʰ示例的xᶦ=feature值，iᵗʰ示例的yᶦ=actual结果。使用这个之后，我们的成本函数图将如下所示</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/bc9e74c4468016f1424a77a040ad4ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*a7BSbhFb9Eaze3jOeb7gBg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">凸图</figcaption></figure><h2 id="7881" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">梯度下降</h2><p id="451b" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">我们在任何ML算法中的目标都是找到使<strong class="lk ja">最小化</strong><strong class="lk ja">成本</strong> <strong class="lk ja">函数</strong>的参数集。为了自动找到最佳参数集，我们使用了优化技术。其中之一就是梯度下降。在这种情况下，我们从参数的随机值开始(在大多数情况下<strong class="lk ja"><em class="lj"/></strong>)然后不断改变参数以减少J( <strong class="lk ja"> θ </strong> ₀，<strong class="lk ja"> θ </strong> ₁)或成本函数，直到我们最终达到最小值。同样的公式是:-</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3eeca8f609fdb9fcea0a2c2be171399a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*EIHMjeNwHRqWE0bi2xhOUg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">梯度下降</figcaption></figure><p id="7b1e" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">它看起来与线性回归完全相同，但不同之处在于假设(<strong class="lk ja"> hθ(x) </strong>)，因为它也使用了<strong class="lk ja"> sigmoid </strong>函数。</p><p id="b7f5" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">我知道这需要很多理论，但这是理解下面的代码片段所必需的。而我只触及了表面，所以请谷歌一下以上的主题，获取更深入的知识。</p><h1 id="2886" class="nl mi iq bd mj nm nn no mm np nq nr mp kf ns kg ms ki nt kj mv kl nu km my nv bi translated">先决条件:</h1><p id="e320" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">我假设您熟悉<strong class="lk ja"> <em class="lj"> python </em> </strong>，并且已经在您的系统中安装了<strong class="lk ja"> <em class="lj"> python 3 </em> </strong>。我用了一个<strong class="lk ja"> <em class="lj"> jupyter笔记本</em> </strong>来做这个教程。你可以使用你喜欢的<strong class="lk ja"> IDE </strong>。所有需要的库都内置在<strong class="lk ja"> <em class="lj"> anaconda </em> </strong>套件中。</p><h1 id="73c1" class="nl mi iq bd mj nm nn no mm np nq nr mp kf ns kg ms ki nt kj mv kl nu km my nv bi translated">让我们编码</h1><p id="e334" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">好了，我已经导入了CSV、numpy(主要用于点积)和math来执行对数和指数计算。</p><h2 id="7db4" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">阅读文件</h2><p id="7ccc" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">首先，我们定义了函数<code class="fe nw nx ny nz b">read_file</code>来读取数据集本身。这里，文件是用<code class="fe nw nx ny nz b">with</code>打开的，所以我们不必关闭它并把它存储在reader变量中。然后我们循环遍历<code class="fe nw nx ny nz b">reader</code>并在名为<strong class="lk ja">数据集</strong>的列表中追加每一行。但是加载的数据是字符串格式，如下图所示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c0669b825a0242d0291b30689d613c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*Ta6oi2zydbGgEsle5ZF0Ww.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">读取CSV后的数据</figcaption></figure><h2 id="351f" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">将字符串转换为浮点型</h2><p id="3bce" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">这里的<code class="fe nw nx ny nz b">string_to_float</code>函数有助于将所有字符串值转换为浮点型，以便对其执行计算。我们简单地循环每一行和每一列，并将每个条目从字符串转换为浮点。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/5273f20cbb3193b5a232cc9fccae8611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*UEHVFQ8nPyr3CA-QLgcxRw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">数据转换为浮点型</figcaption></figure><h2 id="23ef" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">查找最小值最大值</h2><p id="69db" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">现在，为了执行<strong class="lk ja">归一化</strong>或在同一标度上获得所有值，我们必须从每一列中找到最小<strong class="lk ja">值</strong>和最大<strong class="lk ja">值</strong>。在这里的函数中，我们按列进行循环，并在<code class="fe nw nx ny nz b">minmax</code>列表中添加每一列的最大值和最小值。获得的值显示在下面的截图中。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/d1cecd93505b12e9e1f03541398f8eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*mgR1QBOIptfTxAmPlzhvKw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">每列的最小最大值</figcaption></figure><h2 id="d1e3" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">正常化</h2><p id="8c94" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">现在，我们已经遍历了数据集中的每个值，并从中减去(该列的)最小值，然后除以该列的最大值和最小值之差。下面的屏幕截图显示了单行示例的规范化值。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/5890558b3042e62dc85c919ee295d09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*T_SdgBNyAE-hdauUE5WMaQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">标准化后的单行值</figcaption></figure><p id="c708" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated"><strong class="lk ja">列车试分裂</strong></p><p id="30ae" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">在这里，<code class="fe nw nx ny nz b">train_test</code>功能帮助创建训练和测试数据集。我们使用了随机模块中的<code class="fe nw nx ny nz b">shuffle</code>来混洗整个数据集。然后我们将数据集分割成80%并存储在<code class="fe nw nx ny nz b">train_data</code>中，剩下的20%存储在<code class="fe nw nx ny nz b">test_data</code>中。两组的大小如下所示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a1a7ac129182918025e14830512530b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*VL5K5F53wBYw5NxWZbxtNA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">训练和测试集的大小</figcaption></figure><h2 id="0408" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">准确(性)</h2><p id="6a25" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">这个<code class="fe nw nx ny nz b">accuracy_check</code>函数将用于检查我们模型的准确性。这里，我们简单地循环实际或预测列表的长度，因为两者长度相同，如果两者在当前索引处的值相同，我们增加计数<code class="fe nw nx ny nz b">c</code>。然后只需将计数<code class="fe nw nx ny nz b">c</code>除以实际或预测列表长度，并乘以100，即可得到准确率%。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="8391" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">预测或假设函数</h2><p id="ce75" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">是的，这里我们引入了<code class="fe nw nx ny nz b">numpy</code>来计算点积，但是也可以创建函数。<code class="fe nw nx ny nz b">math</code>用于计算指数。<code class="fe nw nx ny nz b">prediction</code>函数是我们的假设函数，它将整行和参数作为参数。然后，我们用θo初始化假设变量，我们循环遍历每个行元素，忽略最后一个，因为它是目标<code class="fe nw nx ny nz b">y</code>变量，并将xᵢ*θ(i+1(I+1在下标中)添加到假设变量。之后，出现了<code class="fe nw nx ny nz b">sigmoid</code>函数1/(1+exp(-hypothesis))，该函数将值压缩到0–1的范围内。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="c27c" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">价值函数</h2><p id="6490" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">我们不一定需要这个函数来让我们的模型工作，但是计算每次迭代的成本并绘制出来是很好的。在<code class="fe nw nx ny nz b">cost_function</code>中，我们循环了数据集中的每一行，并用上述公式计算了该行的<code class="fe nw nx ny nz b">cost</code>,然后将其添加到成本变量中。最后，返回平均成本。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="5b73" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">最优化技术</h2><p id="0e8f" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">这里，我们使用了<code class="fe nw nx ny nz b">gradient_descent</code>为我们的模型自动寻找最佳的参数集。该函数以<strong class="lk ja">数据集</strong>、<strong class="lk ja">历元</strong>(迭代次数)和<strong class="lk ja"> alpha </strong>(学习率)作为参数。在该函数中，<code class="fe nw nx ny nz b">cost_history</code>被初始化以在每个历元后追加成本，<code class="fe nw nx ny nz b">parameters</code>保存参数集(参数数量=特征+1)。之后，我们开始一个循环，重复寻找参数的过程。内部循环用于迭代数据集中的每一行。这里，由于成本函数的偏导数，梯度项对于θo是不同的，这就是为什么它被单独计算并添加到参数表中的第0个位置，然后其他参数使用行的其他特征值(忽略最后一个目标值)计算并添加到参数表中它们各自的位置。对每一行重复相同的过程。此后，1个时期完成，用计算的参数集调用<code class="fe nw nx ny nz b">cost_function</code>，并将获得的值附加到<code class="fe nw nx ny nz b">cost_history</code>。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="7a51" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">组合算法</h2><p id="4dad" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">这里，我们导入了<code class="fe nw nx ny nz b">matplotlib.pyplot</code>只是为了绘制成本图。所以没必要。在<code class="fe nw nx ny nz b">algorithm</code>函数中，我们用epochs= <strong class="lk ja"> 1000 </strong>和learning_rate = <strong class="lk ja"> 0.001 </strong>调用<code class="fe nw nx ny nz b">gradient_descent</code>。之后，在我们的测试数据集上做预测。<code class="fe nw nx ny nz b">round</code>用于对得到的预测值进行四舍五入(即0.7=1，0.3=0)。然后，调用<code class="fe nw nx ny nz b">accuracy_check</code>来获得模型的精度。最后，我们绘制了迭代v/s成本图。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="6497" class="mh mi iq bd mj mk ml dn mm mn mo dp mp me mq mr ms mf mt mu mv mg mw mx my iw bi translated">把所有东西放在一起</h2><p id="0316" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">为了有一个合适的结构，我们将所有的函数放在一个单独的<code class="fe nw nx ny nz b">combine</code>函数中。我们能够实现大约<strong class="lk ja"> 78.5% </strong>的准确度，可以通过对模型进行超调来进一步提高。下图也证明了我们的模型工作正常，因为成本随着迭代次数的增加而降低。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a10ad51ae7b48daad5eaa7dd364801d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*I4xYPI1f4nHRVCSdQzzxCg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">迭代v/s成本</figcaption></figure><h1 id="aa17" class="nl mi iq bd mj nm nn no mm np nq nr mp kf ns kg ms ki nt kj mv kl nu km my nv bi translated">结论</h1><p id="66c1" class="pw-post-body-paragraph lh li iq lk b ll mz ka ln lo na kd lq me nb lt lu mf nc lx ly mg nd mb mc md ij bi translated">我们已经成功地从零开始构建了一个<strong class="lk ja">逻辑</strong> <strong class="lk ja">回归</strong>模型，而没有使用<strong class="lk ja"> <em class="lj">熊猫</em></strong><strong class="lk ja"><em class="lj">scikit</em></strong><strong class="lk ja"><em class="lj">学习库</em> </strong>。我们已经实现了大约<strong class="lk ja"> 78.5% </strong>的准确率，还可以进一步提高。此外，我们可以保留<strong class="lk ja"> numpy </strong>并构建一个计算点积的函数。虽然我们将使用sklearn，但了解其内部工作原理也是很好的。😉</p><p id="4c89" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">源代码可以在<a class="ae oi" href="https://github.com/PushkaraSharma/medium_articles_code/tree/master/Logistic_Regression_from_scratch" rel="noopener ugc nofollow" target="_blank"> <strong class="lk ja"> GitHub </strong> </a>上找到。请随意改进。</p><p id="917b" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">谢谢你宝贵的时间。😊我希望你喜欢这个教程。</p><p id="6776" class="pw-post-body-paragraph lh li iq lk b ll lm ka ln lo lp kd lq me ls lt lu mf lw lx ly mg ma mb mc md ij bi translated">此外，检查我的教程上的<a class="ae oi" href="https://medium.com/towards-artificial-intelligence/gradient-descent-v-s-normal-equation-for-regression-problems-e6c3cdd705f" rel="noopener">梯度下降v/s正常方程</a></p><div class="oj ok gp gr ol om"><a href="https://medium.com/towards-artificial-intelligence/gradient-descent-v-s-normal-equation-for-regression-problems-e6c3cdd705f" rel="noopener follow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ja gy z fp or fr fs os fu fw iz bi translated">回归问题的梯度下降v/s正规方程</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">选择正确的算法来找到使成本函数最小的参数。</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">medium.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa ky om"/></div></div></a></div><div class="oj ok gp gr ol om"><a href="https://towardsdatascience.com/simple-text-summarizer-using-extractive-method-849b65c2dc5a" rel="noopener follow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ja gy z fp or fr fs os fu fw iz bi translated">使用提取方法的简单文本摘要</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">自动对包含最重要句子的文章进行简短总结。</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="pb l ox oy oz ov pa ky om"/></div></div></a></div></div></div>    
</body>
</html>