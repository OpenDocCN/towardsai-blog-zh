<html>
<head>
<title>A Robustly Optimized BERT Pretraining Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种鲁棒优化的BERT预训练方法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6?source=collection_archive---------0-----------------------#2019-09-11">https://pub.towardsai.net/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6?source=collection_archive---------0-----------------------#2019-09-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="c489" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">用RoBERTa | <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">向AI </a>优化BERT</h2><div class=""/><div class=""><h2 id="2f87" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">伯特是什么？</h2></div><blockquote class="kr ks kt"><p id="891e" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">BERT (Devlin et al .，2018)是一种预先训练语言表示的方法，意味着我们在大型文本语料库(如维基百科)上训练一个通用的“语言理解”模型，然后将该模型用于我们关心的下游NLP任务(如问答)。BERT优于以前的方法，因为它是第一个用于预训练NLP的<em class="it">无监督</em>、<em class="it">深度双向</em>系统。</p></blockquote><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/a74420da152ade34f8119d3161803ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5JoMscgU_QTrmu8-"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">由<a class="ae mh" href="https://unsplash.com/@sarabakhshi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Sara Bakhshi </a>在<a class="ae mh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="e6f9" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mi lf lg lh mj lj lk ll mk ln lo lp lq im bi translated">刘等人研究了BERT的许多关键超参数和训练数据大小的影响。他们发现BERT明显训练不足，可以匹配或超过在它之后发布的每个模型的性能。<code class="fe ml mm mn mo b">RoBERTa</code>(<strong class="kx jd">Ro</strong>busly optimized<strong class="kx jd">BERT</strong>T23】aapproach)推出，性能达到或超过原BERT。</p><h1 id="385f" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">BERT培训目标</h1><p id="8671" class="pw-post-body-paragraph ku kv it kx b ky nh kd la lb ni kg ld mi nj lg lh mj nk lk ll mk nl lo lp lq im bi translated">伯特使用<code class="fe ml mm mn mo b">Masked Language Models (MLM)</code>和<code class="fe ml mm mn mo b">Next Sentence Prediction (NSP)</code>来学习文本表示。<code class="fe ml mm mn mo b">MLM</code>是一种屏蔽一些记号并使用其余记号来预测被屏蔽记号的方法。<code class="fe ml mm mn mo b">NSP</code>是预测一对句子是否连续。如果你想更多地了解伯特，你可以访问这个<a class="ae mh" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">故事</a>。</p><h1 id="9ec6" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">模型设置</h1><p id="6205" class="pw-post-body-paragraph ku kv it kx b ky nh kd la lb ni kg ld mi nj lg lh mj nk lk ll mk nl lo lp lq im bi translated">由于RoBERTa是基于BERT开发的，所以它们共享许多配置。RoBERTa和BERT在以下项目上有所不同:</p><ul class=""><li id="323b" class="nm nn it kx b ky kz lb lc mi no mj np mk nq lq nr ns nt nu bi translated">保留标记:BERT使用<code class="fe ml mm mn mo b">[CLS]</code>和<code class="fe ml mm mn mo b">[SEP]</code>分别作为起始标记和分隔符标记，而RoBERTa使用<code class="fe ml mm mn mo b">&lt;s&gt;</code>和<code class="fe ml mm mn mo b">&lt;/s&gt;</code>转换句子。</li><li id="b3bd" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq nr ns nt nu bi translated">子词的大小:BERT有大约30k的子词，而RoBERTa有大约50k的子词。</li></ul><h1 id="218e" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">修正</h1><p id="a8df" class="pw-post-body-paragraph ku kv it kx b ky nh kd la lb ni kg ld mi nj lg lh mj nk lk ll mk nl lo lp lq im bi translated">通过应用以下调整，RoBERTa的表现优于BERT:</p><ol class=""><li id="cb26" class="nm nn it kx b ky kz lb lc mi no mj np mk nq lq oa ns nt nu bi translated">更大的训练数据(16G对161G)</li><li id="8b36" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq oa ns nt nu bi translated">使用动态屏蔽模式(BERT使用静态屏蔽模式)</li><li id="714a" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq oa ns nt nu bi translated">替换下一句预测训练目标</li><li id="cb5f" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq oa ns nt nu bi translated">较长序列的训练</li></ol><h2 id="a19d" class="ob mq it bd mr oc od dn mv oe of dp mz mi og oh nb mj oi oj nd mk ok ol nf iz bi translated">更大的训练数据</h2><p id="82fd" class="pw-post-body-paragraph ku kv it kx b ky nh kd la lb ni kg ld mi nj lg lh mj nk lk ll mk nl lo lp lq im bi translated">RoBERTa使用BookCorpus (16G)、CC-NEWS (76G)、OpenWebText (38G)和Stories (31G)数据，而BERT仅使用BookCorpus作为训练数据。</p><h2 id="f4bb" class="ob mq it bd mr oc od dn mv oe of dp mz mi og oh nb mj oi oj nd mk ok ol nf iz bi translated">静态屏蔽与动态屏蔽</h2><p id="e884" class="pw-post-body-paragraph ku kv it kx b ky nh kd la lb ni kg ld mi nj lg lh mj nk lk ll mk nl lo lp lq im bi translated">BERT为MLM物镜屏蔽了一次训练数据，而RoBERTa复制了10次训练数据，并以不同方式屏蔽了这些数据。在以下实验中，您会注意到动态屏蔽的性能优于静态屏蔽和参考(BERT)。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/bbba68f73848a6961c1b92f079332a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*9nx2N571QOWe_X53xQSStw.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">掩蔽方法的比较(Lie等人，2019年)</figcaption></figure><h2 id="7fea" class="ob mq it bd mr oc od dn mv oe of dp mz mi og oh nb mj oi oj nd mk ok ol nf iz bi translated">不同的培养目标</h2><p id="aa71" class="pw-post-body-paragraph ku kv it kx b ky nh kd la lb ni kg ld mi nj lg lh mj nk lk ll mk nl lo lp lq im bi translated">李提出了以下评价<code class="fe ml mm mn mo b">NSP</code>目标无用性的方法。没有<code class="fe ml mm mn mo b">NSP</code>培训适用于完整句子和文档句子方法</p><ul class=""><li id="4e8a" class="nm nn it kx b ky kz lb lc mi no mj np mk nq lq nr ns nt nu bi translated">带NSP的段对:一对段，每个段可以包含多个自然句。这与最初的BERT培训目标相同。令牌数量少于512。</li><li id="5423" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq nr ns nt nu bi translated">带有NSP的句子对:一对自然句子，或者从一个文档的连续部分取样，或者从单独的文档取样。它与最初的BERT方法略有不同。令牌数量明显少于512。</li><li id="2057" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq nr ns nt nu bi translated">没有NSP的完整句子:输入内容是从一个或多个文档中抽取的句子。当训练数据到达文档末尾时，将对其他文档中的句子进行采样。令牌的数量最多为512。</li><li id="3ef0" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq nr ns nt nu bi translated">没有NSP的文档句子:与完整句子相同，但数据不跨越文档。</li></ul><p id="39b6" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mi lf lg lh mj lj lk ll mk ln lo lp lq im bi translated">从下面的实验中，我们注意到，不用<code class="fe ml mm mn mo b">NSP</code>训练的方法也能取得更好的结果。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/22a8943e1f134dd74c9dd804f795882e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*vbFF-GFFfZpHGgpCsHlxZw.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">培养目标对比(Lie et al .，2019)</figcaption></figure><h2 id="b256" class="ob mq it bd mr oc od dn mv oe of dp mz mi og oh nb mj oi oj nd mk ok ol nf iz bi translated">较长序列的训练</h2><p id="f123" class="pw-post-body-paragraph ku kv it kx b ky nh kd la lb ni kg ld mi nj lg lh mj nk lk ll mk nl lo lp lq im bi translated">BERT-BASE (Devlin等人，2018年)通过1M步训练，批量大小为256个序列。而Lie等人训练的是125k步，2k序列和31k步，8k序列。下面的实验显示了具有2k个序列的125k个步骤实现了更好的结果。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/b447104cc25472e8212d5a9ed8ca7964.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*hwRP2HBAt_kGgA41zPFq4Q.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">超参数比较(Lie等人，2019年)</figcaption></figure><h1 id="8eb7" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">拿走</h1><ul class=""><li id="b0aa" class="nm nn it kx b ky nh lb ni mi op mj oq mk or lq nr ns nt nu bi translated">RoBERTa仅通过增加数据大小和超参数来进一步优化BERT。</li></ul><h1 id="94ec" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph ku kv it kx b ky nh kd la lb ni kg ld mi nj lg lh mj nk lk ll mk nl lo lp lq im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。在<a class="ae mh" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae mh" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上随时与<a class="ae mh" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>联系。</p><h1 id="be7f" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">延伸阅读</h1><ul class=""><li id="6502" class="nm nn it kx b ky nh lb ni mi op mj oq mk or lq nr ns nt nu bi translated"><a class="ae mh" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">伯特</a>简介</li><li id="a617" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq nr ns nt nu bi translated">脸书的原始实现(PyTorch)</li><li id="96bc" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq nr ns nt nu bi translated"><a class="ae mh" href="https://github.com/huggingface/pytorch-transformers/tree/master/pytorch_transformers" rel="noopener ugc nofollow" target="_blank">抱紧脸</a>实现(PyTorch)</li></ul><h1 id="1470" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">参考</h1><ul class=""><li id="a5d0" class="nm nn it kx b ky nh lb ni mi op mj oq mk or lq nr ns nt nu bi translated">J.Devlin，M. W. Chang，K. Lee和K. Toutanova。<a class="ae mh" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特:语言理解深度双向转换器的预训练</a>。2018</li><li id="8e77" class="nm nn it kx b ky nv lb nw mi nx mj ny mk nz lq nr ns nt nu bi translated">Y.刘、m .奥特、n .戈亚尔、j .杜、m .乔希、d .陈、o .利维、m .刘易斯、L. Zettlemoyer和V. Stoyanov。<a class="ae mh" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank"> RoBERTa:一种稳健优化的BERT预训练方法</a>。2019.</li></ul></div></div>    
</body>
</html>