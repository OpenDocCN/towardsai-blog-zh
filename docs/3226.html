<html>
<head>
<title>GELU : Gaussian Error Linear Unit Code (Python, TF, Torch)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GELU:高斯误差线性单位码(Python，TF，Torch)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/gelu-gaussian-error-linear-unit-code-python-tf-torch-neural-network-bert-de539517edef?source=collection_archive---------4-----------------------#2022-10-17">https://pub.towardsai.net/gelu-gaussian-error-linear-unit-code-python-tf-torch-neural-network-bert-de539517edef?source=collection_archive---------4-----------------------#2022-10-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b27f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">代码教程为GELU，高斯误差线性单位激活函数。包括裸python，Tensorflow和Pytorch代码。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cb893f4ff0e03e84fe106f09f750b747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zn8mxHxatwfAaRCd"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@markuswinkler?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马库斯·温克勒</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="de79" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">GELU激活功能</h1><p id="2c16" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">高斯误差线性单元，GELU，是最先进的模型中使用最多的激活函数，包括BERT、GPT、视觉变压器等..</p><p id="f199" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如果你想了解GELU背后的直觉和数学，我建议你看看我以前写的关于GELU论文的文章<a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/is-gelu-the-relu-successor-deep-learning-activations-7506cf96724f"> (GELU，ReLU的继承者？</a>高斯误差线性单位解释)。GELU背后的动机是将随机正则化子(如漏失)与非线性(即激活函数)联系起来。像伯特和GPT这样巨大的变形金刚模型使得GELU激活功能非常受欢迎。</p><div class="mz na gp gr nb nc"><a rel="noopener  ugc nofollow" target="_blank" href="/is-gelu-the-relu-successor-deep-learning-activations-7506cf96724f"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd iu gy z fp nh fr fs ni fu fw is bi translated">葛鲁，是热鲁接班人吗？</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">高斯误差线性单元GELU在激活函数中结合了一个正则化子。有希望替代……</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">pub.towardsai.net</p></div></div><div class="nl l"><div class="nm l nn no np nl nq ks nc"/></div></div></a></div><h1 id="934d" class="lg lh it bd li lj nr ll lm ln ns lp lq jz nt ka ls kc nu kd lu kf nv kg lw lx bi translated"><strong class="ak">葛鲁数学公式</strong></h1><p id="4636" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">高斯误差线性单元激活函数的输出值是不确定的，而是随机地依赖于输入值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/8a59197f8d576e8c4f069350eaa0444f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ojVXKips5i7aV-pAqbpoAg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">高斯误差线性单位公式</figcaption></figure><p id="a950" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">GELU激活可以用下面的两个公式来近似。第一种近似更精确，而第二种不太精确但更快。我们使用第一个公式来编写一个Python实现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/9c32942151816ff84653da60e41d29e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ec61ADf4W1ph9ftfMbnTYw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">高斯误差线性单位激活函数逼近</figcaption></figure><h1 id="0f91" class="lg lh it bd li lj nr ll lm ln ns lp lq jz nt ka ls kc nu kd lu kf nv kg lw lx bi translated">Python中的GELU</h1><p id="3261" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">为了得到精确的公式，我们需要计算高斯误差函数(erf)。这是最耗时但也是最精确的实现。为了更快地实现，我们使用基于tanh()的近似值，因为它更精确。下面的Python代码涵盖了这两个方面。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">GELU激活函数的Python代码。精确的和近似的实现都包括在内。</figcaption></figure><h1 id="f486" class="lg lh it bd li lj nr ll lm ln ns lp lq jz nt ka ls kc nu kd lu kf nv kg lw lx bi translated">Tensorflow -Keras的GELU</h1><p id="06b3" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">Tensorflow在其tf.keras.activations模块中提供激活功能，您可以将其导入为</p><p id="d632" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><code class="fe oa ob oc od b">from tensorflow.keras.activations import gelu</code></p><p id="822c" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">该函数有一个布尔型<code class="fe oa ob oc od b">approximate </code>参数。如果选择<code class="fe oa ob oc od b">True</code>，那么你将得到上面的近似python实现。否则，您会得到精确但较慢的实现，它实际上是按元素计算<code class="fe oa ob oc od b">x</code>的<a class="ae ky" href="https://en.wikipedia.org/wiki/Error_function" rel="noopener ugc nofollow" target="_blank">高斯误差函数</a> (erf)。</p><p id="08d4" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">下面是一个用GELU激活函数构建Keras神经网络的例子。请注意，您可以通过使用别名“gelu”或直接传递导入的<code class="fe oa ob oc od b">gelu </code>模块，将gelu作为层的激活。拟合过程类似于所有其他的Keras网络。</p><pre class="kj kk kl km gt oe od of og aw oh bi"><span id="9f31" class="oi lh it od b gy oj ok l ol om">input_shape = (28, 28, 1)<br/>num_classes = 10<br/>gelu_act = gelu(approximate = False)<br/>model = keras.Sequential(<br/>    [<br/>        keras.Input(shape=input_shape),<br/>        layers.Conv2D(32, kernel_size=(3, 3), activation="gelu"),<br/>        layers.MaxPooling2D(pool_size=(2, 2)),<br/>        layers.Conv2D(64, kernel_size=(3, 3), activation=gelu),<br/>        layers.MaxPooling2D(pool_size=(2, 2)),<br/>        layers.Flatten(),<br/>        layers.Dropout(0.5),<br/>        layers.Dense(num_classes, activation="softmax"),<br/>    ]<br/>)</span><span id="b88e" class="oi lh it od b gy on ok l ol om">model.summary()</span><span id="bda3" class="oi lh it od b gy on ok l ol om"># Compile the GELU network<br/>model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])</span><span id="6613" class="oi lh it od b gy on ok l ol om">#Fit the GELU network <br/>model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)</span></pre><h1 id="3cc0" class="lg lh it bd li lj nr ll lm ln ns lp lq jz nt ka ls kc nu kd lu kf nv kg lw lx bi translated">火炬中的格鲁</h1><p id="b6a5" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">与Tensorflow类似，PyTorch为GELU提供了近似形式和精确形式。如果您想要更快的实现，或者保留默认值<code class="fe oa ob oc od b">None </code>用于计算高斯误差函数的精确形式，您可以通过导入torch模块并将<code class="fe oa ob oc od b">approximate</code>参数设置为<code class="fe oa ob oc od b">True</code>来访问该函数。在<code class="fe oa ob oc od b"><strong class="ma iu">forward </strong></code>方法中，您可以使用gelu函数并将<code class="fe oa ob oc od b">approximate </code>参数设置为真或假。</p><pre class="kj kk kl km gt oe od of og aw oh bi"><span id="1536" class="oi lh it od b gy oj ok l ol om">import torch.nn as nn <br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>import torchvision</span><span id="a78b" class="oi lh it od b gy on ok l ol om">class Net(nn.Module):<br/>    def __init__(self):<br/>        super(Net, self).__init__()<br/>        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)<br/>        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)<br/>        self.conv2_drop = nn.Dropout2d()<br/>        self.fc1 = nn.Linear(320, 50)<br/>        self.fc2 = nn.Linear(50, 10)</span><span id="322e" class="oi lh it od b gy on ok l ol om">def forward(self, x):<br/>        out = self.conv1(x)<br/>        out = F.max_pool2d(out, 2)<br/>        out = F.gelu(out)        # Using exact GELU formula with erf <br/>        out = self.conv2(x)<br/>        out = F.max_pool2d(out, 2)<br/>        out = self.conv2_drop(out)<br/>        out = F.gelu(out, approximate=True) # Using approximation<br/>        out = F.dropout(out)<br/>        out = self.fc2(x)<br/>        return out</span></pre><h1 id="d937" class="lg lh it bd li lj nr ll lm ln ns lp lq jz nt ka ls kc nu kd lu kf nv kg lw lx bi translated">结论</h1><p id="6409" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">在过去的几年中，GELU激活功能的使用出现了巨大的增长。像伯特、GPT和其他视觉变形金刚(vit)这样的巨大变形金刚模型的出现，承载了更强的正则化的必要性。GELU提供了隐藏在激活函数中的正则化，这就是为什么它被广泛用于像伯特和GPT这样的模型中。</p><p id="5426" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">编写GELU非常容易，主流框架在它们的激活功能模块中默认支持它。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="af2c" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">参考</h1><p id="cfc9" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">[1] <a class="ae ky" href="https://arxiv.org/abs/1606.08415" rel="noopener ugc nofollow" target="_blank">高斯误差线性单位(GELUs) </a></p><p id="782b" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">[2] <a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/is-gelu-the-relu-successor-deep-learning-activations-7506cf96724f">葛鲁，热鲁的继承者？高斯误差线性单位解释</a></p><p id="0953" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">[3] <a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a></p><p id="0312" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">[4]<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/gelu" rel="noopener ugc nofollow" target="_blank">TF . keras . activations . gelu</a></p><p id="6d96" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">[5]<a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.gelu.html" rel="noopener ugc nofollow" target="_blank">torch . nn . functional . gelu</a></p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="0d89" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><em class="oo">如果你学到了有用的东西，请关注我，获取更多深度学习内容和技术教程。使劲鼓掌也让我感觉很棒:)</em></p><p id="1a5b" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如果你愿意支持我，你可以使用我的链接成为Medium的一员。我会得到一半的奖励，不需要你额外付费:)</p><p id="ff9c" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><a class="ae ky" href="https://medium.com/@poulinakis.kon/membership" rel="noopener">https://medium.com/@poulinakis.kon/membershipT21</a></p><p id="7cd0" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><em class="oo">感谢阅读，随意伸手！</em></p><p id="2364" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma iu">我的链接:</strong> <a class="ae ky" href="https://medium.com/@poulinakis.kon" rel="noopener">中</a>|<a class="ae ky" href="https://www.linkedin.com/in/konstantinos-poulinakis-4554821a3/" rel="noopener ugc nofollow" target="_blank"><em class="oo"/>LinkedIn</a>|<a class="ae ky" href="https://github.com/Poulinakis-Konstantinos" rel="noopener ugc nofollow" target="_blank">GitHub</a></p></div></div>    
</body>
</html>