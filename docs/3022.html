<html>
<head>
<title>Learn with me: Linear Algebra for Data Science — Part 4: Singular Value Decomposition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">跟我学:数据科学的线性代数—第4部分:奇异值分解</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/learn-with-me-linear-algebra-for-data-science-part-4-singular-value-decomposition-763c88143560?source=collection_archive---------1-----------------------#2022-08-07">https://pub.towardsai.net/learn-with-me-linear-algebra-for-data-science-part-4-singular-value-decomposition-763c88143560?source=collection_archive---------1-----------------------#2022-08-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="47a1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">欢迎来到线性代数系列的下一部分！我们将讨论奇异值分解及其在数据科学中的巨大作用。这篇文章将比前一篇更具技术性，但这仅仅是因为我们在打好基础之后，将触及一些更高级的主题。别担心，你之前所学的一切都是为这一刻做准备的！</p><p id="07dd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">SVD简介</strong></p><p id="c46c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我知道，这个名字听起来很吓人，让人不知所措。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ks"><img src="../Images/1d7677e53af0ac559298c7db9c24627d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ebZhAMd8ytVsvo0Sqt0auQ.jpeg"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">每当有人提到奇异值分解。米歇尔·特雷瑟默在<a class="ae le" href="https://unsplash.com/s/photos/scared-dog?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="fefc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">但是我向您保证，在本文完成时，您将对SVD及其在数据科学中的应用有很好的理解。让我们从示例矩阵A开始:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi lf"><img src="../Images/98aabaf5a101ad6f3e265cd8311e81b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*rVTNjCGB36f2SQG9KhJY5A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="bfd7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">奇异值分解允许我们把矩阵A表示成其他三个矩阵的乘积。它由下式给出:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/f1cc4ae437cc6737ab21a241b941f0dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/0*HSpOJ_ykBlDidSGI.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="333c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果我们扩展上述内容的表示，它看起来会像这样:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi ll"><img src="../Images/28635cbc6d11fcb940ad902ef1115103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gm4nlPu8Hv25F60mBHucvg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="9a90" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">好吧，这有很多需要理解的。但是我们很快就会发现这一切意味着什么，并对它所代表的东西有一种直觉。在此之前，让我们讨论一些基础知识。</p><p id="8282" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们的输入矩阵A是一个<code class="fe lm ln lo lp b">M x N</code>矩阵。我们可以看到，U呈现出<code class="fe lm ln lo lp b">M x M</code>的形状，σ(适马)呈现出<code class="fe lm ln lo lp b">M x N</code>的形状，V是<code class="fe lm ln lo lp b">N x N</code>的形状。这乍一看可能有点奇怪，但在下一节，我们将发现这些矩阵的形状是如何形成的。现在，只要注意它们。也许要讨论的最重要的事情是U，σ和V是什么类型的矩阵。</p><p id="99e4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">u和V就是所谓的正交矩阵(对于那些读过本系列第1部分的读者来说，你会知道正交意味着什么)。标准正交矩阵就是其列相互正交的矩阵，每个列向量的长度为1。由标准正交矩阵产生的最有趣的性质是它们的转置是它们的逆矩阵。也就是说:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/432223742187ff628f3b2bba1667b649.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/0*-LqFelVQhRK8e3HU.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="d0fb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">虽然这现在看起来并不重要，但这些矩阵以这种方式运行的事实使得SVD如此有用。最后，σ是一个对角矩阵，它包含沿对角轴的“奇异值”,所有其他项为零。作为对角线，σ不受转置操作的影响，这在以后也会派上用场。</p><p id="1e3a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最后一个介绍部分是理解所有这些矩阵都是按照重要性排序的。也就是说σ中的第一个奇异值比第二个重要，第二个比第三个等重要。等等。对于U和V也是如此，其中第一个向量最为重要，因为它们对应于最高的奇异值。</p><p id="1433" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">那么，U，σ，V实际上代表什么呢？</strong></p><p id="922c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果我告诉你，我们的朋友，本征向量和本征值，要回到聚会上，会怎么样？</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ks"><img src="../Images/b52fa7506e9991d4e66642bd23165a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*5iaL9wepdN9ZFHaXnwwIqw.jpeg"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">等待eigenfriends返回。照片由Pexels上的<a class="ae le" href="https://www.pexels.com/@diana-42038108/" rel="noopener ugc nofollow" target="_blank">戴安娜</a>拍摄</figcaption></figure><p id="aff9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这将开始有点抽象，但请相信我！让我们从可以在下面创建的列相关矩阵开始:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/7c58d5c61597ca94784aa136081b296c.png" data-original-src="https://miro.medium.com/v2/resize:fit:116/format:webp/0*-WZAu_fJ9w5pTTPZ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="11b7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在，我们之前已经概述过，我们可以将任何矩阵分解为三个矩阵U、σ和v的乘积，因此，首先让我们用分解后的矩阵替换所有原始矩阵。这将为我们提供以下信息:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/268626ce651b97622ac7c0b5ccf375af.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/0*bFnnOLrg6qrr4huG.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="17dc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">假设矩阵u是正交的，那么U^T U正好抵消了单位矩阵，因此我们剩下下面的:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/21d4354c8cac63b673f6772103ec99b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/0*8qbVxKLq04hb9in8.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成</figcaption></figure><p id="0fa5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">以上实际上是特征分解的形式。其中V包含特征向量，σ包含特征值。那么我在这里得到的更大的观点是什么呢？</p><p id="56e7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">v实际上捕获这个列相关矩阵的特征向量，σ捕获那个相关矩阵的特征值。这是有意义的，因为V在两个维度上都呈现出列数的形状。因此，如果输入矩阵A有10列，V将是一个10乘10的矩阵。</p><p id="768d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="lu">我们已经讲了V和σ，那么U呢？</em></p><p id="2759" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们可以通过与上面完全相同的逻辑来揭示矩阵U包含什么。因为V是列相关矩阵的特征向量，U是行相关矩阵的特征向量，表示为:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/7c4accd866f07a01939fe57a91ed7c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/0*pW6cWsJFXZBMMaLY.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="616d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">同样，V是一个标准正交矩阵，所以它乘以它的转置矩阵取消了单位矩阵，我们只剩下:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/8b50c25bd5ffb6b51437d7f0f8f387f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/0*zO4u4-JtwYP4Fj5p.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="9cc0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中U包含本征向量，σ包含本征值(与我们之前发现的本征值相同，很酷，对吧？).所有这些都是为了让你对SVD中的U、σ和V矩阵有一个直观的认识。总结成一句话，U包含行相关矩阵的特征向量，σ包含特征值，V包含列相关矩阵的特征向量。</p><p id="4ca6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一般来说，你永远不会通过对相关矩阵进行特征分解来计算SVD。相关矩阵可能变得非常大，尤其是U，因此需要大量的计算来导出它们。计算SVD的一种流行方法是QR分解，这是NumPy实现的基础。</p><p id="8991" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">矩阵逼近</strong></p><p id="fd3f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们可以开始考虑奇异值分解的另一种方式是，它将原始矩阵分解成秩为1的矩阵之和(我们花那么多时间来理解矩阵秩是有原因的，查看<a class="ae le" href="https://matthew-macias.medium.com/learn-with-me-linear-algebra-for-data-science-part-2-9a3b75b8b80d" rel="noopener">第2部分</a>了解更多信息)。让我们看看我们最初的例子:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi lw"><img src="../Images/d8001217e3887a5f07b5f20913a6f948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IPls-rZFs9LSCDsl.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="7f70" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果我们将右侧的U、σ和V矩阵相乘，您会看到我们会得到以下结果:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/8bd6828fa3aecb99bf4c32e4bdc890aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/0*DTWDKfcLQJNxQey8.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="609b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">U的第一列向量乘以σ中的第一个奇异值以及v中的第一行向量，您可以对U、σ和v的所有组合继续执行此操作。</p><p id="f0cb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">请记住，U、σ和V中的元素按重要性排序，因此第一个元素比第二个元素更重要，依此类推。等。这允许我们只保留每个矩阵的第一个<em class="lu"> r </em> <strong class="jw ir"> </strong>元素，以创建我们的原始矩阵的秩<em class="lu"> r </em>最佳近似。这导致U变成m x <em class="lu"> r </em>，σ变成<em class="lu"> r </em> x <em class="lu"> r，</em>V变成<em class="lu"> r </em> x <em class="lu"> n </em>。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi ly"><img src="../Images/78cd8bf1e0ce1c692ccbe15f5d7e94a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CFSnFW8hNOdseaoKQPqU2g.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">从r+1开始移除条目后的矩阵。来源:图片由作者生成。</figcaption></figure><p id="2b7e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通过这样做，我们最终得到一个更小的矩阵，它仍然可以相对精确地描述原始矩阵A。您可以开始看到SVD在降维技术中是如何有用的了！</p><p id="59c4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">伪反转</strong></p><p id="0386" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">也许奇异值分解在数据科学中最有用的应用是它对非方阵求逆的能力。在这一点上，我们熟悉线性方程组的以下表示:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/1ac2c8ce663d6b48272b103a06910a6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/format:webp/0*_UVtvsRbdR33hVNJ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="c350" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们知道，为了能够求解这个系统，其中A是一个已知的数据矩阵，b是一个已知的标签向量，我们必须对矩阵A求逆，到目前为止，我们只是假设使用传统方法是可行的。然而，如果矩阵是非正方形的(很可能总是这样)，那么这将不起作用。奇异值分解引入了一种方法，允许我们对这些非方阵求逆并求解线性系统。</p><p id="4d07" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">你可能会注意到这里有一点趋势，但我们还是会用奇异值分解代替矩阵A。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/00502785c9b4942314d3936dd401e964.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/0*vCdGHIgqpbjLwjmK.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="391e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在，我们需要做一些事情来解决<em class="lu"> x </em>的问题。它们依赖于我们对U、σ、<strong class="jw ir">、T5和v的组成的理解。我们可以通过执行以下操作来隔离<em class="lu"> x </em>:</strong></p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/181a214645ec821e4b7774594da4780e.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/0*_xXecuIPUu8quLzA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="951e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因为U和V是正交的，我们可以乘以它们的转置来消除它们。对于σ，我们仍然需要乘以它的传统倒数。你可以看到除了x之外的所有东西都会在左边取消。我们剩下的是:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi mc"><img src="../Images/100eeae56aabab97c0e4c838927adac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/0*8F70Ij-EfMAfBjJs.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="1d0b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">你会注意到我们已经颠倒了SVD的所有组件。这就是众所周知的伪逆，它为所有非方阵求逆提供动力！它更正式地表示为:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi md"><img src="../Images/a73de85f0f231788aa9960f33070c904.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/0*bnn3oTQ-KMI-cmLS.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">来源:图片由作者生成。</figcaption></figure><p id="ae3e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">到目前为止，我们已经介绍了SVD在数据科学中的主要用例，但让我们最终进入精彩部分…</p><p id="0b52" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">我们如何在Python中计算SVD？</strong></p><p id="1dee" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，让我们创建一个随机矩阵进行实验:</p><pre class="kt ku kv kw gt me lp mf mg aw mh bi"><span id="f8c9" class="mi mj iq lp b gy mk ml l mm mn">import numpy as np</span><span id="a9f9" class="mi mj iq lp b gy mo ml l mm mn">A = np.random.rand(80,40)</span></pre><p id="6a0e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这就创建了一个80行40列的矩阵。要使用SVD分解这个矩阵，我们需要执行:</p><pre class="kt ku kv kw gt me lp mf mg aw mh bi"><span id="be79" class="mi mj iq lp b gy mk ml l mm mn">U, S, V = np.linalg.svd(A, full_matrices=True)</span></pre><p id="7570" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">就是这样。任务完成。让我们进一步研究一下，以确保我们之前讨论的理论是正确的。我们确定U基于行相关矩阵，因此应该采用行数x行数的形式。当我们跑步时:</p><pre class="kt ku kv kw gt me lp mf mg aw mh bi"><span id="ad78" class="mi mj iq lp b gy mk ml l mm mn">&gt;&gt;&gt; U.shape<br/>(80,80)</span></pre><p id="fb6f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">您将看到它返回的形状是<code class="fe lm ln lo lp b">(80,80)</code>，这正是我们所期望的。让我们重复σ和v。</p><pre class="kt ku kv kw gt me lp mf mg aw mh bi"><span id="2088" class="mi mj iq lp b gy mk ml l mm mn">&gt;&gt;&gt; S.shape<br/>(40,)</span><span id="50c5" class="mi mj iq lp b gy mo ml l mm mn">&gt;&gt;&gt; V.shape<br/>(40,40)</span></pre><p id="367f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">所以V通过了，但是σ发生了什么？我骗你了吗？不，NumPy返回的奇异值是一个向量。我们可以以更传统的形式重新创建σ，但它有点复杂。</p><pre class="kt ku kv kw gt me lp mf mg aw mh bi"><span id="c78a" class="mi mj iq lp b gy mk ml l mm mn">&gt;&gt;&gt; sigma = np.zeros(A.shape, S.dtype)<br/>&gt;&gt;&gt; sigma.shape<br/>(80,40)</span><span id="f825" class="mi mj iq lp b gy mo ml l mm mn">&gt;&gt;&gt; np.fill_diagonal(sigma, S)</span></pre><p id="6a2b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">实际上，我们创建了一个与输入矩阵形状相同的矩阵，并沿着对角线添加了奇异值。最后，让我们用U，σ和v来重构原始A矩阵。</p><pre class="kt ku kv kw gt me lp mf mg aw mh bi"><span id="9577" class="mi mj iq lp b gy mk ml l mm mn">&gt;&gt;&gt; A_new = np.dot(U[:, :40] * S, V)</span></pre><p id="4f74" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">您会注意到，我们必须减小U的大小来完成重建，但这可以通过在原始SVD函数中设置<code class="fe lm ln lo lp b">full_matrices=False</code>来避免。</p><h2 id="c2df" class="mi mj iq bd mp mq mr dn ms mt mu dp mv kf mw mx my kj mz na nb kn nc nd ne nf bi translated"><strong class="ak">结论</strong></h2><p id="3272" class="pw-post-body-paragraph ju jv iq jw b jx ng jz ka kb nh kd ke kf ni kh ki kj nj kl km kn nk kp kq kr ij bi translated">这是数据科学系列线性代数的另一部分。希望所有的部分都开始合二为一，您可以看到您对线性代数的新理解将在您的数据科学之旅中发挥多么强大的作用。接下来，我们将最终报道PCA(我知道你们都兴奋得发抖)。和往常一样，如果您还没有对数据科学的线性代数有一个扎实的了解，我建议您查看本系列的其他部分(<a class="ae le" href="https://matthew-macias.medium.com/learn-with-me-linear-algebra-for-data-science-part-1-802164c3eac9" rel="noopener">第1部分</a>、<a class="ae le" href="https://matthew-macias.medium.com/learn-with-me-linear-algebra-for-data-science-part-2-9a3b75b8b80d" rel="noopener">第2部分</a>、<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/learn-with-me-linear-algebra-for-data-science-part-3-eigenvectors-8d9277bae0d3">第3部分</a>)。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="9e87" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果你有问题或者只是想保持联系，请随时通过<a class="ae le" href="https://www.linkedin.com/in/matthew-macias-8b2328121/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p></div></div>    
</body>
</html>