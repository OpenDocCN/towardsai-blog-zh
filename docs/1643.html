<html>
<head>
<title>1Cycle Learning Rate Scheduling with TensorFlow and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">1使用TensorFlow和Keras的周期学习率计划</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/1cycle-learning-rate-scheduling-with-tensorflow-and-keras-74ae7ce277c8?source=collection_archive---------1-----------------------#2021-03-09">https://pub.towardsai.net/1cycle-learning-rate-scheduling-with-tensorflow-and-keras-74ae7ce277c8?source=collection_archive---------1-----------------------#2021-03-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4f04" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>工程</h2><div class=""/><div class=""><h2 id="4c97" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">设置深度学习中最重要超参数的实用方法</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/104ed33b635a5edcfb6a96e001c14677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RP9EXNzkB_x5H1tg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@mktgmantra?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">大卫·埃弗雷特·斯特里克勒</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="1698" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">问题陈述</h1><p id="eabf" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">训练深度神经网络可能是一项具有挑战性的任务。要拟合的大量参数会使这些模型特别容易过度拟合。根据模型的复杂性、可用的计算资源和要学习的任务，几天或几周的培训时间可能是常见的。如果可用资源有限，避免额外的计算和长时间的训练是重中之重。我将介绍一个你可以遵循的技巧，以确保你正确地初始化和调整你的学习速度。这将有助于实现出色的任务表现，避免不必要的培训时间。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="5458" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">介绍</h1><p id="ddd9" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated"><strong class="mg ja">学习率</strong>是控制训练期间应用的权重更新幅度的值。有一个好的学习率可能是一个差的和一个优秀的模型之间的区别。</p><p id="c780" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">在下面几节中，我将介绍一些基于梯度优化的背景材料，以便更好地理解这个超参数的重要性。然后，我将介绍一种在训练期间初始化和调整您的神经网络学习速率的方法，随后是实验结果和结论。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="243f" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">背景:成本函数、导数和梯度</h1><h2 id="1496" class="nf ln iq bd lo ng nh dn ls ni nj dp lw mn nk nl ly mr nm nn ma mv no np mc iw bi translated">损失和成本函数</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/092e9d47bc9db62a34268ad44771dad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*_O7A6macicJgfni_WMhqyw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">平方误差损失函数。这是预测值和实际值之间的平方差。</figcaption></figure><p id="83a0" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated"><strong class="mg ja">训练</strong>一个神经网络意味着找到一组权重<strong class="mg ja">来优化</strong>某个函数。实际上，这通常意味着最小化预测值和真实值之间的差异，或<strong class="mg ja"> <em class="nr">误差</em> </strong>。使用<strong class="mg ja">损失函数</strong>计算该误差。损失函数的一个例子是平方误差损失函数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ec8865378bc5e6830fb5906afe5a4f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*8R9PMj2i4zeFJPmD"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">均方误差代价函数的定义。来源:<a class="ae le" href="https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/" rel="noopener ugc nofollow" target="_blank">https://www . freecodecamp . org/news/machine-learning-mean-squared-error-regression-line-c 7d de 9 a 26 b 93/</a></figcaption></figure><p id="0286" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">一个<strong class="mg ja">成本函数</strong>测量整个训练集的预测误差。例如，<a class="ae le" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方差</a>成本函数是我们的预测和真实值之间的均方差，是整个训练集的平均值。</p><h2 id="13e1" class="nf ln iq bd lo ng nh dn ls ni nj dp lw mn nk nl ly mr nm nn ma mv no np mc iw bi translated">导数和梯度</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/0b5ff432664a821f02691d480bd0421f.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*mzSOB_pkgW7cJduN.gif"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">函数滑动导数的可视化。来源:<a class="ae le" href="http://www.calcblog.com/calculating-derivatives-on-ti83-ti84-ti89-ti92-voyage-200/" rel="noopener ugc nofollow" target="_blank">http://www . calc blog . com/calculating-derivatives-on-ti83-ti84-ti89-ti92-voyage-200/</a></figcaption></figure><p id="1f4c" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">一元函数<em class="nr"> f </em>的<a class="ae le" href="https://en.wikipedia.org/wiki/Derivative" rel="noopener ugc nofollow" target="_blank">导数</a><em class="nr">f</em>’告诉我们在给定点<em class="nr"> x </em>的<strong class="mg ja">瞬时变化率</strong>；它提供了测量<em class="nr"> f </em>增加或减少多少的方法。</p><p id="ac1d" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">一个<a class="ae le" href="https://en.wikipedia.org/wiki/Gradient" rel="noopener ugc nofollow" target="_blank">梯度</a>是导数对<a class="ae le" href="https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/ways-to-represent-multivariable-functions/a/multivariable-functions" rel="noopener ugc nofollow" target="_blank">多元函数</a>的推广；它是函数的<strong class="mg ja">偏导数</strong>的向量。一个函数的<a class="ae le" href="https://en.wikipedia.org/wiki/Partial_derivative" rel="noopener ugc nofollow" target="_blank">偏导数</a>是相对于其变量中的<strong class="mg ja"> <em class="nr">只有一个</em> </strong>的导数(瞬时变化率)，而将其他视为常数。函数的<strong class="mg ja">梯度</strong>是所有函数偏导数的<strong class="mg ja">向量</strong>。标量函数的梯度表示最大变化的<strong class="mg ja">方向。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/87c542de781701ff709b487ee7746851.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*czqiZO-x4R767WihJpuuKQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">均方误差代价函数的梯度。它是所有偏导数的向量。</figcaption></figure><p id="fa08" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank"> <strong class="mg ja">梯度下降</strong> </a>是一种强大的通用优化算法。它通过<strong class="mg ja">迭代</strong>调整成本函数最大变化方向(梯度)的模型参数来工作。因为我们对<strong class="mg ja">最小化</strong>成本函数感兴趣，我们在梯度的<strong class="mg ja">相反方向</strong>应用更新(换句话说，负梯度)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/95f1e4ab5ca4f9f3ed13e1cbc5156759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*3KmLdc6sE-ijWeu3UvSk-w.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">梯度下降执行的更新步骤。模型参数在负梯度的方向上更新。</figcaption></figure><blockquote class="nv nw nx"><p id="18a8" class="me mf nr mg b mh na ka mj mk nb kd mm ny nc mp mq nz nd mt mu oa ne mx my mz ij bi translated">注意:因为它只使用梯度(一阶导数)信息，梯度下降被称为一阶优化方法。其他方法使用<a class="ae le" href="https://en.wikipedia.org/wiki/Hessian_matrix" rel="noopener ugc nofollow" target="_blank"> Hessian </a>(二阶导数)信息，可能收敛得更快(例如，<a class="ae le" href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" rel="noopener ugc nofollow" target="_blank">牛顿法</a>)。然而，计算黑森是极其昂贵的。这使得它们无法用于大型模型，如深度神经网络。</p></blockquote><h2 id="4814" class="nf ln iq bd lo ng nh dn ls ni nj dp lw mn nk nl ly mr nm nn ma mv no np mc iw bi translated">学习速度效应</h2><p id="847c" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">梯度下降将<a class="ae le" href="https://en.wikipedia.org/wiki/Learning_rate" rel="noopener ugc nofollow" target="_blank">学习率</a>作为比例项应用于负梯度。它控制权重更新的幅度。下图显示了使用不同学习率在CIFAR10数据集上训练的模型的训练损失。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4b11341346f572d0814fa292e0407916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*z6bumOcmKG8xWKJsmktI2w.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">学习率直接影响训练期间模型的收敛(或发散)。良好的学习速度对于在合理的时间内找到好的解决方案至关重要。</figcaption></figure><p id="4bd3" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">使用非常大的学习率会导致很大的损失值。虽然这种损失会减少，但仍会明显高于使用更小、更充分的学习速率所获得的损失。在更极端的情况下，大的学习率会导致发散，到处乱射，永远不会稳定下来。相反，如果学习率太小，训练可能会非常慢。</p><blockquote class="nv nw nx"><p id="c705" class="me mf nr mg b mh na ka mj mk nb kd mm ny nc mp mq nz nd mt mu oa ne mx my mz ij bi translated">在深度学习文献中，学习率通常用希腊字母<em class="iq"> η ( </em> eta)表示。</p></blockquote></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="1087" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">学习率初始化和调度</h1><p id="a9bb" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">正如我们在上一节中所看到的，学习率的选择会极大地影响解决方案的质量。在下面的章节中，我将介绍一个简单有效的<strong class="mg ja">学习率初始化</strong>技术。然后，我将介绍一个<strong class="mg ja">学习率进度表</strong>，用于在训练期间动态修改学习率，并实现更快的收敛。</p><blockquote class="nv nw nx"><p id="6667" class="me mf nr mg b mh na ka mj mk nb kd mm ny nc mp mq nz nd mt mu oa ne mx my mz ij bi translated">注意:使用的代码改编自第11章“使用Scikit-Learn、Keras和TensorFlow进行机器学习”。在这里可以找到原来的笔记本<a class="ae le" href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb#scrollTo=txzRS4NNtU-u" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><h2 id="2e08" class="nf ln iq bd lo ng nh dn ls ni nj dp lw mn nk nl ly mr nm nn ma mv no np mc iw bi translated">指数增量</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/13c1a8d5ecc347934ee3bcded4ad2b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*7rztlh8wwOR33ztErYLDTA.png"/></div></figure><p id="18c5" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">第一种技术是通过数百次迭代来训练您的模型，从非常小的学习速率(例如，1e-6)开始，逐渐以指数方式递增，直到一个大值(例如，10)。最佳学习率将比损失开始激增之前的点低大约10倍。指数增长技术的Keras实现可以在下面找到。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">通过扩展回调类实现指数学习率技术。</figcaption></figure><p id="2dfa" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">使用上述方法找到的学习速率将被用作更有效技术的阈值，该技术用于在训练期间动态修改学习速率。</p><h2 id="ab18" class="nf ln iq bd lo ng nh dn ls ni nj dp lw mn nk nl ly mr nm nn ma mv no np mc iw bi translated">1周期计划</h2><p id="6053" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">我们可以在训练期间动态修改学习率，而不是使用恒定的学习率。如果使用得当，这种技术会带来好的解决方案和更快的收敛。<strong class="mg ja">时间表</strong>是用于修改学习率的策略。2018年，Leslie Smith提出了<strong class="mg ja">1周期时间表</strong>，这是一个简单有效的时间表，学习率在训练的前半段增加，后半段减少。1周期计划的工作方式如下:</p><ol class=""><li id="da6e" class="of og iq mg b mh na mk nb mn oh mr oi mv oj mz ok ol om on bi translated">将<em class="nr"> η </em>初始化为某个初始值<em class="nr"> η </em> 0</li><li id="ced9" class="of og iq mg b mh oo mk op mn oq mr or mv os mz ok ol om on bi translated">线性增加<em class="nr"> η，</em>在训练中途达到最大值<em class="nr"> η </em> 1</li><li id="3d06" class="of og iq mg b mh oo mk op mn oq mr or mv os mz ok ol om on bi translated">线性降低<em class="nr"> η，</em>下降到<em class="nr"> η </em> 0。最后一次迭代应该将<em class="nr"> η </em>降低几个数量级</li></ol><p id="79a3" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">最大值<em class="nr"> η </em> 1 <em class="nr"> </em>可以通过上述的指数增加法选择。初始学习率<em class="nr"> η </em> 0可以设定为比<em class="nr"> η </em> 1 <em class="nr">低大约10倍。</em></p><p id="c6f8" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">下面是1周期调度的Keras实现:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="od oe l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">1周期调度的Keras实现。</figcaption></figure><h1 id="c234" class="lm ln iq bd lo lp ot lr ls lt ou lv lw kf ov kg ly ki ow kj ma kl ox km mc md bi translated">实验结果</h1><p id="9168" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">下一节介绍我的实验结果。对于CIFAR10和时尚MNIST上的图像分类任务，我用以下架构训练了一个卷积神经网络:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/0164a25eb919679b91c4e9db5f9e108f.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*a9RE0LVhcpZyoPZA842geg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">CNN用于图像分类任务。</figcaption></figure><p id="3615" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">对于波士顿住房数据集，我使用以下体系结构训练了一个模型:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/fa0f2a1297c68bd65c3f664e2d25b955.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*Me-Zzx_5BwcXzuO5Z3naGg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">用于波士顿住房回归任务的全连接模型。</figcaption></figure><p id="bb00" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">所有任务都使用32的批处理大小。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/f766d42b10266640a529f815d1f7682b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pfhjurGG-G7Xs6ipDR5PGQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">时尚MNIST结果。到第9世，与最佳基线验证精度相比，1周期模型已经实现了更好的验证精度。这意味着训练周期减少了66%。</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/4c3d724fded7cdc61af1e55a67e6a770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Xk5TTaTDnZuum8y22BQ6Q.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">与基线模型73.4%的验证准确性相比，1周期变量实现了81.2%的验证准确性。这相当于<strong class="bd lo">精度提高了10.6% </strong>。</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/bffbbc045ad74c8231986ba62d46ab10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*evkqUuWG6CEp6f8AVc10cQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">1与基线模型的17.1相比，cycle的RMSE达到了6.27。这相当于RMSE提高了<strong class="bd lo"> 63.2% </strong>。</figcaption></figure></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="e086" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">结论</h1><p id="66fc" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">在本文中，我介绍了<strong class="mg ja">1周期学习率时间表</strong>。<strong class="mg ja"> </strong>当适当调整时，这种技术产生了显著提高的任务性能，同时收敛更快。但是，需要进行一些调整。特别是，我发现批量大小、初始和最大学习速率需要仔细选择。然而，一旦找到合理的值，使用1周期调度的好处就显而易见了。</p><blockquote class="nv nw nx"><p id="875b" class="me mf nr mg b mh na ka mj mk nb kd mm ny nc mp mq nz nd mt mu oa ne mx my mz ij bi translated"><strong class="mg ja">重要提示:</strong>批量大小在这些实验中起到了<strong class="mg ja">至关重要的</strong>作用。32的批量产生了相当好的结果，代价是训练时间稍短。然而，使用大批量(如2048)会导致重量更新和<code class="fe pc pd pe pf b">Nan</code>值激增。</p></blockquote><p id="de69" class="pw-post-body-paragraph me mf iq mg b mh na ka mj mk nb kd mm mn nc mp mq mr nd mt mu mv ne mx my mz ij bi translated">最后，不要忘记缩放您的数据。一种常见且有效的缩放方法是<a class="ae le" href="https://en.wikipedia.org/wiki/Standard_score" rel="noopener ugc nofollow" target="_blank">将</a>数据标准化为零均值和单位方差。对于在神经网络中保持方差的重要性的更深入的解释，<a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/solving-the-vanishing-gradient-problem-with-self-normalizing-neural-networks-using-keras-59a1398b779f">查看这篇文章</a>。</p><h1 id="b4da" class="lm ln iq bd lo lp ot lr ls lt ou lv lw kf ov kg ly ki ow kj ma kl ox km mc md bi translated">来源</h1><ul class=""><li id="dea9" class="of og iq mg b mh mi mk ml mn pg mr ph mv pi mz pj ol om on bi translated"><a class="ae le" href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">https://www . jmlr . org/papers/volume 13/bergstra 12a/bergstra 12a . pdf</a></li><li id="5042" class="of og iq mg b mh oo mk op mn oq mr or mv os mz pj ol om on bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Bayesian_optimization" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Bayesian_optimization</a></li><li id="066c" class="of og iq mg b mh oo mk op mn oq mr or mv os mz pj ol om on bi translated"><a class="ae le" href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2019/05/efficient net-improving-accuracy-and . html</a></li><li id="4c11" class="of og iq mg b mh oo mk op mn oq mr or mv os mz pj ol om on bi translated"><a class="ae le" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn、Keras和TensorFlow Ch1进行机器实践学习</a> 1</li><li id="8afb" class="of og iq mg b mh oo mk op mn oq mr or mv os mz pj ol om on bi translated"><a class="ae le" href="https://arxiv.org/pdf/1803.09820.pdf" rel="noopener ugc nofollow" target="_blank">神经网络超参数的训练方法:第1部分——学习速率、批量大小、动量和权重衰减</a></li><li id="8c60" class="of og iq mg b mh oo mk op mn oq mr or mv os mz pj ol om on bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Standard_score" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Standard_score</a></li><li id="bd6d" class="of og iq mg b mh oo mk op mn oq mr or mv os mz pj ol om on bi translated"><a class="ae le" rel="noopener ugc nofollow" target="_blank" href="/solving-the-vanishing-gradient-problem-with-self-normalizing-neural-networks-using-keras-59a1398b779f">https://pub . toward sai . net/solving-the-vanishing-gradient-problem-with-self-normalizing-neural-networks-using-keras-59a 1398 b779 f</a></li></ul></div></div>    
</body>
</html>