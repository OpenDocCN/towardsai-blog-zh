<html>
<head>
<title>ONNX for Model Interoperability &amp; Faster Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ONNX支持模型互操作性和更快的推理</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/onnx-for-model-interoperability-faster-inference-8709375db9bf?source=collection_archive---------2-----------------------#2021-02-10">https://pub.towardsai.net/onnx-for-model-interoperability-faster-inference-8709375db9bf?source=collection_archive---------2-----------------------#2021-02-10</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="4760" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="59b8" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">了解如何使用ONNX将ML模型从任何框架转换为ONNX格式，并做出更快的推断</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/e7a5637a44f57e2acde7305c60078b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F7bYkmBtbZwyWAB1g6E4JA.png"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">来源:<a class="ae li" href="https://github.com/onnx/onnx/blob/master/docs/ONNX_logo_main.png" rel="noopener ugc nofollow" target="_blank"> ONNX </a></figcaption></figure><p id="3010" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">您很有可能听说过ONNX，但不确定它的功能和使用方法。别担心，你来对地方了。在这篇初学者友好的文章中，您将了解ONNX。让我们开始吧。</p><p id="7456" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">假设你建立了一个深度学习模型，使用TensorFlow框架进行人脸检测。但是不幸的是，您可能必须在使用Pytorch的环境中部署这个模型。你如何处理这种情况？我们可以想出两种方法来解决这个问题:</p><p id="d6f4" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="mf"> a)将现有模型转换成可在任何环境下工作的标准格式。</em></p><p id="2344" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="mf"> b)将模型从一个框架(TensorFlow)转换到另一个所需的框架(Pytorch)。</em></p><p id="9675" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这正是ONNX所做的。使用ONNX平台，您可以将TensorFlow模型转换为ONNX(一种开放的互操作性标准格式)。然后用它来进行推断/预测。或者，您也可以进一步将ONNX转换为Pytorch，并使用Pytorch模型进行推断/预测。</p><p id="1612" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这要归功于微软、脸书和AWS等合作伙伴的共同努力。现在，我们可以无缝地工作，而不用担心构建机器学习模型的底层框架。</p></div><div class="ab cl mg mh hy mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="in io ip iq ir"><h1 id="7245" class="mn mo iu bd mp mq mr ms mt mu mv mw mx kj my kk mz km na kn nb kp nc kq nd ne bi translated">ONNX</h1><p id="76c6" class="pw-post-body-paragraph lj lk iu ll b lm nf ke lo lp ng kh lr ls nh lu lv lw ni ly lz ma nj mc md me in bi translated"><strong class="ll je"> ONNX </strong>代表<strong class="ll je">O</strong>pen<strong class="ll je">N</strong>eural<strong class="ll je">N</strong>network E<strong class="ll je">x</strong>change。它主要可以用于三种不同的任务</p><ul class=""><li id="b5dd" class="nk nl iu ll b lm ln lp lq ls nm lw nn ma no me np nq nr ns bi translated">将模型从任何框架转换为ONNX格式</li><li id="5787" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me np nq nr ns bi translated">将ONNX格式转换成任何需要的框架</li><li id="1ed9" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me np nq nr ns bi translated">在支持的运行时上使用ONNX模型进行更快的推理</li></ul><p id="6322" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">虽然它在定义中说神经网络可以用于深度学习和传统的机器学习模型。所以，不要迷茫。</p><p id="918b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">正如您在下面看到的，ONNX支持大多数框架。所以，如果你在找Pytorch，MXNET，MATLAB，XGBoost，CatBoost模型进行ONNX转换参考<a class="ae li" href="https://github.com/onnx/tutorials#converting-to-onnx-format" rel="noopener ugc nofollow" target="_blank"> <strong class="ll je">官方教程</strong> </a>。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ny"><img src="../Images/de866ef289d44d4b84dfa982498a6e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aoHvjsEBAmFqYvh0bTLP2A.png"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">来源:<a class="ae li" href="https://onnx.ai/supported-tools.html" rel="noopener ugc nofollow" target="_blank">https://onnx.ai/supported-tools.html</a></figcaption></figure></div><div class="ab cl mg mh hy mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="in io ip iq ir"><h1 id="870f" class="mn mo iu bd mp mq mr ms mt mu mv mw mx kj my kk mz km na kn nb kp nc kq nd ne bi translated">ONNX运行时</h1><p id="ba27" class="pw-post-body-paragraph lj lk iu ll b lm nf ke lo lp ng kh lr ls nh lu lv lw ni ly lz ma nj mc md me in bi translated">下图显示了ONNX支持的所有运行时列表。这些运行时引擎有助于高性能深度学习推理，因为它们带有内置优化。<a class="ae li" href="https://www.onnxruntime.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="ll je"> ONNX运行时</strong> </a>也是我们将在本文中用于推理的受支持的运行时引擎之一。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj nz"><img src="../Images/d83e5af4d8d8d136892baf6bd05b6c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5YfmLXReDqWkQsOZpmNlpQ.png"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">来源:https://onnx.ai/supported-tools.html<a class="ae li" href="https://onnx.ai/supported-tools.html" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure></div><div class="ab cl mg mh hy mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="in io ip iq ir"><p id="3f90" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在你已经对<strong class="ll je"> ONNX </strong>和<strong class="ll je"> ONNX运行时</strong>有了大致的了解，让我们跳转到本文的主要话题。在本文中，我们将以<strong class="ll je"> Scikit-learn </strong>和<strong class="ll je"> TensorFlow </strong>模型为例，将它们转换为ONNX格式，并使用<strong class="ll je"> ONNX运行时</strong>进行推理。</p><p id="58da" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在以后的文章中，我们将探讨如何将ONNX转换成任何期望的框架、优化和转换器模型等。</p><h1 id="546d" class="mn mo iu bd mp mq oa ms mt mu ob mw mx kj oc kk mz km od kn nb kp oe kq nd ne bi translated">sci kit-学习ONNX</h1><h2 id="7700" class="of mo iu bd mp og oh dn mt oi oj dp mx ls ok ol mz lw om on nb ma oo op nd ja bi translated"><strong class="ak">安装</strong></h2><pre class="kt ku kv kw gu oq or os ot aw ou bi"><span id="c289" class="of mo iu or b gz ov ow l ox oy">pip install skl2onnx, onnxruntime</span></pre><h2 id="e287" class="of mo iu bd mp og oh dn mt oi oj dp mx ls ok ol mz lw om on nb ma oo op nd ja bi translated">密码</h2><p id="dd90" class="pw-post-body-paragraph lj lk iu ll b lm nf ke lo lp ng kh lr ls nh lu lv lw ni ly lz ma nj mc md me in bi translated">下面的代码简单明了。我们首先构建Sklearn模型(步骤1)，将其转换为ONNX格式(步骤2)，最后使用ONNX进行预测/推理(步骤3)。如您所见，使用ONNX格式的推理比最初的Scikit-learn模型快6-7倍。如果您处理更大的数据集，结果会令人印象深刻。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="oz pa l"/></div></figure><p id="0a38" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">有关<strong class="ll je"> skl2onnx </strong>的更多详情，请参考<a class="ae li" href="http://onnx.ai/sklearn-onnx/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ll je">本</strong> </a> <strong class="ll je"> </strong>文档。</p><h1 id="ebc2" class="mn mo iu bd mp mq oa ms mt mu ob mw mx kj oc kk mz km od kn nb kp oe kq nd ne bi translated">张量流至ONNX</h1><h2 id="e28b" class="of mo iu bd mp og oh dn mt oi oj dp mx ls ok ol mz lw om on nb ma oo op nd ja bi translated"><strong class="ak">安装</strong></h2><pre class="kt ku kv kw gu oq or os ot aw ou bi"><span id="a7cf" class="of mo iu or b gz ov ow l ox oy">pip install tf2onnx, onnxruntime</span></pre><h2 id="09bd" class="of mo iu bd mp og oh dn mt oi oj dp mx ls ok ol mz lw om on nb ma oo op nd ja bi translated">密码</h2><p id="b66a" class="pw-post-body-paragraph lj lk iu ll b lm nf ke lo lp ng kh lr ls nh lu lv lw ni ly lz ma nj mc md me in bi translated">以下代码也是不言自明的。即使在这种情况下，使用ONNX的推断/预测也比原始TensorFlow模型快6-7倍。如前所述，如果您处理更大的数据集，结果会令人印象深刻。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="oz pa l"/></div></figure><p id="ad84" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">有关<strong class="ll je"> tf2onnx </strong>的更多详情，请参考<a class="ae li" href="https://github.com/onnx/tensorflow-onnx" rel="noopener ugc nofollow" target="_blank"> <strong class="ll je">本</strong> </a> <strong class="ll je"> </strong>文档。</p></div><div class="ab cl mg mh hy mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="in io ip iq ir"><h1 id="e7f1" class="mn mo iu bd mp mq mr ms mt mu mv mw mx kj my kk mz km na kn nb kp nc kq nd ne bi translated">其他工具</h1><p id="fc7d" class="pw-post-body-paragraph lj lk iu ll b lm nf ke lo lp ng kh lr ls nh lu lv lw ni ly lz ma nj mc md me in bi translated">ONNX还为<strong class="ll je">优化</strong>和<strong class="ll je">可视化</strong>提供了另外两个工具。<em class="mf">优化将在以后的文章中讨论。</em>有两个可视化工具<a class="ae li" href="https://netron.app/" rel="noopener ugc nofollow" target="_blank"> <strong class="ll je"> NETRON </strong> </a>或<a class="ae li" href="https://www.paddlepaddle.org.cn/paddle/visualdl" rel="noopener ugc nofollow" target="_blank"> <strong class="ll je"> Visual DL </strong> </a>可以用来可视化任何机器学习或深度学习模型。</p><p id="e26b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">下面的截图显示了我们之前使用<a class="ae li" href="https://netron.app/" rel="noopener ugc nofollow" target="_blank"> <strong class="ll je"> NETRON </strong> </a>创建的TensorFlow深度学习模型的可视化。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pb"><img src="../Images/d025fe96b369fdc7ff7a23a22afe6d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQS800ZYSo_gt9jDRxnKGg.png"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">作者图片</figcaption></figure></div><div class="ab cl mg mh hy mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="in io ip iq ir"><h1 id="54a9" class="mn mo iu bd mp mq mr ms mt mu mv mw mx kj my kk mz km na kn nb kp nc kq nd ne bi translated">结论</h1><p id="7428" class="pw-post-body-paragraph lj lk iu ll b lm nf ke lo lp ng kh lr ls nh lu lv lw ni ly lz ma nj mc md me in bi translated">在本文中，您了解了什么是ONNX，以及它将如何给开发人员带来好处。然后我们研究了ONNX转换的例子，发现使用ONNX运行时的推理比原始框架快得多。最后，我们使用NETRON visualizer可视化了机器学习模型。</p><p id="84e7" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="mf">原载于2021年2月3日Pythonsimplified.com</em><em class="mf">的</em> <strong class="ll je"> <em class="mf">。</em></strong></p><p id="d0e9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="mf">阅读更多关于Python和数据科学的有趣文章，</em> <a class="ae li" href="https://pythonsimplified.com/home/" rel="noopener ugc nofollow" target="_blank"> <strong class="ll je"> <em class="mf">订阅</em> </strong> </a> <em class="mf">到我的博客</em><a class="ae li" href="http://www.pythonsimplified.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ll je"><em class="mf">pythonsimplified.com</em></strong></a><strong class="ll je"><em class="mf">。</em> </strong>你也可以在<a class="ae li" href="https://www.linkedin.com/in/chetanambi/" rel="noopener ugc nofollow" target="_blank"><strong class="ll je">LinkedIn</strong></a><strong class="ll je">上联系我。</strong></p><p id="c6d3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我希望你喜欢阅读这篇文章。如果你喜欢我的文章并想订阅Medium，你可以在这里这样做: </p><div class="pc pd gq gs pe pf"><a href="https://chetanambi.medium.com" rel="noopener follow" target="_blank"><div class="pg ab fp"><div class="ph ab pi cl cj pj"><h2 class="bd je gz z fq pk fs ft pl fv fx jd bi translated">Chetan Ambi -介质</h2><div class="pm l"><h3 class="bd b gz z fq pk fs ft pl fv fx dk translated">阅读Chetan Ambi在媒体上的文章。数据科学|机器学习| Python。参观https://pythonsimplified.com/…</h3></div><div class="pn l"><p class="bd b dl z fq pk fs ft pl fv fx dk translated">chetanambi.medium.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt lc pf"/></div></div></a></div></div><div class="ab cl mg mh hy mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="in io ip iq ir"><h1 id="9c79" class="mn mo iu bd mp mq mr ms mt mu mv mw mx kj my kk mz km na kn nb kp nc kq nd ne bi translated">参考</h1><p id="f9d3" class="pw-post-body-paragraph lj lk iu ll b lm nf ke lo lp ng kh lr ls nh lu lv lw ni ly lz ma nj mc md me in bi translated">[1].<a class="ae li" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank">https://onnx.ai/</a></p><p id="8eed" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[2].<a class="ae li" href="https://github.com/onnx" rel="noopener ugc nofollow" target="_blank">https://github.com/onnx</a></p><p id="6b49" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[3].<a class="ae li" href="https://github.com/microsoft/onnxruntime" rel="noopener ugc nofollow" target="_blank">https://github.com/microsoft/onnxruntime</a></p><p id="c5a0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[4].<a class="ae li" href="https://github.com/lutzroeder/netron" rel="noopener ugc nofollow" target="_blank">https://github.com/lutzroeder/netron</a></p></div></div>    
</body>
</html>