<html>
<head>
<title>Hey… Your AI is Hurting Me!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">嘿…你的人工智能弄疼我了！！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/hey-your-ai-is-hurting-me-ae12c2ab688a?source=collection_archive---------1-----------------------#2022-12-15">https://pub.towardsai.net/hey-your-ai-is-hurting-me-ae12c2ab688a?source=collection_archive---------1-----------------------#2022-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq"><p id="2e4c" class="jr js it bd jt ju jv jw jx jy jz ka dk translated">不成熟的人工智能不是允许系统性偏差的理由。</p></blockquote><p id="da28" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ka im bi translated">随着人工智能(AI)的使用不断增长和扩展，对AI系统中偏见的担忧日益突出。人工智能中的偏见可能会产生严重的后果，包括歧视某些人群，使现有的社会不平等永久化，以及做出不公平甚至有害的决定。</p><p id="db54" class="pw-post-body-paragraph kb kc it kd b ke ky kg kh ki kz kk kl km la ko kp kq lb ks kt ku lc kw kx ka im bi translated">让我们来看看一些急于实现人工智能却不尽如人意的突出例子</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/7c297e829b1f053bb23118a775861d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UfITZ4E5hHEgSxu4"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk translated">艾米·埃尔廷在<a class="ae lt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="914b" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">肾病预测AI吸女人:<a class="ae lt" href="https://www.fiercebiotech.com/medtech/troubling-study-finds-googles-kidney-disease-predicting-ai-performs-worse-women-and-may-not" rel="noopener ugc nofollow" target="_blank"> DeepMind health </a></h2><ul class=""><li id="44a7" class="mn mo it kd b ke mp ki mq km mr kq ms ku mt ka mu mv mw mx bi translated">退伍军人管理局的研究人员DeepMind的研究人员承认，训练数据包括6.38%的女性和所有93.62%的男性。据退伍军人事务部的研究人员称，“这一人群的模型性能较低”，他们当时写道，尽管他们的发现仅限于急性肾衰竭早期患者。该模型在男性中的AUC(准确性的度量)为83%,而在女性中为71%。这种类型的偏见可能导致获得医疗保健的不平等，并可能导致个人获得不充分或不适当的护理。现在想象一下，如果这种疾病在不同种族间有不同的生物学特征。这些有代表性吗？</li></ul><h2 id="8af4" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">AI误打误撞残疾人:<a class="ae lt" href="https://www.microsoft.com/en-us/research/uploads/prod/2019/07/Research_Roadmap_ASSETS_2019_Workshop_final.pdf" rel="noopener ugc nofollow" target="_blank">微软研究院</a></h2><ul class=""><li id="0fed" class="mn mo it kd b ke mp ki mq km mr kq ms ku mt ka mu mv mw mx bi translated">此外，这篇<a class="ae lt" href="https://makeitfable.com/article/ai-and-analytics-people-with-disabilities/" rel="noopener ugc nofollow" target="_blank">文章</a>表明，像搜索结果、广告或地图这样的人工智能系统经常显示不相关或对某些人群有偏见的结果，例如残疾人或生活在某些地理区域的人。这个问题的一个延伸是，残疾人不能有效地使用语音识别系统。这种偏见会导致人们在真正需要帮助的时候得不到帮助。即使是像这样的消极偏见也可能导致获取信息和机会方面的现有不平等。</li></ul><h2 id="e564" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">同学们！我们错误地判断了你的潜力:<a class="ae lt" href="https://arxiv.org/abs/2208.10625" rel="noopener ugc nofollow" target="_blank">科内尔研究</a></h2><ul class=""><li id="057b" class="mn mo it kd b ke mp ki mq km mr kq ms ku mt ka mu mv mw mx bi translated">这种人工智能算法可以预测学生在某个学术项目或领域取得成功的可能性。它从未被评估过在得不到充分服务的人群中的公平性。本文评估了不同的群体公平措施的学生成绩预测问题的各种教育数据集和公平意识的学习模型。研究发现，选择公平的措施是重要的，同样的选择等级门槛。这种系统中使用的人工智能歧视某些种族或社会经济背景的学生。这种偏见可能导致学生被置于不适当或不平等的学习环境中，并可能加剧现有的教育不平等。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi my"><img src="../Images/52ac2cecf29f3505f935624366622e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BQaRuvRaW4ffFG-L"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk translated">照片由<a class="ae lt" href="https://unsplash.com/@wacalke?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马特乌斯·瓦卡韦克</a>在<a class="ae lt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><h2 id="8942" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">脸书说打木偶:<a class="ae lt" href="https://www.washingtonpost.com/technology/2021/11/21/facebook-algorithm-biased-race/" rel="noopener ugc nofollow" target="_blank">华盛顿邮报报道</a></h2><ul class=""><li id="4e46" class="mn mo it kd b ke mp ki mq km mr kq ms ku mt ka mu mv mw mx bi translated">用于对用户新闻内容进行优先排序的人工智能算法可以放大有争议或耸人听闻的内容，使用户更有可能遇到和接触这种类型的材料。此外，人工智能聊天机器人和社交媒体账户可用于自动生成和传播分裂或煽动性内容，可能会引发用户的强烈反应。此外，旨在生成个性化内容或推荐的人工智能算法有时可能会将用户带入“回音室”，在那里他们只会遇到与自己的信念相符的信息和观点，这可能会进一步加剧分裂或煽动性的互动。所有这些都是保持“用户参与”的机制，因此数据可以以牺牲用户的精神健康、整个种族或整个少数群体为代价进行出售。</li></ul><h2 id="c207" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">黑人..你们都一样吧？(2019): <a class="ae lt" href="https://www.washingtonpost.com/technology/2019/12/19/federal-study-confirms-racial-bias-many-facial-recognition-systems-casts-doubt-their-expanding-use/" rel="noopener ugc nofollow" target="_blank">哈佛研究</a></h2><ul class=""><li id="3288" class="mn mo it kd b ke mp ki mq km mr kq ms ku mt ka mu mv mw mx bi translated">这项研究发现，亚裔和非裔美国人被误判的可能性是白人的100倍。美洲原住民的假阳性率最高。在警方调查人员使用的各种搜索中，非裔美国女性更容易被错误地识别出来。这些是联邦调查局使用的相同系统(2011年至2019年的390，000次搜索)。根据哈佛大学的一项研究，商业面部识别系统更有可能错误识别肤色较深的人<a class="ae lt" href="https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/" rel="noopener ugc nofollow" target="_blank">。这种类型的偏见可能导致错误的逮捕，并对被系统错误识别的个人造成其他负面后果。</a></li></ul><h2 id="99fa" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">微软人工智能聊天机器人(2016): <a class="ae lt" href="https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation" rel="noopener ugc nofollow" target="_blank">微软</a> - <a class="ae lt" href="https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation" rel="noopener ugc nofollow" target="_blank"> Tay </a></h2><ul class=""><li id="ed38" class="mn mo it kd b ke mp ki mq km mr kq ms ku mt ka mu mv mw mx bi translated">微软-Tay的聊天机器人从Twitter对话中学习被发现表现出种族主义或性别歧视行为，使用歧视性或攻击性的语言。这种类型的偏见可以为聊天机器人所针对的个人创造一个充满敌意的环境，也可以对创建聊天机器人的组织或个人产生不良影响。OpenAI对它免疫吗？不尽然，像这样的故事<a class="ae lt" href="https://twitter.com/spiantado/status/1599462375887114240?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1599462375887114240%7Ctwgr%5Ed871e910844ccf6f332a70d7688f4ffb9ec378c5%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.thedailybeast.com%2Fopenais-impressive-chatgpt-chatbot-is-not-immune-to-racism" rel="noopener ugc nofollow" target="_blank">tweet</a>chatGPT写了一个等式，要成为一个好的科学家，你必须是白人和男性</li></ul><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="mz na l"/></div></figure><h2 id="c8f1" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">用于筛选工作候选人的AI歧视(2022): <a class="ae lt" href="https://www.npr.org/2022/05/12/1098601458/artificial-intelligence-job-discrimination-disabilities" rel="noopener ugc nofollow" target="_blank">链接</a></h2><ul class=""><li id="5efc" class="mn mo it kd b ke mp ki mq km mr kq ms ku mt ka mu mv mw mx bi translated">用于招聘或工作表现评估的人工智能系统已被发现对某些人群存在偏见，如女性或具有特定文化背景的个人。与工作相关的流行人工智能工具的例子包括简历扫描仪、根据按键对员工进行排名的员工监控软件、评估工作技能的类似游戏的在线测试，以及测量一个人的说话模式或面部表情的视频面试软件。这种类型的偏见会妨碍合格的个人被雇用或晋升，并会加剧工作场所现有的不平等。</li></ul><h2 id="8392" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">针对弱势借款人的信用评分算法:<a class="ae lt" href="https://hai.stanford.edu/news/how-flawed-data-aggravates-inequality-credit" rel="noopener ugc nofollow" target="_blank">斯坦福研究</a></h2><ul class=""><li id="98b9" class="mn mo it kd b ke mp ki mq km mr kq ms ku mt ka mu mv mw mx bi translated">用于信用评分和贷款审批的人工智能系统已被发现对某些人群存在偏见，如低收入或居住在特定社区的个人。现在<a class="ae lt" href="https://arxiv.org/abs/2105.07554" rel="noopener ugc nofollow" target="_blank">一项预印本研究</a>中，研究人员使用人工智能测试替代信用评分模型，发现低收入家庭和少数族裔借款人确实存在一个问题:预测工具对这些群体的准确性比高收入和非少数族裔群体低5%至10%。这种偏见会阻碍个人获得信贷或贷款，并会加剧金融系统中现有的不平等。</li></ul><h2 id="7e2e" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">“黑人作为一个群体风险分值更高”:<a class="ae lt" href="https://massivesci.com/articles/machine-learning-compas-racism-policing-fairness/" rel="noopener ugc nofollow" target="_blank"> COMPAS AI </a></h2><ul class=""><li id="9e3d" class="mn mo it kd b ke mp ki mq km mr kq ms ku mt ka mu mv mw mx bi translated">2016年，ProPublica <a class="ae lt" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" rel="noopener ugc nofollow" target="_blank">报道</a>美国各地法庭使用的一种预测未来犯罪的人工智能工具，替代制裁的矫正罪犯管理概况(COMPAS)，对黑人被告有偏见。这种偏见会导致刑事司法系统对个人的不公平待遇。更糟糕的是，COMPAS的母公司Northpointe反驳了ProPublica的说法，称该算法是按照预期的运行的<a class="ae lt" href="https://www.equivant.com/response-to-propublica-demonstrating-accuracy-equity-and-predictive-parity/" rel="noopener ugc nofollow" target="_blank">。Northpointe认为黑人在被捕后有更高的犯罪风险。根据诺索恩特的说法，这导致黑人作为一个群体的风险得分更高。Northpointe现在更名为Equivant，它还没有公开改变计算风险评估的方式。</a></li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nb"><img src="../Images/bbdd8feefec3e1f15dd9acba155a69ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5DdZMsZ60HfOi8R3"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk translated">照片由<a class="ae lt" href="https://unsplash.com/@nampoh?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马克西姆·霍普曼</a>在<a class="ae lt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="7a0d" class="nj lv it bd lw nk nl nm lz nn no np mc nq nr ns mf nt nu nv mi nw nx ny ml nz bi translated">改变需要是普遍的:政策</h1><p id="9eaf" class="pw-post-body-paragraph kb kc it kd b ke mp kg kh ki mq kk kl km oa ko kp kq ob ks kt ku oc kw kx ka im bi translated">为了解决这些问题，政策在减少人工智能在我们生活中的实施偏差方面的作用至关重要。决策者有能力建立指导方针和法规，确保人工智能系统以公平和公正的方式设计、开发和使用。</p><h2 id="1e49" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated"><strong class="ak">透明度&amp;问责制</strong></h2><p id="a78b" class="pw-post-body-paragraph kb kc it kd b ke mp kg kh ki mq kk kl km oa ko kp kq ob ks kt ku oc kw kx ka im bi translated">政策有助于减少人工智能偏见的一种方式是要求人工智能系统的开发和使用具有透明度和问责制。这可以包括一些措施，例如发布有关用于训练和评估人工智能系统的数据的信息，并使个人有可能挑战人工智能系统做出的决定。通过明确人工智能系统是如何开发和使用的，决策者可以帮助确保人工智能系统不会对某些人群产生偏见。</p><h2 id="1e85" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">为质量和准确性设定标准</h2><p id="6f2f" class="pw-post-body-paragraph kb kc it kd b ke mp kg kh ki mq kk kl km oa ko kp kq ob ks kt ku oc kw kx ka im bi translated">政策可以帮助减少人工智能偏见的另一种方式是为用于训练和评估人工智能系统的数据的质量和准确性设定标准。人工智能系统只和它们被训练的数据一样好，如果数据有偏差或有缺陷，人工智能系统也会有偏差或有缺陷。通过建立人工智能系统中使用的数据的质量和准确性标准，决策者可以帮助确保人工智能系统基于高质量的数据，这些数据代表了它们打算服务的人群。</p><h2 id="9177" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">建立减轻偏见的机制</h2><p id="a163" class="pw-post-body-paragraph kb kc it kd b ke mp kg kh ki mq kk kl km oa ko kp kq ob ks kt ku oc kw kx ka im bi translated">最后，当检测到偏见时，该政策可以通过建立解决和减轻偏见的机制来减少人工智能中的偏见。这可以包括为研究人员和开发人员提供资源和支持等措施，这些人员和开发人员致力于检测和减轻人工智能系统中的偏见，并建立在发现偏见时解决偏见的流程。通过创建这些机制，决策者可以帮助确保人工智能系统以公平的方式使用，并使整个社会受益。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="fa37" class="lu lv it bd lw lx ly dn lz ma mb dp mc km md me mf kq mg mh mi ku mj mk ml mm bi translated">结论</h2><p id="09e1" class="pw-post-body-paragraph kb kc it kd b ke mp kg kh ki mq kk kl km oa ko kp kq ob ks kt ku oc kw kx ka im bi translated">人工智能的偏见是新时代的祸根。在我们的生活中，政策在减少人工智能实施偏差方面的作用至关重要。通过建立确保以公平和无偏见的方式设计、开发和使用人工智能系统的指导方针和法规，决策者可以帮助减轻人工智能偏见的潜在负面后果，并确保人工智能的使用方式有利于所有肤色的社会。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi od"><img src="../Images/03ce9729989c46c78d568f637e0e349f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M9FUb4OUIEKfjDoi"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk translated">克里斯·劳顿在<a class="ae lt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="oe na l"/></div></figure><p id="6aed" class="pw-post-body-paragraph kb kc it kd b ke ky kg kh ki kz kk kl km la ko kp kq lb ks kt ku lc kw kx ka im bi translated">支持我🔔<strong class="kd iu"> <em class="of">拍手</em></strong>|<strong class="kd iu">|<em class="of">跟随| </em> </strong> <a class="ae lt" href="https://ithinkbot.com/subscribe" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu"> <em class="of">订阅</em></strong></a><strong class="kd iu"><em class="of">|</em></strong><a class="ae lt" href="https://ithinkbot.com/membership" rel="noopener ugc nofollow" target="_blank"><strong class="kd iu"><em class="of">成为会员</em> </strong> </a> <strong class="kd iu"> <em class="of"> </em>🔔</strong></p><p id="f896" class="pw-post-body-paragraph kb kc it kd b ke ky kg kh ki kz kk kl km la ko kp kq lb ks kt ku lc kw kx ka im bi translated">检查我的其他作品—</p><div class="og oh gp gr oi oj"><a href="https://ithinkbot.com/github-copilot-hit-with-2nd-lawsuit-ed537c0b2c9a" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">GitHub CoPilot遭遇第二起诉讼</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">第二次集体诉讼已于11月10日提交</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">ithinkbot.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ln oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a rel="noopener  ugc nofollow" target="_blank" href="/the-art-of-negotiation-cicero-ai-6e04354fe990"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">谈判的艺术:西塞罗·艾</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">西塞罗·艾在外交的游戏中比人类更能谈判。就像深蓝代表国际象棋，五号代表…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">pub.towardsai.net</p></div></div><div class="os l"><div class="oy l ou ov ow os ox ln oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://ithinkbot.com/human-vs-gpt-methods-to-watermark-gpt-models-e23aefc63db8" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">OpenAI正在给GPT加水印:不再抄袭</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">知识产权保护通常被称为人工智能模型的“水印”，对人工智能的未来用例至关重要。这是被…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">ithinkbot.com</p></div></div><div class="os l"><div class="oz l ou ov ow os ox ln oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://ithinkbot.com/openai-just-released-gpt-3-text-davinci-003-i-compared-it-with-002-the-results-are-impressive-dced9aed0cba" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">OpenAI刚刚发布了GPT-3文本-达芬奇-003，我把它和002进行了对比。结果令人印象深刻！</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">OpenAI GPT-3文本-达芬奇-003产生更好的质量结果(写作质量，格式，语法，和被…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">ithinkbot.com</p></div></div><div class="os l"><div class="pa l ou ov ow os ox ln oj"/></div></div></a></div></div></div>    
</body>
</html>