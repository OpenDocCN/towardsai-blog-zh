<html>
<head>
<title>Decision Tree Splitting: Entropy vs. Misclassification Error</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树分裂:熵与错误分类误差</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/decision-tree-splitting-entropy-vs-misclassification-error-27fdf2f5e3bf?source=collection_archive---------2-----------------------#2022-10-26">https://pub.towardsai.net/decision-tree-splitting-entropy-vs-misclassification-error-27fdf2f5e3bf?source=collection_archive---------2-----------------------#2022-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f094" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么熵优于误分类错误来执行决策树分裂？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1157111c011f5718476b534f981ff4b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aKaWd2hlnteEACR31mSNdw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">通过Adobe Stock</figcaption></figure><p id="0514" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">决策树使用自上而下的贪婪搜索方法和递归分区。在决策树中，目标是递归地划分区域，直到形成同类聚类。为了进行这些划分，需要问足够多的问题。</p><p id="39fc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要在每一步拆分树，我们需要选择最佳属性，最大限度地减少从父节点到子节点的损失。因此，定义合适的损失函数是重要的一步。</p><p id="a424" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这里，我们将试图理解熵和误分类误差。另外，回答为什么分类错误不用于拆分。</p><h2 id="3a9b" class="lr ls iq bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated"><strong class="ak">熵</strong></h2><p id="8051" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">熵是信息论中用来计算信息中不确定性或杂质的现象。ID3树算法使用熵和信息增益作为损失函数来选择每一步的数据分裂属性。</p><p id="4730" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">考虑一个带有<em class="mp"> C </em>类的数据集。区域<em class="mp"> R </em>的交叉熵计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/e4c2c6e5a65bfa1c65528daf904d5e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*GoMNz1kNgG5V-BHG6m9plA.png"/></div></figure><p id="44c7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<em class="mp">Pc</em>= c类中随机选取的样本比例。</p><p id="dba9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">熵的范围在0到1之间。熵的零值表示数据是纯的或同质的。</p><h2 id="c568" class="lr ls iq bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated"><strong class="ak">分类错误</strong></h2><p id="53aa" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">错误分类损失计算错误分类样本的分数。因此，它考虑区域r中的主要类别比例。考虑C目标类别。设Pc是属于C个目标类的C类样本的比例。</p><p id="06c7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">错误分类损失计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/7361fe4d48651a3cd9bc569a50a5438a.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*058dJq5_mKcHDTXclh5GsQ.png"/></div></figure><p id="3792" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">误分类误差范围在0到0.5之间。</p><h2 id="d755" class="lr ls iq bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated"><strong class="ak">熵与误分类误差</strong></h2><p id="d91e" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">从父区域到子节点的损失的最大减少或最小化子节点损失被用于决定树分裂的属性。这种减少被称为信息增益，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/f96d9b9f3ff1f69de4994b5462f0fa24.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*zoNfAgu-0vFgQoQHM9uCcw.png"/></div></figure><p id="bf3f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了计算损失，我们需要定义一个合适的损失函数。让我们借助一个例子来比较熵和误分类损失。</p><p id="85e1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">考虑900个“阳性”样本和100个“阴性”样本。让我们假设X1属性用于在父节点进行拆分。考虑下面拆分后数据样本分布不均匀的决策树。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/415055e942f33da314efe631bdded827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*WU1gffLacnlI_l-J4TjS3g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">决策图表</figcaption></figure><p id="96ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它有一个分类为200个“阳性”样本的纯节点和一个包含700个“阳性”和100个“阴性”样本的不纯节点。</p><p id="1ba8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以<strong class="kx ir">熵</strong>为损失函数，父损失为0.467，子损失为0.544。由于一个节点是纯的，熵为零，不纯的节点具有非零熵值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/5cccef30f8b301e2badecc8c2abc79f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0DfFboCg0h-hZOGHoBE5Pg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">具有熵值的决策树</figcaption></figure><p id="efec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用信息增益公式，从父到子区域的损失减少被计算为:</p><p id="4958" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">增益=熵(父项)—[熵(左子项)*(左子项中的样本数/父项中的样本数)+熵(右子项)*(右子项中的样本数/父项中的样本数)]</p><p id="32ff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">增益= 0.467-[0.544 *(800/1000)+0 *(200/1000)]</p><p id="c455" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">增益= 0.0318</p><p id="9dae" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于<strong class="kx ir">错误分类误差</strong>，父损失为0.1，子损失为0.125。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/e28455fb2d7253b71546f0e5b6c04f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*r3sUmYHlEQiy_YyqYW9RoQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">具有误分类损失的决策树</figcaption></figure><p id="bf87" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">信息增益计算如下:</p><p id="e2f7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Gain = ME(父项)— [ME(左子项)*(左子项中的样本数/父项中的样本数)+ ME(右子项)*(右子项中的样本数/父项中的样本数)]</p><p id="86a9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">增益=(100/1000)—[(100/800)*(800/1000)+0 *(200/1000)]</p><p id="2433" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">增益=0</p><p id="91c0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上面的增益值，我们可以说，由于误分类错误没有获得任何信息，因此，不需要进一步分裂树，并且决策树停止生长。但是在熵的情况下，决策树可以被进一步划分，直到到达叶节点并且熵值变为零。</p><p id="6807" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们用<strong class="kx ir">几何透视</strong>来证明这一点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/1898a50bec4487d35172f8a7737e632e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7_Xk-BqGehWdVIaay9q_7Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk translated">熵和错误分类错误图。</figcaption></figure><p id="3736" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图是在假设数据平均分为两个节点的情况下绘制的。交叉熵函数具有<strong class="kx ir">凹</strong>性质，证明子代的损失总是小于父代。但是错误分类错误的情况并非如此。因此孩子和父母的损失是相等的。</p><p id="be56" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，与熵相比，误分类损失对类别概率的变化不敏感，由于这一点，熵经常用于建立分类的决策树。</p><p id="0629" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> Gini杂质</strong>与熵具有相同的性质，熵也是针对误分类损失构建决策树的首选。</p><h2 id="d2d4" class="lr ls iq bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated"><strong class="ak">参考文献</strong></h2><ul class=""><li id="7f77" class="mx my iq kx b ky mk lb ml le mz li na lm nb lq nc nd ne nf bi translated"><a class="ae ng" href="https://tushaargvs.github.io/assets/teaching/dt-notes-2020.pdf" rel="noopener ugc nofollow" target="_blank">https://tushaargvs . github . io/assets/teaching/dt-notes-2020 . pdf</a></li><li id="6ab4" class="mx my iq kx b ky nh lb ni le nj li nk lm nl lq nc nd ne nf bi translated"><a class="ae ng" href="https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/FAQ/docs/decision tree-error-vs-entropy . html</a></li></ul><p id="ed92" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">检查我以前的故事，</p><ol class=""><li id="ab01" class="mx my iq kx b ky kz lb lc le nm li nn lm no lq np nd ne nf bi translated"><a class="ae ng" href="https://medium.com/geekculture/image-classifier-with-streamlit-887fc186f60" rel="noopener"> <strong class="kx ir">带细流的图像分类器</strong> </a></li></ol><p id="ce55" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2.<a class="ae ng" href="https://medium.com/geekculture/everything-about-focal-loss-f2d8ab294133" rel="noopener"> <strong class="kx ir">关于焦损的一切</strong> </a></p><p id="11c6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">快乐学习！！</p></div></div>    
</body>
</html>