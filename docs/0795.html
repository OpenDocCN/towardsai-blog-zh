<html>
<head>
<title>Object Detection — Document Layout Analysis Using Monk AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对象检测—使用Monk AI进行文档布局分析</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/object-detection-document-layout-analysis-using-monk-object-detection-toolkit-6c57200bde5?source=collection_archive---------1-----------------------#2020-08-13">https://pub.towardsai.net/object-detection-document-layout-analysis-using-monk-object-detection-toolkit-6c57200bde5?source=collection_archive---------1-----------------------#2020-08-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="df59" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="6f47" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">这篇文章比较研究了三种不同的对象检测架构——yolov 3、Faster-RCNN和SSD512，内容是识别文档的不同区域，并使用MonkAI在几行代码中加载这些模型。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/67d6937843168810200b92ac7df34bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*z9shyzT5LX97qlWRQuy32Q.gif"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">使用对象检测的文档布局分析示例</figcaption></figure></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="d8f8" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">介绍</h1><p id="056c" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">这是一篇关于对象检测如何帮助我们预测文档中不同区域的文章。它可以用于裁剪出标题、段落、表格、图像等。可以根据需要对其进行后续处理以获得所需信息。我们比较了3种不同对象检测架构的性能，即YOLOv3、Faster-RCNN和SSD512，并使用Monk库加载这些模型。</p><p id="9878" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><a class="ae nh" href="https://github.com/swapnil-ahlawat/Document_Layout_Analysis-MonkAI" rel="noopener ugc nofollow" target="_blank"> Github </a>详细教程。</p><h2 id="b1db" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">关于数据集</h2><p id="b09b" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">用于该任务的训练数据集是<a class="ae nh" href="https://www.primaresearch.org/dataset/" rel="noopener ugc nofollow" target="_blank"> PRImA布局分析数据集</a>。它包括各种不同的文档类型，反映了布局分析中的各种挑战。特别强调的是:</p><ul class=""><li id="0d45" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb ny nz oa ob bi translated">杂志扫描各种主流新闻、商业和技术出版物，其中包含简单和复杂的混合布局(例如，非曼哈顿，不同的字体大小等。)</li><li id="77a9" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">各种学科的技术文章，包括期刊和会议记录中的论文，有简单的也有复杂的布局。</li></ul><p id="53c8" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">该数据集包含18个标签，即“标题”、“图表”、“信用”、“首字下沉”、“浮动”、“页脚”、“框架”、“图形”、“页眉”、“标题”、“图像”、“线描”、“数学”、“噪音”、“页码”、“段落”、“分隔符”和“表格”</p><p id="8f86" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">可以从<a class="ae nh" href="https://drive.google.com/file/d/1iBfafT1WHAtKAW0a1ifLzvW5f0ytm2i_/view" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><h2 id="2214" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">艾和尚:</h2><p id="6762" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated"><a class="ae nh" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection" rel="noopener ugc nofollow" target="_blank">和尚物体检测</a>是所有物体检测流水线的集合。对于每个管道来说，好处是双重的——使安装兼容多种操作系统、Cuda版本和python版本，并通过标准化的流程降低代码量。Monk对象检测使用户能够用很少几行代码解决计算机视觉问题。对于这个任务，我们将使用这个库的3个不同的管道用于3个不同的架构- <a class="ae nh" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection/tree/master/7_yolov3" rel="noopener ugc nofollow" target="_blank"> <em class="oh"> yolov3 </em> </a>，<a class="ae nh" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection/tree/master/1_gluoncv_finetune" rel="noopener ugc nofollow" target="_blank"><em class="oh">gluonvc _ fine tune</em></a><em class="oh">，</em>和<em class="oh"> mxrcnn </em> 。</p><h1 id="c3c3" class="lo lp it bd lq lr oi lt lu lv oj lx ly ki ok kj ma kl ol km mc ko om kp me mf bi translated">目录</h1><ol class=""><li id="466f" class="nt nu it mi b mj mk mm mn mp on mt oo mx op nb oq nz oa ob bi translated">安装Monk对象检测工具包</li><li id="4959" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated">将预先训练的模型用于文档布局分析任务</li><li id="eb69" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated">训练你自己的模型</li></ol><ul class=""><li id="3e03" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb ny nz oa ob bi translated"><em class="oh">下载和预处理数据(格式转换、选择性数据扩充)</em></li><li id="4ed3" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated"><em class="oh">从零开始训练模型</em></li></ul><p id="5081" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">4.推理和比较</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="1cca" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">1.安装Monk对象检测工具包</h1><p id="3891" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">首先，使用以下命令将库克隆到您的系统中:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="17f0" class="ni lp it os b gy ow ox l oy oz">! git clone <a class="ae nh" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection.git" rel="noopener ugc nofollow" target="_blank">https://github.com/Tessellate-Imaging/Monk_Object_Detection.git</a></span></pre><p id="6fb4" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">然后，根据您的系统的CUDA版本或Colab版本，选择您想要安装的管道和该管道的正确需求文件。以下是我在此任务中使用的管道命令:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="09cf" class="ni lp it os b gy ow ox l oy oz">#For yolov3 (used for yolov3 architecture)<br/>! cd Monk_Object_Detection/7_yolov3/installation &amp;&amp; cat requirements.txt | xargs -n 1 -L 1 pip install</span><span id="bc09" class="ni lp it os b gy pa ox l oy oz">#For gluoncv_finetune (used for SSD512 architecture)<br/>! cd Monk_Object_Detection/1_gluoncv_finetune/installation &amp;&amp; cat requirements_cuda10.1.txt | xargs -n 1 -L 1 pip install</span><span id="c2e4" class="ni lp it os b gy pa ox l oy oz">#For mxrcnn (used for FasterRCNN architecture)<br/>! cd Monk_Object_Detection/3_mxrcnn/installation &amp;&amp; cat requirements_cuda10.1.txt | xargs -n 1 -L 1 pip install</span></pre><p id="c59c" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">欲了解更多管道或安装方式，请访问<a class="ae nh" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection" rel="noopener ugc nofollow" target="_blank">僧侣对象检测库</a>。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="0122" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">2.将预先训练的模型用于文档布局分析任务</h1><p id="3a9a" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">如果您不想自己训练模型，而只想使用我们为此任务训练的模型，您可以使用下面这段代码直接使用它:</p><h2 id="9863" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">对于YOLOv3:</h2><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="59fe" class="ni lp it os b gy ow ox l oy oz"><strong class="os jd">import</strong> <strong class="os jd">os</strong><br/><strong class="os jd">import</strong> <strong class="os jd">sys</strong><br/><strong class="os jd">from</strong> <strong class="os jd">IPython.display</strong> <strong class="os jd">import</strong> Image<br/>sys.path.append("Monk_Object_Detection/7_yolov3/lib")</span><span id="18a4" class="ni lp it os b gy pa ox l oy oz"><strong class="os jd">from</strong> <strong class="os jd">infer_detector</strong> <strong class="os jd">import</strong> Infer<br/>gtf = Infer()</span></pre><p id="ce71" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">下载并初始化预训练模型:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="0fc2" class="ni lp it os b gy ow ox l oy oz">! wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&amp;confirm=$(wget --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=1Si1puABMiijtvLvH-XMnr2pVj4K2lUkO' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/<strong class="os jd">\1\n</strong>/p')&amp;id=1Si1puABMiijtvLvH-XMnr2pVj4K2lUkO" -O obj_dla_yolov3_trained.zip &amp;&amp; rm -rf /tmp/cookies.txt</span><span id="ea71" class="ni lp it os b gy pa ox l oy oz">! unzip -qq obj_dla_yolov3_trained.zip</span><span id="32d2" class="ni lp it os b gy pa ox l oy oz">! mv dla_yolov3/yolov3.cfg .</span><span id="d48d" class="ni lp it os b gy pa ox l oy oz">f = open("dla_yolov3/classes.txt")<br/>class_list = f.readlines()<br/>f.close()</span><span id="550f" class="ni lp it os b gy pa ox l oy oz">model_name = "yolov3"<br/>weights = "dla_yolov3/dla_yolov3.pt"<br/>gtf.Model(model_name, class_list, weights, use_gpu=<strong class="os jd">True</strong>, input_size=416)</span></pre><p id="83ef" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">你可以测试一下:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="70d4" class="ni lp it os b gy ow ox l oy oz">#change test1 to whatever image you want it to test for.<br/>img_path = "test1.jpg"<br/>gtf.Predict(img_path, conf_thres=0.3, iou_thres=0.5)<br/>Image(filename='output/test1.jpg')</span></pre><h2 id="0aa4" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">对于SSD512:</h2><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="44fc" class="ni lp it os b gy ow ox l oy oz"><strong class="os jd">import</strong> <strong class="os jd">os</strong><br/><strong class="os jd">import</strong> <strong class="os jd">sys</strong><br/>sys.path.append("Monk_Object_Detection/1_gluoncv_finetune/lib/")</span><span id="6b77" class="ni lp it os b gy pa ox l oy oz"><strong class="os jd">from</strong> <strong class="os jd">inference_prototype</strong> <strong class="os jd">import</strong> Infer</span></pre><p id="ec22" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">下载并初始化预训练模型:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="a073" class="ni lp it os b gy ow ox l oy oz">! wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&amp;confirm=$(wget --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=1E6T7RKGwy-v1MUxVJm-rxt5XcRyr2SQ7' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/<strong class="os jd">\1\n</strong>/p')&amp;id=1E6T7RKGwy-v1MUxVJm-rxt5XcRyr2SQ7" -O obj_dla_ssd512_trained.zip &amp;&amp; rm -rf /tmp/cookies.txt</span><span id="e103" class="ni lp it os b gy pa ox l oy oz">! unzip -qq obj_dla_ssd512_trained.zip</span><span id="e39f" class="ni lp it os b gy pa ox l oy oz">model_name = "ssd_512_vgg16_atrous_coco";<br/>params_file = "dla_ssd512/dla_ssd512-vgg16.params";<br/>class_list = ["paragraph", "heading", "credit", "footer", "drop-capital", "floating", "noise", "maths", "header", "caption", "image", "linedrawing", "graphics", "fname", "page-number", "chart", "separator", "table"]</span><span id="53bf" class="ni lp it os b gy pa ox l oy oz">gtf = Infer(model_name, params_file, class_list, use_gpu=<strong class="os jd">True</strong>)</span></pre><p id="4005" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">你可以测试一下:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="2ca6" class="ni lp it os b gy ow ox l oy oz">#change test1 to whatever image you want it to test for.<br/>img_name = "test1.jpg"  <br/>visualize = <strong class="os jd">True</strong> <br/>thresh = 0.3<br/>output = gtf.run(img_name, visualize=visualize, thresh=thresh)</span></pre><h2 id="4ff5" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">对于更快的RCNN:</h2><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="0b99" class="ni lp it os b gy ow ox l oy oz"><strong class="os jd">import</strong> <strong class="os jd">os</strong><br/><strong class="os jd">import</strong> <strong class="os jd">sys</strong><br/>sys.path.append("Monk_Object_Detection/3_mxrcnn/lib/")<br/>sys.path.append("Monk_Object_Detection/3_mxrcnn/lib/mx-rcnn")</span><span id="3db2" class="ni lp it os b gy pa ox l oy oz"><strong class="os jd">from</strong> <strong class="os jd">infer_base</strong> <strong class="os jd">import</strong> *</span></pre><p id="bab9" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">下载并初始化预训练模型:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="b854" class="ni lp it os b gy ow ox l oy oz">! wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&amp;confirm=$(wget --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=1TZQSBiMDBrGhcT75AknTbofirSFXprt8' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/<strong class="os jd">\1\n</strong>/p')&amp;id=1TZQSBiMDBrGhcT75AknTbofirSFXprt8" -O obj_dla_faster_rcnn_trained.zip &amp;&amp; rm -rf /tmp/cookies.txt</span><span id="3987" class="ni lp it os b gy pa ox l oy oz">! unzip -qq obj_dla_faster_rcnn_trained.zip</span><span id="4b27" class="ni lp it os b gy pa ox l oy oz">class_file = set_class_list("dla_fasterRCNN/classes.txt")</span><span id="7dc7" class="ni lp it os b gy pa ox l oy oz">set_model_params(model_name="vgg16", model_path="dla_fasterRCNN/dla_fasterRCNN-vgg16.params")</span><span id="e9d0" class="ni lp it os b gy pa ox l oy oz">set_hyper_params(gpus="0", batch_size=1)</span><span id="6b1d" class="ni lp it os b gy pa ox l oy oz">set_img_preproc_params(img_short_side=300, img_long_side=500, mean=(196.45086004329943, 199.09071480252155, 197.07683846968297), std=(0.25779948968052024, 0.2550292865960972, 0.2553027154941914))</span><span id="b2fd" class="ni lp it os b gy pa ox l oy oz">initialize_rpn_params()<br/>initialize_rcnn_params()<br/>sym = set_network()<br/>mod = load_model(sym)</span></pre><p id="c3e5" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">你可以测试一下:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="8f2b" class="ni lp it os b gy ow ox l oy oz">#change test1 to whatever image you want it to test for.<br/>set_output_params(vis_thresh=0.9, vis=<strong class="os jd">True</strong>)<br/>Infer("test1.jpg", mod);</span></pre></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="7f47" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">3.训练你自己的模型</h1><h2 id="e218" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">数据准备</h2><p id="c509" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">可以使用以下命令下载数据集:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="339d" class="ni lp it os b gy ow ox l oy oz">! wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=1iBfafT1WHAtKAW0a1ifLzvW5f0ytm2i_' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/<strong class="os jd">\1\n</strong>/p')&amp;id=1iBfafT1WHAtKAW0a1ifLzvW5f0ytm2i_" -O PRImA_Layout_Analysis_Dataset.zip &amp;&amp; rm -rf /tmp/cookies.txt</span><span id="ba3b" class="ni lp it os b gy pa ox l oy oz">! unzip -qq PRImA_Layout_Analysis_Dataset.zip</span></pre><p id="d3fd" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">数据集中的所有图像都是TIFF格式。TIFF图像的训练速度比JPEG格式的图像慢5倍以上，因为它们非常大。因此，TIFF图像被转换为JPEG格式的图像。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="dc75" class="ni lp it os b gy ow ox l oy oz"><strong class="os jd">for</strong> name <strong class="os jd">in</strong> glob.glob(root_dir+img_dir+'*.tif'):     <br/>im = Image.open(name)     <br/>name = str(name).rstrip(".tif")     <br/>name = str(name).lstrip(root_dir)     <br/>name = str(name).lstrip(img_dir)     <br/>im.save(final_root_dir+ img_dir+ name + '.jpg', 'JPEG')</span></pre><p id="fd40" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">数据以VOC格式显示。为了将它与各种管道一起使用，我们首先将其转换为Monk格式，这与许多Monk管道直接兼容，稍后，如果需要，我们可以很容易地将其转换为其他格式。如果你想跳过转换成Monk格式，而想直接转换成其他需要的格式，那么你可以查看pipelines的示例笔记本<a class="ae nh" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection/tree/master/example_notebooks" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="1f7d" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><strong class="mi jd">和尚格式</strong></p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="a4b6" class="ni lp it os b gy ow ox l oy oz">./Document_Layout_Analysis/ (final_root_dir)<br/>      |<br/>      |-----------Images (img_dir)<br/>      |              |<br/>      |              |------------------img1.jpg<br/>      |              |------------------img2.jpg<br/>      |              |------------------.........(and so on)<br/>      |<br/>      |<br/>      |-----------train_labels.csv (anno_file)</span></pre><p id="c30e" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><strong class="mi jd">标注文件格式</strong></p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="b86c" class="ni lp it os b gy ow ox l oy oz">| Id         | Labels                                 |<br/>| img1.jpg   | x1 y1 x2 y2 label1 x1 y1 x2 y2 label2  |</span></pre><ul class=""><li id="eade" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb ny nz oa ob bi translated">标签:xmin ymin xmax ymax标签</li><li id="51ef" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">xmin，ymin边界框的左上角</li><li id="152e" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb ny nz oa ob bi translated">xmax，ymax边界框的右下角</li></ul><p id="6be4" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">数据转换的代码很简单，但是很长。你可以在这里查看其中一个笔记本<a class="ae nh" href="https://github.com/swapnil-ahlawat/Document_Layout_Analysis-MonkAI" rel="noopener ugc nofollow" target="_blank">中的代码。</a></p><p id="0726" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">以下是用于该任务的各种管道的格式要求:</p><ol class=""><li id="4a0d" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb oq nz oa ob bi translated"><em class="oh"> yolov3 </em>用于yolov3架构的管道需要YOLOv3格式的数据。你可以在这个<a class="ae nh" href="https://github.com/swapnil-ahlawat/Document_Layout_Analysis-MonkAI/blob/master/YOLOv3-Document_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>里查看这个转换。</li><li id="f997" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated"><em class="oh">用于SSD512架构的gluoncv-finetune </em>流水线直接采用Monk格式进行训练。因此，没有必要进一步转换。</li><li id="50f6" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated"><em class="oh"> mxrcnn </em>用于fast-RCNN架构的管道需要COCO格式的数据。您可以在本<a class="ae nh" href="https://github.com/swapnil-ahlawat/Document_Layout_Analysis-MonkAI/blob/master/FasterRCNN-VGG16_Backend-Document_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中查看这一转换。</li></ol><h2 id="0a79" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">选择性数据扩充</h2><p id="6c5e" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">数据集有问题。由于文档的大部分是文本，数据集中的段落比表格或图表等其他标签多得多。为了处理数据集中的这种巨大偏差，我们只增强了那些包含这些少数民族标签之一的文档图像。例如，如果文档只有段落和图像，那么我们就不对其进行扩充。但如果它有表格、图表、图形或任何其他少数民族的标签，我们会将图像放大许多倍。这一过程有助于将数据集中的偏差减少约25%。这种选择和扩充是在从VOC到Monk格式的格式转换过程中完成的。你可以在这里查看其中一个笔记本<a class="ae nh" href="https://github.com/swapnil-ahlawat/Document_Layout_Analysis-MonkAI" rel="noopener ugc nofollow" target="_blank">中的代码。</a></p><p id="457d" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">对于数据扩充，我们使用了白蛋白库。它提供了许多不同的方法来扩充数据，如随机裁剪、转换、色调、饱和度、对比度、亮度等。你可以在这里查看更多关于这个图书馆的信息。它可以使用pip命令直接安装:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="1137" class="ni lp it os b gy ow ox l oy oz">! pip install albumentations</span></pre><p id="6470" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">下面是我们为数据扩充写的函数。很少有边界框超出图像而Albumentations库无法处理的情况，所以我们编写了一个自定义函数来确保标签在图像内部。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="e3d5" class="ni lp it os b gy ow ox l oy oz"><strong class="os jd">def</strong> augmentData(fname, boxes):<br/>    image = cv2.imread(final_root_dir+img_dir+fname)<br/>    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/> <br/>    transform = A.Compose([<br/>        A.IAAPerspective(p=0.7),   <br/>        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=5, p=0.5),<br/>        A.IAAAdditiveGaussianNoise(),<br/>        A.ChannelShuffle(),<br/>        A.RandomBrightnessContrast(),<br/>        A.RGBShift(p=0.8),<br/>        A.HueSaturationValue(p=0.8)<br/>        ], bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.2))<br/>    <br/>    <strong class="os jd">for</strong> i <strong class="os jd">in</strong> range(1, 9):<br/>        label=""<br/>        transformed = transform(image=image, bboxes=boxes)<br/>        transformed_image = transformed['image']<br/>        transformed_bboxes = transformed['bboxes']<br/>        <em class="oh">#print(transformed_bboxes)</em><br/>        flag=<strong class="os jd">False</strong><br/>        <strong class="os jd">for</strong> box <strong class="os jd">in</strong> transformed_bboxes:<br/>            x_min, y_min, x_max, y_max, class_name = box<br/>            <strong class="os jd">if</strong>(xmax&lt;=xmin <strong class="os jd">or</strong> ymax&lt;=ymin):<br/>                flag=<strong class="os jd">True</strong><br/>                <strong class="os jd">break</strong><br/>            label+= str(int(x_min))+' '+str(int(y_min))+' '+str(int(x_max))+' '+str(int(y_max))+' '+class_name+' '<br/>                        <br/>        <strong class="os jd">if</strong>(flag):<br/>            <strong class="os jd">continue</strong><br/>        cv2.imwrite(final_root_dir+img_dir+str(i)+fname, transformed_image)<br/>        label=label[:-1]<br/>        combined.append([str(i) + fname, label])</span></pre><h2 id="253c" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">计算数据集的平均值和标准差</h2><p id="324e" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated"><em class="oh"> mxrcnn </em>流水线(用于fast-RCNN)也需要均值和标准差作为参数之一。可以使用以下函数进行计算:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="9ad1" class="ni lp it os b gy ow ox l oy oz"><strong class="os jd">def</strong> normalize():<br/>    channel_sum = np.zeros(3)<br/>    channel_sum_squared = np.zeros(3)<br/>    num_pixels=0<br/>    count=0<br/>    <strong class="os jd">for</strong> file <strong class="os jd">in</strong> files:<br/>        file_path=final_root_dir+img_dir+file<br/>        img=cv2.imread(file_path)<br/>        img= img/255.<br/>        num_pixels += (img.size/3)<br/>        channel_sum += np.sum(img, axis=(0, 1))<br/>        channel_sum_squared += np.sum(np.square(img), axis=(0, 1))</span><span id="6660" class="ni lp it os b gy pa ox l oy oz">    mean = channel_sum / num_pixels<br/>    std = np.sqrt((channel_sum_squared/num_pixels) - mean**2)<br/>    <br/>    <em class="oh">#bgr to rgb conversion</em><br/>    rgb_mean = list(mean)[::-1]<br/>    rgb_std = list(std)[::-1]<br/>    <strong class="os jd">return</strong> rgb_mean, rgb_std</span><span id="87c8" class="ni lp it os b gy pa ox l oy oz">mean, std = normalize()<br/>mean=[x*255 <strong class="os jd">for</strong> x <strong class="os jd">in</strong> mean]</span></pre></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="c118" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">训练你自己的模型</h1><p id="b085" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">这就是Monk Library的真正威力所在。为对象检测架构编写代码可能是一项非常繁琐的任务，但使用Monk对象检测库只需几行代码就可以完成。</p><p id="1132" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">为了进行比较，所有3种架构都经过了30个时期的训练，学习率为0.003。</p><p id="35be" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><strong class="mi jd">对于约洛夫3: </strong></p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="b95b" class="ni lp it os b gy ow ox l oy oz"><strong class="os jd">import</strong> <strong class="os jd">os</strong><br/><strong class="os jd">import</strong> <strong class="os jd">sys</strong><br/>sys.path.append("Monk_Object_Detection/7_yolov3/lib")</span><span id="dc14" class="ni lp it os b gy pa ox l oy oz"><strong class="os jd">from</strong> <strong class="os jd">train_detector</strong> <strong class="os jd">import</strong> Detector<br/>gtf = Detector()</span><span id="dc63" class="ni lp it os b gy pa ox l oy oz"><em class="oh">#dataset directories</em><br/>img_dir = "Document_Layout_Analysis/Images/"<br/>label_dir = "Document_Layout_Analysis/labels/"<br/>class_list_file = "Document_Layout_Analysis/classes.txt"</span><span id="93d1" class="ni lp it os b gy pa ox l oy oz">gtf.set_train_dataset(img_dir, label_dir, class_list_file, batch_size=16)<br/>gtf.set_val_dataset(img_dir, label_dir)<br/>gtf.set_model(model_name="yolov3")</span><span id="81ea" class="ni lp it os b gy pa ox l oy oz"><em class="oh">#sgd is found out to perform better than adam optimiser on this task</em><br/>gtf.set_hyperparams(optimizer="sgd", lr=0.003, multi_scale=<strong class="os jd">False</strong>, evolve=<strong class="os jd">False</strong>)</span><span id="df51" class="ni lp it os b gy pa ox l oy oz">gtf.Train(num_epochs=30)</span></pre><p id="9a7b" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><strong class="mi jd">对于更快的RCNN: </strong></p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="27ef" class="ni lp it os b gy ow ox l oy oz"><strong class="os jd">import</strong> <strong class="os jd">os</strong><br/><strong class="os jd">import</strong> <strong class="os jd">sys</strong><br/>sys.path.append("Monk_Object_Detection/3_mxrcnn/lib/")<br/>sys.path.append("Monk_Object_Detection/3_mxrcnn/lib/mx-rcnn")</span><span id="0165" class="ni lp it os b gy pa ox l oy oz"><strong class="os jd">from</strong> <strong class="os jd">train_base</strong> <strong class="os jd">import</strong> *</span><span id="03c4" class="ni lp it os b gy pa ox l oy oz"><em class="oh"># Dataset params</em><br/>root_dir = "./";<br/>coco_dir = "Document_Layout_Analysis"<br/>img_dir = "Images"</span><span id="fcf3" class="ni lp it os b gy pa ox l oy oz">set_dataset_params(root_dir=root_dir, coco_dir=coco_dir, imageset=img_dir);<br/>set_model_params(model_name="vgg16")<br/>set_hyper_params(gpus="0", lr=0.003, lr_decay_epoch='20', epochs=30, batch_size=8)<br/>set_output_params(log_interval=500, save_prefix="model_vgg16")</span><span id="c796" class="ni lp it os b gy pa ox l oy oz"><em class="oh">#Preprocessing image parameters(mean and std calculated during data pre-processing)</em><br/>set_img_preproc_params(img_short_side=300, img_long_side=500, mean=(196.45086004329943, 199.09071480252155, 197.07683846968297), std=(0.25779948968052024, 0.2550292865960972, 0.2553027154941914))</span><span id="8132" class="ni lp it os b gy pa ox l oy oz">initialize_rpn_params();<br/>initialize_rcnn_params();</span><span id="b363" class="ni lp it os b gy pa ox l oy oz"><em class="oh">#Removing cache if any<br/></em><strong class="os jd">if</strong> os.path.isdir("./cache/"):<br/>    os.system("rm -r ./cache/")</span><span id="2520" class="ni lp it os b gy pa ox l oy oz">roidb = set_dataset()<br/>sym = set_network()<br/>train(sym, roidb)</span></pre><p id="9e00" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><strong class="mi jd">对于SSD512: </strong></p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="fab9" class="ni lp it os b gy ow ox l oy oz"><strong class="os jd">import</strong> <strong class="os jd">os<br/>import</strong> <strong class="os jd">sys</strong><br/>sys.path.append("Monk_Object_Detection/1_gluoncv_finetune/lib/");</span><span id="102b" class="ni lp it os b gy pa ox l oy oz"><strong class="os jd">from</strong> <strong class="os jd">detector_prototype</strong> <strong class="os jd">import</strong> Detector<br/>gtf = Detector()</span><span id="5a2a" class="ni lp it os b gy pa ox l oy oz">root = "Document_Layout_Analysis/"<br/>img_dir = "Images/"<br/>anno_file = "train_labels.csv"<br/>batch_size=8</span><span id="ff4a" class="ni lp it os b gy pa ox l oy oz">gtf.Dataset(root, img_dir, anno_file, batch_size=batch_size)</span><span id="8465" class="ni lp it os b gy pa ox l oy oz"><em class="oh">#vgg16 architecture, with atrous convolutions, pretrained on COCO dataset is used for this task</em><br/>pretrained = <strong class="os jd">True</strong>         <br/>gpu=<strong class="os jd">True</strong><br/>model_name = "ssd_512_vgg16_atrous_coco"</span><span id="5d49" class="ni lp it os b gy pa ox l oy oz">gtf.Model(model_name, use_pretrained=pretrained, use_gpu=gpu)<br/>gtf.Set_Learning_Rate(0.003)<br/>epochs=30<br/>params_file = "saved_model.params"<br/>gtf.Train(epochs, params_file)</span></pre><p id="10ff" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">这些模型是在16GB的NVIDIA Tesla V100上训练的。YOLOv3在训练中花费的时间最少-6-7小时，SSD512花费了大约11小时，而fast-RCNN花费的时间最多- 24小时以上。</p></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h1 id="97b0" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">4.推理和比较</h1><p id="e8b6" class="pw-post-body-paragraph mg mh it mi b mj mk kd ml mm mn kg mo mp mq mr ms mt mu mv mw mx my mz na nb im bi translated">推理代码与直接使用预训练模型时使用的代码几乎相同。你可以在笔记本<a class="ae nh" href="https://github.com/swapnil-ahlawat/Document_Layout_Analysis-MonkAI" rel="noopener ugc nofollow" target="_blank">这里</a>查看。</p><p id="da6d" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">从零开始训练模型后，在测试图像上获得以下结果:</p><p id="7bba" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><strong class="mi jd">从YOLOv3获得的结果:</strong></p><p id="e13b" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">YOLOv3产生的输出非常准确。这是唯一一个能够在3种架构中识别落差的模型。尽管与其他模型相比，预测的可信度较低，但它们的分类在所有三种模型中是最准确的。</p><div class="ks kt ku kv gt ab cb"><figure class="pb kw pc pd pe pf pg paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/04ed4c732e4fb3d6c4d08b826bf96716.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*aVFFxwRyJcicC9cyolu0XQ.jpeg"/></div></figure><figure class="pb kw ph pd pe pf pg paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/5291a447c4ee7c3b03bb8fc40d00c691.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*tHmU_E2QtLQSyb3zRZHSRA.jpeg"/></div></figure><figure class="pb kw pi pd pe pf pg paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/e87a6b7a1b23d12997249f324a606042.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*aUfCn7LARlpNEqzTu41EJw.jpeg"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk pj di pk pl translated">从YOLOv3架构推断测试图像</figcaption></figure></div><p id="983d" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><strong class="mi jd">从fast-RCNN获得的结果:</strong></p><p id="7c96" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">fast-RCNN以非常高的置信度检测到边界框，但它错过了一些重要的区域，如第一个示例中的页脚、第二个示例中的标题和第三个示例中的首字下沉。如果我们降低得到丢失盒子的阈值置信度，就会产生许多随机的盒子，而不清楚它代表什么。</p><div class="ks kt ku kv gt ab cb"><figure class="pb kw pm pd pe pf pg paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/cc90c5e2ec1d126ca35df4dfd430c92a.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*cKcBFGbJAtRXBXHm-_vQiA.png"/></div></figure><figure class="pb kw pn pd pe pf pg paragraph-image"><img src="../Images/928cb1b606a288b5bdbcfbfbed293c10.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*__md82fJHx7Idn4GYLnOeQ.png"/></figure><figure class="pb kw po pd pe pf pg paragraph-image"><img src="../Images/88ad7700049f4d42018162e17d8e3776.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*dmCS9ZmGuZ7XK3Gqb9auqw.png"/></figure></div><p id="7b13" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated"><strong class="mi jd">从SSD512获得的结果:</strong></p><p id="2100" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">SSD512产生的输出具有非常高的置信度，其中许多置信度为0.9+。它也是唯一能够识别页脚和文档中分隔线等干扰的模型。但它也产生了重复或不正确的标题，如第二个示例中的“浮动”(额外的框带有不正确的标签)，第三个示例中的图形和段落(同一区域的两个框带有不同的标签)。</p><div class="ks kt ku kv gt ab cb"><figure class="pb kw pp pd pe pf pg paragraph-image"><img src="../Images/869095a830efa87990ffef9556b85a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*PcG1NMbrw0nfA9TJzx2QkQ.png"/></figure><figure class="pb kw pq pd pe pf pg paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/3743fec86c4b4dac63dd0949f67c7bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*7JeyWKY29qZGjz8UjqBepA.png"/></div></figure><figure class="pb kw pr pd pe pf pg paragraph-image"><img src="../Images/296f986cb78c20cb961ccb367bb07ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*cj-VYSBWf82FuK4lRbKgog.png"/><figcaption class="ld le gj gh gi lf lg bd b be z dk ps di pt pl translated">从SSD512架构推断测试图像</figcaption></figure></div><p id="4fc2" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">根据它们的输出，可以从本教程中得出以下推论:</p><ol class=""><li id="2ead" class="nt nu it mi b mj nc mm nd mp nv mt nw mx nx nb oq nz oa ob bi translated">Monk library让学生、研究人员和竞争对手可以非常轻松地创建深度学习模型，并尝试不同的超参数调整，以非常少的几行代码提高模型的准确性。</li><li id="1808" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated">fast-RCNN在这项任务中的性能最差，而SSD512和YOLOv3的结果相当。</li><li id="4341" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated">如果你想使用一个不需要花太多时间训练的模型，并且像页脚或分隔符这样的小细节不会影响你的工作，那就用YOLOv3吧。</li><li id="1616" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated">如果这些小细节对您的工作至关重要，并且重点是边界框预测而不是分类，请使用SSD512。还需要考虑的是，Monk AI的<em class="oh"> gluoncv-finetune </em>流水线(已经用于SSD512)也提供了在各种其他数据集上预训练的架构，比如COCO数据集。</li></ol></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><h2 id="fce1" class="ni lp it bd lq nj nk dn lu nl nm dp ly mp nn no ma mt np nq mc mx nr ns me iz bi translated">参考资料:</h2><ol class=""><li id="d587" class="nt nu it mi b mj mk mm mn mp on mt oo mx op nb oq nz oa ob bi translated">教程的GitHub库:<a class="ae nh" href="https://github.com/swapnil-ahlawat/Document_Layout_Analysis-MonkAI" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/swap nil-ahlawat/Document _ Layout _ Analysis-MonkAI</a></li><li id="8c59" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated">数据集:【https://www.primaresearch.org/dataset/ T4】</li><li id="fd5f" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated">和尚物体探测库:<a class="ae nh" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection" rel="noopener ugc nofollow" target="_blank">https://github.com/Tessellate-Imaging/Monk_Object_Detection</a></li><li id="f21a" class="nt nu it mi b mj oc mm od mp oe mt of mx og nb oq nz oa ob bi translated">相册库:<a class="ae nh" href="https://github.com/albumentations-team/albumentations" rel="noopener ugc nofollow" target="_blank">https://github.com/albumentations-team/albumentations</a></li></ol><p id="e60e" class="pw-post-body-paragraph mg mh it mi b mj nc kd ml mm nd kg mo mp ne mr ms mt nf mv mw mx ng mz na nb im bi translated">感谢阅读！我希望这篇文章对你有帮助。请在评论区分享您的反馈！你可以在<a class="ae nh" href="https://www.linkedin.com/in/swapnilahlawat/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p></div></div>    
</body>
</html>