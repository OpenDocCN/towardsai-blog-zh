<html>
<head>
<title>Why Using a Policy-Based algorithm Instead of Deep Q-learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么使用基于策略的算法而不是深度Q学习？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/why-using-a-policy-based-algorithm-instead-of-deep-q-learning-ee6d8b9f0bb0?source=collection_archive---------2-----------------------#2021-07-23">https://pub.towardsai.net/why-using-a-policy-based-algorithm-instead-of-deep-q-learning-ee6d8b9f0bb0?source=collection_archive---------2-----------------------#2021-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8ce7" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="3c95" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">关于政策梯度的超简单解释。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a0d9359e27af5d2071254b5dbffaa4d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vhqJebRMWgTJEdFd"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@gradienta?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拍摄的</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">的</a>上的坡度</figcaption></figure><p id="7df2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我假设你熟悉Q学习和深度Q学习的概念。在最后一种方法中，我们发现Q值是给定一个状态和行为的预期奖励总和。因此，我们可以使用表格方法来存储所有的Q(s，a ),或者训练一个类似神经网络的近似器，用于将状态和动作映射到Q值。在给定的状态下，为了选择采取哪种行动，我们采取Q值最高的行动(我在每个状态下将获得的最大预期未来回报)。</p><p id="0584" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这么深的Q-learning太酷了！为什么我们需要另一种方法？科学家们试图找到另一种方法来解决RL问题，称之为基于政策。在这种方法中，他们试图找到环境中的最佳策略，而不是找到Q值，然后表现得贪婪。</p><p id="2645" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">基于策略的方法具有更好的收敛特性。它们只是沿着梯度寻找最佳参数，因此我们保证收敛于局部最大值(最坏情况)或全局最大值(最好情况)。此外，政策梯度比表格方法更有效。当政策总结行动时，表格方法应该计算所有行动的Q值。想象一下你有连续的行动或者这么多选择。</p><p id="62af" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第三个优势是政策梯度可以学习随机政策，而价值函数不能。这意味着您可以使用分布在动作之间进行选择。选择a1占40%，a2占20%，还有…所以你有更大的政策空间去搜索。随意阅读随机政策相对于确定性政策的好处。比如，想象一下这个小环境。在灰色区域，你要么往右走，要么往左走。当你有一个决定性的政策，我们的代理人卡住了。但是在随机分布中，代理人可以在分布中选择右或左。所以不会卡死，会大概率到达目标状态。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mb"><img src="../Images/a0aa8be18d4591f9f3251182cb2ad72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qYuzeb7XFh2gJZVt6X4SFA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">代理陷入灰色块(来自大卫·西尔弗讲座<a class="ae le" href="https://www.davidsilver.uk/teaching/" rel="noopener ugc nofollow" target="_blank">https://www.davidsilver.uk/teaching/</a>)</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mc"><img src="../Images/a117d7d4abf2e0784099c721d9a423dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpsBhxr5F52DUSYEAx5kaQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">使用随机策略，当你从一个分布中选择行动时，你可能不会被卡住。(摘自大卫·西尔弗讲座<a class="ae le" href="https://www.davidsilver.uk/teaching/" rel="noopener ugc nofollow" target="_blank">https://www.davidsilver.uk/teaching/</a>)</figcaption></figure><p id="65a2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">到目前为止，我们了解另一种类型的算法，它比深度Q学习有一些好处，称为策略梯度，它遵循梯度规则来寻找参数，将状态映射到最佳行动。</p><p id="cb35" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">那么我们应该如何在政策空间中搜索呢？如果我们的选择能最大化预期的回报，那么它就是好的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi md"><img src="../Images/ca5318ae8758df3ac2b48fe33da12049.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bvmsDbBEr4u_xDGnt1fiGA.png"/></div></div></figure><p id="cafe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所以在偶发环境中，奖励的贴现总额意味着从起点返回。假设你总是从s0开始，那么使用该策略从s0得到的期望回报就是你的j。你可以将上面的公式改写为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi me"><img src="../Images/6fca977695c36d0022b10e4d26264b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Za4kHRrbHsfu2ZWYMxbUug.png"/></div></div></figure><p id="c822" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你不能依靠一个特定的开始状态，那么你可以使用平均值。您可以对不同的V(s)进行加权平均，其中权重是从该状态开始的概率(或相应状态出现的概率)。)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/1908de76c3f8dbc916a26be57d29abf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*ybKCWW_kvx7fqN9zzmEy5w.png"/></div></figure><p id="26b1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在你可以将V(s)改写为期望回报的加权平均值，其中权重是选择特定行动的概率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/aaee4b2e647d78ef87f6c027459ea341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*hhy_E-_XqejWC2gQwgn1MA.png"/></div></figure><p id="6b5d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在我们有了目标函数，我们应该使用梯度上升(与梯度下降相反)来最大化j。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/d8e1a10a4cecf4abeda8f4644c9bf989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*2oj7Nc84TI3hYv6d7nLJIA.png"/></div></figure><p id="3f4a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里我们首先需要两个引理。</p><p id="ee7a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">引理1 </strong>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/eeec4b254dede164d7fc13f0f3102e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*xyUxLJnz2Rl_uS43hgRJDQ.png"/></div></figure><p id="745e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">引理2 </strong>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mj"><img src="../Images/4244aef751b62b09565a57bc2deb5794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEPVajXZ6-1vcB33csfDmg.png"/></div></div></figure><p id="571c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">将这两个引理与我们的目标函数相结合，我们可以计算j的梯度。因此，现在梯度仅应用我们的策略，该策略可以使用神经网络来建模。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mk"><img src="../Images/cde600525fad642a44117636c30ebf2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MyX52fGNJ7fdEnVQ4m0R8w.png"/></div></div></figure><p id="898c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">把它写成一个简单的等式我们最后的梯度政策方法叫做<strong class="lh ja">加强</strong>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ml"><img src="../Images/9387f323dfb0d1ad1a5eb1aed5730a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lp2LTpNdkU2G1Mv4tp7R2Q.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">你觉得这里有什么问题吗？！</figcaption></figure><p id="4882" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是政策梯度法的全部公式！作为总结，我把萨顿书中的算法:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mm"><img src="../Images/a421163ac698842fe340d7dcc349b608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KjByUhq9oxfG3OMd0izUAQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">加固方法</figcaption></figure><p id="8cec" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是有一个小问题。我们在目标中使用R，所以我们应该知道在一集结束时的累积奖励。这有点遵守蒙特卡洛规则。等待代理完成剧集，然后更改参数并更新策略。为什么这很重要？如果你在一集中间做了一个错误的动作，但是这一集总体上取得了成功，那么你认为所有的动作都足够好了。这意味着当你看到整体效果时，你不能意识到一个行为是否对情节产生了负面影响。所以也许你可以用你可能从那个状态和行动中得到的预期回报来代替R。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mn"><img src="../Images/5b1de3065dabc431e9995dbe7046bcf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JeqbiaaWkPaj6DA4jsLbKw.png"/></div></div></figure><p id="cad2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个变化之后，你现在也应该估计Q值了。这是第二种叫<strong class="lh ja">演员批评</strong>的方法。我们将在另一个故事中讨论这个话题。请务必理解我们一步一步走过的路。</p></div></div>    
</body>
</html>