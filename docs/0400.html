<html>
<head>
<title>Implementation of Principal Component Analysis from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始实现主成分分析</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/implementation-of-principal-component-analysis-from-scratch-df07c39128a8?source=collection_archive---------0-----------------------#2020-04-13">https://pub.towardsai.net/implementation-of-principal-component-analysis-from-scratch-df07c39128a8?source=collection_archive---------0-----------------------#2020-04-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f05e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们开始吧</h2></div><p id="852c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实时数据可能具有大量属性，这通常会使基本的探索性数据分析变得非常困难。这种数据被称为高度多维数据，其中每个属性被称为一个维度。继续处理多维数据通常会导致:</p><ol class=""><li id="3c70" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="kk iu">缺乏适当的数据可视化</strong>:由于二维以上的数据无法绘制在二维空间上，决策边界可视化是不可能的。在这种情况下，模型的决策/模式识别逻辑不能被正确解释。</li><li id="15be" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">数据分析的麻烦</strong>:对于这样的高维数据，数据分析变得不必要的麻烦。</li><li id="a6f2" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">有缺陷的机器学习模型开发</strong>:很多机器算法，主要是基于树的集成模型，包括随机森林、梯度提升等。高维数据表现不佳。</li><li id="8a43" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="kk iu">深度神经网络的复杂性增加</strong>:为高维数据设计的DNNs在架构方面通常很复杂，并且还会导致过拟合和随后的性能不佳。</li></ol><p id="87b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些问题统称为<strong class="kk iu"> <em class="ls">维度诅咒！！！</em>T11】</strong></p><p id="1b2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，为了避免这种情况，一种常用的技术是<strong class="kk iu">主成分分析(PCA) </strong>。经典的说法是卡尔·皮尔逊在1901年发明的<strong class="kk iu"> <em class="ls">正交线性变换</em></strong>【1】。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><blockquote class="ma mb mc"><p id="6988" class="ki kj ls kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">PCA背后的直觉</strong></p></blockquote><p id="3322" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，基本上，PCA是一种降维算法。假设有一个二维数据，具有两个特征(维度)f1和f2(请看图1):</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/0921c9a0d58d9efc7c64a265d47a2f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*kDjRSbrO5YRZyVX9lS8v4g.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated"><strong class="bd ms">图一。图解解释PCA背后的直觉</strong></figcaption></figure><p id="5eb0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，在图1中，h1、h2、h3、h4和h5是绘制在二维坐标空间上的5个数据点的垂直距离，其中坐标轴代表在<strong class="kk iu">线</strong>上的特征。这条线是代表2个轴(特征，F1和F2)的修改轴，这条线被称为<strong class="kk iu"> <em class="ls"> PCA线</em> </strong>。现在，有无限条这样的线，但是选择最佳拟合线，使得函数f</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/ba9e8abf7476e6a200eac7aaff3cd5b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*VMqQZD8Z6Zie3yXdW2NMLA.png"/></div></figure><p id="03ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">被最小化。因此，函数f是PCA线的代表，取决于从每个点到该线的垂直距离的平方和。</p><blockquote class="ma mb mc"><p id="accc" class="ki kj ls kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">算法解</strong></p></blockquote><ul class=""><li id="ea7f" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld mu lk ll lm bi translated">协方差矩阵的计算:</li></ul><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/d371e0cfc8c7a2fbf3368deee6644da6.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*G3TXr9OcnkwnVeZ7hSsfJg.png"/></div></figure><ul class=""><li id="dc5d" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld mu lk ll lm bi translated">计算协方差矩阵的特征向量，<em class="ls">cov _ matrix</em>【2】。</li><li id="c3bc" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld mu lk ll lm bi translated">现在，新的特征空间被定义为，</li></ul><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/b949f70e78db37f5af24dbe819aee3e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*G2I7NfOJmCSeJaHWbkT7lQ.png"/></div></figure><blockquote class="ma mb mc"><p id="08c1" class="ki kj ls kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">用Python实现</strong></p></blockquote><p id="b86b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，转到PCA的实际实现。让我们以俄勒冈州波特兰市的房价数据集为例。它包含房子的大小(平方英尺)和一些卧室作为特征，房子的价格作为目标变量。该数据集可从以下网址获得</p><div class="mx my gp gr mz na"><a href="https://github.com/navoneel1092283/multivariate_regression" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab fo"><div class="nc ab nd cl cj ne"><h2 class="bd iu gy z fp nf fr fs ng fu fw is bi translated">navoneel 1092283/多元回归</h2><div class="nh l"><h3 class="bd b gy z fp nf fr fs ng fu fw dk translated">通过在GitHub上创建帐户，为navoneel 1092283/multivariate _ regression开发做出贡献。</h3></div><div class="ni l"><p class="bd b dl z fp nf fr fs ng fu fw dk translated">github.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no mm na"/></div></div></a></div><p id="ad61" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">数据读入Numpy数组</strong>:</p><pre class="mh mi mj mk gt np nq nr ns aw nt bi"><span id="3e63" class="nu nv it nq b gy nw nx l ny nz">import numpy as np<br/>data = np.loadtxt('data2.txt', delimiter=',')<br/>X_train = data[:,[0,1]] #feature set<br/>y_train = data[:,2] #label set</span></pre><p id="67e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">特征归一化或特征缩放</strong>:</p><pre class="mh mi mj mk gt np nq nr ns aw nt bi"><span id="9960" class="nu nv it nq b gy nw nx l ny nz">mean = np.ones(X_train.shape[1])<br/>std = np.ones(X_train.shape[1])</span><span id="2b46" class="nu nv it nq b gy oa nx l ny nz">for i in range(0, X_train.shape[1]):<br/>    mean[i] = np.mean(X_train.transpose()[i])<br/>    std[i] = np.std(X_train.transpose()[i])<br/>    for j in range(0, X_train.shape[0]):<br/>        X_train[j][i] = (X_train[j][i] - mean[i])/std[i]</span></pre><p id="7742" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PCA算法:</strong></p><pre class="mh mi mj mk gt np nq nr ns aw nt bi"><span id="4511" class="nu nv it nq b gy nw nx l ny nz">def PCA(X_train, k):<br/>    <strong class="nq iu"># 1. Computation of Co-Variance Matrix:</strong><br/>    cov = np.zeros((X_train.shape[1], X_train.shape[1]))<br/>    for i in range(0, X_train.shape[0]):<br/>        cov = cov + np.matmul(X_train[i].transpose(), X_train[i])<br/>    avg_cov = (1/X_train.shape[0]) * cov<br/>    <strong class="nq iu"># 2. Obtaining the eigen vectors:</strong><br/>    from numpy import linalg as LA<br/>    eigen_values, eigen_vectors = LA.eig(avg_cov)<br/>    <strong class="nq iu"># 3. Selecting k (here 1) Principal Components:</strong><br/>    z = np.matmul(eigen_vectors, X_train.T)[:k]<br/>    return z</span><span id="8856" class="nu nv it nq b gy oa nx l ny nz">z = PCA(X_train, 1) # Calling the PCA function</span></pre><p id="4982" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">原始非标准化数据和新特征空间数据的数据可视化:</strong></p><p id="6eca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">I .重新缩放或特征反规格化</p><pre class="mh mi mj mk gt np nq nr ns aw nt bi"><span id="9d62" class="nu nv it nq b gy nw nx l ny nz"># <strong class="nq iu">Re-scaling or De-normalization:<br/></strong>for i in range(0, X_train.shape[1]):<br/>    for j in range(0, X_train.shape[0]):<br/>        X_train[j][i] = X_train[j][i] * std[i] + mean[i]</span></pre><p id="d00e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">二。三维数据(原始特征图)和二维图(PCA后)</p><pre class="mh mi mj mk gt np nq nr ns aw nt bi"><span id="4619" class="nu nv it nq b gy nw nx l ny nz"><strong class="nq iu"># Plotting the 3-D Data (extra dimension for target):</strong><br/>from mpl_toolkits.mplot3d import Axes3D<br/>import matplotlib.pyplot as plt<br/>sequence_containing_x_vals = list(X_train.transpose()[0])<br/>sequence_containing_y_vals = list(X_train.transpose()[1])<br/>sequence_containing_z_vals = list(y_train)<br/>fig = plt.figure()<br/>ax = Axes3D(fig)<br/>ax.scatter(sequence_containing_x_vals, sequence_containing_y_vals,<br/>           sequence_containing_z_vals)<br/>ax.set_xlabel('Living Room Area', fontsize=10)<br/>ax.set_ylabel('Number of Bed Rooms', fontsize=10)<br/>ax.set_zlabel('Actual Housing Price', fontsize=10)</span><span id="26bb" class="nu nv it nq b gy oa nx l ny nz"><strong class="nq iu"># Plotting the PCA new 2-D Data (extra dimension for target):</strong><br/>plt.scatter(z[0], y_train)<br/>plt.xlabel('z: The New Feature Space after PCA')<br/>plt.ylabel('Actual Housing Price')</span></pre><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ob"><img src="../Images/be886e0f7f9c621f329e211de37dff8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HC4wlSxMomxOqr9H7jSng.jpeg"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated"><strong class="bd ms">图二。使用PCA将三维绘制成二维</strong></figcaption></figure><p id="2467" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从图2可以看出，三维坐标空间中的三维数据已经有效地拟合到图中，并映射到二维坐标空间，因此，主成分分析(PCA)已经从零开始成功地实现。</p><blockquote class="ma mb mc"><p id="a1b9" class="ki kj ls kk b kl km ju kn ko kp jx kq md ks kt ku me kw kx ky mf la lb lc ld im bi translated"><strong class="kk iu">参考文献:</strong></p></blockquote><p id="9569" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]<a class="ae og" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Principal_component_analysis</a></p><p id="06d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]<a class="ae og" href="https://towardsdatascience.com/visualizing-eigenvalues-and-eigenvectors-e2f9e3ac58d7" rel="noopener" target="_blank">https://towards data science . com/visualizing-enforcern-and-features vectors-e 2 F9 E3 AC 58d 7</a></p></div></div>    
</body>
</html>