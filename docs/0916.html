<html>
<head>
<title>How to implement Linear Regression with TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用TensorFlow实现线性回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-implement-linear-regression-with-tensorflow-406b2cff1ffa?source=collection_archive---------1-----------------------#2020-09-11">https://pub.towardsai.net/how-to-implement-linear-regression-with-tensorflow-406b2cff1ffa?source=collection_archive---------1-----------------------#2020-09-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8523" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="688d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过实施线性回归学习张量流基础知识</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e4fed6ac834c416e2f4f11db9671f5ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GUPcI_k9x4d7nyx93R5zhw.gif"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</figcaption></figure><p id="3244" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们先简单回忆一下什么是线性回归:</p><blockquote class="md me mf"><p id="4d4e" class="lh li mg lj b lk ll kd lm ln lo kg lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><em class="it">线性回归是通过一些其他已知变量以线性方式估计未知变量。视觉上，我们通过我们的数据点拟合一条线(或更高维的超平面)。</em></p></blockquote><p id="772f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你对这个概念不太适应，或者想更好地理解它背后的数学原理，你可以阅读我以前写的关于线性回归的文章:</p><div class="mk ml gp gr mm mn"><a href="https://towardsdatascience.com/understanding-linear-regression-eaaaed2d983e" rel="noopener follow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd jd gy z fp ms fr fs mt fu fw jc bi translated">了解线性回归</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">线性回归背后的数学详细解释</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">towardsdatascience.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb lb mn"/></div></div></a></div></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="da9b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">很可能，用TensorFlow实现线性回归是大材小用。这个库是为神经网络、复杂的深度学习架构等更复杂的东西而创建的。尽管如此，我认为使用它来实现更简单的机器学习方法，如线性回归，对于那些想知道如何使用TensorFlow构建自定义内容的人来说是一个很好的练习。</p><p id="caaa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">TensorFlow有很多APIs而且大部分入门课程/教程只讲解一个更高级的API，像Keras。但是这可能还不够，例如，如果您想要使用Keras中尚未实现的自定义丢失和/或激活功能。</p><p id="2ce4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在其核心，TensorFlow只是一个类似于NumPy的数学库，但有两个重要的改进:</p><ul class=""><li id="3593" class="nj nk it lj b lk ll ln lo lq nl lu nm ly nn mc no np nq nr bi translated">它使用GPU来加快运算速度。如果你有合适配置的兼容GPU，TF 2会自动使用；不需要更改代码。</li><li id="1725" class="nj nk it lj b lk ns ln nt lq nu lu nv ly nw mc no np nq nr bi translated">它能够自动区分；这意味着，对于基于梯度的方法，您不需要手动计算梯度，TensorFlow会为您完成。</li></ul><p id="33d7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你可以把TensorFlow想象成服用了类固醇的NumPy。</p><p id="13dd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">虽然这两个功能对于我们在这里想要做的事情(线性回归)来说似乎不是很大的改进，因为这不是非常昂贵的计算，并且手动计算梯度非常简单，但它们在深度学习中有很大的不同，在深度学习中，我们需要大量的计算能力，并且手动计算梯度非常困难。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="124e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们跳到实现。</p><p id="2135" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，很明显，我们需要导入一些库。我们导入<code class="fe nx ny nz oa b">tensorflow</code>，因为它是我们用于实现的主要内容，<code class="fe nx ny nz oa b">matplotlib</code>用于可视化我们的结果，<code class="fe nx ny nz oa b">sklearn</code>中的<code class="fe nx ny nz oa b">make_regression</code>函数，我们将使用它来生成一个回归数据集作为示例，以及python的内置<code class="fe nx ny nz oa b">math</code>模块。</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="776b" class="of og it oa b gy oh oi l oj ok">import tensorflow as tf</span><span id="a302" class="of og it oa b gy ol oi l oj ok">import matplotlib.pyplot as plt</span><span id="ddaa" class="of og it oa b gy ol oi l oj ok">from sklearn.datasets import make_regression</span><span id="3abd" class="of og it oa b gy ol oi l oj ok">import math</span></pre><p id="5b5c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后我们将使用以下方法创建一个<code class="fe nx ny nz oa b">LinearRegression</code>类:</p><ul class=""><li id="989b" class="nj nk it lj b lk ll ln lo lq nl lu nm ly nn mc no np nq nr bi translated"><code class="fe nx ny nz oa b">.fit()</code> —该方法将对我们的线性回归模型进行实际学习；在这里，我们将找到最佳权重</li><li id="bfa0" class="nj nk it lj b lk ns ln nt lq nu lu nv ly nw mc no np nq nr bi translated"><code class="fe nx ny nz oa b">.predict()</code> —这个将用于预测；它将返回我们的线性模型的输出</li><li id="4162" class="nj nk it lj b lk ns ln nt lq nu lu nv ly nw mc no np nq nr bi translated"><code class="fe nx ny nz oa b">.rmse()</code> —用给定的数据计算我们的模型的均方根误差；这个指标有点像“从我们的模型估计值到真实y值的平均距离”</li></ul><p id="5934" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们在<code class="fe nx ny nz oa b">.fit()</code>中做的第一件事是将一个额外的1列连接到我们的输入矩阵x。这是为了简化我们的数学，并将偏差视为一个始终为1的额外变量的权重。</p><p id="36a2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe nx ny nz oa b">.fit()</code>方法将能够通过使用封闭形式的公式或随机梯度下降来学习参数。为了选择使用哪个，我们将有一个名为method的参数，该参数需要一个字符串“solve”或“sgd”。</p><p id="a41d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当<code class="fe nx ny nz oa b">method</code>设置为“求解”时，我们将通过以下公式获得模型的权重:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/81cf1575b2c991ff9ababd19c7e41831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*zIbw0VcfIiZVjuKliUNgRQ.png"/></div></figure><p id="2444" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这要求矩阵X具有满列秩；因此，我们将检查这一点，否则我们会显示一条错误消息。</p><p id="75e3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们的第一部分<code class="fe nx ny nz oa b">.fit()</code>方法是:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="e516" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意<code class="fe nx ny nz oa b">method</code>之后的其他参数是可选的，仅在我们使用SGD的情况下使用。</p><p id="3507" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该方法的第二部分处理<code class="fe nx ny nz oa b">method = ‘sgd’</code>的情况，它不要求X具有完整的列秩。</p><p id="deff" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们的最小二乘线性回归的SGD算法概述如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/10ce771fffe67668a5488a8a04325a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MdsoOAIddbg2TEUdPNKuhw.png"/></div></div></figure><p id="d34d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将通过将weights类属性初始化为TensorFlow变量来开始此算法，tensor flow变量是一个列向量，其值来自均值为0、标准差为1/(列数)的正态分布。我们将标准偏差除以列数，以确保在算法的初始阶段不会得到太大的输出值。这是为了帮助我们更快地收敛。</p><p id="5910" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在每次迭代的开始，我们随机地打乱我们的数据行。然后，对于每一批，我们计算梯度并将其从当前权重向量中减去(乘以学习率),以获得新的权重。</p><p id="7ea4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在上面描述的SGD算法中，我们已经展示了手动计算的梯度；就是那个表达式乘以alpha(学习率)。但是在下面的代码中，我们不会显式地计算这个表达式；相反，我们计算损失值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8db44ab9d7f8802d462587b9257bc9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*-7KNBCYVFYqvNywH5HrmRQ.png"/></div></figure><p id="9130" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后我们让TensorFlow为我们计算梯度。</p><p id="a1ed" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是我们<code class="fe nx ny nz oa b">.fit()</code>方法的后半部分:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="05c7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们需要计算<code class="fe nx ny nz oa b">with tf.GradientTape() as tape</code>块内部的损失值，然后调用<code class="fe nx ny nz oa b">tape.gradient(loss_value, self.weights)</code>来获得梯度。要做到这一点，重要的是梯度所对应的量(<code class="fe nx ny nz oa b">self.weights</code>)是一个<code class="fe nx ny nz oa b">tf.Variable</code>对象。此外，当改变权重时，我们应该使用<code class="fe nx ny nz oa b">.assign_sub()</code>方法而不是<code class="fe nx ny nz oa b">-=</code>。</p><p id="d032" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们从这个方法返回<code class="fe nx ny nz oa b">self</code>，以便能够像这样连接构造函数和<code class="fe nx ny nz oa b">.fit()</code>的调用:<code class="fe nx ny nz oa b">lr = LinearRegression().fit(X, y, ‘solve’)</code>。</p><p id="5096" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe nx ny nz oa b">.predict()</code>方法非常简单。我们首先检查之前是否调用了<code class="fe nx ny nz oa b">.fit()</code>，然后将一列1连接到X，并验证X的形状允许与权重向量相乘。如果一切正常，我们只需返回X和权重向量相乘的结果作为预测。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="ece2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<code class="fe nx ny nz oa b">.rmse()</code>中，我们首先使用<code class="fe nx ny nz oa b">.predict()</code>获得模型的输出，然后如果预测期间没有错误，我们计算并返回均方根误差，该误差可以被认为是“从我们的模型估计值到真实y值的平均距离”。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="8c4d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是<code class="fe nx ny nz oa b">LinearRegression</code>类的完整代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="d3e3" class="of og it bd oq or os dn ot ou ov dp ow lq ox oy oz lu pa pb pc ly pd pe pf iz bi translated">在示例中使用我们的LinearRegression类</h2><p id="b9dc" class="pw-post-body-paragraph lh li it lj b lk pg kd lm ln ph kg lp lq pi ls lt lu pj lw lx ly pk ma mb mc im bi translated">为了展示我们的线性回归实现，我们将使用来自<code class="fe nx ny nz oa b">sklearn</code>的<code class="fe nx ny nz oa b">make_regression()</code>函数生成一个回归数据集。</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="2b05" class="of og it oa b gy oh oi l oj ok">X, y = make_regression(n_features=1, n_informative=1,<br/>                       bias=1, noise=35)</span></pre><p id="b1a5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们绘制这个数据集，看看它是什么样子的:</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="625a" class="of og it oa b gy oh oi l oj ok">plt.scatter(X, y)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ccad00cd2a595bbabf30ab0919f81a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7ZZkeJUkwjQ2dTBYAa4EQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</figcaption></figure><p id="c5d3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe nx ny nz oa b">make_regression()</code>返回的y是平面向量。我们将把它重新整形为一个列向量，用于我们的<code class="fe nx ny nz oa b">LinearRegression</code>类。</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="19cf" class="of og it oa b gy oh oi l oj ok">y = y.reshape((-1, 1))</span></pre><p id="10a1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，我们将使用<code class="fe nx ny nz oa b">method = ‘solve’</code>来拟合回归线:</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="58fc" class="of og it oa b gy oh oi l oj ok">lr_solve = LinearRegression().fit(X, y, method='solve')</span><span id="987a" class="of og it oa b gy ol oi l oj ok">plt.scatter(X, y)</span><span id="d121" class="of og it oa b gy ol oi l oj ok">plt.plot(X, lr_solve.predict(X), color='orange')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/80dfd0e1e3eda8bf0e554b559f3bd70d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CcWb5veSdd1Tpb5q5myGWw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</figcaption></figure><p id="d4a6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上述回归模型的均方根误差为:</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="5c43" class="of og it oa b gy oh oi l oj ok">lr_solve.rmse(X, y)<br/># &lt;tf.Tensor: shape=(), dtype=float32, numpy=<strong class="oa jd">37.436085</strong>&gt;</span></pre><p id="1cad" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，我们也使用<code class="fe nx ny nz oa b">method = ‘sgd’</code>，我们将让其他参数具有它们的默认值:</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="1e5e" class="of og it oa b gy oh oi l oj ok">lr_sgd = LinearRegression().fit(X, y, method='sgd')</span><span id="8834" class="of og it oa b gy ol oi l oj ok">plt.scatter(X, y)</span><span id="33a6" class="of og it oa b gy ol oi l oj ok">plt.plot(X, lr_sgd.predict(X), color='orange')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/7c82a28233a863c9c069d54701429b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Qwhh9NXG9thjJhtec3cMA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</figcaption></figure><p id="d4a4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如你所见，上面两幅图中方法“solve”和“sgd”的回归线几乎相同。</p><p id="175c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用“sgd”时，我们得到的均方根误差为:</p><pre class="ks kt ku kv gt ob oa oc od aw oe bi"><span id="ef78" class="of og it oa b gy oh oi l oj ok">lr_sgd.rmse(X, y)<br/># &lt;tf.Tensor: shape=(), dtype=float32, numpy=<strong class="oa jd">37.86531</strong>&gt;</span></pre><p id="e3b1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是Jupyter笔记本，包含所有代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="on oo l"/></div></figure></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="796b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我希望这些信息对你有用，感谢你的阅读！</p><p id="9aa8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这篇文章也贴在我自己的网站<a class="ae pl" href="https://www.nablasquared.com/how-to-implement-linear-regression-with-tensorflow/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>