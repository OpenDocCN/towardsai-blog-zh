<html>
<head>
<title>Fully Explained Naive Bayes Classification with Python Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python例子充分解释了朴素贝叶斯分类</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fully-explained-naive-bayes-classification-with-python-example-9110fcf2db?source=collection_archive---------1-----------------------#2021-06-26">https://pub.towardsai.net/fully-explained-naive-bayes-classification-with-python-example-9110fcf2db?source=collection_archive---------1-----------------------#2021-06-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="695e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="2568" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">监督机器学习算法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c4d8f15dc1145c7cc9f52c184bed782b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SO62siXdsCp8WJaq"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@ralexnder?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚历克斯·丘马克</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="8351" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">介绍</h2><p id="3b28" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated">朴素贝叶斯是一种简单而流行的机器学习分类算法。它的工作是基于贝叶斯概率定理的原理，在计算条件概率之后预测未知数据点的类别，它的工作是基于贝叶斯定理，假设与预测器无关。</p><p id="432c" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">贝叶斯分类器认为一个特征与具有一组特征的特定类中的其他特征无关。</p><p id="7774" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">朴素贝叶斯模型用于大型数据集，例如在您的数据集中有成百上千个数据点和一些变量的地方或情况，因为这种算法比其他分类算法非常快速且易于实现。</p><p id="be11" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">贝叶斯定理是在已知一个事件发生的概率的情况下，求出另一个事件发生的概率。</p><p id="9268" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">在概率学中，贝叶斯定理描述了一个事件发生的概率，它基于可能与该事件相关的条件的先验知识。</p><p id="c586" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">公式如下所示:</p><p id="f661" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated"><strong class="mf jd"> P(A|B) = P(B|A)*P(A) / P(B) </strong></p><p id="921b" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">借助一个例子，我们可以理解贝叶斯定理的全部工作原理。</p><p id="c767" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">让我们有两台机器，机器1和机器2制作面包。两台机器每小时分别生产30和20个面包。这两台机器每小时生产50个面包。</p><p id="5afb" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">我们必须测量有缺陷零件的概率。</p><p id="23b4" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">下面给出一些先验知识:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="53ad" class="li lj it nc b gy ng nh l ni nj"><strong class="nc jd">Machine1</strong>: 30 Breads / hr      =&gt;   <strong class="nc jd">P(Machine1</strong>) = 30/50 = 0.6</span><span id="7e65" class="li lj it nc b gy nk nh l ni nj"><strong class="nc jd">Machine2</strong>: 20 Breads /hr       =&gt;  <strong class="nc jd">P(Machine2</strong>) = 20/50 =0.4</span><span id="7897" class="li lj it nc b gy nk nh l ni nj">Out of all product parts 1% are defective out of all defective parts =&gt; <strong class="nc jd">P(Defect)</strong> = 1%.</span><span id="dc96" class="li lj it nc b gy nk nh l ni nj">We can see that 50% came from machine1 =&gt; <strong class="nc jd">P(Machine1|Defect)</strong> = 50%</span><span id="b9d5" class="li lj it nc b gy nk nh l ni nj">And 50% came from machine2  =&gt;     <strong class="nc jd">P(Machine2|Defect</strong>) = 50%</span><span id="1f15" class="li lj it nc b gy nk nh l ni nj">                            =&gt;     <strong class="nc jd">P(Defect | Machine2</strong>) = ?</span></pre><p id="54c6" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">上述条件下的公式如下所示:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="d012" class="li lj it nc b gy ng nh l ni nj"><strong class="nc jd">P(Defect|Machine2) = P(Machine2|Defect)*P(Defect) / P(Machine2)</strong></span><span id="3f10" class="li lj it nc b gy nk nh l ni nj"><strong class="nc jd">P(Defect | Machine2)</strong> = 0.5 * 0.01 / 0.4 = 0.0125  =&gt;  <strong class="nc jd">1.25%</strong></span></pre><p id="35f7" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">我们可以说总<strong class="mf jd">不良零件</strong>的<strong class="mf jd"> 1.25% </strong>是由<strong class="mf jd">机2 </strong>制造的，同样我们可以计算<strong class="mf jd">机1 </strong>的不良零件。</p><h2 id="3a43" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">朴素贝叶斯分类器的工作</strong></h2><p id="c0af" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated">下面给我们一个年龄与工资的图表，红色数据点代表A级<strong class="mf jd">和b级<strong class="mf jd"/></strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nl"><img src="../Images/9e9fe228c1a8d1c7064410338ae77a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qbVPEVonepW0VBt9945hiw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图像<a class="ae lh" href="https://techdifferences.com/difference-between-classification-and-clustering.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="c732" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">让我们假设一个<strong class="mf jd">新值(+) </strong>，我们必须在朴素贝叶斯分类器的帮助下预测这个<strong class="mf jd"> (+) </strong>属于A类还是B类。</p><p id="4307" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">因此基于A类和B类的朴素贝叶斯公式会是这样的:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="2e52" class="li lj it nc b gy ng nh l ni nj"><strong class="nc jd">P(A|B) = P(B|A)*P(A) / P(B)</strong></span></pre><p id="c126" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">现在计算所有的概率，如上面的公式所示:</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="06da" class="li lj it nc b gy ng nh l ni nj"><strong class="nc jd">P( ClassA )  = 10 /30 = 0.33</strong>     <br/><strong class="nc jd">P( ClassB )=   20 /30 = 0.66</strong></span><span id="4b07" class="li lj it nc b gy nk nh l ni nj"><strong class="nc jd">Using Circle P(X)  = 4/30 = 0.13</strong>              <br/><strong class="nc jd">Using Circle P(X)  = 4/30 = 0.13</strong></span><span id="83c5" class="li lj it nc b gy nk nh l ni nj"><strong class="nc jd">Using Circle with Red Data   <br/>P(X | ClassA) = 3 / 10  = 0.3</strong></span><span id="4304" class="li lj it nc b gy nk nh l ni nj"><strong class="nc jd">Using Circle with Red Data   <br/>P(X | ClassB) = 1 /20   = 0.05</strong></span><span id="5a9e" class="li lj it nc b gy nk nh l ni nj"><strong class="nc jd">P( ClassB | X)<em class="nm"> =   </em>P(X | ClassB)*P( ClassB ) / P(X) = 0.25</strong></span></pre><p id="ea69" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">在哪里，</p><p id="6aa3" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated"><strong class="mf jd"> P( ClassB | X) — </strong>后验概率</p><p id="000e" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated"><strong class="mf jd">P(X | class b)——</strong>似然概率</p><p id="5330" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated"><strong class="mf jd">P(class b)——</strong>先验概率</p><p id="bcd8" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated"><strong class="mf jd"> P(X) — </strong>边际概率</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="1277" class="li lj it nc b gy ng nh l ni nj">#Similarly for Class A<br/><strong class="nc jd">P( ClassA | X)<em class="nm"> =   </em>P(X | ClassA)*P( ClassA ) / P(X)  =  0.75</strong></span></pre><p id="6b62" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">因此，<strong class="mf jd"> 0.75 &gt; 0.25 </strong></p><p id="fdd8" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated"><strong class="mf jd">我们的新值(+)属于/分配给A类</strong>，A类中存在的数据点数量将变为11。</p><h2 id="8262" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">朴素贝叶斯的类型</strong></h2><ul class=""><li id="793e" class="nn no it mf b mg mh mj mk lr np lv nq lz nr mv ns nt nu nv bi translated"><strong class="mf jd">高斯朴素贝叶斯:</strong>在高斯NB中，特征后面是正态分布曲线，而不是离散值。</li><li id="f58f" class="nn no it mf b mg nw mj nx lr ny lv nz lz oa mv ns nt nu nv bi translated"><strong class="mf jd">多项朴素贝叶斯:</strong>在多项NB特征之后是离散值计数。文本内分类问题我们可以统计一个独特的词在文档中出现的频率。文档可以包括体育、政治、教育等。</li><li id="0350" class="nn no it mf b mg nw mj nx lr ny lv nz lz oa mv ns nt nu nv bi translated"><strong class="mf jd">伯努利朴素贝叶斯:</strong>二项式NB模型仅用于二进制形式的特征向量，即0和1。例如，在文本分类中，0和1决定句子中出现的单词。</li></ul><p id="71d6" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated"><strong class="mf jd">应用:</strong></p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="92a5" class="li lj it nc b gy ng nh l ni nj">1. Making predictions.<br/>2. Multiclass prediction feature<br/>3. Classify articles.<br/>4. Text classification<br/>5. Spam Filtering<br/>6. Sentiment Analysis<br/>7. Recommendation System</span></pre><h2 id="9a9f" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">优点</strong></h2><ul class=""><li id="944f" class="nn no it mf b mg mh mj mk lr np lv nq lz nr mv ns nt nu nv bi translated">预测测试数据属于某个特定的类既快又容易。多类归属预测也可以顺利完成。</li><li id="d2d4" class="nn no it mf b mg nw mj nx lr ny lv nz lz oa mv ns nt nu nv bi translated">预测比其他分类算法更准确，如逻辑回归、KNN等。</li><li id="ca58" class="nn no it mf b mg nw mj nx lr ny lv nz lz oa mv ns nt nu nv bi translated">分类数据的表现非常好。</li></ul><h2 id="e2da" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">缺点</strong></h2><ul class=""><li id="9d92" class="nn no it mf b mg mh mj mk lr np lv nq lz nr mv ns nt nu nv bi translated">假设我们有一个未提供训练的分类变量，那么我们的模型将为其分配0概率，预测不会发生，称为零频率。</li><li id="ed12" class="nn no it mf b mg nw mj nx lr ny lv nz lz oa mv ns nt nu nv bi translated">朴素贝叶斯假设独立的预测器，而在现实生活中，不可能有一组彼此完全独立的预测器。</li></ul><h2 id="7e90" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">朴素贝叶斯的Python代码</strong></h2><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="24d5" class="li lj it nc b gy ng nh l ni nj"># Importing the libraries</span><span id="ed0a" class="li lj it nc b gy nk nh l ni nj">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span></pre><p id="05df" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">导入数据集</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="7a11" class="li lj it nc b gy ng nh l ni nj">dataset = pd.read_csv('/content/LR.csv')<br/>dataset.head(2)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/0ffec2e234ac6e8d435a77cbb5b38cd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*yjVubseIHVqGnaIqCFKF1Q.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="087f" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">将数据分为自变量和因变量。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="4289" class="li lj it nc b gy ng nh l ni nj">X = dataset.iloc[:, [0,1]].values<br/>y = dataset.iloc[:, 2].values</span></pre><p id="a109" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">分为训练集和测试集</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="d2bd" class="li lj it nc b gy ng nh l ni nj">from sklearn.model_selection import train_test_split</span><span id="59ab" class="li lj it nc b gy nk nh l ni nj">X_train, X_test, y_train, y_test = <br/>              train_test_split(X,y,test_size=0.25, random_state=0)</span></pre><p id="e170" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">缩放数据的训练集和测试集。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="9cde" class="li lj it nc b gy ng nh l ni nj">from sklearn.preprocessing import StandardScaler</span><span id="1359" class="li lj it nc b gy nk nh l ni nj">sc_X = StandardScaler()</span><span id="125d" class="li lj it nc b gy nk nh l ni nj">X_train = sc_X.fit_transform(X_train)<br/>X_test = sc_X.fit_transform(X_test)</span></pre><p id="4682" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">拟合和预测模型。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="5393" class="li lj it nc b gy ng nh l ni nj">from sklearn.naive_bayes import GaussianNB</span><span id="35c3" class="li lj it nc b gy nk nh l ni nj">classifer=GaussianNB()</span><span id="8b7b" class="li lj it nc b gy nk nh l ni nj">classifer.fit(X_train,y_train)<br/>GaussianNB(priors=None, var_smoothing=1e-09)</span><span id="bdb1" class="li lj it nc b gy nk nh l ni nj">y_pred = classifer.predict(X_test)<br/>y_pred</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/94f80ed26fd85966132b15a1707e61a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*vSjX6ib1OpC0Zc2zNrPAFA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="298f" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">计算混淆矩阵和模型的准确性。</p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="9ba6" class="li lj it nc b gy ng nh l ni nj">from sklearn.metrics import confusion_matrix</span><span id="2d79" class="li lj it nc b gy nk nh l ni nj">cm =confusion_matrix(y_test, y_pred)<br/>cm</span><span id="821d" class="li lj it nc b gy nk nh l ni nj"><strong class="nc jd">#output:</strong><br/>array([[64,  4],<br/>       [ 5, 27]])</span><span id="9b53" class="li lj it nc b gy nk nh l ni nj">#accuracy<br/>from sklearn.metrics import accuracy_score</span><span id="26dc" class="li lj it nc b gy nk nh l ni nj">accuracy_score(y_test, y_pred)</span><span id="9519" class="li lj it nc b gy nk nh l ni nj"><strong class="nc jd">#output:<br/></strong>0.91</span></pre><p id="c7b2" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">可视化训练集结果</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/1a57d70e28ef1065850003688904b4c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ExpjR4ROoNGMFBYfETqexg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="b760" class="li lj it nc b gy ng nh l ni nj">plt.title('Classifier NBC (Training set)')<br/>plt.xlabel('Age')<br/>plt.ylabel('Estimated Salary')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a90c1c2f192705a9204f00fa7c7e8118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*JB_gTjOpHC7qe7ntxjqdlQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">训练集的分类。作者的照片</figcaption></figure><p id="e102" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">可视化测试集结果</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/9f1dea6216afc3c8efb6aeef6261f752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ELT1Nf_BR4bsgePfJ2q_dg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="c71a" class="li lj it nc b gy ng nh l ni nj">plt.title('Classifier NBC(Test set)')<br/>plt.xlabel('Age')<br/>plt.ylabel('Estimated Salary')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/39dc4bd805a36a0561800fda418ec888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*0U6slFGJRryrxnkRjoikww.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">测试集的分类。作者的照片</figcaption></figure><h2 id="531b" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated"><strong class="ak">结论</strong></h2><p id="3129" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated">本文简要介绍了朴素贝叶斯分类监督机器学习算法，该算法比其他分类算法，如逻辑斯蒂和KNN等，给出更准确的结果。</p><p id="1349" class="pw-post-body-paragraph md me it mf b mg mw kd mi mj mx kg ml lr my mn mo lv mz mq mr lz na mt mu mv im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae lh" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="62ab" class="oh lj it bd lk oi oj ok ln ol om on lq ki oo kj lu kl op km ly ko oq kp mc or bi translated">推荐文章</h1><p id="5488" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml lr mm mn mo lv mp mq mr lz ms mt mu mv im bi translated"><a class="ae lh" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄用Python </a> <br/> 2。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a> <br/> 3。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/deep-learning-88e218b74a14?source=friends_link&amp;sk=540bf9088d31859d50dbddab7524ba35">为什么LSTM在深度学习方面比RNN更有用？</a> <br/> 5。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88?source=friends_link&amp;sk=6844935e3de14e478ce00f0b22e419eb">神经网络:递归神经网络的兴起</a> <br/> 6。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python充分解释了线性回归</a> <br/> 7。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/differences-between-concat-merge-and-join-with-python-1a6541abc08d?source=friends_link&amp;sk=3b37b694fb90db16275059ea752fc16a">concat()、merge()和join()与Python </a> <br/> 9的区别。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a> <br/> 10。<a class="ae lh" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>