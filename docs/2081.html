<html>
<head>
<title>Image Synthesis and Editing from Hand-Drawn Color Strokes: SDEdit. No more tedious training is needed!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">手绘彩色笔画的图像合成和编辑。不再需要繁琐的训练！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/image-synthesis-and-editing-from-hand-drawn-color-strokes-sdedit-8da8592182cb?source=collection_archive---------2-----------------------#2021-08-10">https://pub.towardsai.net/image-synthesis-and-editing-from-hand-drawn-color-strokes-sdedit-8da8592182cb?source=collection_archive---------2-----------------------#2021-08-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f88f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a></h2><div class=""/><div class=""><h2 id="ea13" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">告别复杂的GAN和变压器架构，实现图像生成。这种新方法可以从任何基于用户的输入生成新图像。</h2></div><blockquote class="kr ks kt"><p id="4410" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">原载于<a class="ae lr" href="https://www.louisbouchard.ai/image-synthesis-from-sketches/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/image-synthesis-from-sketches/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/e87ea8db010fe7bccef28f1f4f7bc7e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O9z3VBmFfs49O4r1ZzOJvQ.png"/></div></div></figure><p id="bcaf" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">告别复杂的GAN和变压器架构，实现图像生成。这种新方法通过改变来自斯坦福大学和卡耐基梅隆大学的孟等人可以从任何基于用户的输入中生成新的图像。即使像我这样没有艺术技能的人，现在也可以从快速草图中生成美丽的图像或修改。起初听起来可能很奇怪，但通过在输入中添加噪声，他们可以消除不需要的伪像，如用户编辑，同时保留图像的整体结构。所以图像现在看起来像这样，完全的噪声，但我们仍然可以看到图像和笔画的一些形状，以及特定的颜色。然后，这个新的噪声输入被发送到模型，以反转这个过程，并按照这个整体结构生成图像的新版本。这意味着它将遵循图像的整体形状和颜色，但不是如此精确，以至于它可以创建新的功能，如用看起来真实的胡子替换这张草图。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mh"><img src="../Images/cd6260ba1494b2bb5bf8e434e12f445c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XV2NzSo_rHBhgrgMg8uhHQ.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">SDEdit示例。图片来自<a class="ae lr" href="https://chenlin9.github.io/SDEdit/" rel="noopener ugc nofollow" target="_blank"> SDEdit，孟等，2021 </a></figcaption></figure><p id="0525" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">同样的，你可以像这样发送一个完整的图像草稿，给它添加噪点，它会通过模拟相反的步骤来去除噪点。这样，它将逐渐提高从任何输入中生成的符合特定数据集样式的图像的质量！这就是为什么你不再需要任何绘画技巧了！因为它从噪声中生成图像，所以在应用噪声之前，它不知道也不需要知道初始输入。这是一个很大的区别，也是一个巨大的优势。你训练一个模型从一种风格到另一种风格，这些图像对来自两个不同但相关的数据集。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mm"><img src="../Images/50d63449999e8bbe36da9f62223c0e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*2w1XW4LIoMQGDSUTdIeW_g.gif"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">GANs生成过程。需要两个数据集:真实人脸和卡通风格</figcaption></figure><p id="0a39" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这个名为SDEdits的模型使用随机微分方程或SDEs，这意味着通过注入高斯噪声，他们将任何复杂的数据分布转换为已知的先验分布。这个已知的分布在训练期间被看到，并且这是模型被训练以重建图像的基础。因此，该模型学习如何将这种高斯噪声转换成噪声更小的图像，并重复它，直到我们得到符合所需风格的图像。这种方法适用于任何类型的输入，因为如果你给它添加足够的噪声，图像将变得如此嘈杂，以至于它加入了已知的分布。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mm"><img src="../Images/0368c425154799c9f842e4e906f4abe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*AHpINTkEsRLOBPyoOHQ4Dw.gif"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">SDEdit扰动(噪声附加)。图片来自<a class="ae lr" href="https://chenlin9.github.io/SDEdit/" rel="noopener ugc nofollow" target="_blank"> SDEdit，孟等，2021 </a></figcaption></figure><p id="977d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">然后，该模型可以接受这个已知的输入并执行相反的步骤，根据它被训练的内容对图像进行去噪。事实上，就像GANs一样，我们需要一个目标数据集，也就是我们想要生成的那种数据或图像。例如，要生成真实的人脸，我们需要一个充满真实人脸的数据集。然后，我们向这些人脸图像添加噪声，并教会模型迭代地去噪。这就是这个模型的美妙之处，因为一旦它学会了如何去噪，我们就可以在添加噪声之前对图像做任何事情，比如添加笔画，因为它们混合在我们添加的噪声的预期图像分布中。通常，基于这样的笔画编辑图像对于g an架构来说是一项具有挑战性的任务，因为这些笔画与图像以及模型在训练期间所看到的极其不同。GAN架构需要两个数据集来解决这个问题，一个是目标数据集，它是我们试图模仿的数据集，另一个是源数据集，它是我们试图编辑的带有笔画的图像。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mm"><img src="../Images/e0aedb5f0891e22dee50f82666179fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*c9E_0CSrr0Pvgb7jXCx1AA.gif"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">具有条件变迁的数据的复杂性问题</figcaption></figure><p id="10af" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这些被称为配对数据集，因为我们需要每个图像在两个数据集中成对出现，以训练我们的模型。我们还需要定义一个适当的损失函数来训练它，这使得图像合成过程非常昂贵和耗时。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mn"><img src="../Images/8bd58faeed22e308d3e7577fd9868297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wf_6BUMULUGcBWQzkfQBWw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">SDEdit扰动(噪声添加)和反向(噪声抑制)过程。图片来自<a class="ae lr" href="https://chenlin9.github.io/SDEdit/" rel="noopener ugc nofollow" target="_blank"> SDEdit，孟等，2021 </a></figcaption></figure><p id="dd29" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">在我们的例子中，对于SDEdits，我们不需要任何成对的数据，因为笔画和图像样式由于这种噪声而被合并。这使得新的噪声图像成为模型的已知数据的一部分，模型使用它来生成与训练数据集非常相似的新图像，但是考虑了新的结构。换句话说，它可以很容易地将任何编辑过的图像作为输入，足够模糊，但不会太模糊，以保持全局语义和结构细节，并对其进行降噪，以产生一个新的图像，它会神奇地将您的编辑考虑在内。而且这个模型甚至没有用笔画或者编辑例子训练过，只有原始的人脸图像！当然，在简单的用户编辑的情况下，他们精心设计了架构，只生成编辑过的部分，不重新创建整个画面。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi mo"><img src="../Images/a681a215df14180492a0f97530d32313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-RnlqqDLLffJGzLW.png"/></div></a></figure><p id="074b" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这是非常酷的，因为它使诸如条件图像生成、基于笔划的图像合成和编辑、图像修补、着色和其他逆问题的应用能够使用单个无条件模型来解决，而无需重新训练。当然，这仍然只适用于一个生成样式，也就是它被训练的数据集。然而，它仍然是一个很大的优势，因为你只需要一个数据集，而不是多个基于GAN的图像修复网络的相关数据集，正如我们所讨论的。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mp"><img src="../Images/905b3b78a6fd2ce5f7922f6a98dfb752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Qnb_9LC1WiW6ANVsoOsNog.gif"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk translated">SDEdit示例。图片来自<a class="ae lr" href="https://chenlin9.github.io/SDEdit/" rel="noopener ugc nofollow" target="_blank"> SDEdit，孟等，2021 </a></figcaption></figure><p id="1fad" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">唯一的缺点可能是生成新图像所需的时间，因为这个迭代过程比单次通过更传统的基于GAN的生成模型花费更多的时间。尽管如此，我宁愿等待几秒钟来获得一个图像的好结果，而不是实时模糊失败。你可以用他们公开发布的代码或者用他们网站上的演示来尝试一下，两者都在参考资料中有链接。</p><p id="efee" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">让我知道你认为这个模型怎么样。我很期待看到这种基于SDE的方法在几个月甚至更短的时间内会发生什么！如你所知，这只是这一惊人新技术的概述。我强烈邀请您阅读他们的论文，以更好地了解SDEdit，链接如下。</p><p id="cd94" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">感谢您的阅读！</p><h2 id="3a2b" class="mq mr it bd ms mt mu dn mv mw mx dp my me mz na nb mf nc nd ne mg nf ng nh iz bi translated">观看视频，查看更多示例，并在YouTube上支持我</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="ee97" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)，并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">简讯</strong> </a>！</p><h2 id="f1e2" class="mq mr it bd ms mt mu dn mv mw mx dp my me mz na nb mf nc nd ne mg nf ng nh iz bi translated">支持我:</h2><ul class=""><li id="0138" class="nk nl it kx b ky nm lb nn me no mf np mg nq lq nr ns nt nu bi translated">支持我的最好方式是成为这个网站<strong class="kx jd"> </strong>的成员，或者如果你喜欢视频格式，在<a class="ae lr" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> YouTube </strong> </a> <strong class="kx jd"> </strong>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="0abb" class="nk nl it kx b ky nv lb nw me nx mf ny mg nz lq nr ns nt nu bi translated">在经济上支持我在<a class="ae lr" href="https://www.patreon.com/whatsai" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">地区</strong> </a>的工作</li><li id="0b7a" class="nk nl it kx b ky nv lb nw me nx mf ny mg nz lq nr ns nt nu bi translated">在<a class="ae lr" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kx jd">中</strong> </a>跟我来</li></ul><h2 id="8a26" class="mq mr it bd ms mt mu dn mv mw mx dp my me mz na nb mf nc nd ne mg nf ng nh iz bi translated">参考资料:</h2><p id="62e8" class="pw-post-body-paragraph ku kv it kx b ky nm kd la lb nn kg ld me oa lg lh mf ob lk ll mg oc lo lp lq im bi translated">阅读全文:<a class="ae lr" href="https://www.louisbouchard.ai/image-synthesis-from-sketches/" rel="noopener ugc nofollow" target="_blank">https://www.louisbouchard.ai/image-synthesis-from-sketches/</a><br/>我的时事通讯(每周向你的电子邮件解释一个新的人工智能应用！):【https://www.louisbouchard.ai/newsletter/<br/>SDEdit，孟等人，2021，<a class="ae lr" href="https://arxiv.org/pdf/2108.01073.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2108.01073.pdf</a><br/>项目链接:<a class="ae lr" href="https://chenlin9.github.io/SDEdit/" rel="noopener ugc nofollow" target="_blank">https://chenlin9.github.io/SDEdit/</a><br/>代码:<a class="ae lr" href="https://github.com/ermongroup/SDEdit" rel="noopener ugc nofollow" target="_blank">https://github.com/ermongroup/SDEdit</a><br/>演示:<a class="ae lr" href="https://colab.research.google.com/drive/1KkLS53PndXKQpPlS1iK-k1nRQYmlb4aO?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1k kls 53 pndxkqpppls 1 ik-k 1 nrqymlb 4 ao？usp =共享</a></p></div></div>    
</body>
</html>