<html>
<head>
<title>All About Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于决策树的一切</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/all-about-decision-tree-c252e0612812?source=collection_archive---------1-----------------------#2022-04-04">https://pub.towardsai.net/all-about-decision-tree-c252e0612812?source=collection_archive---------1-----------------------#2022-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="76a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，我们将通过回答以下问题来理解决策树:</p><ul class=""><li id="d860" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">什么是决策树？</li><li id="fe65" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">决策树的核心概念是什么？</li><li id="1082" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">在分类的情况下，决策树中使用的术语有哪些？</li><li id="d59a" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">回归情况下决策树中使用的术语是什么？</li><li id="addc" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">决策树的优点和缺点是什么？</li><li id="3206" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">如何用Scikit-learn实现决策树？</li></ul><h1 id="558d" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">什么是决策树？</strong></h1><p id="19de" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">决策树是监督机器学习中最强大和最重要的算法之一。该算法非常灵活，因为它可以解决回归和分类问题。此外，如果你有编程背景，决策树算法背后的核心概念非常容易理解。因为决策树模仿嵌套的if-else结构来创建树并预测结果。</p><p id="59c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在深入研究决策树算法之前，我们必须了解一些与树相关的基本术语。</p><h2 id="f675" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">在计算机科学领域，树是什么意思？</strong></h2><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/178d214840589892f072bfd3fca68a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*BXKqAfrToHkYNFcMQvrN6w.png"/></div></figure><p id="1f39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">树是计算机科学中使用的非线性数据结构，表示分层数据。它由保存数据的节点和连接这些节点的边组成。最上面的或开始的节点称为根节点。末端节点称为叶节点。具有从其到任何其他节点的分支的节点被称为父节点，并且父节点的直接子节点被称为子节点。看下面的图片可以更好的理解在计算机科学中树是什么样子的。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/5cbaa6aa41c9fabf6d72b4a109e5b343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*54AFwYcU4iq7-J8QL8Rn2w.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">数据结构中的树</figcaption></figure><h1 id="6532" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">决策树的核心概念是什么？</strong></h1><p id="ea36" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">为了理解核心概念，让我们举个例子。考虑下表:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/969e3b69f6b9613c23edf8e2d557997a.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*wm23efGzOfnjHcXay2Zgcg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">资料组</figcaption></figure><p id="a1f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该表有一个相关特征，即儿童是否会在外面玩耍，用“玩耍”表示。和3个独立特征，即天气、时间和在家的父母。如果我们试着写一个程序来估计一个孩子是否在外面玩耍的结果，仅仅通过观察桌子。那么程序看起来会像这样:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ng"><img src="../Images/42707aa6360d27c506e2271d324fe212.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*HEIm-NOXMjp4KNcdf1N1fQ.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">python程序</figcaption></figure><p id="e4a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，如果我们使用这个程序并尝试构建一个决策树，它看起来会像这样:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f3dce6394d227a1b2b3ba890da79ab6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*FeqdbFnhT1LRRvgDN8zPZg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">决策图表</figcaption></figure><p id="e780" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以观察到，决策树只是一个嵌套的if-else语句，以树的格式描述。</p><p id="a2b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们理解决策树的核心概念时，我们偶然发现了一个新问题，即我们如何安排独立特征的顺序来生成决策树，就像在上面的例子中，我们如何知道我们必须首先采用“天气”特征，然后是“时间”，然后是“父母在家”特征。要回答这个问题，我们必须先了解一些统计术语，我们将在下面讨论。</p><h1 id="0714" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">决策树在分类的情况下使用了哪些术语？</strong></h1><h2 id="9a2e" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">熵:</strong></h2><p id="fd16" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">熵是数据随机性的度量。换句话说，它给出了数据集中存在的杂质。它有助于计算决策树中的信息增益。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1f2a2c5b3d333a37c219ad933a3aa511.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*2iEb7_lWCbHSqPaCQzAKZg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">熵公式</figcaption></figure><h2 id="deba" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">基尼杂质:</strong></h2><p id="fb9b" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">基尼系数也和熵一样用来衡量随机性。基尼系数的公式是:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/5561702a807dcf1fc9ebb39f5f7e3c73.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*mpCvfOlK4OGufq1xoW_MmA.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">基尼杂质</figcaption></figure><blockquote class="nk nl nm"><p id="b1fa" class="jn jo nn jp b jq jr js jt ju jv jw jx no jz ka kb np kd ke kf nq kh ki kj kk ij bi translated">唯一的区别是熵在0到1之间，基尼系数在0到0.5之间。</p></blockquote><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/2660534bca0976a3e1b5be5f1a232f39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*ovBMTgXvj3hmDHvqqxm87A.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">图置换基尼杂质和熵</figcaption></figure><h2 id="a48c" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">信息增益:</strong></h2><p id="8cdd" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">信息增益就是分割前后数据集的熵之差。获得的信息越多，去除的熵就越多。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1705263f42943f28a2b1a12536e06314.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*WmNnIOWPCMoe20HhiFTnyQ.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">信息增益</figcaption></figure><p id="7ad7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">具有最高信息增益的列将被分割。然后，决策树以自顶向下的方式应用递归贪婪搜索算法来寻找树的每一层的信息增益。一旦到达叶节点(熵=0)，就不再进行分裂。</p><h2 id="97ed" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">在分类问题的情况下，决策树会发生什么？</strong></h2><p id="ba5b" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我们的主要任务是减少数据的杂质或随机性。在分类问题中，我们使用熵来测量杂质，然后应用分裂并查看信息增益。如果信息增益最高，那么我们将考虑分割。这个过程将递归进行，直到我们到达叶节点或者数据的熵变为零。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nt"><img src="../Images/c7847f45da442dc3501b596b0a8265e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yn31JSplwXWQs525e4cBHg.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">决策图表</figcaption></figure><h1 id="700e" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">在回归的情况下，决策树中使用了哪些术语？</strong></h1><h2 id="4873" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">错误:</strong></h2><p id="bf46" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">就像在分类问题中，决策树通过计算熵或基尼杂质来衡量杂质。在回归中，我们计算方差误差。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b572c3baf67e66d9bcb42f26b999a5ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*1Hwk1kNJoSMhIG2MlhDPew.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">均方误差</figcaption></figure><h2 id="1f57" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">方差减少:</strong></h2><p id="7be3" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">就像在分类问题中一样，决策树测量信息增益。在回归中，我们计算方差减少，它简单地意味着误差减少。我们找到了分割前后数据集误差之间的差异。</p><h2 id="2a0f" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">在回归问题的情况下，决策树会发生什么？</strong></h2><p id="3566" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">在回归问题中，决策树试图识别一组点来绘制决策边界。决策树考虑每一个点，通过计算该点的误差来画出边界。对每个点都进行这个过程，然后从所有误差中考虑最低误差点来画边界。</p><p id="56b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个过程在计算时间方面非常昂贵。因此，决策树选择贪婪方法，在给定的条件下将节点分成两部分。</p><h1 id="f794" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">决策树有什么优点和缺点？</strong></h1><h2 id="1046" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="7cba" class="kl km iq jp b jq lx ju ly jy nv kc nw kg nx kk kq kr ks kt bi translated">决策树是理解和解释起来最简单的算法之一。此外，我们可以想象这棵树。</li><li id="1b82" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">与其他算法相比，决策树需要更少的数据预处理时间。</li><li id="8e4e" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">使用决策树的成本是对数的</li><li id="4123" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">它可用于回归和分类问题。</li><li id="d6fa" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">它能够处理多输出问题。</li></ul><h2 id="6ce7" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">缺点:</strong></h2><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ny"><img src="../Images/e07f3a752f8e2198bfd1f149d05c100a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyM_8RrSNbIkm1wsWLBI0w.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">决策树的过拟合</figcaption></figure><ul class=""><li id="8f68" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">决策树的主要缺点是过度拟合的问题。</li><li id="e51d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">通过引入新的数据点，决策树可能会不稳定，从而导致生成全新的树。</li><li id="387d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">决策树的预测既不平滑也不连续，而是分段常数逼近，如上图所示。所以，他们不擅长外推法。</li></ul><h2 id="05cb" class="mc la iq bd lb md me dn lf mf mg dp lj jy mh mi ln kc mj mk lr kg ml mm lv mn bi translated"><strong class="ak">决策树的缺点有什么解决办法？</strong></h2><ol class=""><li id="3f8d" class="kl km iq jp b jq lx ju ly jy nv kc nw kg nx kk nz kr ks kt bi translated">对于过拟合问题，我们可以通过超调模型来限制决策树的高度、节点或叶子。这个过程被称为树修剪。</li><li id="bbed" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk nz kr ks kt bi translated">为了处理不稳定的决策树，我们可以使用集成技术，如最著名的“随机森林”。</li></ol><h1 id="c660" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">如何用Scikit-learn实现决策树？</h1><p id="7275" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">决策树的编码部分非常简单，因为我们使用的是scikit-learn包，我们只需从中导入决策树模块。</p><p id="4c8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们将从python导入所需的模块。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/1a10826548b3eb0da1b902fe95b42e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*-bjsy5rPqtM-cnF0IneSxw.png"/></div></figure><p id="24ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个例子中，我们使用了来自GitHub的关于葡萄酒质量的数据集。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ob"><img src="../Images/171c48a098107ac8d6b37b4ac5e48e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-K2ved5LPcIGVU_MU1dNoA.png"/></div></div></figure><p id="94bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于数据集没有空值，我们将简单地把它分成训练集和测试集。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oc"><img src="../Images/5deaab975849800a61464b3b87672ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHM_7bn0-a2N6DGb2RBbzA.png"/></div></div></figure><p id="70e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，只需训练模型。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/1bc5c0ed62c4722e885e7c1ae216ba4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*AdGlAGFNFfoY2O4aPzaywA.png"/></div></figure><p id="eac5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">准确度分数:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/19794c64fe66af320fd8d10e8f53ca0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*1zlkPyM7GyIlJGKuDMoYeg.png"/></div></figure><p id="184b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">绘制图表:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/fb502dc43caabee40e3ded3eaca3c6ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*A2AM0H0oIti4iMd5nGBA6A.png"/></div></figure><p id="0e44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想在编码部分探索更多，或者想知道我如何调优超参数。然后，请点击下面的Github资源库链接。</p><div class="og oh gp gr oi oj"><a href="https://github.com/Akashdawari/Articles_Blogs_Content/blob/main/All_About_Decision_tree.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd ir gy z fp oo fr fs op fu fw ip bi translated">Articles _ Blogs _ Content/All _ About _ Decision _ tree . ipynb at main Akashdawari/Articles _ Blogs _ Content</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">这个知识库包含了jupyter关于博客中发表的文章的笔记本。…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">github.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox mu oj"/></div></div></a></div><p id="ccea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">喜欢并分享如果你觉得这篇文章有帮助。还有，关注我的medium，了解更多机器学习和深度学习相关的内容。</p></div></div>    
</body>
</html>