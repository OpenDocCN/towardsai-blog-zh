<html>
<head>
<title>Produce Amazing Artworks with Text and Sketches!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用文字和草图创作出令人惊叹的艺术品！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/produce-amazing-artworks-with-text-and-sketches-128715500b84?source=collection_archive---------4-----------------------#2022-07-19">https://pub.towardsai.net/produce-amazing-artworks-with-text-and-sketches-128715500b84?source=collection_archive---------4-----------------------#2022-07-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="24f2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">“制造一个场景”:文本和素描条件下的图像生成之间的奇妙融合。</h2></div><blockquote class="ki kj kk"><p id="1960" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">最初发表于<a class="ae li" href="https://www.louisbouchard.ai/make-a-scene/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae li" href="https://www.louisbouchard.ai/make-a-scene/" rel="noopener ugc nofollow" target="_blank">我的博客上读到的！</a></p></blockquote><h2 id="f4ae" class="lj lk it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">观看视频，了解更多结果！</h2><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="8962" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">这是大闹一场。这不是“又一场闹剧”。这个新模型的目标不是像dalle那样允许用户根据文本提示生成随机图像——这真的很酷——而是限制了用户对图像生成的控制。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/d516d540fe813aeaece8e5c860bb32aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Dvv_-8gu8eJJfbKiILCw8w.gif"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">图片来自<a class="ae li" href="https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation/" rel="noopener ugc nofollow" target="_blank"> Meta的博文</a>。</figcaption></figure><p id="f5eb" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">相反，Meta希望推动创造性表达向前发展，将这种文本到图像的趋势与以前的草图到图像的模式相结合，导致“制作场景”:文本和草图条件下的图像生成之间的奇妙融合。这仅仅意味着，使用这种新方法，你可以快速勾画出一只猫，并写出你想要的图像，图像生成过程将遵循草图和文本的指导。它让我们更接近于能够在几秒钟内生成我们想要的完美插图。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi mx"><img src="../Images/e68342fc0091683efbd78a60f4d06251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-zSr7ci7_ghT3GV_.png"/></div></a></figure><p id="04e4" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">你可以把这种多模态生成人工智能方法看作一个Dalle模型，对代有更多的控制，因为它也可以把一个快速草图作为输入。这就是为什么我们称之为多模态:因为它可以接受多种模态作为输入，比如文本和图像，在这种情况下是草图，而Dalle只接受文本来生成图像。多模态模型是一种非常有前途的东西——特别是如果我们匹配我们在网上看到的结果的质量——因为我们对结果有更多的控制，更接近一个非常有趣的最终目标，即在没有任何设计技能的情况下生成我们心目中的完美图像。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi my"><img src="../Images/40966e6e3ce23c10f61f9e9dc49475bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mq7YTHjfTItkLg8dIOD8cg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">论文中的更多结果。</figcaption></figure><p id="f6a4" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">当然，这还处于研究状态，是一个探索性的AI研究概念。这并不意味着我们看到的是不可实现的。这只是意味着它将需要更多的时间来达到公众。</p><p id="7bd9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">该领域的进展非常快，如果我很快看到它的现场直播或其他人的类似模型，我不会感到惊讶。我相信这样的草图和基于文本的模型甚至更有趣，特别是对于行业来说，这就是为什么我想在我的频道上报道它，即使结果有点落后于我们在网上看到的Dalle 2。不仅对这个行业，对艺术家也是如此。有些人使用草图功能生成了比Dalle更意想不到的结果。</p><p id="b603" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">我们可以要求它生成一些东西，并绘制一个不代表这个特定事物的形式，如绘制一个花朵形状的水母，这可能不是不可能用dalle实现的，但如果没有草图指导，情况会复杂得多，因为模型只会复制它从现实世界的图像和插图中学到的东西。</p><p id="a0bb" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">所以主要的问题是，他们如何同时用文本输入(如Dalle)和草图来指导一代人，并让模型遵循这两个指导方针？嗯，它与Dalle的工作方式非常非常相似，所以我不会过多地进入生成模型的细节，因为在过去的两个月里我已经介绍了至少五种不同的方法，如果你还没有看过这些方法，你绝对应该看看，因为这些模型像<a class="ae li" href="https://youtu.be/rdGVbPI42sA" rel="noopener ugc nofollow" target="_blank"> Dalle 2 </a>或<a class="ae li" href="https://youtu.be/qhtYPhPWCsI" rel="noopener ugc nofollow" target="_blank"> Imagen </a>都非常棒。</p><p id="8561" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">通常情况下，这些模型需要数百万个训练样本来学习如何从文本中生成图像，这些文本带有从互联网上删除的图像及其标题形式的数据。</p><p id="f71a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">在这里，在训练期间，不是仅仅依赖于标题，生成我们图像的第一个版本并将其与实际图像进行比较，并对我们所有的图像重复这个过程无数次，我们还会给它一个草图。最酷的是，草图很容易生成用于训练:只需使用一个可以在线下载的预训练网络，并执行实例分割。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mz"><img src="../Images/fe1589be14ff591581fc8602f7a6d09a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cGjFv1IF9cG2MMQ2lAVneA.png"/></div></div></figure><p id="19c0" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">对于那些想知道细节的人来说，他们在Imagenet上使用一个免费的预训练VGG模型，所以与今天的网络相比，这是一个相当小的网络，超级准确和快速，产生这样的结果(见上图)…</p><p id="83a3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">称为分割图。他们只需对所有图像进行一次处理，然后获得这些地图来训练模型。</p><p id="8205" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">然后，使用这个地图以及标题来确定模型的方向，以生成初始图像。在推理的时候，或者当我们中的一个人使用它的时候，我们的草图会取代那些地图。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi na"><img src="../Images/456633ca3b9b4b720be9c616d8a33b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIt0PFrcTS08a5qCkWGt6Q.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">场景制作方法概述。图片来自报纸。</figcaption></figure><p id="b6c2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">如我所说，他们用一个叫VGG的模特来制作假的训练草图。他们在图像生成过程中使用了变压器架构，这与<a class="ae li" href="https://youtu.be/rdGVbPI42sA" rel="noopener ugc nofollow" target="_blank"> Dalle-2 </a>不同，如果您想了解更多关于它如何处理和生成图像的细节，我邀请您观看<a class="ae li" href="https://youtu.be/QcCJJOLCeJQ" rel="noopener ugc nofollow" target="_blank">我制作的视频</a>来介绍用于视觉应用的变压器。</p><p id="f984" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">这种草图导向的转换器是与Make-A-Scene的主要区别，同时不使用CLIP这样的图像-文本排序器来测量文本和图像对，这也可以在<a class="ae li" href="https://youtu.be/rdGVbPI42sA" rel="noopener ugc nofollow" target="_blank"> my Dalle video </a>中了解到。相反，所有编码的文本和分割图都被发送到Transformer模型。该模型生成相关的图像令牌，由相应的网络编码和解码，主要用于产生图像。在训练期间使用编码器来计算产生的图像和初始图像之间的差异，但是只需要解码器来获取该变换器输出并将其变换成图像。</p><p id="973a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">瞧！</p><p id="4c0a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">这就是Meta的新模型如何能够获取草图和文本输入，并从中生成高清图像，从而对高质量的结果进行更多控制。正如他们所说，这只是这种新型人工智能模型的开始。这些方法将在质量和对公众的可用性方面不断改进，这是非常令人兴奋的。</p><p id="0cdf" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">正如Meta的博客文章所描述的，许多艺术家已经在他们自己的作品中使用该模型，我对我们什么时候也能使用它感到兴奋。</p><p id="f4b5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">他们的方法不需要任何编码知识，只需要一手好的草图和一些及时的工程设计，这意味着通过文本输入的反复试验来调整公式和单词，以产生不同的更好的结果。</p><p id="57e8" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">当然，这只是一个新的场景制作方法的概述，我邀请你阅读下面链接的完整论文，以获得它如何工作的完整概述。</p><p id="69f5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">我希望你喜欢这篇文章，下周我会带着另一篇精彩的论文来看你！</p><p id="8624" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">路易斯（号外乐团成员）</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h2 id="d02a" class="lj lk it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">参考</h2><p id="9caa" class="pw-post-body-paragraph kl km it ko b kp ni ju kr ks nj jx ku ls nk kx ky lw nl lb lc ma nm lf lg lh im bi translated">梅塔的博文:<a class="ae li" href="https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation" rel="noopener ugc nofollow" target="_blank">https://ai . Facebook . com/blog/greater-creative-control-for-ai-image-generation</a><br/>论文:Gafni，o .，Polyak，a .，Ashual，o .，Sheynin，s .，Parikh，d .，Taigman，y .，2022。有人类先验的基于场景的文本到图像生成。<a class="ae li" href="https://arxiv.org/pdf/2203.13131.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2203.13131.pdf</a><br/>【我的时事通讯(一个新的人工智能应用每周向你的电子邮件解释！):<a class="ae li" href="https://www.louisbouchard.ai/newsletter/" rel="noopener ugc nofollow" target="_blank">https://www.louisbouchard.ai/newsletter/</a></p></div></div>    
</body>
</html>