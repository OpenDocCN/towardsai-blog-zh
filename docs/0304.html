<html>
<head>
<title>Multi-lingual Language Model Fine-tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多语言语言模型微调</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/multi-lingual-language-model-fine-tuning-81922a80438f?source=collection_archive---------0-----------------------#2020-02-05">https://pub.towardsai.net/multi-lingual-language-model-fine-tuning-81922a80438f?source=collection_archive---------0-----------------------#2020-02-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dbe4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">低资源语言的问题</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/04a3fd4ae186a9b767a8907567807e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9_D5OQ5ZJFUww8xv"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@chloeevans?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克洛伊·伊文斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="459b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">英语是自然语言处理领域最丰富的资源之一。许多最先进的自然语言处理模型天生支持英语。为了解决多语言下游问题，提出了<a class="ae ky" href="https://medium.com/towards-artificial-intelligence/cross-lingual-language-model-56a65dba9358" rel="noopener">跨语言模型</a> ( <code class="fe lv lw lx ly b">XLM</code>)和其他解决方案。</p><p id="ab33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，当目标语言具有非常有限的训练数据时，仍然存在挑战。Eisenschlo等人提出了多语种语言模型微调(<code class="fe lv lw lx ly b">MultiFiT</code>)来使我们能够有效地训练目标语言。</p><h1 id="5cbc" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">多重拟合</h1><p id="1d04" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated"><code class="fe lv lw lx ly b">MultiFiT</code> (Eisenschlo等人，2019)旨在解决低资源语言问题。神经网络架构基于<a class="ae ky" href="https://towardsdatascience.com/multi-task-learning-in-language-model-for-text-classification-c3acc1fedd89" rel="noopener" target="_blank">通用语言模型微调</a> (ULMFiT) (Howard和Ruder，2018)和准递归神经网络(QRNN) (Bradbury et al .，2017)。对于文本表示，应用<a class="ae ky" href="https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46" rel="noopener">子词</a> (Kudo，2018)对词进行分词。</p><p id="5b60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">QRNN (Bradbury等人，2017)不同于长短期记忆(LSTM) (Hochreiter和Schmidhuber，1997)和卷积神经网络(CNN)(Krizhevsky等人，2012)。卷积层和池层分别跨时间戳和通道并行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/d9278d873a2b1ec01762b45a349d60a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wR_HNt0jcZKPUSUJbAmYjQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">LSTM、CNN和QRNN之间的视觉比较(Bradbury等人，2017年)</figcaption></figure><p id="6754" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ULMFiT (Howard和Ruder，2018年)包括从源数据到目标数据训练模型的3个步骤。首先，在源数据上从头开始训练模型。微调LM模型并在目标数据上训练分类器层。你可以访问这个<a class="ae ky" href="https://towardsdatascience.com/multi-task-learning-in-language-model-for-text-classification-c3acc1fedd89" rel="noopener" target="_blank">故事</a>来了解更多关于ULMFiT的细节。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/9aacc70c9cb57c99e33c7b9d2c1ed428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y5hbdkiGjTXoJtWh.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">乌尔姆菲特建筑(霍华德和鲁德，2018年)</figcaption></figure><p id="415b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最初的ULMFiT (Howard和Ruder，2018年)使用了平均随机梯度下降权重长期短期记忆(AWD-LSTM)，而由于性能和效率的提高，它被QRNN (Bradbury等人，2017年)取代。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/efcf31508c03edcefa9ed0728524e211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*26oc-mGn9RO3fYLL-4qkyg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">多iT架构(Eisenschlo等人，2019年)</figcaption></figure><p id="7ed5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外</p><p id="6e28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该培训包括5个步骤:</p><ul class=""><li id="bdfd" class="mz na it lb b lc ld lf lg li nb lm nc lq nd lu ne nf ng nh bi translated">激光分类器:使用预先训练的模型在源语言数据上训练分类器层</li><li id="428f" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">目标语言预测标签:将目标语言数据提供给激光分类器</li><li id="be82" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">预翻译的LM:使用目标语言维基百科为目标语言训练一个LM模型</li><li id="d11a" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">微调LM:通过输入目标语言数据来微调LM模型。</li><li id="5c42" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">零镜头分类器:将目标语言预测标签和目标语言文档提供给微调的LM。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/6cd18bd945121c7b3b9d0e8fa8d52dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7bYLcrttceqLcXOnWSstQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">MultiFiT的训练步骤(Eisenschlo等人，2019年)</figcaption></figure><p id="d87a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从下面的实验结果来看，<code class="fe lv lw lx ly b">MultiFiT</code>的表现优于多种语言，尤其是中文(ZH)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/bd887334e7b61832d7a214a95c1207a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kpqmy1O1MnZdTKMTqp3QA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">模型性能比较(Eisenschlo等人，2019年)</figcaption></figure><h1 id="2fa5" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">拿走</h1><ul class=""><li id="c573" class="mz na it lb b lc mr lf ms li np lm nq lq nr lu ne nf ng nh bi translated">从实验来看，在预训练阶段和微调阶段，QRNN的MultiFiT比AWD-LSTM的MultiFit分别快大约2倍和3倍。</li><li id="e779" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">预训练LM模型很重要，因为它提高了对噪声的鲁棒性。</li></ul><h1 id="c310" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">喜欢学习？</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">我是湾区的数据科学家。关注数据科学的最新发展，尤其是NLP、数据扩充和平台相关领域。在<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上随时与<a class="ae ky" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>联系。</p><h1 id="be7f" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">延伸阅读</h1><ul class=""><li id="16db" class="mz na it lb b lc mr lf ms li np lm nq lq nr lu ne nf ng nh bi translated"><a class="ae ky" href="https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46" rel="noopener"> 3子字算法解释</a></li><li id="8164" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://github.com/n-waves/ulmfit-multilingual/tree/master/ulmfit" rel="noopener ugc nofollow" target="_blank"> MultiFit实现</a></li><li id="5c66" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://medium.com/towards-artificial-intelligence/cross-lingual-language-model-56a65dba9358" rel="noopener"> XLM实施</a></li><li id="e335" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://towardsdatascience.com/multi-task-learning-in-language-model-for-text-classification-c3acc1fedd89" rel="noopener" target="_blank"> ULMFiT实施</a></li></ul><h1 id="1470" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">参考</h1><ul class=""><li id="8723" class="mz na it lb b lc mr lf ms li np lm nq lq nr lu ne nf ng nh bi translated">J.Bradbury，S. Merity，C. Xiong和R. Socher。<a class="ae ky" href="https://arxiv.org/pdf/1611.01576.pdf" rel="noopener ugc nofollow" target="_blank">准递归神经网络。</a> 2017</li><li id="8f1f" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">J.霍华德和史路德。<a class="ae ky" href="https://arxiv.org/pdf/1801.06146.pdf" rel="noopener ugc nofollow" target="_blank">文本分类通用语言模型微调</a>。2018</li><li id="50a8" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">J.艾森施洛斯、s .鲁德、p .恰普拉、m .卡尔达斯、s .古格和j .霍华德。<a class="ae ky" href="https://arxiv.org/pdf/1909.04761.pdf" rel="noopener ugc nofollow" target="_blank"> MultiFiT:高效的多语言语言模型微调</a>。2019</li></ul></div></div>    
</body>
</html>