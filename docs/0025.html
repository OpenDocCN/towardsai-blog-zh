<html>
<head>
<title>Billions of Rows, Milliseconds of Time- PySpark Starter Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数十亿行，毫秒级时间——PySpark入门指南</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/billions-of-rows-milliseconds-of-time-pyspark-starter-guide-c1f984023bf2?source=collection_archive---------0-----------------------#2019-03-08">https://pub.towardsai.net/billions-of-rows-milliseconds-of-time-pyspark-starter-guide-c1f984023bf2?source=collection_archive---------0-----------------------#2019-03-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0b21" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><p id="87c0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">目标受众:具备Python、SQL和Linux应用知识的数据科学家</em></p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/d6da23acf7410392a326e11ea2663729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTDEt8iL5ck37gWHsJ6_-w.png"/></div></div></figure><p id="5ef4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们经常看到下面的错误，然后是终端关闭，接着是对丢失工作的绝望:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi lh"><img src="../Images/20cfd997238457080bf0f04d7c34b59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x4eWyi5eNw-xmR4ZicTgkQ.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk translated">内存错误- Jupyter笔记本</figcaption></figure><p id="5762" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">数据是新的石油，与石油不同的是，数据每天都在增加。数据大小的增长超过了RAM升级的速度/成本，这就需要使用多核、并行处理、分块等技术进行智能数据处理。</p><p id="c390" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">请继续学习，到本文结束时，您将会:</p><ul class=""><li id="632b" class="lm ln iq jy b jz ka kd ke kh lo kl lp kp lq kt lr ls lt lu bi translated">在你的机器上有一个运行的火花环境</li><li id="9a36" class="lm ln iq jy b jz lv kd lw kh lx kl ly kp lz kt lr ls lt lu bi translated">有基本的Pandas到Pyspark数据操作经验</li><li id="472f" class="lm ln iq jy b jz lv kd lw kh lx kl ly kp lz kt lr ls lt lu bi translated">拥有在强健的环境中大规模提升数据操作速度的经验</li></ul><p id="834a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae ma" href="http://spark.apache.org/docs/2.2.0/api/python/pyspark.html" rel="noopener ugc nofollow" target="_blank"> PySpark </a>是使用Spark的Python API，Spark是运行大数据应用的并行分布式引擎。本文试图帮助您立即开始使用PySpark！</p><p id="fc85" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">从CSV文件(33 Mn行，5.7GB大小)读取和聚合的不同技术的性能比较:</p><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="c766" class="mg mh iq mc b gy mi mj l mk ml">1. Reading in Chunks(Pandas)</span><span id="303a" class="mg mh iq mc b gy mm mj l mk ml">mylist = []<br/>for chunk in  pd.read_csv('train.csv', chunksize=10000000):<br/>    mylist.append(chunk)<br/>train = pd.concat(mylist, axis= 0)<br/>del mylist</span><span id="03a5" class="mg mh iq mc b gy mm mj l mk ml">print(train.shape)<br/>train.groupby(['passenger_count'])['fare_amount'].mean().reset_index().head(5)</span><span id="61b8" class="mg mh iq mc b gy mm mj l mk ml"><strong class="mc ja"><em class="ku">Time Taken(2 Mins,25 secs)</em></strong></span><span id="2e06" class="mg mh iq mc b gy mm mj l mk ml">2. Dask</span><span id="6461" class="mg mh iq mc b gy mm mj l mk ml">train = dd.read_csv('train.csv')<br/>train = client2.persist(train)<br/>progress(train)<br/>print("No of rows is ",len(train))<br/>train.groupby(['passenger_count'])['fare_amount'].mean().reset_index().compute().head(5)</span><span id="3145" class="mg mh iq mc b gy mm mj l mk ml"><strong class="mc ja"><em class="ku">Time Taken(18 secs)</em></strong></span><span id="f5be" class="mg mh iq mc b gy mm mj l mk ml">3. PySpark</span><span id="fbfb" class="mg mh iq mc b gy mm mj l mk ml">train = spark.read.csv('train.csv',header=True)<br/>print("No of rows is ",train.count())<br/>train.groupBy(['passenger_count']).agg(mean('fare_amount'),count('fare_amount')).orderBy('passenger_count').filter(train.passenger_count&lt;10).show()</span><span id="9ce2" class="mg mh iq mc b gy mm mj l mk ml"><strong class="mc ja">Time Taken(12 Secs)</strong></span></pre><p id="5df2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">Dask和PySpark的主要速度优势是由于在主从节点设置中利用了机器的所有内核。我将把这篇文章献给Pyspark，并在以后的文章中介绍Dask，同时，你可以在https://docs.dask.org/en/latest/<a class="ae ma" href="https://docs.dask.org/en/latest/" rel="noopener ugc nofollow" target="_blank">阅读更多关于DASK的内容。</a></p><p id="9ef3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">PySpark的主要优势:</p><ul class=""><li id="dfea" class="lm ln iq jy b jz ka kd ke kh lo kl lp kp lq kt lr ls lt lu bi translated">在多个内核上实施进程，因此速度更快</li><li id="ae84" class="lm ln iq jy b jz lv kd lw kh lx kl ly kp lz kt lr ls lt lu bi translated">机器学习和神经网络库的可用性</li><li id="80e7" class="lm ln iq jy b jz lv kd lw kh lx kl ly kp lz kt lr ls lt lu bi translated">熊猫、SQL和RDD之间无缝切换的选项。Spark Dataframe上的SQL操作，让数据工程师在不改变基础语言的情况下，轻松学习ML、神经网络等。</li></ul><p id="97e4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">现在让我们安装PySpark(在我的Mac和EC2(Linux机器)上测试):</p><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="334b" class="mg mh iq mc b gy mi mj l mk ml">a. Install JAVA from <a class="ae ma" href="https://www.oracle.com/technetwork/java/javase/downloads/jdk11-downloads-5066655.html" rel="noopener ugc nofollow" target="_blank">https://www.oracle.com/technetwork/java/javase/downloads/jdk11-downloads-5066655.html</a></span><span id="3c18" class="mg mh iq mc b gy mm mj l mk ml">b.Install PySpark from <a class="ae ma" href="http://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">http://spark.apache.org/downloads.html</a><br/>                       /pip install pyspark</span><span id="050e" class="mg mh iq mc b gy mm mj l mk ml">c. Create a symbolic link: ln -s /opt/spark-2.4.0 /opt/spark̀</span><span id="dec5" class="mg mh iq mc b gy mm mj l mk ml">d. Tell your bash where to find Spark:<br/>          export SPARK_HOME=/opt/spark<br/>          export PATH=$SPARK_HOME/bin:$PATH</span><span id="cd02" class="mg mh iq mc b gy mm mj l mk ml">e. Restart your terminal and type <strong class="mc ja">pyspark</strong> for a spark environment.</span></pre><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mn"><img src="../Images/2fe1bea3c5824667894e6a234c49c632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kApkJDHF7xEAGbsW4c8Vzg.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk translated">终端中的PySpark</figcaption></figure><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="f947" class="mg mh iq mc b gy mi mj l mk ml">#PySpark on Jupyter Notebook</span><span id="e120" class="mg mh iq mc b gy mm mj l mk ml">import pyspark<br/>from pyspark.context import SparkContext<br/>from pyspark.sql.session import SparkSession</span><span id="bb3e" class="mg mh iq mc b gy mm mj l mk ml">sc = SparkContext(('local[30]'))<br/>spark = SparkSession(sc)<br/>spark = SparkSession \<br/>   .builder \<br/>   .appName("PySpark Sample") \<br/>   .config("spark.some.config.option", "some-value") \<br/>   .getOrCreate()<br/></span></pre><p id="f4bc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">既然分布式计算的舞台已经搭好，让我们做一些数据辩论练习。我将使用纽约出租车费用数据集进行进一步分析:<a class="ae ma" href="https://www.kaggle.com/c/new-york-city-taxi-fare-prediction" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/new-york-city-taxi-fare-prediction</a></p><p id="170c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">由于这是一个大数据集，在我们的家庭互联网/PC上下载它似乎并不明智。Kaggle发布了一个API，可以直接用来获取我们虚拟机上的数据集:<a class="ae ma" href="https://github.com/Kaggle/kaggle-api" rel="noopener ugc nofollow" target="_blank">https://github.com/Kaggle/kaggle-api</a></p><p id="9d3a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在继续之前，让我们对PySpark进行一次快速的强度测试，以便不面临数据量增加的问题。在第一次测试中，PySpark可以在38秒内执行15亿行(即约1TB数据)的连接和聚合，在21分钟内执行130亿行(即约60 TB数据)的连接和聚合。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mo"><img src="../Images/acc6365ba63dbc54d7395f455cec1e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AD8LrwS6EQ92xOYmZLSMiQ.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk translated">PySpark重数据提升</figcaption></figure><p id="a170" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">30，000英尺<em class="ku">吉安</em>够了，让我们从数据操作开始，同时关注速度基准的执行时间:</p><h2 id="37c2" class="mg mh iq bd mp mq mr dn ms mt mu dp mv kh mw mx my kl mz na nb kp nc nd ne iw bi translated"><strong class="ak">读取CSV文件:</strong></h2><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="60c1" class="mg mh iq mc b gy mi mj l mk ml"># Read CSV, length of file and header</span><span id="8fff" class="mg mh iq mc b gy mm mj l mk ml">train = spark.read.csv('train.csv',header=True)<br/>print("No of rows is ",train.count())<br/>train.show(2)</span></pre><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nf"><img src="../Images/f538eefa862e64a4c47e0cb9c612c966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4P6I2S0B7bd3NnHHMCdzLw.png"/></div></div></figure><h2 id="64a8" class="mg mh iq bd mp mq mr dn ms mt mu dp mv kh mw mx my kl mz na nb kp nc nd ne iw bi translated"><strong class="ak">显示模式、子集和过滤器:</strong></h2><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="75b4" class="mg mh iq mc b gy mi mj l mk ml">#Subset of Data(1 Million Rows)</span><span id="0af7" class="mg mh iq mc b gy mm mj l mk ml">a=sc.parallelize(train.take(1000000))<br/>train=a.toDF()<br/>train.show(2)</span><span id="1bed" class="mg mh iq mc b gy mm mj l mk ml">#Data Schema<br/>train.printSchema()</span><span id="8656" class="mg mh iq mc b gy mm mj l mk ml">#Subset based on a condition<br/>subset=train.filter(train.passenger_count&lt;10).show(2)</span></pre><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ng"><img src="../Images/a33cd654f7d8797997a60114e4e33688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-E5oOREaEEfv1cjlIREhiQ.png"/></div></div></figure><h2 id="2cf7" class="mg mh iq bd mp mq mr dn ms mt mu dp mv kh mw mx my kl mz na nb kp nc nd ne iw bi translated"><strong class="ak">分组、排序和过滤:</strong></h2><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="2b67" class="mg mh iq mc b gy mi mj l mk ml"># Data Type change<br/>train = train.withColumn("passenger_count",train.passenger_count.cast("integer"))</span><span id="c3ae" class="mg mh iq mc b gy mm mj l mk ml">#GroupBy, Rename and Sort</span><span id="9927" class="mg mh iq mc b gy mm mj l mk ml">train.groupBy(['dt_year']).agg(mean('fare_amount'),count('fare_amount')).alias('mean_fare').orderBy('dt_year').show()</span><span id="592e" class="mg mh iq mc b gy mm mj l mk ml">train.groupBy(['passenger_count']).agg(mean('fare_amount'),count('fare_amount')).orderBy('passenger_count').show()</span></pre><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nh"><img src="../Images/d76dc90c9e368c4a0028107cf589e9bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EmsxsLtXtsYHYSJpRMRihg.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk translated"><strong class="bd mp">分组和排序</strong></figcaption></figure><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ni"><img src="../Images/10a7c601f110816a2fa10000d2a4a018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_R0ca_WH1qsZsAv7AJ46w.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk translated"><strong class="bd mp">过滤器</strong></figcaption></figure><h2 id="4855" class="mg mh iq bd mp mq mr dn ms mt mu dp mv kh mw mx my kl mz na nb kp nc nd ne iw bi translated"><strong class="ak">创建新列并更改数据类型(默认为字符串):</strong></h2><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="5777" class="mg mh iq mc b gy mi mj l mk ml">train =train.withColumn("fare",train.fare_amount.cast("float"))<br/>train = train.withColumn("date",train.pickup_datetime.cast("date"))</span></pre><h2 id="9a3c" class="mg mh iq bd mp mq mr dn ms mt mu dp mv kh mw mx my kl mz na nb kp nc nd ne iw bi translated"><strong class="ak">自定义功能:</strong></h2><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="1681" class="mg mh iq mc b gy mi mj l mk ml">#Custom Function- Haversine Distance between 2 latitudes-Longitudes</span><span id="7ebe" class="mg mh iq mc b gy mm mj l mk ml">df=train</span><span id="81c4" class="mg mh iq mc b gy mm mj l mk ml">def dist(long_x, lat_x, long_y, lat_y):<br/>    return acos(<br/>        sin(toRadians(lat_x)) * sin(toRadians(lat_y)) + <br/>        cos(toRadians(lat_x)) * cos(toRadians(lat_y)) * <br/>            cos(toRadians(long_x) - toRadians(long_y))<br/>    ) * lit(6371.0)</span><span id="b412" class="mg mh iq mc b gy mm mj l mk ml">#df=df.head(10)<br/>df2=df.withColumn("distance", dist(<br/>    "pickup_longitude","pickup_latitude","dropoff_longitude","dropoff_latitude"<br/>).alias("distance"))</span><span id="6142" class="mg mh iq mc b gy mm mj l mk ml">df2.select('pickup_datetime','pickup_longitude',<br/>           'pickup_latitude','dropoff_longitude','dropoff_latitude','distance').show(5)</span></pre><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nj"><img src="../Images/5516f424bcefe58419c71666f4c07304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-Cymx5NT5L864RVJaA4wA.png"/></div></div></figure><h2 id="1c50" class="mg mh iq bd mp mq mr dn ms mt mu dp mv kh mw mx my kl mz na nb kp nc nd ne iw bi translated"><strong class="ak">宁滨:</strong></h2><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="7607" class="mg mh iq mc b gy mi mj l mk ml"># Binning in Pyspark</span><span id="52c7" class="mg mh iq mc b gy mm mj l mk ml">from pyspark.ml.feature import Bucketizer</span><span id="2473" class="mg mh iq mc b gy mm mj l mk ml">bucketizer = Bucketizer(splits=[ 0, 5, 10,15,20,25,30,35,40,float('Inf') ],inputCol="distance", outputCol="buckets")</span><span id="f7cb" class="mg mh iq mc b gy mm mj l mk ml">df2 = bucketizer.setHandleInvalid("keep").transform(df2)</span><span id="8cf2" class="mg mh iq mc b gy mm mj l mk ml">df2.groupBy(['buckets']).agg(mean('fare_amount'),count('fare_amount')).orderBy('buckets',asc=False).show(5)</span></pre><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nk"><img src="../Images/2bab228d8cff84aa2ecea5811a97c1a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXo5bwxqSsauqtT7P13gCA.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk translated">箱柜创建</figcaption></figure><h2 id="2cc9" class="mg mh iq bd mp mq mr dn ms mt mu dp mv kh mw mx my kl mz na nb kp nc nd ne iw bi translated"><strong class="ak">转换成熊猫，写CSV: </strong></h2><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="8745" class="mg mh iq mc b gy mi mj l mk ml">df2.groupBy(['buckets']).agg(mean('fare_amount'),count('fare_amount')).toPandas().to_csv('Spark_To_Pandas.csv')</span></pre><h2 id="b710" class="mg mh iq bd mp mq mr dn ms mt mu dp mv kh mw mx my kl mz na nb kp nc nd ne iw bi translated"><strong class="ak"> SQL操作:</strong></h2><p id="1507" class="pw-post-body-paragraph jw jx iq jy b jz nl kb kc kd nm kf kg kh nn kj kk kl no kn ko kp np kr ks kt ij bi translated">对Spark数据帧执行操作的一种方式是通过Spark SQL，这使得数据帧可以像表一样被查询。</p><pre class="kw kx ky kz gt mb mc md me aw mf bi"><span id="d845" class="mg mh iq mc b gy mi mj l mk ml">#SQL Operations</span><span id="74c1" class="mg mh iq mc b gy mm mj l mk ml">#Convert Spark Data Frame to SQL Table</span><span id="efdf" class="mg mh iq mc b gy mm mj l mk ml">df2.createOrReplaceTempView("sql_table")</span><span id="e6eb" class="mg mh iq mc b gy mm mj l mk ml">#SQL Query<br/>sql_output = spark.sql("""<br/>  select dt_month,pickup_latitude,round(mean(fare_amount),2) as avg_fare<br/>  from sql_table<br/>  group by 1<br/>  order by 1 asc<br/>""")<br/>sql_output.show()</span></pre><p id="c003" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">上面的帖子旨在摆脱我们最初的抗拒，开始一项未来的技术。我只是设法了解了一些皮毛，在分布式计算的各层之下蕴藏着巨大的潜力。在我的下一篇文章中，我将使用Pyspark写关于文本处理、机器学习(MLLib)、图形操作(Graphx)和神经网络的内容。</p><p id="7a9e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果您在设置Spark环境时遇到任何问题，或者无法执行任何PySpark操作，甚至无法进行一般的聊天，请在评论中告诉我。快乐学习！！！</p><h2 id="455a" class="mg mh iq bd mp mq mr dn ms mt mu dp mv kh mw mx my kl mz na nb kp nc nd ne iw bi translated">来源:</h2><ul class=""><li id="435b" class="lm ln iq jy b jz nl kd nm kh nq kl nr kp ns kt lr ls lt lu bi translated"><a class="ae ma" href="https://medium.com/dunder-data/minimally-sufficient-pandas-a8e67f2a2428" rel="noopener">https://medium . com/du nder-data/minimally-sufficult-pandas-a 8 e 67 F2 a 2428</a></li><li id="19e5" class="lm ln iq jy b jz lv kd lw kh lx kl ly kp lz kt lr ls lt lu bi translated"><a class="ae ma" href="https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f" rel="noopener ugc nofollow" target="_blank">https://blog . si cara . com/get-started-pyspark-jupyter-guide-tutorial-AE 2 Fe 84 f 594 f</a></li><li id="3055" class="lm ln iq jy b jz lv kd lw kh lx kl ly kp lz kt lr ls lt lu bi translated"><a class="ae ma" href="https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873" rel="noopener" target="_blank">https://towards data science . com/a-brief-introduction-to-py spark-ff 4284701873</a></li></ul></div></div>    
</body>
</html>