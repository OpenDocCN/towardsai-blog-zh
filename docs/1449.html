<html>
<head>
<title>Traditional Recurrent Neural Networks — Reinforcement Learning Part 1/3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">传统的递归神经网络——强化学习(1/3)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/traditional-recurrent-neural-networks-reinforcement-learning-part-1-3-348aec514bdd?source=collection_archive---------2-----------------------#2021-01-29">https://pub.towardsai.net/traditional-recurrent-neural-networks-reinforcement-learning-part-1-3-348aec514bdd?source=collection_archive---------2-----------------------#2021-01-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/7974861159bf1c111a5e79ad7d18b603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7710ekJXsMBpKl0A"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">来源:<a class="ae jd" href="https://unsplash.com/photos/8bghKxNU1j0" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><h2 id="55bb" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><p id="6165" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个博客将由3部分组成，我将解释不同的强化学习算法，</p><p id="c52a" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第一部分:传统递归神经网络的解释。第二部分:GRUs的解释。第3部分:LSTMs的解释。</p><h1 id="b308" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><strong class="ak">传统递归神经网络(RNN): </strong></h1><h1 id="c6e0" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">简介:</h1><p id="b5f2" class="pw-post-body-paragraph km kn jg ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">递归神经网络是一种人工神经网络(ANN ),其中前一步骤的输出作为当前步骤的输入。RNN主要用于预测问题，比如预测天气、股票市场价格，或者根据前面的单词预测句子中的下一个单词。</p><h1 id="2fac" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">它是如何工作的？？</h1><p id="7748" class="pw-post-body-paragraph km kn jg ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">在了解递归神经网络如何工作之前，让我们了解一下RNN中的权重是如何计算的。计算权重的公式如下:</p><h2 id="97a1" class="mn ll jg bd lm mo mp dn lq mq mr dp lu kx ms mt ly lb mu mv mc lf mw mx mg jm bi translated"><strong class="ak"> <em class="my">新权重=当前权重—(学习率*梯度)</em> </strong></h2><p id="4513" class="pw-post-body-paragraph km kn jg ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated"><strong class="ko jq">权重</strong>:为了便于理解，我们可以说这些数字在乘以输入后可以预测输出。<br/> <strong class="ko jq">学习率:</strong>假设我们正在接近准确的预测，那么学习率会告诉我们在每次迭代中我们朝着正确的解决方案前进了多少步。<br/> <strong class="ko jq">渐变:</strong>告诉我们朝着正确的解决方案前进的方向。</p><h1 id="5fce" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><strong class="ak">直觉理解RNN:</strong></h1><p id="ed49" class="pw-post-body-paragraph km kn jg ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">让我们看看如何将我们的思维方式与RNN的工作方式联系起来，比如你想为你的家买一台空气冷却器，而你正在阅读一台空气冷却器的产品评论。审查的一个例子如下:</p><p id="f859" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="mz">伟大的产品，耗电更少，让房间保持真正的凉爽。肯定会建议你买，竖起大拇指。</em>“—{ 1 }</p><p id="f446" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，在任何时候，我们的大脑都能记住这样的词:<br/>“<em class="mz">伟大的产品……更少的能量……酷..绝对建议..赞同..</em></p><p id="9f0f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果有人问你关于产品的问题，你会说:</p><p id="9a0c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这是一款很棒的产品，耗电少，保持室内凉爽，我绝对建议你购买。</p><p id="4c88" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在让我们看看这种直觉是如何在RNN的哲学中用数学表达出来的。</p><h1 id="4675" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">RNN的数学工作；</h1><p id="f144" class="pw-post-body-paragraph km kn jg ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">在RNN中，对于{1}中的句子，首先将单词转换为机器可读的向量，然后算法逐个处理这些向量。</p><p id="09c5" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们看看RNN的画像。</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi na"><img src="../Images/34de213e533bc3dd71e68e919cf20800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*5kbnd8XELKrsFduwmy0KiA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">递归神经网络的展开版本</figcaption></figure><p id="9f2e" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中，<br/><strong class="ko jq">‘ht’</strong>是t时间步的隐藏状态(这个在我们的句子例子中可以相当于我们对已有单词的记忆，给出一个产品复习)，<br/><strong class="ko jq">‘A’</strong>是激活(这里指的是tanh激活)，<br/><strong class="ko jq">‘XT’</strong>是在<strong class="ko jq">‘t’</strong>时间步的输入，<br/><strong class="ko jq">‘t’</strong>是时间步。</p><p id="bd56" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该算法考虑第一输入向量X0，对其进行处理以产生第一隐藏状态h0，这成为下一层计算下一隐藏状态h1的输入，依此类推。因此，隐藏状态充当神经网络存储器，它保存来自前一步骤的数据。</p><p id="0db6" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，在给定输入的情况下，计算隐藏状态的公式如下:</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/2a52c7c5d4a49a2ea05cf470f5a4fcc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*9uqoTkPnYmE-qy76ZKEW-w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">计算隐藏状态的方程(当前隐藏状态是前一个隐藏状态和输入的函数)。</figcaption></figure><p id="e64c" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这进一步用激活函数表示为:</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/9cc31b0ecd6ab31d4bb9e2c638416de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*34PlnzTv5gTHYk24Lp0rkg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">用激活函数和权重计算隐藏状态的扩展形式</figcaption></figure><p id="6134" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中，<br/> <strong class="ko jq"> Whh </strong>为前一隐藏状态的权重，<br/> <strong class="ko jq"> Wxh </strong>为当前隐藏状态的权重，<br/> <strong class="ko jq"> bt </strong>为偏差。</p><p id="1cc8" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq"> tanh </strong>激活函数确保隐藏状态的值向量保持在-1到+1之间，这是因为如果没有tanh激活函数，隐藏状态向量中的某些值会变得极高，而其他值会随着信息传递到下一层而保持较低。</p><p id="2a3d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">培训流程:</strong></p><p id="65fc" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">1 —给定时间步长的输入提供给网络。<br/>2——使用先前状态计算当前输入的当前状态。<br/> 3 —当前状态<strong class="ko jq"> ht </strong>变为下一时间步的隐藏状态<strong class="ko jq">h【t-1】</strong>。<br/> 4 —人们可以根据问题陈述计算尽可能多的时间步长，并加入来自先前状态的信息，然而，这导致了消失/爆炸梯度的问题，这将在我关于LSTMs的下一篇文章中解释。<br/> 5 —一旦所有时间步骤完成，最终隐藏状态用于计算当前输出。<br/> 6 —将生成的输出与原始输出进行比较，并计算误差。<br/> 7 —该误差被反向传播，以提高精度并计算新的权重。要了解更多关于反向传播的信息，你可以<a class="ae jd" href="https://medium.com/swlh/artificial-neural-networks-simplified-from-perceptrons-to-backpropagation-ab8b5770ded6" rel="noopener">点击这里</a>。</p><p id="081b" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这是RNN的工作原理，它对短序列非常有效，并且在计算上比长短期记忆网络(LSTM)或门控循环单元(GRU)网络更便宜。</p><h1 id="ea92" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">递归神经网络的应用；</h1><p id="5db5" class="pw-post-body-paragraph km kn jg ko b kp mi kr ks kt mj kv kw kx mk kz la lb ml ld le lf mm lh li lj ij bi translated">1 —时间序列异常检测。2 —音乐创作。<br/> 3 —单词预测。<br/> 4 —人体动作识别。<br/> 5 —预测趋势(天气预测、股价预测等)</p><p id="6e29" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我希望这篇帖子能给你一些知识和动力，进一步探索深度学习的其他主题。</p><p id="9898" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果这是有见识的，请用你的<strong class="ko jq">掌声</strong>鼓励。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="5b0d" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果你想更深入地探索递归神经网络并理解其中的数学原理，请参考下面这个知识渊博的博客:</p><p id="416a" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae jd" href="https://neptune.ai/blog/recurrent-neural-network-guide" rel="noopener ugc nofollow" target="_blank">递归神经网络指南——RNN深潜——Neptune . ai</a></p><p id="4e40" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">1 —如果你对<strong class="ko jq">计算机视觉感兴趣，</strong>请查看下面的帖子，我在那里解释了如何开发一个可以执行背景减法的简单代码:</p><div class="ip iq gp gr ir no"><a href="https://medium.com/@shabarish033/opencv-background-subtraction-and-music-in-background-of-video-c8fe10b66bc8" rel="noopener follow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd jq gy z fp nt fr fs nu fu fw jp bi translated">OpenCV背景减法和视频背景中的音乐</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">我们很多人喜欢旅行，然而，由于封锁，我们不能去我们想去的地方，在这篇文章中…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">medium.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc ix no"/></div></div></a></div><p id="71e8" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2 —如果你有兴趣了解<strong class="ko jq">人工神经网络</strong>和<strong class="ko jq">反向传播</strong>，请查看下面的帖子，我在那里用简单的语言解释了这些概念:</p><div class="ip iq gp gr ir no"><a href="https://medium.com/swlh/artificial-neural-networks-simplified-from-perceptrons-to-backpropagation-ab8b5770ded6" rel="noopener follow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd jq gy z fp nt fr fs nu fu fw jp bi translated">简化的人工神经网络:从感知器到反向传播</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">这篇文章是我试图解释神经网络的工作原理，我想尽可能简单地通过…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">medium.com</p></div></div><div class="nx l"><div class="od l nz oa ob nx oc ix no"/></div></div></a></div></div></div>    
</body>
</html>