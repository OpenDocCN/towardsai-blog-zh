<html>
<head>
<title>PySpark AWS S3 Read Write Operations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark AWS S3读写操作</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/pyspark-aws-s3-read-write-operations-f473e5bee30f?source=collection_archive---------1-----------------------#2021-02-02">https://pub.towardsai.net/pyspark-aws-s3-read-write-operations-f473e5bee30f?source=collection_archive---------1-----------------------#2021-02-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="588e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">云计算</a></h2><div class=""/><p id="6c0c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">本文的目标是理解亚马逊网络存储服务<strong class="jy ja"> S3 </strong>的基本读写操作。更具体地说，使用Apache Spark Python API PySpark在AWS S3上执行读写操作。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ku"><img src="../Images/b2441d997dff9adb4bff60116689fef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*3UyxI0Cqb9hIb4yz72mr-g.png"/></div></figure><ol class=""><li id="7d2f" class="lc ld iq jy b jz ka kd ke kh le kl lf kp lg kt lh li lj lk bi translated"><strong class="jy ja">在Spark独立集群上设置Spark会话</strong></li></ol><pre class="kv kw kx ky gt ll lm ln lo aw lp bi"><span id="d811" class="lq lr iq lm b gy ls lt l lu lv">import findspark<br/>findspark.init()<br/>import pyspark<br/>from pyspark.sql import SparkSession<br/>from pyspark import SparkContext, SparkConf</span><span id="e07a" class="lq lr iq lm b gy lw lt l lu lv">import os<br/>os.environ['PYSPARK_SUBMIT_ARGS'] = '-- packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'</span></pre><p id="b461" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">设置Spark属性连接到Spark会话:</p><pre class="kv kw kx ky gt ll lm ln lo aw lp bi"><span id="99c1" class="lq lr iq lm b gy ls lt l lu lv">#spark configuration<br/>conf = SparkConf().set(‘spark.executor.extraJavaOptions’,’-Dcom.amazonaws.services.s3.enableV4=true’). \<br/> set(‘spark.driver.extraJavaOptions’,’-Dcom.amazonaws.services.s3.enableV4=true’). \<br/> setAppName(‘pyspark_aws’).setMaster(‘local[*]’)</span><span id="4c31" class="lq lr iq lm b gy lw lt l lu lv">sc=SparkContext(conf=conf)<br/>sc.setSystemProperty(‘com.amazonaws.services.s3.enableV4’, ‘true’)</span><span id="2cb0" class="lq lr iq lm b gy lw lt l lu lv">print(‘modules imported’)</span></pre><p id="4982" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为所有工作节点设置Spark Hadoop属性，如下所示:</p><pre class="kv kw kx ky gt ll lm ln lo aw lp bi"><span id="63e4" class="lq lr iq lm b gy ls lt l lu lv">accessKeyId=’xxxxxxxxxx’<br/>secretAccessKey=’xxxxxxxxxxxxxxx’</span><span id="47fd" class="lq lr iq lm b gy lw lt l lu lv">hadoopConf = sc._jsc.hadoopConfiguration()<br/>hadoopConf.set(‘fs.s3a.access.key’, accessKeyId)<br/>hadoopConf.set(‘fs.s3a.secret.key’, secretAccessKey)<br/>hadoopConf.set(‘fs.s3a.endpoint’, ‘s3-us-east-2.amazonaws.com’)<br/>hadoopConf.set(‘fs.s3a.impl’, ‘org.apache.hadoop.fs.s3a.S3AFileSystem’)</span><span id="e8ba" class="lq lr iq lm b gy lw lt l lu lv">spark=SparkSession(sc)</span></pre><p id="b788" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> s3a写</strong>:目前有三种方式可以读写文件:s3、s3n、s3a。在这篇文章中，我们将只讨论s3a，因为它是最快的。请注意，s3将不会在未来的版本中提供。</p><p id="c9b8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> v4认证</strong> : AWS S3支持两个版本的认证——v2和v4。如需更多详细信息，请参考以下链接:<a class="ae lx" href="https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html" rel="noopener ugc nofollow" target="_blank">认证请求(AWS签名版本4)——亚马逊简单存储服务</a></p><p id="fc24" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 2。读取本地系统上的数据集</strong></p><pre class="kv kw kx ky gt ll lm ln lo aw lp bi"><span id="57de" class="lq lr iq lm b gy ls lt l lu lv">emp_df=spark.read.csv(‘D:\python_coding\GitLearn\python_ETL\emp.dat’,header=True,inferSchema=True)<br/>emp_df.show(5)</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/8d1b8371d63f705bc13c62e57dee28a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*K10XHPiKt2LQ12BzTl3PcQ.png"/></div></figure><p id="c82d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 3。到AWS S3存储器的PySpark数据帧</strong></p><pre class="kv kw kx ky gt ll lm ln lo aw lp bi"><span id="40f5" class="lq lr iq lm b gy ls lt l lu lv">emp_df.write.format('csv').option('header','true').save('s3a://pysparkcsvs3/pysparks3/emp_csv/emp.csv',mode='overwrite')</span></pre><p id="cc25" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">验证S3桶中的数据集，如下所示:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lz"><img src="../Images/53a5648430336057aaaa4276ccd33042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0SoyyBbLVCGbPbFaFW-5jA.png"/></div></div></figure><p id="5828" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们已成功将Spark数据集写入AWS S3存储桶"<strong class="jy ja">pysparcsvs 3</strong>"。</p><p id="99a6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 4。将数据从自动气象站S3读入PySpark数据帧</strong></p><pre class="kv kw kx ky gt ll lm ln lo aw lp bi"><span id="71c2" class="lq lr iq lm b gy ls lt l lu lv">s3_df=spark.read.csv(‘s3a://pysparkcsvs3/pysparks3/emp_csv/emp.csv/’,header=True,inferSchema=True)<br/>s3_df.show(5)</span></pre><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi me"><img src="../Images/1fe03c15da470029ee6164be452a8aeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*0APvZwhG5HFmvMayZlPk1w.png"/></div></figure><p id="28ab" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在PySpark的帮助下，我们已经成功地将数据写入AWS S3存储并从中检索数据。</p><p id="d02c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> 5。我面临的问题</strong></p><p id="4efa" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">将PySpark数据帧写入S3时，该过程多次失败，并显示以下错误。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mf"><img src="../Images/9f79dda729d99e993578c521c8529c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W-itcCjJ7iuQJAo19RacFA.png"/></div></div></figure><p id="53db" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">解决方法:从<a class="ae lx" href="https://github.com/cdarlint/winutils/tree/master/hadoop-3.2.1/bin" rel="noopener ugc nofollow" target="_blank">https://github . com/cdarlint/winutils/tree/master/Hadoop-3 . 2 . 1/bin</a>下载<strong class="jy ja">hadoop.dll</strong>文件，放在C:\Windows\System32目录路径下。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/470304438d19e71facd8b28d60dac5f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*0sfAJZL6F9s4ceOZ710aEw.png"/></div></figure><p id="2edb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">博客到此为止。感谢大家阅读我的博客。一定要分享你的观点/反馈，它们很重要。</p></div></div>    
</body>
</html>