<html>
<head>
<title>The 3 Key Variations of Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的3个关键变量</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-3-key-variations-of-linear-regression-8454d61c798b?source=collection_archive---------4-----------------------#2022-02-17">https://pub.towardsai.net/the-3-key-variations-of-linear-regression-8454d61c798b?source=collection_archive---------4-----------------------#2022-02-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d7fd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/09bea64248ee5b0eb7ae1d403e04df76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EOlHsSlNFVfmPFEIictmzw.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@markuswinkler?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">马库斯·温克勒</a>在<a class="ae ko" href="https://unsplash.com/s/photos/graph?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="bafe" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">任何项目中建立的第一个预测模型通常是回归模型。作为所有“白盒模型”之母，线性回归提供了直接的实现和解释。我们可以清楚地看到哪些变量对我们的目标有很大的影响，我们还可以使用统计测试来检查这些系数的显著性。遵循奥卡姆剃刀的灵感，即“实体的繁殖不应超过必要性”[1]，在我们的模型中建立复杂性是合乎逻辑的。</p><p id="dc7c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们的分析工具箱中有3种重要的线性回归方法。本文旨在详细介绍它们，从它们的方法和设计到它们各自的独特性和有效性。</p><h2 id="cf54" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">普通最小二乘法(OLS)</h2><p id="c127" class="pw-post-body-paragraph kp kq it kr b ks mf ku kv kw mg ky kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">普通最小二乘法或OLS模型是线性回归的基本方法，通常是论文或项目提到进行回归时描述或参考的方法。sk learn<em class="mk">linear regression()</em>函数默认调用OLS模型，R的<em class="mk"> lm() </em>函数也是如此。</p><p id="6cc3" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">OLS模型旨在找到最小化残差平方和的线性估计值，残差平方和以数据点和估计值之间的欧氏距离计算。这在下面的二维设置中用图表表示。</p><figure class="ml mm mn mo gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/70e1b2b40245f684dc4aa60c9680713a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P0_JjlhtSQ3sZRpRWgFhYQ.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">作者图片</figcaption></figure><p id="4000" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在一个更高维的设置中，这条线转换成一个维数=D-1的超平面，其中D是数据点的维数(解释变量的数目)。</p><p id="c293" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从数学上讲，OLS方法是通过以下函数计算的，这是残差平方和的公式。</p><figure class="ml mm mn mo gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mp"><img src="../Images/260b3e7d2a0bdfb5a3af8a162c59d1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2VtEENeh-KHBrPWgElp6sw.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">作者图片</figcaption></figure><p id="0a4f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">由于我们知道残差平方和函数是一个凸函数，因此OLS方法的解可以以封闭形式计算，并通过扩展上述公式，在设置微分等于0后微分并计算β来实现。</p><figure class="ml mm mn mo gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mq"><img src="../Images/a8f3c1977e8861071b16d6243eb78b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qpnYGy8TavKKagG7Z32DoA.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">作者图片</figcaption></figure><p id="a4e4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">那么，这些如何转化为一个适用的场景呢？事实上，我们的解决方案是一个封闭的形式(本质上这意味着它可以精确计算，我们不需要任何形式的数值方法来估计解决方案)，这意味着性能通常是相当快和可解释的。回归被称为“白盒模型”,因为我们可以看到他们为了得出一个解决方案而进行的所有计算，上面的证明显示了这一点。计算中的这种透明性也反映在输出中，通常，除了我们的标准误差图之外，回归还为我们提供了系数值(beta_hat值)和p值，以检查这些值的显著性。这有助于我们轻松地识别作为我们目标的关键预测因素的变量，并立即提出新的研究，我们可以研究为什么某些变量具有高度影响力，而其他变量则没有。</p><h2 id="e1f3" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">里脊回归</h2><p id="0f82" class="pw-post-body-paragraph kp kq it kr b ks mf ku kv kw mg ky kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">岭回归是我们在本文中讨论的两种收缩方法中的第一种，之所以这样命名是因为这个模型的目标是将系数的估计值收缩到0。这样做的好处是，我们可以创建更简单、更稀疏的模型(参数更少的模型)，这很有用，因为它可以帮助我们真正隔离重要的变量，并丢弃那些对我们的研究没有帮助的变量。</p><p id="ce46" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这两种收缩方法实现这一点的方式是在损失函数计算中增加一个额外的惩罚项。寻找最低损失现在是估计值与目标值的接近程度和每个系数的大小之间的平衡。相对于它们的大小，没有成比例地有助于减少OLS损耗的系数开始向0收缩，因为它们的大小不能被模型证明是合理的。这种行为可以用下面新的损失函数来概括。</p><figure class="ml mm mn mo gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/48eafaa24fe02118becfc3a7aec5e6b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywkaYaZ3gcrf_853JSNzUw.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">作者图片</figcaption></figure><p id="35ad" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种关键变量“隔离”的额外好处是，收缩法的表现往往略好于标准的OLS模型。由于增加了惩罚参数，它们在训练数据上的准确性通常较低，但在评估我们构建的任何模型时，我们优先考虑的是它的泛化能力，而ridge和lasso往往在这方面做得相当好。更简单的模型更容易避免过度拟合，这有助于推广到看不见的数据。</p><p id="ec33" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你还会注意到，在我们的惩罚项中有一个lambda变量。这被称为调整参数，本质上决定了损失和损失之间的平衡是如何分配的。当λ= 0时，我们的模型将与之前的OLS方法完全一样，λ值太小将导致模型过于复杂，可能会过度拟合我们的数据。λ值太高，我们的模型将完全集中在惩罚项上，所有变量将向0收缩，影响模型的性能和我们对解释变量可预测性的解释。因此，选择一个好的λ值至关重要。</p><h2 id="4708" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">套索回归</h2><p id="3c79" class="pw-post-body-paragraph kp kq it kr b ks mf ku kv kw mg ky kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">Lasso不是一种严格的回归，而是一种可应用于各种回归方法的要素选择工具，包括OLS和逻辑回归。从公式上来说，lasso模型看起来非常类似于ridge模型，但是有一个关键的区别-&gt; lasso不是在惩罚项中计算参数的平方值，而是使用系数的绝对值。</p><figure class="ml mm mn mo gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/b5054fe51e74069c231fbbdd533e14c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*es4LKomZnXlQMuQ3FeYeDQ.jpeg"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">作者图片</figcaption></figure><p id="5b8c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">lasso和ridge回归之间的行为差异在于，虽然ridge系数趋向于向0收缩，但在lasso中，这些系数恰好变为0。我们将在下一篇文章中详细介绍这是如何发生的，但是这样做的主要好处是我们现在可以使用套索作为精确特征选择的一种形式。系数正好为0的变量可以立即从模型中丢弃，因为它们不提供对目标的任何预测能力。这有助于我们清理我们的模型，并将焦点放在有明确关联的变量上。</p><h2 id="34a1" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">参考</h2><p id="8661" class="pw-post-body-paragraph kp kq it kr b ks mf ku kv kw mg ky kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">[1] — <a class="ae ko" href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener ugc nofollow" target="_blank">奥卡姆剃刀—维基百科</a></p></div></div>    
</body>
</html>