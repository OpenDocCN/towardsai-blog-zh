<html>
<head>
<title>FAIR Paper Review: A ConvNet for the 2020s</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">公平论文评论:2020年的会议</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fair-paper-review-a-convnet-for-the-2020s-75c9a2206de5?source=collection_archive---------1-----------------------#2022-01-25">https://pub.towardsai.net/fair-paper-review-a-convnet-for-the-2020s-75c9a2206de5?source=collection_archive---------1-----------------------#2022-01-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="30dd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="5836" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">尝试“现代化”ConvNets，以匹配最先进的视觉变压器的性能</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d5b35cbdf242b189546403efc0d04f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cnipmv1oQVho8yFZ"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@nasa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> NASA </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="e80b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">直到2020年，卷积神经网络一直是计算机视觉任务的标准。在过去的两年里，变形金刚已经入侵了计算机视觉和自然语言处理领域。视觉变形金刚的表现在2021年取代了最先进的CNN的表现。从那以后，CNN和《视觉变形金刚》之间有过无数次的比较。</p><p id="be9c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇论文是关于在不使用任何变形金刚组件(如全球注意力)的情况下，使经典CNN现代化，以匹配视觉变形金刚的性能。有趣的是，它揭示了CNN和视觉变形金刚之间的战争历史，并且它分解了用于改善CNN性能的零碎内容，我个人认为这些内容非常有见地和有趣。</p><h2 id="2680" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">介绍</h2><p id="0e09" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">这一切都是从vanilla Vision Transformer (ViT)开始的，尽管它在图像上表现得相当好，但在对象检测和语义分割上表现不太好。之后，<a class="ae lh" href="https://arxiv.org/abs/2103.14030" rel="noopener ugc nofollow" target="_blank"> Swin变压器</a>(也称为分级变压器)作为经典vit的升级版推出。现在，变形金刚的问题是，整体注意力相对于输入大小具有二次复杂度[1],这意味着它们对较小的图像工作得很好，但对较高分辨率的图像来说就不太好了。</p><p id="4d96" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该论文的作者还认为，经典的CNN具有归纳偏差，这有助于它们在计算机视觉任务中表现良好，如“<a class="ae lh" href="https://datahacker.rs/convolutional-operation-of-sliding-windows/" rel="noopener ugc nofollow" target="_blank">滑动窗口</a>”策略、翻译等值。这并不意味着CNN只对计算机视觉有好处，因为1D卷积也可以用于NLP，然而，它们在计算机视觉中占优势。</p><blockquote class="nb nc nd"><p id="b812" class="li lj ne lk b ll lm kd ln lo lp kg lq nf ls lt lu ng lw lx ly nh ma mb mc md im bi translated">平移等方差或仅等方差是卷积神经网络的一个非常重要的属性，在卷积神经网络中，图像中对象的位置不应该是固定的，以便它被CNN检测到。这仅仅意味着如果输入改变，输出也会改变。</p></blockquote><p id="7fae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">来源:<a class="ae lh" href="https://towardsdatascience.com/translational-invariance-vs-translational-equivariance-f9fbc8fca63a#:~:text=Translational%20Equivariance%20or%20just%20equivariance,changes%2C%20the%20output%20also%20changes." rel="noopener" target="_blank"> TDS </a></p><p id="412e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">同样值得注意的是，计算机视觉不仅仅是图像分类，我认为大多数人(包括我自己)在谈到计算机视觉时主要考虑图像分类。计算机视觉包括图像分类、对象检测、语义分割和许多其他内容。虽然纯视觉变形器在图像分类方面表现出色，但它们在其他主要任务方面并不出色。</p><p id="06f5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Swin transformers试图升级普通变形金刚，以更好地执行各种各样的计算机视觉任务，它采用了视觉变形金刚和CNN的混合。例如，他们在局部窗口[1]中使用注意力，这基本上是一种滑动窗口策略。</p><h2 id="3da0" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">ConvNeXts的路线图</h2><p id="8d1a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">本文主要是对CNN和vit的检查，试图将CNN带回到计算机视觉中。为此，他们从标准ResNet开始，逐步[1]提高其性能，以达到ViT，而最终不会成为ViT！</p><p id="5e91" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里正在探索的主要设计决策有[1]:</p><ol class=""><li id="95f9" class="ni nj it lk b ll lm lo lp lr nk lv nl lz nm md nn no np nq bi translated">宏观设计</li><li id="87e1" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">ResNeXt</li><li id="4c36" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">倒置瓶颈</li><li id="34c8" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">大颗粒尺寸</li><li id="8450" class="ni nj it lk b ll nr lo ns lr nt lv nu lz nv md nn no np nq bi translated">各种分层微设计</li></ol><p id="9988" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">他们还关注训练过程，因为不仅结构影响神经网络的最终性能。测试了各种各样有趣的优化，如[1] <a class="ae lh" href="https://www.fast.ai/2018/07/02/adam-weight-decay/" rel="noopener ugc nofollow" target="_blank"> AdamW优化器</a>、Mixup &amp; Cutmix增强等等。</p><p id="d8cb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">至于宏观设计研究，他们首先查看阶段计算比率，该比率决定了神经网络中块的数量的比率和分布。典型的ResNets有一个计算1:3比率的级，而Swin变压器的比率为1:3。他们发现，将ResNets中的这个比率与Swin-T的浮点运算对齐，可以将模型精度从78.8%提高到79.4%。</p><p id="fc09" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下一步是研究一个经典的ResNeXt网络。与普通resnet相比，ResNeXt网络具有更好的浮点运算权衡。他们还[1]使用分组卷积，其中卷积滤波器被分成不同的组，还使用深度卷积，这类似于自关注中的加权和操作。它们本质上是通过使用深度方向的卷积来重定向网络的FLOPs，从而将网络的性能提高到80.5%。</p><p id="4742" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">他们关注的下一个有趣的想法是“反向瓶颈”[1]。这意味着网络从一开始就极大地扩展了图像。经典的MLP块的隐藏维度比输入维度宽4倍。添加这种反向瓶颈设计减少了整个网络的FLOPs，并略微提高了性能。</p><p id="abd5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，他们开始考虑增加内核大小，试图模仿《变形金刚》中非局部自我关注的全局感受域[1]。他们将内核大小从3逐渐增加到11，并观察到深度方向卷积的网络性能在7 x7时最佳(80.6%)。同样值得注意的是，随着这一增长，网络的失败次数几乎保持不变。</p><h2 id="4349" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">其他微设计技巧</h2><p id="c1fc" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">神经网络设计一直是一种“艺术”，而不是如何设计它们的一步一步。当然，关于如何通过改变神经网络的设计来提高其性能，有一些标准的提示和技巧，但是我认为到目前为止，还没有什么具体的东西可以解释为什么结构的各种变化会导致性能的变化。这就是为什么许多机器学习论文是一系列从各个方面改变架构并检查性能和拟合变化的反复试验。</p><p id="495f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章最棒的一点是它的结构，它们布局了正在改变的架构的不同区域，故意详细解释了它们正在改变的内容，并给出了每项改变的结果，而不是给出所有改变的最终结果。ML中的一些论文只是布置了一系列巨大的变化，这些变化会突然导致性能的巨大变化，但你并不真正理解每个变化的意义，但这里的情况并非如此。</p><p id="d94c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了简洁起见，我们在这里要提到的最后一个变化是减少激活函数的数量。虽然向每个卷积层添加激活函数是常见的，但他们发现，消除其中的一部分(保留1xt层之间的一个并消除其他层)会导致另一个性能提高到81.3%，现在<strong class="lk jd">与Swin转换器的性能相当。</strong></p><h2 id="cd30" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">结论</h2><p id="19ac" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">请注意，并非所有的更改都是成功的，有些更改会导致性能下降。然而，为了简洁起见，我不可能包括所有的变化，所以如果你感兴趣，我强烈推荐你看看这篇文章。</p><p id="303a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想定期收到关于人工智能和机器学习的最新论文的评论，请在这里添加你的电子邮件并订阅！</p><p id="6bea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">https://artisanal-motivator-8249.ck.page/5524b8f934<a class="ae lh" href="https://artisanal-motivator-8249.ck.page/5524b8f934" rel="noopener ugc nofollow" target="_blank"/></p><h2 id="209f" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">参考</h2><p id="673a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">[1]<a class="ae lh" href="https://arxiv.org/pdf/2201.03545.pdf" rel="noopener ugc nofollow" target="_blank">2020年代的一篇论文</a></p></div></div>    
</body>
</html>