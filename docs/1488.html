<html>
<head>
<title>Embedded COVID mask detection on an Arm Cortex-M7 processor using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch在Arm Cortex-M7处理器上进行嵌入式COVID屏蔽检测</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/embedded-covid-mask-detection-on-an-arm-cortex-m7-processor-using-pytorch-771fabc845e2?source=collection_archive---------2-----------------------#2021-02-06">https://pub.towardsai.net/embedded-covid-mask-detection-on-an-arm-cortex-m7-processor-using-pytorch-771fabc845e2?source=collection_archive---------2-----------------------#2021-02-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ecda" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="6733" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><strong class="ak">我们如何在OpenMV-H7板上构建在设备上运行的可视新冠肺炎掩模质量检测原型，以及在此过程中面临的挑战。</strong></h2></div><p id="4717" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">TLDR；</strong>训练和部署自己的图像分类器的源代码可以在这里找到:<a class="ae lk" href="https://github.com/ARM-software/EndpointAI/tree/master/ProofOfConcepts/Vision/OpenMvMaskDefaults" rel="noopener ugc nofollow" target="_blank">https://github . com/ARM-software/endpoint ai/tree/master/ProofOfConcepts/Vision/OpenMvMaskDefaults</a></p><p id="c2c2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2020年夏天，我们与Arm合作，构建了一个易于使用的教程，介绍如何在Arm微控制器上训练和部署图像分类器。在本帖中，我们展示了我们是如何应对和解决以下挑战的:</p><ul class=""><li id="bcd1" class="ll lm iq kq b kr ks ku kv kx ln lb lo lf lp lj lq lr ls lt bi translated">将PyTorch ResNet转换为TensorFlow，并将其量化为使用8位整数值</li><li id="a1ff" class="ll lm iq kq b kr lu ku lv kx lw lb lx lf ly lj lq lr ls lt bi translated">收集、选择和注释有缺陷和无缺陷掩模的数据</li><li id="ece4" class="ll lm iq kq b kr lu ku lv kx lw lb lx lf ly lj lq lr ls lt bi translated">使用自我监督的预训练来提高处理较少图像时的模型性能。</li></ul><h1 id="dfbf" class="lz ma iq bd mb mc md me mf mg mh mi mj kf mk kg ml ki mm kj mn kl mo km mp mq bi translated">预期的结果</h1><p id="6ec5" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">这个项目的目标是展示一个端到端的工作流程，说明如何在OpenMV-H7板上训练和部署卷积神经网络。</p><p id="7613" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下面的视频展示了我们的分类器如何实时检测有缺陷的面具。</p><figure class="mw mx my mz gt na"><div class="bz fp l di"><div class="nb nc l"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk translated">OpenMV-H7支持的缺陷新冠肺炎掩模实时图像分类</figcaption></figure><h1 id="adc2" class="lz ma iq bd mb mc md me mf mg mh mi mj kf mk kg ml ki mm kj mn kl mo km mp mq bi translated">OpenMV-H7板</h1><p id="67f7" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">该板由一个运行频率为480MHz的<a class="ae lk" href="https://www.st.com/en/microcontrollers/stm32h743vi.html" rel="noopener ugc nofollow" target="_blank">STM 32h 743 VI</a>Arm Cortex-M7 CPU、多个外设和一个安装在其上的摄像头模块组成。<br/>摄像头模块有一个来自OmniVision的<a class="ae lk" href="http://www.ovt.com/products/sensor.php?id=80" rel="noopener ugc nofollow" target="_blank"> OV7725 </a>传感器，可以以75 FPS的VGA分辨率(640x480)进行录制。</p><p id="9f3d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于主板的计算能力和内存有限，我们的目标是一个非常小的深度学习模型。我们称该变体为ResNet-9，因为它更像是对ResNet-18变体的删减。下面您可以找到一些关于模型配置、运行时间和其他指标的数字。</p><ul class=""><li id="16ca" class="ll lm iq kq b kr ks ku kv kx ln lb lo lf lp lj lq lr ls lt bi translated"><strong class="kq ja">输入尺寸:</strong> 64x64x3</li><li id="59b9" class="ll lm iq kq b kr lu ku lv kx lw lb lx lf ly lj lq lr ls lt bi translated"><strong class="kq ja"> CPU频率。:</strong> 480兆赫</li><li id="5f92" class="ll lm iq kq b kr lu ku lv kx lw lb lx lf ly lj lq lr ls lt bi translated"><strong class="kq ja">操作:</strong> 33.4拖把</li><li id="0470" class="ll lm iq kq b kr lu ku lv kx lw lb lx lf ly lj lq lr ls lt bi translated"><strong class="kq ja">型号大小:</strong> 90千字节</li><li id="4012" class="ll lm iq kq b kr lu ku lv kx lw lb lx lf ly lj lq lr ls lt bi translated"><strong class="kq ja">推断时间:</strong> 150毫秒</li><li id="ec14" class="ll lm iq kq b kr lu ku lv kx lw lb lx lf ly lj lq lr ls lt bi translated"><strong class="kq ja">操作/秒</strong>:249 MOp/秒</li></ul><p id="8eb3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">详细规格可以在OpenMV官网<a class="ae lk" href="https://openmv.io/products/openmv-cam-h7" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/6e9acabe7c6f00c2328ff5d4ab19c94b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*tN005NiVE1kZbBK9BwCRmw.jpeg"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk translated">我们用的OpenMV H7板的特写图。</figcaption></figure><h1 id="ce55" class="lz ma iq bd mb mc md me mf mg mh mi mj kf mk kg ml ki mm kj mn kl mo km mp mq bi translated">数据收集</h1><p id="3a47" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">神经网络非常需要数据。为了有效地收集足够的训练数据，我们做了以下工作:</p><ol class=""><li id="560c" class="ll lm iq kq b kr ks ku kv kx ln lb lo lf lp lj nk lr ls lt bi translated">我们使用OpenMV-H7板上的摄像头来记录视频序列。通过USB接口和<a class="ae lk" href="https://openmv.io/pages/download" rel="noopener ugc nofollow" target="_blank"> OpenMV IDE </a>，我们能够轻松记录相机流并将其保存为视频文件。</li><li id="8c82" class="ll lm iq kq b kr lu ku lv kx lw lb lx lf ly lj nk lr ls lt bi translated">为了模拟真实的生产线，我们将摄像机安装在纸板上，以确保摄像机稳定。光学器件指向生产线，生产线是一个带有高边框的金属板。这种设置确保相机在相同的环境中看到缺陷和非缺陷掩模。</li><li id="882f" class="ll lm iq kq b kr lu ku lv kx lw lb lx lf ly lj nk lr ls lt bi translated">最后，我们通过推拉相结合的方式将口罩移过我们的检测线。</li></ol><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nl"><img src="../Images/441f2c67cedc504207209cad3979bf5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fULWu10pKzaaYevH6g1wAw.jpeg"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk translated">我们数据收集管道的图片。我们在纸板上切了一个小洞来夹住USB桌，把板放进去。</figcaption></figure><h1 id="c12d" class="lz ma iq bd mb mc md me mf mg mh mi mj kf mk kg ml ki mm kj mn kl mo km mp mq bi translated">数据选择和注释</h1><p id="5c6a" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">在这个阶段，我们有多个视频文件，每个文件都拍摄了几分钟。下一个挑战是提取帧并注释数据。我们使用FFmpeg进行帧提取，使用<a class="ae lk" href="http://lightly.ai/" rel="noopener ugc nofollow" target="_blank">和</a>来选择不同的帧。请注意，我们有超过20k帧，但没有时间对它们进行注释。轻轻使用<a class="ae lk" href="http://lightly.ai/" rel="noopener ugc nofollow" target="_blank"/>，我们选择了几百帧覆盖所有相关场景。<br/> <a class="ae lk" href="http://lightly.ai/" rel="noopener ugc nofollow" target="_blank">轻度</a>使用自我监督学习来获得图像的良好表示。然后，它使用这些表示来选择应该被注释的最有趣的图像。这种方法的好处是，我们可以访问预训练的模型，并仅在少数标记图像上对其进行微调。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nq"><img src="../Images/af6980cc673882204f282a80eb398412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PReJtvsyDDVwhWxQXDaI9w.jpeg"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk translated">使用OpenMV H7相机拍摄的示例图像显示了数据的三个标签。从左到右:好面膜，缺陷面膜，无面膜。</figcaption></figure><p id="bdc8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">模型微调<br/>然后，我们在总共500幅带注释的图像上训练分类器100个时期。</strong></p><h1 id="9217" class="lz ma iq bd mb mc md me mf mg mh mi mj kf mk kg ml ki mm kj mn kl mo km mp mq bi translated">从PyTorch到Keras再到TensorFlow Lite</h1><p id="361e" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">将预先训练好的PyTorch模型迁移到TensorFlow Lite是我们工作中最困难的部分。</p><p id="432d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们用ONNX尝试了几个技巧来导出我们的模型。一个简单的名为<a class="ae lk" href="https://github.com/nerox8664/pytorch2keras" rel="noopener ugc nofollow" target="_blank"> pytorch2keras </a>的库对于一个仅由线性层组成的模型来说工作得很好，但是对于我们的conv +线性模型来说就不行了。</p><figure class="mw mx my mz gt na"><div class="bz fp l di"><div class="nr nc l"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk translated">使用pytorch2keras将PyTorch模型导出到Keras的代码。</figcaption></figure><p id="3682" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们遇到的主要问题是PyTorch对tensors使用CxHxW(通道、高度、宽度)格式，而TensorFlow使用HxWxC。这意味着，在将我们的模型转换为TensorFlow Lite后，分类器之前的层的输出被置换，因此，分类器的输出是不正确的。为了解决这个问题，我们考虑手动置换线性分类器的权重。</p><figure class="mw mx my mz gt na"><div class="bz fp l di"><div class="nr nc l"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk translated">我们的代码在从PyTorch导出到Keras后手动改变权重。</figcaption></figure><p id="5ae1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，我们决定采用更简单的解决方案。我们将最后一个卷积层的输出汇集成一个Cx1x1形状。这样，改变通道的顺序不会影响神经网络的输出。</p><p id="87a9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后一步是量化和导出Keras模型到TensorFlow Lite。在我们的例子中，量化减少了模型的大小，加快了模型推理的速度，代价是精度降低了几个百分点。</p><figure class="mw mx my mz gt na"><div class="bz fp l di"><div class="nr nc l"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk translated">将Keras模型量化并导出到Tensorflow Lite的代码。我们损失了几个百分点的精度，因为量化后我们没有重新训练。</figcaption></figure><p id="0f61" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">特别感谢Arm的合作者和Lightly的Philipp Wirth使这个项目成为可能。这里有<a class="ae lk" href="https://github.com/ARM-software/EndpointAI/tree/master/ProofOfConcepts/Vision/OpenMvMaskDefaults" rel="noopener ugc nofollow" target="_blank">的完整源代码</a>。您可以轻松训练自己的分类器，并在嵌入式设备上运行它。如果您有任何问题，请随时联系我们或发表评论！</p><p id="ba2e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Igor<br/>联合创始人<a class="ae lk" href="http://lightly.ai/" rel="noopener ugc nofollow" target="_blank"> Lightly.ai </a></p></div></div>    
</body>
</html>