<html>
<head>
<title>Linear Regression Math Intuitions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归数学直觉</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/linear-regression-59d13f00ca53?source=collection_archive---------1-----------------------#2022-05-10">https://pub.towardsai.net/linear-regression-59d13f00ca53?source=collection_archive---------1-----------------------#2022-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c2aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">线性回归是机器学习中最简单的模型，其原始版本描述了两个变量之间的关系。理解线性回归概念的基础仍然很重要，因为它是理解许多其他机器学习模型和神经网络的基础。</p><p id="73a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们深入线性回归工作流程之前，让我们描述一下该模型的关键组件或功能及其推论，如下所示:</p><ul class=""><li id="1a78" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">均方误差</li><li id="62f5" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">预言；预测；预告</li><li id="6372" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">梯度下降</li></ul></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h2 id="0055" class="lg lh iq bd li lj lk dn ll lm ln dp lo jy lp lq lr kc ls lt lu kg lv lw lx ly bi translated"><strong class="ak">均方误差(MSE) </strong></h2><p id="ee89" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">在这里，我们将对MSE进行扣除，MSE将在该模型中用作衡量整体模型平均误差的指标。所以，让我们来看看下面的插图:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi me"><img src="../Images/5f62d0186bb28758fbdec825d87cb223.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*SA_FfHN1jxNzDg_evWpqcg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">线性回归模型组件</strong></figcaption></figure><p id="8644" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里我们有<strong class="jp ir"> <em class="mq">伊</em> </strong>这是答案，<strong class="jp ir"> <em class="mq"> y_hat </em> </strong>这是预测，最后还有<strong class="jp ir"> <em class="mq"> Ei </em> </strong>这是关于模型的误差，也称为<em class="mq">ε</em>。</p><p id="6b00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上图我们可以推断出，误差是由答案<em class="mq">易</em>和预测<em class="mq"> y_hat </em>之差定义的，可以表示为:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/87bb72cd8fe4d99a18a2a2f4861ecb89.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*ghEtW48pdk1bynEoHd2nxw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">基础误差公式</strong></figcaption></figure><p id="227e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可能已经注意到，这种差异并不总是正的，因为图中的寄存器或点有时会位于模型函数的下方。为了解决这个问题，我们简单地平方表达式:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/b0b02d982daa8225854579e3239dba3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*wHTj7g-JXxkVBKH-EpDqRw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">二次误差公式</strong></figcaption></figure><p id="4439" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们得到这些，我们就有了确定数据集中单点误差的表达式，我们可以扩展它，将整个数据集的误差表示为以下表达式，它们是相同的:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/88e4218d2ff914d386471a1c4036149b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*nxHCK9EiF733WGhwsdlOVw.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">均方差公式</strong></figcaption></figure><p id="3296" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的公式是MSE本身，但它也可以用矩阵形式表示，如下图所示:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/6d321bc7f4e32a0b5d37c06180a774e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*AmqG3LGagi9vTYQ-PVNaPQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">最小平方误差的矩阵形式</strong></figcaption></figure><p id="89b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请记住这种表达MSE的不同形式，因为我们将使用任何这种符号进行进一步的解释。</p></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h2 id="d241" class="lg lh iq bd li lj lk dn ll lm ln dp lo jy lp lq lr kc ls lt lu kg lv lw lx ly bi translated"><strong class="ak">预测</strong></h2><p id="58b9" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">我们的预测是一个建立模型的函数，在这种情况下是线性回归。在此模型中，我们将模型函数定义为向量参数与数据集的乘积:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/60cd32462c4ef60c05e8ab1bc6bf8e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*zSF6_LRiW3bRbQ0VVyK6FQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">单个寄存器的预测公式</strong></figcaption></figure><p id="353d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，n表示数据集中寄存器的总数，D是数据集特征的总大小，也称为维度大小。如您所见，我们的参数将是一个具有数据集维度大小的向量，我们知道数据集通常是一个矩阵，因此该表达式可以简化如下:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/a73a699b7d9996b992e3b10b543cede6.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*NdNV1_EkNKB6XCvO0yL64Q.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">线性回归预测公式</strong></figcaption></figure><p id="15f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图中的两个表达式完全相同。</p></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h2 id="679a" class="lg lh iq bd li lj lk dn ll lm ln dp lo jy lp lq lr kc ls lt lu kg lv lw lx ly bi translated"><strong class="ak">梯度下降</strong></h2><p id="45bb" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">为了优化参数，我们使用梯度下降，对于这个特定的模型，我们也可以使用直接方法，我们将在稍后解释，现在让我们专注于梯度下降。</p><p id="cbb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降使用实际误差的梯度，该梯度是实际误差的导数，该导数与我们需要进行的一些运算一起表示如下，以获得梯度:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/5e2a76bb3bcb135bb60080a32b653568.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*JzJRhUGLXRSl-e0m9L8JzA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">最小二乘误差的梯度</strong></figcaption></figure><p id="30b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，你可以看到我们的最终结果是误差相对于w的梯度，w代表参数。</p><p id="6c6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们有了梯度，让我们看看下面表示的梯度下降:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi my"><img src="../Images/1649423f544fb0dca47ddcc02f912d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*rqWeKqQzET-MYDbB7B5MKA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">梯度下降公式</strong></figcaption></figure><p id="5a35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的公式是原始的梯度下降其中eta是学习率，误差的梯度是我们之前刚刚推导的。然后，如果我们用刚刚推导出的表达式替换误差的梯度，我们会得到以下结果:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/fe3d04f96e31fcb83195c88dcd612579.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*AIH66FI_O_Lbcb5psoQFoQ.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">线性回归的梯度下降</strong></figcaption></figure></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h2 id="236f" class="lg lh iq bd li lj lk dn ll lm ln dp lo jy lp lq lr kc ls lt lu kg lv lw lx ly bi translated"><strong class="ak">直接法</strong></h2><p id="0217" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">线性回归模型中梯度下降的另一种方法是直接法，该方法简单地将误差表达式的梯度等于0。下图显示了此方法的扣除额:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a7b767de1caea4640eb592b6e4224f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*rVhmNrHtW-i18fJO9bs35w.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">参数优化的直接方法</strong></figcaption></figure></div><div class="ab cl kz la hu lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ij ik il im in"><h2 id="41d3" class="lg lh iq bd li lj lk dn ll lm ln dp lo jy lp lq lr kc ls lt lu kg lv lw lx ly bi translated">线性回归实现</h2><p id="b2ed" class="pw-post-body-paragraph jn jo iq jp b jq lz js jt ju ma jw jx jy mb ka kb kc mc ke kf kg md ki kj kk ij bi translated">现在我们知道了线性回归的关键方法，让我们用下图来解释它的工作流程:</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/65e05e267330d730cfb776ed89da277a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1VsVSlSJoLpcQGKGX-Au7A.jpeg"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk translated"><strong class="bd li">线性回归实现</strong></figcaption></figure><p id="1c9a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如您所见，我们开始随机设置参数，并将其与数据一起输入线性回归模型，然后测量误差，并基于此使用梯度下降优化参数。我们继续进行这个过程，直到误差稳定在最小值，然后我们用调整参数的训练模型退出这个循环。请注意，参数充当模型的张量。</p><p id="5921" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在使用直接方法而不是梯度下降的情况下，参数的调整将仅发生一次，这意味着我们不需要像梯度下降那样的循环。</p><p id="710e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注意事项</strong></p><ol class=""><li id="c143" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk ng kr ks kt bi translated">您需要记住的一点是，线性回归模型只是预测，梯度下降和误差函数是实现该模型的完整机制，也用于其他模型，但模型本身是定义预测的公式。</li><li id="8234" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk ng kr ks kt bi translated">第二件要考虑的事情是，在这个解释中，我们使用均方误差作为评估模型误差的指标，但如果您希望对其进行更多的研究，您会发现最大似然法最终与评估误差的方法相同。</li></ol><p id="7081" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望这对你有帮助。</p></div></div>    
</body>
</html>