<html>
<head>
<title>Video to Text Description Using Deep Learning and Transformers | COOT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习和变形金刚的视频到文本描述</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/video-to-text-description-using-deep-learning-and-transformers-coot-e05b8d0db110?source=collection_archive---------2-----------------------#2020-11-14">https://pub.towardsai.net/video-to-text-description-using-deep-learning-and-transformers-coot-e05b8d0db110?source=collection_archive---------2-----------------------#2020-11-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2030" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>、<a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>、<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="8d98" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">NeurIPS2020会议上发布的这一新模型使用变压器为视频的每个序列生成准确的文本描述，使用视频和对它的一般描述作为输入。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4a235fa684e866dc555e89dde4871192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YMTKrHSzBXrRPcuE2kcNDg.png"/></div></div></figure><h2 id="017c" class="ld le it bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx iz bi translated">它理解视频中每个片段发生的事情，就像人类一样。让我们看看他们是如何做到的。</h2><p id="a319" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg lm mh mi mj lq mk ml mm lu mn mo mp mq im bi translated">正如你们中的许多人可能已经知道的，我们正在接近神经信息处理系统会议的日期，也被称为NeurIPS会议。在那里，许多优秀的论文将会公之于众，与更多的读者分享。我肯定会报道最有趣的，就像这个视频里的那个。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mr"><img src="../Images/541e6fc27b50d3af0303295094febb69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*rWMaOjVmKGIfCRzikFeTqA.png"/></div></div></figure><p id="8753" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">它被称为COOT:用于视频-文本表示学习的协作分层变换器。顾名思义，它使用转换器为视频的每个序列生成准确的文本描述，将视频和视频的一般描述作为输入。</p><p id="9eaa" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">在继续之前，如果您不熟悉transformer的架构，我邀请您观看我制作的解释它的视频。这将有助于您理解视频的其余部分，在那里我不会深入介绍这种架构的细节。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="f50f" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">在深入这个网络之前，我们还需要回顾一个重要的概念。粒度。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/977e9b45f0daa31f73aada7416b54a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/1*1EEzdsGqYkBBBh34hl9bPQ.gif"/></div></figure><p id="30b9" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">理解粒度以及它如何应用于我们的工作是极其重要的。尤其是在从视频生成文本描述时。粒度是指特定实体的大小。在我们的例子中，它指的是视频和文本，其中有许多粒度级别。视频可以被称为帧、剪辑或直接称为完整视频。在这里，单词也可以被看作句子、段落，甚至是字母。它们各有不同的语义和含义。例如，生成一个视频的一般描述比生成整个视频的多个更深入的描述要简单得多，正如本文中所实现的那样。</p><p id="f321" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">他们通过对这些不同粒度级别之间的交互进行建模来实现这一点，以便更好地理解视频帧和剪辑的上下文。他们的方法基本上由三个主要部分组成。该架构的第一部分在一个剪辑中工作。它利用本地时间上下文，即每个剪辑中与理解上下文相关的信息，通过使用注意感知特征聚合层。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/e5c8d35539316863d8aaed2f763f3ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*6qxfupfM6S__ViwT2k2g7Q.png"/></div></figure><p id="3b66" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">然后，有一个上下文转换器，它使用这个时间转换器作为基础，学习每个剪辑、句子、段落如何相互作用，以产生最终的视频/段落。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/3d46a3992e108e40e0d51f882d7cb858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r24CstDsUNATjnRI1c5T8w.png"/></div></div></figure><p id="d56d" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">简而言之，这个上下文转换器鼓励模型针对局部和全局上下文之间的交互来优化表示。像人类一样，结合全球和本地背景来理解整个视频中发生的事情是很重要的。这样，模型就知道每个剪辑都连接在一起，下一个剪辑是当前剪辑的延续。在这个例子中，模型知道在整个视频中仍然是同一个人用多个步骤做同一件事，这就是巧克力饼干的配方。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/174b43ea863afd0b35c1c8585f902fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HeLlvq7ikG758JO0vOCoxA.png"/></div></div></figure><p id="270e" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">该部分由两个分支组成，一个用于视频输入，一个用于文本输入。给定一个特定的视频和它的一般文本描述，他们将它们编码到尽可能最小的粒度，即帧和单词级别。然后，这些编码输入被发送到这些时间变换器和注意力特征聚集模块，以从帧和单词信息中获得片段级和句子级特征。使用关注这些低层次实体之间的互动，我重复一下，在我们的例子中，是一般描述的词和视频的帧。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/680bcca23908ba14210ba4e1b633dcc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n62asgizq3AcdZRUtJmHtw.png"/></div></div></figure><p id="1887" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">最后，就像在每个深度学习模型中一样，有一个损失函数。在这种情况下，将视频连接到文本并改善结果是一种跨模态的循环一致性损失。使用联合嵌入空间，他们在每个句子的剪辑序列中找到最近的邻居，并以相反的方式做同样的事情。</p><p id="3de7" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">这是他们的模型在试图随机给视频剪辑加标题并把它们放在一起时的输出。观看此视频，了解更好的解释和更多示例:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="d7d2" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">当然，这只是这篇新论文的一个简单概述，我强烈邀请你阅读下面我的参考资料中链接的论文。你也可以在下面的GitHub链接上找到他们的公开代码！</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="a73a" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">如果你喜欢我的工作并想支持我，我会非常感谢你在我的社交媒体频道上关注我:</p><ul class=""><li id="af6c" class="nl nm it ma b mb ms me mt lm nn lq no lu np mq nq nr ns nt bi translated">支持我的最好方式就是跟随我上<a class="ae nu" href="https://medium.com/@whats_ai" rel="noopener"><strong class="ma jd"/></a>。</li><li id="87b1" class="nl nm it ma b mb nv me nw lm nx lq ny lu nz mq nq nr ns nt bi translated">订阅我的<a class="ae nu" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank">T5】YouTube频道T7】。</a></li><li id="1b74" class="nl nm it ma b mb nv me nw lm nx lq ny lu nz mq nq nr ns nt bi translated">在<a class="ae nu" href="https://www.linkedin.com/company/what-is-artificial-intelligence" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd"> LinkedIn </strong> </a>上关注我的项目</li><li id="22da" class="nl nm it ma b mb nv me nw lm nx lq ny lu nz mq nq nr ns nt bi translated">一起学习AI，加入我们的<a class="ae nu" href="https://discord.gg/SVse4Sr" rel="noopener ugc nofollow" target="_blank"> <strong class="ma jd"> Discord社区</strong> </a>，<em class="oa">分享你的项目、论文、最佳课程，寻找Kaggle队友，等等！</em></li></ul></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="62fd" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated"><strong class="ma jd">参考文献:</strong></p><p id="add6" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">论文:<a class="ae nu" href="https://arxiv.org/pdf/2011.00597.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2011.00597.pdf</a></p><p id="bc06" class="pw-post-body-paragraph ly lz it ma b mb ms kd md me mt kg mg lm mu mi mj lq mv ml mm lu mw mo mp mq im bi translated">GitHub:<a class="ae nu" href="https://github.com/gingsi/coot-videotext" rel="noopener ugc nofollow" target="_blank">https://github.com/gingsi/coot-videotext</a></p></div></div>    
</body>
</html>