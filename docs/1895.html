<html>
<head>
<title>Diving Deep into Linear Regression and Polynomial Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入研究线性回归和多项式回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/diving-deep-into-linear-regression-and-polynomial-regression-34a71a02ade5?source=collection_archive---------0-----------------------#2021-06-05">https://pub.towardsai.net/diving-deep-into-linear-regression-and-polynomial-regression-34a71a02ade5?source=collection_archive---------0-----------------------#2021-06-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="6531" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/2608f50c248c1dfb1034082cff69bbaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OsOcHPscjy1BN5ZjZiKmAA.png"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated"><a class="ae kl" href="https://iconscout.com/illustrations/office" rel="noopener ugc nofollow" target="_blank">办公室职员通宵工作插图</a>由<a class="ae kl" href="https://iconscout.com/contributors/iconscout" rel="noopener ugc nofollow" target="_blank"> Iconscout免费赠送</a>在<a class="ae kl" href="https://iconscout.com/" rel="noopener ugc nofollow" target="_blank"> Iconscout </a></figcaption></figure><p id="7716" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我之前的文章中，我们探索了人工智能的不同分支。我几乎可以肯定，现在您可能想更详细地了解这些分支。不要担心，我肯定会在以后的文章中向这些子集敞开大门。如果你错过了我的帖子，可以在以下链接找到:<a class="ae kl" href="https://jashrathod.github.io/2020-11-24-branches-in-artificial-intelligence-to-transform-your-businesses/" rel="noopener ugc nofollow" target="_blank"> <strong class="ko ja">人工智能的分支</strong> </a>。</p><p id="bba7" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">之前我们讨论过机器学习。我们还讨论了它的子集——监督学习、非监督学习和强化学习。在本帖中，我们将讨论监督学习中最基本的算法之一— <strong class="ko ja">回归</strong>。回归可以有多种类型。你可能以前遇到过一些类型的回归，或者可能是第一次听说。特别是，我们将在本文中查看两种类型的回归，即<strong class="ko ja">“线性回归”</strong>和<strong class="ko ja">“多项式回归”</strong>，以及它们的数学公式和python代码。</p><p id="3d75" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在进入这些类型之前，我们先来了解一下什么是<strong class="ko ja">“回归”。</strong></p><blockquote class="lk ll lm"><p id="d1e0" class="km kn ln ko b kp kq kr ks kt ku kv kw lo ky kz la lp lc ld le lq lg lh li lj ij bi translated">“回归是因变量与一组自变量之间关系强度的一种统计方法。”</p></blockquote><p id="d725" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">迷茫？我们来打个比方理解一下。想象一下，你想买房子。现在，我想你会同意我的观点，房子的价格将取决于房子的大小(以平方米计算)。英尺)和它的位置(可以有更多的参数，房子的价格可以依赖于这些参数，但是为了简单起见，让我们假设这两个是主要的驱动因素。)如果你选择更大的房子，房价自然会飙升，如果你梦想中的家是曼哈顿的顶层公寓，可以看到天际线，那么它会比皇后区的贵得多(嗯，我个人的选择是摩纳哥的家！)</p><p id="5a96" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里，房子的大小和位置将是自变量。顾名思义，改变一个对另一个没有影响，这意味着它们彼此“独立”。房子的价格将是因变量，因为它取决于大小和位置。因此，<strong class="ko ja">“回归分析”</strong>是利用自变量预测因变量的方法。</p><p id="57a4" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">没有进一步的原因，让我们深入线性和多项式回归。</p><h1 id="8e80" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated"><strong class="ak">线性回归</strong></h1><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mp"><img src="../Images/2ff2c85c892d84a53f1d276aa27ad9be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MfubpnNuncaKGfb-"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod" rel="noopener ugc nofollow" target="_blank">吴家浩·拉瑟德</a>拍摄</figcaption></figure><p id="51ec" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">线性回归是一种回归分析方法，<strong class="ko ja">假设因变量和自变量之间存在线性关系。这意味着当我们在因变量和自变量之间绘制图表时，会形成一条直线。这是一种解决需要将预测作为连续值的问题的方法。作为最基本的算法，对于我们希望预测连续值的任何任务，这是通常实现的第一个算法。</strong></p><p id="ffc0" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在只有一个自变量的情况下，我们称之为<strong class="ko ja">“简单线性回归”</strong> <em class="ln"> </em>就这么简单！比方说，<em class="ln"> y </em>为因变量，<em class="ln"> x </em>为自变量。设<em class="ln"> w </em>为变量的权重，<em class="ln"> b </em>为偏差(偏差有时可以为零)。权重和偏差只不过是将自变量转换为因变量的值。因此，从数学上来说，</p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f7d27d160cd923457fa0290410e357b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*S9u1Vuyfx4sP2C8DxH4PAw.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">简单线性回归。图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod/" rel="noopener ugc nofollow" target="_blank">吴家浩·拉霍德</a>拍摄</figcaption></figure><p id="5a4f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果你觉得很难理解，就想想我们的类比。房子的价格与房子的大小成比例。所以我们的等式变成了:</p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/7952cd8ed2089eda516539f20245c430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*jB-G7B0vRKlGxDxdOoZ1Hg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">用简单线性回归预测房价。图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod/" rel="noopener ugc nofollow" target="_blank">吴家浩·罗德</a>拍摄</figcaption></figure><p id="1512" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在自变量和因变量很多的情况下，我们称之为<strong class="ko ja">“多元线性回归”</strong>所以，设<em class="ln"> X </em>为一组<em class="ln"> n </em>独立特征，<em class="ln"> W </em>为<em class="ln"> X </em>中每个值的新权重集合。</p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/d27991ccc9e4186f6316f962c942e051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*GuDIcdAHGbjUY6Uc3FFOqQ.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod/" rel="noopener ugc nofollow" target="_blank">吴家浩·拉霍德</a>拍摄</figcaption></figure><p id="ce8d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们的多元线性回归方程为:</p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mx"><img src="../Images/1421058680dc0532518f3f0ccbf69f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CgP7oKA2YBPUuyfDOVbULA.png"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">多元线性回归。图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod/" rel="noopener ugc nofollow" target="_blank">吴家浩·拉霍德</a>拍摄</figcaption></figure><p id="5092" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">相应地，我们也可以修改我们的类比方程。</p><p id="fadb" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们同意这种算法的预测，那么我们很乐意去做！简单，容易，可解释。但是一般我们观察到，对于现实世界的问题，情况并非如此。在大多数现实世界的问题中，因变量和自变量之间的关系不是线性的，在这种情况下，通常会发现线性回归表现不佳。</p><h1 id="cdc8" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated"><strong class="ak">多项式回归</strong></h1><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi my"><img src="../Images/bab1bde5f4a2acdf78349267b7e3624e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBI3byzRAj0J9Z5GNIFQHg.png"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">多项式回归。图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod/" rel="noopener ugc nofollow" target="_blank">吴家浩·罗德</a>拍摄</figcaption></figure><p id="8fe0" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了解决非线性问题，我们在我们的方法中引入了一个小小的调整— <strong class="ko ja">多项式回归</strong>。这种回归分析方法，自变量可以线性或非线性地依赖于因变量。这有助于我们构建复杂的曲线，有助于设计更合适的现实世界场景表示。</p><p id="48a1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">甚至多项式回归也可以分为简单多项式回归和多重多项式回归。简单多项式回归的等式可以给出为:</p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/0e97466f5a469219e69c9c537ce45af1.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*oIH2Yu4O5Edu6byQoWwoXg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">简单多项式回归。图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod/" rel="noopener ugc nofollow" target="_blank">吴家浩·罗德</a>拍摄</figcaption></figure><p id="2625" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请注意，已经考虑了独立变量的平方。这引入了非线性。</p><p id="c0aa" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">类似地，多元多项式回归的等式可以给出为:</p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi na"><img src="../Images/d1619b99dcdc15a35337afcca30caf6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ynrIjjpt4EeAAKoSvkD5Ww.png"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">多元多项式回归。图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod/" rel="noopener ugc nofollow" target="_blank">吴家浩·拉霍德</a>拍摄</figcaption></figure><p id="1ced" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中<em class="ln"> n，p，q，r </em>可以是引入非线性的整数。</p><p id="0e3c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这种方法表现出比线性回归技术更好的性能，因为它能够处理更复杂的关系。但是对于这种方法来说，重要的是我们知道<em class="ln"> X </em>和变量<em class="ln"> y </em>中的值是如何相关的。如果不知道这一点，就更像是一种尝试和错误。此外，如果没有仔细选择功率，这很容易过度拟合。</p><h1 id="fd62" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated"><strong class="ak">初始化方程系数</strong></h1><p id="51d4" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">你可能已经想过，在训练过程之前，给系数<em class="ln"> w </em>和<em class="ln"> b </em>分配什么值，以及在训练过程中如何优化这些值？如果我们仔细观察，我们会意识到，为了使我们的ML模型更好地预测，我们希望在训练过程中估计这些系数。</p><p id="45bb" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">分配初始值的过程称为‘初始化’</strong>。有多种方法可以初始化系数。一些常见的初始化有:</p><ol class=""><li id="0f68" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj nl nm nn no bi translated">零</li><li id="28fe" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj nl nm nn no bi translated">二进制反码</li><li id="bcba" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj nl nm nn no bi translated">正态分布</li><li id="c656" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj nl nm nn no bi translated">截尾正态分布</li></ol><p id="81db" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了理解如何在训练过程中优化这些系数，我们需要研究两个概念:<em class="ln">【损失函数】</em>和<em class="ln">【随机梯度下降(SGD)】</em>。这些条款可能听起来有点难以承受，但不要担心，我会支持你的！</p><h1 id="60cc" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated"><strong class="ak">损失函数</strong></h1><p id="9830" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">让我们考虑一个有两个独立变量的多元线性回归。我们的等式是:</p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5e95fcdd5c52556a20bcc3ad53392aa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*KBxlOtnn-1J0NpOj6UBnsg.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">真正的价值观。图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod/" rel="noopener ugc nofollow" target="_blank">吴家浩·罗德</a>拍摄</figcaption></figure><p id="80ad" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这个等式中，<em class="ln"> y </em>是真值，而<em class="ln"> wi </em>和<em class="ln"> b </em>是最佳系数。当我们用次优系数初始化时，我们的结果也会偏离最优结果。因此，在我们获得最佳系数和最佳结果之前，我们的等式将是:</p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9e5431ba245debfc7a0261b4a2d28978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*-4Vmgb2kELk9pCKnMtGyiA.png"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">预测值。图片由<a class="ae kl" href="https://jashrathod0.wixsite.com/jashrathod/" rel="noopener ugc nofollow" target="_blank">吴家浩·拉霍德</a>拍摄</figcaption></figure><p id="1be0" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里，新的<em class="ln"> y，wi，</em>和<em class="ln"> b </em>(带帽)为次优值。从现在开始，我们将把<em class="ln"> y </em>称为“真值”，把<em class="ln"> y </em>(帽子)称为“预测值”。</p><p id="8de3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在数学优化和决策理论中,<strong class="ko ja">损失函数可以定义为预测成本的量化表示。真实值和预测值的偏差越大，成本越高，预测越差。</strong></p><p id="bdfd" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">基于手头的任务类型(回归或分类)，有几种方法来定义损失函数。回归的一些损失函数是:</p><ol class=""><li id="1ce0" class="ng nh iq ko b kp kq kt ku kx ni lb nj lf nk lj nl nm nn no bi translated">均方误差损失</li><li id="5d20" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj nl nm nn no bi translated">均方对数误差损失</li><li id="b042" class="ng nh iq ko b kp np kt nq kx nr lb ns lf nt lj nl nm nn no bi translated">平均绝对误差损失</li></ol><p id="10d4" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们将在以后的文章中详细讨论它们。</p><h2 id="17fc" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated"><strong class="ak">随机梯度下降</strong></h2><p id="e719" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">我们需要将<em class="ln"> wi </em>和<em class="ln"> b </em>优化至最佳系数，从而获得接近实际值的结果。为了做到这一点，我们使用优化器。这些优化器在训练时使用数学函数使系数值更接近最佳值。我们可以使用许多优化器。但是，让我们看一个例子来理解优化器是如何工作的。我们将关注的是随机梯度下降(SGD)。</p><p id="a149" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">设<em class="ln"> L </em>为损失函数(可以是上面提到的3个中的任何一个，甚至可以不是这些)。对此的数学方程式是:</p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f6915226d5989a001c165598bfba93e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*fjCo-q6jYh5n28sSdsgnPA.png"/></div></figure><p id="956f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中<em class="ln"> η </em>是步长(也称为学习率),它乘以损失函数相对于正在更新的系数的偏导数。</p><p id="6956" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个等式的美妙之处在于，在没有任何人工干预的情况下，随着训练的进行，它会优化系数。这是通过最小化损失函数来实现的。因此，这个过程也被称为<strong class="ko ja">“最小化损失函数”。</strong></p><figure class="mq mr ms mt gt ka gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/64421a1a2755298b6ddf8abdb03a6194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*eT8rCW0oU8uwUhLm"/></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated"><a class="ae kl" href="https://gfycat.com/angryinconsequentialdiplodocus" rel="noopener ugc nofollow" target="_blank">图片来源</a></figcaption></figure><p id="0540" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了清楚地理解这是如何工作的，让我们假设有多个山头，而你正站在其中一个山头上。如果我告诉你到达最底部，你必须一步一步来。尽管有一个条件。在任何给定的时间，你的下一步必须是在该点最陡下降的方向。你下山的方式就是梯度下降的原理！</p><p id="29b7" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">新加坡元也有一定的负面影响。此外，有许多不同的优化器以不同的方式工作。我们将在今后讨论这些问题。</p><p id="3c00" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">目标达成，线性和多项式回归学会了！</strong></p><p id="55e2" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">到目前为止，我们学习了线性和多项式回归。</p><p id="2d91" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">要了解线性和多项式回归如何在<strong class="ko ja">‘房价数据集’</strong>上执行，请查看下面的代码。</p><h1 id="66a1" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">房价数据集</h1><p id="071c" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">本教程使用的数据集是<strong class="ko ja">房价预测</strong>。我已经修改过了，让它更容易理解。完整代码(Jupyter笔记本)和数据集可在以下网址找到:</p><div class="oi oj gp gr ok ol"><a href="https://github.com/jashrathod/machine-learning-series/blob/master/Linear%20and%20Polynomial%20Regression/Linear%20and%20Polynomial%20Regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">jashrathod/机器学习系列</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">github.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz kf ol"/></div></div></a></div><p id="c9fa" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">修改后的数据集<strong class="ko ja"> "data.csv" </strong>包含3个自变量和1个因变量。</p><p id="8006" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">自变量:</strong></p><p id="eb21" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="ln">1层平方英尺</em> —一层平方英尺<br/><em class="ln">2层平方英尺</em> —二层平方英尺<br/> <em class="ln">年份建造</em> —原建造日期</p><p id="4e68" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko ja">因变量:</strong></p><p id="29c1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="ln">销售价格</em> —以美元计算的房产销售价格。(这是你试图预测的目标变量。)</p><p id="7efb" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果您希望处理完整的数据集，可以在以下位置找到:</p><div class="oi oj gp gr ok ol"><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">房价-高级回归技术</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">预测销售价格并实践特征工程、RFs和梯度推进</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">www.kaggle.com</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz kf ol"/></div></div></a></div><p id="f648" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们马上进入代码！</p><h2 id="d03b" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated">第一步。导入Python库</h2><p id="26b1" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">所有库的简短描述:</p><p id="b545" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae kl" href="https://docs.python.org/3/library/datetime.html" rel="noopener ugc nofollow" target="_blank">日期时间</a> —提供处理日期和时间的类。</p><p id="502c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae kl" href="https://docs.python.org/3/library/sys.html" rel="noopener ugc nofollow" target="_blank"> sys </a> —提供对解释器使用或维护的一些变量以及与解释器交互强烈的函数的访问</p><p id="6ee5" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae kl" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> pandas </a> —一个快速、强大、灵活且易于使用的开源数据分析和操作工具，构建在<a class="ae kl" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python </a>编程语言之上。</p><p id="516d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae kl" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank">scikit-learn</a>——包含了大量用于机器学习和统计建模的高效工具，包括分类、回归、聚类和降维(作为sklearn导入)</p><p id="d8af" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae kl" href="https://docs.python.org/3/library/math.html" rel="noopener ugc nofollow" target="_blank">数学</a> —提供对C标准定义的数学函数的访问</p><figure class="mq mr ms mt gt ka"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="5d05" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated">第二步。使用Pandas将数据集作为数据帧读取</h2><p id="dc21" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">需要首先调用文件<strong class="ko ja"> "data.csv" </strong>，以使用它来训练我们的模型。我们将其读取为一个<strong class="ko ja">熊猫</strong>数据帧，并将其赋给变量<strong class="ko ja"> df </strong>。为此，我们将利用<a class="ae kl" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ko ja"> read_csv() </strong> </a>函数。</p><figure class="mq mr ms mt gt ka"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="1dbc" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated">第三步。初始化自变量和因变量</h2><p id="8a45" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">我们现在需要将因变量(<em class="ln"> y </em>)与自变量(<em class="ln"> X </em>)分开。这可以通过运行以下代码来完成:</p><figure class="mq mr ms mt gt ka"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="357b" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated">第四步。培训和测试数据</h2><p id="ebec" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">我们的任务需要两组数据，一组用于训练，另一组用于测试。Sklearn提供了一个函数<a class="ae kl" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"><strong class="ko ja">train _ test _ split()</strong></a>来做到这一点。对于我们打算执行的任务，我们将使用总数据的80%进行训练，剩余的20%进行测试。</p><p id="c228" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="ln"> X_train </em> —部分X用于训练<br/> <em class="ln"> y_train </em> —部分y用于训练<br/> <em class="ln"> X_test </em> —部分X用于测试<br/> <em class="ln"> y_test </em> —部分y用于测试</p><figure class="mq mr ms mt gt ka"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="5b87" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated">第五步。线性回归模型</h2><p id="c2b5" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">最后，我们的线性回归模型终于来了！变量<strong class="ko ja">“模型”</strong>创建了我们的<a class="ae kl" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linear%20regression#sklearn.linear_model.LinearRegression" rel="noopener ugc nofollow" target="_blank"> <strong class="ko ja">线性回归</strong> </a>模型的一个实例。在第二行，我们可以看到<strong class="ko ja">【fit】</strong>函数。<a class="ae kl" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit" rel="noopener ugc nofollow" target="_blank"> <strong class="ko ja"> fit() </strong> </a>函数真正做的是使用训练数据训练我们的模型。现在我们的模型已经训练好了。我们将使用<a class="ae kl" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict" rel="noopener ugc nofollow" target="_blank"> <strong class="ko ja"> predict() </strong> </a>函数来获得对<em class="ln"> X_test </em>的预测。这些预测可以与实际值进行比较，以确定我们的模型表现如何。</p><figure class="mq mr ms mt gt ka"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="856d" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated">第六步。多项式特征</h2><p id="1a80" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">为了获得多项式相关的特性，scikit-learn提供了一个名为<a class="ae kl" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html" rel="noopener ugc nofollow" target="_blank">多项式特性()</a>的函数。如果一个变量<em class="ln"> p </em>与<em class="ln"> q </em>二次相关，那么<em class="ln"> p </em>线性依赖于<em class="ln"> q </em>。因此，我们将生成更高功效的特征，并将它们提供给线性回归模型。这将使我们能够实现多项式回归。在下面的代码中，X_poly将作为新的X_train，用于训练任务。</p><figure class="mq mr ms mt gt ka"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="7063" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated">第七步。多项式回归模型</h2><p id="8503" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">类似于线性回归，我们使用具有多项式特征的<a class="ae kl" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linear%20regression#sklearn.linear_model.LinearRegression" rel="noopener ugc nofollow" target="_blank">线性回归</a>模型作为输入。</p><figure class="mq mr ms mt gt ka"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="54a8" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated">第八步。将结果列表</h2><p id="02be" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">这一步是创建一个新的数据帧，存储两个模型的实际/真实值以及预测值。这是使用函数<a class="ae kl" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ko ja"> DataFrame() </strong> </a>完成的。</p><figure class="mq mr ms mt gt ka"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h2 id="9ac0" class="nv ls iq bd lt nw nx dn lx ny nz dp mb kx oa ob mf lb oc od mj lf oe of mn iw bi translated">第九步。评估指标—均方根误差(RMSE)</h2><p id="a698" class="pw-post-body-paragraph km kn iq ko b kp nb kr ks kt nc kv kw kx nd kz la lb ne ld le lf nf lh li lj ij bi translated">我们如何定量评估我们的模型表现如何？为此，我们使用一种叫做“评估指标”的东西，它将预测值和实际值进行比较，并给出一个数字。根据值的高低，我们可以说我们的模型有多好。一个这样的评估标准是<strong class="ko ja">“均方根误差”</strong>或简单的<strong class="ko ja"> RMSE </strong>。我相信你可能已经研究过了。如果你想了解RMSE更多，看看这篇文章:<a class="ae kl" href="https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e" rel="noopener" target="_blank"><strong class="ko ja">【RMSE到底是什么意思？</strong> </a></p><figure class="mq mr ms mt gt ka"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="44cb" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通过线性回归得到的RMSE是<strong class="ko ja">413955685014</strong>而通过多项式回归得到的是<strong class="ko ja">41356</strong>。48656.66668666666</p></div><div class="ab cl pd pe hu pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="ij ik il im in"><p id="db9c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我知道RMSEs太差了。我们将使用完整的数据，执行更好的特征工程，并实施更鲁棒的算法，以在未来获得更好的结果。</p><p id="a0a9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以观察到一点，多项式回归的RMSE比线性回归的好。因此，<strong class="ko ja">我们可以得出结论，多项式回归通常优于线性回归，因为多项式基本上适合大范围的曲率。</strong></p><p id="d748" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">感谢您的阅读！</p><p id="214f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有任何问题或建议吗？想和我分享任何想法或观点吗？随时联系我<a class="ae kl" href="https://linkedin.com/in/jash-rathod-902512145" rel="noopener ugc nofollow" target="_blank"> <strong class="ko ja"> LinkedIn </strong> </a>。随时乐意帮忙！</p><p id="e431" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另外，你可以在GitHub的<a class="ae kl" href="https://github.com/jashrathod" rel="noopener ugc nofollow" target="_blank"><strong class="ko ja"/></a>和我的博客的<a class="ae kl" href="https://jashrathod.github.io" rel="noopener ugc nofollow" target="_blank"><strong class="ko ja"/></a>上查看其他作品。</p><p id="b096" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在那之前，在我的下一篇文章中再见！</p></div></div>    
</body>
</html>