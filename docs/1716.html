<html>
<head>
<title>Ensemble Methods Explained in Plain English: Bagging</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用简单英语解释的集合方法:装袋</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/ensemble-methods-explained-in-plain-english-bagging-47bef8ac7690?source=collection_archive---------2-----------------------#2021-03-28">https://pub.towardsai.net/ensemble-methods-explained-in-plain-english-bagging-47bef8ac7690?source=collection_archive---------2-----------------------#2021-03-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="408a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="a130" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用Python中的例子理解bagging背后的直觉</h2></div><p id="baa5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，我将介绍一种流行的同质模型集成方法——bagging。同质集成结合了大量的基础估计器或相同算法的弱学习器。</p><p id="2b1d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">同质集合背后的原则是“群体智慧”的想法——许多不同模型的集体预测比单个模型做出的任何一组预测都好。实现这一点有三个要求:</p><ol class=""><li id="3823" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">模型必须是独立的；</li><li id="3671" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">每个模型的表现都略好于随机猜测；</li><li id="8700" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">所有的单个模型本身都有相似的性能。</li></ol><p id="2264" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当这三个要求都满足时，添加更多的模型应该会提高您的整体性能。</p><p id="7a0d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">集成方法有助于减少方差并防止过度适应训练数据集，从而使模型能够更好地学习泛化模式，而不是过度适应训练数据集中的噪声。</p><h1 id="f5c6" class="mb mc it bd md me mf mg mh mi mj mk ml ki mm kj mn kl mo km mp ko mq kp mr ms bi translated">制袋材料</h1><h2 id="b3eb" class="mt mc it bd md mu mv dn mh mw mx dp ml la my mz mn le na nb mp li nc nd mr iz bi translated">装袋的工作原理</h2><p id="d120" class="pw-post-body-paragraph kr ks it kt b ku ne kd kw kx nf kg kz la ng lc ld le nh lg lh li ni lk ll lm im bi translated">在bagging中，大量独立的弱模型被组合在一起，以同样的目标学习同样的任务。术语“bagging”来自<code class="fe nj nk nl nm b"><strong class="kt jd">b</strong>ootstrap + <strong class="kt jd">agg</strong>regat<strong class="kt jd">ing</strong></code>，其中每个弱学习者都在用替换(bootstrapping)采样的数据的随机子样本上接受训练，然后模型的预测被聚合。</p><p id="edf8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Bootstrapping保证了独立性和多样性，因为每个数据子样本都是用替换法单独采样的，我们只剩下不同的子集来训练我们的基本估计量。</p><p id="6880" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">基本估计器是弱学习器，其性能仅比随机猜测稍好。这种模型的一个例子是最大深度限制为三的浅层决策树。然后通过平均将这些模型的预测结合起来。</p><p id="39f2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Bagging可以应用于分类和回归问题。对于回归问题，最终预测将是基本估计量预测的平均值(软投票)。对于分类问题，最终的预测将是多数票(硬投票)。</p><figure class="no np nq nr gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi nn"><img src="../Images/b85d5b62b6e8b362151740eb321134ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VtOUmMr25vw4-fvRkGUgWQ.png"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk translated">Bagging算法图</figcaption></figure><h2 id="b575" class="mt mc it bd md mu mv dn mh mw mx dp ml la my mz mn le na nb mp li nc nd mr iz bi translated">用Scikit-Learn实现Bagging算法</h2><p id="04a8" class="pw-post-body-paragraph kr ks it kt b ku ne kd kw kx nf kg kz la ng lc ld le nh lg lh li ni lk ll lm im bi translated">您可以使用Python包Scikit-Learn中的<code class="fe nj nk nl nm b"><a class="ae od" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html" rel="noopener ugc nofollow" target="_blank">BaggingRegressor</a></code>或<code class="fe nj nk nl nm b"><a class="ae od" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html" rel="noopener ugc nofollow" target="_blank">BaggingClassifier</a></code>构建自己的打包算法。</p><p id="b372" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，实例化您的基本估算器，并将其作为您的基本估算器输入到<code class="fe nj nk nl nm b">BaggingRegressor</code>或<code class="fe nj nk nl nm b">BaggingClassifier</code>中。下面是一个以线性回归作为基本估计量的bagging回归器示例，以及一个以决策树分类器作为基本估计量的bagging分类器示例。估计值的默认数量是10。</p><pre class="no np nq nr gt oe nm of og aw oh bi"><span id="7bf7" class="mt mc it nm b gy oi oj l ok ol">from sklearn.linear_model import LinearRegression<br/>from sklearn.ensemble import BaggingRegressor</span><span id="8677" class="mt mc it nm b gy om oj l ok ol">reg_lr = LinearRegression()<br/>reg_bag = BaggingRegressor(base_estimator=reg_lr)<br/>reg_bag.fit(X_train, y_train)</span><span id="257a" class="mt mc it nm b gy om oj l ok ol">====================================================================</span><span id="bde9" class="mt mc it nm b gy om oj l ok ol">from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import BaggingClassifier</span><span id="95b9" class="mt mc it nm b gy om oj l ok ol">clf_dt = DecisionTreeClassifier(max_depth=3)<br/>clf_bag = BaggingClassifier(base_estimator=clf_dt)<br/>clf_bag.fit(X_train, y_train)</span></pre><h2 id="0e54" class="mt mc it bd md mu mv dn mh mw mx dp ml la my mz mn le na nb mp li nc nd mr iz bi translated">随机森林</h2><p id="6004" class="pw-post-body-paragraph kr ks it kt b ku ne kd kw kx nf kg kz la ng lc ld le nh lg lh li ni lk ll lm im bi translated">随机森林是打包算法的一个流行例子。它使用平均来集成在训练数据集的子集上训练的多个单独的决策树。</p><p id="8efe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用Python中的<a class="ae od" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn的随机森林</a>，可以指定特定于树的参数。以下是一些需要调整的重要超参数，以便针对您的数据集进行优化:</p><ul class=""><li id="b733" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm on lt lu lv bi translated"><code class="fe nj nk nl nm b">n_estimators</code>:待汇总训练的树数。通常100到500棵树就足够了，一般来说，更多的树会改善你的模型(收益递减),但这也会增加计算量；</li><li id="3f89" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm on lt lu lv bi translated"><code class="fe nj nk nl nm b">max_depth</code>:一棵树的最大深度。更深的树将有助于减少偏差，但代价是增加方差。随机森林中多个树的聚合有助于解决这个问题，但您仍然应该小心；</li><li id="ecd7" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm on lt lu lv bi translated"><code class="fe nj nk nl nm b">max_features</code>:每次分割要考虑的最大特征数。一个好的起点通常是特性数量的平方根。</li></ul><p id="3e38" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随机森林中的另一个重要概念是出袋(OOB)分数。当执行自举时，会有实例被遗漏在用于训练估计器的子样本之外。这些样本外实例可用于评估模型，以获得随机(OOB)得分，本质上类似于随机森林模型的伪验证集。要获得OOB分数，请在初始化随机森林对象时设置<code class="fe nj nk nl nm b">oob_score=True</code>。</p><pre class="no np nq nr gt oe nm of og aw oh bi"><span id="34fe" class="mt mc it nm b gy oi oj l ok ol">rf = RandomForestClassifier(n_estimators=100, oob_score=True)<br/>rf.fit(X_train, y_train)<br/>print(rf.oob_score_)</span></pre><p id="5180" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请注意，如果您的要素中存在空值，scikit-learn的随机森林算法将返回一个错误，因此请记住在对您的数据调用fit之前用pandas的<code class="fe nj nk nl nm b">fillna</code>填充空值，否则它将抛出一个错误。</p><h2 id="00bc" class="mt mc it bd md mu mv dn mh mw mx dp ml la my mz mn le na nb mp li nc nd mr iz bi translated">装袋的利弊</h2><ul class=""><li id="0faf" class="ln lo it kt b ku ne kx nf la oo le op li oq lm on lt lu lv bi translated"><strong class="kt jd">减少方差</strong>:假设抽样是通过自举真正随机完成的，装袋通常有助于减少方差和防止过度拟合。</li><li id="72e0" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm on lt lu lv bi translated"><strong class="kt jd">易于并行化</strong>:估值器是独立的，因此模型可以与bagging并行构建。</li><li id="760c" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm on lt lu lv bi translated"><strong class="kt jd">更高的稳定性和稳健性</strong>:聚集在一起的大量估计量有助于为预测提供更高的稳定性和稳健性。</li><li id="b7f5" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm on lt lu lv bi translated"><strong class="kt jd">难以解释</strong>:bagging算法的最终预测是基于基本估计量的平均预测。虽然这提高了准确性，但解释模型变得更加困难。</li></ul><h1 id="8464" class="mb mc it bd md me mf mg mh mi mj mk ml ki mm kj mn kl mo km mp ko mq kp mr ms bi translated">摘要</h1><p id="4c8f" class="pw-post-body-paragraph kr ks it kt b ku ne kd kw kx nf kg kz la ng lc ld le nh lg lh li ni lk ll lm im bi translated">Bagging基于集体学习的思想，其中许多独立的弱学习者在数据的自举子样本上接受训练，然后通过平均进行聚合。它可以应用于分类和回归问题。</p><p id="5567" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随机森林是打包算法的一个流行例子。当在随机森林中为数据集调整超参数时，需要注意的三个重要方面是:I)树的数量(<code class="fe nj nk nl nm b">n_estimators</code>)；ii)修剪树(从<code class="fe nj nk nl nm b">max_depth</code>开始，但也要探索节点和/或分割所需的样本)iii)每次分割要考虑的最大要素数量(<code class="fe nj nk nl nm b">max_features</code>)。</p><p id="a7ba" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Bagging是一个非常强大的概念和模型集成方法的例子。希望这能激励你在下次处理预测建模问题时尝试bagging算法！</p></div></div>    
</body>
</html>