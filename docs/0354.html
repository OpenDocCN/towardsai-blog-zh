<html>
<head>
<title>NLP News Cypher | 03.08.20</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP新闻密码| 03.08.20</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/nlp-news-cypher-03-08-20-64918ffd1f?source=collection_archive---------1-----------------------#2020-03-09">https://pub.towardsai.net/nlp-news-cypher-03-08-20-64918ffd1f?source=collection_archive---------1-----------------------#2020-03-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/4374b30bb32cfd11378c70cf40c0dec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LXumBy0G_kE2h3hU"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">尼古拉·努南在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="4951" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph">自然语言处理每周时事通讯</h2><div class=""/><div class=""><h2 id="7a7c" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">它以这种方式肆虐</h2></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="9140" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">前进，我们走。在上周的<a class="ae jd" href="https://medium.com/towards-artificial-intelligence/nlp-news-cypher-03-01-20-ccfa7a6cf8" rel="noopener">专栏</a>中，我提出了一个关于情感分析复杂性的问题。在我们的例子中:</p><blockquote class="mh mi mj"><p id="9c6b" class="ll lm mk ln b lo lp kq lq lr ls kt lt ml lv lw lx mm lz ma mb mn md me mf mg ij bi translated">由于冠状病毒爆发的下行压力给股票带来压力，黄金在盘前上涨了6%。T9】</p></blockquote><p id="170a" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在处理复杂系统时，特别是在处理这个例子时，其后果是，一种一般化的方法是困难的。很难将复杂性降低到一个向量。但是如果我们自下而上，如果我们把它定位到客户，我们会看到隧道尽头的光。为了了解真相，我们必须将情感定位于用户的持有物。如果用户持有黄金，那么它是看涨的，如果客户押注黄金下跌，那么它是看跌的，如果没有头寸，那么它是中性的声明。换句话说，对于这个例子，个性化不仅仅是一个营销噱头，而是一个功能需求。没有完美的解决方案，每个独特的领域都需要自己的本地规则来解释基本事实。</p><p id="c143" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">尽管如此，这种说法是最难分析的，而且通常是一种边缘情况。但是在深度学习中，数据集中的离群值是模型被烟熏的原因。</p><p id="0cd8" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><em class="mk">(我们可能会遇到其他复杂的瓶颈，如n阶逻辑、不明确的基本事实、域转移等。我将在即将发布的白皮书中讨论这些和其他因素。敬请期待！)</em></p><p id="8a76" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">你这周过得怎么样？</p><p id="7347" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">顺便说一句，我们更新了BBN数据库，感谢所有的贡献者！</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="49f2" class="mu mv jg bd mw mx my mz na nb nc nd ne kv nf kw ng ky nh kz ni lb nj lc nk nl bi translated">本周:</h1><blockquote class="mh mi mj"><p id="db6e" class="ll lm mk ln b lo lp kq lq lr ls kt lt ml lv lw lx mm lz ma mb mn md me mf mg ij bi translated">行走的维基百科</p><p id="8323" class="ll lm mk ln b lo lp kq lq lr ls kt lt ml lv lw lx mm lz ma mb mn md me mf mg ij bi translated">从未标记数据中学习</p><p id="f4e8" class="ll lm mk ln b lo lp kq lq lr ls kt lt ml lv lw lx mm lz ma mb mn md me mf mg ij bi translated">我们中的一员</p><p id="94af" class="ll lm mk ln b lo lp kq lq lr ls kt lt ml lv lw lx mm lz ma mb mn md me mf mg ij bi translated">拥抱脸的笔记本</p><p id="79e0" class="ll lm mk ln b lo lp kq lq lr ls kt lt ml lv lw lx mm lz ma mb mn md me mf mg ij bi translated">构成语义</p><p id="5847" class="ll lm mk ln b lo lp kq lq lr ls kt lt ml lv lw lx mm lz ma mb mn md me mf mg ij bi translated">变压器尺寸、训练和压缩</p><p id="342d" class="ll lm mk ln b lo lp kq lq lr ls kt lt ml lv lw lx mm lz ma mb mn md me mf mg ij bi translated">制图知识</p><p id="2894" class="ll lm mk ln b lo lp kq lq lr ls kt lt ml lv lw lx mm lz ma mb mn md me mf mg ij bi translated">本周数据集:DVQA</p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="b123" class="mu mv jg bd mw mx nm mz na nb nn nd ne kv no kw ng ky np kz ni lb nq lc nk nl bi translated">行走的维基百科</h1><p id="4bfa" class="pw-post-body-paragraph ll lm jg ln b lo nr kq lq lr ns kt lt lu nt lw lx ly nu ma mb mc nv me mf mg ij bi translated">正在进行的用变压器定位图结构的研究继续前进。研究表明，一个新的模型如何能够遵循英语维基百科的推理路径来回答HotpotQA中发现的多跳问题。这意味着开放域规模。</p><p id="28e2" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">(上周的专栏有一篇类似的论文，看起来开放域、多跳在研究人员中真的越来越流行。✨😎)</p><p id="4519" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq"> GitHub: </strong></p><div class="ip iq gp gr ir nw"><a href="https://github.com/AkariAsai/learning_to_retrieve_reasoning_paths" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jq gy z fp ob fr fs oc fu fw jp bi translated">AkariAsai/learning _ to _ retrieve _ reasoning _ path</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">这是以下论文的正式实施:浅井明里，和马桥本，汉纳内赫Hajishirzi，理查德…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">github.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok ix nw"/></div></div></a></div><p id="e401" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">论文:</strong></p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ol mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://arxiv.org/pdf/1911.10470.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="416d" class="mu mv jg bd mw mx my mz na nb nc nd ne kv nf kw ng ky nh kz ni lb nj lc nk nl bi translated">从未标记数据中学习</h1><p id="a6bb" class="pw-post-body-paragraph ll lm jg ln b lo nr kq lq lr ns kt lt lu nt lw lx ly nu ma mb mc nv me mf mg ij bi translated">Thang的演讲幻灯片，2020年2月的某个时候(Thang是最近来自Google的Meena chatbot论文/模型的作者)。他回顾了彻底改变了NLP的自我监督学习的重要性，以及它是如何传播到计算机视觉的。接近尾声时，他讨论了米娜论文:</p><div class="ip iq gp gr ir nw"><a href="https://drive.google.com/file/d/1ax1-XprJHDRRv2Ru3dJwPLs3ShxcpQ3r/view" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jq gy z fp ob fr fs oc fu fw jp bi translated">ThangLuong-talk-Feb-2020.pdf</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">编辑描述</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">drive.google.com</p></div></div><div class="of l"><div class="om l oh oi oj of ok ix nw"/></div></div></a></div><h1 id="5ad2" class="mu mv jg bd mw mx my mz na nb nc nd ne kv nf kw ng ky nh kz ni lb nj lc nk nl bi translated">我们中的一员</h1><p id="88e7" class="pw-post-body-paragraph ll lm jg ln b lo nr kq lq lr ns kt lt lu nt lw lx ly nu ma mb mc nv me mf mg ij bi translated">我们有一个新的NLU框架可以玩！框架的伟大之处在于它允许您扩展实验，这也是Jiant利用配置文件的原因。是的，它建在PyTorch的顶部。</p><p id="c01a" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">博客:</strong></p><div class="ip iq gp gr ir nw"><a href="https://jiant.info/" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jq gy z fp ob fr fs oc fu fw jp bi translated">用于通用文本理解模型的jiant工具包</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">jiant是一个进行中的自然语言处理研究软件工具包，旨在促进…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">jiant.info</p></div></div></div></a></div><p id="26b1" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq"> GitHub: </strong></p><div class="ip iq gp gr ir nw"><a href="https://github.com/nyu-mll/jiant" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jq gy z fp ob fr fs oc fu fw jp bi translated">纽约大学-mll/jiant</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">jiant是一个用于自然语言处理研究的软件工具包，旨在促进多任务学习的工作</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">github.com</p></div></div></div></a></div><h1 id="445d" class="mu mv jg bd mw mx my mz na nb nc nd ne kv nf kw ng ky nh kz ni lb nj lc nk nl bi translated">拥抱脸的笔记本</h1><p id="e462" class="pw-post-body-paragraph ll lm jg ln b lo nr kq lq lr ns kt lt lu nt lw lx ly nu ma mb mc nv me mf mg ij bi translated">几天前，🤗展示了4款Colab笔记本电脑，帮助您的NLP管道从他们的图书馆开始。请关注这个GitHub页面，因为他们正在寻找社区来做出贡献。</p><div class="ip iq gp gr ir nw"><a href="https://github.com/huggingface/transformers/tree/master/notebooks" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jq gy z fp ob fr fs oc fu fw jp bi translated">拥抱脸/变形金刚</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">你可以在这里找到拥抱脸提供的官方笔记本列表。此外，我们想在这里列出…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">github.com</p></div></div><div class="of l"><div class="on l oh oi oj of ok ix nw"/></div></div></a></div><h1 id="4a6c" class="mu mv jg bd mw mx my mz na nb nc nd ne kv nf kw ng ky nh kz ni lb nj lc nk nl bi translated">构成语义</h1><p id="a9c3" class="pw-post-body-paragraph ll lm jg ln b lo nr kq lq lr ns kt lt lu nt lw lx ly nu ma mb mc nv me mf mg ij bi translated">谷歌发布了一个新的数据集，名为组合自由基础问题(CFQ)和一个新的基准来衡量一个程序概括组合性的程度。如果你认为语义解析已经被忽略了，你应该看看这个:</p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/3af96f02061c767ecb5a9e5667be5f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*DnFGDbnsCooCNRHT.gif"/></div></figure><div class="ip iq gp gr ir nw"><a href="https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jq gy z fp ob fr fs oc fu fw jp bi translated">测量成分概括</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">人们能够学习一个新单词的意思，然后将其应用到其他语言环境中。作为湖和…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">ai.googleblog.com</p></div></div><div class="of l"><div class="op l oh oi oj of ok ix nw"/></div></div></a></div><h1 id="1505" class="mu mv jg bd mw mx my mz na nb nc nd ne kv nf kw ng ky nh kz ni lb nj lc nk nl bi translated">变压器尺寸、训练和压缩</h1><p id="78b1" class="pw-post-body-paragraph ll lm jg ln b lo nr kq lq lr ns kt lt lu nt lw lx ly nu ma mb mc nv me mf mg ij bi translated">所以增加模型的规模提高了训练/推理速度？反直觉对吗？伯克利的新研究强调了一个有趣的权衡。它表明，训练非常大的模型并尽早切断它们比使用训练更多时期的较小模型要好得多。此外，当压缩非常大的模型时，你会比少量压缩较小的模型得到更多的回报。</p><p id="c1b5" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">博客</strong>:</p><div class="ip iq gp gr ir nw"><a href="https://bair.berkeley.edu/blog/2020/03/05/compress/" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jq gy z fp ob fr fs oc fu fw jp bi translated">通过增加模型大小来加速变压器训练和推理</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">在深度学习中，使用更多计算(例如，增加模型大小、数据集大小或训练步骤)通常会导致…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">bair.berkeley.edu</p></div></div><div class="of l"><div class="oq l oh oi oj of ok ix nw"/></div></div></a></div><h1 id="78f8" class="mu mv jg bd mw mx my mz na nb nc nd ne kv nf kw ng ky nh kz ni lb nj lc nk nl bi translated">制图知识</h1><p id="38df" class="pw-post-body-paragraph ll lm jg ln b lo nr kq lq lr ns kt lt lu nt lw lx ly nu ma mb mc nv me mf mg ij bi translated">如果你想了解关于知识图表的所有事情，上周发布了一篇全面的论文👀：</p><figure class="mo mp mq mr gt is"><div class="bz fp l di"><div class="ol mt l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://arxiv.org/pdf/2003.02320.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="0f33" class="mu mv jg bd mw mx my mz na nb nc nd ne kv nf kw ng ky nh kz ni lb nj lc nk nl bi translated">本周数据集:<strong class="ak"> DVQA </strong></h1><p id="e8b1" class="pw-post-body-paragraph ll lm jg ln b lo nr kq lq lr ns kt lt lu nt lw lx ly nu ma mb mc nv me mf mg ij bi translated"><strong class="ln jq">什么事？</strong></p><p id="8bef" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">DVQA数据集将条形图理解转换为问答框架。</p><p id="4245" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">样本:</strong></p><figure class="mo mp mq mr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/3467661834d8edaf21826d0983d5323e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BKAKVLL8aKM2LN0e"/></div></div></figure><p id="6dec" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">在哪里？</strong></p><div class="ip iq gp gr ir nw"><a href="https://github.com/kushalkafle/DVQA_dataset" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jq gy z fp ob fr fs oc fu fw jp bi translated">kushalkafle/DVQA_dataset</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">这个库提供了图像、元数据和问答配对，如论文:DVQA:理解…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">github.com</p></div></div><div class="of l"><div class="os l oh oi oj of ok ix nw"/></div></div></a></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><blockquote class="ot"><p id="1c85" class="ou ov jg bd ow ox oy oz pa pb pc mg dk translated"><em class="pd">每周日，我们都会对来自世界各地的研究人员的NLP新闻和代码进行每周综述。</em></p><p id="edc1" class="ou ov jg bd ow ox oy oz pa pb pc mg dk translated">如果您喜欢这篇文章，请帮助我们并与朋友分享！</p><p id="69af" class="ou ov jg bd ow ox oy oz pa pb pc mg dk translated"><em class="pd">如需完整报道，请关注我们的Twitter:</em><a class="ae jd" href="http://twitter.com/Quantum_Stat" rel="noopener ugc nofollow" target="_blank"><em class="pd">@ Quantum _ Stat</em></a></p></blockquote><figure class="pf pg ph pi pj is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/0e50f74ee3189e6b9468cf05ea22071f.png" data-original-src="https://miro.medium.com/v2/resize:fit:108/0*7AfU76ycEZDHJ0FO"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="http://www.quantumstat.com/" rel="noopener ugc nofollow" target="_blank">www.quantumstat.com</a></figcaption></figure></div></div>    
</body>
</html>