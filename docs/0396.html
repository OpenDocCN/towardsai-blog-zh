<html>
<head>
<title>Let’s see the sales in a Big Mart Store</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们看看大型超市的销售情况</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/lets-see-the-sales-in-a-big-mart-store-250fb7e1f704?source=collection_archive---------2-----------------------#2020-04-11">https://pub.towardsai.net/lets-see-the-sales-in-a-big-mart-store-250fb7e1f704?source=collection_archive---------2-----------------------#2020-04-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/f4848c368e540bbfbe015e1bc438b5e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k87FAYnO7k5ce5kHJtN3qw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图片由Unsplash网站上的Anastasia Dulgier 提供</figcaption></figure><div class=""/><blockquote class="kd"><p id="4eaa" class="ke kf jg bd kg kh ki kj kk kl km kn dk translated">预测特定商店中每种产品的销售额</p></blockquote><p id="8c88" class="pw-post-body-paragraph ko kp jg kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk kn ij bi ll translated"><span class="l lm ln lo bm lp lq lr ls lt di">我们</span>作为数据分析师和数据科学从业者，经常面临预测影响刺激因素的挑战。这些因素包括预测商店销售、预测明天的天气、预测第二天的股票价格等等。</p><p id="7dbf" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">我们知道<a class="ae jd" href="https://www.analyticsvidhya.com/blog/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq jh"> <em class="lz">分析Vidhya </em> </strong> </a> <strong class="kq jh"> </strong>是一个非常知名的平台，可以了解几乎所有与分析相关的内容。全球许多经验丰富的数据科学家、分析师和开发人员通过浏览他们信息丰富的文章，不断更新数据科学和机器学习领域的最新动态，这些文章包含了几乎所有我们需要知道的与该领域相关的信息，包括我在内。</p><blockquote class="ma mb mc"><p id="4d15" class="ko kp lz kq b kr lu kt ku kv lv kx ky md lw lb lc me lx lf lg mf ly lj lk kn ij bi translated">他们经常在自己的网站上举办各种黑客马拉松，帮助我们踏上数据科学之旅。事实上，了解更多数据科学的最佳方式之一是坚持编码，参加黑客马拉松，并亲自检查他们的结果。他们平台中的一个这样的黑客马拉松处理销售商店中特定商品的预测。</p></blockquote><blockquote class="kd"><p id="70e5" class="ke kf jg bd kg kh ki kj kk kl km kn dk translated">本次竞赛使用的评估指标是<strong class="ak">均方根值</strong>。</p></blockquote><blockquote class="ma mb mc"><p id="f741" class="ko kp lz kq b kr ks kt ku kv kw kx ky md la lb lc me le lf lg mf li lj lk kn ij bi translated"><strong class="kq jh">均方根误差</strong> ( <strong class="kq jh"> RMSE </strong>)是残差(预测误差)的标准差。残差是数据点离回归线有多远的度量；<strong class="kq jh"> RMSE </strong>是衡量这些残差分布程度的指标。换句话说，它告诉我们，数据在最佳拟合线附近有多集中。</p></blockquote><p id="96a0" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">对于这场比赛，我们有一堆的功能，我们被给予，我们需要预测每个产品在商店的销售。</p><p id="6eb1" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated"><strong class="kq jh">要查看这一款的完整代码，请随意查看我的Github代码</strong> <a class="ae jd" href="https://github.com/SaiBiswas/Big-Mart-Sales-Hackathon" rel="noopener ugc nofollow" target="_blank"> <strong class="kq jh">这里</strong> </a> <strong class="kq jh">。</strong></p><figure class="mh mi mj mk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/d81bcbb2a02ca067436da5b72fd637fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ApqY5soOddRufcQKVrH1w.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">没有这个就无法预测，我们能不能…..</figcaption></figure><p id="f166" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">我们将这篇文章分成三个部分。</p><ol class=""><li id="9317" class="ml mm jg kq b kr lu kv lv kz mn ld mo lh mp kn mq mr ms mt bi translated"><strong class="kq jh"> EDA(探索性数据分析)</strong>:我们会看到一些关于这一个的训练数据的分析。</li><li id="4b75" class="ml mm jg kq b kr mu kv mv kz mw ld mx lh my kn mq mr ms mt bi translated"><strong class="kq jh">特征工程</strong>:这是我们可以看到的有助于预测的特征。</li><li id="abc6" class="ml mm jg kq b kr mu kv mv kz mw ld mx lh my kn mq mr ms mt bi translated">建模:这是魔法发生的地方，我们将利用机器学习的力量看到它在我们眼前展现。</li></ol><p id="204a" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">所以，事不宜迟，让我们看看代码。</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="9df2" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># importing all the important libraries for analysis</em><br/><br/><strong class="na jh">import</strong> <strong class="na jh">pandas</strong> <strong class="na jh">as</strong> <strong class="na jh">pd</strong><br/><strong class="na jh">import</strong> <strong class="na jh">numpy</strong> <strong class="na jh">as</strong> <strong class="na jh">np</strong><br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.model_selection</strong> <strong class="na jh">import</strong> train_test_split<br/>pd.set_option('display.max_columns', <strong class="na jh">None</strong>)<br/><strong class="na jh">import</strong> <strong class="na jh">pandas_profiling</strong> <strong class="na jh">as</strong> <strong class="na jh">pp</strong><br/><br/><strong class="na jh">import</strong> <strong class="na jh">matplotlib.pyplot</strong> <strong class="na jh">as</strong> <strong class="na jh">plt</strong><br/><strong class="na jh">import</strong> <strong class="na jh">seaborn</strong> <strong class="na jh">as</strong> <strong class="na jh">sns</strong><br/><strong class="na jh">import</strong> <strong class="na jh">warnings</strong><br/>%matplotlib inline<br/>plt.style.use('fivethirtyeight')<br/>warnings.filterwarnings('ignore')</span></pre></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><pre class="mz na nb nc aw nd bi"><span id="e27e" class="ne nf jg na b gy nr ns nt nu nv nh l ni nj"><em class="lz"># loading the train and the test data</em></span><span id="7fa7" class="ne nf jg na b gy nw nh l ni nj">train = pd.read_csv('train_bigmart.csv')<br/>test = pd.read_csv('test_bigmart.csv')</span></pre></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><p id="ccd1" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated"><strong class="kq jh"> <em class="lz"> Item_Weight </em> </strong>和<strong class="kq jh"> <em class="lz"> Outlet_Size </em> </strong>变量中有空值。因此，我们将通过将<strong class="kq jh"> <em class="lz"> Item_Weight </em> </strong>替换为其<strong class="kq jh"> mean </strong>并将<strong class="kq jh"> <em class="lz"> Outlet_Size </em> </strong>替换为其<strong class="kq jh"> mode </strong>来解决这个问题。</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="e096" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># filling the missing values in the Item_Weight column with mean and Outlet_Size with mode as there are missing values</em></span><span id="c087" class="ne nf jg na b gy nw nh l ni nj">data['Item_Weight'].fillna(data['Item_Weight'].mean(), inplace = <strong class="na jh">True</strong>)<br/><br/>data['Outlet_Size'].fillna(data['Outlet_Size'].mode()[0], inplace = <strong class="na jh">True</strong>)<br/><br/>data['Item_Outlet_Sales'] = data['Item_Outlet_Sales'].replace(0, np.NaN)<br/>data['Item_Outlet_Sales'].fillna(data['Item_Outlet_Sales'].mode()[0], inplace = <strong class="na jh">True</strong>)</span></pre></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><p id="389b" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">我们每个人在Jupyter笔记本上必须拥有的另一个重要的EDA库是<a class="ae jd" href="https://github.com/pandas-profiling/pandas-profiling" rel="noopener ugc nofollow" target="_blank"><strong class="kq jh"><em class="lz">pandas profiling</em></strong></a>库，它可以帮助我们浏览数据，我们可以检查从变量系数到变量分布的几乎任何内容。这个库的安装相当简单，一个简单的pip安装就可以完成这项工作，然后我们就可以让这个库发挥它的魔力了。</p></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><p id="e570" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">现在，我们将看到一些重要的功能，我们将需要在这一个完整的建模。</p><p id="acec" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">我们将创建一些特征，用于这一个的建模。</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="5470" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># Getting the first two characters of ID to separate them into different categories</em><br/><br/>data['Item_Identifier'] = data['Item_Identifier'].apply(<strong class="na jh">lambda</strong> x: x[0:2])<br/><br/>data['Item_Identifier'] = data['Item_Identifier'].map({'FD':'Food', 'NC':'Non_Consumable', 'DR':'Drinks'})<br/><br/>data['Item_Identifier'].value_counts()</span></pre></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><p id="961a" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">为我们的建模目的创建一个新特征<strong class="kq jh"> <em class="lz">插座设施</em> </strong>。</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="b923" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># determining the time establishment started</em><br/><br/>data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']<br/>data['Outlet_Years'].value_counts()</span></pre></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><pre class="mz na nb nc aw nd bi"><span id="0ef5" class="ne nf jg na b gy nr ns nt nu nv nh l ni nj"><em class="lz"># Getting the first two characters of ID to separate them into different categories</em><br/><br/>data['Item_Identifier'] = data['Item_Identifier'].apply(<strong class="na jh">lambda</strong> x: x[0:2])<br/><br/>data['Item_Identifier'] = data['Item_Identifier'].map({'FD':'Food', 'NC':'Non_Consumable', 'DR':'Drinks'})<br/><br/>data['Item_Identifier'].value_counts()</span></pre></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><p id="297d" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">下一步是标签编码数据为<strong class="kq jh"> <em class="lz">标签编码</em> </strong>是指将<strong class="kq jh">标签</strong>转换成数字形式，从而转换成机器可读的形式。然后，机器学习算法可以以更好的方式决定如何操作这些<strong class="kq jh">标签</strong>。在监督学习中，结构化数据集是非常重要的。</p><p id="6534" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">一旦我们完成了标签编码，我们继续进行<strong class="kq jh"> <em class="lz">一个热编码</em> </strong>数据，或者简单地说，我们将数据转换成它的虚拟形式，我们这样做是为了让我们的机器理解分类变量。</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="5ce8" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># one hot encoding the data to get dummy variables</em><br/><br/>data = pd.get_dummies(data)<br/><br/>print(data.shape)</span></pre></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><p id="4045" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">下一步是为训练和测试拆分数据。</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="5720" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># splitting into train and test for modelling</em><br/><br/>train = data.iloc[:8523,:] # all rows till 8523 and all cols<br/>test = data.iloc[8523:,:]  # last row and last col</span><span id="a399" class="ne nf jg na b gy nw nh l ni nj"><em class="lz"># making x_train, x_test, y_train, y_test</em><br/><br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.model_selection</strong> <strong class="na jh">import</strong> train_test_split<br/><br/>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)<br/><br/>print(x_train.shape)<br/>print(y_train.shape)<br/>print(x_test.shape)<br/>print(y_test.shape)</span></pre><p id="2313" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">3.<strong class="kq jh">造型</strong>。</p><p id="99dc" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">A.线性回归</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="c735" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># Linear Regression</em><br/><br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.linear_model</strong> <strong class="na jh">import</strong> LinearRegression<br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.metrics</strong> <strong class="na jh">import</strong> mean_squared_error<br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.metrics</strong> <strong class="na jh">import</strong> r2_score<br/><br/>model = LinearRegression()<br/>model.fit(x_train, y_train)<br/><br/><em class="lz"># predicting the  test set results</em><br/>y_pred = model.predict(x_test)<br/>print(y_pred)<br/><br/><em class="lz"># finding the mean squared error and variance</em><br/>mse = mean_squared_error(y_test, y_pred)<br/>print('RMSE :', np.sqrt(mse))<br/>print('Variance score: <strong class="na jh">%.2f</strong>' % r2_score(y_test, y_pred))</span><span id="7d73" class="ne nf jg na b gy nw nh l ni nj"><strong class="na jh">OUTPUT</strong></span><span id="ada3" class="ne nf jg na b gy nw nh l ni nj">RMSE : 9.186108167282568e-13<br/>Variance score: 1.00</span></pre><p id="0c04" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">B.随机森林回归量</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="a7f9" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># Random Forest Regressor</em><br/><br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.ensemble</strong> <strong class="na jh">import</strong> RandomForestRegressor<br/><br/>model = RandomForestRegressor(n_estimators = 100 , n_jobs = -1)<br/>model.fit(x_train, y_train)<br/><br/><em class="lz"># predicting the  test set results</em><br/>y_pred = model.predict(x_test)<br/>print(y_pred)<br/><br/><em class="lz"># finding the mean squared error and variance</em><br/>mse = mean_squared_error(y_test, y_pred)<br/>print("RMSE :",np.sqrt(mse))<br/>print('Variance score: <strong class="na jh">%.2f</strong>' % r2_score(y_test, y_pred))<br/><br/>print("Result :",model.score(x_train, y_train))</span><span id="d6f1" class="ne nf jg na b gy nw nh l ni nj"><strong class="na jh">OUTPUT</strong></span><span id="a2c5" class="ne nf jg na b gy nw nh l ni nj">RMSE : 39.62801138375659<br/>Variance score: 1.00<br/>Result : 0.9999916265613055</span></pre><p id="e861" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">C.支持向量机</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="389a" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># Support Vector Machine</em><br/><br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.svm</strong> <strong class="na jh">import</strong> SVR<br/><br/>model = SVR()<br/>model.fit(x_train, y_train)<br/><br/><em class="lz"># predicting the x test results</em><br/>y_pred = model.predict(x_test)<br/><br/><em class="lz"># Calculating the RMSE Score</em><br/>mse = mean_squared_error(y_test, y_pred)<br/>print("RMSE :", np.sqrt(mse))<br/>print('Variance score: <strong class="na jh">%.2f</strong>' % r2_score(y_test, y_pred))<br/><br/>print("Result :",model.score(x_train, y_train))</span><span id="018f" class="ne nf jg na b gy nw nh l ni nj"><strong class="na jh">OUTPUT</strong></span><span id="9ab6" class="ne nf jg na b gy nw nh l ni nj">Variance score: 0.74<br/>Result : 0.7490125535919516</span></pre><p id="b5ae" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">D.梯度推进算法</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="395d" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># Gradient Boosting Algorithm</em><br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.ensemble</strong> <strong class="na jh">import</strong> GradientBoostingRegressor<br/><br/>model = GradientBoostingRegressor()<br/>model.fit(x_train, y_train)<br/><br/><em class="lz"># predicting the test set results</em><br/>y_pred = model.predict(x_test)<br/>print(y_pred)<br/><br/><em class="lz"># Calculating the root mean squared error</em><br/>print("RMSE :", np.sqrt(((y_test - y_pred)**2).sum()/len(y_test)))<br/>print('Variance score: <strong class="na jh">%.2f</strong>' % r2_score(y_test, y_pred))<br/><br/>print("Result :",model.score(x_train, y_train))</span><span id="0c4a" class="ne nf jg na b gy nw nh l ni nj"><strong class="na jh">OUTPUT</strong></span><span id="d08c" class="ne nf jg na b gy nw nh l ni nj">RMSE : 30.42838358308164<br/>Variance score: 1.00<br/>Result : 0.9999400663817338</span></pre><p id="73e4" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">E.决策树回归器</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="2592" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># Decision Tree Regressor </em><br/><br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.tree</strong> <strong class="na jh">import</strong> DecisionTreeRegressor<br/><br/>model = DecisionTreeRegressor()<br/>model.fit(x_train, y_train)<br/><br/><em class="lz"># predicting the test set results</em><br/>y_pred = model.predict(x_test)<br/>print(y_pred)<br/><br/>print(" RMSE : " , np.sqrt(((y_test - y_pred)**2).sum()/len(y_test)))<br/>print('Variance score: <strong class="na jh">%.2f</strong>' % r2_score(y_test, y_pred))<br/><br/>print("Result :",model.score(x_train, y_train))</span><span id="b307" class="ne nf jg na b gy nw nh l ni nj"><strong class="na jh">OUTPUT</strong></span><span id="5d1f" class="ne nf jg na b gy nw nh l ni nj">RMSE :  30.028006703468485<br/>Variance score: 1.00<br/>Result : 1.0</span></pre><p id="c809" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">6.Adaboost回归器</p><pre class="mh mi mj mk gt mz na nb nc aw nd bi"><span id="4d37" class="ne nf jg na b gy ng nh l ni nj"><em class="lz"># Adaboost</em><br/><strong class="na jh">from</strong> <strong class="na jh">sklearn.ensemble</strong> <strong class="na jh">import</strong> AdaBoostRegressor<br/><br/>model =  AdaBoostRegressor(random_state=0, n_estimators=100)<br/>model.fit(x_train, y_train)<br/><br/><em class="lz"># predicting the test set results</em><br/>y_pred = model.predict(x_test)<br/>print(y_pred)<br/><br/>print(" RMSE : " , np.sqrt(((y_test - y_pred)**2).sum()/len(y_test)))<br/>print('Variance score: <strong class="na jh">%.2f</strong>' % r2_score(y_test, y_pred))<br/><br/>print("Result :",model.score(x_train, y_train))</span><span id="f32b" class="ne nf jg na b gy nw nh l ni nj"><strong class="na jh">OUTPUT</strong></span><span id="7d9d" class="ne nf jg na b gy nw nh l ni nj">RMSE :  121.48048104773004<br/>Variance score: 0.99<br/>Result : 0.9939541181004856</span></pre><p id="dbed" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi ll translated"><span class="l lm ln lo bm lp lq lr ls lt di">答</span>在对<strong class="kq jh">训练</strong>数据使用<strong class="kq jh"> <em class="lz"> 6算法</em> </strong>并对未见过的测试数据进行同样的测试后，我们可以看到，线性回归产生了最低的<strong class="kq jh"> RMSE </strong>得分9.18，因为<strong class="kq jh"> RMSE </strong>值越低越好，因为它能够以更好的方式解释数据点中的方差。</p><blockquote class="kd"><p id="c56a" class="ke kf jg bd kg kh nx ny nz oa ob kn dk translated">RMSE由预测值与观察数据点的接近程度来定义。RMSE值越低表示拟合度越好。因此，将选择线性回归结果。</p></blockquote><figure class="od oe of og oh is gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/63c8a47f64d159dddc5bca3f1d130376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*7SSxaG1aOYUanIVKbdzL7A.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">图片来源谷歌</figcaption></figure><p id="14a1" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated">这是真的，不是吗？我们越是拷问数据，试图从中获得洞见，当我们将数据用于预测时，它就会承认并导致更好的结果，就像我们在这个案例中看到的那样，我们通过6种算法并选择RMSE最小的算法。</p><blockquote class="ma mb mc"><p id="5104" class="ko kp lz kq b kr lu kt ku kv lv kx ky md lw lb lc me lx lf lg mf ly lj lk kn ij bi translated">可以查看更多我写的机器学习相关文章:</p></blockquote><div class="ip iq gp gr ir oi"><a href="https://medium.com/towards-artificial-intelligence/gradient-descent-in-layman-language-d4028b486103" rel="noopener follow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jh gy z fp on fr fs oo fu fw jf bi translated">梯度下降:用外行人的话来说</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">在5分钟内介绍最流行和最常用的机器学习优化技术。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">medium.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow ix oi"/></div></div></a></div><div class="ip iq gp gr ir oi"><a href="https://medium.com/towards-artificial-intelligence/importance-of-k-fold-cross-validation-in-machine-learning-a0d76f49493e" rel="noopener follow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jh gy z fp on fr fs oo fu fw jf bi translated">K-Fold交叉验证在机器学习中的重要性</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">在将数据输入我们的机器学习模型之前，最重要的步骤之一</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">medium.com</p></div></div><div class="or l"><div class="ox l ot ou ov or ow ix oi"/></div></div></a></div><p id="fe48" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated"><em class="lz">你可以在Linkedin </em> <a class="ae jd" href="https://www.linkedin.com/in/saikat-biswas-713610159/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq jh"> <em class="lz">这里</em> </strong> </a>进一步和我联系</p><p id="8c0c" class="pw-post-body-paragraph ko kp jg kq b kr lu kt ku kv lv kx ky kz lw lb lc ld lx lf lg lh ly lj lk kn ij bi translated"><em class="lz">或在我的推特账号上与我联系</em> <a class="ae jd" href="https://twitter.com/saitkaka" rel="noopener ugc nofollow" target="_blank"> <strong class="kq jh"> <em class="lz">此处</em> </strong> </a></p><blockquote class="kd"><p id="102a" class="ke kf jg bd kg kh nx ny nz oa ob kn dk translated">这本书里就有这些。下次见，再见..！！！</p></blockquote></div></div>    
</body>
</html>