<html>
<head>
<title>Model Explainability - SHAP vs. LIME vs. Permutation Feature Importance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模型可解释性——SHAP对莱姆对置换特征的重要性</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/model-explainability-shap-vs-lime-vs-permutation-feature-importance-98484efba066?source=collection_archive---------0-----------------------#2022-07-22">https://pub.towardsai.net/model-explainability-shap-vs-lime-vs-permutation-feature-importance-98484efba066?source=collection_archive---------0-----------------------#2022-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4052" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">解释的方式我希望有人给我解释一下。我90岁的奶奶会懂这个</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/ce4b735dcc74510627df461cc57d06b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YRKIFZntwZ6BhSgBaD0oEw.jpeg"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://unsplash.com/@hongochai10?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Hồ Ngọc Hải </a>在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="f67e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">解释复杂的模型有助于我们理解模型如何以及为什么做出决策，以及哪些特征在得出该结论时是重要的，这将有助于克服在决策中使用机器学习的信任和道德问题。选择期望的模型解释通常取决于以下三个问题的答案:(I)模型是否足够简单以提供内在的解释，(ii)可解释的模型应该是模型特定的还是模型不可知的？以及(iii)我们想要本地的还是全球的解释？</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi lw"><img src="../Images/281ab81ff4364877b13b2429aa2fc068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g9G-velEE-z0TYg4m1EFRA.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">图片作者。</figcaption></figure><p id="41dc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本文中，我们将了解一些用于模型可解释性的事后的、局部的和模型不可知的技术。这类方法的几个例子是PFI排列特征重要性(费希尔，a .等人，2018年)，石灰局部可解释模型不可知解释(里贝罗等人，2016年)，和SHAP沙普利加法解释(伦德伯格，S. M .，&amp;李，S. I .，2017年)。该员额将分为以下几个部分:</p><p id="ac79" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> A .数据介绍</strong></p><p id="4af1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> B .解释3种技术背后的直觉:SHAP、莱姆和排列特征重要性</strong></p><p id="a9e4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> C .潜在的陷阱</strong></p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="4047" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">得到💬任何数据科学或编程问题的GPT式答案。为成千上万的人生成摘要和学习笔记📚只需一次点击即可获得学习资源。👉</p><div class="me mf gp gr mg mh"><a href="https://aigents.co/learn" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd iu gy z fp mm fr fs mn fu fw is bi translated">面向数据科学家和开发人员的免费学习资源。精选的博客、教程、书籍和…</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">机器学习和人工智能工程师的培训课程、黑客马拉松、活动和工作</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">aigents.co</p></div></div><div class="mq l"><div class="mr l ms mt mu mq mv kt mh"/></div></div></a></div></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="d358" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">A.数据介绍</h1><p id="f7b6" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">在整篇文章中，模型和技术应用于实时系列数据集，包括:(I)2018-2022年期间荷兰的电力消耗数据，这是非公开数据，以及(ii)荷兰的天气数据，这是荷兰皇家气象研究所的公开数据，可在此处访问<a class="ae kz" href="https://daggegevens.knmi.nl/klimatologie/daggegevens" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="492d" class="mw mx it bd my mz nt nb nc nd nu nf ng jz nv ka ni kc nw kd nk kf nx kg nm nn bi translated">B.直觉</h1><h2 id="cf95" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated"><strong class="ak"> 1。SHAP </strong></h2><p id="070f" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">SHAP-代表Shapley Additive exPlanations，是一种算法，首次发表于2017年[1]，是对任何黑盒模型的输出进行逆向工程的一种很好的方式。SHAP是一个框架，它提供了计算Shapley值的高效工具，Shapley值是合作博弈论中的一个概念，可以追溯到1950年。</p><h2 id="9102" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated">博弈论和机器学习的可解释性</h2><p id="f8e5" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">博弈论和机器学习可解释性有什么联系？我们不再有一个机器学习问题，即我们训练一个使用多个特征来创建预测的模型，我们现在想象一个游戏，其中每个特征(“玩家”)合作来获得一个预测(“得分”)。解释机器学习现在变成了问这样一个问题:每个玩家(特征)如何有助于获得那个分数(预测)[2]？答案是<strong class="lc iu">每个特性的贡献由Shapley值给出，它告诉我们如果我们在没有那个特性</strong>的情况下玩那个游戏，我们会赢或输多少分。换句话说，Shapley值有助于我们解释如何在特征之间分配预测。</p><h2 id="481d" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated">沙普利值</h2><p id="35f2" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">更准确地说，计算Shapley值需要知道所有玩家组合的博弈结果。一个独特的组合被称为联盟[2]。例如，如果我们有一个有三个玩家的游戏，Alice，Bob &amp; Cheryl，一些有效的联盟是(Alice)和(Bob，Cheryl)。可能的联盟总数将是2 × 2 × 2 = 2^3 = 8，因为每个玩家都可以在游戏中或出局。在与所有可能的联盟博弈之后，我们准备计算每个玩家的Shapley值。让我们首先考虑爱丽丝。【Alice的Shapley值是她在场的游戏和她不在场的游戏得分差异的加权和:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/bf1be1ff84c7d08afece8327ff6b0848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*JNaXS8l_Y-PITRX4bikaPg.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">等式1。图片作者。基于<a class="ae kz" href="https://arxiv.org/abs/1705.07874" rel="noopener ugc nofollow" target="_blank"> SHAP的论文</a></figcaption></figure><p id="dfd5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">其中ωi是权重，将在下面进一步解释，表示该游戏中缺席的玩家。例如，f (xA，-，xC)是Bob缺席的博弈，f()是所有参与者都不参与的博弈。<br/>权重由下式给出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/de45426f9bf62841bcf745d25a8303be.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*k7Xgsps1sOCTSZUFDf-BFg.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">等式2。图片作者。基于<a class="ae kz" href="https://arxiv.org/abs/1705.07874" rel="noopener ugc nofollow" target="_blank"> SHAP论文</a></figcaption></figure><p id="38d1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">其中|S|是联盟中玩家的数量，包括爱丽丝。对于等式1中的<br/>四行，我们有|S| = 1，2，2，3，相应地ω1，2，3，4 = 1/3，1/6，1/6，1/3。</p><p id="d1ea" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">回到解释机器学习模型，我们希望<br/>使用等式(1)来计算特征在创建预测中的贡献。通常一个模型有一个固定的输入，我们都知道如果我们删除/添加一个特性，模型就会改变。因此，通过搜索所有可能的特征组合来计算Shapley值将意味着对每个可能的特征子集重新训练模型，这在计算上是昂贵的，并且在现实世界中可能毫无意义。然而，我们可以像SHAP的作者建议的那样做一个<strong class="lc iu">近似:从模型中移除一个或多个特征大约等于计算被移除特征的所有可能值的预测期望值</strong>。让我们看一个例子来说明这是如何实现的。</p><p id="f921" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">假设我们有一个具有三个特征A、B和c的模型。我们试图解释的预测是，例如，f(5，3，10) = 7。例如，获取每个特性的Shapley值的步骤之一是计算f (5，3)。用期望值代替它等于问我们自己这个问题:给定xA = 5和xB = 3，我们期望预测是什么？我们可以从训练集中取出xC的所有可能值，并使用它们来计算期望值。换句话说，为来自训练集的每个值xC计算预测f(5，3，xC ),然后取平均整体预测。如果遗漏了一个以上的特性，我们必须付出更多的努力，但是原则是一样的。计算f(10)时，我们会问:当xC = 10时，我们期望模型预测值是多少？我们再次从训练集中获取xA和xB的所有可能值，但是现在在创建所有预测和计算平均值之前创建所有可能的(xA，xB)对。如果训练集由100行数据组成，我们将有100 × 100 = 10.000对值来进行预测。最后，f()以类似方式计算。如果我们不知道哪些值进入模型，我们期望模型预测什么？和以前一样，我们从训练集中取出所有可能的值xA，xB，xC，创建所有可能的三元组，生成100 × 100 × 100 = 1.000.000个预测，并计算平均值。现在，我们已经对每个联盟的模型预测进行了近似，我们可以将这些数字代入等式1，并获得每个特征的Shapley值。</p><h2 id="e013" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated">SHAP解释的例子</h2><p id="4ed8" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">最后一个例子表明，即使对于一个小的训练集和少量的特征，也需要计算…百万次预测。幸运的是，有一些方法可以有效地逼近期望值，这就是SHAP的作用。如前所述，SHAP是一个框架，提供了有效的计算工具来计算沙普利值。<a class="ae kz" href="https://arxiv.org/abs/1705.07874" rel="noopener ugc nofollow" target="_blank"> SHAP论文</a>的作者提出了KernelSHAP，一种受局部代理模型启发的基于核的Shapley值估计方法。他们后来还提出了TreeSHAP，这是一种基于树的模型的有效估计方法。</p><p id="0758" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">回到我们的真实数据集，我们尝试使用历史/滞后能耗、历史/滞后日照时间和邻近地区的地理位置来解释Xgboost模型对不同邻近地区第二天能耗的预测。下图显示了TreeSHAP中XGBOOST模型的功能重要性，该模型按功能重要性递减排序。水平条的颜色显示了与特定实例(在本例中为实例1)的预期预测值相比，某个特征对预测的影响是积极的(绿色)还是消极的(红色)。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">图一。数据集中实例1的SHAP解释。将鼠标悬停在地图上以查看互动数字。水平条-重要分数显示每个特征值的Shapley值。例如，对于此实例，要素sunshine _ lag 7(7天前的日照持续时间)的值为11.3。该特征值对模型预测的边际贡献为-72.84。图片作者。</figcaption></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="bb7b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> <em class="oo">加入</em> </strong> <a class="ae kz" href="https://huonglanchu.medium.com/membership" rel="noopener"> <strong class="lc iu"> <em class="oo">中等会员</em> </strong> </a> <strong class="lc iu"> <em class="oo">计划继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em> </strong> <em class="oo">如果你决定这样做，多谢！</em></p><div class="me mf gp gr mg mh"><a href="https://huonglanchu.medium.com/membership" rel="noopener follow" target="_blank"><div class="mi ab fo"><div class="mj ab mk cl cj ml"><h2 class="bd iu gy z fp mm fr fs mn fu fw is bi translated">通过我的推荐链接加入媒体——蓝初</h2><div class="mo l"><h3 class="bd b gy z fp mm fr fs mn fu fw dk translated">阅读兰楚的每一个故事(以及媒体上成千上万的其他作家)。你的会员费直接支持兰初…</h3></div><div class="mp l"><p class="bd b dl z fp mm fr fs mn fu fw dk translated">huonglanchu.medium.com</p></div></div><div class="mq l"><div class="op l ms mt mu mq mv kt mh"/></div></div></a></div></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="9be5" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated"><strong class="ak"> 2。石灰</strong></h1><p id="d2cd" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">除了SHAP，莱姆是解释黑盒模型的一个流行选择。在之前的<a class="ae kz" href="https://medium.com/towards-data-science/opening-black-box-models-with-lime-beauty-and-the-beast-9daaf02f584a" rel="noopener">帖子</a>中，我彻底解释了石灰是什么，它是如何工作的，以及它潜在的陷阱。</p><h2 id="2db8" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated">局部线性假设</h2><p id="cd5f" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">本质上，LIME试图理解影响感兴趣的单个实例周围的黑盒模型的预测的特征。在实践中，黑盒模型的决策边界可能看起来非常复杂，例如图2中的蓝粉色背景。然而，研究(Baehrens、David等人(2010年)、Laugel等人(2018年)等。)已经表明，当你放大并观察一个足够小的邻域时，无论模型在全局水平上有多复杂，局部邻域上的决策边界可以简单得多，事实上甚至可以是线性的。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d766eea66e606476aa6d1aa582e4b9c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*mGP-XD7jXK2lnFq8pPPThw.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">图二。蓝粉色背景代表黑盒模型的决策边界。加粗的红叉是正在解释的例子。LIME将通过对所选实例周围的邻域进行采样，并对该邻域应用黑盒模型来生成相应的预测，从而生成新的实例(未加框的红色实例)。最后，用一个线性模型来解释黑箱模型的预测。虚线是黑盒模型在特定位置的行为。<a class="ae kz" href="https://arxiv.org/pdf/1602.04938.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="36ed" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">也就是说，LIME所做的是在一个非常局部的水平上前进，并到达一个点，在这个点上，它变得如此局部，以至于一个<strong class="lc iu">线性模型足够强大，可以解释黑箱模型在该位置的行为</strong>【4】。你选择一个实例X0来解释，LIME会在这个实例X0周围生成新的伪实例，通过对这个选定实例周围的邻域进行采样。接下来，它将原始黑盒模型应用于置换实例以生成相应的预测，并根据这些生成的实例到被解释实例的距离对它们进行加权。权重由核函数确定，该函数将欧几里德距离和核宽度作为输入，并输出每个生成实例的重要性分数(权重)。</p><p id="3336" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这个获得的数据集中(包括生成的实例、相应的预测和权重)，LIME训练一个可解释的模型(例如，线性模型)，该模型捕捉该邻域中复杂模型的行为。该局部线性模型的系数将告诉我们哪些特征以这种或那种方式驱动预测，最重要的是，在该位置。</p><h2 id="2c7b" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated">石灰说明示例</h2><p id="d4a5" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">与SHAP类似，LIME的输出是一个解释列表，反映了每个特征值对模型预测的贡献。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">图3。莱姆的解释。power _ lag 7(7天前的能耗)的重要分值最大。此实例的特征power_lag7的值是94.284。负的重要分数意味着power_lag7增加一个单位将导致响应变量——能耗减少108.2797。作者图片</figcaption></figure><h2 id="69c9" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated">3.排列特征重要性</h2><h2 id="9dec" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated">模型性能下降</h2><p id="dffb" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">PFI背后的想法很简单。它测量了在我们改变了特征值之后模型性能的下降(例如RMSE)，因此解释了哪些特征错误地驱动了模型的性能[5]。简而言之，如果打乱某个要素的值会增加模型误差，则该要素很重要，因为在这种情况下，模型依赖于该要素进行预测。如果改变某个特征的值使模型误差保持不变，那么该特征就不那么重要，因为在这种情况下，模型会忽略该特征进行预测[2]。作为一个例子，让我们假设一个验证集上RMSE等于42的模型。如果你洗牌，验证RMSE上升到47，这意味着这个功能的重要性是5。</p><h2 id="5db7" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated">PFI解释示例</h2><p id="96f0" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">回到我们的真实例子，PFI的解释说地理位置特征(位置A、位置B、位置C)起着最重要的作用，对模型误差的增加具有最大的影响。这与SHAP和莱姆的解释大相径庭，但也许可以解释。因为我们在看时间序列数据，滞后特征是高度相关的，例如，一周前的<code class="fe or os ot ou b">sunshine duration</code>(<code class="fe or os ot ou b">sunshine_lag7</code>)与六天前的<code class="fe or os ot ou b">sunshine duration</code>(<code class="fe or os ot ou b">sunshine_lag6</code>)相关。假设我们现在洗牌的特点<code class="fe or os ot ou b">sunshine_lag7</code>。当两个特征相关时，其中一个特征被置换，模型仍然可以通过其相关特征访问该特征内部的信息，并且模型现在可以依赖<code class="fe or os ot ou b">sunshine_lag6</code>测量。这导致两个特征的重要性值较低，而它们实际上可能很重要。这正是本案中发生的情况。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk translated">图4。实例1的PFI说明。所有相关特征(滞后特征)的重要性都很低。作者图片</figcaption></figure><h1 id="94bb" class="mw mx it bd my mz nt nb nc nd nu nf ng jz nv ka ni kc nw kd nk kf nx kg nm nn bi translated">C.潜在的陷阱</h1><p id="c4b5" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">PFI方法解释了哪些特征驱动了模型的性能(例如RMSE)，而LIME和SHAP等方法解释了哪些特征在生成预测时发挥了更重要的作用。</p><p id="7355" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在机器学习解释能力方面，SHAP可能是最先进的。它有明确的解释和坚实的博弈论基础。Shapley值可能是提供完整解释的唯一方法，因为它基于可靠的理论，并通过计算添加特征值时模型预测与缺少特征值时预期预测之间的差异来公平分配影响。SHAP的缺点是Shapley值需要<strong class="lc iu">大量的计算时间，因为训练时间随着特征的数量呈指数增长</strong>。保持计算时间可管理的一个解决方案是仅计算可能联盟的几个样本的贡献[2]。另一个限制是，对于单个实例，Shapley值返回每个特征值的单个Shapley值，它不像LIME那样给出<strong class="lc iu">预测模型</strong>。这意味着它不能用于对输入变化的预测中的变化进行陈述。</p><p id="aadb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">莱姆的解释相当简单。然而，它需要定义一个邻域，并且有时显示出<strong class="lc iu">不稳定并且容易操作。LIME解释的不稳定性来自于这样一个事实，即它取决于生成的实例数量和选择的核宽度，这决定了邻域的大小。这个邻域定义了解释的位置级别。有目的的邻域需要足够小以实现局部线性，但又足够大以避免欠采样或偏向全局解释的威胁。在某些情况下，可以通过改变内核宽度来改变解释的方向。像LIME这样的方法是基于这样一个假设，即事物在局部水平上将有一个线性关系，但是没有理论解释为什么这个会起作用。有时，当你的黑盒模型非常复杂，并且模型根本不是局部线性的，使用线性模型的局部解释将不够好。LIME可能能够给出一个很好的局部解释——只要达到正确的邻域和局部线性。</strong></p><p id="08d7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">置换特征重要性与模型的误差相关联，这并不总是您想要的。<strong class="lc iu"> </strong> PFI也不太适合用相关特征训练的模型，因为添加相关特征可以通过在两个特征之间分割重要性来降低相关特征<strong class="lc iu"> </strong>的重要性。如上所述，这通常会导致误导性的解释，因此<strong class="lc iu">不适合解释时间序列模型或存在强相关特征的情况。</strong></p><h2 id="d5e7" class="ny mx it bd my nz oa dn nc ob oc dp ng lj od oe ni ln of og nk lr oh oi nm oj bi translated">参考</h2><p id="b717" class="pw-post-body-paragraph la lb it lc b ld no ju lf lg np jx li lj nq ll lm ln nr lp lq lr ns lt lu lv im bi translated">[1] [SHAP主论文(2017)]解释模型预测的统一方法:<a class="ae kz" href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pd" rel="noopener ugc nofollow" target="_blank">http://papers . nips . cc/paper/7062-A-Unified-Approach-to-interpretation-Model-Predictions . PD</a>f</p><p id="348b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[2][书]可解释的机器学习:<a class="ae kz" href="https://christophm.github.io/interpretable-ml-book/shap.html" rel="noopener ugc nofollow" target="_blank">https://christophm . github . io/Interpretable-ml-Book/shap . html</a></p><p id="2d4d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">【3】https://github.com/slundberg/shapSHAP包:<a class="ae kz" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"/></p><p id="fd7b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[4]里贝罗，M. T .，辛格，s .，&amp; Guestrin，C. (2016年8月)。“我为什么要相信你？”解释任何分类器的预测。第22届ACM SIGKDD知识发现和数据挖掘国际会议论文集<em class="oo">(第1135-1144页)。</em></p><p id="3d55" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[5]奥特曼、托洛西、桑德和朗高尔(2010年)。排列重要性:一个修正的特征重要性度量。<em class="oo">生物信息学</em>，<em class="oo"> 26 </em> (10)，1340–1347。</p><p id="d932" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[6]<a class="ae kz" href="https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30" rel="noopener" target="_blank">https://towards data science . com/shap-explained-the-way-I-wish-someone-explained-it-to-me-ab 81 cc 69 ef 30</a></p></div></div>    
</body>
</html>