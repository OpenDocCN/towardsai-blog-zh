<html>
<head>
<title>Introduction to PySpark via AWS EMR and Hands-on EDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过AWS EMR和动手EDA介绍PySpark</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/introduction-to-pyspark-via-aws-emr-and-hands-on-eda-de1866d641f5?source=collection_archive---------4-----------------------#2021-02-28">https://pub.towardsai.net/introduction-to-pyspark-via-aws-emr-and-hands-on-eda-de1866d641f5?source=collection_archive---------4-----------------------#2021-02-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="208b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-analysis" rel="noopener ugc nofollow" target="_blank">数据分析</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><div class=""><h2 id="b62b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在纽约出租车费数据集上执行EDA以查看PySpark的运行情况——因为云计算是下一件大事！</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/43b43e759610f077650495c97c552041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d3jLbRiQJQwhjRQU"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@dianamia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> C达斯汀</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="cb5f" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍技术</h1><h2 id="a965" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">什么是Spark和PySpark — Spark SQL和Spark MLlib？</h2><p id="74ee" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">来自维基百科，Spark by Apache是一个用于大规模数据处理的开源分析引擎。它使程序员能够利用固有的数据并行性和容错能力处理存储在多个集群中的数据。</p><p id="b52f" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">Spark引擎的基础是弹性分布式数据库(rdd ),它是以容错方式在机器集群上维护的一组数据项。开发这些rdd是为了克服Map-Reduce范式的限制，这种范式通过从磁盘读取数据、<em class="nm">映射</em>和<em class="nm">还原</em>并写回磁盘来强制程序中的线性数据流。另一方面，rdd作为一个跨集群的工作集，在程序执行期间提供受限形式的共享内存。</p><p id="27a7" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated"><strong class="mq jd">星火核心</strong>是整个项目的基础。它提供分布式任务分派、调度和基本的I/O功能，通过以上面讨论的RDD抽象为中心的API公开。<strong class="mq jd"> Spark SQL </strong>是Spark Core之上的一个组件，它引入了一个名为DataFrames的数据抽象，为结构化和半结构化数据提供支持。它提供了一种特定于领域的语言(类似于pandas)来操作数据帧，并通过CLIs和ODBC服务器提供了基本的SQL支持。同样，<strong class="mq jd"> Spark MLlib </strong>是一个分布式机器学习框架，位于Spark Core之上。</p><p id="1001" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">基本上，<strong class="mq jd"> PySpark </strong>是Scala编写的Spark框架的Python API。该API包含库和包装器，允许程序员利用底层SparkSQL和Spark MLlib模块的功能和特性。</p><h2 id="fdd9" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">什么是AWS EMR？</h2><p id="1e5f" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">Amazon Elastic Map Reduce是业界领先的云大数据平台，使用Apache Spark、Apache Hive、Apache HBase、Apache Flink和Apache胡迪等开源工具处理海量数据。Amazon EMR通过自动执行配置容量和调整集群等耗时的任务，简化了大数据环境的设置、操作和扩展。</p><blockquote class="nn no np"><p id="2515" class="mo mp nm mq b mr nh kd mt mu ni kg mw nq nj my mz nr nk nb nc ns nl ne nf ng im bi translated">基本上就是在云上使用Spark的分布式数据处理。</p></blockquote><h1 id="2dce" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">设置— AWS EMR与本地PySpark实例</h1><p id="cc44" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">程序员在使用Spark框架时有两种选择。他可以决定将它安装在自己的本地系统上，或者使用某个云提供商提供的服务。</p><p id="207d" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">如果他选择第一个选项，那么本地系统中的处理器内核将充当集群，并在内核级别实现并行性。这种方法的问题是，RAM的某个部分已经被保留作为工作存储器<em class="nm">用于在数据的分布式处理完成后组合结果。这与今天个人笔记本电脑的RAM规格(8GB或16GB)一起，使得这个选项非常没有吸引力；可以理解，数据处理非常缓慢。另一方面，在按使用付费的模式下使用云中的集群提供了无限的系统配置选项，显然是更好的选择。</em></p><p id="d61e" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">下面简要介绍如何在<a class="ae lh" href="https://console.aws.amazon.com/elasticmapreduce/home?region=us-east-1" rel="noopener ugc nofollow" target="_blank"> AWS EMR </a>上设置集群和笔记本:</p><ol class=""><li id="1c82" class="nt nu it mq b mr nh mu ni mf nv mi nw ml nx ng ny nz oa ob bi translated">点击上面的链接。然后点击边栏上的“集群”。</li><li id="5f97" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ny nz oa ob bi translated">点击蓝色的“创建集群”,进入“高级选项”</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/00d48624a8aced02ed6233dbb12a94f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ruEOeK3_Bk30fiyNNs_fyw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">步骤3:检查要安装在集群上的Spark框架。软件设置需要一个存储在S3桶中的配置文件或JSON文件，该文件指定工作内存和核心内存的数量。来源:图片由作者提供。</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/21ca2417d78deed93551672d0cdd43d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THS2PgxH_EO9oqxIq1myng.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">步骤4:在硬件中，根据任务需要选择集群中实例的类型和数量。其他的都可以保持原样。来源:图片由作者提供。</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/f290dfd158dbc6c4d339e1264a83545d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZUSuYfnelfWPFZGloApAmA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">步骤5:我们可以保留“常规群集设置”中的默认设置。在“安全性”中，必须在最终创建集群之前指定EC2密钥对。来源:图片由作者提供。</figcaption></figure><p id="e097" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">创建集群后，转到边栏上的“笔记本”并创建一个与集群相关联的笔记本。你可以走了！</p><h1 id="e1ab" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">数据集</h1><p id="f217" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">现在，我们已经简要了解了我们将使用的不同技术，并决定了配置选项，让我们看看我们将处理的数据集。</p><p id="c12b" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">该数据集可以在<a class="ae lh" href="https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上下载，包含5500万条纽约出租车出行记录——非常适合大数据和Spark。它具有以下特点:</p><ul class=""><li id="2c56" class="nt nu it mq b mr nh mu ni mf nv mi nw ml nx ng ok nz oa ob bi translated"><em class="nm">键</em>:这是旅行开始的时间戳</li><li id="a28e" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ok nz oa ob bi translated"><em class="nm">车费金额</em>:该行程的车费(美元)</li><li id="1bfa" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ok nz oa ob bi translated"><em class="nm"> pickup_datetime: </em>与key相同，这是旅行开始的时间戳</li><li id="0376" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ok nz oa ob bi translated"><em class="nm">皮卡_经度:</em>皮卡的经度坐标</li><li id="81a6" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ok nz oa ob bi translated"><em class="nm">皮卡_纬度</em>:皮卡的纬度坐标</li><li id="d5ae" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ok nz oa ob bi translated"><em class="nm">drop off _ longitude:</em>drop的经度坐标</li><li id="e5d0" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ok nz oa ob bi translated"><em class="nm"> dropoff_latitude: </em>空投的纬度坐标</li><li id="19a7" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ok nz oa ob bi translated"><em class="nm">乘客计数:</em>旅途中出租车上的乘客数量</li></ul><p id="5462" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">本问题陈述的目的是应用适当的EDA和特征工程来尽可能接近地预测从特定上车点到下车点的出租车费用(与测试集地面真实情况进行比较)。</p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><h1 id="1013" class="li lj it bd lk ll os ln lo lp ot lr ls ki ou kj lu kl ov km lw ko ow kp ly lz bi translated">最后，EDA</h1><p id="a78a" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">现在，让我们深入了解针对该数据集完成的EDA。正如之前顺便提到的，使用云提供商服务进行分布式数据处理是有益的，并且通过查看纽约出租车数据集的大小证明了这一观点——无论如何都不可能在PC上进行处理。</p><h2 id="ecb0" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">初步分析</h2><p id="ba16" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">我们导入库并读入下面给出的数据。接下来，我们对数据集、模式和其中的列的大小进行非常基本的分析。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="738a" class="ma lj it oy b gy pc pd l pe pf">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import pandas as pd</span><span id="43f3" class="ma lj it oy b gy pg pd l pe pf"># all data manipulation functions<br/>import pyspark.sql.functions as F<br/>from pyspark.sql import Row</span><span id="22d8" class="ma lj it oy b gy pg pd l pe pf">#PySpark does not have visualization capabilities<br/>from pyspark_dist_explore import Histogram, hist, distplot, pandas_histogram</span><span id="b4b2" class="ma lj it oy b gy pg pd l pe pf">train_data_path = 's3://ny-taxi-emr/train.csv'<br/>test_data_path = 's3://ny-taxi-emr/test.csv'</span><span id="3017" class="ma lj it oy b gy pg pd l pe pf">train_data = spark.read.csv(train_data_path, header = True, inferSchema = True, mode="DROPMALFORMED")<br/>test_data = spark.read.csv(test_data_path, header = True, inferSchema = True, mode="DROPMALFORMED")</span></pre><p id="8875" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">接下来，我们将带有日期时间值的列的数据类型转换为<em class="nm">日期时间</em>数据类型。这里，<strong class="mq jd"> withColumn() </strong>用于创建和替换现有列，而<strong class="mq jd"> col() </strong>用于访问列值。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="5a93" class="ma lj it oy b gy pc pd l pe pf">train_data = train_data.withColumn("pickup_datetime", F.to_timestamp(F.col("pickup_datetime"), "yyyy-MM-dd HH:mm:ss"))</span><span id="896e" class="ma lj it oy b gy pg pd l pe pf">def create_date_columns():<br/>    """<br/>        Create new rows from pickup_datetime, <br/>        namely, date, day, hour, day_of_week, month and year<br/>    """<br/>    # Get datetime.date objects and create a new column pickup_date<br/>    new_train_data = train_data.withColumn("pickup_date", F.to_date(F.col("pickup_datetime")))<br/>    new_train_data = new_train_data.withColumn("pickup_day", F.dayofmonth(F.col("pickup_datetime")))<br/>    new_train_data = new_train_data.withColumn("pickup_hour", F.hour(F.col("pickup_datetime")))<br/>    new_train_data = new_train_data.withColumn("pickup_day_of_week", F.dayofweek(F.col("pickup_datetime")))<br/>    new_train_data = new_train_data.withColumn("pickup_month", F.month(F.col("pickup_datetime")))<br/>    new_train_data = new_train_data.withColumn("pickup_year", F.year(F.col("pickup_datetime")))<br/>    <br/>    return new_train_data</span></pre><p id="814f" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">接下来，我们检查空值并删除包含空值的行。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="ae58" class="ma lj it oy b gy pc pd l pe pf">new_train_data.select([F.count(F.when(F.isnull(col), col)).alias(col) for col in new_train_data.columns]).show()</span></pre><h2 id="78d3" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">可视化维度分布</h2><p id="aa18" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">我们首先从可视化适当维度的分布开始。我们从<em class="nm">票价</em>维度开始，删除过高或负值的票价。<strong class="mq jd"> collect() </strong>用于从所有从节点检索到主节点或驱动节点的最终结果。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="47f3" class="ma lj it oy b gy pc pd l pe pf">print("Maximum Fare:{}\n Minimum Fare: {}".format(new_train_data.agg({"fare_amount": "max"}).collect()[0],\<br/>      new_train_data.agg({"fare_amount": "min"}).collect()[0]))</span><span id="8704" class="ma lj it oy b gy pg pd l pe pf">train_data_fare_filtered = new_train_data.filter(F.col("fare_amount").between(2.5, 100))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/fc14d1934aa4479e95de4958f3e9238e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0pQve395IwlUqA-OjJKTDw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">票价金额的分配(装箱)。来源:图片由作者提供。</figcaption></figure><p id="1d4d" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">同样，我们分析乘客数分布，发现最大值是208，没有意义。我们将7座车作为最大乘客数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/774c8a5874f753c80de27e387f2ac3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hV2_W_3vsblpYeZOR0TBLA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">乘客数量分布。来源:图片由作者提供。</figcaption></figure><p id="c6b9" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">同样，我们对坐标字段进行相同的边界分析，并删除坐标距离纽约中心超过2度的行程。(40.771133, -73.974187).</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="7262" class="ma lj it oy b gy pc pd l pe pf">train_data_filtered = train_data_passenger_filtered.filter((F.col("pickup_latitude").between(38.5, 42.5)))<br/>train_data_filtered = train_data_filtered.filter((F.col("dropoff_latitude").between(38.5, 42.5)))<br/>train_data_filtered = train_data_filtered.filter((F.col("pickup_longitude").between(-76, -72)))<br/>train_data_filtered = train_data_filtered.filter((F.col("dropoff_longitude").between(-76, -72)))</span></pre><h2 id="0c2c" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">机场票价分析</h2><p id="b60a" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">在地图上绘制行程坐标，我们看到纽约机场有许多接送服务。在获得纽约机场的坐标后，我们继续对此进行分析。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="6a3b" class="ma lj it oy b gy pc pd l pe pf">1. Coordinates of Newark Airport = 40.6895° N, 74.1745° W<br/><br/>2. Coordinates of JFK Airport = 40.6413° N, 73.7781° W<br/><br/>3. Coordinates of La Guardia Airport = 40.7769° N, 73.8740° W</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/7a36d6af74cfef1398f53274a7d73a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dey_H63AzAuigDGmskmLXg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">JFK的接送费用分配。来源:图片由作者提供</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/862c9fb2f13fe5c73efa784e51f869df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yzr2mhJbzpm2lVy0eEubGQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">JFK的落客票价分布。来源:图片由作者提供。</figcaption></figure><p id="9c10" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">我们可以看到，JFK的接送费用更多地在40美元到60美元之间，这表明价格是固定的。为了证实这一点，让我们将它与所有旅行的平均费用进行比较。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/4b0326dacbddf4411665f168e3ca1255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YjyD8vPsRhGTVLLu3AE7tg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">票价金额的分配。来源:图片由作者提供。</figcaption></figure><p id="c176" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">我们发现，当旅行与机场有关时，平均票价要高得多。因此，通过特征工程区分这些记录将有助于我们建立一个更好的模型，因为它将能够区分和识别机场旅行和非机场旅行之间的模式差异。因此，我们创建了指定记录(行程)是否到达三个机场中的任何一个的特征。</p><h2 id="5fe5" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">特征工程</h2><p id="adcd" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">对于功能工程和制作与机场相关的额外维度，我们在PySpark中使用了一个叫做<a class="ae lh" href="https://docs.databricks.com/spark/latest/spark-sql/udf-python.html" rel="noopener ugc nofollow" target="_blank">的用户自定义函数</a> (UDF)。这允许我们在Spark框架中使用用户定义的Python函数。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="e9f4" class="ma lj it oy b gy pc pd l pe pf">from pyspark.sql.functions import udf<br/>from pyspark.sql.types import BooleanType</span><span id="af1c" class="ma lj it oy b gy pg pd l pe pf"># user-defined Python function<br/>def isTripRelatedToAirport(latitude,longitude,airport_name='JFK'):   <br/>    if ((latitude &gt;= nyc_airports[airport_name]['min_lat']) &amp; <br/>        (latitude &lt;= nyc_airports[airport_name]['max_lat']) &amp; <br/>        (longitude &gt;= nyc_airports[airport_name]['min_lng']) &amp; <br/>       (longitude &lt;= nyc_airports[airport_name]['max_lng'])):<br/>        return True<br/>    else:<br/>        return False</span><span id="7fab" class="ma lj it oy b gy pg pd l pe pf"># name the UDF using lambdas<br/>featureEngineeringUDF = udf(lambda x, y, z: isTripRelatedToAirport(x,y,z),BooleanType())</span><span id="57a3" class="ma lj it oy b gy pg pd l pe pf"># register the Python UDF with Spark<br/>spark.udf.register("featureEngineeringUDF", featureEngineeringUDF)</span></pre><p id="cda4" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">使用这个UDF，我们构建与机场相关的维度。<strong class="mq jd"> lit </strong>用于将硬编码的字符串传递到UDF。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="3e9c" class="ma lj it oy b gy pc pd l pe pf">train_data_filtered = train_data_filtered.withColumn("is_pickup_JFK", featureEngineeringUDF(F.col("pickup_latitude"),F.col("pickup_longitude"), lit("JFK")))</span><span id="b7ea" class="ma lj it oy b gy pg pd l pe pf">train_data_filtered = train_data_filtered.withColumn("is_dropoff_JFK", featureEngineeringUDF(F.col("dropoff_latitude"),F.col("dropoff_longitude"), lit("JFK")))</span><span id="892a" class="ma lj it oy b gy pg pd l pe pf">train_data_filtered = train_data_filtered.withColumn("is_pickup_EWR", featureEngineeringUDF(F.col("pickup_latitude"),F.col("pickup_longitude"), lit("EWR")))</span><span id="4865" class="ma lj it oy b gy pg pd l pe pf">train_data_filtered = train_data_filtered.withColumn("is_dropoff_EWR", featureEngineeringUDF(F.col("dropoff_latitude"),F.col("dropoff_longitude"), lit("EWR")))</span><span id="292d" class="ma lj it oy b gy pg pd l pe pf">train_data_filtered = train_data_filtered.withColumn("is_pickup_LaGuardia", featureEngineeringUDF(F.col("pickup_latitude"),F.col("pickup_longitude"), lit("LaGuardia")))</span><span id="afa8" class="ma lj it oy b gy pg pd l pe pf">train_data_filtered =  train_data_filtered.withColumn("is_dropoff_LaGuardia", featureEngineeringUDF(F.col("dropoff_latitude"),F.col("dropoff_longitude"), lit("LaGuardia")))</span></pre><p id="69fc" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">接下来，我们使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Haversine_formula" rel="noopener ugc nofollow" target="_blank"> <strong class="mq jd">哈弗辛公式</strong> </a> <strong class="mq jd">计算距离坐标的公里数。</strong></p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="8d45" class="ma lj it oy b gy pc pd l pe pf">from pyspark.sql.types import DoubleType, ArrayType, DecimalType<br/>import math<br/><br/>def trip_distance(lat1, lat2, lon1, lon2):<br/>    """<br/>    Calculate trip distance based on Haversine formula<br/>    Args:<br/>        lat1: Latitude of first point<br/>        lat2: Latitude of second point<br/>        lon1: Longitude of first point<br/>        lon2: :ongitude of second point<br/>    Returns:<br/>        Distance between the two points in miles<br/>    """<br/>    # pi / 180<br/>    p = 0.017453292519943295<br/>    a = 0.5 - math.cos((lat2 - lat1) * p)/2 + math.cos(lat1 * p) * math.cos(lat2 * p) * (1 - math.cos((lon2 - lon1) * p)) / 2<br/>    return 0.6213712 * 12742 * math.asin(math.sqrt(a))<br/><br/>haversineUDF = udf(lambda a, b, c, d: trip_distance(a,b,c,d), DoubleType())<br/>spark.udf.register("haversineUDF", haversineUDF)</span><span id="7fb1" class="ma lj it oy b gy pg pd l pe pf">train_data_filtered = train_data_filtered.withColumn("trip_distance", haversineUDF(F.col("pickup_latitude"), F.col("dropoff_latitude"), F.col("pickup_longitude"), F.col("dropoff_longitude")))</span></pre><h2 id="ca12" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">逐区分析</h2><p id="9618" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">我们还通过获取纽约所有行政区的坐标(皇后区、布鲁克林区、布朗克斯区、曼哈顿区、斯塔滕岛区)进行行政区旅行费用分析，类似于机场分析。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="2d26" class="ma lj it oy b gy pc pd l pe pf">def getBorough(lat,lng):<br/>    """<br/>    Get the borough based on latitude and longitude<br/>    Args:<br/>        lat: Latitude of the place<br/>        lng: Longitude of the place<br/>    Returns:<br/>        A string representing the name of the borough<br/>    """<br/>    locations = nyc_boroughs.keys()<br/>    for location in locations:<br/>        if (lat &gt;= nyc_boroughs[location]['min_lat'] and <br/>            lat &lt;= nyc_boroughs[location]['max_lat'] and <br/>            lng &gt;= nyc_boroughs[location]['min_lng'] and <br/>            lng &lt;= nyc_boroughs[location]['max_lng']):<br/>            return location<br/>    return 'others'<br/><br/>boroughUDF = udf(lambda a, b: getBorough(a,b))<br/>spark.udf.register("boroughUDF", boroughUDF)</span><span id="a4ec" class="ma lj it oy b gy pg pd l pe pf"><br/>train_data_filtered = train_data_filtered.withColumn("pickup_borough", boroughUDF(F.col("pickup_latitude"), F.col("pickup_longitude")))</span><span id="adaa" class="ma lj it oy b gy pg pd l pe pf">train_data_filtered = train_data_filtered.withColumn("dropoff_borough", boroughUDF(F.col("dropoff_latitude"), F.col("dropoff_longitude")))</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/319b41e21f7f156722fba95cf0836e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUf2YZnb7nMuRNCVSW9FFQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">曼哈顿的票价分布。深红色代表接送车费，浅红色代表接送车费。来源:图片由作者提供。</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/93ec1dd5052da7783deeac328e0ded54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*linApxSg-QUDo2wPlautVQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">布朗克斯的票价分布。深红色代表接送车费，浅红色代表接送车费。来源:图片由作者提供。</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/2c4c9f449688185a7e9df992b62abc5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ny-91mbxh0DLNchoP1D4Gw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">皇后区的票价分布。深红色代表接送车费，浅红色代表接送车费。来源:图片由作者提供。</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pp"><img src="../Images/08ee511e90575d9d1dc72ffb24170163.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkUA-IEmBMgEo5r58_NY7w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">布鲁克林的票价分布。深红色代表接送车费，浅红色代表接送车费。来源:图片由作者提供。</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pq"><img src="../Images/73d2bbbf4fa287b771012387954dc259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xcIbC4-0VGSc90fsoFOfyw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">斯塔滕岛的票价分布。深红色代表接送车费，浅红色代表接送车费。来源:图片由作者提供。</figcaption></figure><p id="a9e0" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">我们从上面的逐区图表中得出以下结论:</p><ol class=""><li id="874d" class="nt nu it mq b mr nh mu ni mf nv mi nw ml nx ng ny nz oa ob bi translated">对于曼哈顿，接送费用金额分布几乎相似。</li><li id="759a" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ny nz oa ob bi translated">布朗克斯、布鲁克林和斯塔滕岛的折扣价格更高，这意味着人们从很远的地方来到布朗克斯、布鲁克林和斯塔滕岛</li><li id="7829" class="nt nu it mq b mr oc mu od mf oe mi of ml og ng ny nz oa ob bi translated">对于皇后区，这两个平均值都更高，这意味着人们通常会进行更长时间的旅行</li></ol><h2 id="533a" class="ma lj it bd lk mb mc dn lo md me dp ls mf mg mh lu mi mj mk lw ml mm mn ly iz bi translated">基于时间的分析</h2><p id="3564" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">最后，在使用Spark MLlib进行预测之前，我们进行一些基于时间的分析。为了更好地帮助可视化，使用<strong class="mq jd"> toPandas() </strong>将PySpark数据帧的最终结果转换成Pandas数据帧</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="eaf6" class="ma lj it oy b gy pc pd l pe pf">trips_by_year = train_data_filtered.groupBy("pickup_year").count().toPandas()<br/>trips_by_year = trips_by_year.sort_values(by = "pickup_year")<br/>trips_by_year</span><span id="1ac5" class="ma lj it oy b gy pg pd l pe pf">trips_by_month = train_data_filtered.groupBy("pickup_month").count().toPandas()<br/>trips_by_month = trips_by_month.sort_values(by = "pickup_month")<br/>trips_by_month</span><span id="68b5" class="ma lj it oy b gy pg pd l pe pf">#similar for other granularities of time</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pr"><img src="../Images/1276918752e89e1b18920ea64b86d866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LpKX_bHME2d0FWmVwtXHiw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">每年的旅行次数。来源:图片由作者提供</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ps"><img src="../Images/3ed5c8df6952b800a2405dcbc931b4b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mU75j2s8gIu3Mb6U1Eh0jQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">按月旅行。来源:图片由作者提供</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/a8aec18925c57361e4c6a65b48de267a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_BqLlkITvhbuThHuy99A0w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">按星期几的旅行。来源:图片由作者提供</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/81d064007fbcb6a310b2a2a6de3dc21d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MKQUNItPDmpoC6ny8Jhgxw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">每小时的行程。来源:图片由作者提供。</figcaption></figure></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><h1 id="c3a1" class="li lj it bd lk ll os ln lo lp ot lr ls ki ou kj lu kl ov km lw ko ow kp ly lz bi translated">构建ML管道</h1><p id="baf6" class="pw-post-body-paragraph mo mp it mq b mr ms kd mt mu mv kg mw mf mx my mz mi na nb nc ml nd ne nf ng im bi translated">首先，我们删除数据类型为<em class="nm"> date-time </em>的列，因为这些列需要时间序列概念。我们还对与每次旅行相关的行政区进行编码。<strong class="mq jd"> StringIndexer </strong>用于编码(从分类转换为数字)，而<strong class="mq jd"> OneHotEncoderEstimator </strong>用于一次性编码随后的行政区数字编码。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="c379" class="ma lj it oy b gy pc pd l pe pf">train_data_filtered = train_data_filtered.drop("pickup_datetime", "pickup_date", "key")</span><span id="1edf" class="ma lj it oy b gy pg pd l pe pf"># create object of StringIndexer class and specify input and output column<br/>SI_pickup = StringIndexer(inputCol='pickup_borough',outputCol='pickup_borough_encoded')<br/>SI_dropoff = StringIndexer(inputCol='dropoff_borough',outputCol='dropoff_borough_encoded')<br/><br/># transform the data<br/>train_data_filtered = SI_pickup.fit(train_data_filtered).transform(train_data_filtered)<br/>train_data_filtered = SI_dropoff.fit(train_data_filtered).transform(train_data_filtered)<br/><br/># create object and specify input and output column<br/>OHE = OneHotEncoderEstimator(inputCols=['pickup_borough_encoded', 'dropoff_borough_encoded'],outputCols=['pickup_borough_OHE', 'dropoff_borough_OHE'])<br/><br/># transform the data<br/>train_data_filtered = OHE.fit(train_data_filtered).transform(train_data_filtered)<br/><br/>train_data_filtered = train_data_filtered.drop("pickup_borough", "dropoff_borough", "pickup_borough_encoded", dropoff_borough_encoded")</span></pre><p id="ad17" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">接下来，<strong class="mq jd"> VectorAssembler </strong>用于将所有特征组合成单个特征向量，这简化了机器学习算法的数据处理和预测。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="eefd" class="ma lj it oy b gy pc pd l pe pf">featureColumns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'pickup_day', 'pickup_hour', 'pickup_day_of_week', 'pickup_month', 'pickup_year', 'is_pickup_JFK', 'is_dropoff_JFK', 'is_pickup_EWR', 'is_dropoff_EWR', 'is_pickup_LaGuardia', 'is_dropoff_LaGuardia', 'trip_distance', 'is_pickup_in_lower_manhattan', 'is_drop_in_lower_manhattan', 'pickup_borough_OHE', 'dropoff_borough_OHE']<br/><br/>assembler = VectorAssembler(inputCols= featureColumns, outputCol="features")<br/><br/>full_set = assembler.transform(train_data_filtered)</span></pre><p id="54ff" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">最后，我们应用训练-测试分裂，并使用决策树和随机森林进行模型训练。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="be92" class="ma lj it oy b gy pc pd l pe pf">train_set, test_set = full_set.randomSplit([0.9, 0.1], seed=0)</span><span id="b5fb" class="ma lj it oy b gy pg pd l pe pf">from pyspark.ml.regression import RandomForestRegressor<br/>from pyspark.ml.evaluation import RegressionEvaluator<br/><br/>rf = RandomForestRegressor(labelCol="fare_amount", featuresCol= "features")<br/>model = rf.fit(train_set)<br/><br/>predictions = model.transform(test_set)</span><span id="7aa2" class="ma lj it oy b gy pg pd l pe pf">evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="fare_amount",metricName="rmse")<br/>print("RMSE Error on test set: ", evaluator.evaluate(predictions)</span></pre><p id="94cb" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">随机森林模型的精度为4.4201。</p><pre class="ks kt ku kv gt ox oy oz pa aw pb bi"><span id="1e18" class="ma lj it oy b gy pc pd l pe pf">from pyspark.ml.regression import DecisionTreeRegressor<br/>from pyspark.ml.evaluation import RegressionEvaluator</span><span id="29dc" class="ma lj it oy b gy pg pd l pe pf">dt = DecisionTreeRegressor(labelCol="fare_amount", featuresCol= "features")<br/>model = dt.fit(train_set)<br/><br/>predictions = model.transform(test_set)</span><span id="67a6" class="ma lj it oy b gy pg pd l pe pf">evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="fare_amount",metricName="rmse")<br/>print("RMSE Error on test set: ", evaluator.evaluate(predictions))</span></pre><p id="7fb9" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">我们得到的最佳RMSE误差是针对决策树模型<strong class="mq jd"> — 4.2814。</strong>你可以在<a class="ae lh" href="https://github.com/kunjmehta/ny-taxi-prediction-pyspark-emr/blob/main/NY%20Taxi%20PySpark.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上查阅完整的代码(包括可视化)和输出。</p><p id="9e27" class="pw-post-body-paragraph mo mp it mq b mr nh kd mt mu ni kg mw mf nj my mz mi nk nb nc ml nl ne nf ng im bi translated">我很乐意在Linkedin上与你联系！</p></div></div>    
</body>
</html>