<html>
<head>
<title>Univariate Linear Regression From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始的一元线性回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/univariate-linear-regression-from-scratch-68065fe8eb09?source=collection_archive---------0-----------------------#2022-09-19">https://pub.towardsai.net/univariate-linear-regression-from-scratch-68065fe8eb09?source=collection_archive---------0-----------------------#2022-09-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a8fa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Python代码</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/a1a53c1dea88741d3624c437ad6874bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*lEy6nZXfCm9xBZ4WR8wqHw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">一元线性回归-作者图片</figcaption></figure><h1 id="10b1" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">介绍</h1><p id="c51a" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">一般一个<em class="mf">机器学习</em>课程的第一个科目是<strong class="ll ir">线性回归</strong>，不是很复杂，很容易理解。此外，它包括许多<em class="mf">机器学习</em>概念，可以在以后更复杂的概念中使用。所以从<strong class="ll ir">线性回归</strong>到新的<em class="mf">机器学习</em>基础系列开始是合理的。</p><p id="caec" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在这篇博文中，我们将了解<strong class="ll ir">单变量线性回归</strong>，这意味着<strong class="ll ir">线性回归</strong>只有<em class="mf">一个变量</em>。在本文结束时，我们将了解基本的<em class="mf">机器学习</em>概念，如<strong class="ll ir">假设</strong>、<strong class="ll ir">成本函数</strong>和<strong class="ll ir">梯度下降</strong>。此外，我们将为这些函数编写必要的代码，并使用<strong class="ll ir"><em class="mf">Python</em></strong>【1】为Kaggle数据集训练/测试线性模型。</p><p id="a90c" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在深入<strong class="ll ir">一元线性回归</strong>之前，我们先简单说一下一些基本概念:</p><h2 id="0cd2" class="ml ks iq bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh mw bi translated">基本概念</h2><p id="ff1b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated"><em class="mf">机器学习</em>是人工智能的子领域，它使用数据为计算机编写一些任务。典型的<em class="mf">机器学习</em>应用使用数据集和学习算法来建立模型，该模型预测应用的新输入的输出。</p><p id="ef91" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="mf">机器学习</em>的应用一般分为三类:</p><ul class=""><li id="43fd" class="mx my iq ll b lm mg lp mh ls mz lw na ma nb me nc nd ne nf bi translated">监督学习</li><li id="947a" class="mx my iq ll b lm ng lp nh ls ni lw nj ma nk me nc nd ne nf bi translated">无监督学习</li><li id="78a9" class="mx my iq ll b lm ng lp nh ls ni lw nj ma nk me nc nd ne nf bi translated">强化学习</li></ul><p id="1a5b" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在监督学习[2]中，数据具有可以映射到某个标签的相同特征。监督学习有两种类型。</p><p id="aecf" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">如果标签的数量有限，并且可以转换成一些类别，则称为<strong class="ll ir">分类</strong>。识别图像中的动物或手写数字是分类的例子。</p><p id="9da7" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">如果标签集不局限于一些小数量的元素，而是可以假设为无限多，那么它被称为<strong class="ll ir">回归</strong>。预测一个地区的房价或每公里汽车的耗油量都是回归的例子。</p><p id="a539" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在无监督学习[3]中，数据集没有标记数据。在教学过程之后，模型发现数据中的模式。对电子商务网站的客户进行分类是无监督学习的一个例子。</p><p id="c259" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">强化学习[4]使用根据奖惩机制行动的智能代理[5]。它主要用于电脑游戏。</p><h1 id="fbd7" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">线性回归</h1><p id="72cb" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">如上所述，回归将输入映射到无限多的输出。此外，线性回归表明输入和输出之间存在线性关系[6]；如果我们有一个以上的输入变量，这是很难想象的。如果我们有一个输入变量，我们需要在XY平面上画一条线；如果我们有两个输入变量，我们需要在三维坐标系中构造一个平面。超过两个输入变量是很难想象的。因此，我们将使用单变量数据演示线性回归。下面是俄勒冈州波特兰市数据集【7】中的<a class="ae nl" href="https://www.kaggle.com/datasets/kennethjohn/housingprice" rel="noopener ugc nofollow" target="_blank">房价，我们将以此为例。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c26e8cecfb8b5323d292ebbc9f9e45e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*IlEh02y2KIE0jbHko4w8IA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">俄勒冈州波特兰市的房价——作者图片</figcaption></figure><p id="24c8" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在上图中，y轴是波特兰一些房子的价格(K$)，x轴是房子的大小(英尺)。我们想在这个图表上画一条直线(如下图所示)，这样当我们知道一个新房子的尺寸不在这个数据集中时，我们就可以用这条线来预测它的价格。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/a1a53c1dea88741d3624c437ad6874bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*lEy6nZXfCm9xBZ4WR8wqHw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">俄勒冈州波特兰市房价与假设-图片由作者提供</figcaption></figure><p id="247f" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">例如，我们可以预测一栋3000英尺的房子的价格大约是50万美元，如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/902528d6481c0f858e7ad2e6154f5982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*FEAseL5rLip4kIB_cMme2Q.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">计算俄勒冈州波特兰市房价的新输入值-图片由作者提供</figcaption></figure><p id="b1ba" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">因此，事不宜迟，让我们深入更深刻的概念，以了解如何划清界限。</p><blockquote class="nn no np"><p id="95e7" class="lj lk mf ll b lm mg jr lo lp mh ju lr nq mi lu lv nr mj ly lz ns mk mc md me ij bi translated">注意，本文使用了来自Kaggle的<a class="ae nl" href="https://www.kaggle.com/datasets/andonians/random-linear-regression" rel="noopener ugc nofollow" target="_blank"> <strong class="ll ir">线性回归</strong> </a>数据集【1】和<a class="ae nl" href="https://www.kaggle.com/code/erkanhatipoglu/univariate-linear-regression-from-scratch" rel="noopener ugc nofollow" target="_blank"> <strong class="ll ir">一元线性回归</strong> </a>笔记本【8】。如数据集的内容部分所述，它有700个训练数据集和300个元素的测试数据集。数据由(x，y)对组成，其中x是0到100之间的数字，y是使用Excel函数NORMINV(RAND()，x，3)生成的。也给出了y的最佳估计应该是x。</p></blockquote><p id="4b29" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">首先，我们必须编写三个函数来寻找数据集上的一元线性回归解。这些函数按顺序用于假设、成本和梯度下降计算。首先，我们将逐一解释它们。</p><h2 id="5789" class="ml ks iq bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh mw bi translated">假设</h2><p id="6d28" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们的目标是找到一个可以用来预测新输入的模型。一个<a class="ae nl" href="https://machinelearningmastery.com/what-is-a-hypothesis-in-machine-learning/#:~:text=A%20statistical%20hypothesis%20is%20an,for%20mapping%20inputs%20to%20outputs." rel="noopener ugc nofollow" target="_blank">假设</a>是一个机器学习系统的潜在模型【9】。</p><p id="a6fb" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">如下图所示，我们可以通过使用学习算法找到数据集的假设[10]。稍后我们将讨论这个假设与我们的数据的吻合程度，但现在，让我们先定义一下假设公式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi nt"><img src="../Images/88406445d5165643aa114df149aefe33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DG8FEpHACfVTz_NFJanTVQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">假设—作者的形象</figcaption></figure><p id="c780" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">对于<strong class="ll ir">单变量线性回归</strong>任务，假设的形式为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/579f0ad82d43c1b8127156ecab1d5324.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*pus49RG2MNNcYpaZn8iGPw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">假设公式—作者的形象</figcaption></figure><p id="377c" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">y轴与假设线的交点称为截距。这是我们公式中的θ₀，假设线的斜率是θ₁.</p><p id="81d4" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">如果我们知道x、θ₀和θ₁值，我们就可以预测机器学习系统的目标值。例如，在上面的房价例子中，<strong class="ll ir"> <em class="mf">红线</em> </strong>的θ₀ = 0.00008，θ₁ = 0.16538。所以对于一栋3000英尺(x=3000)的房子，期望价格是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/fe6a883d6bd267d3b99b9868cb4a0c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*ZZF7WrKd_yZpRYHaiYsekg.png"/></div></figure><p id="b9a9" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">可以看出，我们需要三个参数来写假设函数。这些是x(输入)，θ₀和θ₁.因此，让我们为假设编写Python函数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">假设函数的Python代码</figcaption></figure><p id="8dd9" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这个过程非常简单。我们只需要返回<em class="mf">(</em>θ₀<em class="mf">+</em>θ₁<em class="mf">* x)</em>。</p><h2 id="de34" class="ml ks iq bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh mw bi translated">成本函数</h2><p id="207f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">当我们有一个假设时(例如，我们知道θ₀和θ₁的值)，我们必须决定这个假设有多符合我们的数据。回到房价的例子，让我们以θ₀为500，θ₁为-0.1，在我们的数据上画线。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b60745cc906a063fb65bcdef4593b818.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*WmvMQ6AgdKnJjdr1FoZ4FQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">俄勒冈州波特兰的房价不符合假设——图片由作者提供</figcaption></figure><p id="c606" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">如上图所示，θ₀ = 500，θ₁ = -0.1的假设与我们的数据集不太匹配。我们不希望这样的假设成为我们的模型，因为除了几个点之外，它给出了错误的结果。让我们多画一些线，以便更好地理解。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/3379b737c73bee370e385f301b43910a.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*z_D_UXObhKZqn8yQg2yK6A.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">俄勒冈州波特兰市不同假设线的房价-图片由作者提供</figcaption></figure><p id="1f27" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">上图中我们有三个不同θ值的假设。凭直觉，我们可以很容易地选择蓝线(θ₀ = 0，θ₁ = 0.165)作为我们数据的最佳匹配。问题是，我们如何从数学上选择更好的拟合？换句话说，我们如何选择最佳的θ₀和θ₁值，为我们找到一个可用于预测新输入的模型的目标提供最佳解决方案？</p><p id="fecd" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">为此，我们可以选择θ₀和θ₁，使h(x)接近所有训练示例的实际值[10]。我们针对特定假设(已知θ₀和θ₁值)对数据集上的每个点进行误差分析。如果我们以某种方式将数据集上每个数据点产生的所有误差相加，我们可以通过选择最小和来选择最佳拟合线(这意味着我们产生的误差更少)。</p><p id="768a" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">因此，我们定义了一个成本函数来衡量我们的总误差。有几种解决方案可供我们选择。一种称为平方误差函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/10017f20fb67afa289c6ed824fd76f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*mpnORyzfBRd_tbWqgUMf3g.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">成本函数——作者的图像</figcaption></figure><p id="1b38" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">θ₁j(θ₀)称为成本函数。它是θ₀和θ₁的函数，它们是假设的截距和斜率。h(xᵢ) — y(xᵢ)是我们为第I个训练例子所犯的错误。通过平方它，我们得到两个显著的好处；第一，我们惩罚大错误，第二，我们去掉负号if h(x)&lt; y. By dividing the total sum of errors by the number of samples, we find the average error we made. As described above, smaller cost function values suggest a better match, so we are trying to minimize the cost function for a better fit.</p><p id="d18b" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">To write the cost function, we must import <a class="ae nl" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"><strong class="ll ir"><em class="mf">NumPy</em></strong></a>，一个著名的用于线性代数任务的机器学习库。NumPy会加速我们的计算。我们还需要四个参数，它们是θ₀、θ₁、x(输入)和y(实际输出)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">成本函数的Python代码</figcaption></figure><p id="73ec" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在函数内部，首先要做的是使用NumPy的subtract函数计算每个数据点的误差。然后，我们可以使用NumPy的平方函数计算每个数据点的平方误差。最后，我们可以使用sum()计算总平方误差，并除以示例的数量，以获得平均误差。请注意，所有训练数据都作为NumPy数组传递给函数。此外，由于我们在成本函数中使用假设函数，我们必须事先编写它。还要记住，我们还没有试图最小化成本函数。我们将在后面的梯度下降主题中看到如何做。</p><h2 id="b44e" class="ml ks iq bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh mw bi translated">梯度下降</h2><p id="e8ef" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在，我们可以计算数据集上给定的一组θ₀和θ₁的成本函数。我们还知道，使成本函数最小的θ₀和θ₁值是我们数据的最佳匹配。下一个问题是如何找到使成本函数最小的θ₀和θ₁值。</p><p id="7e85" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">梯度下降算法可用于此目的，我们可以使用攀登者类比来更好地解释梯度下降。假设在山顶有一个登山者。他想尽快下到山坡上。所以，他观察了周围的环境，找到了通往地面的最陡的路径，向那个方向迈了一小步。然后，他再次观察他的周围环境，以看到新的、最垂直的移动方向。他不断重复这个过程，直到他到达山坡。</p><p id="105f" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">解释超出了这篇博文的范围，但是我们知道平方误差成本函数有一个全局最小值，没有局部最小值。此外，它在某点的负梯度是这个全局最小值的最陡方向。因此，就像山顶上的登山者找到通往山坡的路一样，我们可以使用梯度下降来找到使成本函数最小的θ值。</p><p id="beb8" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">要做到这一点，首先，我们需要选择一些初始θ₀和θ₁开始。然后，我们可以计算在这一点的梯度下降，并改变θ₀和θ₁值成比例的负梯度计算。然后重复相同的步骤，直到收敛。</p><p id="975e" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">为了更好地理解梯度的概念，我们将使用θ₀ = 0的随机平方误差成本函数，如下所示。我们选择θ₀为零；否则，我们将会在3D图形中挣扎，而不是处理2D图形。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8c47bf962a97874ac33dc192c269a5d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*g90M7KIfC9-yAkBBZRSmwg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">j(θ₁)——作者图片</figcaption></figure><p id="88e8" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在上图中，假设我们在点= 6(也就是说，我们已经选择我们的初始θ₁为6)，并试图达到全局最小值。该点的负梯度向下，如箭头所示。所以如果我们在那个方向上更进一步，我们会得到一个小于6的点。然后，如下所示，我们可以重复相同的过程，直到收敛在点= 2。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f82cd3259c5457a8aa18d3ece15a0025.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*IfYFwCL8WZiNnbx1zRM6pg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">J(θ₁的收敛)在局部最小值—作者图片</figcaption></figure><p id="458b" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">让我们试试下面的另一个观点。如果我们选择了另一个θ₁值，如点= -2，则该点的负梯度将向下，如箭头所示。因此，如果我们在这个方向上更进一步，我们将得到一个大于-2的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/aa74760fc24b9315825a7329310093ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*j-ws4wzLZTbgxh_5dPnP8g.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">j(θ₁)——作者图片</figcaption></figure><p id="b102" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">然后，如下所示，我们可以重复相同的过程，直到收敛在点= 2。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8114806b2345da155a05fad3faa5ce92.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*Rpmo0GcaIR9Sh88vgmMR9A.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">J(θ₁的收敛)在局部最小值—作者图片</figcaption></figure><p id="0564" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">一个很好的问题是如何决定我们何时收敛。也就是说，我们在代码处的停止条件是什么？我们有两个选择。我们可以检查成本函数值，如果它小于或等于预定值就停止，或者我们可以检查连续成本函数值之间的差异，如果它小于或等于阈值就停止。我们将选择后者，因为由于初始θ值的选择，我们永远无法达到我们选择的预定义成本值。</p><p id="7440" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">取函数的梯度下降是大一级别的微积分概念，我们就不深究了；相反，我们将只给出结果。单变量数据集的平方误差成本函数的梯度如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7b75a4be4c4d57ed444a992b525cb699.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*hEm-8gA6xDMxfQ_0vdk_Tw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">平方误差代价函数的梯度计算—作者图片</figcaption></figure><p id="ee31" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在写梯度下降的代码之前，我们必须引入最后一个概念:学习率。正如我们上面讨论的，梯度下降算法需要很小的步长，如下所示，如果步长太大，我们可能会有超过局部最小值的风险。因此，在计算新的θ值时，我们将梯度结果乘以学习速率系数。学习率是一个需要在训练中调整的超参数。虽然较高的学习率值可能导致超调，但较小的学习率值可能导致较高的训练时间[10]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/9913f474a24fb057096659fc34ef12f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*741YG9SNAy67KUBxnF11GA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">超越——作者图片</figcaption></figure><p id="abb0" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">因此，对于具有形式为h(x) = θ₀ <em class="mf"> + </em> θ₁ <em class="mf"> * x的假设和平方误差成本函数J( </em> θ₀，θ₁ <em class="mf">)的单变量数据集，每个步骤的</em> θ <em class="mf">值可以计算如下:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/103b0da92347c1c1df69e9928f3086de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*CAk2_EQ3bwRFJ8WBwzWmxw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">用梯度下降法计算θ值的算法——作者图片</figcaption></figure><p id="fad1" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">如前所述，α是学习率，m是训练样本的数量。</p><p id="e57f" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">我们必须像在成本函数中一样导入NumPy来编写梯度下降函数。<strong class="ll ir"> <em class="mf"> </em> </strong>我们还需要五个参数，它们是θ₀、θ₁、α(学习率)、x(输入)和y(实际输出)。我们还必须确定<strong class="ll ir"> <em class="mf">没有更新</em> </strong> θ₀才计算θ₁ [10]。否则，我们将以错误的结果告终。梯度函数的代码如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">渐变函数的Python代码</figcaption></figure><p id="a25a" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在函数内部，首先要做的是使用NumPy的subtract函数计算每个数据点的误差，并将其存储在一个变量中。然后我们可以把前面的结果乘以x，存放在另一个变量里。请注意，所有训练数据都作为NumPy数组传递给函数。因此，这些变量属于数组类型，sum()函数可用于获取总值，然后将总值乘以学习率，再除以示例数量。最后一步，我们可以计算并返回新的θ值。</p><h2 id="df7f" class="ml ks iq bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh mw bi translated">模特培训</h2><p id="c1ea" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">既然已经把需要的函数都写好了，就该训练模型了。如前所述，我们需要决定梯度下降何时收敛。为此，我们将减去连续的成本值。如果差值小于某个阈值，我们将得出梯度下降收敛的结论。</p><p id="c020" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">我们还为每次迭代画出假设，看看它是如何变化的。但是这降低了代码的速度。初始值的选择可能会显著地改变迭代的次数，因此对于复制它的人来说，改变这部分代码可能是合适的。也可以为此从零开始复制<a class="ae nl" href="https://www.kaggle.com/code/erkanhatipoglu/univariate-linear-regression-from-scratch" rel="noopener ugc nofollow" target="_blank"> <strong class="ll ir"> <em class="mf">一元线性回归</em> </strong> </a>笔记本，其中一节画出只因为迭代次数高而训练好的模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">模型训练代码块</figcaption></figure><p id="f892" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在代码块中，我们初始化α、θ₀、θ₁和收敛阈值。我们还通过调用成本函数来计算初始成本值。</p><p id="8c28" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">如前所述，while循环的停止条件是连续成本函数值的差小于阈值。我们首先为每次while循环迭代绘制训练数据和假设，用于演示目的。然后，我们通过调用成本函数来计算成本函数值。接下来，我们通过调用梯度函数来计算新的θ₀和θ₁值。接下来，我们使用新的θ值来重新计算成本值，以找出差异来检查停止条件。在循环结束时，我们为每次迭代打印一些必要的信息。</p><h2 id="d302" class="ml ks iq bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh mw bi translated">结果呢</h2><p id="42d6" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">下面的结果是针对模型定型代码块中的初始条件的。训练进行了27次迭代。据计算，θ₀约为0.015，θ₁约为0.999。最终成本值计算为约3.936。结果非常接近于在<a class="ae nl" href="https://www.kaggle.com/datasets/andonians/random-linear-regression" rel="noopener ugc nofollow" target="_blank">线性回归数据集</a>中讨论的y = x线。我们还可以看到每次迭代中假设的变化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f02760361bf583352ec2f8dd070e87fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*baApGfOHMUKiNUewlx0dpw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者使用测试数据集-图像的验证结果</figcaption></figure><h2 id="82ef" class="ml ks iq bd kt mm mn dn kx mo mp dp lb ls mq mr ld lw ms mt lf ma mu mv lh mw bi translated">确认</h2><p id="cc84" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们还可以用测试数据集来验证我们的结果，这个数据集还没有在训练过程中使用过。下面你可以看到我们的模型在测试数据集上的表现。模型的成本值约为4.702，比预期的3.936(培训成本值)略差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/e9198d7a3adcf5a10718dfc403c0d5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*MrdV-2zub-XK39GiAwtdjA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk translated">作者使用测试数据集-图像的验证结果</figcaption></figure><h1 id="04b1" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">结论</h1><p id="7c18" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">在这篇博文中，我们学习了线性回归的基础知识，比如假设、成本函数和使用单变量数据的梯度下降。我们也用Python写了那些函数。此外，使用它们，我们已经训练了一个单变量线性模型，并利用<a class="ae nl" href="https://www.kaggle.com/datasets/andonians/random-linear-regression" rel="noopener ugc nofollow" target="_blank">线性回归</a>数据集对其进行了测试。</p><p id="a6d7" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">感兴趣的读者可以在Kaggle笔记本<a class="ae nl" href="https://www.kaggle.com/code/erkanhatipoglu/univariate-linear-regression-from-scratch" rel="noopener ugc nofollow" target="_blank"> <strong class="ll ir"> <em class="mf">一元线性回归从头开始</em> </strong> </a>中找到这篇博文中用到的代码。</p><p id="9fce" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">感谢您的阅读！</p><h1 id="e6df" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">参考</h1><p id="2ca2" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">[1] V. Andonians，《线性回归》，<em class="mf">Kaggle.com</em>，2017。【在线】。可用:<a class="ae nl" href="https://www.kaggle.com/datasets/andonians/random-linear-regression." rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/datasets/andonians/random-linear-regression。</a>【访问日期:2022年9月12日】。</p><p id="d75e" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">[2]《监督学习——维基百科》，<em class="mf">En.wikipedia.org</em>，2022年。【在线】。可用:【https://en.wikipedia.org/wiki/Supervised_learning. T2】【访问时间:2022年9月13日】。</p><p id="370d" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">[3]《无监督学习——维基百科》，【En.wikipedia.org】T4，2022年。【在线】。可用:<a class="ae nl" href="https://en.wikipedia.org/wiki/Unsupervised_learning." rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Unsupervised_learning.</a>【访问时间:2022年9月13日】。</p><p id="d30a" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">[4]《强化学习——维基百科》，<em class="mf">En.wikipedia.org</em>，2022年。【在线】。可用:<a class="ae nl" href="https://en.wikipedia.org/wiki/Reinforcement_learning." rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Reinforcement_learning.</a>【访问时间:2022年9月13日】。</p><p id="4ca8" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">[5] J. Carew，“什么是强化学习？全面概述”，<em class="mf">search enterprisesai</em>，2021。【在线】。可用:<a class="ae nl" href="https://www.techtarget.com/searchenterpriseai/definition/reinforcement-learning." rel="noopener ugc nofollow" target="_blank">https://www . techtarget . com/search enterprise ai/definition/reinforcement-learning。</a>【访问日期:2022年9月12日】。</p><p id="c8b7" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">[6] J. Brownlee，“用于机器学习的线性回归”，<a class="ae nl" href="https://machinelearningmastery.com/," rel="noopener ugc nofollow" target="_blank"><em class="mf">https://machinelearningmastery.com/</em>，</a> 2022。【在线】。可用:<a class="ae nl" href="https://machinelearningmastery.com/linear-regression-for-machine-learning/." rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/linear-regression-for-machine-learning/。</a>【访问日期:2022年9月11日】。</p><p id="0072" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">[7]约翰克斯，“房价，波特兰，或，”<em class="mf">Kaggle.com</em>，2017年。【在线】。可用:<a class="ae nl" href="https://www.kaggle.com/datasets/kennethjohn/housingprice." rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/kennethjohn/housingprice.</a>【访问时间:2022年9月11日】。</p><p id="e7fe" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">[8] E. Hatipoglu，“一元线性回归从零开始”，<em class="mf">Kaggle.com</em>，2022。【在线】。可用:<a class="ae nl" href="https://www.kaggle.com/code/erkanhatipoglu/univariate-linear-regression-from-scratch." rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/code/erkanhatipoglu/单变量-线性-回归-从头开始。【访问日期:2022年9月12日】。</a></p><p id="0771" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">[9] J. Brownlee，“机器学习中的假设是什么？”，<a class="ae nl" href="https://machinelearningmastery.com/" rel="noopener ugc nofollow" target="_blank"><em class="mf">machinelearningmastery.com</em></a>，2019。【在线】。可用:<a class="ae nl" href="https://machinelearningmastery.com/what-is-a-hypothesis-in-machine-learning/." rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/what-is-a hypothesis-in-machine-learning/。</a>【访问时间:2022年9月12日】。</p><p id="9e54" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">[10] A. Ng，《斯坦福CS229:机器学习——线性回归与梯度下降|第二讲(2018年秋季)》，<em class="mf">Youtube.com</em>，2018。【在线】。可用:<a class="ae nl" href="https://www.youtube.com/watch?v=4b4MUYve_U8&amp;list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;index=2." rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=4b4MUYve_U8&amp;list = ploromvodv 4 rmigqp 3 wxshtmggzpvfbu&amp;index = 2。【访问日期:2022年9月12日】。</a></p></div></div>    
</body>
</html>