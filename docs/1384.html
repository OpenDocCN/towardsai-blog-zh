<html>
<head>
<title>Find Unauthorized Constructions Using Aerial Photography and Deep Learning with Code (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用航拍和代码深度学习发现未经授权的建筑(第2部分)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/find-unauthorized-constructions-using-aerial-photography-and-deep-learning-with-code-part-2-b56ca80c8c99?source=collection_archive---------1-----------------------#2021-01-15">https://pub.towardsai.net/find-unauthorized-constructions-using-aerial-photography-and-deep-learning-with-code-part-2-b56ca80c8c99?source=collection_archive---------1-----------------------#2021-01-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="a9e2" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/89af5a16ca9d5ff982a63b4ae8c3bff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QG4F0YMSOjQlNYFoZ0NEvQ.png"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">我们探测器的最终结果(图片由作者提供)</figcaption></figure></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="a287" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak">目录</strong></h1><p id="bec3" class="pw-post-body-paragraph lq lr iq ls b lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">1.<a class="ae mo" href="#9407" rel="noopener ugc nofollow">简介</a> <br/> 2。<a class="ae mo" href="#ddaa" rel="noopener ugc nofollow">详细项目工作流程</a> <br/> 3。<a class="ae mo" href="#d11d" rel="noopener ugc nofollow"> U-Net分割模型</a> <br/> 4。<a class="ae mo" href="#b20d" rel="noopener ugc nofollow">结果</a> <br/> 5。<a class="ae mo" href="#19d2" rel="noopener ugc nofollow">资源和作者</a></p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="9407" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">介绍</h1><p id="0e01" class="pw-post-body-paragraph lq lr iq ls b lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">这个项目旨在寻找缺乏建筑许可的潜在建筑。在我们的<a class="ae mo" href="https://zieniewicz-m-1992.medium.com/find-unauthorized-constructions-using-aerial-photography-and-deep-learning-with-code-part-1-6d3ca7ff6fa0" rel="noopener"> <strong class="ls ja">上一篇</strong> </a>中，我们讨论了问题陈述以及收集和预处理数据的过程。现在，有了50，000多张正射影像和相应的掩膜，我们可以专注于项目的第二阶段——识别建筑物的语义分割模型。</p><p id="e1e9" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">要理解这个过程，您应该熟悉在Keras中创建深层网络的基础知识和Python中的简单数据操作技术。</p><p id="7004" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">由于我们希望在透明度和隐私之间做出权衡，我们决定不公布我们的数据集。请尝试使用我们的方法和前一篇文章中提供的代码，并自行收集。</p><p id="afb7" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">您可以在参考资料和作者部分找到官方笔记本的链接。</p><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi mu"><img src="../Images/e4ee6c473d8873e36b1cc91b8fadbb25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpPtye1gqAyvmtK-19ibzQ.png"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">项目流程图(图片由作者提供)</figcaption></figure></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="ddaa" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak">详细的项目工作流程</strong></h1><p id="b292" class="pw-post-body-paragraph lq lr iq ls b lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">我们将尝试以简单的方式指导您完成训练深度学习模型的各个阶段，包括:</p><ul class=""><li id="5af8" class="mz na iq ls b lt mp lx mq mb nb mf nc mj nd mn ne nf ng nh bi translated">将数据集分成训练集和验证集，并使用基本的数据扩充技术创建数据生成器，</li><li id="bbe6" class="mz na iq ls b lt ni lx nj mb nk mf nl mj nm mn ne nf ng nh bi translated">为我们的模型的质量控制创建一个定制的度量，并为保存学习曲线的历史创建一个定制的回调，</li><li id="99d9" class="mz na iq ls b lt ni lx nj mb nk mf nl mj nm mn ne nf ng nh bi translated">使用Keras创建模型架构并训练它，</li><li id="9487" class="mz na iq ls b lt ni lx nj mb nk mf nl mj nm mn ne nf ng nh bi translated">使用回调优化培训过程并实时显示进度，</li><li id="9dc3" class="mz na iq ls b lt ni lx nj mb nk mf nl mj nm mn ne nf ng nh bi translated">对测试集的评估，</li><li id="5527" class="mz na iq ls b lt ni lx nj mb nk mf nl mj nm mn ne nf ng nh bi translated">显示一些示例结果。</li></ul><p id="ee96" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">所以…让我们把手弄脏吧。:)</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="d11d" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak"> U-Net分割模型</strong></h1><p id="67c8" class="pw-post-body-paragraph lq lr iq ls b lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">首先，我们将花一点时间关注语义分割的概念。这种方法包括为每个像素分配一个特定类别存在的概率值(范围从0到1)，在我们的例子中是“建筑物”类别。</p><p id="f9f0" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">我们的语义分割模型将在训练期间将正射影像作为输入，而输出将是具有相同高度和宽度的2D矩阵，每个像素具有特定类别的概率值(0或1)。由于这种解决方案，神经网络学会了如何将图像转换为包含所需类别和背景的遮罩。</p><p id="4406" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">事实证明，编码器-解码器深度神经网络非常适合这项任务，因为它们首先试图识别照片中任何建筑物的特征，然后将它们解码到具有给定类别概率值的掩模上。在我们的项目中，我们将使用U-Net架构，该架构最初于2015年提出，用于医学图像的分割。你可以在这里  <strong class="ls ja"> </strong>找到一篇关于优信网的优秀文章<a class="ae mo" href="https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47" rel="noopener" target="_blank"> <strong class="ls ja">(我们发现它非常有用)，同时我们专注于解决我们问题的实用方法。</strong></a></p><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi nn"><img src="../Images/9e0849f5f1da0e1f142b413a6a520959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DSLfJXPbHgEeeOgs"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">U-Net模型架构(来源:官方U-Net <a class="ae mo" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank">出版物</a>)</figcaption></figure><p id="3969" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">首先，让我们导入所需的工具，尤其是来自Keras的各种元素，它们将用于创建模型。此外，我们将使用<a class="ae mo" href="https://p.migdal.pl/livelossplot/" rel="noopener ugc nofollow" target="_blank"> <strong class="ls ja"> livelossplot </strong> </a>来可视化每个时期后的学习曲线。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="3cd4" class="nt kt iq np b gy nu nv l nw nx">import os<br/>import random<br/>from datetime import datetime<br/><br/>import numpy as np<br/>import pandas as pd<br/><br/>import matplotlib.pyplot as plt<br/>from tqdm import tqdm<br/>from sklearn.model_selection import train_test_split<br/>from skimage.transform import rotate<br/>from tensorflow import keras<br/><br/>from tensorflow.keras.models import Model, load_model<br/>from tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, MaxPooling2D, GlobalMaxPool2D, Conv2D, Conv2DTranspose<br/>from tensorflow.keras.layers import concatenate, add<br/>from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau<br/>from tensorflow.keras.optimizers import Adam<br/>from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img<br/>from tensorflow.keras.metrics import Recall, Precision<br/><br/>import keras.backend as K<br/><br/>from livelossplot import PlotLossesKeras</span></pre><p id="934c" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">接下来，我们将设置以下参数:带有照片和蒙版的目录的路径、图像形状、训练时期的数量和批量大小。我们建议的设置适应免费的Google Colab设置的能力。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="56ca" class="nt kt iq np b gy nu nv l nw nx"># images attributes <br/>img size = 256 <br/> <br/># training attributes<br/>batch_size = 64<br/>epochs = 50<br/> <br/># data paths<br/>project_path ="./"<br/>orto path = "data/geoportal orto/"<br/>masks_path = project_path + "data/geoportal_build_mask/"<br/>test_orto_path = project_path + "test/geoportal_orto/"<br/>test_masks_path = project_path + "test/geoportal_build_mask/"</span></pre><p id="51c9" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">在训练之前，让我们将整个数据集分成一个训练集和一个验证集。在数据收集阶段，测试集被分离为一个选定城市的一组图像和蒙版。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="22a9" class="nt kt iq np b gy nu nv l nw nx"># list of names of all images in the given path<br/>ids = sorted(os.listdir(orto_path))<br/>test_ids = sorted(os.listdir(test_orto_path))<br/><br/># split train and valid<br/>ids_train, ids_valid = train_test_split(ids, test_size=0.2, random_state=123)<br/><br/>print("Total images = ", len(ids))<br/>print("Train images = ", len(ids_train))<br/>print("Valid images = ", len(ids_valid))<br/>print("Test images = ", len(test_ids))</span></pre><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ny"><img src="../Images/d3eedf08c4d271e3d88382bc2089e89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7fkXVCqnd6h2RfY5tSsGEQ.png"/></div></div></figure><p id="dd71" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">现在，我们将着重于创建一个具有简单扩充(旋转90度倍数)的数据生成器。可定制的<strong class="ls ja"> </strong>数据生成器帮助我们动态地将一批照片和蒙版加载到模型中，并且根据我们是否需要，使用可选的<strong class="ls ja"> rotate_image() </strong>函数。在我们的例子中，我们不使用数据扩充进行验证和测试。如果您需要对您的图像集进行更高级的增强，Keras内置的ImageDataGenerator是一种方法。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="7bfa" class="nt kt iq np b gy nu nv l nw nx">def rotate_image(image, angle=180):<br/>    """Optional augumentation"""<br/>    return rotate(image, angle, resize=True, preserve_range=True)<br/><br/><br/>def data_gen(img_folder, mask_folder, img_ids, img_size, batch_size, random_rotate=True):<br/>    """Function for data generator"""<br/>    c = 0<br/>    random.shuffle(img_ids)<br/><br/>    while True:<br/><br/>        img = np.zeros((batch_size, img_size, img_size, 3)).astype(float)<br/>        mask = np.zeros((batch_size, img_size, img_size, 1)).astype(float)<br/><br/>        for i in range(c, c+batch_size): # initially from 0 to 64, c = 0. <br/><br/>            # load image    <br/>            img_data = img_to_array(load_img(img_folder+img_ids[i]))/255.0<br/>            <br/>            <br/>            # load mask<br/>            img_mask = img_to_array(load_img(mask_folder+img_ids[i], color_mode="grayscale"))<br/>            img_mask = (img_mask &gt; img_mask.min()).astype(int)<br/>            # add extra dimension for parity with train_img size [img_size * img_size * 3]<br/>            img_mask = img_mask.reshape(img_size, img_size, 1)<br/>            <br/>            if random_rotate:<br/>                rotate_angle = random.choice((0, 90, 180, 270))<br/>                img_data = rotate_image(img_data, rotate_angle)<br/>                img_mask = rotate_image(img_mask, rotate_angle)             <br/>            <br/>            # add to array - img[0], img[1], and so on.<br/>            img[i-c] = img_data <br/>            mask[i-c] = img_mask<br/><br/>        c += batch_size<br/>        if c+batch_size &gt;= len(img_ids):<br/>            c=0<br/>            random.shuffle(img_ids)<br/><br/>        yield img, mask<br/><br/># preparing data generators<br/>train_gen = data_gen(orto_path, masks_path, ids_train, img_size, batch_size, random_rotate=True)<br/>valid_gen = data_gen(orto_path, masks_path, ids_valid, img_size, batch_size, random_rotate=False)<br/>test_gen = data_gen(test_orto_path, test_masks_path, test_ids, img_size, batch_size, random_rotate=False)</span></pre><p id="0f94" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">最后，让我们创建我们的架构。首先，我们将创建一个辅助函数来构建一个卷积块，它由两个卷积层组成，可以归一化它们的输出。激活功能是ReLU。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="c695" class="nt kt iq np b gy nu nv l nw nx">def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):<br/>    """Function to add 2 convolutional layers with the parameters passed to it"""<br/>    # first layer<br/>    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), kernel_initializer = 'he_normal', padding = 'same')(input_tensor)<br/>    if batch norm:<br/>        x = BatchNormalization()(x)<br/>    x = Activation('relu')(x)<br/>    <br/>    # second layer<br/>    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), kernel_initializer = 'he_normal', padding = 'same')(x)<br/>    if batch norm:<br/>        x = BatchNormalization()(x)<br/>    x = Activation('relu')(x)<br/>    <br/>    return x</span></pre><p id="ef3d" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">现在，我们将处理main函数，该函数创建一个结合收缩和扩展路径的模型，输出是0.0–1.0范围内的分段预测掩码。我们还添加了一个dropout来帮助模型更好地概括。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="d688" class="nt kt iq np b gy nu nv l nw nx">def get_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True):<br/>    """Function to define the UNET Model"""<br/>    # Contracting Path<br/>    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)<br/>    p1 = MaxPooling2D((2, 2))(c1)<br/>    p1 = Dropout(dropout)(p1)<br/>    <br/>    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)<br/>    p2 = MaxPooling2D((2, 2))(c2)<br/>    p2 = Dropout(dropout)(p2)<br/>    <br/>    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)<br/>    p3 = MaxPooling2D((2, 2))(c3)<br/>    p3 = Dropout(dropout)(p3)<br/>    <br/>    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)<br/>    p4 = MaxPooling2D((2, 2))(c4)<br/>    p4 = Dropout(dropout)(p4)<br/>    <br/>    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)<br/>    <br/>    # Expansive Path<br/>    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)<br/>    u6 = concatenate([u6, c4])<br/>    u6 = Dropout(dropout)(u6)<br/>    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)<br/>    <br/>    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)<br/>    u7 = concatenate([u7, c3])<br/>    u7 = Dropout(dropout)(u7)<br/>    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)<br/>    <br/>    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)<br/>    u8 = concatenate([u8, c2])<br/>    u8 = Dropout(dropout)(u8)<br/>    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)<br/>    <br/>    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)<br/>    u9 = concatenate([u9, c1])<br/>    u9 = Dropout(dropout)(u9)<br/>    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)<br/>    <br/>    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)<br/>    model = Model(inputs=[input_img], outputs=[outputs])<br/>    return model</span></pre><p id="8fda" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated"><strong class="ls ja"> custom_f1 </strong>指标是使用张量运算创建的，如下文<a class="ae mo" href="https://medium.com/@aakashgoel12/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d" rel="noopener">出版物</a>所述。它将允许我们通过F1分数来控制模型的质量，F1分数是精确度和召回率的调和平均值。通过这种方式，我们试图避免模型通过预测实际上整个数据集的0或1值而走向极值的情况，这种情况通常在这种不平衡的数据中发生。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="d4bb" class="nt kt iq np b gy nu nv l nw nx">def custom_f1(y_true, y_pred): <br/>    """Function to calculate F1-score"""<br/>    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))<br/>    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))<br/>    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))<br/>    precision = true_positives / (predicted_positives + K.epsilon())<br/>    recall = true_positives / (possible_positives + K.epsilon())<br/>    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())<br/>    return f1 val</span></pre><p id="15c7" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">是时候编译我们的模型并检查它的摘要了。我们使用Adam optimizer、BCE(二进制交叉熵)作为损失函数，并将准确度、召回率、精确度和custom_f1作为我们将在训练期间监控的指标。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="6fd9" class="nt kt iq np b gy nu nv l nw nx">input_img = Input((img_size, img_size, 3), name='img')<br/>model = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=False)<br/>model.compile(optimizer=Adam(lr=0.001), <br/>              loss="binary_crossentropy", <br/>              metrics=[ "accuracy", Recall(), Precision(), custom_f1])<br/>model.summary()</span></pre><p id="125c" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">下一步是创建一个定制的回调类，以便能够将我们的培训历史保存为一个单独的文件。在我们的例子中，在每个时期之后，我们简单地用当前的损失和度量值创建pandas DataFrame，并将其保存为Google Drive上的. csv文件。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="2000" class="nt kt iq np b gy nu nv l nw nx">class HistoryCallback(keras.callbacks.Callback):<br/>    def on_epoch_end(self, epoch, logs=None):<br/>        """Callback function at the end of each epoch"""      <br/>        history_path = checkpoints_path + "history/"<br/>        if not os.path.exists(history_path): <br/>          os.makedirs(history_path)<br/><br/>        filename = history_path + model_name + "_history.csv"<br/>        keys = list(logs.keys())<br/>        values = np.array(list(logs.values()))<br/>        df = pd.DataFrame(data= [values], columns=keys)<br/>        df['epoch'] = epoch<br/>        df.reset_index(drop=True)<br/>        with open(filename, 'a') as f: <br/>          df.to_csv(f, header=f.tell()==0)<br/><br/>        print(f"Saved history after epoch {epoch} to: {filename}")</span></pre><p id="6d3d" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">现在，我们可以创建一个回调列表，包括:</p><ul class=""><li id="fa01" class="mz na iq ls b lt mp lx mq mb nb mf nc mj nd mn ne nf ng nh bi translated">提前停止(如果训练在一定时间内没有进展)，</li><li id="68ab" class="mz na iq ls b lt ni lx nj mb nk mf nl mj nm mn ne nf ng nh bi translated">当验证丢失卡住时，降低优化器的学习率，</li><li id="7e13" class="mz na iq ls b lt ni lx nj mb nk mf nl mj nm mn ne nf ng nh bi translated">使用验证集上的最佳custom_f1()保存模型(如果您监视自定义指标，请记住设置最小/最大模式)，</li><li id="0c97" class="mz na iq ls b lt ni lx nj mb nk mf nl mj nm mn ne nf ng nh bi translated">在培训过程中实时绘图以查看学习曲线。</li></ul><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="2316" class="nt kt iq np b gy nu nv l nw nx">callbacks = [<br/>    EarlyStopping(patience=10, verbose=1),<br/>    ReduceLROnPlateau(factor=0.33, patience=4, min_lr=0.000001, verbose=1),<br/>    ModelCheckpoint(checkpoints_path + model_name + ".h5", <br/>                    verbose=1, <br/>                    save_best_only=True, <br/>                    save_weights_only=False,<br/>                    monitor='val_custom_f1',<br/>                    mode = "max"),<br/>    PlotLossesKeras(),<br/>    HistoryCallback()<br/>]</span></pre><p id="6237" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">现在，我们终于可以拟合我们的模型，并在几个小时内欣赏所有按照预期方向绘制的美丽图表，当F1分数上升时(大多数时间)，损失越来越低。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="ef3b" class="nt kt iq np b gy nu nv l nw nx">results = model.fit(<br/>    train_gen, <br/>    steps_per_epoch=len(ids_train)//batch_size,<br/>    epochs=epochs, <br/>    callbacks=callbacks,<br/>    validation_data=valid_gen,<br/>    validation_steps=len(ids_valid)//batch_size, <br/>    verbose=1<br/>)</span></pre></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><h1 id="b20d" class="ks kt iq bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">结果</h1><p id="8f0a" class="pw-post-body-paragraph lq lr iq ls b lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">在验证期间，基本模型似乎显示出F1分数高达0.71的有希望的结果(参见基准测试结果<a class="ae mo" href="https://www.diva-portal.org/smash/get/diva2:1417200/FULLTEXT01.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>，第42页)，所以让我们来看看它能提供什么。当您检查学习曲线时，您可以看到val_loss值减少的停滞导致在epochs限制之前提前停止，而验证集的最大F1分数是在训练的后半部分开始时获得的。</p><figure class="mv mw mx my gt ka gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/127df7321537bef1aca40fa77759c9ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/0*MwaboefCrAmTwJdB"/></div></figure><figure class="mv mw mx my gt ka gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/cfdf609fbc2f3d56ff43d42be815e76f.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/0*ztF40qkV8WtlRjVc"/></div></figure><p id="546c" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">对测试集的评估显示结果略好于验证集，获得0.74的F1分数。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="f9b3" class="nt kt iq np b gy nu nv l nw nx">model.evaluate(<br/>    test_gen, <br/>    steps=len(test_ids)//batch_size,<br/>    verbose=1<br/>)</span></pre><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ob"><img src="../Images/8143c78591fa6b6c44469e1e770c0835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Z9StDWGLvw5hzJEqhsGLw.png"/></div></div><figcaption class="kh ki gj gh gi kj kk bd b be z dk translated">测试集结果</figcaption></figure><p id="adcf" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">现在，让我们来看看我们的细分模型实际上是如何工作的。我们将并排绘制正射影像、真实遮罩、预测遮罩以及阈值预测和真实遮罩之间的差异。</p><pre class="mv mw mx my gt no np nq nr aw ns bi"><span id="4a57" class="nt kt iq np b gy nu nv l nw nx">def plot_sample(img_folder, mask_folder, img_ids, thresh=0.5, i=None):<br/>    """Function to plot the results"""<br/>    if i is None:<br/>        i = random.randint(0, len(img_ids))<br/>    <br/>    img = np.zeros((1, img_size, img_size, 3)).astype(float)<br/>    mask = np.zeros((1, img_size, img_size, 1)).astype(float)<br/>    <br/>    # load image    <br/>    img_data = img_to_array(load_img(img_folder+img_ids[i]))/255.0<br/><br/>    # load mask<br/>    img_mask = img_to_array(load_img(mask_folder+img_ids[i], color_mode="grayscale"))<br/>    img_mask = (img_mask &gt; img_mask.min()).astype(int)<br/>    # add extra dimension for parity with train_img size [img_size * img_size * 3]<br/>    img_mask = img_mask.reshape(img_size, img_size, 1)             <br/><br/>    # add to array<br/>    img[0] = img_data <br/>    mask[0] = img_mask<br/>    <br/>    # print file id for reference<br/>    print(img_ids[i])<br/><br/>    # mask prediction<br/>    mask_pred = model.predict(img)[0]<br/>    mask_pred_t = (mask_pred &gt; thresh).astype(np.uint8)<br/><br/>    # mask diff<br/>    mask_diff = mask_pred_t * (img_mask&lt;1)<br/><br/>    fig, ax = plt.subplots(1, 5, figsize=(30, 10))<br/>    ax[0].imshow(img[0])<br/>    ax[0].set_title('orto rgb')<br/><br/>    ax[1].imshow(mask.squeeze())<br/>    ax[1].set_title('mask')<br/><br/>    ax[2].imshow(mask_pred.squeeze())<br/>    ax[2].set_title('mask predicted')<br/><br/>    ax[3].imshow(mask_pred_t.squeeze())<br/>    ax[3].set_title('mask predicted binary')<br/>    <br/>    ax[4].imshow(mask_diff.squeeze())<br/>    ax[4].set_title('mask predicted binary - mask');</span><span id="6784" class="nt kt iq np b gy oc nv l nw nx">plot_sample(test_orto_path, test_masks_path, img_ids=test_ids, thresh=0.5)</span></pre><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi od"><img src="../Images/2b0c42c8e98395465393fb31a6459692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ivq_qUi0vjt6dxii"/></div></div></figure><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi od"><img src="../Images/c401395b7fbfc2ff71c142b5415c5675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2vdoW-q5txB0qWQP"/></div></div></figure><figure class="mv mw mx my gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi od"><img src="../Images/423f211dfd506d5521340416e3086e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gz9LixVMqLrRS-3f"/></div></div></figure><p id="60f6" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">而且……模型工作正常。😁</p></div><div class="ab cl kl km hu kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="ij ik il im in"><p id="569d" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated"><a class="ae mo" href="https://github.com/DataWorkshop-Foundation/olsztyn-project-samowola/blob/main/about/notebooks/2_building_unet_model.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ls ja">链接到官方笔记本</strong> </a></p><p id="6b23" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">认识一下我们的团队:<br/> <a class="ae mo" href="https://www.linkedin.com/in/martaaugustynowicz/" rel="noopener ugc nofollow" target="_blank">玛尔塔·奥古斯特诺维奇</a> <br/> <a class="ae mo" href="https://www.linkedin.com/in/sawaniewski/" rel="noopener ugc nofollow" target="_blank">茹卡斯兹·萨瓦尼耶夫斯基</a> <br/> <a class="ae mo" href="https://www.linkedin.com/in/dtanajewski/" rel="noopener ugc nofollow" target="_blank">达留什·塔纳耶夫斯基</a> <br/> <a class="ae mo" href="https://www.linkedin.com/in/igor-wieczorek/" rel="noopener ugc nofollow" target="_blank">伊戈尔·维乔雷克</a> <br/> <a class="ae mo" href="https://www.linkedin.com/in/maciej-zieniewicz/" rel="noopener ugc nofollow" target="_blank">马切伊·杰涅维奇</a></p><p id="7211" class="pw-post-body-paragraph lq lr iq ls b lt mp lv lw lx mq lz ma mb mr md me mf ms mh mi mj mt ml mm mn ij bi translated">本项目在<a class="ae mo" href="https://dataworkshop.foundation/" rel="noopener ugc nofollow" target="_blank"> <strong class="ls ja"> DataWorkshop基金会</strong> </a> <strong class="ls ja"> </strong>社区内进行。</p><figure class="mv mw mx my gt ka gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/175b93acb806829514fda09f4e359140.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*8xyrYQm2fHyKZhnEZxqXDA.jpeg"/></div></figure><blockquote class="of og oh"><p id="2cf7" class="lq lr oi ls b lt mp lv lw lx mq lz ma oj mr md me ok ms mh mi ol mt ml mm mn ij bi translated">DataWorkshop Foundation主要是关于机器学习和人。我们专注于亲社会活动，利用机器学习的潜力。DataWorkshop基金会的目标是通过实践活动传播有关机器学习和人工智能的知识，从而解决当前的重要问题。</p></blockquote></div></div>    
</body>
</html>