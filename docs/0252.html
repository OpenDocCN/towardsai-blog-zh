<html>
<head>
<title>A Practical Tip When Working With Random Samples On Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Spark上处理随机样本时的实用技巧</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-practical-tip-when-working-with-random-samples-on-spark-23f6dbbe722b?source=collection_archive---------0-----------------------#2019-12-26">https://pub.towardsai.net/a-practical-tip-when-working-with-random-samples-on-spark-23f6dbbe722b?source=collection_archive---------0-----------------------#2019-12-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f6a92f2000ad3383198c29bc14ec3ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vPkL_eBPGKc7E_2-l2Jfw.png"/></div></div></figure><h1 id="39f5" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">介绍</h1><p id="b74d" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在本文中，我将分享使用Spark分析数据帧的随机样本时的一个重要技巧。重现结果的代码可以在<a class="ae lx" href="https://gist.github.com/hsm207/ebf98586fe21fc59f31143160de80698" rel="noopener ugc nofollow" target="_blank">这里</a>找到。这是一个HTML版本的Databricks笔记本，所以你所要做的就是下载它的原始格式，然后在网络浏览器中显示下载的文件。</p><h1 id="4933" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">问题陈述</h1><p id="da17" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">假设我们正在处理一个大型数据集。出于本文的目的，我们将假设所讨论的数据集是航班起飞延误数据集，如下所示:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/a1b071651807d457bcb67f8d725103b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*yf3LEUS0S7ZKkSwpb4ohTA.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图1:航班起飞延误数据集的摘录。来源:Databricks的数据集</figcaption></figure><p id="2903" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">由于这是一个大型数据集，我们决定从它的一个子集开始分析。很多人说我们可以对这个数据集进行采样，所以让我们假设我们决定随机样本<code class="fe mm mn mo mp b">origin</code>的所有延迟都适用于我们希望我们的分析回答的任何业务问题。</p><p id="4381" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">下面是我们如何使用Pandas获得所需子集的方法:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/852ed583771db6d54c09511eb7c37da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fxGgcOA0cG69QOen6KWXjA.png"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图2:使用Pandas获得所需的子集</figcaption></figure><p id="60e0" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">在图2中，我们从将数据集读入<code class="fe mm mn mo mp b">df</code>开始。然后，我们构建另一个包含随机样本<code class="fe mm mn mo mp b">origin</code>的数据帧，我们将使用它来过滤<code class="fe mm mn mo mp b">df</code>，并将过滤后的数据帧保存到<code class="fe mm mn mo mp b">df_sample</code>。<code class="fe mm mn mo mp b">df_sample</code>将成为所有进一步分析的起点。</p><p id="9ece" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">我们如何使用Spark构建相同的子集？</p><h1 id="abdf" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">解决办法</h1><p id="a4f3" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">构建子集的一个自然方法是用Spark可以理解的术语重写Python代码。下面是在Scala中使用Spark的Dataframe API的一种方法:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b8f4c5d73aac84155bbef0f02c9ef706.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*1E1x6kbloB4ZJmN16eBt-g.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图3:使用Spark的Dataframe API获得所需的子集</figcaption></figure><h1 id="c987" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">陷阱</h1><p id="df29" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">下面就来对<code class="fe mm mn mo mp b">dfSample</code>做一些分析。以下是唯一原始值的计数:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/49860314a4ec82149fcdee2c5fbcee09.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*Yj2w5Y7M2nbxZw0xaaGw1g.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图4:随机样本中唯一来源的计数</figcaption></figure><p id="c3dc" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">以下是唯一目标值的计数:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/bf5895da6e1eafb98132946c2d934366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*jHon3roDAh4yhbBqqDRxfQ.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图5:随机样本中唯一目的地的计数</figcaption></figure><p id="5023" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">这里有一个值得思考的问题:用来计算唯一目的地计数的<code class="fe mm mn mo mp b">dfSample</code>和用来计算唯一起点计数的是同一个吗？</p><p id="f1c5" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">让我们再次尝试执行图5中的代码来找出答案:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/1b75bf982ac83e2cb713a0670e51b2b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*3fGE_3sPeMsvF7Xu8JqUvg.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图6:重新执行图5中的代码</figcaption></figure><p id="8fa7" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">很显然，<code class="fe mm mn mo mp b">dfSample</code>每次调用都会改变！这意味着我们基于对<code class="fe mm mn mo mp b">dfSample</code>的分析得出的任何结论都是无效的，因为每次计算都会产生一个新的样本。</p><h1 id="6549" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">实际解决方案</h1><p id="91d3" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们应该这样定义<code class="fe mm mn mo mp b">dfSample</code>:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/6310b711753392b978d37a9cf89a99fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*_f-lNwsmYZV6mI_Jar4qPQ.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk translated">图7:构建dfSample的正确方法</figcaption></figure><p id="c03d" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">请注意，计算目的地唯一数量的两个调用在两种情况下产生相同的结果。</p><p id="036d" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">以前的解决方案不起作用的原因是，按照设计，每次需要结果时，Spark都会从头重新执行整个转换。这意味着对于如图5所示的代码，除了计算<code class="fe mm mn mo mp b">dfSample</code>中目的地的唯一数量，Spark还将重新计算<code class="fe mm mn mo mp b">dfSample</code>，这需要重新执行图4中定义了产生<code class="fe mm mn mo mp b">dfSample</code>的转换的代码。因此，每次执行图5中的代码时，原因<code class="fe mm mn mo mp b">dfSample</code>都会改变。</p><p id="993c" class="pw-post-body-paragraph kz la it lb b lc mh le lf lg mi li lj lk mj lm ln lo mk lq lr ls ml lu lv lw im bi translated">解决这个问题的方法是缓存<code class="fe mm mn mo mp b">dfSample</code>的结果，如图7所示。其效果是显示唯一原点计数结果的调用将触发计算<code class="fe mm mn mo mp b">dfSample</code>的调用，因为结果取决于<code class="fe mm mn mo mp b">dfSample</code>。然后，任何依赖于<code class="fe mm mn mo mp b">dfSample</code>的后续结果都将重用缓存的版本。</p><h1 id="7405" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结论</h1><p id="8750" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">本文的重点是展示在进行进一步分析之前缓存随机采样数据帧的结果的重要性。这是值得强调的一点，因为它是Spark独有的，这是由于它的惰性评估性质，这对于其他数据分析工具(如Pandas或dplyr)的用户来说可能不明显。</p></div></div>    
</body>
</html>