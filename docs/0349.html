<html>
<head>
<title>Why Perceptron Neurons Need Bias Input?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么感知器神经元需要偏置输入？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/why-perceptron-neurons-need-bias-input-2144633bcad4?source=collection_archive---------0-----------------------#2020-03-07">https://pub.towardsai.net/why-perceptron-neurons-need-bias-input-2144633bcad4?source=collection_archive---------0-----------------------#2020-03-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/27d5d079cfec26707a2d5c049517280a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XJ3DzpUx05BUUTa1GZBWKw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">如何避免神经网络中的偏差输入？请不要。</figcaption></figure><p id="c097" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">大家都知道什么是神经网络<em class="la">偏差</em>输入。自从第一个感知器实现以来，每个人都在使用它。但是<strong class="ke ir">我们为什么要用</strong>呢？你考虑过吗？就我自己而言，直到前一段时间我才知道。我和一个大学生讨论一些神经网络模型，不知何故，她把<strong class="ke ir"> <em class="la">偏差</em>输入</strong>和<strong class="ke ir">统计偏差</strong>搞错了。我很容易地向她解释了这些概念，但我很难解释为什么我们使用<em class="la">偏差</em>。过了一会儿，我决定尝试一些代码来进一步研究它。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="0415" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">先简单说一下背景。</p><p id="3174" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">感知器是多层感知器(MLP)人工神经网络的前身。这是一个众所周知的，受生物启发的算法来进行监督学习。正如我们在图中看到的，它是一个线性分类器:</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi li"><img src="../Images/b831fbaffe9aaf5b47358a30ce83335c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LJvykRaV3mszPCM5HUzmcg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">一个简单的感知器图形描述。</figcaption></figure><p id="438c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下面我们可以看到这个模型的数学方程式:</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/c1ebfd892a3f856853c42d4230795581.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*IpRRPdM_qZIeHgZX6Ti9qg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">其中:<strong class="bd lo"> f(x) </strong>为激活函数(一般为阶跃函数)。偏差是<strong class="bd lo"> b </strong>，而<strong class="bd lo"> p </strong>和<strong class="bd lo"> w </strong>分别是输入和权重。</figcaption></figure><p id="ab0e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">你可能会注意到与线性函数的标准形式的相似性。如果我们移除激活函数，这些公式将是相同的(为了清楚起见，这里我们只考虑一个输入):</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/369bb03e9d61d009fad05b621d059ea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ADFPN1Ky3Kts4UWdcvhaA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">在第一个例子中，bias=1。</figcaption></figure><p id="f2e8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">比较这两个公式，很明显我们的<strong class="ke ir"> <em class="la">偏差</em> </strong>是一个线性函数的<strong class="ke ir"> <em class="la"> b </em> </strong>分量。那么，现在的问题是:线性函数中的<strong class="ke ir"> <em class="la"> b </em> </strong>分量的重要性是什么？如果你在过去的几年里没有上过任何线性代数课程(就像我一样)，可能很难记住。但这是一个简单的概念，很容易用图形来理解:</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/9045c72bdc63461fb1ea16837bbcd64d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AxnCXDsw-W0C2O51mx1gyw.png"/></div></div></figure><p id="bec6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">所以，我们很容易注意到，在<strong class="ke ir"> <em class="la"> b=0，</em> </strong> <em class="la"> </em>的情况下，函数总是会经过原点<strong class="ke ir">【0，0】</strong>。而当我们引入值给<strong class="ke ir"><em class="la"/></strong>b保持<strong class="ke ir"><em class="la"/></strong>a固定时，新的函数将总是相互平行的。那么，我们能从中学到什么呢？</p><blockquote class="lr"><p id="e556" class="ls lt iq bd lu lv lw lx ly lz ma kz dk translated">我们可以说，a分量决定了函数的角度，而b分量决定了函数与x轴相交的位置。</p></blockquote><p id="d2c3" class="pw-post-body-paragraph kc kd iq ke b kf mb kh ki kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz ij bi translated">我想你已经注意到其中的问题了，对吗？没有b组件，我们会失去很多灵活性。对一些发行版进行分类可能有用，但不是对所有人都有用。测试一下怎么样，看看实际效果如何？让我们用一个简单的例子:OR函数。我们来看看它的分布(其实就是真值表):</p><p id="80db" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果我们把它画在笛卡尔平面上:</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/4c81add0fb267a77c0adff3b9e1a98ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*1hs-1fFk5dT1Y2w7L8clCA.png"/></div></figure><p id="529e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我想你已经解决了这个问题。有两个点(<strong class="ke ir">【0，0】</strong>和<strong class="ke ir">【1，0】</strong>)经过原点，分类不同。一个过线投掷<strong class="ke ir">【0，0】</strong>是没有办法分割这两个种群的。感知器会怎么处理？有<em class="la">偏置</em>和没有<em class="la">偏置</em>会怎么样？让我们进入一些代码，看看事情将如何发生！感知器神经元有一个简单的Python实现:</p><pre class="lj lk ll lm gt mh mi mj mk aw ml bi"><span id="cf49" class="mm mn iq mi b gy mo mp l mq mr">class Perceptron():</span><span id="3c1f" class="mm mn iq mi b gy ms mp l mq mr">def __init__ (self, n_input, alpha=0.01, has_bias=True):<br/>    self.has_bias = has_bias<br/>    self.bias_weight = random.uniform(-1,1)<br/>    self.alpha = alpha<br/>    self.weights = []<br/>    for i in range(n_input):<br/>        self.weights.append(random.uniform(-1,1))</span><span id="a082" class="mm mn iq mi b gy ms mp l mq mr">def classify(self, input):<br/>    summation = 0<br/>    if(self.has_bias):<br/>        summation += self.bias_weight * 1<br/>    for i in range(len(self.weights)):<br/>        summation += self.weights[i] * input[i]<br/>    return self.activation(summation)</span><span id="3443" class="mm mn iq mi b gy ms mp l mq mr">def activation(self, value):<br/>    if(value &lt; 0):<br/>        return 0<br/>    else: <br/>        return 1<br/>    <br/>def train(self, input, target):<br/>    guess = self.classify(input)<br/>    error = target - guess<br/>    if(self.has_bias):<br/>        self.bias_weight += 1 * error * self.alpha<br/>    for i in range(len(self.weights)):<br/>        self.weights[i] += input[i] * error * self.alpha</span></pre><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/70946dc8b08307f5914b664042b48a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*nqajDBLFXu9Y-4WStzY2Og.gif"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">感知器无偏差训练</figcaption></figure><p id="ff9c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">先来看看没有<em class="la">偏差</em>的训练。我们知道，分类规则(我们的函数，在这种情况下)总是会通过抛出点<strong class="ke ir">【0，0】</strong>。正如我们在下面可以注意到的，分类器将永远无法将类分开。在这种情况下，它非常接近a do it，但它不能将<strong class="ke ir">【0，0】</strong>和<strong class="ke ir">【1，0】</strong>分开。</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/04a2a2847b1af2326a988a2508abf441.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*r8Lit5OMMk21_Almorkntw.gif"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">有偏差的感知器训练</figcaption></figure><p id="62fa" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在让我们看看带有<em class="la">偏置</em>输入的感知器。首先，注意分类器的自由度。正如我们之前所说，它有更大的灵活性来创建不同的规则。另外，我们可以注意到，它正在寻找与上一个例子相同的局部最小值，但现在他可以公开移动，并找到分割数据的最佳位置。</p><p id="e93a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">所以，我认为<em class="la">偏差</em>输入的重要性现在已经很明显了。我知道，你可能在思考激活函数。我们在python的例子中使用了一个阶跃函数，如果我们使用一个sigmoid作为激活函数，它可能会在没有<em class="la">偏差</em>的情况下工作得更好。相信我:不会的。让我们看看当我们将线性函数插入到sigmoid激活函数中时，函数是如何变化的(然后我们得到σ(f(x))):</p><figure class="lj lk ll lm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/66157512a5d13364eb09823649c4c84e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Fr7yCcmv_dHXNSt9Qu3Xg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">σ(f(x))的不同曲线</figcaption></figure><p id="a236" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">你注意到了吗，这里的例子和线性函数很相似。sigmoid函数改变了格式，但是我们还是有同样的问题:没有<em class="la"> bias </em>，所有的函数传递都抛出原点。当我们试图用一条曲线来拟合一个总体时，这仍然是一个不受欢迎的行为。如果您想尝试一下，看看它是如何工作的，您只需要对python代码做一些小小的修改。</p><p id="dc2f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我真诚地感谢你对这个问题的兴趣。如果你有什么建议，意见，或者只是想打个招呼，请留下评论！我将很高兴与你讨论它！</p></div></div>    
</body>
</html>