<html>
<head>
<title>Natural Language Processing: What, Why, and How?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理:什么，为什么，如何？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/natural-language-processing-what-why-and-how-3936b8a85e0d?source=collection_archive---------2-----------------------#2021-05-04">https://pub.towardsai.net/natural-language-processing-what-why-and-how-3936b8a85e0d?source=collection_archive---------2-----------------------#2021-05-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="effe" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="a4d7" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">NLP初学者完全手册</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/b7ecb5437e2779c9f30d3577bae18d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*0sxbtoBhMRKMJbPX98V4Mw.png"/></div></figure><h2 id="aea5" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq iw bi translated">目录:</h2><ul class=""><li id="7407" class="lr ls iq lt b lu lv lw lx lf ly lj lz ln ma mb mc md me mf bi translated">什么是自然语言处理(NLP)？</li><li id="b063" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">自然语言处理是如何工作的？</li><li id="a2f2" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">标记化</li><li id="11e2" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">词干化和词汇化</li><li id="485f" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">停止言语</li><li id="9344" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">正则表达式</li><li id="1f14" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">一袋单词</li><li id="c0a9" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">N-grams</li><li id="b939" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">TF-IDF</li></ul></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><p id="9e9a" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">想知道谷歌搜索如何准确显示你想看的内容吗？”<strong class="lt ja"> Puma </strong>“既可以是动物也可以是鞋类公司，但对你来说，主要是鞋类公司和谷歌知道吧！</p><p id="a776" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">它是如何发生的？搜索引擎如何理解你想说的话？</p><p id="ed0a" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">聊天机器人是如何回复你问他们的问题而不被偏离的？Siri、Alexa、Cortana、Bixby 是如何工作的？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/880f018cb0a608af14655fe8e70c0a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AleRfAkV6NffUbFb"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk translated">照片由<a class="ae nr" href="https://unsplash.com/@lazargugleta?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">耶戈·古格莱塔</a>在<a class="ae nr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="11d5" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">这些都是<strong class="lt ja">自然语言处理(NLP)的奇迹。</strong></p><h1 id="98f9" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">什么是自然语言处理？</h1><p id="5de9" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">计算机太好了，不能处理表格/结构化数据，它们可以很容易地检索特征，学习它们并产生所需的输出。但是，为了创建一个健壮的虚拟世界，我们需要一些技术，通过这些技术，我们可以让机器像人类一样理解和交流，即通过自然语言。</p><p id="0e57" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated"><strong class="lt ja">自然语言处理</strong>是<strong class="lt ja">人工智能</strong>的子领域，处理机器和人类语言。它用于理解人类语言的逻辑意义，通过考虑不同的方面，如形态学，句法，语义和语用学。</p><p id="5323" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">NLP的一些应用有:</p><ul class=""><li id="2b58" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb mc md me mf bi translated"><em class="nh">机器音译。</em></li><li id="62bb" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><em class="nh">语音识别。</em></li><li id="339b" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><em class="nh">情绪分析。</em></li><li id="8f84" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">文本摘要。</li><li id="6288" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">聊天机器人。</li><li id="9778" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><em class="nh">文本分类。</em></li><li id="619b" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><em class="nh">字符识别。</em></li><li id="e65b" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">拼写检查。</li><li id="bd1e" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><em class="nh">垃圾邮件检测。</em></li><li id="f786" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">自动完成。</li><li id="58b8" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><em class="nh">命名实体识别。</em></li></ul><h1 id="277c" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">自然语言处理是如何工作的？</h1><p id="7711" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">人类的语言并不遵循一套明确的规则，我们模糊地交流。“<em class="nh">好的</em>”可以多次使用，在不同的句子中仍然传递不同的意思。</p><p id="aa19" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">如果我们希望我们的机器能够准确处理自然语言，我们需要为它们提供一套特定的规则，并且必须考虑各种其他因素，如语法结构、语义、情感、过去和未来单词的影响。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/516b6d28f4c2e0eb9f3fa14d18a85ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*9tMASuRa9yKTAf2x.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk translated">NLP的阶段，<a class="ae nr" href="https://www.tutorialandexample.com/nlp-tutorial/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="7181" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated"><strong class="lt ja">词法分析:</strong>这是负责检查单词的结构，它是通过将句子和段落分解成一大块文本来完成的。</p><p id="e419" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated"><strong class="lt ja">句法分析:</strong>当我们试图理解单词之间的语法关系时，它就发挥作用了。这也需要帮助安排的话，以产生真正的和合乎逻辑的意义。</p><p id="d619" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">例如<em class="nh">“上学他”</em>，这在逻辑上是正确的，但在语法上，更好的词语安排会有很大帮助。</p><p id="32b4" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated"><strong class="lt ja">语义分析:</strong>我们仅仅把一个句子中的单词的意思连接起来，并不能真正得到这个句子的意思。我们需要考虑其他因素，如过去和未来单词的影响。下面是语义分析的帮助。</p><p id="3e79" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">例如，“cold fire”可能看起来语法正确，但逻辑上是不相关的，因此它将被语义分析器丢弃。</p><p id="6761" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated"><strong class="lt ja">披露整合</strong>:遵循明确定义的方法，考虑过去陈述的影响，以生成下一个陈述的含义。</p><p id="3969" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">汤姆食物中毒，因为他吃了垃圾食品。现在使用这个句子，我们可以得出结论，汤姆遇到了一场悲剧，这是他的错，但如果我们删除一些短语或只考虑几个短语，意思可能会改变。</p><p id="d781" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated"><strong class="lt ja">语用分析</strong>:它有助于发现文本中隐藏的意义，我们需要结合上下文对其进行更深入的理解。</p><p id="9949" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">汤姆买不起汽车，因为他没有钱。</p><p id="1ec2" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">汤姆不会有汽车，因为他不需要。</p><p id="fe82" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">两个句子中“他”的意思是完全不同的，为了弄清楚这种区别，我们需要世界知识和造句的语境。</p><h1 id="5e63" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">标记化</h1><p id="4cb6" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated"><strong class="lt ja">标记化</strong>可以定义为以更短的形式打断句子或单词。接下来的想法可能是，如果我们观察到句子中的任何标点符号，就马上打破它，对于单词，如果我们看到任何空格字符分割句子。</p><h2 id="3189" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq iw bi translated">句子标记化</h2><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="9179" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">作为输出，我们得到两个独立的句子。</p><pre class="kp kq kr ks gt om on oo op aw oq bi"><span id="76e7" class="kw kx iq on b gy or os l ot ou">Google is a great search engine that outperforms Yahoo and Bing.</span><span id="3d0b" class="kw kx iq on b gy ov os l ot ou">It was found in 1998</span></pre><h2 id="7958" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq iw bi translated">单词标记化</h2><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="fa93" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">输出:</p><pre class="kp kq kr ks gt om on oo op aw oq bi"><span id="fecb" class="kw kx iq on b gy or os l ot ou">['Google', 'is', 'a', 'great', 'search', 'engine', 'that', 'outperforms', 'Yahoo', 'and', 'Bing', '.'] </span><span id="0fb2" class="kw kx iq on b gy ov os l ot ou">['It', 'was', 'found', 'in', '1998']</span></pre><h1 id="11e3" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">词干化和词汇化</h1><p id="4952" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">从语法上来说，不同形式的词根在时态和用例上的变化意味着相同的意思。举例来说，<strong class="lt ja"> <em class="nh">驱动</em>，<em class="nh">驱动</em>，<em class="nh">驱动</em>，<em class="nh">驱动</em> </strong>都是逻辑上相同的意思，只是用在不同的场景中。</p><p id="271e" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">为了将单词转换成通用形式，我们使用词干化和词汇化。</p><h2 id="feb0" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq iw bi translated">词干:</h2><p id="8a60" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">这种技术倾向于通过使用机器生成算法将词根格式化为词干来生成词根。</p><p id="ada4" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">例如“<strong class="lt ja">研究</strong>”、“<strong class="lt ja">研究</strong>”、“<strong class="lt ja">研究</strong>”、“<strong class="lt ja">研究</strong>”都会转换为“<strong class="lt ja">研究</strong>”，而不是“<strong class="lt ja">研究</strong>”(这是一个准确的词根)。</p><p id="3932" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">词干提取的输出可能不总是符合语法逻辑和语义，这是因为它完全由算法驱动。</p><p id="3b2f" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">不同类型的斯特梅尔有:</p><ul class=""><li id="ced8" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb mc md me mf bi translated"><strong class="lt ja">波特·斯特梅尔</strong></li><li id="e145" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><strong class="lt ja">雪球斯特梅尔</strong></li><li id="0b91" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><strong class="lt ja">洛温·斯特梅尔</strong></li><li id="9727" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">道森·斯特梅尔</li></ul><h2 id="c4fc" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq iw bi translated">词汇化:</h2><p id="2a0f" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">词汇化试图实现词干化的目的，但它不是计算机生成的算法，而是基于人类生成的单词字典，并试图产生基于字典的单词。</p><p id="19c1" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">这样往往更准确。</p><p id="80d6" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">比如“<strong class="lt ja">学</strong>”、“<strong class="lt ja">学</strong>”、“<strong class="lt ja">学</strong>”、“<strong class="lt ja">学</strong>”都会转换成“<strong class="lt ja">学</strong>”(这是一个准确的词根)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ow"><img src="../Images/b09b975fe0e4aba3228b7c548690f72f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8tdOzBs8taMbg6olZznSQ.png"/></div></div></figure><p id="8171" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">输出:</p><pre class="kp kq kr ks gt om on oo op aw oq bi"><span id="ff0a" class="kw kx iq on b gy or os l ot ou">Stemmer: studi <br/>Lemmatizer: study</span></pre><h2 id="2b9a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq iw bi translated">词干化与词汇化</h2><p id="8bfd" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">词干化和词元化对于它们的中心用例都是有用的，但是一般来说，如果我们的模型的目标是在没有任何期限的情况下实现更高的准确性，我们更喜欢词元化。但是，如果我们的动机是快速输出，词干是首选。</p><h1 id="4f1b" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">停止言语</h1><p id="5265" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated"><strong class="lt ja">停用词</strong>是我们的文档需要审查的那些词。这些是不相关的词，通常对文本的逻辑意义没有贡献，但有助于语法结构。在将我们的数学模型应用于文本时，这些单词可能会添加大量噪声，从而改变输出。</p><p id="2557" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">停用词通常包括大多数常用词，如“<strong class="lt ja"> a </strong>”、“<strong class="lt ja"> the </strong>”、“中的<strong class="lt ja">”、“<strong class="lt ja">何</strong>”、“<strong class="lt ja">我</strong>”、“<strong class="lt ja">我</strong>”、“<strong class="lt ja">我自己</strong>”。</strong></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="1bf0" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">输出:</p><pre class="kp kq kr ks gt om on oo op aw oq bi"><span id="c82d" class="kw kx iq on b gy or os l ot ou">['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]</span></pre><h1 id="6f23" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">正则表达式</h1><p id="b095" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated"><strong class="lt ja"> Regex </strong>是<strong class="lt ja">正则表达式</strong>的简称，可以定义为一组定义搜索模式的字符串。</p><ul class=""><li id="84e1" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb mc md me mf bi translated"><code class="fe ox oy oz on b">\w</code> -匹配<strong class="lt ja">所有字</strong></li><li id="c41d" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><code class="fe ox oy oz on b">\d</code> -匹配<strong class="lt ja">所有数字</strong></li><li id="61c3" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><code class="fe ox oy oz on b">\W</code>——配<strong class="lt ja">不字</strong></li><li id="f56d" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><code class="fe ox oy oz on b">\D</code> -匹配<strong class="lt ja">而非数字</strong></li><li id="f632" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><code class="fe ox oy oz on b">\S</code> -匹配<strong class="lt ja">而不是空白</strong></li><li id="1529" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><code class="fe ox oy oz on b">[abc]</code> -匹配a、b或c中的任意一个</li><li id="10c5" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><code class="fe ox oy oz on b">[<strong class="lt ja">^</strong>abc]</code> -与a、b或c都不匹配</li><li id="2471" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><code class="fe ox oy oz on b">[a<strong class="lt ja">-</strong>z]</code> -在和&amp; z之间匹配一个字符<strong class="lt ja">，即字母表</strong></li><li id="0454" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><code class="fe ox oy oz on b">[1-100]</code> -在 1 &amp; 100之间匹配一个字符<strong class="lt ja"/></li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="0c0b" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">输出:</p><pre class="kp kq kr ks gt om on oo op aw oq bi"><span id="677a" class="kw kx iq on b gy or os l ot ou">Google is a great search engine that outperforms Yahoo and Bing. It was found in     .</span></pre><h1 id="2f6b" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">一袋单词</h1><p id="3a48" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">机器学习算法大多基于数学计算，它们不能直接处理文本数据。为了使我们的算法与自然语言兼容，我们需要将原始文本数据转换成数字。这种技术被称为特征提取。</p><p id="d45f" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">BoW(<strong class="lt ja">B</strong>ag<strong class="lt ja">o</strong>f<strong class="lt ja">W</strong>ords)是特征提取技术的一个例子，用于定义文本中每个单词的出现。</p><p id="c715" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">这项技术如其名<strong class="lt ja">一样工作，单词存储在没有顺序的袋子里</strong>。这样做的目的是检查输入到我们模型中的单词是否存在于我们的语料库中。</p><p id="3cba" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">例如</p><ol class=""><li id="0c6b" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb pa md me mf bi translated">达克什、拉克沙伊和梅格纳是好朋友。</li><li id="c229" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb pa md me mf bi translated">达克什很酷。</li><li id="2d71" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb pa md me mf bi translated">拉克谢是书呆子。</li><li id="91f2" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb pa md me mf bi translated">Meghna疯了。</li></ol><ul class=""><li id="acaa" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb mc md me mf bi translated"><strong class="lt ja">创建基本结构:</strong></li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/287bab8507a5895d075b81ff65242e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*aVApv3txnWG_EInKYx-MnQ.png"/></div></figure><ul class=""><li id="221c" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb mc md me mf bi translated"><strong class="lt ja">查找每个单词的频率:</strong></li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/dc2fc59b7d4f857d7a35e3b746cc0bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*NbNI4pQ-tcltYx4hFB9urw.png"/></div></figure><ul class=""><li id="0d05" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb mc md me mf bi translated"><strong class="lt ja">结合上一步的输出:</strong></li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/0deb23d8fa861b7cb89167ed59b7a92f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*ibyxlK4O4wVAHq6rxVljpQ.png"/></div></figure><p id="7282" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated"><strong class="lt ja"> d .最终输出:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi pe"><img src="../Images/ccf0fa0de769175b22167216e85e3f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*elSsvsXMz_8hNamKBY9P_A.png"/></div></div></figure><p id="e4ce" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">当我们的输入语料库增加时，词汇量增加，从而增加了向量表示，这导致我们的向量中有许多零，这些向量被称为<strong class="lt ja">稀疏向量</strong>，求解起来更复杂。</p><p id="57ed" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">为了限制向量表示的大小，我们可以使用几种文本清理技术:</p><ul class=""><li id="deea" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb mc md me mf bi translated"><strong class="lt ja">忽略</strong>标点符号。</li><li id="856b" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated"><strong class="lt ja">去掉</strong>停止字。</li><li id="f10c" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">将单词转换成它们的通用形式(<strong class="lt ja">词干化和词条化</strong>)</li><li id="1e02" class="lr ls iq lt b lu mg lw mh lf mi lj mj ln mk mb mc md me mf bi translated">为统一起见，将输入文本转换为小写<strong class="lt ja">字母</strong>。</li></ul><h1 id="5358" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">N-grams</h1><p id="6e3b" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">N-grams是一种创建词汇的强大技术，因此为BoW模型提供了更多功能。n元语法是由“n”个项目组合而成的集合。</p><p id="8b45" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">一个<strong class="lt ja">一元词</strong>是一个词的集合，一个<strong class="lt ja">二元词</strong>是两个词的集合，一个<strong class="lt ja">三元词</strong>带有三个项目，以此类推。它们只包含已经可用的序列，而不是所有可能的序列，因此限制了语料库的大小。</p><p id="4b8a" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated"><strong class="lt ja">例</strong> <br/> <code class="fe ox oy oz on b">He will go to school tomorrow.</code></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/f1635989c30d084e3ac21e8e99445269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*0vpdImRps4JLFCqluKyWBw.png"/></div></figure><h1 id="f0f7" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">TF-IDF</h1><p id="4df5" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated"><strong class="lt ja">T</strong>erm<strong class="lt ja">F</strong>frequency-<strong class="lt ja">I</strong>nverse<strong class="lt ja">D</strong>document<strong class="lt ja">F</strong>frequency(TF-IDF)是一种生成分数的度量，用于定义文档中每个术语的相关性。</p><p id="96ba" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">TF-IDF基于<strong class="lt ja">词频(TF) </strong>和I <strong class="lt ja">倒序文档频率(IDF)的思想。</strong></p><p id="8f60" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">TF表示，如果一个单词被重复多次，这意味着它比其他单词更重要。</p><p id="2513" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">根据<strong class="lt ja"> IDF </strong>的说法，如果同一个出现频率更高的单词在其他文档中出现，那么它的相关性不高。</p><p id="8372" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">TF和IDF的结合为每个单词生成一个分数，帮助我们的机器学习模型从文档中获得精确的高相关性文本。</p><p id="22c4" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">TF-IDF评分是<strong class="lt ja">正比</strong>于该词的出现频率，但它是<strong class="lt ja">反比</strong>于该词在其他文档中的出现频率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi pg"><img src="../Images/271ea46448fe1b8f47e2b32c06a07f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3vUAwjaVSXqQYAsJ.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk translated">文档y中给定术语x的TF-IDF，<a class="ae nr" href="http://filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><ul class=""><li id="353a" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb mc md me mf bi translated"><strong class="lt ja">词频(TF) </strong>:检查词频。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/1f12c4a804a7ab2603c912a4da43b73c.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/0*54vANqsEE5-FbZzn.png"/></div></figure><ul class=""><li id="7fb8" class="lr ls iq lt b lu mu lw mw lf og lj oh ln oi mb mc md me mf bi translated"><strong class="lt ja">逆词频(ITF) </strong>:检查单词的稀有度。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/0f470e65ef182207d6a25056a2f34571.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/0*jZYER2G8CfsSM5vs.png"/></div></figure><p id="b670" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">结合以上公式，我们可以得出结论:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/50015c36f775d1c0cb178657999db38d.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/0*DK1Gld27frUZqWph.png"/></div></figure><p id="53dc" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated"><strong class="lt ja">如果你喜欢这篇文章，请考虑订阅我的简讯:</strong> <a class="ae nr" href="https://mailchi.mp/b535943b5fff/daksh-trehan-weekly-newsletter" rel="noopener ugc nofollow" target="_blank"> <strong class="lt ja">达克什·特雷汉每周简讯</strong> </a> <strong class="lt ja">。</strong></p><h1 id="bb54" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">结论</h1><p id="5113" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">这篇文章帮助我们了解了自然语言处理及其所有基本术语和技术。如果您希望使用<a class="ae nr" rel="noopener ugc nofollow" target="_blank" href="/diving-deep-into-deep-learning-f34497c18f11"> <strong class="lt ja">神经网络</strong> </a>更深入地研究NLP，您可以阅读更多关于<a class="ae nr" href="https://medium.com/towards-artificial-intelligence/recurrent-neural-networks-for-dummies-8d2c4c725fbe" rel="noopener"> <strong class="lt ja">递归神经网络</strong> </a>、<a class="ae nr" href="https://medium.com/towards-artificial-intelligence/understanding-lstms-and-gru-s-b69749acaa35" rel="noopener"><strong class="lt ja">lst ms&amp;GRUs</strong></a>的信息。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h1 id="44ee" class="ns kx iq bd ky nt pk nv lb nw pl ny le kf pm kg li ki pn kj lm kl po km lq oc bi translated">参考资料:</h1><p id="360d" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated">[1] <a class="ae nr" rel="noopener ugc nofollow" target="_blank" href="/nlp-zero-to-hero-with-python-2df6fcebff6e"> NLP —用Python从零到英雄。自然语言处理基础知识学习手册… |作者:Amit Chauhan |走向人工智能</a></p><p id="6e75" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">【2】<a class="ae nr" href="https://monkeylearn.com/natural-language-processing/" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP):它是什么&amp;它是如何工作的？(monkeylearn.com)</a></p><p id="edf0" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">[3] <a class="ae nr" href="https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63" rel="noopener" target="_blank">文本的自然语言处理介绍| Ventsislav Yordanov |走向数据科学</a></p><p id="9852" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">[4] <a class="ae nr" rel="noopener ugc nofollow" target="_blank" href="/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0">使用Python的自然语言处理(NLP)——教程|由走向AI团队|走向AI </a></p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><p id="8bc6" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">请随意连接:</p><blockquote class="pp pq pr"><p id="99d5" class="ms mt nh lt b lu mu ka mv lw mw kd mx ps my mz na pt nb nc nd pu ne nf ng mb ij bi translated">【组合~】<a class="ae nr" href="http://www.dakshtrehan.com/" rel="noopener ugc nofollow" target="_blank"><em class="iq">【https://www.dakshtrehan.com】</em></a></p><p id="2fa1" class="ms mt nh lt b lu mu ka mv lw mw kd mx ps my mz na pt nb nc nd pu ne nf ng mb ij bi translated"><em class="iq">LinkedIn ~</em><a class="ae nr" href="https://www.linkedin.com/in/dakshtrehan/" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://www.linkedin.com/in/dakshtrehan</em></a></p></blockquote><p id="103e" class="pw-post-body-paragraph ms mt iq lt b lu mu ka mv lw mw kd mx lf my mz na lj nb nc nd ln ne nf ng mb ij bi translated">关注更多机器学习/深度学习博客。</p><blockquote class="pp pq pr"><p id="caf0" class="ms mt nh lt b lu mu ka mv lw mw kd mx ps my mz na pt nb nc nd pu ne nf ng mb ij bi translated"><em class="iq">中等~</em><a class="ae nr" href="https://medium.com/@dakshtrehan" rel="noopener"><em class="iq">https://medium.com/@dakshtrehan</em></a></p></blockquote><h1 id="1be8" class="ns kx iq bd ky nt nu nv lb nw nx ny le kf nz kg li ki oa kj lm kl ob km lq oc bi translated">想了解更多？</h1><p id="643e" class="pw-post-body-paragraph ms mt iq lt b lu lv ka mv lw lx kd mx lf od mz na lj oe nc nd ln of nf ng mb ij bi translated"><a class="ae nr" href="https://medium.com/swlh/are-you-ready-to-worship-ai-gods-818c9b7490dc" rel="noopener">准备好拜AI神了吗？</a> <br/> <a class="ae nr" href="https://towardsdatascience.com/detecting-covid-19-using-deep-learning-262956b6f981" rel="noopener" target="_blank">利用深度学习检测新冠肺炎</a> <br/> <a class="ae nr" href="https://towardsdatascience.com/the-inescapable-ai-algorithm-tiktok-ad4c6fd981b8" rel="noopener" target="_blank">无法逃脱的AI算法:抖音</a> <br/> <a class="ae nr" rel="noopener ugc nofollow" target="_blank" href="/gpt-3-explained-to-a-5-year-old-1f3cb9fa030b"> GPT-3向一个5岁的孩子解释。</a> <br/> <a class="ae nr" href="https://medium.com/towards-artificial-intelligence/tinder-ai-a-perfect-matchmaking-b0a7b916e271" rel="noopener"> Tinder+AI:一场完美的牵线搭桥？</a> <br/> <a class="ae nr" href="https://medium.com/towards-artificial-intelligence/an-insiders-guide-to-cartoonization-using-machine-learning-ce3648adfe8" rel="noopener">一个使用机器学习的卡通化内幕指南</a> <br/> <a class="ae nr" href="https://medium.com/towards-artificial-intelligence/reinforcing-the-science-behind-reinforcement-learning-d2643ca39b51" rel="noopener">强化强化学习背后的科学</a> <br/> <a class="ae nr" href="https://medium.com/towards-artificial-intelligence/decoding-science-behind-generative-adversarial-networks-4d188a67d863" rel="noopener">解码生成性对抗网络背后的科学</a> <br/> <a class="ae nr" href="https://medium.com/towards-artificial-intelligence/understanding-lstms-and-gru-s-b69749acaa35" rel="noopener">了解LSTM和GRU的</a> <br/> <a class="ae nr" href="https://medium.com/towards-artificial-intelligence/recurrent-neural-networks-for-dummies-8d2c4c725fbe" rel="noopener">用于假人的递归神经网络</a> <br/> <a class="ae nr" href="https://medium.com/towards-artificial-intelligence/convolutional-neural-networks-for-dummies-afd7166cd9e" rel="noopener">用于假人的卷积神经网络</a></p><blockquote class="pp pq pr"><p id="7ab0" class="ms mt nh lt b lu mu ka mv lw mw kd mx ps my mz na pt nb nc nd pu ne nf ng mb ij bi translated"><em class="iq">欢呼</em></p></blockquote></div></div>    
</body>
</html>