<html>
<head>
<title>Explain Your Machine Learning Predictions With Kernel SHAP (Kernel Explainer)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用内核SHAP解释你的机器学习预测</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/explain-your-machine-learning-predictions-with-kernel-shap-kernel-explainer-fed56b9250b8?source=collection_archive---------0-----------------------#2020-12-05">https://pub.towardsai.net/explain-your-machine-learning-predictions-with-kernel-shap-kernel-explainer-fed56b9250b8?source=collection_archive---------0-----------------------#2020-12-05</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="2052" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="cd12" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">如何使用SHAP库用内核解释器解释你的机器学习预测</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/88fedf1b90b89291ea6e553e249f14f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nKeRrYJssPzJzTiF.png"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">来源:<a class="ae li" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> SHAP </a></figcaption></figure></div><div class="ab cl lj lk hy ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="in io ip iq ir"><h1 id="de6c" class="lq lr iu bd ls lt lu lv lw lx ly lz ma kj mb kk mc km md kn me kp mf kq mg mh bi translated">什么是SHAP</h1><p id="56df" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">正如在<a class="ae li" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> Github </a>页面— <em class="ne">上所述，“SHAP(SHapley Additive explaints)是一种解释任何机器学习模型输出的博弈论方法。它使用博弈论的经典Shapley值及其相关扩展将最优信用分配与本地解释联系起来。</em></p><p id="04bc" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">在上一篇文章<a class="ae li" href="https://medium.com/towards-artificial-intelligence/how-to-explain-your-machine-learning-predictions-with-shap-values-a8332c3e5a11" rel="noopener"> <strong class="mk je">这里</strong> </a>中，我们谈到了针对波士顿房价预测数据集的<code class="fe nk nl nm nn b">TreeExplainer</code>(它使用了树SHAP算法)。在这篇博客中，让我们了解如何使用<code class="fe nk nl nm nn b">KernelExplainer</code>(核SHAP算法)进行模型解释。</p><h1 id="c2c7" class="lq lr iu bd ls lt no lv lw lx np lz ma kj nq kk mc km nr kn me kp ns kq mg mh bi translated">内核SHAP(线性石灰+沙普利值)</h1><p id="1395" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">核SHAP是一种模型不可知的方法，用于估计任何机器学习模型的SHAP值。正如您从标题中可以猜到的，核SHAP算法基于两个组件-局部代理(LIME)和Shapley值。它是在SHAP库中的<strong class="mk je"> KernelExplainer </strong>方法中实现的。</p><div class="nt nu gq gs nv nw"><a href="https://medium.com/towards-artificial-intelligence/how-to-explain-your-machine-learning-predictions-with-shap-values-a8332c3e5a11" rel="noopener follow" target="_blank"><div class="nx ab fp"><div class="ny ab nz cl cj oa"><h2 class="bd je gz z fq ob fs ft oc fv fx jd bi translated">如何用SHAP值解释你的机器学习预测</h2><div class="od l"><h3 class="bd b gz z fq ob fs ft oc fv fx dk translated">SHapley加法解释</h3></div><div class="oe l"><p class="bd b dl z fq ob fs ft oc fv fx dk translated">medium.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok lc nw"/></div></div></a></div><p id="c767" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">请阅读下面的章节，了解内核SHAP背后的详细讨论和数学知识。</p><div class="nt nu gq gs nv nw"><a href="https://christophm.github.io/interpretable-ml-book/shap.html#kernelshap" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fp"><div class="ny ab nz cl cj oa"><h2 class="bd je gz z fq ob fs ft oc fv fx jd bi translated">5.10 SHAP (SHapley附加解释)|可解释的机器学习</h2><div class="od l"><h3 class="bd b gz z fq ob fs ft oc fv fx dk translated">本章目前仅在此网络版本中可用。电子书和印刷品将紧随其后。SHAP(沙普利添加剂…</h3></div><div class="oe l"><p class="bd b dl z fq ob fs ft oc fv fx dk translated">christophm.github.io</p></div></div><div class="of l"><div class="ol l oh oi oj of ok lc nw"/></div></div></a></div><h1 id="ed9b" class="lq lr iu bd ls lt no lv lw lx np lz ma kj nq kk mc km nr kn me kp ns kq mg mh bi translated">例子</h1><p id="5198" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">在本示例部分，让我们使用sklearn解决一些葡萄酒质量数据集的回归问题。</p><pre class="kt ku kv kw gu om nn on oo aw op bi"><span id="2d5e" class="oq lr iu nn b gz or os l ot ou">import sklearn<br/>import pandas as pd<br/>import numpy as np<br/>import shap<br/>import time<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_squared_errorfrom sklearn.linear_model import LinearRegression</span><span id="f69f" class="oq lr iu nn b gz ov os l ot ou">dataset_url = ‘https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'</span><span id="9c24" class="oq lr iu nn b gz ov os l ot ou">df = pd.read_csv(dataset_url, sep=’;’)</span><span id="43e2" class="oq lr iu nn b gz ov os l ot ou">y = df[‘quality’]<br/>X = df[[‘fixed acidity’, ‘volatile acidity’, ‘citric acid’, ‘residual sugar’,’chlorides’, ‘free sulfur dioxide’, ‘total sulfur dioxide’, ‘density’,’pH’, ‘sulphates’, ‘alcohol’]]</span><span id="bb13" class="oq lr iu nn b gz ov os l ot ou"># Split the data into train and test data:<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)</span><span id="0bf3" class="oq lr iu nn b gz ov os l ot ou"># Build the model with the random forest regression algorithm:<br/>lin_regr = LinearRegression()<br/>lin_regr.fit(X_train, y_train)<br/>y_pred = lin_regr.predict(X_test)</span><span id="22bc" class="oq lr iu nn b gz ov os l ot ou">print(‘RMSE:’, np.sqrt(mean_squared_error(y_test, y_pred)))</span><span id="9783" class="oq lr iu nn b gz ov os l ot ou"># rather than use the whole training set to estimate expected <br/># values, we summarize with a set of weighted kmeans, each weighted # by the number of points they represent.<br/>X_train_summary = shap.kmeans(X_train, 10)</span></pre><p id="3db7" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">KernelExplainer方法需要三个参数— <code class="fe nk nl nm nn b">model</code>、<code class="fe nk nl nm nn b">background dataset</code>、&amp;、<code class="fe nk nl nm nn b">link</code>。在下面的代码中，我们将传递一个经过训练的线性回归模型。背景数据集用于生成扰动数据集。如果背景数据集很小，则使用整个数据；否则可以使用如上所示的汇总数据集。对于分类问题，我们可以将参数<code class="fe nk nl nm nn b">link</code>设置为‘logit ’,以获得logit中的特征贡献。</p><pre class="kt ku kv kw gu om nn on oo aw op bi"><span id="17b5" class="oq lr iu nn b gz or os l ot ou">shap.initjs()<br/>ex = shap.KernelExplainer(lin_regr.predict, X_train_summary)<br/>shap_values = ex.shap_values(X_test.iloc[0,:])</span></pre><h2 id="75dd" class="oq lr iu bd ls ow ox dn lw oy oz dp ma mr pa pb mc mv pc pd me mz pe pf mg ja bi translated">力图</h2><p id="301a" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">力图用于解释个别情况的预测。以下示例显示了测试数据集中第一个实例的力图。</p><pre class="kt ku kv kw gu om nn on oo aw op bi"><span id="6d19" class="oq lr iu nn b gz or os l ot ou">shap.force_plot(ex.expected_value, shap_values, X_test.iloc[0,:])</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pg"><img src="../Images/c90eeccb196e6cb67b6d13820a8c7bea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J1kELIUkUdWND63Mhkwhdg.png"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">作者图片</figcaption></figure><ul class=""><li id="4cdb" class="ph pi iu mk b ml nf mo ng mr pj mv pk mz pl nd pm pn po pp bi translated"><code class="fe nk nl nm nn b">f(x)</code>是模型预测(5.35)。</li><li id="0596" class="ph pi iu mk b ml pq mo pr mr ps mv pt mz pu nd pm pn po pp bi translated"><code class="fe nk nl nm nn b">base value</code>是整个测试数据集的平均预测值。这是在我们不知道当前输出的任何特征的情况下预测的值。</li><li id="bc7f" class="ph pi iu mk b ml pq mo pr mr ps mv pt mz pu nd pm pn po pp bi translated">将预测值推高的要素显示为红色，将预测值推低的要素显示为蓝色。</li><li id="96fa" class="ph pi iu mk b ml pq mo pr mr ps mv pt mz pu nd pm pn po pp bi translated"><code class="fe nk nl nm nn b">alcohol</code>特征对葡萄酒质量有很高的负面影响，并将预测向左推。将葡萄酒质量推向较低值的其他重要特征是<code class="fe nk nl nm nn b">chlorides, volatile acidity </code> &amp; <code class="fe nk nl nm nn b">fixed acidity</code>。</li><li id="760f" class="ph pi iu mk b ml pq mo pr mr ps mv pt mz pu nd pm pn po pp bi translated"><code class="fe nk nl nm nn b">pH</code>特征对葡萄酒质量有积极影响，其次是<code class="fe nk nl nm nn b">citric acid</code>特征。</li></ul><h2 id="6a13" class="oq lr iu bd ls ow ox dn lw oy oz dp ma mr pa pb mc mv pc pd me mz pe pf mg ja bi translated">汇总图</h2><p id="668f" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">摘要图用于找出对模型最重要的特征。在下面的示例中，我们绘制了每个样本的每个要素的SHAP值。然后，根据所有样本的SHAP值的总和对该图进行排序。</p><pre class="kt ku kv kw gu om nn on oo aw op bi"><span id="c499" class="oq lr iu nn b gz or os l ot ou">shap_values = ex.shap_values(X_test)<br/>shap.summary_plot(shap_values, X_test)</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pv"><img src="../Images/051fbd80826b1c4cb6e738acd2171ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*IIvHCZCLx0OnDZT7HIfuJw.png"/></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">作者图片</figcaption></figure><ul class=""><li id="f786" class="ph pi iu mk b ml nf mo ng mr pj mv pk mz pl nd pm pn po pp bi translated">特征按<code class="fe nk nl nm nn b">feature importance</code>降序排列。</li><li id="676d" class="ph pi iu mk b ml pq mo pr mr ps mv pt mz pu nd pm pn po pp bi translated"><code class="fe nk nl nm nn b">Color</code>表示该变量在该观察中是高(红色)还是低(蓝色)。</li><li id="4ea9" class="ph pi iu mk b ml pq mo pr mr ps mv pt mz pu nd pm pn po pp bi translated">每个特征的水平线上的每个点显示该值<em class="ne">的影响是否与更高(红色)或更低(蓝色)的预测</em>相关联。</li><li id="7e96" class="ph pi iu mk b ml pq mo pr mr ps mv pt mz pu nd pm pn po pp bi translated">我们还可以看到每个特征和目标变量之间的<code class="fe nk nl nm nn b">correlation</code>。高水平的“酒精”对葡萄酒质量有着积极的影响。请注意，“高”来自红色，而“积极”影响显示在X轴上。同样，我们可以说“挥发性酸度”的高值与目标变量负相关。</li></ul><p id="d807" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">带<code class="fe nk nl nm nn b">plot_type='bar'</code>的汇总图将给出可变重要性图。预测能力高的特征显示在顶部，预测能力低的特征显示在底部。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pv"><img src="../Images/b25182e8ee05fe0a879f342232d91209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*_Yp7hMSkbf7xBOe7izchew.png"/></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">作者图片</figcaption></figure><h2 id="4a0a" class="oq lr iu bd ls ow ox dn lw oy oz dp ma mr pa pb mc mv pc pd me mz pe pf mg ja bi translated">依赖图</h2><p id="07ab" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">为了理解单个特征如何影响模型的输出，可以使用依赖图。部分相关性图显示了一个或两个特征对机器学习模型的预测结果的边际影响。它表明目标和特征之间的关系是线性的、单调的还是更复杂的。它会自动包含您选择的变量与之交互最多的另一个变量。</p><p id="a1d8" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">在下图中，<code class="fe nk nl nm nn b">alcohol</code>特征与<code class="fe nk nl nm nn b">sulphates</code>特征交互最多，我们可以看到<code class="fe nk nl nm nn b">alcohol</code>与目标变量之间的线性趋势。</p><pre class="kt ku kv kw gu om nn on oo aw op bi"><span id="9acb" class="oq lr iu nn b gz or os l ot ou">shap.dependence_plot(“alcohol”, shap_values, X_test)</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pw"><img src="../Images/43b22719a108ace21d62333c1a4fb6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*gAQKCOyilv_P1wqsta1MzQ.png"/></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">作者图片</figcaption></figure><p id="02ab" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">请浏览更多官方文档中的<a class="ae li" href="https://shap.readthedocs.io/en/latest/examples.html#kernel-explainer" rel="noopener ugc nofollow" target="_blank"> <strong class="mk je">内核解释器示例</strong> </a>以及分类示例。</p><h2 id="a0fc" class="oq lr iu bd ls ow ox dn lw oy oz dp ma mr pa pb mc mv pc pd me mz pe pf mg ja bi translated">密码</h2><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="px py l"/></div></figure><h1 id="07a3" class="lq lr iu bd ls lt no lv lw lx np lz ma kj nq kk mc km nr kn me kp ns kq mg mh bi translated">结论</h1><p id="2c27" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">在本文中，您已经了解了线性时间、Shapley值，以及这两者与内核SHAP的关系。您还了解了如何使用Kernel Explainer实现模型可解释性的例子。在以后的博文中，我会尽量涵盖来自SHAP图书馆的<code class="fe nk nl nm nn b">Deep Explainer</code>和<code class="fe nk nl nm nn b">Linear Explainer</code>。</p><p id="96ce" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">如果你正在读这篇文章，那么我相信你也会对下面关于同一主题的文章感兴趣。</p><div class="nt nu gq gs nv nw"><a href="https://medium.com/towards-artificial-intelligence/how-to-explain-your-machine-learning-predictions-with-shap-values-a8332c3e5a11" rel="noopener follow" target="_blank"><div class="nx ab fp"><div class="ny ab nz cl cj oa"><h2 class="bd je gz z fq ob fs ft oc fv fx jd bi translated">如何用SHAP值解释你的机器学习预测</h2><div class="od l"><h3 class="bd b gz z fq ob fs ft oc fv fx dk translated">SHapley加法解释</h3></div><div class="oe l"><p class="bd b dl z fq ob fs ft oc fv fx dk translated">medium.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok lc nw"/></div></div></a></div><div class="nt nu gq gs nv nw"><a href="https://medium.com/towards-artificial-intelligence/lime-explaining-any-machine-learning-prediction-d663c457a740" rel="noopener follow" target="_blank"><div class="nx ab fp"><div class="ny ab nz cl cj oa"><h2 class="bd je gz z fq ob fs ft oc fv fx jd bi translated">LIME解释任何机器学习预测</h2><div class="od l"><h3 class="bd b gz z fq ob fs ft oc fv fx dk translated">用石灰向可解释的人工智能迈出第一步</h3></div><div class="oe l"><p class="bd b dl z fq ob fs ft oc fv fx dk translated">medium.com</p></div></div><div class="of l"><div class="pz l oh oi oj of ok lc nw"/></div></div></a></div><p id="6102" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated"><em class="ne">阅读更多关于Python和数据科学的此类有趣文章，</em> <a class="ae li" href="https://pythonsimplified.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="mk je"> <em class="ne">订阅</em> </strong> </a> <em class="ne">到我的博客</em><a class="ae li" href="http://www.pythonsimplified.com" rel="noopener ugc nofollow" target="_blank"><strong class="mk je">【www.pythonsimplified.com】</strong></a><strong class="mk je"><em class="ne">。</em> </strong>你也可以通过<a class="ae li" href="https://www.linkedin.com/in/chetanambi/" rel="noopener ugc nofollow" target="_blank"> <strong class="mk je"> LinkedIn </strong> </a>联系我。</p><p id="530d" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">我希望你喜欢阅读这篇文章。如果你喜欢我的文章并想订阅Medium，你可以在这里订阅: </p><div class="nt nu gq gs nv nw"><a href="https://chetanambi.medium.com" rel="noopener follow" target="_blank"><div class="nx ab fp"><div class="ny ab nz cl cj oa"><h2 class="bd je gz z fq ob fs ft oc fv fx jd bi translated">Chetan Ambi -介质</h2><div class="od l"><h3 class="bd b gz z fq ob fs ft oc fv fx dk translated">阅读Chetan Ambi在媒体上的文章。数据科学|机器学习| Python。参观https://pythonsimplified.com/…</h3></div><div class="oe l"><p class="bd b dl z fq ob fs ft oc fv fx dk translated">chetanambi.medium.com</p></div></div><div class="of l"><div class="qa l oh oi oj of ok lc nw"/></div></div></a></div></div><div class="ab cl lj lk hy ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="in io ip iq ir"><h1 id="f496" class="lq lr iu bd ls lt lu lv lw lx ly lz ma kj mb kk mc km md kn me kp mf kq mg mh bi translated">参考</h1><p id="74c6" class="pw-post-body-paragraph mi mj iu mk b ml mm ke mn mo mp kh mq mr ms mt mu mv mw mx my mz na nb nc nd in bi translated">[1].<a class="ae li" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">https://github.com/slundberg/shap</a></p><p id="234e" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">[2].<a class="ae li" href="https://medium.com/analytics-vidhya/shap-part-2-kernel-shap-3c11e7a971b1" rel="noopener">https://medium . com/analytics-vid hya/shap-part-2-kernel-shap-3c 11e 7a 971 b 1</a></p><p id="b554" class="pw-post-body-paragraph mi mj iu mk b ml nf ke mn mo ng kh mq mr nh mt mu mv ni mx my mz nj nb nc nd in bi translated">[3].<a class="ae li" href="https://towardsdatascience.com/understanding-how-lime-explains-predictions-d404e5d1829c" rel="noopener" target="_blank">https://towards data science . com/understanding-how-lime-explains-predictions-d 404 e 5d 1829 c</a></p></div></div>    
</body>
</html>