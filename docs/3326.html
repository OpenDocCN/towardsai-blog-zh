<html>
<head>
<title>Transformers for Multi-Regression — [PART2]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多元回归的变形金刚—[第二部分]</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/transformers-for-multi-regression-task-part2-fine-tuning-2683ef134d1c?source=collection_archive---------4-----------------------#2022-11-18">https://pub.towardsai.net/transformers-for-multi-regression-task-part2-fine-tuning-2683ef134d1c?source=collection_archive---------4-----------------------#2022-11-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="ad93" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🤖微调🤖</h1><p id="1d8d" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在<a class="ae lj" href="https://www.kaggle.com/competitions/feedback-prize-english-language-learning" rel="noopener ugc nofollow" target="_blank"> FB3竞赛</a>的背景下，我们的目标是使用8-12年级英语学习者写的预先评分的议论文建立六个分析指标模型。我们必须模拟的技能如下:<strong class="kn ir">衔接、句法、词汇、措辞、语法</strong>和<strong class="kn ir">约定</strong>。分数范围从1.0到5.0，增量为0.5。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/57ecb0c8dc8d8c9903e0758d96e20555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tGODcZqHO9H7KjtKldraVw.jpeg"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">致敬马赛尔·普鲁斯特:@ <a class="ae lj" href="https://fr.dreamstime.com/marcel-proust-auteur-fran%C3%A7ais-illustration-vecteur-image131669085" rel="noopener ugc nofollow" target="_blank">马里奥·布雷达</a></figcaption></figure><p id="696e" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">在我的上一篇文章中，我向您展示了如何使用一个预先训练好的转换器来提取上下文捕获嵌入，并使用它们来训练一个多元回归器。</p><p id="bbd7" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">这次我将向您展示如何对整个变压器进行端到端的训练，这也意味着更新预训练模型的参数。</p><p id="ef98" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">此外，我将向您展示如何使用weights and biases平台:从使用<strong class="kn ir"> wandb </strong> API登录，到创建和使用模型工件以及通过模型跟踪。</p><p id="04cd" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">所有代码源都可以从<a class="ae lj" href="https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners" rel="noopener ugc nofollow" target="_blank">我的Kaggle笔记本</a>中检索到</p><p id="45a1" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">信用</strong>:这部分我借用了<a class="ae lj" href="https://www.kaggle.com/code/debarshichanda/fb3-custom-hf-trainer-w-b-starter#notebook-container" rel="noopener ugc nofollow" target="_blank"> @debarshichanda </a>模型的架构。</p><h1 id="add2" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🎛Imports和配置</h1><p id="9a23" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">首先，我们将定义<code class="fe mf mg mh mi b">CONFIG</code>字典和与transformer相关的导入，我们将在整个项目中使用它们:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="5c5d" class="mn jo iq mi b be mo mp l mq mr">import torch<br/>import torch.nn as nn<br/>import transformers<br/>from transformers import (<br/>    AutoModel, AutoConfig, <br/>    AutoTokenizer, logging,<br/>    AdamW, get_linear_schedule_with_warmup,<br/>    DataCollatorWithPadding,<br/>    Trainer, TrainingArguments<br/>)<br/>from transformers.modeling_outputs import SequenceClassifierOutput<br/><br/>logging.set_verbosity_error()<br/>logging.set_verbosity_warning()<br/><br/>CONFIG = {<br/>    "model_name": "microsoft/deberta-v3-base",# "distilbert-base-uncased",<br/>    "device": 'cuda' if torch.cuda.is_available() else 'cpu',<br/>    "dropout": random.uniform(0.01, 0.60),<br/>    "max_length": 512,<br/>    "train_batch_size": 8,<br/>    "valid_batch_size": 16,<br/>    "epochs": 10,<br/>    "folds" : 3,<br/>    "max_grad_norm": 1000,<br/>    "weight_decay": 1e-6,<br/>    "learning_rate": 1e-5,<br/>     "loss_type": "rmse",<br/>    "n_accumulate" : 1,<br/>    "label_cols" : ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'], <br/>    <br/>}</span></pre><h1 id="6f17" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🧮Custom数据集迭代器:</h1><p id="3697" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如前一篇文章所解释的，我们将定义一个<code class="fe mf mg mh mi b">torch.utils.data.Dataset</code>的子类，并覆盖<code class="fe mf mg mh mi b">__init__</code>、<code class="fe mf mg mh mi b">__len__</code>和<code class="fe mf mg mh mi b">__getitem__</code>特殊方法，如下所示:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="7e48" class="mn jo iq mi b be mo mp l mq mr">import pandas as pd<br/><br/>train = pd.read_csv(PATH_TO_TRAIN)<br/>test = pd.read_csv(PATH_TO_TEST)<br/><br/># lets define the batch genetator<br/>class CustomIterator(torch.utils.data.Dataset):<br/>    def __init__(self, df, tokenizer, labels=CONFIG['label_cols'], is_train=True):<br/>        self.df = df<br/>        self.tokenizer = tokenizer<br/>        self.max_seq_length = CONFIG["max_length"]# tokenizer.model_max_length<br/>        self.labels = labels<br/>        self.is_train = is_train<br/>        <br/>    def __getitem__(self,idx):<br/>        tokens = self.tokenizer(<br/>                    self.df.loc[idx, 'full_text'],#.to_list(),<br/>                    add_special_tokens=True,<br/>                    padding='max_length',<br/>                    max_length=self.max_seq_length,<br/>                    truncation=True,<br/>                    return_tensors='pt',<br/>                    return_attention_mask=True<br/>                )     <br/>        res = {<br/>            'input_ids': tokens['input_ids'].to(CONFIG.get('device')).squeeze(),<br/>            'attention_mask': tokens['attention_mask'].to(CONFIG.get('device')).squeeze()<br/>        }<br/>        <br/>        if self.is_train:<br/>            res["labels"] = torch.tensor(<br/>                self.df.loc[idx, self.labels].to_list(), <br/>            ).to(CONFIG.get('device')) <br/>            <br/>        return res<br/>    <br/>    def __len__(self):<br/>        return len(self.df)</span></pre><h1 id="d264" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🤖微调变压器</h1><p id="6572" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">使用这种方法，隐藏状态不是固定的，而是可训练的:为此，它要求分类头是<strong class="kn ir">可微分的。通常，我们使用神经网络作为分类器。</strong></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ms"><img src="../Images/876fd3f755637417b3a6c3f5f849c015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cHRLl9_JJ4yDCxNI"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk translated">Zeineb Ghrib </figcaption></figure><p id="4e8f" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">在本节中，我们将看到如何使用HuggingFace提供的简单且功能完整的训练和评估API:<code class="fe mf mg mh mi b">Trainer</code>基于<code class="fe mf mg mh mi b">microsoft/deberta-v3-base</code>预训练模型微调编码器变压器。</p><p id="2d45" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">我们将定义一个自定义模型，用一个可训练的神经网络头来扩展<code class="fe mf mg mh mi b">microsoft/deberta-v3-base</code>。</p><p id="d9d0" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">定制模型将包括:</p><ul class=""><li id="5788" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><strong class="kn ir">预训练基线模型</strong>:加载带有<code class="fe mf mg mh mi b">AutoModel.from_pretrained</code>功能的预训练<code class="fe mf mg mh mi b">microsoft/deberta-v3-base</code></li><li id="3fd2" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">平均池层</strong>:我们需要对之前的平均池函数做一些修改(参见<a class="ae lj" href="https://medium.com/@zghrib/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9" rel="noopener">第一部分</a>帖子):从<code class="fe mf mg mh mi b">torch.nn.Module</code>继承池类，并在<code class="fe mf mg mh mi b">forward</code>方法中定义平均池函数(参见下面的代码)</li><li id="650c" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">漏失层</strong>:添加一个漏失层进行正则化</li><li id="b68c" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">线性图层</strong>:输入尺寸= <code class="fe mf mg mh mi b">hidden_state_dim</code>，输出尺寸=目标特征数量(6)</li></ul><p id="7943" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">线性层的logits输出通过<code class="fe mf mg mh mi b">forward</code>方法上的<code class="fe mf mg mh mi b">SequenceClassifierOutput</code>-<code class="fe mf mg mh mi b">ModelOutput</code>类的子类返回(所有模型的输出必须是<code class="fe mf mg mh mi b">ModelOutput</code>子类的实例:<a class="ae lj" href="https://huggingface.co/docs/transformers/main_classes/output" rel="noopener ugc nofollow" target="_blank">引用此处为</a>)</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="757c" class="mn jo iq mi b be mo mp l mq mr">class MeanPooling(nn.Module):<br/>    def __init__(self):<br/>        super(MeanPooling, self).__init__()  <br/>    def forward(self, last_hidden_state, attention_mask):<br/>        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()<br/>        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)<br/>        sum_mask = input_mask_expanded.sum(1)<br/>        sum_mask = torch.clamp(sum_mask, min=1e-9)<br/>        mean_embeddings = sum_embeddings / sum_mask<br/>        return mean_embeddings<br/><br/>class FeedBackModel(nn.Module):<br/>    def __init__(self, model_name):<br/>        super(FeedBackModel, self).__init__()<br/>        self.config = AutoConfig.from_pretrained(model_name)<br/>        self.config.hidden_dropout_prob = 0<br/>        self.config.attention_probs_dropout_prob = 0<br/>        self.model = AutoModel.from_pretrained(model_name, config=self.config)<br/>        self.drop = nn.Dropout(p=0.2)<br/>        self.pooler = MeanPooling()<br/>        self.fc = nn.Linear(self.config.hidden_size, len(CONFIG['label_cols']))<br/>        <br/>    def forward(self, input_ids, attention_mask):<br/>        out = self.model(input_ids=input_ids,<br/>                         attention_mask=attention_mask, <br/>                         output_hidden_states=False)<br/>        out = self.pooler(out.last_hidden_state, attention_mask)<br/>        out = self.drop(out)<br/>        outputs = self.fc(out)<br/>        return SequenceClassifierOutput(logits=outputs)</span></pre><h2 id="9ce1" class="nj jo iq bd jp nk nl dn jt nm nn dp jx kw no np kb la nq nr kf le ns nt kj nu bi translated">损失和度量</h2><p id="14b8" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">由于我们将使用<code class="fe mf mg mh mi b">Trainer</code>，我们需要定义一个对应于目标评估指标的新损失函数(在我们的例子中为MCRMSE)。该损失函数将用于训练变压器。实现的方法是定义一个<code class="fe mf mg mh mi b">subclassing Trainer</code>并覆盖<code class="fe mf mg mh mi b">compute_loss()</code>方法。</p><p id="a313" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">同样，我们希望在评估步骤中获得每个目标类的局部评估，因此我们将为<code class="fe mf mg mh mi b">Trainer</code>提供一个自定义的<code class="fe mf mg mh mi b">compute_metrics()</code>函数，该函数允许计算六个目标中每个目标的RMSE(否则，评估将只返回损失评估MCRMSE)。</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="50c6" class="mn jo iq mi b be mo mp l mq mr">class RMSELoss(nn.Module):<br/>    """<br/>    Code taken from Y Nakama's notebook (https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)<br/>    """<br/>    def __init__(self, reduction='mean', eps=1e-9):<br/>        super().__init__()<br/>        self.mse = nn.MSELoss(reduction='none')<br/>        self.reduction = reduction<br/>        self.eps = eps<br/><br/>    def forward(self, predictions, targets):<br/>        loss = torch.sqrt(self.mse(predictions, targets) + self.eps)<br/>        if self.reduction == 'none':<br/>            loss = loss<br/>        elif self.reduction == 'sum':<br/>            loss = loss.sum()<br/>        elif self.reduction == 'mean':<br/>            loss = loss.mean()<br/>        return loss<br/><br/>class CustomTrainer(Trainer):<br/>    def compute_loss(self, model, inputs, return_outputs=False):<br/>        outputs = model(inputs['input_ids'], inputs['attention_mask'])<br/>        loss_func = RMSELoss(reduction='mean')<br/>        loss = loss_func(outputs.logits.float(), inputs['labels'].float())<br/>        return (loss, outputs) if return_outputs else loss<br/><br/>def compute_metrics(eval_pred):<br/>    predictions, labels = eval_pred<br/>    colwise_rmse = np.sqrt(np.mean((labels - predictions) ** 2, axis=0))<br/>    res = {<br/>        f"{analytic.upper()}_RMSE" : colwise_rmse[i]<br/>        for i, analytic in enumerate(CONFIG["label_cols"])<br/>    }<br/>    res["MCRMSE"] = np.mean(colwise_rmse)<br/>    return res</span></pre><h1 id="9297" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🧚Weights和Biases🧚</h1><p id="4d7b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">尽管HuggingFace Transformers提供了广泛的训练检查点功能。W &amp; B 提供强大的实验跟踪和模型版本化工具，带有友好的交互式仪表盘。每个实验项目都是独立划分的。</p><p id="e567" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">查看<a class="ae lj" href="https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases#%F0%9F%96%A5-Dashboard-(experiment-tracking" rel="noopener ugc nofollow" target="_blank">这款出色的笔记本</a>，它详细描述了如何在kaggle中使用W &amp; B:</p><blockquote class="nv nw nx"><p id="8553" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated"><em class="iq">W&amp;B提供了两个主要的实用程序:</em></p><p id="0370" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated">🤙<strong class="kn ir">仪表板</strong>(实验跟踪):实时记录和可视化实验=将数据和结果保存在一个方便的地方。把这看作是一个实验的宝库。</p><p id="7cc0" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated">🤙<strong class="kn ir">工件</strong>(数据集+模型版本化):存储和版本化数据集、模型和结果=确切地知道模型被训练的数据。</p></blockquote><p id="f1b9" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">为了连接到Weights &amp; Biases，我们需要从<a class="ae lj" href="https://wandb.ai/authorize" rel="noopener ugc nofollow" target="_blank">https://wandb.ai/authorize</a>访问您的API密钥。<br/>有两种方法可以使用Kaggle内核登录:</p><ul class=""><li id="7783" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated">运行<code class="fe mf mg mh mi b">wandb.login(key=your-api-key)</code> cmd:它会要求API key:你可以从<a class="ae lj" href="https://wandb.ai/authorize" rel="noopener ugc nofollow" target="_blank">https://wandb.ai/authorize</a>复制/粘贴。</li></ul><p id="f8ad" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><em class="ny">如果你不使用Kaggle </em>，可以跳过这一部分</p><ul class=""><li id="b83c" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated">使用Kaggle secrets存储您的API密钥:并使用下面的代码片段登录。</li></ul><ol class=""><li id="7453" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li oc nb nc nd bi translated">点击笔记本编辑器中的<code class="fe mf mg mh mi b">Add-ons</code>菜单，然后点击<code class="fe mf mg mh mi b">Secrets</code>:</li></ol><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi od"><img src="../Images/10c8db78e76639fdd664e0949ea47fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/0*XibclEUc-hrZaCIw"/></div></figure><p id="e7da" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">2.将api-key存储为将附加到当前笔记本的键值对:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oe"><img src="../Images/257fe84457dc82921c33891157d60fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1J-jerDNO6RSpzus"/></div></div></figure><p id="a255" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">3.复制并粘贴代码片段以访问api-key，然后使用<code class="fe mf mg mh mi b">wandb.login()</code>连接到W &amp; B:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="f43c" class="mn jo iq mi b be mo mp l mq mr">from kaggle_secrets import UserSecretsClient<br/>import wandb<br/><br/>user_secrets = UserSecretsClient()<br/>api_key = user_secrets.get_secret("wandb_api")<br/>wandb.login(key=api_key)</span></pre><h1 id="6a0c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🛠Wandb论点</h1><p id="0ad9" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对于每个CV迭代‘I ’,我们将创建一个名为<strong class="kn ir"> FB3-fold-i </strong>的新运行，其中‘I ’=迭代的编号，在一个名为<strong class="kn ir"> Feedback3-deberta </strong>的项目中。</p><p id="c9ca" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">一些其他参数:</p><ul class=""><li id="da7d" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><code class="fe mf mg mh mi b">group</code>:群组参数特别用于将单个实验组织成一个更大的实验，<a class="ae lj" href="https://docs.wandb.ai/guides/track/advanced/grouping" rel="noopener ugc nofollow" target="_blank">下面是一些用例示例</a></li><li id="3d9f" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">tags</code>:我们将添加型号名称和公制标签。正如W &amp; B中所解释的，doc标签对于组织一起运行或应用临时标签如“基线”或“生产”是有用的。很容易在UI中添加和删除标签，或者过滤到只运行特定的标签。</li><li id="a077" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated">通常不是“训练”就是“评估”。稍后，它将允许对相似的运行进行过滤和分组。我们将把job_type设置为<strong class="kn ir">“train”</strong></li><li id="7b87" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">anonymous</code>:该参数允许控制匿名记录。我们将把它设置为<strong class="kn ir">“must”</strong>，这将把跑步发送到一个匿名帐户，而不是一个注册用户帐户。对于其他选项，您可以查看<a class="ae lj" href="https://docs.wandb.ai/ref/python/init" rel="noopener ugc nofollow" target="_blank">文档</a></li></ul><p id="aa74" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">对于每个CV迭代，我们可以如下实例化一个wandb运行:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="2e94" class="mn jo iq mi b be mo mp l mq mr">run = wandb.init(project="FB3-deberta-v3", <br/>                   config=CONFIG,<br/>                   job_type='train',<br/>                   group="FB3-BASELINE-MODEL",<br/>                   tags=[CONFIG['model_name'], CONFIG['loss_type'], "10-epochs"],<br/>                   name=f'FB3-fold-{fold}',<br/>                   anonymous='must')</span></pre><p id="fdff" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">现在让我们定义拥抱脸<code class="fe mf mg mh mi b">Trainer</code>将使用的训练参数</p><h1 id="4f4c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🛠Training论点</h1><p id="dc95" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在实例化我们的定制教练之前，我们将创建一个<code class="fe mf mg mh mi b">TrainingArguments</code>来定义训练配置。<br/>我们将设置以下参数:</p><ul class=""><li id="6f47" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><code class="fe mf mg mh mi b">output_dir</code>:模型预测和检查点将被写入的输出目录:每个CV迭代将有它自己的目录，其名称等于以<strong class="kn ir">“输出-”</strong>为前缀的迭代次数</li><li id="1250" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">evaluation_strategy</code>:设置为<strong class="kn ir">“epoch”</strong>，表示在每个epoch结束时进行评估。</li><li id="76af" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">per_device_train_batch_size</code>:训练的批量。我们将它设置为8</li><li id="c817" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">per_device_eval_batch_size</code>:评估的批量。我们将它设置为16(以加快时间执行)</li><li id="1a25" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">num_train_epochs</code>:训练时期数。提醒一下，在一个时期内，模型已经看到了训练数据集的每个样本</li><li id="835a" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">group_by_length</code>:只要我们将使用动态填充，我们就将该参数设置为<code class="fe mf mg mh mi b">True</code>，以将训练数据集中长度大致相同的样本分组在一起(以最小化所应用的填充并提高效率)</li><li id="93b6" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">max_grad_norm</code>:最大梯度范数(用于梯度裁剪)。</li><li id="ec46" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">learning_rate</code>:AdamW优化器的初始学习率。提醒一下，AdamW优化器</li><li id="af3d" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">weight_decay</code>:应用于AdamW优化器的权重衰减:在我们的例子中，我们将权重衰减应用于除偏差和标准化层之外的所有层<strong class="kn ir"/></li></ul><p id="8d1c" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated"><strong class="kn ir">注:</strong> <br/>权重衰减是一种正则化技术，它给损失函数(通常是权重的L2范数)增加了一个小的惩罚。<br/>损耗=损耗+权重_衰减_参数* L2 _范数_权重</p><p id="adc3" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">一些实现仅对权重而不是偏差应用权重衰减。另一方面，PyTorch将权重衰减应用于权重和偏移。</p><blockquote class="nv nw nx"><p id="06dd" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated"><strong class="kn ir">为什么体重会衰减？</strong></p><p id="fa10" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated">1.以防止过度拟合。<br/> 2。为了避免爆炸梯度:由于额外的L2范数，除了损失之外，网络的每次迭代将试图优化模型权重。这将有助于保持权重尽可能小，防止权重增长失控，从而避免爆炸梯度</p></blockquote><ul class=""><li id="b07d" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><code class="fe mf mg mh mi b">gradient_accumulation_steps</code>:在执行向后传递之前，梯度应该累积的步数:当使用梯度累积时，梯度计算是在较小的步中完成的，而不是一次为一批完成；1(表示没有梯度累积))</li></ul><blockquote class="nv nw nx"><p id="4b93" class="kl km ny kn b ko ma kq kr ks mb ku kv nz mc ky kz oa md lc ld ob me lg lh li ij bi translated"><strong class="kn ir">注</strong> : <br/>在本<a class="ae lj" href="https://stackoverflow.com/questions/74065165/getting-cuda-error-when-trying-to-train-mbart-model" rel="noopener ugc nofollow" target="_blank"> Stackoverflow讨论</a>中，已经解释了如何使用set <code class="fe mf mg mh mi b">gradient_accumulation_steps</code>参数来避免OOM错误:将<code class="fe mf mg mh mi b">gradient_accumulation_steps</code>参数设置为一个适合内存的数字，并将<code class="fe mf mg mh mi b">per_device_train_batch_size</code>修改为<code class="fe mf mg mh mi b">original_batch_size/gradient_accumulation_steps</code>:这样梯度将在<code class="fe mf mg mh mi b">gradient_accumulation_steps</code>上累积，并通过<code class="fe mf mg mh mi b">gradient_accumulation_steps</code> * <code class="fe mf mg mh mi b">original_batch_size/gradient_accumulation_steps</code> = <code class="fe mf mg mh mi b">original_batch_size</code>样本执行向后传递。训练步骤的总数是:</p></blockquote><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi of"><img src="../Images/2903204a08c267677da85ead89619bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*P35rCfoKGywIUS9M07o4vA.png"/></div></figure><ul class=""><li id="ac45" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><code class="fe mf mg mh mi b">load_best_model_at_end</code>:我们将把它设置为True，以便在训练结束时加载训练过程中找到的最佳模型:在这种情况下，<code class="fe mf mg mh mi b">save_strategy</code>必须与<code class="fe mf mg mh mi b">evaluation_strategy</code> : <strong class="kn ir"> epoch </strong>相同</li><li id="e97b" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">metric_for_best_model</code>:我们将设置竞争指标<strong class="kn ir"> MCRMSE </strong>或<strong class="kn ir"> eval_MCRMSE </strong>(带有eval_ prefix)</li><li id="b76f" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">greater_is_better</code>:设置为False，因为我们想得到MCRMSE较低的模型</li><li id="d5ba" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">save_total_limit</code>:我们将把它设置为1，以便每次总是保留一个检查点(output_dir中较旧的检查点将被删除)。</li><li id="dc65" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">report_to</code>:由于我们连接到W &amp; B，我们将把report_to logs设置为<strong class="kn ir">“wandb”</strong></li><li id="f562" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><code class="fe mf mg mh mi b">label_name</code>:将label_name参数列表设置为<strong class="kn ir"> ["labels"]，</strong>，对应目标类对应的自定义<code class="fe mf mg mh mi b">Dataloader</code>生成的预定义字段</li></ul><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="474a" class="mn jo iq mi b be mo mp l mq mr">training_args = TrainingArguments(<br/>        output_dir=f"outputs-{fold}/",<br/>        evaluation_strategy="epoch",<br/>        per_device_train_batch_size=CONFIG['train_batch_size'],<br/>        per_device_eval_batch_size=CONFIG['valid_batch_size'],<br/>        num_train_epochs=CONFIG['epochs'],<br/>        learning_rate=CONFIG['learning_rate'],<br/>        weight_decay=CONFIG['weight_decay'],<br/>        gradient_accumulation_steps=CONFIG['n_accumulate'],<br/>        seed=SEED,<br/>        group_by_length=True,<br/>        max_grad_norm=CONFIG['max_grad_norm'],<br/>        metric_for_best_model='eval_MCRMSE',<br/>        load_best_model_at_end=True,<br/>        greater_is_better=False,<br/>        save_strategy="epoch",<br/>        save_total_limit=1,<br/>        report_to="wandb",<br/>        label_names=["labels"]<br/>    )</span></pre><p id="5805" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">此外，我们将为<code class="fe mf mg mh mi b">Trainer</code>定义一些其他参数:</p><ul class=""><li id="0b47" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><strong class="kn ir">数据整理器</strong>:我们需要定义如何从<code class="fe mf mg mh mi b">Dataloader<br/></code>返回的数据输入列表中创建一个批处理，我们将使用<code class="fe mf mg mh mi b">DataCollatorWithPadding</code>来动态填充接收到的输入。</li><li id="2149" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">优化器</strong>:我们将在所有层使用<code class="fe mf mg mh mi b">AdamW</code>，除了偏置和标准化层</li><li id="1290" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><strong class="kn ir">调度器</strong>:我们将使用<code class="fe mf mg mh mi b">get_linear_schedule_with_warmup</code>创建一个带有预热期的调度，在此期间，学习率从0线性增加到初始lr(在优化器中设置)，然后从优化器中设置的初始lr线性减少到0。<br/>调度器允许我们保持对学习率的控制，例如，如果我们想要确保学习率的每次更新不超过λ值(查看这个<a class="ae lj" href="https://stackoverflow.com/questions/39517431/should-we-do-learning-rate-decay-for-adam-optimizer" rel="noopener ugc nofollow" target="_blank"> Stackoverflow讨论</a>关于优化器调度器的效用)</li></ul><p id="ecca" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">要启动交叉验证培训，首先，我们必须按照<a class="ae lj" href="https://zghrib.medium.com/transformers-for-multi-regression-task-part1-transformers-as-feature-extractor-9f174ab66ce9" rel="noopener">第一部分帖子</a>中的说明创建简历折叠栏:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="332e" class="mn jo iq mi b be mo mp l mq mr"># set seed to produce similar folds<br/>cv = MultilabelStratifiedKFold(n_splits=CONFIG.get("folds", 3), shuffle=True, random_state=SEED)<br/><br/>train = train.reset_index(drop=True)<br/>for fold, ( _, val_idx) in enumerate(cv.split(X=train, y=train[CONFIG['label_cols']])):<br/>    train.loc[val_idx , "fold"] = int(fold)<br/>    <br/>train["fold"] = train["fold"].astype(int)</span></pre><p id="0e3d" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">CV培训工作流程可以按如下方式实施:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="7630" class="mn jo iq mi b be mo mp l mq mr"># Data Collator for Dynamic Padding<br/>collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)<br/># init predictions by fold<br/>predictions = {}<br/>for fold in range(0, CONFIG['folds']):<br/>    print(f" ---- Fold: {fold} ----")<br/>    run = wandb.init(project="FB3-deberta-v3", <br/>                     config=CONFIG,<br/>                     job_type='train',<br/>                     group="FB3-BASELINE-MODEL",<br/>                     tags=[CONFIG['model_name'], CONFIG['loss_type'], "10-epochs"],<br/>                     name=f'FB3-fold-{fold}',<br/>                     anonymous='must')<br/>    # the reset index is VERY IMPORTANT for the Dataset iterator<br/>    df_train = train[train.fold != fold].reset_index(drop=True)<br/>    df_valid = train[train.fold == fold].reset_index(drop=True)<br/>    # create iterators<br/>    train_dataset = CustomIterator(df_train, tokenizer)<br/>    valid_dataset = CustomIterator(df_valid, tokenizer)<br/>    # init model<br/>    model = FeedBackModel(CONFIG['model_name'])<br/>    model.to(CONFIG['device'])<br/>    <br/>    # SET THE OPITMIZER AND THE SCHEDULER<br/>    # no decay for bias and normalization layers<br/>    param_optimizer = list(model.named_parameters())<br/>    no_decay = ["bias", "LayerNorm.weight"]<br/>    optimizer_parameters = [<br/>        {<br/>            "params": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],<br/>            "weight_decay": CONFIG['weight_decay'],<br/>        },<br/>        {<br/>            "params": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],<br/>            "weight_decay": 0.0,<br/>        },<br/>    ]<br/>    optimizer = AdamW(optimizer_parameters, lr=CONFIG['learning_rate'])<br/>    num_training_steps = (len(train_dataset) * CONFIG['epochs']) // (CONFIG['train_batch_size'] * CONFIG['n_accumulate'])<br/>    scheduler = get_linear_schedule_with_warmup(<br/>        optimizer,<br/>        num_warmup_steps=0.1*num_training_steps,<br/>        num_training_steps=num_training_steps<br/>    )<br/>    # CREATE THE TRAINER<br/>    trainer = CustomTrainer(<br/>        model=model,<br/>        args=training_args,<br/>        train_dataset=train_dataset,<br/>        eval_dataset=valid_dataset,<br/>        data_collator=collate_fn,<br/>        optimizers=(optimizer, scheduler),<br/>        compute_metrics=compute_metrics<br/>    )<br/>    # LAUNCH THE TRAINER<br/>    trainer.train()</span></pre><p id="eebe" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">你可以访问我为这个项目创建的公共W&amp;B仪表板:<a class="ae lj" href="https://wandb.ai/athena75/Feedback3-deberta?workspace=user-athena75" rel="noopener ugc nofollow" target="_blank">https://wandb.ai/athena75/FB3-deberta-v3?工作空间=用户-雅典娜75 </a></p><h1 id="301d" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🗿创建W&amp;B工件</h1><p id="988f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对于以后的使用，一旦模型被微调，W&amp;B非常方便地创建模型工件。我们可以在以后使用它们，并创建我们模型的新版本。</p><p id="38c1" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">要创建一个模型工件，您所要做的就是:</p><ol class=""><li id="8137" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li oc nb nc nd bi translated">创建一个名字清晰一致的<code class="fe mf mg mh mi b">wandb.Artifact</code>对象，你必须指定<code class="fe mf mg mh mi b">type</code>参数，它可以是<code class="fe mf mg mh mi b">dataset</code>或<code class="fe mf mg mh mi b">model</code>，在我们的例子中是<code class="fe mf mg mh mi b">model</code></li><li id="24a3" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">将本地目录添加到工件中:事实上，一旦您实例化了模型并开始微调它，它就会创建一个包含模型<code class="fe mf mg mh mi b">bin</code>以及模型状态和配置的本地检查点。你得把它加到艺术品上。</li><li id="b29a" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">一旦工件拥有了所有需要的文件，您就可以调用<code class="fe mf mg mh mi b">wandb.log_artifact()</code>来记录它。</li></ol><p id="ab37" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">下面是一个为每个CV模型创建工件的代码片段示例:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="b156" class="mn jo iq mi b be mo mp l mq mr">for fold in range(0, CONFIG['folds']):<br/>    run = wandb.init(project="FB3-deberta-v3", <br/>                         config=CONFIG,<br/>                         job_type='train',<br/>                         group="FB3-BASELINE-MODEL",<br/>                         tags=[CONFIG['model_name'], CONFIG['loss_type'], "10-epochs"],<br/>                         name=f'FB3-fold-{fold}',<br/>                         anonymous='must')<br/><br/>    trainer = CustomTrainer(<br/>              .....<br/>    )<br/>    ##### TRAIN / FINE-TUNE ####<br/>    # create model artifact<br/>    model_artifact = wandb.Artifact(f'FB3-fold-{fold}', type="model",<br/>                                   description=f"MultilabelStratified - fold--{fold}")<br/>    # save locally the model - it would create a local dir<br/>    trainer.save_model(f'fold-{fold}')<br/>    # add the local dir to the artifact<br/>    model_artifact.add_dir(f'fold-{fold}')<br/>    # log artifact<br/>    # it would save the artifact version and declare the artifact as an output of the run<br/>    run.log_artifact(model_artifact)<br/>    <br/>    run.finish()<br/>  <br/></span></pre><h1 id="e4fd" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">inference✨:的✨Use W&amp;B艺术品</h1><p id="d920" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">一旦训练完成，我们可以<strong class="kn ir">使用Weights&amp;bias服务器上存储的工件</strong>，在我们的例子中，生成模型预测并生成聚合输出预测。</p><p id="38e1" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">PS:可以直接从W&amp;B接口提取使用代码<a class="ae lj" href="https://wandb.ai/athena75/Feedback3-deberta/artifacts/model/FB3-fold-0/93c08783e5b7c696451a/usage" rel="noopener ugc nofollow" target="_blank">https://wandb . ai/Athena 75/feedback 3-deberta/artifacts/model/FB3-fold-0/93c 08783 e5b 7c 696451 a/usage</a></p><ol class=""><li id="0917" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li oc nb nc nd bi translated">登录到您的wandb帐户，用<code class="fe mf mg mh mi b">wandb.init()</code>实例化一个默认运行</li><li id="5c9c" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">向<code class="fe mf mg mh mi b">use_artifact()</code>方法指出工件的路径以及检索工件的类型(在我们的例子中是<code class="fe mf mg mh mi b">model</code>)</li><li id="b690" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">使用<code class="fe mf mg mh mi b">download()</code>方法在本地下载工件目录</li><li id="a452" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li oc nb nc nd bi translated">加载本地模型并使用它进行预测</li></ol><p id="a99c" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">实施示例:</p><pre class="ll lm ln lo gt mj mi mk bn ml mm bi"><span id="7920" class="mn jo iq mi b be mo mp l mq mr">predictions = torch.zeros((len(test), len(CONFIG['label_cols']))<br/><br/>for fold in range(CONFIG["folds"]):<br/>    print(f"---- FOLD {fold} -------")<br/>    # instantiate deafault run<br/>    run = wandb.init()<br/>    # Indicate the artifact we want to use with the use_artifact method.<br/>    artifact = run.use_artifact(f'athena75/FB3-deberta-10/FB3-fold-{fold}:v0', type='model')<br/>    # download locally the model<br/>    artifact_dir = artifact.download()<br/>    # load the loacal model<br/>    # it is a pytorch moeal: loaded as follows<br/>    # https://pytorch.org/tutorials/beginner/saving_loading_models.html<br/>    model = FeedBackModel(CONFIG['model_name'])<br/>    model.load_state_dict(torch.load(f'artifacts/FB3-fold-{fold}:v0/pytorch_model.bin'))<br/>    # generate test embediings<br/>    test_dataset = CustomIterator(test, tokenizer, is_train=False)<br/>    test_dataloader = torch.utils.data.DataLoader(<br/>            test_dataset, <br/>            batch_size=CONFIG["train_batch_size"],<br/>            shuffle=False<br/>        )<br/>    input_ids, attention_mask = tuple(next(iter(test_dataloader)).values())<br/>    input_ids = input_ids.to('cpu')<br/>    attention_mask = attention_mask.to('cpu')<br/>    # genreate predictions<br/>    fold_preds = model(input_ids, attention_mask)<br/>    predictions = fold_preds.logits.add(predictions)<br/>    # remove local dir to save space<br/>    shutil.rmtree('artifacts')</span></pre><h1 id="53d8" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">🙏学分:</h1><p id="15b4" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这项工作是这些优秀资源的独特起源，请不要犹豫投票支持Kaggle资源:</p><ul class=""><li id="d2ec" class="mv mw iq kn b ko ma ks mb kw mx la my le mz li na nb nc nd bi translated"><a class="ae lj" href="https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @rhtsingh </strong>笔记本</a>:探索变压器表象的不同方式</li><li id="f44a" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><a class="ae lj" href="https://www.kaggle.com/code/debarshichanda/fb3-custom-hf-trainer-w-b-starter#Loss-Function" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @debarshichanda </strong>作品</a>:借用了训练器超参数和模型架构</li><li id="87e7" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><a class="ae lj" href="https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @Y.NAKAMA </strong>笔记本</a>:损失函数和多标签—分层交叉验证</li><li id="832a" class="mv mw iq kn b ko ne ks nf kw ng la nh le ni li na nb nc nd bi translated"><a class="ae lj" href="https://www.kaggle.com/code/shreydan/using-transformers-for-the-first-time-pytorch/notebook" rel="noopener ugc nofollow" target="_blank"> <strong class="kn ir"> @shreydan </strong>笔记本</a>:笔记本的全球风格</li></ul><h1 id="f70c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论:</h1><p id="5693" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">非常感谢你阅读我的帖子！🥰，我跟你分享我所有的作品:这个<a class="ae lj" href="https://www.kaggle.com/code/schopenhacker75/transformers-for-us-beginners" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>，还有我的公<a class="ae lj" href="https://wandb.ai/athena75/Feedback3-deberta?workspace=user-athena75" rel="noopener ugc nofollow" target="_blank"> W &amp; B仪表盘</a>。</p><p id="0706" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">我希望这是明确的，并随时问我问题。</p><p id="107e" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">📬我的邮件地址是:<strong class="kn ir">schopenhacker75@gmail.com</strong>📬</p><p id="ff92" class="pw-post-body-paragraph kl km iq kn b ko ma kq kr ks mb ku kv kw mc ky kz la md lc ld le me lg lh li ij bi translated">在后面的一篇文章中，我打算讨论如何在生产中部署一个转换器。或者如何为NLP变压器建立一个<strong class="kn ir"> MLOps管道，</strong>我还没有决定…</p></div></div>    
</body>
</html>