<html>
<head>
<title>Building and Visualizing Machine Language Translation from Scratch using TF2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TF2从头开始构建和可视化机器语言翻译</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/building-and-visualizing-machine-language-translation-using-seq2seq-models-with-attention-from-6803b9b4bd6a?source=collection_archive---------2-----------------------#2020-08-05">https://pub.towardsai.net/building-and-visualizing-machine-language-translation-using-seq2seq-models-with-attention-from-6803b9b4bd6a?source=collection_archive---------2-----------------------#2020-08-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f466" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>，<a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="b798" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">学习理解建立机器语言翻译，深度学习模型，用TensorFlow(注意使用Seq2seq模型)。</h2></div><p id="0bbc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这篇文章需要一些递归神经网络和GRU的基本知识。我将在本文中简要介绍这个概念。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lp lq l"/></div></figure><p id="7d35" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在序列对序列的深度学习中，模型已经在机器语言翻译的任务中取得了很大的成功。在本文中，我们将构建一个西班牙语到英语的翻译器。</p><p id="71ce" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">记住，动画中的任何凹凸都是指在幕后进行的数学运算。</strong>另外，如果你在动画中遇到任何法语单词，把它们当成西班牙语单词，然后继续(我尽力从网上收集最好的动画(͡ ͜ʖ ͡))</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/9347c17a6084b4a788d61de823668526.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*AvuNyEYeqwXdAIG8OpL2Cw.gif"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated">序列2序列可视化</figcaption></figure><p id="d0a9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">RNN简介:</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/c6fb8c4ba8b90f075390b26e488bba81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Y4WLkoW39iZa9XUWwRhuuw.gif"/></div></figure><p id="0c39" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">每个RNN单元接受一个输入向量(<strong class="kq ja">输入#1 </strong>)和前一个隐藏状态向量(<strong class="kq ja">隐藏状态#0 </strong>)来计算该特定单元的当前隐藏状态向量(<strong class="kq ja">隐藏状态#1 </strong>)和输出向量(<strong class="kq ja">输出#1 </strong>)。我们堆叠了许多这样的单元来构建我们的模型(如果我们专门讨论第一个单元，逻辑上就没有先前的状态，所以用零初始化它)。在接下来的动画中，你会遇到编码器和解码器部分，这些编码器-解码器部分中的每一个都与这些RNN堆叠在一起(在我们当前的例子中，GRU的，他们也可以是LSTM的，但GRU的就足够了)。</p><p id="0106" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">先说seq2seq型号:</strong></p><p id="e123" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个seq2seq模型将输入作为一个项目序列，输出另一个项目序列。例如，在这个模型中，它将接受一个西班牙句子作为输入<strong class="kq ja">“我喜欢艺术”</strong>并输出翻译的英语句子<strong class="kq ja">“我喜欢艺术”</strong>。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/fdaac30a8de55ba127fc0883c9b9467c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*zVG-Fuh1QPhNAg9HtefLZA.gif"/></div></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/a047f4916f28cec2ad7e47e391b07a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*sxOWTZMK1p7IBAjVs--z4g.jpeg"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated">高级seq2seq注意模型</figcaption></figure><p id="7428" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">注意力</strong>机制的诞生是为了帮助记忆神经机器翻译(NMT)中的长源句。<strong class="kq ja"> attention </strong>发明的秘方是在上下文向量和整个源输入之间创建快捷方式，而不是从编码器的最后一个隐藏状态构建一个单独的上下文向量。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/dd61e2116f33c9efd46b2a47a82a3579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*U4D6in9uy1ENxdqdTLNQ-A.gif"/></div></figure><p id="0de2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在机器翻译的情况下，上下文是一个向量(基本上是一组数字)。编码器和解码器往往都是递归神经网络。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi md"><img src="../Images/cda409e7e163be14abc295682c4f7659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*foM2SXSyyby3f2m4YfTjoA.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated"><strong class="bd me">上下文</strong>是一个浮点向量。在本帖的后面，我们将通过为具有较高值的单元格分配较亮的颜色来可视化彩色矢量。</figcaption></figure><p id="ddec" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">您可以在设置模型时设置上下文向量的大小。它是编码器RNN中隐藏单元的数量。这些可视化显示了大小为4的向量，但在现实世界的应用程序中，上下文向量的大小可能是256、512或1024。在这个特定的实现中，我们将使用<strong class="kq ja"> 1024 </strong>，随着我们进一步深入，您将会看到它。</p><p id="24bb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">好了，让我们开始实施吧。</strong></p><p id="3499" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">完整实现google co-lab笔记本的是<a class="ae mf" href="https://bit.ly/3ilL63V" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">这里的</strong> </a>。我建议你一段一段地去读，我会试着解释几个我觉得理解实现有困难的部分。</p><p id="0df6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">进口第一</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mg lq l"/></div></figure><p id="90c0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在让我们装载我们的火车，测试和预处理它。我将使用来自这个<a class="ae mf" href="http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip" rel="noopener ugc nofollow" target="_blank">链接</a>的数据。数据由英语句子和西班牙语句子组成，用制表符(\t)分隔。</p><pre class="lk ll lm ln gt mh mi mj mk aw ml bi"><span id="459d" class="mm mn iq mi b gy mo mp l mq mr">(An example line in the dataset)<br/>Go see Tom.      Ve a ver a Tom.</span></pre><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mg lq l"/></div></figure><p id="d299" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在我们需要标记数据。<strong class="kq ja">标记化</strong>是将<strong class="kq ja">标记化</strong>或将字符串、文本拆分成一系列标记的过程。我们可以把记号看作是部分，就像单词是句子中的记号，句子是段落中的记号一样。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mg lq l"/></div></figure><pre class="lk ll lm ln gt mh mi mj mk aw ml bi"><span id="26e2" class="mm mn iq mi b gy mo mp l mq mr">The output of <strong class="mi ja">tokenizer </strong>for the sentence "I am worried." would be <br/>[1, 4, 568, 3, 2] 1 and 2 are mappings of start and end of the sentence. This is telling our model when to start or stop translation.</span></pre><p id="23e1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">既然我们对数据进行了标记化，让我们先取30，000个样本，让我们将训练集和验证集分成24，000个测试数据，6，000个val数据，总共30，000个句子。30，000拆分是为了更快地进行培训，您可以在合作实验室中实施时更改该数字。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mg lq l"/></div></figure><p id="9960" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">验证您的令牌化器</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mg lq l"/></div></figure><pre class="lk ll lm ln gt mh mi mj mk aw ml bi"><span id="0dea" class="mm mn iq mi b gy mo mp l mq mr">Input Language; index to word mapping <br/>1 ----&gt; &lt;start&gt; <br/>24 ----&gt; estoy <br/>36 ----&gt; muy <br/>1667 ----&gt; confundida <br/>3 ----&gt; . <br/>2 ----&gt; &lt;end&gt;</span><span id="714d" class="mm mn iq mi b gy ms mp l mq mr">Target Language; index to word mapping <br/>1 ----&gt; &lt;start&gt; <br/>4 ----&gt; i <br/>18 ----&gt; m <br/>85 ----&gt; so <br/>561 ----&gt; confused <br/>3 ----&gt; . <br/>2 ----&gt; &lt;end&gt;</span></pre><p id="664e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">创建一个tf.data这是一个非常有用的步骤，可以在不丢失任何一致性的情况下，对数据进行洗牌或对整个数据执行任何数据扩充操作。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mg lq l"/></div></figure><p id="687d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里我们定义了在网络中使用的参数。我们将使用256个嵌入维度，如果vocab大小为9414，那么我们的嵌入矩阵形状将为256*9414。这意味着对于我们的vocab集中的9414个单词中的每一个，我们将发布一个256长度的列向量，因此对于我们的网络来说，每个单词都是一个256长度的列向量。稍后，如果你想更深入地了解Keras嵌入层教程，查看这个解释得很好的<a class="ae mf" href="https://www.youtube.com/watch?v=OuNH5kT-aD0" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> youtube教程</strong> </a>。</p><p id="32db" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">回顾我们的高级seq2seq注意力模型，让我们实现我们的编码器和解码器网络，(记得我在<strong class="kq ja">RNN节简介</strong>中说过，编码器和解码器网络是堆叠的<a class="ae mf" href="https://d2l.ai/chapter_recurrent-modern/gru.html" rel="noopener ugc nofollow" target="_blank"> GRU单元</a>)。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/a047f4916f28cec2ad7e47e391b07a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*sxOWTZMK1p7IBAjVs--z4g.jpeg"/></div></div></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/8ee88545e90b5dc74a25f119f3e9f997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*gPHO3MURK-LsuYc08dJexQ.gif"/></div></figure><p id="7b1a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">在计算注意力、上下文向量和解码部分时，请留意上图中的时间戳，以便更好地理解。</strong></p><p id="7f47" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">下面的编码器从时间戳1开始，到时间戳3结束</strong></p><p id="1ee1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们将向编码网络(64，16)发送64个批量大小的输入，这里16是我们的标记化填充序列句子的形状。在我们所有30，000个(如果我们仅考虑训练数据，则为26，000个)样本中，存在一个句子，该句子中最多有16个单词，这就是我们从中获得16个单词的地方。如果句子只包含两个单词，我们为这两个单词发出标记，并用零填充剩余的14个槽。</p><p id="c2c0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个(64，16)通过嵌入层(9414，256)然后用1024个单位通过GRU(还记得吗？在解释上下文视觉的时候，我说我们会用1024)。1024是隐藏状态的形状，也是GRU的输出。</p><pre class="lk ll lm ln gt mh mi mj mk aw ml bi"><span id="e2ba" class="mm mn iq mi b gy mo mp l mq mr">Encoder output shape: (batch size, sequence length, units) (64, 16, 1024) <br/>Encoder Hidden state shape: (batch size, units) (64, 1024)</span></pre><p id="3854" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">好吧！查看编码器片段，更好地理解解释。我们首先用零初始化第一编码器GRU的隐藏状态。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mg lq l"/></div></figure><p id="e2b5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">就这样，我们构建了编码器，现在来看注意力和上下文向量以及计算解码器的主要部分。</strong></p><p id="ba63" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，编码器向解码器传递更多的数据。编码器不是传递编码阶段的最后一个隐藏状态，而是将所有隐藏状态传递给解码器:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/fa269683f94c790a7c9327505ecfc3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*f_PATlwMJhLnSuYJ7FnD1Q.gif"/></div></figure><p id="20d0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第二，注意力解码器在产生输出之前做了额外的步骤。为了关注与该解码时间步长相关的输入部分，解码器执行以下操作:</p><ol class=""><li id="7c5c" class="mu mv iq kq b kr ks ku kv kx mw lb mx lf my lj mz na nb nc bi translated">查看它接收到的编码器隐藏状态集——每个编码器隐藏状态与输入句子中的某个单词最相关</li><li id="649d" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj mz na nb nc bi translated">给每个隐藏的州打分</li><li id="9af1" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj mz na nb nc bi translated">将每个隐藏状态乘以其softmaxed分数，从而放大分数高的隐藏状态，淹没分数低的隐藏状态。</li></ol><p id="e21a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">计算隐藏状态分数:</strong></p><p id="3c3f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以下部分直接取自TensorFlow示例的合作实验室笔记本，因为这是解释的最佳方式。</p><p id="f188" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本教程将<a class="ae mf" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank"> Bahdanau的注意力</a>用于编码器。在写出简化形式之前，让我们先决定符号:</p><ul class=""><li id="70e9" class="mu mv iq kq b kr ks ku kv kx mw lb mx lf my lj ni na nb nc bi translated">FC =完全连接(密集)层</li><li id="06ce" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj ni na nb nc bi translated">EO =编码器输出</li><li id="354a" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj ni na nb nc bi translated">H =隐藏状态</li><li id="2a45" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj ni na nb nc bi translated">X =解码器的输入</li></ul><p id="7071" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">伪代码是:</p><ul class=""><li id="ee57" class="mu mv iq kq b kr ks ku kv kx mw lb mx lf my lj ni na nb nc bi translated"><code class="fe nj nk nl mi b">score = FC(tanh(FC(EO) + FC(H)))</code></li><li id="e449" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj ni na nb nc bi translated"><code class="fe nj nk nl mi b">attention weights = softmax(score, axis = 1)</code>。默认情况下，Softmax应用于最后一个轴，但这里我们想将其应用于第一个轴<em class="mt"/>，因为分数的形状是<em class="mt"> (batch_size，max_length，hidden_size) </em>。<code class="fe nj nk nl mi b">Max_length</code>是我们输入的长度。因为我们试图为每个输入分配一个权重，所以softmax应该应用于该轴。</li><li id="6565" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj ni na nb nc bi translated"><code class="fe nj nk nl mi b">context vector = sum(attention weights * EO, axis = 1)</code>。与上面选择轴为1的原因相同。</li><li id="950e" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj ni na nb nc bi translated"><code class="fe nj nk nl mi b">embedding output</code> =解码器X的输入通过嵌入层。</li><li id="2156" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj ni na nb nc bi translated"><code class="fe nj nk nl mi b">merged vector = concat(embedding output, context vector)</code></li><li id="a4d8" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj ni na nb nc bi translated">然后，这个合并的矢量被提供给GRU</li></ul><p id="f2cb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">每一步所有向量的形状都在代码的注释中指定了。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nm"><img src="../Images/a89863abc710292edc8faec000260bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZA-bVx0O_p1qUFL3TJf-XQ.jpeg"/></div></div></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nm"><img src="../Images/2d913bfc2a0ffe447001d6be74bfa40e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Y-0FpHu2flA19CHc1Noyw.jpeg"/></div></div></figure><p id="4dcf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">看上面的等式，所有hs_dash对应于编码器网络的隐藏状态，ht对应于解码器网络的隐藏状态。【摘自<a class="ae mf" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"><em class="mt">https://jalammar.github.io/illustrated-transformer/</em></a><em class="mt">】</em></p><p id="0e8e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这种评分练习在解码器端的每个时间步进行。</p><p id="e680" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在让我们把整个事情放在一起，在下面的视觉化中，看看注意力过程是如何工作的:</p><ol class=""><li id="83b0" class="mu mv iq kq b kr ks ku kv kx mw lb mx lf my lj mz na nb nc bi translated">注意力解码器RNN接收<end>令牌的嵌入和初始解码器隐藏状态。</end></li><li id="faa6" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj mz na nb nc bi translated">RNN处理它的输入，产生一个输出和一个新的隐藏状态向量(h4)。输出被丢弃。</li><li id="9197" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj mz na nb nc bi translated">注意步骤:我们使用编码器隐藏状态和h4向量来计算这个时间步骤的上下文向量(C4)。</li><li id="b24f" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj mz na nb nc bi translated">我们将h4和C4连接成一个向量。</li><li id="8950" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj mz na nb nc bi translated">我们将这个向量通过一个前馈神经网络(一个与模型联合训练的网络)。</li><li id="4866" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj mz na nb nc bi translated">前馈神经网络的输出表示该时间步长的输出字。</li><li id="3b39" class="mu mv iq kq b kr nd ku ne kx nf lb ng lf nh lj mz na nb nc bi translated">重复接下来的时间步骤</li></ol><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/487cefe6258c7825608dc0dbcf27cf23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*lHiKVOxaMc0lpWmFpzJLlw.gif"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated">在对关注值评分后计算上下文向量</figcaption></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/905fd07e0ab5f94b6c10a7cf9d2a1e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*Bw9IhaNZFuozyjOUILaqEg.gif"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk translated">整个画面发生在解码器端</figcaption></figure><p id="53f6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">解码器代码:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mg lq l"/></div></figure><p id="3938" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，我们对模型进行优化和训练，以实现完整的端到端机器翻译。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/fa269683f94c790a7c9327505ecfc3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*f_PATlwMJhLnSuYJ7FnD1Q.gif"/></div></figure><p id="195a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请从TensorFlow官方教程中找到解释模型的完整实现的合作实验室笔记本，尝试自己实现它。</p><p id="ee6e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">链接:<a class="ae mf" href="https://colab.research.google.com/drive/1GhWYcxpoK97hyXGVtz6Bk7Iwl1WRe3gj" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">点击这里</strong> </a></p><p id="72e5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注意:这是我的第一篇关于媒体的文章，欢迎任何建议:)。</p><p id="8f09" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请随意评论您的疑问。</p><p id="9633" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae mf" href="https://www.linkedin.com/in/pvbhanuteja/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">在LinkedIn上连接</strong> </a></p><p id="ddeb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">图片来源:<em class="mt">阿拉玛，杰伊(2018)。图解变压器[博客帖子]。检索自</em><a class="ae mf" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"><em class="mt"/></a></p><p id="edb6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">参考资料:</p><div class="no np gp gr nq nr"><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ja gy z fp nw fr fs nx fu fw iz bi translated">谷歌联合实验室</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">编辑描述</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">colab.research.google.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of ls nr"/></div></div></a></div><div class="no np gp gr nq nr"><a href="https://arxiv.org/abs/1508.04025v5" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ja gy z fp nw fr fs nx fu fw iz bi translated">基于注意力的神经机器翻译的有效方法</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">一种注意力机制最近被用于通过选择性地聚焦于……来改善神经机器翻译(NMT)</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="no np gp gr nq nr"><a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ja gy z fp nw fr fs nx fu fw iz bi translated">可视化神经机器翻译模型(Seq2seq模型的机制，注意)</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">翻译:中文(简体)，日文，韩文，俄文观察:麻省理工学院的深度学习艺术讲座…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">jalammar.github.io</p></div></div><div class="oa l"><div class="og l oc od oe oa of ls nr"/></div></div></a></div><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="oh lq l"/></div></figure></div></div>    
</body>
</html>