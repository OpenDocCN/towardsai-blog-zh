<html>
<head>
<title>Building Neural Networks with Python Code and Math in Detail — II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python代码和数学详细构建神经网络— II</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/building-neural-networks-with-python-code-and-math-in-detail-ii-bbe8accbf3d1?source=collection_archive---------0-----------------------#2020-06-30">https://pub.towardsai.net/building-neural-networks-with-python-code-and-math-in-detail-ii-bbe8accbf3d1?source=collection_archive---------0-----------------------#2020-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/8b47268a54b91d05e54da9a87661e235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u_u1-D9R0i8yqQmm4Tvtmw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://pixabay.com/photos/neural-network-brain-neurons-brains-5350782/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><h2 id="e1a3" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>、<a class="ae ep" href="https://towardsai.net/p/category/scholarly" rel="noopener ugc nofollow" target="_blank">学者型</a>、<a class="ae ep" href="https://towardsai.net/p/category/tutorial" rel="noopener ugc nofollow" target="_blank">教程型</a></h2><div class=""/><div class=""><h2 id="edc1" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">我们关于<a class="ae jg" href="https://towardsai.net/p/machine-learning/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf" rel="noopener ugc nofollow" target="_blank">神经网络教程的第二部分从零开始</a>。从它们背后的数学到Python中一步一步的实现案例研究。在Google Colab上发布示例。</h2></div><p id="e576" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后更新于2021年1月7日</p><p id="03d5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">作者:</strong>普拉蒂克·舒克拉，<a class="ae jg" href="https://mktg.best/vguzs" rel="noopener ugc nofollow" target="_blank">罗伯特·伊里翁多</a></p><div class="is it gp gr iu md"><a href="https://members.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">加入人工智能，成为会员，你将不仅支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><p id="f279" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi ms translated"><span class="l mt mu mv bm mw mx my mz na di">在我们的<a class="ae jg" href="https://towardsai.net/p/machine-learning/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">神经网络教程</strong> </a>的第一部分中，我们解释了关于神经网络的基本概念，从它们背后的数学到用Python实现无任何隐藏层的神经网络。我们展示了即使在没有使用任何隐藏层的情况下，如何做出令人满意的预测。然而，单层神经网络有几个限制。</span></p><p id="eabc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本教程中，我们将深入探讨在<a class="ae jg" href="https://mld.ai/mldcmu" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">机器学习</strong> </a>中使用神经网络的局限性和优势。我们将展示如何实现具有隐藏层的神经网络，以及这些如何导致我们预测的更高准确率，以及在Google Colab上用Python实现的示例。</p><h2 id="c898" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">索引:</h2><ol class=""><li id="2c6d" class="nt nu jj lj b lk nv ln nw lq nx lu ny ly nz mc oa ob oc od bi translated"><a class="ae jg" href="#67a1" rel="noopener ugc nofollow">神经网络的局限性和优势</a></li><li id="99b0" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated"><a class="ae jg" href="#8c06" rel="noopener ugc nofollow">如何选择一个隐藏层中的几个神经元？</a></li><li id="6817" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated"><a class="ae jg" href="#55f2" rel="noopener ugc nofollow">人工神经网络的一般结构。</a></li><li id="3eed" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated"><a class="ae jg" href="#8c76" rel="noopener ugc nofollow">用Python实现多层神经网络。</a></li><li id="2fe6" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated"><a class="ae jg" href="#6232" rel="noopener ugc nofollow">与单层神经网络的比较。</a></li><li id="391d" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated"><a class="ae jg" href="#3393" rel="noopener ugc nofollow">用神经网络非线性分离数据。</a></li><li id="2540" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated"><a class="ae jg" href="#edf5" rel="noopener ugc nofollow">结论。</a></li></ol><blockquote class="oj"><p id="3bad" class="ok ol jj bd om on oo op oq or os mc dk translated">📚查看我们对<a class="ae jg" href="https://towardsai.net/p/machine-learning/best-machine-learning-books-free-and-paid-ml-book-recommendations-40c9ab30b0c" rel="noopener ugc nofollow" target="_blank">最佳机器学习书籍</a>的编辑推荐。📚</p></blockquote></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h1 id="67a1" class="pa nc jj bd nd pb pc pd ng pe pf pg nj ky ph kz nm lb pi lc np le pj lf ns pk bi translated">1.神经网络的局限性和优势</h1><h2 id="9a10" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">单层神经网络的局限性:</h2><ul class=""><li id="0e6c" class="nt nu jj lj b lk nv ln nw lq nx lu ny ly nz mc pl ob oc od bi translated">它们只能代表有限的一组功能。如果我们一直在训练一个使用复杂函数的模型(这是一般情况)，那么使用单层神经网络会导致我们的预测率准确性较低。</li><li id="c514" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">他们只能预测线性可分的数据。如果我们有非线性数据，那么训练我们的单层神经网络将导致我们预测率的准确性较低。</li><li id="da2a" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">单层神经网络的决策边界必须在超平面中，这意味着如果我们的数据分布在3维中，那么我们的决策边界必须在2维中。</li></ul><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/f4bc200aae3855af15ac20a7d10a4880.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/0*8JShtGmzsGvKdkwO.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图0:非线性可分离数据的示例。</figcaption></figure><p id="6310" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了克服这些限制，我们在神经网络中使用隐藏层。</p><h2 id="e0bd" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">单层神经网络的优点:</h2><ul class=""><li id="c696" class="nt nu jj lj b lk nv ln nw lq nx lu ny ly nz mc pl ob oc od bi translated">单层神经网络很容易建立。</li><li id="1215" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">与多层神经网络相比，单层神经网络需要较少的训练时间。</li><li id="b986" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">单层神经网络与统计模型有明确的联系。</li><li id="326d" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">单层神经网络的输出是输入的加权和。这意味着我们可以可行地解释单层神经网络的输出。</li></ul><h2 id="4cc1" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">多层神经网络的优点:</h2><ul class=""><li id="7039" class="nt nu jj lj b lk nv ln nw lq nx lu ny ly nz mc pl ob oc od bi translated">他们通过考虑处理单元的层次来构建更广泛的网络。</li><li id="e3cc" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">它们可用于对非线性可分数据进行分类。</li><li id="a6a5" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">多层神经网络比单层神经网络更可靠。</li></ul></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h1 id="8c06" class="pa nc jj bd nd pb pc pd ng pe pf pg nj ky ph kz nm lb pi lc np le pj lf ns pk bi translated">2.如何选择一个隐藏层中的几个神经元？</h1><p id="ab59" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">有许多方法可以确定在隐藏层中使用的神经元的正确数量。我们将在这里看到其中的一些。</p><ul class=""><li id="c5b0" class="nt nu jj lj b lk ll ln lo lq pu lu pv ly pw mc pl ob oc od bi translated">隐藏节点的数量应小于输入层中节点大小的两倍。</li></ul><p id="0abd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:如果我们有2个输入节点，那么我们的隐藏节点应该少于4个。</p><p id="bae3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> a. </strong> 2个输入，4个隐藏节点:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi px"><img src="../Images/d1c61b565377ecbc1c4f234f94d2bba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/0*auvAMM5rI7f5GVG-.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:有2个输入和4个隐藏节点的神经网络。</figcaption></figure><p id="4be8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> b. </strong> 2个输入，3个隐藏节点:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/151fb785799aea9b52c87eeeff0e5e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/0*GM1i962-kdOhMMDU.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:有2个输入和3个隐藏节点的神经网络。</figcaption></figure><p id="4d72" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> c. </strong> 2个输入，2个隐藏节点:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/11adcd8848e2a339b6efea25e9c587bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/0*P1iV_IcFtYTMz_91.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3:有2个输入和2个隐藏节点的神经网络。</figcaption></figure><p id="7a7f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> d. </strong> 2个输入，1个隐藏节点:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/df5dbba9c35002dd881a8de01efbef2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/0*QHivZJL8i-N0ULdQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:有2个输入和1个隐藏节点的神经网络。</figcaption></figure><ul class=""><li id="6845" class="nt nu jj lj b lk ll ln lo lq pu lu pv ly pw mc pl ob oc od bi translated">隐藏节点的数量应该是输入节点大小的2/3加上输出节点的大小。</li></ul><p id="79fd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:如果我们有2个输入节点和1个输出节点，那么隐藏节点应该= floor(2*2/3 + 1) = 2</p><p id="5172" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> a. </strong> 2个输入，2个隐藏节点:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/f1efb940c358c2b051ccaf182e8121d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/0*d_59iR8WA0I68Gif.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图5:有2个输入和2个隐藏节点的神经网络。</figcaption></figure><ul class=""><li id="7df6" class="nt nu jj lj b lk ll ln lo lq pu lu pv ly pw mc pl ob oc od bi translated">隐藏节点的数量应该介于输入节点和输出节点的大小之间。</li></ul><p id="573a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:如果我们有3个输入节点和2个输出节点，那么隐藏节点应该在2和3之间。</p><p id="6e2f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> a. </strong> 3个输入，2个隐藏节点，2个输出:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/027945526a3423deb656951d66e5ba8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/0*cd_FjujrAYsG2Kj0.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图6:具有3个输入、2个隐藏节点和2个输出的神经网络。</figcaption></figure><p id="3369" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> b. </strong> 3个输入，3个隐藏节点，2个输出:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/acbfe629e75f017dbc5909cd5336437e.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*XKGOtFrsIZH6QKLX.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7:具有3个输入、3个隐藏节点和2个输出的神经网络。</figcaption></figure><h2 id="0c0e" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">我们需要多少重量值？</h2><ol class=""><li id="603e" class="nt nu jj lj b lk nv ln nw lq nx lu ny ly nz mc oa ob oc od bi translated">对于隐藏层:输入数*隐藏层节点数</li><li id="2f72" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">对于输出层:隐藏层节点数*输出数</li></ol></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h1 id="55f2" class="pa nc jj bd nd pb pc pd ng pe pf pg nj ky ph kz nm lb pi lc np le pj lf ns pk bi translated">3.人工神经网络(ANN)的一般结构:</h1><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/8bd3720db732b4fb3afd6c68d89367a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*LoRLQNfmyK0Jd7T7.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:具有三层的人工神经网络的一般结构，输入层、隐藏层和输出层。</figcaption></figure><h2 id="d423" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">人工神经网络概述；</h2><ol class=""><li id="4240" class="nt nu jj lj b lk nv ln nw lq nx lu ny ly nz mc oa ob oc od bi translated">听取意见。</li><li id="8150" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">添加偏置(如果需要)。</li><li id="4619" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">在隐藏层和输出层中分配随机权重。</li><li id="6f13" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">运行代码进行培训。</li><li id="66c9" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">找出预测中的错误。</li><li id="596e" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">用梯度下降算法更新隐层和输出层的权值。</li><li id="c845" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">用更新的重量重复训练阶段。</li><li id="3ec3" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">做预测。</li></ol><h2 id="a5b3" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">多层神经网络的执行；</h2><p id="d3e4" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">在阅读了第一篇文章之后，我们看到那里只有1个执行阶段。在该阶段，我们找到更新的权重值，并重新运行代码以实现最小误差。不过，这里事情有点辣。多层神经网络的执行分两个阶段进行。在阶段1中，我们更新weight_output的值(输出层的权重值)，在阶段2中，我们更新weight_hidden的值(隐藏层的权重值)。阶段1类似于没有任何隐藏层的神经网络。</p><h2 id="615e" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">第一阶段的执行:</h2><p id="8b6e" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">为了找到导数，我们将使用梯度下降算法来更新权重值。这里我们不打算推导那些函数的导数，我们已经在神经网络的第一部分中推导过了。在这个阶段，我们的目标是找到输出层的权重值。这里我们将计算与输出重量变化相关的误差变化。</p><p id="548b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们首先定义一些将在这些衍生工具中使用的术语:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qd"><img src="../Images/a579eeba938c98e3d5cb6c66d6575ea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/0*Wa_K-XvqEDFlZu60.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:定义我们的衍生品。</figcaption></figure><p id="a7b1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> a .求一阶导数:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/e9bc4ec4946f26cbccf8e994afc3d193.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/0*JBHBZr83jiykgwuj.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:寻找一阶导数。</figcaption></figure><p id="0c80" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> b .求二阶导数:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/598f3b21b5e4738c449f817b2cf0233b.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/0*-ytDR8lonJdUytBd.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:寻找二阶导数。</figcaption></figure><p id="dfa8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> c .求三阶导数:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/f53869edf9991312457d70670e5c016e.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/0*EfabGm0JJm661KYP.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12:寻找三阶导数。</figcaption></figure><p id="b26b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意，我们已经在教程 的第一部分<a class="ae jg" href="https://towardsai.net/p/machine-learning/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">中推导了这些导数。</strong></a></p><h1 id="ac6b" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">第二阶段的执行:</h1><p id="970d" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">在阶段1中，我们找到输出层的更新权重。在第二阶段，我们需要为隐藏层找到更新的权重。因此，找出隐藏权重的变化如何影响误差值的变化。</p><p id="7688" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">表示为:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qm"><img src="../Images/8a2b75b0b0c5e68c1a7faa93027ec0e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*QcKSbnuHPEJPGBnK.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图13:查找隐藏层的更新权重。</figcaption></figure><p id="eec4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> a .求一阶导数:</strong></p><p id="7e62" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里我们要用链式法则来求导数。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qn"><img src="../Images/d2cf29c5da565afbee0d858adae075ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/0*Z--x1uDa0rmijTUa.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图14:寻找一阶导数。</figcaption></figure><p id="7f08" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">再次使用链式法则。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/8c0e86547aac5e3b7498ae54fc559f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/0*sS5cKWwq8wTY2HX9.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图15:再次应用变更规则。</figcaption></figure><p id="8041" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面的步骤类似于我们在神经网络 的<a class="ae jg" href="https://towardsai.net/p/machine-learning/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">教程的第一部分中所做的。</strong></a></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/f35ad3156e8c395dcb0b18b186791d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/0*5HDk13kp24JaQkgH.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图16:扩展一阶导数的结果，得到输出权重。</figcaption></figure><p id="b58e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> b .求二阶导数:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/6f0bb058e54030c9220861a516fea42c.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/0*qXoVqO51KLJVPI-7.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图17:寻找二阶导数。</figcaption></figure><p id="2ce2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> c .求三阶导数:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/91c0770e594bf51ea44450f5fade312c.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/0*0zNzhnsGPUKwFQ-O.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图18:寻找三阶导数。</figcaption></figure></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h1 id="8c76" class="pa nc jj bd nd pb pc pd ng pe pf pg nj ky ph kz nm lb pi lc np le pj lf ns pk bi translated">4.用Python实现多层神经网络</h1><blockquote class="oj"><p id="803f" class="ok ol jj bd om on qs qt qu qv qw mc dk translated">📚多层神经网络:具有隐藏层的神经网络📚有关更多定义，请查看我们在<a class="ae jg" href="https://towardsai.net/machine-learning-definitions" rel="noopener ugc nofollow" target="_blank">机器学习术语</a>中的文章。</p></blockquote><p id="cc92" class="pw-post-body-paragraph lh li jj lj b lk qx kt lm ln qy kw lp lq qz ls lt lu ra lw lx ly rb ma mb mc im bi translated">下面我们将实现没有偏差值的“或”门。总之，在神经网络中添加隐藏层有助于我们在模型中实现更高的准确性。</p><h1 id="db10" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">代表性:</h1><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rc"><img src="../Images/fe7391e192cc10588c3d9682d51baaf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/0*mRN1PHvi1nzwAH6O.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图19:或门。</figcaption></figure><h1 id="b2e4" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">真值表:</h1><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rd"><img src="../Images/be6eaa0f907280ee70e0a4225e0c4672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/0*2_9MzuxTD0a7HyFC.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图20:输入特性。</figcaption></figure><h1 id="318b" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">神经网络:</h1><p id="23c7" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">请注意，这里我们有2个输入要素和1个输出要素。在这个神经网络中，我们将使用1个具有3个节点的隐藏层。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi re"><img src="../Images/2059c14e4177c97ec62d4661ffd1fce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/0*6CA5HkC5bJHlMgO1.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图21:神经网络。</figcaption></figure><h1 id="d69b" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">图形表示:</h1><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rf"><img src="../Images/1da4aaaa7e0b487d66bfb142b931bce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/0*ZnDa6doLfsmqt846.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图22:图中的输入，注意相同颜色的点有相同的输出。</figcaption></figure><h1 id="2258" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">Python中的实现:</h1><p id="d23b" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">下面，我们要用Python一步一步实现我们的带隐藏层的神经网络，我们来编码:</p><p id="2dc3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> a .导入所需的库:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi rg"><img src="../Images/446b2dacf49d9e89a057170c3147e9cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/0*S2w4O61l_b1xd0dU.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图23:导入NumPy。</figcaption></figure><p id="6784" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> b .定义输入特征:</strong></p><p id="2118" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们获取想要训练神经网络的输入值。我们可以看到我们采用了两个输入特征。在有形数据集上，输入要素的价值通常很高。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rh"><img src="../Images/d72a10e800106383ca7a8e232eb06382.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/0*0yzavsGaSvMjqL3B.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图24:分配输入值来训练我们的神经网络。</figcaption></figure><p id="dbe4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> c .定义目标输出值:</strong></p><p id="9d86" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于输入特性，我们希望特定的输入特性有特定的输出。它被称为目标输出。我们将训练为我们的输入特征提供目标输出的模型。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi ri"><img src="../Images/6e6b31d3682ce6e8670ef1685de81983.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/0*LA9MlsGRKXBEK8EW.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图25:定义我们的目标输出，并将我们的目标输出重塑为一个向量</figcaption></figure><p id="8242" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> d .分配随机权重:</strong></p><p id="0639" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们将为输入要素分配随机权重。请注意，我们的模型会将这些权重值修改为最佳值。此时，我们随机取这些值。这里我们有两层，所以我们必须分别为它们分配权重。</p><p id="dbbe" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另一个变量是学习率。我们将在梯度下降算法中使用学习率(LR)来更新权重值。一般来说，我们将LR保持在尽可能低的水平，这样我们可以实现最小的错误率。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rj"><img src="../Images/a55e944f179c2d0c3aa80142fa4699fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/0*7P1P9g4KgILtuguO.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图26:定义我们的神经网络的权重，以及我们的学习率(LR)</figcaption></figure><p id="340f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">乙状结肠功能:</strong></p><p id="ddf0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦我们有了权重值和输入特征，我们将把它发送给预测输出的主函数。请注意，我们的输入要素和权重值可以是任何值，但是这里我们希望对数据进行分类，因此我们需要0到1之间的输出。对于这样的输出，我们将使用sigmoid函数。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rk"><img src="../Images/aa6b3240f8ff7d2d309b93c6b3aa9901.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/0*XNerBDmhEp_Pdep3.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图27:应用我们的sigmoid函数。</figcaption></figure><p id="8add" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> f. Sigmoid函数导数:</strong></p><p id="2792" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在梯度下降算法中，我们需要sigmoid函数的导数。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rl"><img src="../Images/acf70746abd995c2771079fefcb7e5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/0*vU1c2d7-uLrBBf7b.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图28:对我们的sigmoid函数进行求导。</figcaption></figure><p id="7669" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> g .预测输出和更新权重值的主要逻辑:</strong></p><p id="6703" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将逐步理解下面的代码。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rm"><img src="../Images/325b5feca45c4bd1e9e588ac6b4e8786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/0*v0lrZ_pWNYvze6x0.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图29:我们神经网络训练的第一阶段。</figcaption></figure><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rn"><img src="../Images/3d0a80a55066af8ca3b30839cedf12ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/0*u7P484Xd6yqVuHPZ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图30:我们神经网络训练的第二阶段。</figcaption></figure><h2 id="431e" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">它是如何工作的？</h2><p id="30f0" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated"><strong class="lj jt"> a. </strong>首先，我们运行上面的代码200000次。请记住，如果我们只运行这段代码几次，那么我们很可能会有更高的错误率。因此，我们将权重值更新10，000次，以达到可能的最佳值。</p><p id="705f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们找到隐藏层的输入。由以下公式定义:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi ro"><img src="../Images/eac12eb498c27022379c74fc48c457fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/0*Hdjge8HzXmm3qpg5.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图31:寻找神经网络隐藏层的输入。</figcaption></figure><p id="d510" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们也可以用矩阵来表示，以便更好地理解。</p><p id="a279" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里的第一个矩阵是大小为(4*2)的输入要素，第二个矩阵是大小为(2*3)的隐藏图层的权重值。因此得到的矩阵大小为(4*3)。</p><p id="2039" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最终矩阵大小背后的直觉:</p><p id="b499" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最终矩阵的行大小与第一个矩阵的行大小相同，最终矩阵的列大小与乘法(点积)中第二个矩阵的列大小相同。</p><p id="b3b7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在下面的表示中，每个框代表一个值。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rp"><img src="../Images/c3e22868d834f0f3ff3c949cc38fc818.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/0*9npWuuHgpQsO7pQE.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图32:矩阵值表示。</figcaption></figure><p id="4099" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> c. </strong>之后，我们有了隐藏层的输入，它将通过应用sigmoid函数来计算输出。下面是隐藏层的输出:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rq"><img src="../Images/b8c61d166eb55c20437bffa33b444cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/0*2qeJpjimY-pJSwpr.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图33:隐藏层的输出。</figcaption></figure><p id="1424" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> d. </strong>接下来，我们将隐藏层的输出乘以输出层的权重:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rr"><img src="../Images/9495fe9c9fd404c4cfb58fb4fbb500bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/0*rBCl8QPiFotX2viB.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图34:表示隐藏层输出的公式，带有输出层的权重。</figcaption></figure><p id="5295" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第一个矩阵显示了隐藏层的输出，其大小为(4*3)。第二矩阵表示输出层的权重值，</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rs"><img src="../Images/fda5aa237c46dbeb43286dd71a3d6593.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/0*t4i_u0NhTYxKZ2uz.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图35:隐藏层的表示，以及我们的输出层。</figcaption></figure><p id="d13a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">之后，我们通过应用sigmoid函数来计算输出层的输出。也可以用如下矩阵形式表示。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rt"><img src="../Images/636ab1a7243753c19e876d272253c830.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/format:webp/0*4OreBdiQGGntR8Tw.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图36:sigmoid函数后我们层的输出。</figcaption></figure><p id="d536" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> f. </strong>现在我们有了预测输出，我们找到了目标输出和预测输出之间的均方差。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi ru"><img src="../Images/ede7f19bde678a9bb8f9a004b655f7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/0*u69Zy1Jspekskoqh.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图37:找出我们的目标输出和我们的预测输出之间的平均值。</figcaption></figure><p id="30c5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们开始第一阶段的训练。在这一步中，我们将更新输出图层的权重值。我们需要找出输出权重对误差值的影响程度。为了更新权重，我们使用梯度下降算法。请注意，我们已经找到了将在培训阶段使用的导数。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rv"><img src="../Images/cdf68d3e2675b3cddc7676844901d4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*gzWxLibMN8EJv7Ri.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图38:更新我们输出层的权重值。</figcaption></figure><p id="baab" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> g.a. </strong>一阶导数的矩阵表示。矩阵大小(4*1)。</p><p id="4f72" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">derror_douto = output_op -target_output</code></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sa"><img src="../Images/359b7ac743fcb550754f858258c060b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/0*cjyBRak8DoDnvbKX.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图39:一阶导数矩阵表示。</figcaption></figure><p id="bef9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> g.b. </strong>二阶导数的矩阵表示。矩阵大小(4*1)。</p><p id="2566" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">dout_dino = sigmoid_der(input_op)</code></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sb"><img src="../Images/1e62914efde1a47ba92d2f521581b742.png" data-original-src="https://miro.medium.com/v2/resize:fit:122/format:webp/0*JZeKec3I8AypWRie.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图40:二阶导数矩阵表示。</figcaption></figure><p id="4a3e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> g.c. </strong>三阶导数的矩阵表示。矩阵大小(4*3)。</p><p id="d98b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">dino_dwo = output_hidden</code></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rq"><img src="../Images/63fb41e3404b6943519a3db5d184cfb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/0*gQTI85UrziW7r9W7.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图41:三阶导数矩阵表示。</figcaption></figure><p id="ed72" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> g.d. </strong>矩阵表示<code class="fe rw rx ry rz b">dino_dwo</code>的转置。矩阵大小(3*4)。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sc"><img src="../Images/7e978c2220aec5812be5546fc639d3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/0*2Bj4xl0dq65srNAk.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图42:我们的变量dino_dwo的矩阵表示，细节见实现。</figcaption></figure><p id="bcc4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> g.e. </strong>现在，我们要找到输出权重的最终矩阵。关于这一步的详细解释，请查阅<a class="ae jg" href="https://medium.com/towards-artificial-intelligence/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf#24ec" rel="noopener"> <strong class="lj jt">我们之前的教程</strong> </a>。矩阵大小为(3*1)，与<code class="fe rw rx ry rz b">output_weight</code>矩阵相同。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sd"><img src="../Images/1af86167360be9d9a029eec05a0bfa27.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/0*4XMjbO4Glne_ME3E.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图43:输出权重的最终矩阵。</figcaption></figure><p id="e1db" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，我们成功地找到了导数值。接下来，我们借助梯度下降算法相应地更新权重值。</p><p id="a7f6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">尽管如此，我们还必须找到阶段2的导数。让我们首先找到它，然后我们将在最后更新两层的权重。</p><p id="8bd9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第二阶段。更新隐藏层中的权重。</p><p id="929d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因为我们已经讨论了我们是如何导出导数值的，我们将会看到每个导数值的矩阵表示，以便更好地理解它。我们的目标是找到隐藏层的权重矩阵，大小为(2*3)。</p><p id="a957" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> h.a. </strong>一阶导数的矩阵表示。</p><p id="b8ab" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">derror_dino = derror_douto * douto_dino</code></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi se"><img src="../Images/8df23f933c7558bc3961b456e08666c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:158/format:webp/0*HJrc1RqNN91mNlLM.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图44:一阶导数的矩阵表示。</figcaption></figure><p id="5712" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> h.b. </strong>二阶导数的矩阵表示。</p><p id="32aa" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">dino_douth = weight_output</code></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sf"><img src="../Images/d8967dcc6bfb496c017bd1819c938009.png" data-original-src="https://miro.medium.com/v2/resize:fit:148/format:webp/0*DbqdJo1rNPZTdHIH.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图45:二阶导数的矩阵表示。</figcaption></figure><p id="bee1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> h.c. </strong>三阶导数的矩阵表示。</p><p id="bf53" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">derror_douth = np.dot(derror_dino , dino_douth.T)</code></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sg"><img src="../Images/ab16a26c993e5b26418b7a586352252e.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/0*nZKpAKOeuI8jsTgE.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图46:三阶导数的矩阵表示。</figcaption></figure><p id="7b68" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> h.d. </strong>四阶导数的矩阵表示。</p><p id="2bd4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">douth_dinh = sigmoid_der(input_hidden)</code></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sh"><img src="../Images/725f78535c3ec46838c42a9ff4388e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/0*QxUp8ASbNZ1qky7Q.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图47:四阶导数的矩阵表示。</figcaption></figure><p id="9b5b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> h.e. </strong>五阶导数的矩阵表示。</p><p id="7b6d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">dinh_dwh = input_features</code></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi si"><img src="../Images/9fa3b41ffab7ea48147c14bed33c6181.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/format:webp/0*6gnEDDtA6XKFHqxM.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图48:五阶导数的矩阵表示。</figcaption></figure><p id="a319" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> h.f. </strong>六阶导数的矩阵表示。</p><p id="8d68" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">derror_dwh = np.dot(dinh_dwh.T, douth_dinh * derror_douth)</code></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sj"><img src="../Images/b3b6bded7bab33983c829990bfad8181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/0*a5jsuFTl_kCUbkMh.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图49:六阶导数的矩阵表示。</figcaption></figure><p id="e285" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，我们的目标是找到一个大小为(2*3)的隐藏权重矩阵。此外，我们成功地找到了它。</p><p id="fd93" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> h.g. </strong>更新重量值:</p><p id="2035" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将使用梯度下降算法来更新这些值。它需要三个参数。</p><ol class=""><li id="46a9" class="nt nu jj lj b lk ll ln lo lq pu lu pv ly pw mc oa ob oc od bi translated">原重量:我们已经有了。</li><li id="9721" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">学习率(LR):我们将其赋值为0.05。</li><li id="b34e" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc oa ob oc od bi translated">导数:在上一步中找到。</li></ol><p id="2290" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">梯度下降算法:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi rv"><img src="../Images/5e654ce682541e6529ec8fefa23e25dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*I9hzp5jD4EkZWNwZ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图50:梯度下降算法的公式</figcaption></figure><p id="37d4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因为我们已经有了所有的参数值，所以这将是一个简单的操作。首先，我们更新输出层的权重值，然后更新隐藏层的权重值。</p><p id="c5d4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">一、最终重量值:</strong></p><p id="614b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面，我们显示了两个图层的更新权重值-我们的预测基于这些值。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sk"><img src="../Images/e63bcacdd130fd9e82d4c9f0e9a0b6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/0*V8-kVin1CnYP87q8.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图51:显示最终的隐藏层权重值。</figcaption></figure><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sl"><img src="../Images/dc9209e2e570f8f67b6e9672fd69d94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/0*Mm1MjT1XnW4vMZfV.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图52:显示最终的输出层权重值。</figcaption></figure><p id="f222" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> j .做预测:</strong></p><p id="0dc8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> j.a. </strong>预测为(1，1)。</p><p id="013a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标产量= 1</p><p id="6862" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">解释:</p><p id="493e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，我们将获取想要预测输出的输入值。“结果1”变量存储输入变量和隐藏层权重的点积值。我们通过应用sigmoid函数获得输出，结果存储在<code class="fe rw rx ry rz b">result2</code>变量中。这就是输出图层的输入要素。我们通过将输入要素乘以输出图层权重来计算输出图层的输入。为了找到最终的输出值，我们取它的sigmoid值。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/9a63fb14cfdd0ae4a0b220f02f4a8f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/0*8KbGaJ6RmV6eJYMs.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图53:打印目标输出= 1的结果。</figcaption></figure><p id="9a96" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，预测的输出非常接近1。所以我们成功地做出了准确的预测。</p><p id="cb6e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> j.b. </strong>预测为(0，0)。</p><p id="246e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标输出= 0</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sm"><img src="../Images/64bc94edbd992e242ebda88b97af1b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/0*Ix111fjRpJbgniN_.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图54:打印目标输出= 0的结果。</figcaption></figure><p id="ae40" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，预测的输出非常接近0，这表明我们的模型的成功率。</p><p id="4c53" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> k .最终误差值:</strong></p><p id="b0c5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">经过200，000次迭代后，我们得到了最终的误差值——误差越低，模型的精度越高。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sn"><img src="../Images/197d5bfb9a78c225e248195d6384459f.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/0*7rvJsHcf7g_tGc2n.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图55:显示200，000次迭代后的最终误差值。</figcaption></figure><p id="fe82" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如上图，我们可以看到误差值是0.0000000189。该值是经过200，000次迭代后预测的最终误差值。</p><h2 id="3773" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">将所有这些放在一起:</h2><pre class="pn po pp pq gt so rz sp sq aw sr bi"><span id="0ca6" class="nb nc jj rz b gy ss st l su sv"># Import required libraries :<br/>import numpy as np# Define input features :<br/>input_features = np.array([[0,0],[0,1],[1,0],[1,1]])<br/>print (input_features.shape)<br/>print (input_features)# Define target output :<br/>target_output = np.array([[0,1,1,1]])# Reshaping our target output into vector :<br/>target_output = target_output.reshape(4,1)<br/>print(target_output.shape)<br/>print (target_output)# Define weights :<br/># 6 for hidden layer<br/># 3 for output layer<br/># 9 totalweight_hidden = np.array([[0.1,0.2,0.3],<br/> [0.4,0.5,0.6]])<br/>weight_output = np.array([[0.7],[0.8],[0.9]])# Learning Rate :<br/>lr = 0.05# Sigmoid function :<br/>def sigmoid(x):<br/> return 1/(1+np.exp(-x))# Derivative of sigmoid function :<br/>def sigmoid_der(x):<br/> return sigmoid(x)*(1-sigmoid(x))for epoch in range(200000):<br/> # Input for hidden layer :<br/> input_hidden = np.dot(input_features, weight_hidden)<br/> <br/> # Output from hidden layer :<br/> output_hidden = sigmoid(input_hidden)<br/> <br/> # Input for output layer :<br/> input_op = np.dot(output_hidden, weight_output)<br/> <br/> # Output from output layer :<br/> output_op = sigmoid(input_op)#==========================================================<br/> # Phase1<br/> <br/> # Calculating Mean Squared Error :<br/> error_out = ((1 / 2) * (np.power((output_op — target_output), 2)))<br/> print(error_out.sum())<br/> <br/> # Derivatives for phase 1 :<br/> derror_douto = output_op — target_output<br/> douto_dino = sigmoid_der(input_op) <br/> dino_dwo = output_hiddenderror_dwo = np.dot(dino_dwo.T, derror_douto * douto_dino)#===========================================================<br/> # Phase 2 <br/> # derror_w1 = derror_douth * douth_dinh * dinh_dw1<br/> # derror_douth = derror_dino * dino_outh<br/> <br/> # Derivatives for phase 2 :<br/> derror_dino = derror_douto * douto_dino<br/> dino_douth = weight_output<br/> derror_douth = np.dot(derror_dino , dino_douth.T)<br/> douth_dinh = sigmoid_der(input_hidden) <br/> dinh_dwh = input_features<br/> derror_wh = np.dot(dinh_dwh.T, douth_dinh * derror_douth)# Update Weights<br/> weight_hidden -= lr * derror_wh<br/> weight_output -= lr * derror_dwo<br/> <br/># Final hidden layer weight values :<br/>print (weight_hidden)# Final output layer weight values :<br/>print (weight_output)# Predictions :#Taking inputs :<br/>single_point = np.array([1,1])<br/>#1st step :<br/>result1 = np.dot(single_point, weight_hidden) <br/>#2nd step :<br/>result2 = sigmoid(result1)<br/>#3rd step :<br/>result3 = np.dot(result2,weight_output)<br/>#4th step :<br/>result4 = sigmoid(result3)<br/>print(result4)#=================================================<br/>#Taking inputs :<br/>single_point = np.array([0,0])<br/>#1st step :<br/>result1 = np.dot(single_point, weight_hidden) <br/>#2nd step :<br/>result2 = sigmoid(result1)<br/>#3rd step :<br/>result3 = np.dot(result2,weight_output)<br/>#4th step :<br/>result4 = sigmoid(result3)<br/>print(result4)#=====================================================<br/>#Taking inputs :<br/>single_point = np.array([1,0])<br/>#1st step :<br/>result1 = np.dot(single_point, weight_hidden) <br/>#2nd step :<br/>result2 = sigmoid(result1)<br/>#3rd step :<br/>result3 = np.dot(result2,weight_output)<br/>#4th step :<br/>result4 = sigmoid(result3)<br/>print(result4)</span></pre><p id="2335" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面，请注意，我们在本例中使用的数据是线性可分的，这意味着通过一条线，我们可以对值为1的输出和值为0的输出进行分类。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sw"><img src="../Images/9e82099bd80f9ccc58687f65f47d2123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/0*0LjRrjg89LeBPLPL.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图56:图表显示数据是线性可分的，允许用1值或0值对输出进行分类。</figcaption></figure><p id="2db5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">在Google Colab上推出:</strong></p><div class="is it gp gr iu md"><a href="https://colab.research.google.com/drive/1Q4hIQRkabKxZXQPgNaQzYYJocSPlSE-B?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">谷歌联合实验室</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">用Python II构建神经网络—https://towardsai.net/building-neural-nets-with-python</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">colab.research.google.com</p></div></div><div class="mm l"><div class="sx l mo mp mq mm mr ja md"/></div></div></a></div></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h1 id="6232" class="pa nc jj bd nd pb pc pd ng pe pf pg nj ky ph kz nm lb pi lc np le pj lf ns pk bi translated">5.与单层神经网络的比较</h1><p id="9dd3" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">注意，我们在这里没有使用偏差值。现在让我们快速地看一下对于相同的输入特征和目标值没有隐藏层的神经网络。我们要做的是找出最终的错误率，并进行比较。由于我们在<a class="ae jg" href="https://medium.com/towards-artificial-intelligence/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf#0ea5" rel="noopener"> <strong class="lj jt">之前的教程</strong> </a>中已经实现了代码，为此，我们打算快速分析一下。[ <a class="ae jg" href="https://towardsai.net/neural-networks-with-python" rel="noopener ugc nofollow" target="_blank"> 2 </a></p><p id="a275" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以下代码的最终错误值是:</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi sy"><img src="../Images/839359b48adc9fe90712f55c03177c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/0*fTYSMOAXrKKRKGhs.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图57:显示最终误差值。</figcaption></figure><p id="8d69" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如我们可以看到的，与我们在使用隐藏层的神经网络实现中发现的错误相比，错误值太高了，这是在神经网络中使用隐藏层的主要原因之一。</p><pre class="pn po pp pq gt so rz sp sq aw sr bi"><span id="2a20" class="nb nc jj rz b gy ss st l su sv"># Import required libraries :<br/>import numpy as np# Define input features :<br/>input_features = np.array([[0,0],[0,1],[1,0],[1,1]])<br/>print (input_features.shape)<br/>print (input_features)# Define target output :<br/>target_output = np.array([[0,1,1,1]])# Reshaping our target output into vector :<br/>target_output = target_output.reshape(4,1)<br/>print(target_output.shape)<br/>print (target_output)# Define weights :<br/>weights = np.array([[0.1],[0.2]])<br/>print(weights.shape)<br/>print (weights)# Define learning rate :<br/>lr = 0.05# Sigmoid function :<br/>def sigmoid(x):<br/>    return 1/(1+np.exp(-x))# Derivative of sigmoid function :<br/>def sigmoid_der(x):<br/>    return sigmoid(x)*(1-sigmoid(x))# Main logic for neural network :<br/># Running our code 10000 times :for epoch in range(10000):<br/>    inputs = input_features#Feedforward input :<br/>    pred_in = np.dot(inputs, weights)#Feedforward output :<br/>    pred_out = sigmoid(pred_in)#Backpropogation <br/>    #Calculating error<br/>    error = pred_out - target_output<br/>    x = error.sum()<br/>    <br/>    #Going with the formula :<br/>    print(x)<br/>    <br/>    #Calculating derivative :<br/>    dcost_dpred = error<br/>    dpred_dz = sigmoid_der(pred_out)<br/>    <br/>    #Multiplying individual derivatives :<br/>    z_delta = dcost_dpred * dpred_dz#Multiplying with the 3rd individual derivative :<br/>    inputs = input_features.T<br/>    weights -= lr * np.dot(inputs, z_delta)#Predictions :#Taking inputs :<br/>single_point = np.array([1,0])<br/>#1st step :<br/>result1 = np.dot(single_point, weights) <br/>#2nd step :<br/>result2 = sigmoid(result1)<br/>#Print final result<br/>print(result2)#====================================<br/>#Taking inputs :<br/>single_point = np.array([0,0])<br/>#1st step :<br/>result1 = np.dot(single_point, weights) <br/>#2nd step :<br/>result2 = sigmoid(result1)<br/>#Print final result<br/>print(result2)#===================================<br/>#Taking inputs :<br/>single_point = np.array([1,1])<br/>#1st step :<br/>result1 = np.dot(single_point, weights) <br/>#2nd step :<br/>result2 = sigmoid(result1)<br/>#Print final result<br/>print(result2)</span></pre><p id="fde0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">在Google Colab上发布:</strong></p><div class="is it gp gr iu md"><a href="https://colab.research.google.com/drive/1Q4hIQRkabKxZXQPgNaQzYYJocSPlSE-B#scrollTo=dLelqmJaXNIw&amp;line=59&amp;uniqifier=1" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">谷歌联合实验室</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">用Python II构建神经网络—https://towardsai.net/building-neural-nets-with-python</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">colab.research.google.com</p></div></div><div class="mm l"><div class="sz l mo mp mq mm mr ja md"/></div></div></a></div></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h1 id="3393" class="pa nc jj bd nd pb pc pd ng pe pf pg nj ky ph kz nm lb pi lc np le pj lf ns pk bi translated">6.具有神经网络的非线性可分离数据</h1><p id="aeab" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">在这个例子中，我们要取一个不能被一条直线分开的数据集。如果我们试图用一行来分隔它，那么一个或多个输出可能会被错误分类，我们将会有非常高的误差。因此，我们使用隐藏层来解决这个问题。</p><h1 id="337b" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">输入表:</h1><figure class="pn po pp pq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ta"><img src="../Images/a88c1082df87b476fc7ab8c7b706bccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/0*1s7qtpHZDXPSdhGh.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图58:输入特性。</figcaption></figure><h1 id="034b" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">数据点的图形表示:</h1><p id="f86f" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">如下所示，我们在坐标平面上表示数据。请注意，我们有两个彩色点(黑色和红色)。如果我们试图画一条线，那么输出将会被错误分类。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tb"><img src="../Images/02e64376929ab0422febb01897be795a.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/0*RD_N9sQ8JcApxToO.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图59:带有输入点的坐标平面。</figcaption></figure><p id="304d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如图59所示，我们有2个输入和1个输出。在这个例子中，我们将使用4个隐藏感知器。红点的输出值为0，黑点的输出值为1。因此，我们不能简单地用一条直线对它们进行分类。</p><h1 id="b9bd" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">神经网络:</h1><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tc"><img src="../Images/19bd6f21489d477083647c76b4152a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/0*5aF5xn9zQVgikdD-.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图60:一个人工神经网络。</figcaption></figure><h1 id="d3b9" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">Python中的实现:</h1><p id="4db6" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated"><strong class="lj jt">答:导入所需的库:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi td"><img src="../Images/28d423d45c04ea148a7ae830ee1836c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/0*ejJTgLaOGoZ4Jjwb.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图61:用Python导入NumPy。</figcaption></figure><p id="e127" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> b .定义输入特征:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi te"><img src="../Images/e90eaf550df6d5e1b5c5e82c74086668.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/0*SWkmsHEJWRAtMC5A.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图62:定义我们的输入特征。</figcaption></figure><p id="de0e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> c .定义目标输出:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tf"><img src="../Images/46898da8bc541ac3f87143d914d77de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/0*HkAY5N_dqH7rwZy7.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图63:定义我们的目标输出。</figcaption></figure><p id="2874" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> d .分配随机权重值:</strong></p><p id="0185" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图64中，注意我们正在使用NumPy的随机库函数来生成随机值。</p><p id="4413" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe rw rx ry rz b">numpy.random.rand(x,y)</code>:这里x是行数，y是列数。它生成[0，1]范围内的输出值。这意味着包含0，但1不包含在值生成中。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tg"><img src="../Images/f01558c8b8bffce46d3c227b4db2041d.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/0*p8N8LKunH8o8aG4U.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图64:用NumPy的库np.random.rand生成随机值</figcaption></figure><p id="f021" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">乙状结肠功能:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi th"><img src="../Images/a981f3166e44f153d52a223fa810242a.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/0*8bgFf1XOwLwFfC0A.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图65:定义我们的sigmoid函数</figcaption></figure><p id="a5b0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> f .用sigmoid函数求导数:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ti"><img src="../Images/eef4d7158343619d9dd9d950eda06c33.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*vcdIb-H-8xlTo8Db.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图66:寻找sigmoid函数的导数</figcaption></figure><p id="b4b7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> g .训练我们的神经网络:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tj"><img src="../Images/7f48000e9299f5c25c0dff1726ea71b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/0*774FZQONnOLMh7Cl.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图67:我们神经网络训练的第一阶段</figcaption></figure><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tk"><img src="../Images/98c37a92f0a224e5e1e7abf5a09fd680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/0*egx1OmmaRJH1m2Gu.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图68:我们神经网络训练的第二阶段</figcaption></figure><p id="1de6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> h .隐藏层的权值:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tl"><img src="../Images/c303a3763a25c48c7949d48ecb4b458c.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/0*_NCPrsbSq4qfBk3U.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图69:显示隐藏层中权重的最终值。</figcaption></figure><p id="61e9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">一、输出层的权重值:</strong></p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tm"><img src="../Images/b15cc5a859118c07b50edafe0bfe0e97.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*KPmO54Qe62fu_voc.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图70:显示我们输出层的最终权重值。</figcaption></figure><p id="c2ab" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> j .最终误差值:</strong></p><p id="fc1c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在对我们的模型进行200，000次迭代训练后，我们最终获得了一个低误差值。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tn"><img src="../Images/77745432a426037a24f740fe4ee4e280.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/0*C0yvF9yD3TqrSEa0.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图71:在200，000次迭代中训练的模型的低误差值</figcaption></figure><p id="5243" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> k .根据训练好的模型进行预测:</strong></p><p id="90f7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">又名</strong>预测(0.5，2)的输出。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi to"><img src="../Images/4e5d72ad4ee99dcfcc604b9bb54b8066.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/0*kSecKq39a0L3ILEF.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图72:预测(0.5，2)的结果。</figcaption></figure><p id="622b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">预测输出更接近于1。</p><p id="495f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> k.b. </strong>预测(0，-1)的输出</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tp"><img src="../Images/8b3d003bb8e4a230d5f17ee5333d200c.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/0*p901oEKK4R9fFvAk.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图73:预测(0，-1)的结果</figcaption></figure><p id="1b96" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">预测的输出非常接近于0。</p><p id="6381" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> k.c. </strong>预测输出为(0，5)</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tq"><img src="../Images/003d85ed7d10ec9a28240ba076ee99a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/0*MrNzpVQmPjAQPaa0.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图74:预测(0，5)的结果。</figcaption></figure><p id="63b7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">预测产量接近1。</p><p id="6eef" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> k.d. </strong>预测输出为(1，1.2)</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div class="gh gi tr"><img src="../Images/c00d6b7f3230d695c9faeed0e86d5d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*GmyKm9q7ug4GUBze.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图75:预测(1，1.2)的结果。</figcaption></figure><p id="248b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">预测产量接近0。</p><p id="c6dd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基于输出值，我们的模型已经完成了预测值的高级工作。</p><p id="2b2d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以按照图76所示的方式分离数据。请注意，这不是分离这些值的唯一可能的方法。</p><figure class="pn po pp pq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ts"><img src="../Images/d46f1004c05d721503f9eceec38c315f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*G0FrV7YlSF6nMPXI.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图76:分离我们价值观的可能方式。</figcaption></figure><p id="f20a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，总而言之，在我们的神经网络上使用隐藏层有助于我们在拥有非线性可分离数据时降低错误率。即使训练时间延长了，我们必须记住我们的目标是做出高精度的预测，这样就可以满足了。</p><h2 id="484b" class="nb nc jj bd nd ne nf dn ng nh ni dp nj lq nk nl nm lu nn no np ly nq nr ns jp bi translated">将所有这些放在一起:</h2><pre class="pn po pp pq gt so rz sp sq aw sr bi"><span id="68a6" class="nb nc jj rz b gy ss st l su sv"># Import required libraries :<br/>import numpy as np# Define input features :<br/>input_features = np.array([[0,0],[0,1],[1,0],[1,1]])<br/>print (input_features.shape)<br/>print (input_features)# Define target output :<br/>target_output = np.array([[0,1,1,0]])# Reshaping our target output into vector :<br/>target_output = target_output.reshape(4,1)<br/>print(target_output.shape)<br/>print (target_output)# Define weights :<br/># 8 for hidden layer<br/># 4 for output layer<br/># 12 total <br/>weight_hidden = np.random.rand(2,4)<br/>weight_output = np.random.rand(4,1)# Learning Rate :<br/>lr = 0.05# Sigmoid function :<br/>def sigmoid(x):<br/> return 1/(1+np.exp(-x))# Derivative of sigmoid function :<br/>def sigmoid_der(x):<br/> return sigmoid(x)*(1-sigmoid(x))# Main logic :<br/>for epoch in range(200000):<br/> # Input for hidden layer :<br/> input_hidden = np.dot(input_features, weight_hidden)<br/> <br/> # Output from hidden layer :<br/> output_hidden = sigmoid(input_hidden)<br/> <br/> # Input for output layer :<br/> input_op = np.dot(output_hidden, weight_output)<br/> <br/> # Output from output layer :<br/> output_op = sigmoid(input_op)#========================================================================<br/> # Phase1<br/> <br/> # Calculating Mean Squared Error :<br/> error_out = ((1 / 2) * (np.power((output_op — target_output), 2)))<br/> print(error_out.sum())<br/> <br/> <br/> # Derivatives for phase 1 :<br/> derror_douto = output_op — target_output<br/> douto_dino = sigmoid_der(input_op) <br/> dino_dwo = output_hiddenderror_dwo = np.dot(dino_dwo.T, derror_douto * douto_dino)# ========================================================================<br/> # Phase 2# derror_w1 = derror_douth * douth_dinh * dinh_dw1<br/> # derror_douth = derror_dino * dino_outh<br/> <br/> # Derivatives for phase 2 :<br/> derror_dino = derror_douto * douto_dino<br/> dino_douth = weight_output<br/> derror_douth = np.dot(derror_dino , dino_douth.T)<br/> douth_dinh = sigmoid_der(input_hidden) <br/> dinh_dwh = input_features<br/> derror_dwh = np.dot(dinh_dwh.T, douth_dinh * derror_douth)# Update Weights<br/> weight_hidden -= lr * derror_dwh<br/> weight_output -= lr * derror_dwo<br/> <br/> <br/># Final values of weight in hidden layer :<br/>print (weight_hidden)# Final values of weight in output layer :<br/>print (weight_output)#Taking inputs :<br/>single_point = np.array([0,-1])<br/>#1st step :<br/>result1 = np.dot(single_point, weight_hidden) <br/>#2nd step :<br/>result2 = sigmoid(result1)<br/>#3rd step :<br/>result3 = np.dot(result2,weight_output)<br/>#4th step :<br/>result4 = sigmoid(result3)<br/>print(result4)#Taking inputs :<br/>single_point = np.array([0,5])<br/>#1st step :<br/>result1 = np.dot(single_point, weight_hidden) <br/>#2nd step :<br/>result2 = sigmoid(result1)<br/>#3rd step :<br/>result3 = np.dot(result2,weight_output)<br/>#4th step :<br/>result4 = sigmoid(result3)<br/>print(result4)#Taking inputs :<br/>single_point = np.array([1,1.2])<br/>#1st step :<br/>result1 = np.dot(single_point, weight_hidden) <br/>#2nd step :<br/>result2 = sigmoid(result1)<br/>#3rd step :<br/>result3 = np.dot(result2,weight_output)<br/>#4th step :<br/>result4 = sigmoid(result3)<br/>print(result4)</span></pre><p id="be8d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">在Google Colab上发布:</strong></p><div class="is it gp gr iu md"><a href="https://colab.research.google.com/drive/1Q4hIQRkabKxZXQPgNaQzYYJocSPlSE-B#scrollTo=nmFrl4yudnrB&amp;line=49&amp;uniqifier=1" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">谷歌联合实验室</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">用Python II构建神经网络—https://towardsai.net/building-neural-nets-with-python</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">colab.research.google.com</p></div></div><div class="mm l"><div class="tt l mo mp mq mm mr ja md"/></div></div></a></div></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h1 id="edf5" class="pa nc jj bd nd pb pc pd ng pe pf pg nj ky ph kz nm lb pi lc np le pj lf ns pk bi translated">7.结论</h1><ul class=""><li id="ed5a" class="nt nu jj lj b lk nv ln nw lq nx lu ny ly nz mc pl ob oc od bi translated">神经网络可以从错误中学习，它们可以产生不限于提供给它们的输入的输出。</li><li id="48eb" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">输入存储在其网络中，而不是数据库中。</li><li id="7655" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">这些网络可以从例子中学习，我们可以预测类似事件的结果。</li><li id="8e24" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">在一个神经元失效的情况下，网络可以检测到故障并且仍然产生输出。</li><li id="363d" class="nt nu jj lj b lk oe ln of lq og lu oh ly oi mc pl ob oc od bi translated">神经网络可以并行执行多项任务。</li></ul></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><figure class="pn po pp pq gt iv gh gi paragraph-image"><a href="https://www.buymeacoffee.com/pratu"><div class="gh gi tu"><img src="../Images/7ba4f2d6d12c187d5442ad1a88605fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-5oP6Xss4YY44doo.png"/></div></a><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">给普拉蒂克买杯咖啡！</figcaption></figure><p id="92a5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">免责声明:</strong>本文表达的观点仅代表作者个人观点，不代表卡内基梅隆大学的观点，也不代表与作者有直接或间接关联的其他公司的观点。这些文章并不打算成为最终产品，而是当前思想的反映，同时也是讨论和改进的催化剂。</p><p id="c675" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过<a class="ae jg" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向AI </a>发布</p></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h1 id="607e" class="pa nc jj bd nd pb pc pd ng pe pf pg nj ky ph kz nm lb pi lc np le pj lf ns pk bi translated">推荐文章</h1><p id="27e7" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">一、<a class="ae jg" href="https://towardsai.net/p/machine-learning/best-datasets-for-machine-learning-and-data-science-d80e9f030279" rel="noopener ugc nofollow" target="_blank">机器学习和数据科学最佳数据集</a> <br/>二。<a class="ae jg" href="http://towardsai.net/ai-salaries" rel="noopener ugc nofollow" target="_blank">艾薪资冲天</a>三世<br/>。<a class="ae jg" href="https://towardsai.net/p/machine-learning/what-is-machine-learning-ml-b58162f97ec7" rel="noopener ugc nofollow" target="_blank">什么是机器学习？</a> <br/>四世。<a class="ae jg" href="https://towardsai.net/ml-masters" rel="noopener ugc nofollow" target="_blank">2020年最佳机器学习硕士项目</a> <br/>五、<a class="ae jg" href="https://towardsai.net/ml-phd" rel="noopener ugc nofollow" target="_blank">2020年最佳机器学习博士项目</a> <br/>六、<a class="ae jg" href="https://towardsai.net/p/machine-learning/best-machine-learning-blogs-6730ea2df3bd" rel="noopener ugc nofollow" target="_blank">最佳机器学习博客</a> <br/>七。<a class="ae jg" href="https://towardsai.net/p/machine-learning/key-machine-learning-ml-definitions-43e837ec6add" rel="noopener ugc nofollow" target="_blank">关键机器学习定义</a> <br/>八。<a class="ae jg" href="https://towardsai.net/ml-captcha" rel="noopener ugc nofollow" target="_blank">用机器学习在0.05秒内破解验证码</a> <br/>九。<a class="ae jg" href="https://towardsai.net/p/machine-learning/machine-learning-vs-ai-important-differences-between-them/robiriondo/3432/" rel="noopener ugc nofollow" target="_blank">机器学习vs. AI及其重要区别</a> <br/>十.<a class="ae jg" href="https://towardsai.net/p/machine-learning/moocs-vs-academia-ensuring-success-starting-in-a-machine-learning-ml-career-304b2e42315e" rel="noopener ugc nofollow" target="_blank">确保成功开创机器学习事业(ML) </a> <br/> XI。<a class="ae jg" href="https://towardsai.net/p/machine-learning/machine-learning-algorithms-for-beginners-with-python-code-examples-ml-19c6afd60daa" rel="noopener ugc nofollow" target="_blank">机器学习算法初学者</a> <br/>十二。<a class="ae jg" href="https://towardsai.net/neural-networks-with-python" rel="noopener ugc nofollow" target="_blank">神经网络从零开始详细用Python代码和数学</a> <br/> XIII。<a class="ae jg" href="https://towardsai.net/p/machine-learning/building-neural-networks-with-python-code-and-math-in-detail-ii-bbe8accbf3d1" rel="noopener ugc nofollow" target="_blank">用Python构建神经网络</a> <br/> XIV。<a class="ae jg" href="https://towardsai.net/p/machine-learning/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e" rel="noopener ugc nofollow" target="_blank">神经网络的主要类型</a> <br/>十五。<a class="ae jg" href="https://towardsai.net/p/machine-learning/monte-carlo-simulation-an-in-depth-tutorial-with-python-bcf6eb7856c8" rel="noopener ugc nofollow" target="_blank">用Python编写的蒙特卡洛模拟教程</a> <br/> XVI。<a class="ae jg" href="https://towardsai.net/p/nlp/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0" rel="noopener ugc nofollow" target="_blank">Python自然语言处理教程</a></p></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h1 id="0b36" class="pa nc jj bd nd pb pc pd ng pe pf pg nj ky ph kz nm lb pi lc np le pj lf ns pk bi translated">引用</h1><p id="5ae0" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">对于学术背景下的归属，请引用该工作为:</p><pre class="pn po pp pq gt so rz sp sq aw sr bi"><span id="db29" class="nb nc jj rz b gy ss st l su sv">Shukla, et al., “Building Neural Networks with Python Code and Math in Detail — II”, Towards AI, 2020</span></pre><h1 id="1eb3" class="pa nc jj bd nd pb qh pd ng pe qi pg nj ky qj kz nm lb qk lc np le ql lf ns pk bi translated">BibTex引文:</h1><pre class="pn po pp pq gt so rz sp sq aw sr bi"><span id="7554" class="nb nc jj rz b gy ss st l su sv">@article{pratik_iriondo_2020, <br/> title={Building Neural Networks with Python Code and Math in Detail — II}, <br/> url={<a class="ae jg" href="https://towardsai.net/building-neural-nets-with-python" rel="noopener ugc nofollow" target="_blank">https://towardsai.net/building-neural-nets-with-python</a>}, <br/> journal={Towards AI}, <br/> publisher={Towards AI Co.}, <br/> author={Pratik, Shukla and Iriondo, Roberto},  <br/> year={2020}, <br/> month={Jun}<br/>}</span></pre><blockquote class="oj"><p id="2824" class="ok ol jj bd om on oo op oq or os mc dk translated">📚你是机器学习新手吗？查看<a class="ae jg" href="https://towardsai.net/machine-learning-algorithms" rel="noopener ugc nofollow" target="_blank">机器学习算法</a>的概述，为初学者提供Python代码示例📚</p></blockquote><h2 id="a577" class="nb nc jj bd nd ne tv dn ng nh tw dp nj lq tx nl nm lu ty no np ly tz nr ns jp bi translated">参考资料:</h2><p id="b91a" class="pw-post-body-paragraph lh li jj lj b lk nv kt lm ln nw kw lp lq pr ls lt lu ps lw lx ly pt ma mb mc im bi translated">[1]统计堆栈交换，【https://stats.stackexchange.com T2】</p><p id="c98b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]神经网络从无到有详细的Python代码和数学——我，普拉蒂克·舒克拉，罗伯特·伊里翁多，【https://towardsai.net/neural-networks-with-python T4】</p></div></div>    
</body>
</html>