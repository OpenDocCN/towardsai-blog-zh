# 理解降维

> 原文：<https://pub.towardsai.net/understanding-dimensionality-reduction-6be2cfb70ab1?source=collection_archive---------1----------------------->

![](img/3b42cb4c16f7aae1f0b118f895defadf.png)

我们都明白，更多的数据意味着更好的 AI。听起来很棒！但是，随着最近的信息爆炸，我们经常会遇到数据过多的问题！我们需要所有的数据。但事实证明这对我们的处理来说太多了。因此，我们需要寻找精简可用数据的方法，以便在不损失价值的情况下对其进行压缩。降维是实现这一目的的重要技术。

考虑基于几个参数预测医疗费用的简单情况。该数据可以包括与一个人的所有负面习惯相关的不同参数——过量摄入烟草、酒精、咖啡因、麻醉剂等。单独累积这些参数可能会很好。但这真的能增加价值吗？这些是独立的参数吗？人们可以猜测它们之间有很多关联。例如，一个酗酒或吸食毒品的人很可能在烟草和咖啡因上很自由。我们需要另一个参数来描述吗？

很容易看出，我们真的不需要所有这些参数。但是，有没有办法只选择其中的两三个呢？这也不太直观。降维可以帮助我们从中提取几个独立的参数。这可以简化我们的预测模型。

那是一个微不足道的例子。但是，在现实生活中，输入中有成百上千个要素是很常见的。在处理这类问题时，降维是一个主要的辅助手段。

有两种降维方法。这是解决这个问题的两种不同模式。没有好的或坏的方法——我们需要根据手头的问题选择其中之一。

*   特征选择——如果某个特定特征的影响几乎是多余的，我们可以放弃它，选择独立影响结果的重要特征。
*   功能组合——这有点复杂。当我们有 N 个不同的特征携带着相当于 M 个特征的信息时，我们需要创建一种方法来将 N 个特征映射到一组新的 M 个特征中。这才是真正的降维。

# 缺少值

这是最简单的降维技术之一。这听起来可能微不足道。但它比人们想象的要有用得多。当我们寻求某种形式的降维时，这应该是我们首先考虑的。本质上，这种技术建议我们丢弃没有足够数据的特征。

听起来不错。但是我们如何决定是否有足够的数据呢？在实际场景中，我们查看可用的数据，以了解其中的内容。这需要软件代码来识别哪些特征具有缺失的数据。然后，根据领域和场景，我们需要确定一个阈值，它可以帮助我们确定我们是否真的可以从给定参数的可用数据中获得任何东西。

通过所有这些处理，我们可以删除一些特征，从而获得更易于管理的数据。

## Python 代码

一旦我们知道我们想要什么，Python 代码就不复杂了。我们只需要遍历数据集中的列列表；检查缺失值的分数。当我们这样做时，我们删除那些丢失值多于定义的阈值的列。

考虑到阈值为 10%，这段代码只选择那些至少有 90%好数据的列。当然，这个阈值是可配置的，我们可以根据可用的数据量来调整它。

# 低方差滤波器

低方差滤波是一种有用的降维算法。为了从概念上理解它，我们可以看看这个概念的世俗等价物。简单地说，你的观点只有在改变时才算数。如果你太始终如一，没有人需要问你的选择！这同样适用于输入参数。只有当输入参数的值发生显著变化时，它才是重要的。如果我们拥有的所有数据都显示某个特定参数的值相同，我们就真的不需要考虑这个特性。

更正式的方法是使用低方差滤波器。方差是给定变量变化量的统计度量。如果方差太低，这意味着它变化不大，因此可以忽略。

## Python 代码

我们可以使用 var()方法在 Python 中实现这一点。但是，在开始之前，我们需要提取数据集中的数值字段。我们可以有不变的非数字参数，但是方差过滤器只对数字数据有意义。

这将只选择方差大于 10 的列。这个阈值必须根据手头的问题来选择。

# 高相关滤波器

这种降维算法试图丢弃与其他输入非常相似的输入。简单来说，如果你的意见和老板一致，就不需要你们中的一个。如果两个输入参数的值总是相同的，这意味着它们表示同一个实体。那么我们不需要两个参数。一个就够了。

用技术术语来说，如果两个输入变量之间有非常高的相关性，我们可以放心地去掉其中一个。

## Python 代码

corr()方法可用于识别字段之间的相关性。当然，在开始之前，我们必须只选择数值字段，因为 corr()方法只处理数值字段。我们可以在非数字字段之间有很高的相关性。但是这个方法只对数值型字段有效。

这给了我们一个可以删除的列的列表。

# 向后和向前选择

这个降维算法看起来可能不是很刺激。这是一种艰难的做事方式。这里，我们取训练数据的一个非常小的子集，并尝试使用它来进行特征选择。我们尝试仅使用几个可用的特征来训练模型，并识别给定特征产生的影响量。

在正向选择中，我们只从一个特征开始——向上确定哪些特征影响最大。在反向选择中，我们走另一条路——一个接一个地消除特征，以识别几乎没有区别的特征。

这些似乎仅仅是学术方法。但是，如果我们有大量的数据，并且这些数据可以用一个小的子集来合理地表示，那么它们会非常有用。它可能不会对构建模型做出重大贡献。但是肯定有助于识别可以从预测和连续训练实现中丢弃的参数。

# 要素分析

因子分析以类似于高相关性过滤器的原理执行维度减少。但是效率更高。如果两个参数之间的相关性太高，我们可以跳过其中一个。但通常，我们最终会得到这样一个场景，其中的相关性相当好，但并没有高到我们可以丢弃其中一个。

例如，考虑四个参数——教育、经验、工资和银行存款余额。这些当然不是独立的。但是他们每个人都贡献了一些信息。我们不能抛弃其中任何一个。因子分析是一种统计技术，可以识别独立的潜在隐藏参数(因此数量较少)。

这些隐藏的参数在现实世界中可能没有任何特别的意义。但是可以将它们组合起来，生成我们所拥有的一组相关参数。

当然，数学实现并不像概念那么简单。但是我们真的不用担心。SktLearn 和许多其他 Python 库为我们提供了一个简单的实现。

## Python 代码

SktLearn 有一个提供简单实现的模块 FactorAnalysis。我们可以对其进行配置，以从可用数据中指定我们想要的组件数量。

这就是我们所需要的。它将提取组件并将其传递到 FA 数据帧。

# 主成分分析

PCA 是一种有趣的降维算法。它基于我们上面讨论的几种技术。我们可用的各种不同的参数通常不是正交的。这意味着，它们之间有相当大的相关性。然而，他们还没有近到可以抛弃一个。对于这样一个问题，因子分析试图识别较少数量的潜在参数，这些参数可以携带几乎所有的信息。

另一方面，PCA 试图生成大量重要性递减的参数。

主成分是可用特征的线性组合。第一个主成分解释了数据集中的大部分差异。第二个主成分试图解释第一个主成分没有解释的差异。第三个试图解释前两个没有解释的差异..诸如此类。

一旦我们发现了这些成分，我们就能确定它们的重要性顺序。我们也知道这些分量是正交的。和他们一起工作要容易得多。

## Python 代码

感谢 SktLearn 等库提供的实用程序，实现 PCA 并不是什么大事。

我们选择的元件数量取决于我们希望它有多精确。这是我们在训练过程中需要优化的另一个超参数。

# 独立成分分析

ICA 是一种类似于 PCA 的降维算法。但是，它更多地借鉴了信息论，而不是统计学。PCA 试图生成正交的分量。但是 ICA 专注于识别独立的组件。

用一个例子来说明，一个人的年龄和他一天燃烧的卡路里数有显著的相关性。它们不是正交的。然而他们是独立的。正如 PCA 试图识别正交的潜在变量一样，ICA 认为所有的参数都是独立的潜在参数的线性混合。

寻找独立参数比正交参数更有意义。自变量是独立变化的变量。虽然它们的值一起移动(意味着它们不是正交的)，但是如果一个变量独立于另一个变量而变化，它们就是独立的。

独立参数肯定会给我们一个更好的模型。因为独立变异是最重要的。

## Python 代码

使用 Python 代码实现独立分量分析非常容易。SktLearn 为我们提供了一个简单的函数。

# 非线性技术

正如人们所料，非线性技术比线性技术更有效也更昂贵。

## t-SNE

降维算法 t-SNE(t-分布式随机邻居嵌入)可以被认为是 PCA 的扩展，它试图识别模式而不仅仅是成分。实现这一点有两种主要方法

*   局部方法:它们将流形上的邻近点映射到低维表示中的邻近点。
*   全局方法:他们试图在所有尺度上保持几何形状，即在低维表示中将流形上的邻近点映射到邻近点，以及将远点映射到远点。

本质上，它计算高维和低维空间中点的概率相似性。然后，它会尽量缩小两者之间的差距。因此，它试图将高维空间中的点放在一起。这就是我们在降维中所需要的。

理解这个概念是主要的任务——Python 代码不是什么大问题

## UMAP

使用 UMAP(均匀流形逼近和投影)的维数减少可以被认为是 PCA 的扩展。它朝着类似的目标努力。但是更加高效和准确

让我们试着对 UMAP 有个直观的了解。它使用 k-最近邻的概念，并使用随机梯度下降优化结果。它首先计算高维空间中的点之间的距离，将其投影到低维空间中，然后计算这个低维空间中的点之间的距离。然后，它使用随机梯度下降来最小化这些距离之间的差异。

如果我们理解这个概念，Python 的实现是简单的