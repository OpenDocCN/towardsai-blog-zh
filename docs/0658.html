<html>
<head>
<title>Scraping Your Medium Stories</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">刮掉你的媒介故事</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/scraping-your-medium-stories-a3a078ab2e7f?source=collection_archive---------2-----------------------#2020-07-07">https://pub.towardsai.net/scraping-your-medium-stories-a3a078ab2e7f?source=collection_archive---------2-----------------------#2020-07-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4fea" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-mining" rel="noopener ugc nofollow" target="_blank">数据挖掘</a>，<a class="ae ep" href="https://towardsai.net/p/category/data-mining" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><div class=""><h2 id="cbe0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">在了解和探索web抓取功能的同时</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/acb0faa44f569c8cb341033cf8f82e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_PsDWU8_LnrX8mGSLVgdPQ@2x.jpeg"/></div></div></figure><p id="f34d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在“过去的好时光”中，媒体允许我们为自己的出版物定制网站。不幸的是，这一功能最近已被否决。</p><p id="35f6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">尽管如此，作为开发人员，我们足智多谋，不那么容易气馁。</p><p id="7861" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们可以使用网络抓取来获取每个媒体报道的细节(标题、描述、链接、特写图片，甚至标签！)并在单独的网站上展示。</p><p id="fe40" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个过程将分为两篇文章。第一部分将只关注抓取文章，在这里我们可以发现网络抓取的力量。在第二篇文章中，我们将使用我们收集的数据向数据库添加和更新条目，并自动化甚至安排收集过程。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="5069" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated">网页抓取</h1><p id="a90b" class="pw-post-body-paragraph la lb iq lc b ld mv ka lf lg mw kd li lj mx ll lm ln my lp lq lr mz lt lu lv ij bi translated">网络抓取是从网站提取数据的过程。我们可以用它来“收集”社交媒体上的帖子、新闻文章，就我们而言，还有媒体报道。为了帮助我们，我们可以使用几个库，包括BeautifulSoup和requests。有了这些，只需要三到四行代码就可以完成几乎所有的事情:</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="b505" class="nf me iq nb b gy ng nh l ni nj">url='<a class="ae nk" href="https://medium.com/feed/@joaquindecastro'" rel="noopener">https://medium.com'</a><br/>response = requests.get(url)<br/>content = BeautifulSoup(response.content, 'html.parser')<br/>some_tag = content.find_all('tag')</span></pre><p id="c2af" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">第一行只是定义了我们将从中获取数据的URL。第二行实际访问URL，就像人们在搜索引擎中粘贴URL一样。第三行解析从第二行返回的响应对象，并给出我们实际可以阅读的HTML内容。最后，第四行在内容中搜索特定的HTML标记。</p><p id="aa7d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">当然，一个网络刮刀很少只有四行。我们可能希望搜索分散在整个网页中的不同标签，并对它们进行格式化，以便人们能够阅读。但是整个过程是一样的。</p><h1 id="843e" class="md me iq bd mf mg nl mi mj mk nm mm mn kf nn kg mp ki no kj mr kl np km mt mu bi translated">用代码弄脏我们的手</h1><p id="5f95" class="pw-post-body-paragraph la lb iq lc b ld mv ka lf lg mw kd li lj mx ll lm ln my lp lq lr mz lt lu lv ij bi translated">因此，不再拖延，让我们进入实际的代码。同样，我们的目标是收集作者的中间故事，并提供每个故事的相关信息。</p><h2 id="36e8" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">网址</h2><p id="1667" class="pw-post-body-paragraph la lb iq lc b ld mv ka lf lg mw kd li lj mx ll lm ln my lp lq lr mz lt lu lv ij bi translated">正如我们在上面看到的，我们需要一个真正的网页，在那里我们可以获得所有这些有趣的数据。我们可以尝试我们的中型个人资料页面(像https://medium.com/@joaquindecastro的T4 T5)，但是那里的信息隐藏在混乱的源代码中，解析起来很麻烦。相反，我们可以使用Medium为每个作者提供的RSS提要(比如<a class="ae nk" href="https://medium.com/feed/@joaquindecastro)" rel="noopener">https://medium.com/feed/@joaquindecastro)</a>。</p><p id="f4ce" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">当我们打开这个页面时，我们可以看到信息已经在XML标记中显示出来了。这不仅更容易刮擦，而且我们可以完全避免检查任何源代码！</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="e615" class="nf me iq nb b gy ng nh l ni nj">url='<a class="ae nk" href="https://medium.com/feed/@joaquindecastro'" rel="noopener">https://medium.com/feed/@joaquindecastro'</a></span></pre><h2 id="07a5" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">使用请求和BeautifulSoup解析</h2><p id="c606" class="pw-post-body-paragraph la lb iq lc b ld mv ka lf lg mw kd li lj mx ll lm ln my lp lq lr mz lt lu lv ij bi translated">接下来，我们可以简单地按照上面概述的过程来获取URL的内容:</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="b30a" class="nf me iq nb b gy ng nh l ni nj">response = requests.get(url)<br/>content = BeautifulSoup(response.content, 'html.parser')</span></pre><p id="1bc3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在，在我们开始检索信息之前，注意一些文本嵌套在XML字符数据块()中。让我们删除它，因为我们将把提要解析为HTML。</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="6ef2" class="nf me iq nb b gy ng nh l ni nj">content = str(content).replace('&lt;![CDATA[','').replace(']]&gt;','')<br/>content = BeautifulSoup(content, 'html.parser')</span></pre><blockquote class="oa ob oc"><p id="aca4" class="la lb od lc b ld le ka lf lg lh kd li oe lk ll lm of lo lp lq og ls lt lu lv ij bi translated"><em class="iq">💡</em>记得先将内容变量转换成一个<strong class="lc ja">字符串</strong>，然后再转换回一个BeautifulSoup <strong class="lc ja">对象</strong>，否则我们会遇到属性和类型错误</p></blockquote><h2 id="5359" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">获取最近文章的列表</h2><p id="e8fe" class="pw-post-body-paragraph la lb iq lc b ld mv ka lf lg mw kd li lj mx ll lm ln my lp lq lr mz lt lu lv ij bi translated">查看RSS提要，我们看到每篇文章的信息都在一个<item>标签中。</item></p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="9914" class="nf me iq nb b gy ng nh l ni nj">A snippet of my RSS feed</span><span id="94db" class="nf me iq nb b gy oh nh l ni nj"><strong class="nb ja">&lt;item&gt;</strong><br/>&lt;title&gt;<br/>&lt;![CDATA[ The Powers of Two: Why Is 1 + 2 + 4 + 8 + … = -1 ]]&gt;<br/>&lt;/title&gt;<br/>&lt;description&gt;<br/>&lt;![CDATA[ &lt;div class="medium-feed-item"&gt;&lt;p class="medium-feed-image"&gt;&lt;a href="<a class="ae nk" href="https://medium.com/cantors-paradise/the-powers-of-two-why-is-1-2-4-8-1-19d8f00be228?source=rss-46b79b6c143b------2" rel="noopener">https://medium.com/cantors-paradise/the-powers-of-two-why-is-1-2-4-8-1-19d8f00be228?source=rss-46b79b6c143b------2</a>"&gt;&lt;img src="<a class="ae nk" href="https://cdn-images-1.medium.com/max/2048/1*geeybYiAeCd1vFkKuO75lA@2x.jpeg" rel="noopener">https://cdn-images-1.medium.com/max/2048/1*geeybYiAeCd1vFkKuO75lA@2x.jpeg</a>" width="2048"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p class="medium-feed-snippet"&gt;On calculating infinite divergent series sums&lt;/p&gt;&lt;p class="medium-feed-link"&gt;&lt;a href="<a class="ae nk" href="https://medium.com/cantors-paradise/the-powers-of-two-why-is-1-2-4-8-1-19d8f00be228?source=rss-46b79b6c143b------2" rel="noopener">https://medium.com/cantors-paradise/the-powers-of-two-why-is-1-2-4-8-1-19d8f00be228?source=rss-46b79b6c143b------2</a>"&gt;Continue reading on Cantor’s Paradise »&lt;/a&gt;&lt;/p&gt;&lt;/div&gt; ]]&gt;<br/>&lt;/description&gt;<br/>&lt;link&gt;<a class="ae nk" href="https://medium.com/cantors-paradise/the-powers-of-two-why-is-1-2-4-8-1-19d8f00be228?source=rss-46b79b6c143b------2" rel="noopener">https://medium.com/cantors-paradise/the-powers-of-two-why-is-1-2-4-8-1-19d8f00be228?source=rss-46b79b6c143b------2</a>&lt;/link&gt;<br/>&lt;guid isPermaLink="false"&gt;<a class="ae nk" href="https://medium.com/p/19d8f00be228" rel="noopener">https://medium.com/p/19d8f00be228</a>&lt;/guid&gt;<br/>&lt;category&gt;<br/>&lt;![CDATA[ science ]]&gt;<br/>&lt;/category&gt;<br/>&lt;category&gt;<br/>&lt;![CDATA[ infinity ]]&gt;<br/>&lt;/category&gt;<br/>&lt;category&gt;<br/>&lt;![CDATA[ math ]]&gt;<br/>&lt;/category&gt;<br/>&lt;category&gt;<br/>&lt;![CDATA[ education ]]&gt;<br/>&lt;/category&gt;<br/>&lt;category&gt;<br/>&lt;![CDATA[ numbers ]]&gt;<br/>&lt;/category&gt;<br/>&lt;dc:creator&gt;<br/>&lt;![CDATA[ Joaquin de Castro ]]&gt;<br/>&lt;/dc:creator&gt;<br/>&lt;pubDate&gt;Wed, 01 Jul 2020 08:36:11 GMT&lt;/pubDate&gt;<br/>&lt;atom:updated&gt;2020-07-01T10:45:28.172Z&lt;/atom:updated&gt;<br/><strong class="nb ja">&lt;/item&gt;</strong></span></pre><p id="f981" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">由于我们想要获得每篇文章的信息<strong class="lc ja">，我们应该遍历所有这些条目标签。</strong></p><p id="83f4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">要获得一个“可迭代”对象，我们可以使用BeautifulSoup的find_all属性，如下所示:</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="96a0" class="nf me iq nb b gy ng nh l ni nj">articles = content.find_all('item')</span></pre><p id="ddf3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然后，我们可以循环这个:</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="cb5f" class="nf me iq nb b gy ng nh l ni nj">for a in articles:<br/>   # GET TITLE<br/>   # GET SUBTITLE<br/>   # GET DATA<br/>...</span></pre><h2 id="e0c4" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">标题</h2><p id="f831" class="pw-post-body-paragraph la lb iq lc b ld mv ka lf lg mw kd li lj mx ll lm ln my lp lq lr mz lt lu lv ij bi translated">首先，获取每篇文章的标题是有意义的。扫描XML文档，我们可以发现所有的标题都在一个<title>标签中</title></p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="cb7c" class="nf me iq nb b gy ng nh l ni nj">&lt;item&gt;<br/><strong class="nb ja">&lt;title&gt;&lt;![CDATA[ The Powers of Two: Why Is 1 + 2 + 4 + 8 + … = -1 ]]&gt;&lt;/title&gt;</strong><br/>...</span></pre><p id="8852" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">要检索它，我们所要做的就是使用BeautifulSoup的‘find’属性(每篇文章只有一个标题，所以不需要使用find_all)</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="e432" class="nf me iq nb b gy ng nh l ni nj">title = a.find('title').text</span></pre><blockquote class="oa ob oc"><p id="7be1" class="la lb od lc b ld le ka lf lg lh kd li oe lk ll lm of lo lp lq og ls lt lu lv ij bi translated"><em class="iq">💡</em>这个。文本只是删除标签，在本例中是&lt;标题&gt;和&lt;/标题&gt;，只留下我们需要的文本</p></blockquote><h2 id="d6bf" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">小标题</h2><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="7e49" class="nf me iq nb b gy ng nh l ni nj">&lt;item&gt;<br/>...<br/>&lt;description&gt;<br/>&lt;![CDATA[ &lt;div class="medium-feed-item"&gt;&lt;p class="medium-feed-image"&gt;&lt;a href="https://medium.com/cantors-paradise/the-powers-of-two-why-is-1-2-4-8-1-19d8f00be228?source=rss-46b79b6c143b------2"&gt;&lt;img src="https://cdn-images-1.medium.com/max/2048/1*geeybYiAeCd1vFkKuO75lA@2x.jpeg" width="2048"&gt;&lt;/a&gt;&lt;/p&gt;<strong class="nb ja">&lt;p class="medium-feed-snippet"&gt;On calculating infinite divergent series sums&lt;/p&gt;</strong>&lt;p class="medium-feed-link"&gt;&lt;a href="https://medium.com/cantors-paradise/the-powers-of-two-why-is-1-2-4-8-1-19d8f00be228?source=rss-46b79b6c143b------2"&gt;Continue reading on Cantor’s Paradise »&lt;/a&gt;&lt;/p&gt;&lt;/div&gt; ]]&gt;<br/>&lt;/description&gt;<br/>...</span></pre><p id="8e5c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在我们的中型文章上加一个副标题是一个很好的做法，这样读者就知道从一篇文章中可以期待什么。回头看看我们的RSS提要，我们可以在一个段落标签中找到我们的副标题。但是页面中有多个段落标签，所以我们需要一种方法来区分它和其他的。这可以通过唯一的class = " medium-feed-snippet "--我们可以看到它只分配给了副标题-和attrs参数来实现。</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="d00c" class="nf me iq nb b gy ng nh l ni nj">for subtitle in a.find_all('p', attrs={'class':'medium-feed-snippet'}):<br/>  subtitle = subtitle.text</span></pre><p id="6563" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们在这里使用find_all是因为使用find会导致属性错误:</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="6136" class="nf me iq nb b gy ng nh l ni nj">subtitle = a.find('subtitle').text<br/>   AttributeError: 'NoneType' object has no attribute 'text'</span></pre><h2 id="9b91" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">特征图像</h2><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="8c51" class="nf me iq nb b gy ng nh l ni nj">&lt;item&gt;<br/>...<br/>&lt;description&gt;<br/>...<br/><strong class="nb ja">&lt;img src="https://cdn-images-1.medium.com/max/2048/1*geeybYiAeCd1vFkKuO75lA@2x.jpeg" width="2048"&gt;</strong><br/>...</span></pre><p id="be81" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">提要还为我们提供了图像源。我们可以应用到目前为止我们所做的，除了我们使用一个['src']键来获得图像的链接:</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="9466" class="nf me iq nb b gy ng nh l ni nj">for img in a.find_all('img', src=True):<br/>  img = img['src']</span></pre><h2 id="303f" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">链接到实际的媒体文章</h2><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="9bae" class="nf me iq nb b gy ng nh l ni nj">&lt;item&gt;<br/>...<br/><strong class="nb ja">&lt;link&gt;https://medium.com/cantors-paradise/the-powers-of-two-why-is-1-2-4-8-1-19d8f00be228?source=rss-46b79b6c143b------2&lt;/link&gt;</strong><br/>...</span></pre><p id="bc55" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">长话短说，我们不会抓取整篇文章的内容，但是我们可以获得中间文章的链接。当我们将这些文章列在一个单独的网站(比如博客或作品集)上时，这是很有帮助的，我们可以在那里将浏览者重定向到完整的文章。同样，除了我们使用['href']键之外，它几乎与特征图像完全相同。</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="81b0" class="nf me iq nb b gy ng nh l ni nj">for link in a.find_all('a', href=True):<br/>  link = link['href']</span></pre><h2 id="26a5" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">出版者</h2><p id="fd42" class="pw-post-body-paragraph la lb iq lc b ld mv ka lf lg mw kd li lj mx ll lm ln my lp lq lr mz lt lu lv ij bi translated">找到发表该故事的出版物可能会有点棘手。没有直接指定发布名称。但是只要稍微动动脑筋，我们就可以看到，通过文章链接可以找到该出版物。因此，给定用户提交的出版物以及相应的出版物slug(medium.com/的“走向人工智能”部分<strong class="lc ja">走向人工智能</strong>)，我们可以如下运行可能的出版物:</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="1305" class="nf me iq nb b gy ng nh l ni nj">if 'towards-artificial-intelligence' in link:<br/>  publisher = "Towards AI"<br/> elif 'cantors-paradise' in link:<br/>  publisher = "Cantor's Paradise"<br/> elif 'mindreform' in link:<br/>  publisher = 'MindReform'</span></pre><h2 id="7eb9" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">标签</h2><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="ad0f" class="nf me iq nb b gy ng nh l ni nj">&lt;category&gt;<br/>&lt;![CDATA[ <strong class="nb ja">science </strong>]]&gt;<br/>&lt;/category&gt;<br/>&lt;category&gt;<br/>&lt;![CDATA[ <strong class="nb ja">infinity </strong>]]&gt;<br/>&lt;/category&gt;<br/>&lt;category&gt;<br/>&lt;![CDATA[ <strong class="nb ja">math </strong>]]&gt;<br/>&lt;/category&gt;<br/>&lt;category&gt;<br/>&lt;![CDATA[ <strong class="nb ja">education </strong>]]&gt;<br/>&lt;/category&gt;<br/>&lt;category&gt;<br/>&lt;![CDATA[ <strong class="nb ja">numbers </strong>]]&gt;<br/>&lt;/category&gt;</span></pre><p id="a3e3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">刮中等标签可以帮助是在我们的网站后来纳入搜索功能。扫描文档时，我们看到一篇文章的标签都嵌套在一个<category>标签中。由于一篇文章可以有多个标签，我们将使用find_all并将每个标签附加到一个列表中，如下所示</category></p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="eec4" class="nf me iq nb b gy ng nh l ni nj">tags = []<br/> for tag in a.find_all('category'):<br/>  tag = tag.text<br/>  tags.append(tag)</span></pre><h2 id="3aec" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">日期</h2><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="f20e" class="nf me iq nb b gy ng nh l ni nj">&lt;item&gt;<br/>...<br/><strong class="nb ja">&lt;pubDate&gt;Wed, 01 Jul 2020 08:36:11 GMT&lt;/pubDate&gt;</strong><br/>...</span></pre><p id="79a7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最后但同样重要的是，日期。我们在解析日期时必须小心。我们不仅要注意文档中的日期格式，还要注意我们希望将其转换成的格式(尤其是在处理数据库时)。我们将使用<code class="fe oi oj ok nb b">datatime</code>模块来完成这项工作。</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="6146" class="nf me iq nb b gy ng nh l ni nj">import datetime # DON'T FORGET!</span><span id="e5a6" class="nf me iq nb b gy oh nh l ni nj">date = a.find('pubdate').text<br/> date = str(date).replace(' GMT','') # REMOVE GMT STRING<br/> date = datetime.strptime(date, '%a, %d %b %Y %H:%M:%S') # CONVERT date TO DATETIME OBJECT<br/> date = date.strftime('%Y-%m-%d') # CONVERT TO DJANGO DateField FORMAT</span></pre><p id="1055" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">正如我们在<pubdate>标签中看到的，日期的格式是“工作日，日，月，年，小时，分钟，秒，GMT”。GMT可以忽略，所以我们将完全删除它。有了这些，让我们把剩下的数据转换成<code class="fe oi oj ok nb b">strptime()</code>可以理解的代码(查看<a class="ae nk" href="https://www.programiz.com/python-programming/datetime/strftime#format-code" rel="noopener ugc nofollow" target="_blank">这个链接</a>获取完整的日期格式代码)。</pubdate></p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="81e3" class="nf me iq nb b gy ng nh l ni nj">'%a, %d %b %Y %H:%M:%S'</span></pre><blockquote class="oa ob oc"><p id="d033" class="la lb od lc b ld le ka lf lg lh kd li oe lk ll lm of lo lp lq og ls lt lu lv ij bi translated"><em class="iq">💡</em>注意RSS提要中的十进制数字是用零填充的(例如01而不是1)</p></blockquote><p id="fc32" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最后，让我们将它转换成我们想要的格式，我只是遵循了Django的默认日期字段格式，即“年-月-日”或</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="364f" class="nf me iq nb b gy ng nh l ni nj">'%Y-%m-%d'</span></pre><h2 id="9b4e" class="nf me iq bd mf nq nr dn mj ns nt dp mn lj nu nv mp ln nw nx mr lr ny nz mt iw bi translated">组织数据</h2><p id="949e" class="pw-post-body-paragraph la lb iq lc b ld mv ka lf lg mw kd li lj mx ll lm ln my lp lq lr mz lt lu lv ij bi translated">仅此而已！</p><p id="ea11" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在我们有了每篇媒体文章的精彩数据。不幸的是，提要并没有给出我们所有的文章，但这在将来应该不是问题，因为我们会定期收集这些文章。</p><p id="89e3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">为了保持输出清晰，让我们将所有这些数据传递到每篇文章的字典中，然后再传递到一个列表中:</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="053d" class="nf me iq nb b gy ng nh l ni nj">   ... # EVERYTHING WE DID UP THERE<br/>   # PUT ALL INFO IN A DICTIONARY<br/>   article_info = {<br/>   'title':title,<br/>   'img':img,<br/>   'date':date,<br/>   'subtitle':subtitle,<br/>   'publisher':publisher,<br/>   'link':link,<br/>   'tags':tags <br/>   }<br/>   # PUT article_info IN article_all<br/>   articles_all.append(article_info)<br/>print(articles_all)</span></pre><h1 id="3bb5" class="md me iq bd mf mg nl mi mj mk nm mm mn kf nn kg mp ki no kj mr kl np km mt mu bi translated">最终代码</h1><p id="7d1c" class="pw-post-body-paragraph la lb iq lc b ld mv ka lf lg mw kd li lj mx ll lm ln my lp lq lr mz lt lu lv ij bi translated">这是我们所有的代码。最终结果是我们所有文章的列表——每篇文章都由包含标题、副标题、图像、链接、日期、出版商和标签的字典表示。</p><pre class="kp kq kr ks gt na nb nc nd aw ne bi"><span id="b332" class="nf me iq nb b gy ng nh l ni nj">import requests<br/>from bs4 import BeautifulSoup<br/>import pandas<br/>from datetime import datetime</span><span id="62e8" class="nf me iq nb b gy oh nh l ni nj"># GET ACTUAL CONTENT<br/>url='<a class="ae nk" href="https://medium.com/feed/@joaquindecastro'" rel="noopener">https://medium.com/feed/@joaquindecastro'</a><br/>response = requests.get(url)<br/>content = BeautifulSoup(response.content, 'html.parser')</span><span id="05c2" class="nf me iq nb b gy oh nh l ni nj"># REMOVE UNNECESSARY STRINGS<br/>content = str(content).replace('&lt;![CDATA[','').replace(']]&gt;','')<br/>content = BeautifulSoup(content, 'html.parser')</span><span id="5134" class="nf me iq nb b gy oh nh l ni nj"># GET LIST OF ALL ARTICLES <br/># FOUND IN &lt;item&gt; TAG<br/>articles = content.find_all('item')</span><span id="c108" class="nf me iq nb b gy oh nh l ni nj"># CREATE ARRAY FOR ALL article_info<br/>articles_all = []</span><span id="da54" class="nf me iq nb b gy oh nh l ni nj"># GET ARTICLE INFO PER ARTICLE<br/>for a in articles:<br/> # GET TITLE<br/> title = str(a.find('title').text)<br/> # GET SUBTITLE<br/> for subtitle in a.find_all('p', attrs={'class':'medium-feed-snippet'}):<br/>  subtitle = subtitle.text<br/> # GET IMG SOURCE<br/> for img in a.find_all('img', src=True):<br/>  img = img['src']<br/> # GET DATE<br/> date = a.find('pubdate').text<br/> date = str(date).replace(' GMT','') # REMOVE GMT STRING<br/> date = datetime.strptime(date, '%a, %d %b %Y %H:%M:%S') # CONVERT date TO DATETIME OBJECT<br/> date = date.strftime('%Y-%m-%d') # CONVERT TO DJANGO DateField FORMAT<br/> # GET LINK TO MEDIUM ARTICLE<br/> for href in a.find_all('a', href=True):<br/>  link = href['href']<br/> # GET PUBLISHER BASED ON LINK (eg medium.com/my-publication/article-slug)<br/> if 'towards-artificial-intelligence' in link:<br/>  publisher = "Towards AI"<br/> elif 'cantors-paradise' in link:<br/>  publisher = "Cantor's Paradise"<br/> elif 'mindreform' in link:<br/>  publisher = 'MindReform'<br/> # GET LIST OF TAGS<br/> tags = []<br/> for tag in a.find_all('category'):<br/>  tag = tag.text<br/>  tags.append(tag)<br/> # PUT ALL INFO IN A DICTIONARY<br/> article_info = {<br/> 'title':title,<br/> 'img':img,<br/> 'date':date,<br/> 'subtitle':subtitle,<br/> 'publisher':publisher,<br/> 'link':link,<br/> 'tags':tags <br/> }<br/> # PUT article_info IN article_all<br/> articles_all.append(article_info)<br/>print(articles_all)</span></pre><p id="acb2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这里是Github库:<a class="ae nk" href="https://github.com/JoaquindeCastro/medium_scraper" rel="noopener ugc nofollow" target="_blank">https://github.com/JoaquindeCastro/medium_scraper</a></p></div></div>    
</body>
</html>