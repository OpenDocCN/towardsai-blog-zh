<html>
<head>
<title>Adaptive Learning for Time Series Forecasting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间序列预测的自适应学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/adaptive-learning-for-time-series-forecasting-b34e640b865b?source=collection_archive---------0-----------------------#2022-10-01">https://pub.towardsai.net/adaptive-learning-for-time-series-forecasting-b34e640b865b?source=collection_archive---------0-----------------------#2022-10-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="9751" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">时间序列预测应用在从能源到医疗保健等各种行业中的重要性不言而喻。因此，我们直接进入正题。我们在处理时间序列数据集时可能面临的复杂而困难的挑战之一是它们在统计特征上的多样性，这可能会导致它们的分布发生变化，从而导致各种行为，使它们难以通过模型来理解。本文提供了一个两阶段模型来处理时间协变量移位(TCS)；为了简单起见，我们称之为ADaRNN(自适应学习和RNN的结合)。你可以找到所有章节的详尽解释；同时，你可以看到数学公式，以便更好地理解。坦率地说，这是我们第一次可以从分布的角度研究时间序列数据集。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/69d0dcbeb93b7116a3357ca5faadb71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Fk-wT2ExFLJ2cX-TZl-Deg.gif"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated"><a class="ae la" href="https://giphy.com/gifs/MentoringMinds-lcj8Wjb92d4cBj45hg" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="1660" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">问题)</h2><p id="8b4a" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">时间序列的特征应该是随时间而变化的，所以我们称这类数据(作为时间的函数)<strong class="js iu">非平稳</strong>。我们不能处理静态数据；我的意思是，如果数据不随着时间的推移而改变，那还有什么意义呢(<em class="lz">真的很简单</em>！) ?？！！所以，这种不稳定的事实相当于在一段时间内有不同的分布。这是时间序列捕捉的一个要点。为了更好的理解，你可以看看下图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/ba4a5c11730e72bf35b1ce6bbdd52ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*nRTDUvHWBoyRc9ZeCqrFmw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图一。[ <a class="ae la" href="https://arxiv.org/pdf/2108.04443.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="e6ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图1中的数据集是一个多变量时间序列数据集，分为三个时段A、B、C和Test。我们很容易看到这些分布是不平等的。</p><h2 id="55bb" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">解决方案)</h2><p id="19a9" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">阿达恩</p><h2 id="f3cd" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">解读)</h2><p id="03a1" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">我们可以将RNNs视为最流行的算法架构之一，因此我们可以从使用其类型中最流行的开始。为了防止这篇文章更长，我不打算详细描述RNNs不过可以给<a class="ae la" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">斯坦福深度学习单</a>关于RNNs。</p><blockquote class="mb mc md"><p id="ec3b" class="jq jr lz js b jt ju jv jw jx jy jz ka me kc kd ke mf kg kh ki mg kk kl km kn im bi translated">别担心，我知道应用变形金刚有很多进步，但这只是一个开始；我们稍后再讨论。</p></blockquote><p id="867a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嗯，嗯，嗯；我告诉过你AdaRNN有两个步骤:</p><ol class=""><li id="6f9c" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn mm mn mo mp bi translated"><strong class="js iu">时间分布特征化(TDC)算法</strong></li><li id="e07b" class="mh mi it js b jt mq jx mr kb ms kf mt kj mu kn mm mn mo mp bi translated"><strong class="js iu">时间分布匹配(TDM)算法</strong></li></ol><p id="e9fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总而言之:</p><p id="91f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> TDC </strong>将训练数据集分成K个最多样化的区间。(具有最大分布间隙)然后:<strong class="js iu"> TDM </strong>利用RNN减少发散分布。</p><blockquote class="mb mc md"><p id="2885" class="jq jr lz js b jt ju jv jw jx jy jz ka me kc kd ke mf kg kh ki mg kk kl km kn im bi translated">正如我所说，这是我们第一次从分布角度考虑时间序列！！那么这意味着什么呢？</p></blockquote><h2 id="202c" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">分配</h2><p id="056d" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">通常，有两种主要的分发方法:</p><ol class=""><li id="4577" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn mm mn mo mp bi translated"><strong class="js iu">领域综合(DG)算法</strong></li><li id="634a" class="mh mi it js b jt mq jx mr kb ms kf mt kj mu kn mm mn mo mp bi translated"><strong class="js iu">域自适应(DA)算法</strong></li></ol><p id="646b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总而言之:</p><p id="3023" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> DG </strong>表示模型在处理未知数据和处理几个相互关联的不同领域时的表现，而<strong class="js iu"> DA </strong>则试图在包含大量异常或意外数据行为的输出数据上改进模型。这两个不同之处在于，一个访问<strong class="js iu">测试数据集</strong>而另一个不访问。然而，本文中的情况并非如此，因为我们用我们的方法解决了这个问题。:)</p><h1 id="82b2" class="mv lc it bd ld mw mx my lg mz na nb lj nc nd ne lm nf ng nh lp ni nj nk ls nl bi translated">恰当的例子</h1><h2 id="213f" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">一般来说</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/cc25fd9d48ec056bbdfc8adcb3b51c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*_JsaYg_SCP_-Jc6uaqe8BQ.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图二。AdaRNN架构[ <a class="ae la" href="https://arxiv.org/pdf/2108.04443.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="78e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以很容易地看到两个<strong class="js iu">颜色框</strong>表达了这个模型的两个主要步骤，<strong class="js iu"> TDC </strong>和<strong class="js iu"> TDM </strong>，第一个<strong class="js iu">通过将它分解为<strong class="js iu"> </strong>过程来指定分布</strong>，随后<strong class="js iu">调整分布</strong>以馈给主模型<strong class="js iu"> M </strong>用于<strong class="js iu">预测</strong>。最后，用自适应RNN模型进行预测。</p><p id="a6e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一分！！合理的是学习/练习/锻炼，让自己做好最坏情况的准备；所以，对于AdaRNN来说是真的，研究人员用困难的任务(我指的是分配)训练模型。</p><h2 id="30a6" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">TDC(时间分布特征化)</h2><p id="99f5" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">TDC的基本原理是最大熵原理；它试图最大限度地利用时间序列中的共享知识，是如何做到的？？通过在广泛的分布范围上训练模型；换句话说，具有最大数量的非相似分布。逻辑很简单，如果模型可以处理这一点，它将能够管理未来/看不见的数据。请看图3:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b6a39473b391665e2be6ad9f27c196f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*IiJvzW_0NZst0o8gc00U2g.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图3。时间分布特征(TDC) [ <a class="ae la" href="https://arxiv.org/pdf/2108.04443.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="60bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从数学角度来看，我们有等式1:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi no"><img src="../Images/f510cd8b0feeebcc6f249b99f8ae0cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IavBLMSvW76AyiyZ1AOoeQ.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">等式1。</figcaption></figure><p id="13d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们使用<strong class="js iu">超婴儿车</strong>来防止<strong class="js iu">过度分割</strong>，使用<strong class="js iu">预定义婴儿车</strong>来防止<strong class="js iu">琐碎解决方案</strong>。</p><blockquote class="nt"><p id="5cb9" class="nu nv it bd nw nx ny nz oa ob oc kn dk translated">让我们不要在数学上迷惑自己。🤯😅</p></blockquote><p id="f4f8" class="pw-post-body-paragraph jq jr it js b jt od jv jw jx oe jz ka kb of kd ke kf og kh ki kj oh kl km kn im bi translated">等式1的目标是最大化距离d(它可以是从欧几里得到etc的任何类型的距离函数。)如果我们选择一个好的距离度量，我们可以通过使用<strong class="js iu">动态规划</strong>来完成TDC的目的地。</p><p id="0275" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这项研究中，研究人员将数据集分成N=10个比例，我们不能再分成更小的比例。</p><ul class=""><li id="4fb0" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn oi mn mo mp bi translated"><strong class="js iu"> K </strong>的显著<strong class="js iu">大</strong>或<strong class="js iu">小</strong>值会导致<strong class="js iu">没有期望的</strong>型号<strong class="js iu">性能</strong>。</li></ul><h2 id="5f5f" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">TDM(时间分布匹配)</h2><p id="74c9" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">我们使用TDM通过使用各种分布周期来研究共享的公共知识。其损失函数可表示如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oj"><img src="../Images/45e336db76ab2c5a757e48e872d674fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DoVCBv8n_BxYoNifdcQx5A.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">等式2。</figcaption></figure><p id="7fd8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这项研究中，分布的正则化是在RNN模型的最终输出上实现的。等式3是pare (Di，Dj)的最终隐藏状态的周期式分布匹配:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ok"><img src="../Images/54c75deb0b2fc7035b77069c46d987bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cY4CeJJ85IeLduJw9Ic5Ag.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">等式3。</figcaption></figure><p id="88d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，对于隐藏状态，这个正则化项在依赖于时间时丢失。为什么？？因为每个隐藏状态单独地包括关于分布的全部信息的一部分。</p><blockquote class="mb mc md"><p id="9c21" class="jq jr lz js b jt ju jv jw jx jy jz ka me kc kd ke mf kg kh ki mg kk kl km kn im bi translated">因此，这是一个问题！！不是吗？？我们能做什么？？🤔</p></blockquote><p id="2955" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嗯，在构建分布匹配正则化子的过程中，我们必须考虑每个<strong class="js iu"> <em class="lz">隐藏的</em>状态</strong>。那么这意味着什么呢？？首先，看看TDM架构:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/cb5b3579827419c87690b56c1fd800cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*JPIhhsMJt1jDtWW5MNnYww.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">图4。时间分布匹配(TDM) [ <a class="ae la" href="https://arxiv.org/pdf/2108.04443.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="a76d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">TDM的要点是同时预测时间序列数据和匹配RNN单元之间的分布。有一个名为<em class="lz">重要性向量</em>的参数，用于装备模型以减少分布差异。等式4给出了时序分布匹配的损失:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/cfdc36f8c4c400712847af2aa29dcd57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*iRK4BsNKtg5TqoWt27vVCQ.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">Eq 4。</figcaption></figure><p id="8317" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过使用等式4和2，我们可以导出等式5 ( <strong class="js iu">我们的模型</strong>的终端目标函数):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi on"><img src="../Images/265e0bf9a0e5471fb6830386e82a114c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_YWqkX6VGzHmjqwYehZFDA.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">Eq 5。</figcaption></figure><blockquote class="mb mc md"><p id="f149" class="jq jr lz js b jt ju jv jw jx jy jz ka me kc kd ke mf kg kh ki mg kk kl km kn im bi translated">我不会详细描述它数学原理；但是，你可以通读主要论文。</p></blockquote></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><h1 id="9518" class="mv lc it bd ld mw ov my lg mz ow nb lj nc ox ne lm nf oy nh lp ni oz nk ls nl bi translated">资料组</h1><p id="685b" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">模型评估(分类和回归)使用了不同行业的四个时间序列数据集(<strong class="js iu">人类活动识别、空气质量预测、家庭用电量、</strong>和<strong class="js iu">股价预测</strong>)。您可以在表1中看到数据集大小的汇总。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/24982a76afc0e1807b4b64bdf173b851.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*ohPHD45-F4ivL5COiTJWDw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">表1。[ <a class="ae la" href="https://arxiv.org/pdf/2108.04443.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><h1 id="85bc" class="mv lc it bd ld mw ov my lg mz ow nb lj nc ox ne lm nf oy nh lp ni oz nk ls nl bi translated">结果</h1><p id="c6e9" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">AdaRNN与其他深度/机器学习模型进行了比较，包括:</p><ol class=""><li id="f155" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn mm mn mo mp bi translated">传统的(ARIMA、先知、GRU等。)</li><li id="e318" class="mh mi it js b jt mq jx mr kb ms kf mt kj mu kn mm mn mo mp bi translated">最新款(条纹等。)</li><li id="94a2" class="mh mi it js b jt mq jx mr kb ms kf mt kj mu kn mm mn mo mp bi translated">变形金刚(电影名)</li></ol><p id="c4c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以在表2中看到ADaRNN与其他模型的结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pb"><img src="../Images/f6544422cd07be41cf7c9674c56314aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*jYHBucqZepV3kfhUeJwGwg.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">表二。[ <a class="ae la" href="https://arxiv.org/pdf/2108.04443.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="8806" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">五个指标</strong>被用于<strong class="js iu">评估</strong>，包括<strong class="js iu"> ACC(准确度)、P(精度)、recall (R)、F1和AUC(曲线下面积</strong>)。</p><p id="6a8c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如您所见，AdaRNN与同类产品相比显示出更好的结果。</p><p id="3386" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本研究中，该方法被应用于每个自我注意块中的变压器(AdaTransformer)。如表3所示，与原始版本(Vanilla Transformer)相比，AdaTransformer的结果更好:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/64e09000a3afacd17a0b131c4149f682.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*zQFH5euS_zh4FTaHHdIQIA.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">表3。[ <a class="ae la" href="https://arxiv.org/pdf/2108.04443.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="94db" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated"><strong class="ak">结尾</strong></h2></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><blockquote class="mb mc md"><p id="61e9" class="jq jr lz js b jt ju jv jw jx jy jz ka me kc kd ke mf kg kh ki mg kk kl km kn im bi translated">在以后的文章中，我将向您展示如何为我们的模型编写TDM和TDC代码，以及如何实现AdaRNN。</p></blockquote></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><blockquote class="mb mc md"><p id="27f3" class="jq jr lz js b jt ju jv jw jx jy jz ka me kc kd ke mf kg kh ki mg kk kl km kn im bi translated">来源是<a class="ae la" href="https://arxiv.org/pdf/2108.04443.pdf" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="7f14" class="jq jr lz js b jt ju jv jw jx jy jz ka me kc kd ke mf kg kh ki mg kk kl km kn im bi translated">你可以<strong class="js iu">在<strong class="js iu"> Twitter </strong> <a class="ae la" href="https://twitter.com/reza__yazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a>或者<strong class="js iu"> LinkedIn </strong> <a class="ae la" href="http://www.linkedin.com/in/rezayazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a>联系</strong>我。最后，如果你觉得这篇文章有趣并且有用，你可以在<strong class="js iu">媒体</strong>上<strong class="js iu">关注</strong>我以获取更多来自我的文章。</p></blockquote></div></div>    
</body>
</html>