<html>
<head>
<title>Different Data Splitting Cross-Validation Strategies with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python的不同数据分割交叉验证策略</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/different-data-splitting-cross-validation-strategies-with-python-ec7cd93764ac?source=collection_archive---------0-----------------------#2021-03-17">https://pub.towardsai.net/different-data-splitting-cross-validation-strategies-with-python-ec7cd93764ac?source=collection_archive---------0-----------------------#2021-03-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8069" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="b610" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">机器学习中的训练集、验证集和测试集</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/beb45a2faef0cdbb2344590d163f58ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*qhqvjQdxUViFfD0q69IpBA.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">将数据集拆分为训练集、验证集和测试集。作者的照片</figcaption></figure><p id="f9ec" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在本文中，我们将介绍均匀分割数据集的交叉验证方法，以获得良好的预测性能。</p><p id="ee7c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们看到我们的数据如何在我们的机器学习算法中分裂成训练集和测试集。但是，如果你曾经试图认为这两套足以建立生产模型。在我看来，我们应该在预测测试集之前包含验证集。这很重要，因为如果模型过度拟合，那么我们可以在验证集检查后调整超参数，并为我们的测试集设置好的参数。</p><div class="lz ma gp gr mb mc"><a rel="noopener  ugc nofollow" target="_blank" href="/z-statistics-t-statistics-p-statistics-are-still-confusing-you-87557047e20a"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd jd gy z fp mh fr fs mi fu fw jc bi translated">Z-统计量，T-统计量，P-统计量还在迷惑你？</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">机器学习统计学中的定义和概念</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">pub.towardsai.net</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq kx mc"/></div></div></a></div><p id="36e6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Python的例子:</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="29ab" class="mw mx it ms b gy my mz l na nb"># Splitting the dataset into the Training set and Test set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)</span></pre><p id="fe56" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">SVM的例子</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="3736" class="mw mx it ms b gy my mz l na nb">from sklearn.svm import SVC<br/>classifier = SVC(kernel = 'linear', random_state= 0, C=1)<br/>classifier.fit(X_train, y_train)</span></pre><p id="02ed" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">学习对人生的成长非常重要，至于算法，如果他们在相同的数据上学习和测试，那就是一个错误。</p><p id="599c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当我们尝试建立模型时，验证集的作用就出现了，在评估部分，超参数仍然需要尽可能精确，以获得最佳性能。所以，可以有一个<strong class="lf jd"> <em class="nc">的数据泄露</em> </strong>的测试集，这个模型不再是一个好的估计器。这个场景的解决方案是在验证集上评估fit模型，然后在测试集上进行最终评估。</p><p id="a450" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有三个集合会出现一个问题，即我们的训练数据变得更少，对于我们的模型来说不是一个好的选择。因此，交叉验证出现在图片中，我们不需要单独的验证集。在交叉验证中，训练集被分成小的<strong class="lf jd"> <em class="nc"> k个折叠</em> </strong>，训练将发生在k-1个折叠上，剩余的用于评估。</p><p id="bb68" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从循环中测量平均性能，然后在测试中进行最终评估。</p><p id="4b66" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">虽然，它需要更多的时间，但非常有用，因为所有的训练数据都用于训练和第一阶段的评估。</p><p id="8d60" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">使用<strong class="lf jd"><em class="nc">cross _ val _ score</em></strong>测量不同交叉验证分数的示例</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="90bd" class="mw mx it ms b gy my mz l na nb">from sklearn.model_selection import cross_val_score<br/>clf = svm.SVC(kernel='linear', C=1)<br/>scores = cross_val_score(clf, X, y, cv=5)<br/>scores<br/>array([0.94, 0.98, 0.96, 0.98, 1. ])</span></pre><p id="0063" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们也可以得到分数的平均值</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="f40b" class="mw mx it ms b gy my mz l na nb">scores.mean()</span><span id="57ae" class="mw mx it ms b gy nd mz l na nb">#output:<br/>0.97</span></pre><p id="0a49" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面是另一个分数计算，通过<strong class="lf jd"><em class="nc">shuffles split</em></strong>方法将数据分为训练集和测试集。</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="c960" class="mw mx it ms b gy my mz l na nb">import numpy as np<br/>from sklearn.model_selection import ShuffleSplit<br/>X = np.array([[2, 5], [3, 4], [4, 5], [8, 4], [2, 6], [6, 7]])<br/>y = np.array([2, 1, 2, 1, 2, 1])<br/>rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)</span><span id="39b6" class="mw mx it ms b gy nd mz l na nb">print(rs.get_n_splits(X))</span></pre><p id="74f7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个rs变量可以给<strong class="lf jd"> <em class="nc"> cross_val_score。</em> </strong></p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="64c4" class="mw mx it ms b gy my mz l na nb">cross_val_score(clf, X, y, cv=rs)<br/>array([0.944, 0.966, 1., 0.977, 1.])</span></pre><p id="d8ae" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">不同的交叉验证迭代器如下所示:</p><ul class=""><li id="e87b" class="ne nf it lf b lg lh lj lk lm ng lq nh lu ni ly nj nk nl nm bi translated"><strong class="lf jd"> K倍</strong></li><li id="7bf4" class="ne nf it lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><strong class="lf jd">重复K倍</strong></li><li id="bbda" class="ne nf it lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><strong class="lf jd">漏掉一个(LOO) </strong></li><li id="bc78" class="ne nf it lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><strong class="lf jd">省去P(LPO)</strong></li><li id="4b96" class="ne nf it lf b lg nn lj no lm np lq nq lu nr ly nj nk nl nm bi translated"><strong class="lf jd">分层的K形褶皱</strong></li></ul><div class="lz ma gp gr mb mc"><a rel="noopener  ugc nofollow" target="_blank" href="/reading-different-data-inputs-in-machine-learning-with-python-ddae1d73c157"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd jd gy z fp mh fr fs mi fu fw jc bi translated">使用Python读取机器学习中的不同数据输入</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">用python读取输入的有用方法</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">pub.towardsai.net</p></div></div><div class="ml l"><div class="ns l mn mo mp ml mq kx mc"/></div></div></a></div><h2 id="5996" class="mw mx it bd nt nu nv dn nw nx ny dp nz lm oa ob oc lq od oe of lu og oh oi iz bi translated">k倍</h2><p id="059b" class="pw-post-body-paragraph ld le it lf b lg oj kd li lj ok kg ll lm ol lo lp lq om ls lt lu on lw lx ly im bi translated">这是一个非常简单的过程，将样本分成k个样本。训练在k-1个样本上进行，左边的样本用于测试集。</p><p id="9c30" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Python的例子:</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="77fe" class="mw mx it ms b gy my mz l na nb">from sklearn.model_selection import KFold</span><span id="2378" class="mw mx it ms b gy nd mz l na nb">X = [2,4,6,3]<br/>kf = KFold(n_splits=4)<br/>for train, test in kf.split(X):<br/>    print("%s %s" % (train, test))</span><span id="b0ae" class="mw mx it ms b gy nd mz l na nb">#output:<br/>[1 2 3] [0]<br/>[0 2 3] [1]<br/>[0 1 3] [2]<br/>[0 1 2] [3]</span></pre><p id="5742" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">上面的输出显示第0个元素用于测试，其余用于训练集。类似地，迭代发生4次，并且训练在所有元素上均匀地发生。注意n_splits的数量不应该大于x中的元素总数。</p><h2 id="1141" class="mw mx it bd nt nu nv dn nw nx ny dp nz lm oa ob oc lq od oe of lu og oh oi iz bi translated">重复K倍</h2><p id="895e" class="pw-post-body-paragraph ld le it lf b lg oj kd li lj ok kg ll lm ol lo lp lq om ls lt lu on lw lx ly im bi translated">该过程用于重复k-fold n次，即，在上述示例中，输出显示4次迭代，而在该过程中，k-fold在这4次迭代之后再次重复。</p><p id="8f10" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Python的例子:</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="2cf8" class="mw mx it ms b gy my mz l na nb">from sklearn.model_selection import RepeatedKFold<br/>X = [2,4,6,3]<br/>rkf = RepeatedKFold(n_splits=2, n_repeats=2)<br/>for train, test in rkf.split(X):<br/>    print("%s %s" % (train, test))</span><span id="bed8" class="mw mx it ms b gy nd mz l na nb">#output:<br/>[1 3] [0 2]<br/>[0 2] [1 3]<br/>[0 3] [1 2]<br/>[1 2] [0 3]</span></pre><p id="11d6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">上面的输出显示，k-fold的迭代次数是两次，因为n_splits=2，k-fold重复了两次，因为n_repeats=2。</p><h2 id="cc00" class="mw mx it bd nt nu nv dn nw nx ny dp nz lm oa ob oc lq od oe of lu og oh oi iz bi translated">漏掉一个(LOO)</h2><p id="9500" class="pw-post-body-paragraph ld le it lf b lg oj kd li lj ok kg ll lm ol lo lp lq om ls lt lu on lw lx ly im bi translated">这与k-fold相同，在k-fold中，元素的数量等于拆分的数量。在这个过程中，在每一次迭代中，只有一个测试集在训练集中。不需要指定拆分的数量。</p><p id="b74d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Python的例子:</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="3a63" class="mw mx it ms b gy my mz l na nb">from sklearn.model_selection import LeaveOneOut<br/>X = [2,4,6,3]<br/>loo = LeaveOneOut()<br/>for train, test in loo.split(X):<br/>    print("%s %s" % (train, test))</span><span id="6559" class="mw mx it ms b gy nd mz l na nb">#output:<br/>[1 2 3] [0]<br/>[0 2 3] [1]<br/>[0 1 3] [2]<br/>[0 1 2] [3]</span></pre><p id="0936" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">与k-fold相比，100给出了更高方差且在计算上更昂贵。</p><h2 id="1ee6" class="mw mx it bd nt nu nv dn nw nx ny dp nz lm oa ob oc lq od oe of lu og oh oi iz bi translated">省去P(LPO)</h2><p id="9ebf" class="pw-post-body-paragraph ld le it lf b lg oj kd li lj ok kg ll lm ol lo lp lq om ls lt lu on lw lx ly im bi translated">在这个过程中，分裂发生在训练和测试集中的非重复样本基础上。</p><p id="d678" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Python中的示例:</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="0c8e" class="mw mx it ms b gy my mz l na nb">from sklearn.model_selection import LeavePOut<br/>X = [2,4,6,3]<br/>lpo = LeavePOut(p=2)<br/>for train, test in lpo.split(X):<br/>    print("%s %s" % (train, test))</span><span id="59e8" class="mw mx it ms b gy nd mz l na nb">#output:<br/>[2 3] [0 1]<br/>[1 3] [0 2]<br/>[1 2] [0 3]<br/>[0 3] [1 2]<br/>[0 2] [1 3]<br/>[0 1] [2 3]</span></pre><p id="3eeb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在输出中，训练和测试的组合是不重复的。</p><h2 id="7909" class="mw mx it bd nt nu nv dn nw nx ny dp nz lm oa ob oc lq od oe of lu og oh oi iz bi translated"><strong class="ak">分层K折</strong></h2><p id="ad2d" class="pw-post-body-paragraph ld le it lf b lg oj kd li lj ok kg ll lm ol lo lp lq om ls lt lu on lw lx ly im bi translated">当我们对k-fold和k-fold进行分层时，分层后的样本在训练集和测试集中保持一致。</p><p id="3794" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Python的例子:</p><pre class="ks kt ku kv gt mr ms mt mu aw mv bi"><span id="92c5" class="mw mx it ms b gy my mz l na nb">from sklearn.model_selection import StratifiedKFold, KFold<br/>import numpy as np<br/>X, y = np.ones((60, 1)), np.hstack(([0] * 50, [1] * 10))<br/>skf = StratifiedKFold(n_splits=3)<br/>for train, test in skf.split(X, y):<br/>    print('train -  {}   |   test -  {}'.format(np.bincount(y[train]), np.bincount(y[test])))</span><span id="fc18" class="mw mx it ms b gy nd mz l na nb">#output:</span><span id="618a" class="mw mx it ms b gy nd mz l na nb">train -  [30  3]   |   test -  [15  2]<br/>train -  [30  3]   |   test -  [15  2]<br/>train -  [30  4]   |   test -  [15  1]</span></pre><p id="b915" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">箱计数用于计算样本中1的数量。</p><h2 id="17a1" class="mw mx it bd nt nu nv dn nw nx ny dp nz lm oa ob oc lq od oe of lu og oh oi iz bi translated">结论:</h2><p id="aee6" class="pw-post-body-paragraph ld le it lf b lg oj kd li lj ok kg ll lm ol lo lp lq om ls lt lu on lw lx ly im bi translated">有许多交叉验证技术，各有利弊。本文解释了一些简单易懂的方法，如<strong class="lf jd">分层洗牌拆分、K-fold分组、留出一个分组、留出P个分组、分组洗牌拆分、时间序列拆分。</strong></p><div class="lz ma gp gr mb mc"><a rel="noopener  ugc nofollow" target="_blank" href="/custom-statistical-details-of-data-frame-with-python-745d652b363f"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd jd gy z fp mh fr fs mi fu fw jc bi translated">用Python定制数据框的统计细节</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">数据集中特征的统计分析</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">pub.towardsai.net</p></div></div><div class="ml l"><div class="oo l mn mo mp ml mq kx mc"/></div></div></a></div><p id="4504" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae op" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae op" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="1176" class="oq mx it bd nt or os ot nw ou ov ow nz ki ox kj oc kl oy km of ko oz kp oi pa bi translated">推荐文章</h1><p id="9170" class="pw-post-body-paragraph ld le it lf b lg oj kd li lj ok kg ll lm ol lo lp lq om ls lt lu on lw lx ly im bi translated"><a class="ae op" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄与Python </a> <br/> 2。<a class="ae op" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a>T5】3 .<a class="ae op" rel="noopener ugc nofollow" target="_blank" href="/data-preprocessing-concepts-with-python-b93c63f14bb6?source=friends_link&amp;sk=5cc4ac66c6c02a6f02077fd43df9681a">数据预处理概念同Python </a> <br/> 4。<a class="ae op" rel="noopener ugc nofollow" target="_blank" href="/principal-component-analysis-in-dimensionality-reduction-with-python-1a613006d531?source=friends_link&amp;sk=3ed0671fdc04ba395dd36478bcea8a55">用Python进行主成分分析降维</a> <br/> 5。<a class="ae op" href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a?source=friends_link&amp;sk=9c5c613ceb10f2d203712634f3b6fb28" rel="noopener">用Python全面讲解K-means聚类</a> <br/> 6。<a class="ae op" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae op" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae op" rel="noopener ugc nofollow" target="_blank" href="/step-by-step-basic-understanding-of-neural-networks-with-keras-in-python-94f4afd026e5?source=friends_link&amp;sk=5530f9bb5374adde2a9e1a83272a9364">用Python中的Keras一步一步的基本了解神经网络</a> <br/> 9。<a class="ae op" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a> <br/> 10。<a class="ae op" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>