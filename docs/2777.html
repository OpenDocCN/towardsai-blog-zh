<html>
<head>
<title>Image Classification with Python: CNN vs Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python进行图像分类:CNN与变形金刚</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/image-classification-with-python-cnn-vs-transformers-fe509cbbc2d0?source=collection_archive---------0-----------------------#2022-05-23">https://pub.towardsai.net/image-classification-with-python-cnn-vs-transformers-fe509cbbc2d0?source=collection_archive---------0-----------------------#2022-05-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/bf7a59d41b19f964ce71b34753c515bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgqxDMP5qD1HE_uM33zZrg.png"/></div></div></figure><div class=""/><div class=""><h2 id="d988" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">计算机视觉&amp;用卷积神经网络、迁移学习、ViT、TensorFlow和HuggingFace进行解释</h2></div><h2 id="0c3e" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">摘要</h2><p id="c73d" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">在这篇文章中，我将使用计算机视觉和Python来解释3种不同的图像分类策略:从头开始构建一个<em class="mi"> CNN </em>，利用一个预先训练好的模型，并应用尖端的<em class="mi">Vison Transformers</em>(<em class="mi">ViT)</em>。</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mj"><img src="../Images/b47a3df98ed8688d5cd8334e1326e2fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-tboi8vaxhR3fYYT"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">照片由<a class="ae ms" href="https://unsplash.com/@jamie452?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰米街</a>上的<a class="ae ms" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="dccb" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated"><a class="ae ms" href="https://en.wikipedia.org/wiki/Computer_vision" rel="noopener ugc nofollow" target="_blank"> <strong class="lr jf">【计算机视觉(CV) </strong> </a> <strong class="lr jf"> </strong>人工智能领域研究计算机如何获得对数字图像或视频的高层次理解。CV始于20世纪60年代的麻省理工学院，当时一名暑期学生的任务是通过将相机连接到计算机来描述图像中的物体。虽然这个项目不是很成功，但它为几十年来开发现代计算机视觉的实验奠定了基础。</p><p id="0598" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">CV的主要任务是图像分类和目标检测。<strong class="lr jf">图像分类</strong>是根据特定的规则或模型对图像中的像素组进行分类和标记的过程。</p><p id="8053" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">本教程比较了著名的<em class="mi">卷积神经网络</em>和最先进的<em class="mi">T21<em class="mi">基于注意力的</em>变形金刚</em>，它们彻底改变了人工智能的面貌。除此之外，我还将展示如何从头开始构建一个模型，转移学习，并解释模型结果。</p><p id="8189" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">我将展示一些有用的Python代码，这些代码可以很容易地应用于其他类似的情况(只需复制、粘贴、运行)，并通过注释遍历每一行代码，以便您可以复制这个示例(下面是完整代码的链接)。</p><div class="is it gp gr iu my"><a href="https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/computer_vision/example_img_classification.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jf gy z fp nd fr fs ne fu fw jd bi translated">data science _ artificial intelligence _ Utils/example _ img _ classification . ipynb at master…</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">数据科学项目和人工智能用例的示例…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">github.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ja my"/></div></div></a></div><p id="2fe9" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">我将使用“<strong class="lr jf">犬种</strong>”<strong class="lr jf"/>数据集，其中为您提供了几幅不同犬种的图片(链接如下)。</p><div class="is it gp gr iu my"><a href="https://www.kaggle.com/competitions/dog-breed-identification/overview" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jf gy z fp nd fr fs ne fu fw jd bi translated">犬种识别</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">确定图像中狗的品种</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">www.kaggle.com</p></div></div><div class="nh l"><div class="nn l nj nk nl nh nm ja my"/></div></div></a></div><p id="0d83" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">特别是，我将经历:</p><ul class=""><li id="c55a" class="no np je lr b ls mt lv mu lc nq lg nr lk ns mh nt nu nv nw bi translated">设置:导入包、数据分析、预处理</li><li id="6aaf" class="no np je lr b ls nx lv ny lc nz lg oa lk ob mh nt nu nv nw bi translated">用<em class="mi">张量流、</em>评估&amp;可解释性、数据扩充从零开始构建CNN</li><li id="916c" class="no np je lr b ls nx lv ny lc nz lg oa lk ob mh nt nu nv nw bi translated">CNN迁移学习与<em class="mi">张量流，</em>评估&amp;可解释性</li><li id="2463" class="no np je lr b ls nx lv ny lc nz lg oa lk ob mh nt nu nv nw bi translated">使用预先训练的<em class="mi"> ViT </em>和<em class="mi">变压器</em></li></ul></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h2 id="5f13" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">设置</h2><p id="5156" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">首先，我需要导入以下库:</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="c482" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## for data</em><br/></strong>import <strong class="ok jf">os</strong><br/>import <strong class="ok jf">pandas</strong> as pd<br/>import <strong class="ok jf">numpy</strong> as np<br/>from <strong class="ok jf">tqdm</strong>.notebook  import tqdm<br/><br/><strong class="ok jf"><em class="mi">## for plotting</em><br/></strong>import <strong class="ok jf">matplotlib</strong>.pyplot as plt<br/>import <strong class="ok jf">seaborn</strong> as sns<br/>import <strong class="ok jf">plotly</strong>.express as px<br/><br/><strong class="ok jf"><em class="mi">## for metrics</em><br/></strong>from <strong class="ok jf">sklearn</strong> import metrics<br/><br/><strong class="ok jf"><em class="mi">## for cnn</em><br/></strong>from <strong class="ok jf">tensorflow</strong>.keras import models, layers, callbacks <strong class="ok jf"><em class="mi">#(2.6.0)</em></strong><br/><br/><strong class="ok jf"><em class="mi">## for explainer</em><br/></strong>from <strong class="ok jf">lime</strong> import lime_image<br/>from <strong class="ok jf">skimage</strong> import segmentation<br/><br/><strong class="ok jf"><em class="mi">## for vit</em><br/></strong>import <strong class="ok jf">transformers</strong> <strong class="ok jf"><em class="mi">#(4.18.0)</em></strong></span></pre><p id="cdd3" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">当处理图像时，有两个主要的Python包:<a class="ae ms" href="https://pillow.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"><em class="mi"/></a><em class="mi"/>和<a class="ae ms" href="https://github.com/opencv/opencv-python" rel="noopener ugc nofollow" target="_blank"> <em class="mi"> OpenCV </em> </a>。它们都执行相同的任务，但后者要快一点，因此我倾向于使用<strong class="lr jf"><em class="mi">【OpenCV】</em></strong><em class="mi">【cv2】</em>。</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="1a67" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## for images</em></strong><br/>import <strong class="ok jf">cv2</strong></span></pre><p id="9bfa" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">为了读取数据，应该将图像保存在本地，并加载将图片id与其标签进行映射的数据帧。</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="ee2c" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf">## for this tutorial I will use a subset of labels</strong><br/>labels = ["scottish_deerhound", "maltese_dog", "afghan_hound", "entlebucher", "bernese_mountain_dog"]<br/><br/>dtf = pd.read_csv("dogs_labels.csv").rename(columns={"breed":"label"})</span><span id="ee23" class="kt ku je ok b gy os op l oq or">dtf = dtf[dtf["label"].isin(labels)].sort_values("id").reset_index(drop=True)<br/><br/>dtf["y"] = dtf["label"].factorize(sort=True)[0]</span><span id="9b45" class="kt ku je ok b gy os op l oq or">dic_y_mapping = dict( dtf[['y','label']].drop_duplicates().sort_values('y').values )</span><span id="969f" class="kt ku je ok b gy os op l oq or">print(dic_y_mapping)<br/>dtf</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/876052d08904bf65b67fe5a75bbc11da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgS1CnHLKbRjy2vaF4Mumg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="3917" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">对于计算机来说，图像只是一个像素矩阵，每个像素是一个代表RGB颜色组合的点。换句话说，彩色图像是一个3D矩阵，其值在0到255之间，其中值0表示黑色，值255表示白色。</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/a2aece58ae1da758faa71fb2ada9843d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*svszYqSGQSwrvYfNsBWVCA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="9e29" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">我将写下两个有用的函数来加载和绘制这些3D数组(plot函数可以应用于单个图像，也可以应用于多个图片)。</p><figure class="mk ml mm mn gt iv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="bbbe" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">让我们在一张图片上尝试一下:</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="708d" class="kt ku je ok b gy oo op l oq or">img = <strong class="ok jf">load_img</strong>(file="data_dogs/0042188c895a2f14ef64a918ed9c7b64.jpg")<br/><strong class="ok jf">plot_imgs</strong>(img, "shape: "+str(img.shape))</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/c56a7c4b41af1b0ef83200d95d6a720f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*og5xXEqggWAB-yJoM-zbOw.png"/></div></div></figure><p id="ddbe" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">因此，如果我们想分解这张图片的RGB通道，我们可以这样做:</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="d5f4" class="kt ku je ok b gy oo op l oq or">lst = []<br/>for i in range(3):<br/>    tmp = np.zeros(img.shape, dtype='uint8')<br/>    tmp[:,:,i] = img[:,:,i]<br/>    lst.append(tmp)<br/>    <br/><strong class="ok jf">plot_imgs</strong>(lst, ["r","g","b"])</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/6984a5c69c186b1c7ec1dc9f5fc88395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqt08wslM__z5dHIlI4TaA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="4899" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">现在让我们加载整个数据集:</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="f776" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## load all</em></strong><br/>dirpath = "folder name here"<br/>ext = ['.png','.jpg','.jpeg','.JPG']<br/><br/>lst_imgs = []<br/>errors = 0<br/>for file in <strong class="ok jf">tqdm</strong>(sorted(<strong class="ok jf">os</strong>.listdir(dirpath))):<br/>    try:<br/>        if file.endswith(tuple(ext)):<br/>            img = load_img(file=<strong class="ok jf">os</strong>.path.join(dirpath, file), <br/>                           ext=ext)<br/>            lst_imgs.append(img)<br/>    except Exception as e:<br/>        print("failed on:", file, "| error:", e)<br/>        errors += 1<br/>        lst_imgs.append(np.nan)<br/>        pass<br/><br/>dtf["img"] = lst_imgs<br/>dtf = dtf[["id","img","label","y"]]</span><span id="8388" class="kt ku je ok b gy os op l oq or">print("check:", len(lst_imgs), "=", len(dtf), " |  Nas:", errors, "=", dtf["img"].isna().sum())<br/>dtf.head()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/ec84ba95694aaa77bb896f5b398cc5c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IQdPqTsa8X6GvOSBSVLzlg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="ed10" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf">plot_imgs</strong>(dtf["img"].head(), dtf["label"].head())</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/8f13541a82ce1e83c068a058ef94ed00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eTMirvUyn7XALEgq1ZtcGQ.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="275a" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">在开始研究这些模型之前，有一些<strong class="lr jf">数据分析</strong>要做。具体来说，我将回答3个问题:</p><p id="3b56" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">1.数据集平衡吗？似乎是这样，不过它很小。</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="1609" class="kt ku je ok b gy oo op l oq or">dtf["y"].value_counts().plot(kind="barh", title="Y", figsize=(5,3)).grid(axis='x')<br/>plt.show()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/5be0aa34f4f85a715940346a8d127739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G81Y79A0YlMfZvdrYb4qQg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="ade9" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">2.既然所有的图像都必须有相同的形状，那么选择什么样的尺寸才是合适的呢？通常，我会选择最常用的形状，并根据它来调整大小。</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="373e" class="kt ku je ok b gy oo op l oq or">width = [img.shape[0] for img in dtf["img"]]<br/>height = [img.shape[1] for img in dtf["img"]]</span><span id="68a3" class="kt ku je ok b gy os op l oq or">fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))<br/><br/><strong class="ok jf"><em class="mi">## all</em></strong><br/>ax[0].scatter(x=width, y=height, color="black")<br/>ax[0].set(xlabel='width', ylabel="height", title="Size distribution")<br/>ax[0].grid()<br/><br/><strong class="ok jf"><em class="mi">## zoom</em></strong><br/>ax[1].scatter(x=width, y=height, color="black")<br/>ax[1].set(xlabel='width', ylabel="height", xlim=[100,700], ylim=[100,700], title="Zoom")<br/>ax[1].grid()<br/><br/>plt.show()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/4033886424ac0f689fb52acac931b945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*34YJ8Ah6IYkoSwvULcTyTg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="7e2a" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## resize</em></strong><br/>img_size = (500,500)  <strong class="ok jf"><em class="mi">#&lt;-- 500x500</em></strong></span><span id="9e71" class="kt ku je ok b gy os op l oq or">dtf["img"] = [cv2.resize(img, img_size) for img in dtf["img"]]</span><span id="c760" class="kt ku je ok b gy os op l oq or">plot_imgs(dtf["img"].head(), dtf["y"].head())</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/b7e82b91d97b8a4ce50b5c3ee419ba90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2zCdI8dABbXOGX7-iTzjbg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="5435" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">3.我应该保留颜色还是转换黑白图像(2D阵列而不是3D)？一般来说，如果机器能够处理训练，我会说保留颜色。无论如何，如果有人想检查颜色和类别之间是否存在相关性(即所有1都系统地比0暗)，我建议从每个图像中提取主色，并绘制RGB分布。因此，如果有一个模式，图像数据点将出现在集群中。</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="fa07" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## try one</em></strong><br/>i = 0<br/><br/>rgb = dtf["img"][i].copy()<br/>unique, counts = np.unique(rgb.reshape(-1,3), axis=0, return_counts=True)<br/>rgb[:,:,0], rgb[:,:,1], rgb[:,:,2] = unique[np.argmax(counts)]</span><span id="b63b" class="kt ku je ok b gy os op l oq or">plot_imgs([dtf["img"][i], rgb], ["image","main color"])</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pe"><img src="../Images/b7b97e2e2e5d5c8de627b0ce7f5959d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T6HdLmiK88SLfxiDlp8x0A.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="55a9" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## do all</em></strong><br/>r,g,b,y = [],[],[],[]<br/>for img,label in tqdm(zip(dtf["img"],dtf["y"])):<br/>    unique, counts = np.unique(img.reshape(-1,3), axis=0, <br/>                               return_counts=True)<br/>    red, green, blue = unique[np.argmax(counts)]<br/>    r.append(red)<br/>    g.append(green)<br/>    b.append(blue)<br/>    y.append(label)<br/><br/><strong class="ok jf"><em class="mi">## plot 3D (with plotly)</em></strong><br/>data = pd.DataFrame({"r":r,"g":g,"b":b,"y":y})<br/>data["y"] = data["y"].astype(str)<br/>fig = px.scatter_3d(data, x='r', y='g', z='b', color='y',<br/>                    labels={"r":"red","g":"green","b":"blue"})<br/>fig.show()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/ce738151611ba63343460059917ca069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/1*8T2SZhF_RebCNk5ogAViPA.gif"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="588b" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">最后，我们可以继续进行<strong class="lr jf">预处理</strong> : scale(因为当我们处理神经网络时，数据最好在0和1之间)并将其分成训练集和测试集。</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="aeb9" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf">## scaling</strong><br/>dtf["img"] = dtf["img"]/255</span><span id="07d9" class="kt ku je ok b gy os op l oq or"><strong class="ok jf"><br/>## partitioning<br/></strong>split = 500</span><span id="5d7d" class="kt ku je ok b gy os op l oq or">X_train = np.array([x for x in dtf["img"].head(split).values])<br/>y_train = np.array([y for y in dtf["y"].head(split).values])<br/><br/>X_test = np.array([x for x in dtf["img"].tail(len(dtf)-split).values])<br/>y_test = np.array([y for y in dtf["y"].tail(len(dtf)-split).values])</span></pre><p id="374c" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">现在一切都准备好进入模型了。</p><h2 id="1557" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">卷积神经网络</h2><p id="6716" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated"><a class="ae ms" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank"> <em class="mi"> CNN </em> </a>是一种专门用于处理像素数据的人工神经网络。它们是在90年代发明的，但在2012年得到了广泛应用，当时技术开始允许重型模型通过利用GPU在笔记本电脑上运行(<a class="ae ms" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"><em class="mi">tensor flow</em></a><em class="mi"/>于2015年发布)。</p><p id="d2ab" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated"><em class="mi"> CNN的</em>解决了一个具体问题:如何从3D矩阵中提取特征，这是一个复杂的数据结构。主要思想是算法学习如何用一个数学公式(即均值)来概括一组像素，而不是做手工的特征工程。</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/21ed1e0cb131d223a2dd9892ae856fe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HYWACTmo757slbtHTbuckg.gif"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated"><a class="ae ms" href="https://miro.medium.com/max/2340/1*Fw-ehcNBR9byHtho-Rxbtw.gif" rel="noopener">来源</a></figcaption></figure><p id="ec14" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">这样，维数逐渐减少，直到每个图像由单个特征向量表示。</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/ea23d1a0223f046eb0e468f4e2a8b0e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RmBdfE7I1pmVs6qvJe4QyQ.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated"><a class="ae ms" href="https://www.domsoria.com/2019/10/come-funziona-una-rete-neurale-cnn-convolutional-neural-network/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="ed8a" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">我准备从零开始训练一个基本的<em class="mi">CNN</em>(2层<a class="ae ms" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D" rel="noopener ugc nofollow" target="_blank"> <em class="mi">卷积</em> </a> + <a class="ae ms" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D" rel="noopener ugc nofollow" target="_blank"> <em class="mi">池化</em> </a>)作为其他模型的基线。我将使用<em class="mi">tensor flow</em>/<em class="mi">Keras:</em>中的<a class="ae ms" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="noopener ugc nofollow" target="_blank"> <em class="mi">模型</em> </a> <em class="mi"> </em>类</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="2db8" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## Input</em></strong><br/>x_in = layers.Input(name="x_in", shape=(500,500,3))<br/><br/><strong class="ok jf"><em class="mi">## Conv + MaxPool</em></strong><br/>x_conv2d = layers.<strong class="ok jf">Conv2D</strong>(name="x_conv2d", filters=32, kernel_size=(3,3), activation="relu")(x_in)<br/>x_maxpool = layers.<strong class="ok jf">MaxPooling2D</strong>(name='x_maxpool', pool_size=(2,2))(x_conv2d)<br/><br/><strong class="ok jf"><em class="mi">## Conv + MaxPool</em></strong><br/>x_conv2d2 = layers.<strong class="ok jf">Conv2D</strong>(name="x_conv2d2", filters=32, kernel_size=(3,3), activation="relu")(x_maxpool)<br/>x_maxpool2 = layers.<strong class="ok jf">MaxPooling2D</strong>(name='x_maxpool2', pool_size=(2,2))(x_conv2d2)<br/><br/><strong class="ok jf"><em class="mi">## Flat + Dense</em></strong><br/>flat = layers.<strong class="ok jf">Flatten</strong>(name="flat")(x_maxpool2)<br/>dense = layers.<strong class="ok jf">Dense</strong>(name="dense", units=128, activation='relu')(flat)<br/><br/><strong class="ok jf"><em class="mi">## Output</em></strong><br/>y_out = layers.<strong class="ok jf">Dense</strong>(name="y_out", units=n_classes, activation="softmax")(dense) <strong class="ok jf"><em class="mi">#if binary -&gt; 1 neuron &amp; sigmoid</em></strong><br/><br/><strong class="ok jf"><em class="mi">## Compile</em></strong><br/>model = models.<strong class="ok jf">Model</strong>(inputs=x_in, outputs=y_out, name="CNN")<br/>model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"]) <strong class="ok jf"><em class="mi">#if binary -&gt; binary_crossentropy loss</em></strong></span><span id="eb18" class="kt ku je ok b gy os op l oq or">model.summary()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pi"><img src="../Images/7cad8294c9b74f0ec3410a2fd5e6f900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLFKgLIxD3_3GRXpzhcDPA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="e105" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">如果您想要检查图像在每一层中经历了什么样的变换，您可以像这样将其可视化(考虑到这些层仍然是随机的，因为模型还没有被训练):</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="a7ff" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## check layers</em></strong><br/>x = X_train[0]<br/>convx = model.layers[1](x)<br/>maxpx = model.layers[2](convx)<br/>convx2 = model.layers[3](maxpx)<br/>maxpx2 = model.layers[4](convx2)<br/><br/>print("shape:", maxpx2.shape[1:])<br/><br/>plot_imgs([x[0], convx[0][:,:,24:27], maxpx[0][:,:,22:25], <br/>          convx2[0][:,:,10:13], maxpx2[0][:,:,13:16]], <br/>          ["input","conv2d","maxpool","2nd conv2d","2nd maxpool"])</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pj"><img src="../Images/f3f1bd7d7fe7576d10a68448df287c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Na0UElju8yEkdNndESK1Lg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="2b05" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">我们可以<strong class="lr jf">训练模型</strong>，并在实际测试集上测试之前，检查用于验证的训练集子集的性能。您可以使用以下功能来可视化培训:</p><figure class="mk ml mm mn gt iv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="40cf" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## Train</em></strong><br/>training = model.fit(x=X_train, y=y_train, <br/>                     epochs=100, batch_size=64, shuffle=True, <br/>                     verbose=0, validation_split=0.2,<br/>                     callbacks=[callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)])</span><span id="6e0c" class="kt ku je ok b gy os op l oq or">model = training.model<br/><strong class="ok jf">utils_plot_training</strong>(training)</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/d37ba092cf99b00993756c5ae7bab07e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FdhKyYiro6Xp9twWYkWTSg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="9d55" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">请注意，我在回调中使用了<a class="ae ms" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping" rel="noopener ugc nofollow" target="_blank"> <em class="mi">提前停止</em> </a> <em class="mi"> </em>工具，当受监控的指标(即验证损失)停止改善时，该工具将终止培训。</p><p id="5a44" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">让我们<strong class="lr jf">测试</strong>这个模型:</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="1a1d" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## Test</em></strong><br/>predicted_prob = model.predict(X_test)<br/>predicted = [np.argmax(pred) for pred in predicted_prob]</span></pre><p id="c12b" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">为了<strong class="lr jf">评估</strong>CNN的表现，我将使用以下指标:</p><ul class=""><li id="6085" class="no np je lr b ls mt lv mu lc nq lg nr lk ns mh nt nu nv nw bi translated"><em class="mi">准确性</em>:模型预测正确的比例。</li><li id="47eb" class="no np je lr b ls nx lv ny lc nz lg oa lk ob mh nt nu nv nw bi translated"><em class="mi">混淆矩阵</em>:一个汇总表，按类别细分了正确和错误预测的数量。</li><li id="1f83" class="no np je lr b ls nx lv ny lc nz lg oa lk ob mh nt nu nv nw bi translated"><em class="mi"> ROC </em>:图示在各种阈值设置下的真阳性率对假阳性率的曲线图。曲线下的面积(<em class="mi"> AUC </em>)表示分类器将随机选择的阳性观察值排序高于随机选择的阴性观察值的概率。</li><li id="96be" class="no np je lr b ls nx lv ny lc nz lg oa lk ob mh nt nu nv nw bi translated"><em class="mi">精度</em>:相关实例在检索到的实例中所占的比例。</li><li id="0f3b" class="no np je lr b ls nx lv ny lc nz lg oa lk ob mh nt nu nv nw bi translated"><em class="mi"> Recall: </em>实际检索到的相关实例总数的一部分。</li></ul><figure class="mk ml mm mn gt iv"><div class="bz fp l di"><div class="ov ow l"/></div></figure><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="0249" class="kt ku je ok b gy oo op l oq or">evaluate_multi_classif(y_test, predicted, predicted_prob, figsize=(15,5))</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/ecbcd950182f167f60b63e5bb0ff6d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O4xtzjqVK46el-vDEPMTYA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="7c6c" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">不是很好，它有很多情况，但是也有很多错误，特别是对于类3和类4。在这种情况下，最好理解模型为什么用某个标签对图像进行分类，并评估预测的<strong class="lr jf">可解释性</strong>。<a class="ae ms" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"> <em class="mi">石灰</em>包</a>可以帮助我们建立一个解释器。为了举例说明，我将从测试集中随机观察，看看模型预测了什么，为什么。</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="1114" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## img instance</em><br/></strong>i = 10<br/>img_instance = X_test[i]</span><span id="3298" class="kt ku je ok b gy os op l oq or"><strong class="ok jf"><em class="mi">## explain<br/></em></strong>explainer = <strong class="ok jf">lime_image</strong>.LimeImageExplainer()<br/>explained = explainer.explain_instance(img_instance, model.predict, num_samples=1000)</span><span id="2421" class="kt ku je ok b gy os op l oq or"><strong class="ok jf"><em class="mi">## visualize<br/></em></strong>fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))<br/>ax[0].imshow(img_instance)<br/>temp, mask = explained.get_image_and_mask(explained.top_labels[0], positive_only=False, hide_rest=False)<br/>ax[1].imshow(segmentation.mark_boundaries(temp/2+0.5, mask))<br/>plt.show()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pm"><img src="../Images/97226c94440fcff3f998379d344f8bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PaBh1o2u-SyhOyVxur-0rQ.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="c56b" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">正如我们所见，该模型没有使用正确的特征(即狗的像素)，因此它变得混乱，主要原因是数据集对于计算机视觉用例来说不够大。</p><p id="1357" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">有两种方法可以解决这个问题:数据扩充和迁移学习。<a class="ae ms" href="https://en.wikipedia.org/wiki/Data_augmentation" rel="noopener ugc nofollow" target="_blank"> <strong class="lr jf">数据扩充</strong> </a>是一种通过添加对已有图像稍加修改的副本来增加数据量的技术。它充当正则化器，有助于在训练机器学习模型时减少过拟合。使用<em class="mi"> TensorFlow: </em>可以轻松设置数据扩充</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="bac8" class="kt ku je ok b gy oo op l oq or">data_augmentation = models.Sequential(name="data_augmentation", layers=[<br/>    layers.<strong class="ok jf">RandomFlip</strong>("horizontal_and_vertical"),<br/>    layers.<strong class="ok jf">RandomRotation</strong>(factor=0.2),<br/>    layers.<strong class="ok jf">RandomZoom</strong>(height_factor=0.2, width_factor=0.2)<br/>])</span><span id="3798" class="kt ku je ok b gy os op l oq or"><strong class="ok jf"><em class="mi">## try one</em></strong><br/>plot_imgs([X_train[0], <strong class="ok jf">data_augmentation</strong>(X_train[0])], ["original","augmented"])</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pn"><img src="../Images/3e30187901b5302cb9afbd3a804e845d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kz7NmfMt6N0CjKD3Mby1bg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="6992" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">数据扩充可在训练前应用于数据集，或作为图层添加到模型中，如下所示:</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="cd77" class="kt ku je ok b gy oo op l oq or"><em class="mi">## Input</em><br/>x_in = layers.Input(name="x_in", shape=(500,500,3))</span><span id="6ede" class="kt ku je ok b gy os op l oq or"><strong class="ok jf"><em class="mi">## Data Augmentation</em><br/><em class="mi">x_augm = data_augmentation(x_in)</em></strong></span><span id="5369" class="kt ku je ok b gy os op l oq or"><em class="mi">## Conv + MaxPool</em><br/>x_conv2d = layers.Conv2D(name="x_conv2d", filters=32, kernel_size=(3,3), activation="relu")(<strong class="ok jf"><em class="mi">x_augm</em></strong>)<br/>x_maxpool = layers.MaxPooling2D(name='x_maxpool', pool_size=(2,2))(x_conv2d)<br/><br/><em class="mi">## Conv + MaxPool</em><br/>x_conv2d2 = layers.Conv2D(name="x_conv2d2", filters=32, kernel_size=(3,3), activation="relu")(x_maxpool)<br/>x_maxpool2 = layers.MaxPooling2D(name='x_maxpool2', pool_size=(2,2))(x_conv2d2)<br/><br/><em class="mi">## Flat + Dense</em><br/>flat = layers.Flatten(name="flat")(x_maxpool2)<br/>dense = layers.Dense(name="dense", units=128, activation='relu')(flat)<br/><br/><em class="mi">## Output</em><br/>y_out = layers.Dense(name="y_out", units=n_classes, activation="softmax")(dense)</span><span id="e015" class="kt ku je ok b gy os op l oq or"><em class="mi">## Compile</em><br/>model = models.Model(inputs=x_in, outputs=y_out, name="CNN")<br/>model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])</span></pre><p id="ba98" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">请注意，将它作为模型中的一个层不会增加观察的数量，它只是在每个时期随机修改图像以减少过度拟合。</p><p id="9648" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">或者，人们可以使用迁移学习。</p><h2 id="768a" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">迁移学习</h2><p id="7547" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">简而言之，<a class="ae ms" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank">迁移学习</a>意味着采用一个已经在大型数据集上进行了预训练的模型，并使其适应您的用例。例如，在这里，我将选择一个已经可以识别狗的模型，并根据狗的品种(我的标签)对其进行微调。显然，数据集越相似，迁移学习就越有效……用一个在汽车上训练过的模型对动物进行分类是行不通的。</p><p id="fe7e" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">我将使用<a class="ae ms" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"><strong class="lr jf"><em class="mi">vgg 16</em></strong></a><strong class="lr jf"><em class="mi"/></strong>一个16层的深度<em class="mi"> CNN </em>对来自<a class="ae ms" href="https://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mi"> ImageNet </em>数据库</a>的100多万张图像进行训练，以识别成千上万的物体和动物。所以计划是这样的:加载预训练的模型，它不会被重新训练，没有最终层，并用一些新的最终层替换它的头部，这些最终层将学习如何预测我的目标。</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/2a453bc8d6f6af69da047651dc03cd8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5csbCfnbjwOmRav-Ou_sPQ.gif"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="36d8" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">对于这个小操作，我将使用<a class="ae ms" href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential" rel="noopener ugc nofollow" target="_blank"> <em class="mi">序列</em> </a> <em class="mi"> </em>类，因为它可以自动处理张量形状的差异，从而简化分离模型的合并。</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="af3e" class="kt ku je ok b gy oo op l oq or">from <strong class="ok jf">tensorflow.keras.applications</strong> import <strong class="ok jf">vgg16</strong></span><span id="8f52" class="kt ku je ok b gy os op l oq or"><strong class="ok jf"><em class="mi">## Load pre-trained</em><br/></strong>base = vgg16.VGG16(weights="imagenet", include_top=False, <br/>                   input_shape=(500,500,3))<br/>base.trainable = False</span><span id="baa3" class="kt ku je ok b gy os op l oq or"><strong class="ok jf"><em class="mi">## Add new head</em></strong><em class="mi"><br/></em>model = models.Sequential(name="TransferLearning", layers=[<br/>    base,<br/>    layers.Flatten(name="flat"),<br/>    layers.Dense(name="dense", units=128, activation='relu'),<br/>    layers.Dense(name="y_out", units=n_classes, activation="softmax")<br/>])<br/><br/>model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])<br/>model.summary()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/6d19a5d1bbd6a6cdfbf2a23eee51bcb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DQsvi-uSg4n4Io2NsjJSVw.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="0299" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">通过使用相同的代码进行训练、测试和评估，我们这次得到了更好的结果(<em class="mi">精确度</em>为0.7，详情请见笔记本)。此外，感谢<em class="mi"> lime </em>讲解器，我们可以看到这个模型知道它在做什么:</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/f63452eef178ae1bbbd4230bd54b045f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIfRXgH0cUbLjp1NsKeaRQ.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><h2 id="5bed" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">变形金刚(电影名)</h2><p id="d22e" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated"><a class="ae ms" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener ugc nofollow" target="_blank"> <em class="mi">变形金刚</em> </a> <em class="mi"> </em>是Google的论文<a class="ae ms" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="mi">提出的一种新的建模技术</em></a><em class="mi"/>【2017】<em class="mi"/>，其中论证了经典的深度神经网络完全可以被<a class="ae ms" href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" rel="noopener ugc nofollow" target="_blank">注意力机制</a>所取代，甚至获得更好的性能。虽然它们已经迅速接管了NLP领域，但是对CV的应用仍然有限。</p><p id="facc" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">2020年，提出了<a class="ae ms" href="https://github.com/google-research/vision_transformer" rel="noopener ugc nofollow" target="_blank"> <em class="mi">视觉转换器</em> ( <em class="mi"> ViT </em> ) </a>，这是一种将文字嵌入应用于图像的模型，而不是像<em class="mi"> CNN </em>那样使用像素阵列。基本上，输入图像被分解成固定大小的小块(或扁平的像素组)，就像一个文本序列，然后每个小块被嵌入并传递给关注层(编码器)。这种架构允许模型学习局部特征以及重建图像的完整结构。</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pr"><img src="../Images/5adabce73ca15cebd40f2917f476eee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z2XknpDj6Zg7ASz_90YjIw.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="60b5" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">这些模型的主库是<a class="ae ms" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> <em class="mi">变形金刚</em> </a> <em class="mi"> </em> by <a class="ae ms" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>:</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="a9f3" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi">## Load model</em><br/></strong>model_name = "<strong class="ok jf">google/vit-base-patch16-224-in21k</strong>"<br/><br/>prep = transformers.AutoFeatureExtractor.from_pretrained(model_name)<br/>vit = transformers.TFAutoModel.from_pretrained(model_name)</span></pre><p id="65fa" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">通常，每个变形金刚模型都有自己的预处理器。事实上，如果您打印<em class="mi"> prep </em>对象，您会看到<em class="mi"> ViT </em>需要一些规范化和调整大小:</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ps"><img src="../Images/6d13391ab7bfcbded9634581caf0cc01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JtcWuHfkFAvFv5LGwbznCA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="7a63" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">让我们这样做:</p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="9a14" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi"># Preprocess </em><br/></strong>X_train_vit = np.array([prep(img)["pixel_values"][0] for img in X_train])<br/>X_test_vit = np.array([prep(img)["pixel_values"][0] for img in X_test])<br/> <br/>print(X_train_vit[0].shape)</span></pre><p id="8894" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">新的形状是<em class="mi"> (3，224，224) </em>。这意味着RGB通道是第一维度，图像需要为<em class="mi"> 224x224。</em></p><pre class="mk ml mm mn gt oj ok ol om aw on bi"><span id="4dfb" class="kt ku je ok b gy oo op l oq or"><strong class="ok jf"><em class="mi"># Model</em></strong><br/>x_in = layers.Input(name="x_in", shape=(3,224,224))</span><span id="b7c9" class="kt ku je ok b gy os op l oq or">vit_out = vit(x_in)[0][:,0,:]</span><span id="f4f2" class="kt ku je ok b gy os op l oq or">y_out = layers.Dense(name="y_out", units=n_classes, <br/>                     activation="softmax")(vit_out)<br/><br/>model = models.Model(inputs=x_in, outputs=y_out, name="ViT")</span><span id="98ef" class="kt ku je ok b gy os op l oq or">model.compile(loss="sparse_categorical_crossentropy", <br/>              optimizer="adam", metrics=["accuracy"])</span><span id="e683" class="kt ku je ok b gy os op l oq or">model.summary()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pt"><img src="../Images/91c4bb0be2fa9edff46dc75c139c7654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ynvEV2Ui1ceQZFk-wY9MSg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk translated">作者图片</figcaption></figure><p id="1ecc" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">您可以使用与之前相同的代码来训练、测试和评估模型。不幸的是，对于<em class="mi"> ViT </em>要求的输入形状，不可能使用<em class="mi"> lime </em>解释器。</p><h2 id="ef2e" class="kt ku je bd kv kw kx dn ky kz la dp lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">结论</h2><p id="df7f" class="pw-post-body-paragraph lp lq je lr b ls lt kf lu lv lw ki lx lc ly lz ma lg mb mc md lk me mf mg mh im bi translated">这篇文章是一个教程，演示如何应用不同的计算机视觉模型进行图像分类。我比较了3种方法:<em class="mi"> CNN </em>从头开始，从一个预先训练好的<em class="mi"> CNN (VGG16) </em>迁移学习，以及图像的前沿变形金刚(<em class="mi"> ViT </em>)。我经历了数据分析、预处理、模型训练、数据扩充、评估和可解释性。</p><p id="9573" class="pw-post-body-paragraph lp lq je lr b ls mt kf lu lv mu ki lx lc mv lz ma lg mw mc md lk mx mf mg mh im bi translated">我希望你喜欢它！如有问题和反馈，或者只是分享您感兴趣的项目，请随时联系我。</p><blockquote class="pu"><p id="289b" class="pv pw je bd px py pz qa qb qc qd mh dk translated">👉<a class="ae ms" href="https://linktr.ee/maurodp" rel="noopener ugc nofollow" target="_blank">我们来连线</a>👈</p></blockquote></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><blockquote class="qe qf qg"><p id="63eb" class="lp lq mi lr b ls mt kf lu lv mu ki lx qh mv lz ma qi mw mc md qj mx mf mg mh im bi translated">本文是Python 系列<strong class="lr jf"> CV的一部分，参见:</strong></p></blockquote><div class="is it gp gr iu my"><a href="https://towardsdatascience.com/how-to-detect-objects-with-your-webcam-82693c47bd8" rel="noopener follow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jf gy z fp nd fr fs ne fu fw jd bi translated">用Python和YOLO进行对象检测</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">使用网络摄像头的计算机视觉</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="qk l nj nk nl nh nm ja my"/></div></div></a></div><div class="is it gp gr iu my"><a href="https://towardsdatascience.com/document-parsing-with-python-ocr-75543448e581" rel="noopener follow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jf gy z fp nd fr fs ne fu fw jd bi translated">使用Python和OCR进行文档解析</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">使用计算机视觉从任何类型的文档中检测和提取文本、图形、表格</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="ql l nj nk nl nh nm ja my"/></div></div></a></div></div></div>    
</body>
</html>