<html>
<head>
<title>An Overview of Support Vector Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机综述</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-brief-overview-of-support-vector-machines-11ee4f77d819?source=collection_archive---------1-----------------------#2021-07-10">https://pub.towardsai.net/a-brief-overview-of-support-vector-machines-11ee4f77d819?source=collection_archive---------1-----------------------#2021-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="71b6" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="4908" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">快速阅读支持向量机(SVM)及其在多类分类中的应用</h2></div><p id="59fc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你会学到的</p><ul class=""><li id="bf0b" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">什么是支持向量机？</li><li id="0798" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">SVM的特点及其应用</li><li id="4c84" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">不同SVM超参数的解释</li><li id="525d" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">多类分类的Python实现</li></ul><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/034aba56dc1813f7676c95c4daecac72.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*0dKtUo6rTjszTMUka472Vg.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk translated">作者图片</figcaption></figure><p id="4632" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> SVM算法是由弗拉迪米尔·n·瓦普尼克和阿列克谢耶维奇提出的。1963年切尔沃嫩基斯</strong></p><blockquote class="mr"><p id="3970" class="ms mt it bd mu mv mw mx my mz na lm dk translated">支持向量机是一种监督机器学习算法，用于分类和回归。SVM的目标是确定一个超平面，通过最大化两个类别的支持向量之间的间隔将数据点分成两个类别</p></blockquote><figure class="nc nd ne nf ng mg gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/eed2bed76f399a2d7d57060890f2e295.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*kWm8nMIsfGFBmo7aMrAhxA.png"/></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk translated">作者图片</figcaption></figure><h2 id="0fe6" class="nh ni it bd nj nk nl dn nm nn no dp np la nq nr ns le nt nu nv li nw nx ny iz bi translated">支持向量</h2><p id="0e98" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated"><strong class="kt jd">支持向量是数据集中最接近超平面</strong>的数据点。移除支持向量将改变分隔两个类别的超平面，因为它们强烈影响超平面的位置和方向。支持向量是数据集的关键元素，因为SVM是基于它们构建的。</p><h2 id="9202" class="nh ni it bd nj nk nl dn nm nn no dp np la nq nr ns le nt nu nv li nw nx ny iz bi translated">超平面和边缘</h2><p id="e814" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated"><strong class="kt jd">超平面是以最佳方式将数据分成不同类别的决策边界</strong>。选择超平面以最大化支持向量的余量。</p><h2 id="14ac" class="nh ni it bd nj nk nl dn nm nn no dp np la nq nr ns le nt nu nv li nw nx ny iz bi translated">支持向量机的特点</h2><ul class=""><li id="53d9" class="ln lo it kt b ku nz kx oa la oe le of li og lm ls lt lu lv bi translated"><strong class="kt jd"> SVM既适用于回归，也适用于分类</strong></li><li id="501a" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">它对边缘错误一侧的点的总距离进行惩罚，</strong>特别是对于两个类的重叠数据点，从而限制边缘附近的错误分类</li><li id="36fb" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd"> SVM可以处理高维数据</strong></li><li id="b771" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">当类别之间有明确的界限时，SVM运行良好</strong></li><li id="03c2" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">对异常值检测有效</strong></li><li id="a5cc" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">当样本数量少于数据集中的要素数量时，SVM表现良好</li><li id="9a39" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd"> SVM使用不同的内核，如线性、径向偏置函数、多项式或二次函数，扩展到线性和非线性可分离类</strong></li><li id="8e22" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">它在大型数据集或数据集有更多噪声时效果不佳</li><li id="7e55" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">支持向量机使用完整的数据集进行训练；因此训练是缓慢的；然而，预测更快</li></ul><h2 id="f27a" class="nh ni it bd nj nk nl dn nm nn no dp np la nq nr ns le nt nu nv li nw nx ny iz bi translated">SVM的应用</h2><p id="d4c0" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">SVM用于<strong class="kt jd">图像分类</strong></p><ul class=""><li id="319e" class="ln lo it kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><strong class="kt jd">面部表情分类</strong></li><li id="3d8b" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">神经成像</strong></li><li id="ae4d" class="ln lo it kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated"><strong class="kt jd">识别植被类型</strong></li></ul><p id="b091" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SVM也用于<strong class="kt jd">归纳和直推模型的文本分类</strong></p><p id="5d78" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">也在<strong class="kt jd">生物信息学中用于蛋白质分类和癌症分类。</strong></p><h2 id="0602" class="nh ni it bd nj nk nl dn nm nn no dp np la nq nr ns le nt nu nv li nw nx ny iz bi translated">SVM超参数</h2><p id="995b" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">详细了解<a class="ae oh" href="https://medium.datadriveninvestor.com/support-vector-machines-ae0ff2375479" rel="noopener ugc nofollow" target="_blank"> SVM超参数</a></p><p id="6697" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">正则化(C) </strong> : s <strong class="kt jd">指定允许的错误分类程度，并且</strong>与边距成反比。更接近0的C值允许更多的错误分类，并且更高的C值将成功地对正确分离的训练数据点进行分类。</p><p id="660c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">内核:指定算法</strong>中使用的内核类型，如线性、径向基函数(RBF)、多项式或sigmoid。它将接受输入数据并将其转换成指定的内核。</p><p id="fbb2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> gamma:定义一个训练样本的影响达到多远；它是影响半径的倒数。当决定决策边界</strong>时，较低的伽玛值将考虑远点，而较高的伽玛值将考虑近点。当内核是rbf、poly或sigmoid时适用。</p><p id="fb6a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">decision _ function _ shape:</strong>通常用于多类分类。它有<strong class="kt jd">两个值，“ovr”(一对静止)或“ovo”(一对一)</strong>。<strong class="kt jd">在“ovr”中，每个类都像二元分类器一样独立于所有其他类进行评估，而“ovo”采用一对类来训练分类器</strong>。“ovo”总是被用作默认的多类策略。</p><h2 id="bf89" class="nh ni it bd nj nk nl dn nm nn no dp np la nq nr ns le nt nu nv li nw nx ny iz bi translated">使用支持向量分类器的多类分类</h2><p id="3e96" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">这里我们将使用葡萄酒数据集并运行支持向量分类器</p><p id="a5e6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">加载葡萄酒数据集</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="82d7" class="nh ni it oj b gy on oo l op oq"><strong class="oj jd">from sklearn import datasets<br/>data_wine = datasets.load_wine()</strong></span></pre><p id="c168" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">缩放输入数据并编码目标变量</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="1156" class="nh ni it oj b gy on oo l op oq"><strong class="oj jd">from sklearn.preprocessing import scale<br/>X_scaled = scale(data_wine.data)</strong></span><span id="f865" class="nh ni it oj b gy or oo l op oq"># Encoding our dependent variable:Quality column<br/><strong class="oj jd">from sklearn.preprocessing import LabelEncoder<br/>y_scaled = LabelEncoder()<br/>y = y_scaled.fit_transform(data_wine.target)</strong></span></pre><p id="7d95" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">将数据拆分为定型数据集和测试数据集</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="dcd1" class="nh ni it oj b gy on oo l op oq"><strong class="oj jd">from sklearn.model_selection import train_test_split</strong></span><span id="42d3" class="nh ni it oj b gy or oo l op oq"># Split dataset into training set and test set<br/><strong class="oj jd">X_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size=0.5,random_state=0)</strong></span></pre><p id="161e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过混洗数据集将数据集分成10个连续的折叠。每个折叠将被用作一次验证，而剩余的9个折叠形成训练集</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="df33" class="nh ni it oj b gy on oo l op oq"># creating a KFold object with 10 splits <br/>from sklearn.model_selection import KFold<br/>folds = KFold(n_splits = 10, shuffle = True, random_state = 10)</span></pre><p id="3231" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用GridSearchCV找到SVC超参数的最佳值:gamma、kernel、C、degree和decision_function_shape</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="77c7" class="nh ni it oj b gy on oo l op oq"><strong class="oj jd">from sklearn import svm<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.model_selection import validation_curve, KFold, cross_val_score, GridSearchCV<br/>clf = svm.SVC()<br/>hyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],<br/>                     'C': [2,5,10],<br/>                  'decision_function_shape':['ovr', 'ovo'],<br/>                  'degree':[3,5,10],<br/>                  'kernel':['linear', 'rbf', 'poly'],<br/>                 }]</strong><br/># set up GridSearchCV()<br/><strong class="oj jd">model_gscv = GridSearchCV(estimator = clf, <br/>                        param_grid = hyper_params, <br/>                        scoring= 'accuracy', <br/>                        cv = folds, <br/>                        verbose = 1,<br/>                        return_train_score=True)</strong></span><span id="5a84" class="nh ni it oj b gy or oo l op oq"># fit the model<br/><strong class="oj jd">model_gscv.fit(X_train, y_train)</strong></span><span id="a160" class="nh ni it oj b gy or oo l op oq"># printing the optimal accuracy score and hyperparameters<br/><strong class="oj jd">best_score = model_gscv.best_score_<br/>best_hyperparams = model_gscv.best_params_</strong></span><span id="9bad" class="nh ni it oj b gy or oo l op oq"><strong class="oj jd">print("The best train score is {0} corresponding to hyperparameters {1}".format(best_score, best_hyperparams))</strong></span></pre><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi os"><img src="../Images/ebc28bf448be2a6078a1ff9ebbe9955c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbgwsyiG5Kz6qX77FMMtiA.png"/></div></div></figure><p id="1400" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">基于使用GridSearchCV方法确定的最佳超参数创建SVC。拟合训练数据并在测试数据上运行准确性。</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="db01" class="nh ni it oj b gy on oo l op oq">#Create a svm Classifier<br/><strong class="oj jd">clf = svm.SVC(kernel='rbf',C=5,  decision_function_shape='ovr') # Linear Kernel</strong></span><span id="73bc" class="nh ni it oj b gy or oo l op oq">#Train the model using the training sets<br/><strong class="oj jd">clf.fit(X_train, y_train)</strong></span><span id="b820" class="nh ni it oj b gy or oo l op oq">#Predict the response for test dataset<br/><strong class="oj jd">y_pred = clf.predict(X_test)</strong><br/>#Import scikit-learn metrics module for accuracy calculation<br/><strong class="oj jd">from sklearn import metrics</strong></span><span id="49fa" class="nh ni it oj b gy or oo l op oq"># Model Accuracy: how often is the classifier correct?<br/><strong class="oj jd">print("Accuracy:",metrics.accuracy_score(y_test, y_pred))<br/>print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))</strong></span></pre><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/c25f8dbbe91b2902df3dae26dd0b2fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*n2BaBlyvk1Al6bHx4o_PpQ.png"/></div></figure><p id="1fae" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了可视化葡萄酒数据集上的超平面，我们首先使用PCA降低输入特征的维数。</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="44ef" class="nh ni it oj b gy on oo l op oq"><strong class="oj jd">from sklearn.decomposition import PCA<br/>pca_wine = PCA(n_components=2)<br/>pca_wine_train = pca_wine.fit_transform(X_train)</strong></span></pre><p id="6d5a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">创建地块创建网格栅格和等高线的步骤</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="a512" class="nh ni it oj b gy on oo l op oq"><strong class="oj jd">import numpy as n<br/>def make_meshgrid(x, y, h=.02):<br/>    x_min, x_max = x.min() - 1, x.max() + 1<br/>    y_min, y_max = y.min() - 1, y.max() + 1<br/>    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))<br/>    return xx, yy</strong></span><span id="798b" class="nh ni it oj b gy or oo l op oq"><strong class="oj jd">def plot_contours(ax, clf, xx, yy, **params):<br/>    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br/>    Z = Z.reshape(xx.shape)<br/>    out = ax.contourf(xx, yy, Z, **params)<br/>    return out</strong></span></pre><p id="37c0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为简化的葡萄酒数据创建SVC</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="32c4" class="nh ni it oj b gy on oo l op oq">#Create a svm Classifier<br/><strong class="oj jd">clf_pca = svm.SVC(kernel='rbf',C=10,  decision_function_shape='ovr') </strong></span><span id="7454" class="nh ni it oj b gy or oo l op oq">#Train the model using the training sets<br/><strong class="oj jd">clf_pca.fit(pca_wine_train, y_train)</strong></span></pre><p id="9118" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用SVC绘制数据点</p><pre class="mc md me mf gt oi oj ok ol aw om bi"><span id="4a87" class="nh ni it oj b gy on oo l op oq">fig, ax = plt.subplots()</span><span id="4e3f" class="nh ni it oj b gy or oo l op oq"># title for the plots<br/><strong class="oj jd">title = ('RBF SVC  on Wine dataset')</strong></span><span id="8273" class="nh ni it oj b gy or oo l op oq"># Set-up grid for plotting.<br/><strong class="oj jd">X0, X1 = pca_wine_train[:, 0], pca_wine_train[:, 1]<br/>xx, yy = make_meshgrid(X0, X1)<br/>plot_contours(ax, clf_pca, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)<br/>ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k' )<br/>ax.set_ylabel('PC2')<br/>ax.set_xlabel('PC1')<br/>ax.set_xticks(())<br/>ax.set_yticks(())<br/>ax.set_title('Decision surface for Wine dataset using PCA reduced features')<br/>plt.show()</strong></span></pre><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ou"><img src="../Images/a35deba56be9367fb77aad9f76ba6164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWZvd1PldWAScz5gI5DC1A.png"/></div></div></figure><h2 id="c19d" class="nh ni it bd nj nk nl dn nm nn no dp np la nq nr ns le nt nu nv li nw nx ny iz bi translated">结论:</h2><p id="1354" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">支持向量机是一种用于分类和回归的监督机器学习算法，它通过最大化两类支持向量之间的差值来识别超平面，以线性或非线性地将数据点分成两类。SVM能很好地处理高维数据，对于离群点检测也很有效。</p><h2 id="338e" class="nh ni it bd nj nk nl dn nm nn no dp np la nq nr ns le nt nu nv li nw nx ny iz bi translated">参考资料:</h2><div class="ov ow gp gr ox oy"><a href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">1.4.支持向量机-sci kit-学习0.24.2文档</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">支持向量机是一组用于分类、回归和分类的监督学习方法</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">scikit-learn.org</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm ml oy"/></div></div></a></div><p id="e2a5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae oh" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . SVM . SVC . html</a></p></div></div>    
</body>
</html>