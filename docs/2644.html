<html>
<head>
<title>These Three Theories Help Us Understand Overfitting and Underfitting in Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这三个理论帮助我们理解机器学习模型中的过拟合和欠拟合</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/these-three-theories-help-us-understand-overfitting-and-underfitting-in-machine-learning-models-b2e960d17141?source=collection_archive---------0-----------------------#2022-03-28">https://pub.towardsai.net/these-three-theories-help-us-understand-overfitting-and-underfitting-in-machine-learning-models-b2e960d17141?source=collection_archive---------0-----------------------#2022-03-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0f8f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">奥卡姆剃刀、VC维度和免费的午餐定理可以帮助我们思考ML解决方案中的过度拟合和欠拟合。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dffd1c870f0c2feff242cf9bffa2df43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h4YQEuVi4xnzvHms.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://medium.com/@minions.k/underfit-and-overfit-explained-8161559b37db" rel="noopener">https://medium . com/@ minions . k/under fit-and-over fit-explained-8161559 b 37 db</a></figcaption></figure><blockquote class="kz la lb"><p id="59e0" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过10万名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="lz ma gp gr mb mc"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">序列</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">订阅人工智能世界中最相关的项目和研究论文。受到120，000+的信任…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">thesequence.substack.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq ks mc"/></div></div></a></div><p id="cb5b" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">欠拟合和过拟合是现代机器学习(ML)解决方案中普遍存在的挑战。这两个挑战都与机器学习模型基于初始训练样本集构建相关知识的能力有关。从概念上讲，欠拟合与机器学习算法无法从初始训练数据中推断出有效知识有关。与此相反，过度拟合与创建假设的模型相关联，这些假设过于一般化或抽象，无法产生实际结果。简单来说，不适合的模型有点愚蠢，而过度适合的模型容易产生幻觉(想象不存在的东西):)。</p><p id="98d2" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">量化ML模型中过度拟合或欠拟合倾向的最佳方法之一是理解其能力。从概念上讲，容量代表机器学习模型可以选择作为可能解决方案的函数的数量。例如，线性回归模型可以将y = w*x + b形式的所有1次多项式作为容量(意味着所有潜在的解决方案)。在机器学习模型中，容量是一个非常重要的概念。从技术上讲，当机器学习算法的能力与其任务的复杂性和训练数据集的输入成比例时，它的性能最佳。低容量的机器学习模型在解决复杂任务时不切实际，并且往往不适合。同样，容量高于需求的模型很快就会过度适应。从这个角度来看，容量代表了一种度量，通过这种度量，我们可以估计模型欠拟合或过拟合的倾向。</p><p id="a7a4" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">有几种方法可以帮助我们量化ML模型的容量。从哲学/数学的角度来看，我发现有三个理论在考虑欠拟合和过拟合的情况时非常有用。</p><h2 id="9f59" class="mu mv it bd mw mx my dn mz na nb dp nc mr nd ne nf ms ng nh ni mt nj nk nl nm bi translated">①奥卡姆剃刀</h2><p id="641d" class="pw-post-body-paragraph lc ld it lf b lg nn ju li lj no jx ll mr np lo lp ms nq ls lt mt nr lw lx ly im bi translated">奥卡姆剃刀的原理是当哲学家参与机器学习时会发生什么:)这一古老的哲学理论的起源可以追溯到1287年至1347年之间，与托勒密等哲学家联系在一起。本质上，奥卡姆剃刀理论指出，如果我们有解释已知观察的竞争性假设，我们应该选择最简单的一个。从夏洛克·福尔摩斯到蒙克，奥卡姆剃刀在世界级的侦探中无处不在，他们经常遵循最简单和最符合逻辑的假设来揭开复杂的谜团。</p><p id="8df4" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">奥卡姆剃刀是我们日常生活中要遵循的明智的哲学原则，但它在机器学习中的应用充其量只会引起争议。从计算的角度来看，更简单的假设当然是更可取的，因为在这个世界上，算法因耗费资源而臭名昭著。此外，更简单的假设在计算上更容易推广。然而，超简单假设的挑战在于，它们经常导致过于抽象，无法模拟复杂的场景。因此，具有足够大的训练集和适当大小的维数的模型应该选择能够产生低训练误差的足够复杂的假设。否则，它将被提示填充不足。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/7adb4155ac75947e58b4a8daf026689f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQO7-HJxrY2bO3pE5klphA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图片鸣谢:谷歌人工智能</figcaption></figure><h2 id="0e6b" class="mu mv it bd mw mx my dn mz na nb dp nc mr nd ne nf ms ng nh ni mt nj nk nl nm bi translated">2)风险资本维度</h2><p id="36c0" class="pw-post-body-paragraph lc ld it lf b lg nn ju li lj no jx ll mr np lo lp ms nq ls lt mt nr lw lx ly im bi translated">奥卡姆剃刀是一个很好的简约原则，但这些抽象的想法不会直接转化为生活在数字宇宙中的机器学习模型。统计理论的创始人Vapnik和Chervonekis(VC)解决了这一挑战，他们提出了一个模型来量化统计算法的能力。这种技术被称为VC维，它基于确定最大数<em class="le"> m </em>，从该最大数存在目标机器学习功能可以任意标记的<em class="le"> m </em>个不同的<em class="le"> x </em>个点的训练集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/0db0800931b0d783dd7a5eb7befb81ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_oYLxOi5YdROYA1YRZUSsA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:斯坦福大学</figcaption></figure><p id="9fd7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">VC维度是统计学习的基石之一，并被用作许多有趣理论的基础。例如，VC维度有助于解释机器学习模型中泛化误差和训练误差之间的差距随着训练集大小的增加而减小，但随着模型容量的增加，这种差距也会增加。换句话说，拥有大量训练集的模型更有可能选择近似正确的假设，但如果有太多潜在的假设，那么我们很可能以错误的假设告终。</p><h2 id="ba3b" class="mu mv it bd mw mx my dn mz na nb dp nc mr nd ne nf ms ng nh ni mt nj nk nl nm bi translated">3)没有免费的午餐定理</h2><p id="81f3" class="pw-post-body-paragraph lc ld it lf b lg nn ju li lj no jx ll mr np lo lp ms nq ls lt mt nr lw lx ly im bi translated">我想用我最喜欢的机器学习原则之一来结束这篇文章，这个原则与过度适应-欠适应问题有关。没有免费的午餐定理表明，平均所有可能的数据生成分布，每个分类算法在分类以前未观察到的点时具有大致相同的错误率。我喜欢把没有免费的午餐定理看作是机器学习算法局限性的数学反理论，这些算法迫使我们使用有限的训练集来概括半绝对的知识。例如，在逻辑学中，从有限的例子中推断普遍规则被认为是“不合逻辑的”。对于机器学习从业者来说，没有免费的午餐定理是另一种说法，即在给定足够多的观察值的情况下，没有算法比其他算法更好。换句话说，机器学习模型的作用不是找到通用的学习函数，而是找到更适合目标场景的假设。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1f38c80f5341cdb425009c0a65473fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*WDV6g95mZD6n7co2ki2jog.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图片鸣谢:谷歌人工智能</figcaption></figure><p id="67bd" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mr ln lo lp ms lr ls lt mt lv lw lx ly im bi translated">过拟合和欠拟合仍然是机器学习应用中最严重的两个挑战。像VC维、奥卡姆剃刀和没有免费的午餐定理这样的理论为分析机器学习解决方案中过拟合和欠拟合条件的根源提供了强大的理论基础。理解和量化机器学习模型的能力仍然是理解其过拟合或欠拟合倾向的基本步骤。</p></div></div>    
</body>
</html>