<html>
<head>
<title>Why do We Need Activation Functions in Neural Networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么我们需要神经网络中的激活函数？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/why-do-we-need-activation-functions-in-neural-networks-4c7b0499365f?source=collection_archive---------0-----------------------#2020-07-26">https://pub.towardsai.net/why-do-we-need-activation-functions-in-neural-networks-4c7b0499365f?source=collection_archive---------0-----------------------#2020-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b295" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="b89a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">由示例激发的激活功能</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/305a9040d837611da94661e1ed89083d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QK1Cx2q32OHYeNtuS7Scuw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@marius?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">马里乌斯·马萨拉尔</a>在<a class="ae lh" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="69d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你正在读这篇文章，那么很可能你已经知道什么是神经网络，什么是激活函数，但是，一些关于机器学习的入门课程没有足够清楚地说明，为什么我们需要这些激活函数。我们需要它们吗？没有它们，神经网络还能工作吗？</p><p id="8a81" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们首先提醒自己一些关于神经网络的事情。它们通常在视觉上表示为类似图形的结构，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/4cb0893e4aca6cae9d2366b24f06248d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYvRWrzDhWZH4nl944zkVw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg" rel="noopener ugc nofollow" target="_blank">图片</a>由Glosser.ca / CC BY-SA在<a class="ae lh" href="https://commons.wikimedia.org/" rel="noopener ugc nofollow" target="_blank">维基共享</a></figcaption></figure><p id="a317" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面表示为一个具有3层的神经网络:输入层、隐藏层和输出层，由3、4和2个神经元组成。<br/>输入图层的结点数量与数据集的要素数量相同。对于隐藏层，您可以自由选择想要多少个节点，并且可以使用多个隐藏层。</p><p id="131f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了输入层中的神经元之外，网络中的每个神经元都可以被认为是一个线性分类器，它将前一层中神经元的所有输出作为输入，并计算这些输出加上一个偏差项的加权和。然后，下一层中的神经元将把前一层线性分类器计算的值作为输入，然后计算这些值的加权和，等等。我们希望，通过以这种方式组合线性分类器，我们可以构建更复杂的分类器，这些分类器可以表示我们数据中的非线性模式。</p><p id="6fdb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们看一下下面的数据集示例:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/8b23fbc20ba4d93c5dae8e3d6cb16b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9n-PPpihxrRYlIrhrTVpkA.png"/></div></figure><p id="213e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个数据集不是线性可分的，我们不能用一条线将一个类与另一个类分开。但是我们可以通过使用2条线作为判定边界来进行这种分离。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/e619b5c0fbe8d0e255a24e70fcbc9277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*td9QqrRFg_O4lyvKmbptnw.png"/></div></figure><p id="f6cf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，我们可能认为两个中间神经元可以完成这项工作。这两个神经元将学习上图中的两条分隔线。然后，我们将需要一个输出神经元，它将把这两个先前的神经元作为输入，然后它将能够正确地进行分类。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/7b8151bffb3ecf4867f5509e3256d8f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N4oFGLz1cAWF-YLHfB4P4A.png"/></div></figure><p id="b4b4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了使最后一个神经元正确分类，如果我们将n1和n2个隐藏神经元绘制在2d平面上，则它们的输出需要是线性可分的。上面绘制的2条线具有以下等式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/170d115cab3937312ed611d41649f51b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KjMbcWkiIHVaVcEce5FKlQ.png"/></div></figure><p id="60c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这意味着2个隐藏神经元正在计算输入x1和x2的以下线性组合:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/e199d6c369b5cf5188a5f6cc2c01975b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xn67xYIpFossCLZUs9E0jw.png"/></div></figure><p id="1f02" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在让我们画出n1和n2，看看它们是否帮助了我们。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/c5d288267a2bcb0637b0280a39c0a397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hw6v6N82LpgKISW3Ip3D_w.png"/></div></figure><p id="9e18" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们对我们小小的神经网络感到失望。n1和n2的输出仍然不是线性可分的，因此输出神经元不能正确分类。那么，问题是什么呢？事情是这样的<strong class="lk jd">任何线性函数的线性组合仍然是线性的</strong>，在一张纸上说服自己这是真的并不困难。本文末尾有这一事实的证明。因此，<em class="mf">无论我们使用多少层或多少个神经元，按照我们迄今为止的方式，我们的神经网络仍然只是一个线性分类器。</em></p><p id="a2e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们需要更多的东西。我们需要将每个神经元<em class="mf">计算出的加权和通过一个非线性函数</em>，然后将这个函数的输出视为那个神经元的输出。这些函数被称为<strong class="lk jd">激活函数</strong>，正如你将在本文中看到的，它们对于神经网络学习复杂的数据模式是必不可少的。</p><p id="044d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">已经证明[1]具有2层(除了输入层)和非线性激活函数的神经网络可以逼近任何函数，只要它在那些层中具有足够大数量的神经元。那么，如果只有两层就足够了，为什么现在人们使用更深的网络呢？嗯，仅仅因为这些2层网络“能够”学习任何东西，并不意味着它们容易优化。在实践中，如果我们给我们的网络<strong class="lk jd">过剩的容量</strong>，他们会给我们足够好的解决方案，即使他们没有优化得尽可能好。</p><p id="5830" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还有更多种类的激活函数，其中两种我们想在上面的例子中使用。它们是ReLU(<strong class="lk jd">Re</strong>ctified<strong class="lk jd">L</strong>linear<strong class="lk jd">U</strong>nit)和tanh(双曲正切)，如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi me"><img src="../Images/ee4c50d3aa81c8992d4b7660b20ab087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qq_tJBBv8ExaC4-mUUzvZg.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/991c8e801fe9b16e76cae7aa3a4a7c24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQaSQWBKcEEF7B1VYF4IZA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/38eebcf22e925683c5e038854189b678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqzuMrcf8e2ZKHFuWNyFLg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/b3eaa7978ac110e40618b7e1cfc79437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*un2BrgHR3eJkQfspnn5Yjg.png"/></div></figure><p id="8c73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们在例子中使用ReLU激活，会发生什么？下面绘制了施加ReLU激活后神经元n1和n2的输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/0f9b0b8f75cb83381b0e4133e0f5013d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hVniyoUsJ9q2AjHafaFkbA.png"/></div></figure><p id="36b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们的两类点可以由一条线分开，因此输出神经元可以正确地对它们进行分类。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/7d19b40ddda095159e0d34b58ddee727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqUWv-0xehCPndYVfH2wbQ.png"/></div></figure><p id="eaba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们使用tanh激活，会发生类似的事情，但这次我们的点被更大的余量分开得更好。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/4bfaecf0b38a04bbe45793ef3fe3f294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ajSK6oLoOl1kJ21oKEFTw.png"/></div></figure><p id="5610" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">同样，输出神经元可以正确分类这些点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/30bd28b7f73b529f60e41cab75d4bf49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gc8x5sVUarAxJP6iOPqCfg.png"/></div></figure></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="f4ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里有一个简单的数学证明，证明了任何线性函数的线性组合仍然是线性的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/a98df0d9eebc3871cab32de51a55b7da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*coHk4eFGmcwfVpJJhbIjNA.png"/></div></figure><p id="48f7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中a0，a1，…，an是不依赖于输入x1，…，xn的常数。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h2 id="2f68" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">参考</h2><p id="8773" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">[1] Cybenko，G.V. (2006年)。“通过叠加s形函数进行近似”。在范舒彭，简h(编辑。).控制、信号和系统数学。斯普林格国际公司。第303-314页。</p><p id="555e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望这些信息对你有用，感谢你的阅读！</p><p id="f673" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章也贴在我自己的网站<a class="ae lh" href="https://www.nablasquared.com/why-do-we-need-activation-functions-in-neural-networks/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>