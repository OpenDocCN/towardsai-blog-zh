<html>
<head>
<title>What is Overfitting and How to Solve It?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是过拟合，如何解决？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/machine-learning-f940984ff3df?source=collection_archive---------2-----------------------#2022-01-25">https://pub.towardsai.net/machine-learning-f940984ff3df?source=collection_archive---------2-----------------------#2022-01-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4667" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>:</h2><div class=""/><p id="0a5b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi ku translated">除非规范化，否则装修可能会很危险。事实上，初级数据分析师往往倾向于减少训练数据集中的偏差，并获得尽可能准确的估计，而忽略模型的样本外使用。在这里，我们将回顾岭回归，黄土，Lowess，光滑样条根据正则化的技术。</p><h2 id="c8c1" class="ld le iq bd lf lg lh dn li lj lk dp ll kh lm ln lo kl lp lq lr kp ls lt lu iw bi translated">介绍</h2><p id="06c0" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ij bi translated">作为一名<strong class="jy ja">计量经济学家</strong>或<strong class="jy ja">数据科学家，</strong>你的任务是开发稳健的模型，你需要掌握回归(我喜欢J.Angrist的说法)以及如何将其正规化。<br/>例如，对于线性模型，尽管<strong class="jy ja">线性回归</strong>是建模的黄金标准，但在某些情况下可能不是最佳解决方案。简而言之，如果你的模型对于底层数据来说太复杂，而底层数据需要一个简单的模型，那么我们就面临<strong class="jy ja">过度拟合</strong>！</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/ccbe22b19c17305bf0aa8cb0d350c0cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*3puza1Kv3ZC6Iss37wmmwA.gif"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">过度拟合</figcaption></figure><h2 id="a662" class="ld le iq bd lf lg lh dn li lj lk dp ll kh lm ln lo kl lp lq lr kp ls lt lu iw bi translated">过度拟合以及如何解决？</h2><p id="6de8" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ij bi translated">过度拟合是危险的，因为当模型对变化的方差赋予太多权重时，它是敏感的，结果是我们的模型对数据集中甚至最轻微的变化都反应过度。在数据科学和机器学习中，我们模型的这种过度反应被称为过度拟合。在线性模型中，只要您的要素与其他要素高度相关(多重共线性)，您的模型就可能会过度拟合。<br/>为了避免这种情况发生，你需要使用一种称为系数正则化(“收缩”)的技术，使模型变得健壮。换句话说，您需要通过减少将估计系数的系数推向零来正则化您的模型。<br/>数据科学中的这种技术也被称为“回归的惩罚”。<br/>根据具体情况，有几种方法可以使用这种技术。我们今天将看到<strong class="jy ja">岭回归、平滑样条、局部回归和Lowess(局部加权回归)。</strong></p><h2 id="524f" class="ld le iq bd lf lg lh dn li lj lk dp ll kh lm ln lo kl lp lq lr kp ls lt lu iw bi translated">里脊回归</h2><p id="4e12" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ij bi translated">是计量经济学、工程学等中常用的模型之一。当在<strong class="jy ja">线性回归</strong>中观察到独立变量之间的<strong class="jy ja">多重共线性</strong>(高相关性)时，通常使用回归。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mx"><img src="../Images/a4685cd88436c5be9b80f6e4e5a7c5ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGEvcoWfqgjyjXyjX-QKag.png"/></div></div></figure><p id="4a4b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">RR比OLS(通常是无偏的)具有更小的方差，并刺激模型对变化更稳健。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi my"><img src="../Images/b1e2281650811810b8249e4c8ab5034d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JkvsWl_sVYcCAOlS2teAmA.png"/></div></div></figure><p id="8436" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在上面的公式中，λ≥0是一个调整参数，它实际上不利于回归以降低复杂性。</p><h2 id="da54" class="ld le iq bd lf lg lh dn li lj lk dp ll kh lm ln lo kl lp lq lr kp ls lt lu iw bi translated"><strong class="ak">平滑样条</strong></h2><p id="ebde" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ij bi translated">当我们谈论平滑样条时，我们指的是非线性模型(例如多项式)。此外，当模型过于复杂时，非线性模型会遭受过度拟合。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/a884048f53c616de4bd83690c10d1446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*L8nmsjhnYTeZEYtNfwBFrg.gif"/></div></div></figure><p id="a07b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在这种情况下，我们将像在岭回归中一样使用复杂度惩罚，以及我们的公式将会是什么样子。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi my"><img src="../Images/7cf61eda5a355c0d9a84a6f407093600.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3kNJPavkgDC7jj4LC8i23Q.png"/></div></div></figure><p id="ef1c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">正如您已经注意到的，它与RR模型相似，唯一的区别是RR中预测变量的函数是β0+β1X1+β2X2+e。在我们的情况下，g(x)可以是任何其他非线性函数，这会导致过度拟合(通常是多项式)。λ是一种平滑效果。λ越大，函数g(x)越平滑。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mz"><img src="../Images/fdf61634be9f3b3b80334b1e76b0df66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R3acDtSjZ9KBwVQyXkq-Rg.png"/></div></div></figure><p id="7c98" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">正如你看到的，更平滑的样条模型是预测优化的。</p><h2 id="35fe" class="ld le iq bd lf lg lh dn li lj lk dp ll kh lm ln lo kl lp lq lr kp ls lt lu iw bi translated">局部加权回归(LOWESS)</h2><p id="2b6d" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ij bi translated">该技术背后的主要思想是在每个邻域内使用KNN回归。这项技术也有助于模型的稳健性。KNN的邻居是通过欧几里德距离找到的(但这不是我们今天的话题)</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi na"><img src="../Images/a128810d6541ffd536721f0611b8931f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wCAwDprR0HJsBGFF"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">欧几里德距离</figcaption></figure><h2 id="a54c" class="ld le iq bd lf lg lh dn li lj lk dp ll kh lm ln lo kl lp lq lr kp ls lt lu iw bi translated">局部回归(黄土)</h2><p id="751d" class="pw-post-body-paragraph jw jx iq jy b jz lv kb kc kd lw kf kg kh lx kj kk kl ly kn ko kp lz kr ks kt ij bi translated">黄土是一种强大的技术，可用于线性或非线性最小二乘回归。黄土是一种非参数方法，因此它没有固定的公式。然而，为了计算X=x0的黄土，您需要遵循以下步骤。</p><ul class=""><li id="8e01" class="nb nc iq jy b jz ka kd ke kh nd kl ne kp nf kt ng nh ni nj bi translated">收集Xi最接近X0的训练数据的s= k/n</li><li id="d0cd" class="nb nc iq jy b jz nk kd nl kh nm kl nn kp no kt ng nh ni nj bi translated">指定一个权重Ki0=K(xi，x0 ),其中最近的邻居得到某个权重，最远的邻居得到0。超出这些邻居的所有点的权重为零。</li><li id="6305" class="nb nc iq jy b jz nk kd nl kh nm kl nn kp no kt ng nh ni nj bi translated">拟合加权最小二乘回归</li></ul><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi np"><img src="../Images/52aac855e4ea0d42467fc778481e31b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*dAEIAt0gZ5SJit40INsdbA.png"/></div></figure><ul class=""><li id="4115" class="nb nc iq jy b jz ka kd ke kh nd kl ne kp nf kt ng nh ni nj bi translated">x0处的拟合值由下式给出</li></ul><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b5fb1a63247f9a8393d978387b7bf575.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*R0Qu1xWX_Bg19yeZyAHpAQ.png"/></div></figure></div></div>    
</body>
</html>