<html>
<head>
<title>Linear Regression from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始线性回归</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/linear-regression-from-scratch-d7cb0a22ddfb?source=collection_archive---------0-----------------------#2020-04-22">https://pub.towardsai.net/linear-regression-from-scratch-d7cb0a22ddfb?source=collection_archive---------0-----------------------#2020-04-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/772ede581d1ae69ce2743e9c4405a36e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xjb36_k8NXCaolbLq1rwXQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">威廉·戴尼奥在<a class="ae jd" href="https://unsplash.com/s/photos/stairs-line?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><div class=""/><div class=""><h2 id="c020" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">如果你开始学习数据科学或机器学习，你也听说过线性回归，但它是什么，我们应该在什么时候使用它？</h2></div><p id="5859" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">线性回归是一种统计模型，是一种监督算法；我们用它来预测线性值，这意味着我们试图用我们的数据拟合一条线，并从中预测自变量。为了更好地理解线性回归，让我们从它的方程开始。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/1abf9d986ac267cb45ff44e3249bbe10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDo4fp95lQI77kttpF2fWw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">简单的线性回归方程</figcaption></figure><p id="6bab" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中，y是预测值，x是独立变量，m是斜率，b是y截距，即直线与y轴的交点。斜率表示线的倾斜度，其中0表示水平线，但通常情况下，等式为:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/48a8c23b8b754fe5c10336719d3bbfc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F6N3t_RiAWNaNSRzh6e-ZQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">斜率方程</figcaption></figure><p id="d806" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个简短的理论介绍之后，让我们解释一下如何训练和测试我们的第一个线性回归模型。我们将根据多年的经验，使用线性回归模型来预测工资。首先，我们必须确定我们试图预测的是一个线性值；了解线性回归模型是否适合我们的数据的一种方法是将它们可视化，让我们看一个样本数据集图:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="e43d" class="mb mc jg lx b gy md me l mf mg">salary = df["Salary"]<br/>years_experience = df["YearsExperience"]</span><span id="9f63" class="mb mc jg lx b gy mh me l mf mg">#generating the scatter plot<br/>plt.figure(figsize=(7, 5))<br/>plt.scatter(years_experience, salary)<br/>plt.xlabel("Years of Experience")<br/>plt.ylabel("Salary")</span></pre><p id="d955" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面代码的结果将类似如下:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/22e0effb8058ae4958202827c951a5fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*SXk7ZEHiLzgC1mDYcSm80w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">经验年限与薪水</figcaption></figure><p id="b31e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如您所见，散点图中绘制的值描绘了一条直线。那么我们可以说，一个线性回归模型可以拟合这个数据。现在让我们用Sklearn训练一个线性回归模型:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="42cf" class="mb mc jg lx b gy md me l mf mg">#train test split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span><span id="14e2" class="mb mc jg lx b gy mh me l mf mg">#training<br/>lr = LinearRegression().fit(X_train,y_train)</span></pre><p id="109c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">恭喜你，你已经训练出了一个线性回归模型！现在，我们必须看看结果。让我们先看看直线的交点和斜率。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="7605" class="mb mc jg lx b gy md me l mf mg">print("Intercept = ", lr.intercept_)<br/>print("Slope = ", lr.coef_)</span><span id="5f5e" class="mb mc jg lx b gy mh me l mf mg">#output<br/>Intercept =  26777.391341197625<br/>Slope =  [9360.26128619]</span></pre><p id="3cf9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们测试我们的模型，从测试集中预测一些值。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="46a3" class="mb mc jg lx b gy md me l mf mg">y_pred = lr.predict(X_test)</span><span id="d3c8" class="mb mc jg lx b gy mh me l mf mg">df = pd.DataFrame({"Actual": y_test, "Predicted": y_pred})<br/>df</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/c3d96024ea7921e172416cd6ce73ca1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_vUNZcbFxWN_HKcQ8vHJA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">实际值与预测值。</figcaption></figure><p id="3fc6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从结果中可以看出，预测值接近实际值，但它们并不相同，这是因为直线并不完全符合每个值，让我们看一下直线本身。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="e80f" class="mb mc jg lx b gy md me l mf mg">#generating the scatter plot<br/>plt.scatter(X_test, y_test)<br/>plt.plot(X_test, y_pred, color="red", linewidth=2)<br/>plt.xlabel("Years of Experience")<br/>plt.ylabel("Salary")</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/c48295b4a2edd12607020dc4e39cf3fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*U5MzKlUai_pUHoBxkU6BSA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">线性回归散点图。</figcaption></figure><p id="f2e1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">蓝点表示测试集上的实际值，而红线是基于训练数据的预测。你可以看到，实际值和预测线之间有一个距离，这个距离叫做残差，我们可以计算出来:残差=实际值—预测值。</p><h2 id="4565" class="mb mc jg bd mj mk ml dn mm mn mo dp mp le mq mr ms li mt mu mv lm mw mx my mz bi translated">估价</h2><p id="53ea" class="pw-post-body-paragraph kv kw jg kx b ky na kh la lb nb kk ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">有不同的指标来评估我们的回归模型:让我们看看三个最重要的:</p><ul class=""><li id="3dc7" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated">均方差(MSE)是估计值和实际值之间的平均平方差。这是最受欢迎的指标，因为它侧重于较大的误差，因为平方项会使较大的误差与较小的误差相比呈指数增加。</li></ul><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/dc3c1c9e4bc4181977102a0eefe5597d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vcn5sET1W-KzfZw9yx4oRA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">MSE方程</figcaption></figure><ul class=""><li id="7285" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated">平均绝对误差(MAE)是估计值和实际值之间的平均差值。这是最容易理解的指标，因为它只是一个平均误差。</li></ul><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/dd1f3ca75bda1b296645ab92da4ed3d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pmjFwB55ggM7i22_pK5HGg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">梅方程</figcaption></figure><ul class=""><li id="dfb4" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated">均方根误差(RMSE)是估计值和实际值之间的平均差值的平方根；基本上就是均方误差的平方根。</li></ul><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/97f676037326d30a2012e9c25ab32481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HG_dOlBqkMj62VuAFGBViA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">RMSE方程</figcaption></figure><ul class=""><li id="78de" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated">r平方是一个流行的度量标准，而不是误差，它表示数据与拟合回归线的接近程度。分数越高，模型就越符合您的数据。最好的可能得分是1.0，但是对于一个坏的模型也可能是负的。</li></ul><p id="5646" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上面的公式中，n表示预测的次数，y表示实际值的向量，yi表示预测值的向量。</p><p id="5c88" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们刚刚训练的模型也称为简单线性回归，因为只有一个自变量。</p><h1 id="c019" class="no mc jg bd mj np nq nr mm ns nt nu mp km nv kn ms kp nw kq mv ks nx kt my ny bi translated">多元线性回归</h1><p id="b6df" class="pw-post-body-paragraph kv kw jg kx b ky na kh la lb nb kk ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">在多元线性回归中，我们有不止一个自变量，这意味着我们可以检查更多特征对因变量的影响。当您检查数据集时，多元线性回归非常有用，因为您可以比较不同的要素，并检查它们对因变量的影响有多大差异，以便您可以只选择最佳的要素。让我们看看它的公式:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/a4a17996e82f2b527cd3a684118baeda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*55GXeOxkNtbjVCe5NiN-HA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">多元线性回归方程</figcaption></figure><p id="342d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上式中，b是y截距，m是每个特征的系数，x代表每个特征；我们将通过一个示例数据集来更好地理解这一点。</p><p id="9f00" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将用来更好地解释多元线性回归的数据集是油耗数据集，我们将创建一个多元线性回归模型来查看多个特征(发动机尺寸、气缸和油耗)对因变量(二氧化碳排放量)的影响。让我们来看第一个散点图，它代表了发动机尺寸和二氧化碳排放量之间的关系。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="5275" class="mb mc jg lx b gy md me l mf mg">#generating the scatter plot<br/>plt.figure(figsize=(7, 5))<br/>plt.scatter(df.ENGINESIZE, df.CO2EMISSIONS,  color='blue')<br/>plt.xlabel("Engine size")<br/>plt.ylabel("CO2 Emissions")</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/9edf3082405440ea812b4e3fbd475b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*c7lpd7khcYwtK1oUh9eV7g.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">发动机尺寸与二氧化碳排放</figcaption></figure><p id="a23c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另外两个代表我们正在研究的另外两个特征与二氧化碳排放的关系，与第一个特征相比，这两个特征有很大的不同，并且没有第一个特征那么线性:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/a8931514d2788d5cd448f1e3a86b7385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*xDKVaZhV9y9-YmrYQoWRoA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">燃料消耗与二氧化碳排放</figcaption></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/a8f0248e5a3bf598f254157ab933fa25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*2F-hwe34622JkMxM2Ac47A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">气缸与二氧化碳排放</figcaption></figure><p id="6e86" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们用这三个特征训练一个多元回归模型:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="9634" class="mb mc jg lx b gy md me l mf mg">#train test split randomly with numpy<br/>msk = np.random.rand(len(df)) &lt; 0.8<br/>train = df[msk]<br/>test = df[~msk]<br/>X_train = np.asanyarray(train[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])<br/>y_train = np.asanyarray(train[['CO2EMISSIONS']])</span><span id="a9cc" class="mb mc jg lx b gy mh me l mf mg">#training<br/>lr.fit (X_train, y_train)</span></pre><p id="070e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们从公式中看到的，有y截距，有多个系数，每个特征一个。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="0aa7" class="mb mc jg lx b gy md me l mf mg">print ("Intercept: ", lr.intercept_)<br/>print ("Coefficients: ", lr.coef_)</span><span id="133d" class="mb mc jg lx b gy mh me l mf mg">#output<br/>Intercept:  [67.3968258]<br/>Coefficients:  [[10.23919289  7.99343458  9.31617168]]</span></pre><p id="16c6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">既然我们训练了模型，让我们评估它，预测测试集中的一些值。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="7835" class="mb mc jg lx b gy md me l mf mg">y_pred= lr.predict(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])<br/>X_test = np.asanyarray(test[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB']])<br/>y_test = np.asanyarray(test[['CO2EMISSIONS']])</span></pre><p id="19e4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们从数字上看它的分数，计算MSE、MAE、RMSE和R平方分数。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="1a83" class="mb mc jg lx b gy md me l mf mg">print("Mean Squared Error:", metrics.mean_squared_error(y_test, y_pred)) <br/>print("Mean Absolute Error: ", metrics.mean_absolute_error(y_test, y_pred))<br/>print("Root Mean Squared Error: ", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))<br/>print("R-Squared Score: ", metrics.r2_score(y_test, y_pred))</span><span id="bc9b" class="mb mc jg lx b gy mh me l mf mg">#output<br/>Mean Squared Error: 513.8862607223467<br/>Mean Absolute Error:  16.718115229014394<br/>Root Mean Squared Error:  22.66905954649082<br/>R-Squared Score:  0.8789478910480176</span></pre><p id="3898" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">而且，我们完了！我们学习了线性回归是如何工作的，我们训练并评估了一个简单的线性回归模型和一个多元线性回归模型。如果你想了解更多关于线性回归的知识，你可以访问<a class="ae jd" href="https://github.com/nlogallo/datascience/tree/master/linear_regression_from_scratch" rel="noopener ugc nofollow" target="_blank">我的GitHub </a>，在那里你可以看到我关于它的笔记本。</p><p id="beed" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">感谢阅读。</p></div></div>    
</body>
</html>