<html>
<head>
<title>Googles Imagen Model Better than DALLE-2?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌Imagen模型比DALLE-2更好？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/googles-imagen-model-better-than-dalle-2-45e4d4aff072?source=collection_archive---------0-----------------------#2022-05-29">https://pub.towardsai.net/googles-imagen-model-better-than-dalle-2-45e4d4aff072?source=collection_archive---------0-----------------------#2022-05-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="800e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谷歌的模型似乎优于OpenAI，是吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/573d06d54ad09bb813b22d9ad4b1c956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q1FX2WqOQgelptoiTo4R7Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">肖恩·盖洛普/盖蒂图片社</figcaption></figure><p id="a2cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你认为Dall-E 2带来了令人印象深刻的结果，那就等着看这个最新的谷歌大脑模型能完成什么吧。Dalle-e很棒，但它经常缺乏真实感，该团队试图使用Imagen这种新模型来解决这一问题。在他们的项目网站上，他们公布了一些结果以及他们为评估文本到图像模型而创建的基准，在这些结果中，他们明显超过了Dall-E 2和早期的图片创建算法。</p><p id="c344" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">随着我们检查越来越多的文本到图像算法，比较结果变得越来越不可能——除非我们假设结果很糟糕，而我们经常这样做。但是这款机型和Dell-e 2一样，都不太可能成功。</p><p id="6e07" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">TL；dr:这是一种新的文本到图像的模型，可与Dalle-E 2相媲美，但根据人类测试人员的说法，它更现实。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/d43063cb338e2f2057fd06380140ff14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*gxpk1qGNw-5AoHLULJqAwA.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">GIF来自Imagen网站</figcaption></figure><p id="748e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，类似于我大约一个月前写的Dall-E，这个模型采用类似“一只戴着蓝色方格贝雷帽和红色圆点高领的金毛寻回犬”的文本，并试图用它制作一个照片般逼真的图像。<br/>这里的要点是Imagen不仅可以抓住文本，还可以抓住它制作的视觉效果，这比以前的任何作品都更真实。</p><p id="a1c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，当我说理解时，我指的是它自己与我们截然不同的理解。模型无法理解它创建的文本或图像。它无疑对此有所了解，但它主要理解的是如何使用图像上的像素来描绘包括这些项目在内的这种特定类型的文本。然而，当我们检查结果时，它看起来确实理解我们发送给它的内容！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/231c4f8a1fac11dbc1cf770f8fd5aecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9huVSpJWt6pii1inVKK13g.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">GIF来自Imagen网站</figcaption></figure><p id="86cb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你显然可以用一些看起来不真实的奇怪句子来愚弄它，比如这个，但它偶尔会超越你的想象，做出一些真正了不起的东西。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/43e042c138f29f24cc8d0da041e2d606.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5UOSY8h7DQUAke8bzbidWg.png"/></div></div></figure><p id="95c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更有趣的是它是如何利用扩散模型工作的，这是我从未在频道中提到过的。然而，在我们能够使用这种扩散模型之前，我们必须首先理解文本输入。这也是Dall-e和其他人的根本区别。为了像人工智能系统一样掌握材料，他们使用了一个类似于GPT 3的大型文本模型。他们不是在图像生成模型旁边训练文本模型，而是简单地利用一个大的预先训练的模型，该模型被冻结，使得它在整个图像生成模型的训练过程中不会改变。根据他们的研究，这导致了显著更高的结果，并且该模型似乎对语言有更好的理解。</p><p id="38f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，这个文本模块是模型理解文本的方式，这种知识用编码来表示，这是模型在大规模数据集上被教会做的事情，以将文本输入转换为它可以利用和理解的信息空间。现在，我们必须利用修改后的文本数据来创建图片，正如我之前所说，他们使用了扩散模型。</p><p id="0d00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，首先，什么是扩散模型？</p><p id="c6b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">扩散模型是学习如何反复反转高斯噪声以将随机高斯噪声转换成图片的生成模型。它们是超分辨率或其他图像到图像转换的有效模型，在这种情况下，它们采用了经过修改的U-Net架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/b55ae3f795fd9eb96943bf76f6bfad50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I6-HHVRs38ZNGaZFRN7S3Q.png"/></div></div></figure><p id="f210" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从本质上来说，该模型已经训练过从纯噪声中去除图片的噪声，他们使用文本编码和一种称为无分类器引导的技术来定向图片，他们声称这对于研究结果的质量至关重要，并在他们的工作中进行了详细描述。通过下面参考资料中的链接，我将让您阅读该策略的更多细节。</p><p id="ede1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们有了一个模型，可以采用随机高斯噪声和我们的文本编码，并使用文本编码作为指导来消除噪声，以获得我们的图像。但是，正如你在上面的图表中看到的，这并不像看起来那么简单。我们刚刚创建的图像是相当适度的，因为更大的图像需要更多的计算和更大的模型，这两者都是不切实际的。相反，我们使用我们刚刚概述的扩散模型创建一个照片级真实感图像，然后使用各种扩散模型逐渐增加图像的质量。同样，我们想要噪声而不是图片，所以我们用一些高斯噪声污染这个低分辨率图像，并训练我们的第二扩散模型来增强它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/0f7f0f7ae9668f2b5f14274b9217ade7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*r5FgUZbx6s2ftg6vKYquTg.gif"/></div></div></figure><p id="4649" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，对于另一个模型，我们重复这两个阶段，但这次只使用图片的一部分来实现相同的放大比例，同时保持计算上的可行性。</p><p id="1bda" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在你知道了！最终我们得到了逼真的高分辨率图像！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/e40edc47ddcc7cfc65330504ab6fdb16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*WpSBypTahgY69-2BUCAoow.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来自Imagen网站</figcaption></figure><p id="12b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，这只是对这个迷人的新模型及其令人印象深刻的发现的一个快速概括。我强烈建议你阅读他们的优秀作品，以便更好地掌握他们的方法，并彻底检查他们的发现。</p><p id="b756" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你相信结果和Dell-e 2相当吗？是变好了还是变坏了？我相信现在是达尔-梅杰的竞争。请让我知道你对这个最新的谷歌大脑发布以及解释的看法。</p></div></div>    
</body>
</html>