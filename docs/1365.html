<html>
<head>
<title>Step-by-step implementation of GANs on custom image data in PyTorch: Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中定制图像数据的GANs的逐步实现:第2部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/step-by-step-implementation-of-gans-on-custom-image-data-in-pytorch-part-2-182f2fa16114?source=collection_archive---------1-----------------------#2021-01-10">https://pub.towardsai.net/step-by-step-implementation-of-gans-on-custom-image-data-in-pytorch-part-2-182f2fa16114?source=collection_archive---------1-----------------------#2021-01-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="df22" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="497d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">了解GAN架构中的不同层，调试一些常见的运行时错误，并深入了解用PyTorch编写代码背后的直觉。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/13e6faf32348f2251d93f41a1dca3dca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j5UdVyoysjRRJtyK-qpJwA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">图片来自<a class="ae lh" href="https://pixabay.com/users/comfreak-51581/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1947878" rel="noopener ugc nofollow" target="_blank">com break</a>来自<a class="ae lh" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1947878" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></figcaption></figure><p id="0e85" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在关于GANs的第1部分 中，我们开始建立直觉，关于GANs是什么，为什么我们需要它们，以及训练GANs背后的整个要点是如何创建一个生成器模型，该模型知道如何将随机噪声向量转换成(漂亮的)<em class="me">几乎</em>真实的图像。因为我们已经在第1部分中深入讨论了伪代码，所以一定要检查一下，因为会有很多对它的引用！</p><p id="b452" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想继续下去，这里有<a class="ae lh" href="https://github.com/V-Sher/GANs_Pytorch" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Github笔记本</strong> </a>包含使用PyTorch框架训练GANs的源代码。</p><blockquote class="mf mg mh"><p id="b173" class="li lj me lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">训练GAN网络背后的整个思想是获得生成器网络(具有最优的模型权重和层等)。)很擅长吐出看起来像真的假货！</p></blockquote><p id="e554" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">注:我想花一点时间来真正欣赏Nathan Inkawhich，他写了一篇精彩的文章</em>   <em class="me">，解释了DCGANs的内部工作原理，以及Pytorch的官方库</em><a class="ae lh" href="https://github.com/pytorch/examples/tree/master/dcgan" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"><em class="me">Github</em></strong></a><em class="me">，帮助我实现代码，特别是生成器和鉴别器的网络架构。希望我在这篇文章中给出的解释能够帮助你获得关于GANs的更进一步的清晰(比前面提到的博客中已经给出的更清晰),并在你自己的用例中更好地实现它们！</em></p><h1 id="f969" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">准备图像数据集</h1><p id="cd4f" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">我开始写这篇文章的主要原因之一是因为我想尝试在自定义图像数据集上编码GANs。我遇到的大多数教程都使用了一个流行的数据集(如MNIST、CIFAR-10、Celeb-A等),这些数据集预装在框架中，可以开箱即用。相反，我们将使用Kaggle上的<a class="ae lh" href="https://www.kaggle.com/ashwingupta3012/human-faces" rel="noopener ugc nofollow" target="_blank">人脸数据集</a>，它包含大约7k张图片——有各种各样的侧面/正面姿势、年龄组、性别等——这些图片都是从网上搜集来的。</p><p id="6e04" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将图像文件夹(名为<code class="fe ni nj nk nl b">Humans</code>)解压并加载到当前工作目录后，让我们开始在Jupyter笔记本中编写代码:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/bbc6a8296f1912eb1249315018b110b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BR0XmmlptBKGeaJOWleBvw.png"/></div></div></figure><p id="0fbb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了便于处理，我们将高清图像缩小到了更小的分辨率，即32x32。一旦您完成了本教程，您就可以在更改了<code class="fe ni nj nk nl b">img_shape</code>参数和其他一些东西(我们将在文章末尾讨论)之后重新运行代码。</p><p id="24eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们将把所有的图像转换成NumPy数组，并把它们一起存储到<code class="fe ni nj nk nl b">X_train</code>中。同样，显式地将图像转换成RGB格式总是一个好主意(以防某些图像<em class="me">看起来</em>是灰度级，但实际上不是！).</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/45dcc2c94dce0eb8a7bd80e57a2e830b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_UjOxw17ftmAXYEL5yNxQ.png"/></div></div></figure><p id="3103" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意:有些文件名不包含任何图像(或者肯定是损坏的)，这就是为什么我在编码时倾向于使用try-except块。</p><p id="1f27" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">注意:将图像转换为各自的NumPy格式可能需要一段时间。因此，最好将</em> <code class="fe ni nj nk nl b"><em class="me">X_train</em></code> <em class="me">作为</em> <code class="fe ni nj nk nl b"><em class="me">.npy</em></code> <em class="me">文件存储在本地以备将来使用。为此:</em></p><pre class="ks kt ku kv gt no nl np nq aw nr bi"><span id="05e0" class="ns mm it nl b gy nt nu l nv nw">from numpy import asarray<br/>from numpy import savez_compressed</span><span id="570d" class="ns mm it nl b gy nx nu l nv nw"><em class="me"># save to npy file</em><br/>savez_compressed('kaggle_images_32x32.npz', X_train)</span></pre><p id="d0b6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要在以后重新加载文件:</p><pre class="ks kt ku kv gt no nl np nq aw nr bi"><span id="2945" class="ns mm it nl b gy nt nu l nv nw"><em class="me"># load dict of arrays<br/></em>dict_data = np.load('kaggle_images_32x32.npz')</span><span id="b53b" class="ns mm it nl b gy nx nu l nv nw"><em class="me"># extract the first array<br/></em>data = dict_data['arr_0']</span><span id="827c" class="ns mm it nl b gy nx nu l nv nw"><em class="me"># print the array<br/></em>print(data)</span></pre><h1 id="ef8d" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">导入库</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/d41e5f0dcd8103dd2ee479be1a973d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wKGjIEryrm0CCXeWxhD_Q.png"/></div></div></figure><h1 id="1581" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">设置GPU支持</h1><p id="f035" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">虽然我们今天正在编写的代码可以在CPU和GPU上运行，但是检查可用性并尽可能使用后者总是明智的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/9ac589b8670f74e35fcdfa57e1e85edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hMwmte6hlq1iBPN94rDPFA.png"/></div></div></figure><p id="ca7a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用<code class="fe ni nj nk nl b">torch.cuda.is_available()</code>检查GPU是否可用，如果可用，使用<code class="fe ni nj nk nl b">torch.device</code>功能将其设置为设备。</p><h1 id="9f92" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">定义助手功能</h1><p id="b303" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">我们主要需要一个助手函数<code class="fe ni nj nk nl b">plot_images()</code>，它接受一个NumPy图像数组作为输入，并在一个5x5的网格中显示图像。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/20ea019a1821920db63337db54e8b580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M2LajeBhRQqmzgy4-NFC3w.png"/></div></div></figure><p id="ef35" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了以网格网络的形式布局图像，我们为每个要显示的图像在绘图区域添加了一个子绘图。使用<code class="fe ni nj nk nl b">fig.add_subplot</code>，子情节将在具有<code class="fe ni nj nk nl b">r</code>行和<code class="fe ni nj nk nl b"><em class="me">c</em></code>列的网格上占据具有位置的<em class="me">。最后，可以使用<code class="fe ni nj nk nl b">plt.show()</code>显示整个网格。</em></p><p id="152c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了查看我们的功能是否如预期的那样工作，我们可以显示一些来自我们的训练集的图像:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/7e8bd1797e107ff9d138ace142a9d5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QtADt2pCQQYITBcNvWDNqw.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e8a4aa68b8eb423d310e0ce0079bfc49.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*0EmspY44hpfu4G3XSScUNg.jpeg"/></div></figure><p id="38cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">注意:请忽略失真的质量(与我们在Kaggle上看到的原始数据相比)，因为我们使用的是32x32的图像，而不是高分辨率的图像！</em></p><h1 id="8adb" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">正在准备自定义数据集类</h1><p id="774f" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">我知道你在想什么— <em class="me">为什么我需要为我的数据集创建一个特殊的类？按原样使用我的数据集有什么问题？</em></p><p id="68da" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">嗯，简单的答案是PyTorch就是这么喜欢它的！要获得详细的答案，你可以在这里阅读这篇文章<a class="ae lh" href="https://towardsdatascience.com/recreating-keras-code-in-pytorch-an-introductory-tutorial-8db11084c60c" rel="noopener" target="_blank"/>，它很好地解释了如何使用PyTorch中的<code class="fe ni nj nk nl b">torch.utils.data.Dataset</code>类为任何数据集创建一个定制的<code class="fe ni nj nk nl b">Dataset</code>对象。</p><blockquote class="mf mg mh"><p id="af38" class="li lj me lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">在非常基础的层面上，你为自己的数据集扩展的<code class="fe ni nj nk nl b">Dataset</code>类应该有<code class="fe ni nj nk nl b">__init__</code>、<code class="fe ni nj nk nl b">__len__()</code>和<code class="fe ni nj nk nl b">__getitem__ </code>方法。</p></blockquote><p id="a321" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您需要进一步帮助创建数据集类，请查看PyTorch文档<a class="ae lh" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/7729b8d2c24de472ae94017c87f38b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1YI2QfYa99h3evTO-sVbIg.png"/></div></div></figure><h1 id="6c6c" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">创建数据加载器以加载图像</h1><p id="f61c" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">再说一遍— <em class="me">我到底为什么需要数据加载器？</em></p><p id="8064" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在<a class="ae lh" href="https://pytorch.org/docs/stable/data.html#module-torch.utils.data" rel="noopener ugc nofollow" target="_blank">文档</a>页面列出的许多原因中，例如定制数据加载顺序和自动内存锁定——数据加载器对于创建作为模型输入发送的批次(用于训练和测试集)非常有用。</p><p id="8d5f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">PyTorch中定义数据加载器的方式如下:</p><pre class="ks kt ku kv gt no nl np nq aw nr bi"><span id="3137" class="ns mm it nl b gy nt nu l nv nw">dataloader = DataLoader(dataset = dset, batch_size = batch_size, shuffle = shuffle)</span></pre><p id="890b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe ni nj nk nl b">dset</code>基本上是我们之前创建的<code class="fe ni nj nk nl b">HumanFacesDataset</code>类的一个对象。让我们定义它，以及我们的<code class="fe ni nj nk nl b">batch_size</code>和<code class="fe ni nj nk nl b">shuffle</code>变量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/4e1b33d4d216fb11232f674004740e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ov0Scc0tZr5OeHaYUIOXOg.png"/></div></div></figure><p id="ea92" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">需要注意的重要一点是，在为类创建构造函数时，我们并不是简单地将图像数组传递给它。相反，我们传递对原始图像数组执行了一些计算的<code class="fe ni nj nk nl b">transpose_imgs</code>。主要是，我们将</p><ul class=""><li id="0dab" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated">(a)使用<code class="fe ni nj nk nl b">np.float32()</code>显式地将图像表示设置为float这是因为默认情况下NumPy输入使用double作为数据类型(您可以使用<code class="fe ni nj nk nl b">imgs['arr_0'][0].dtype</code>对单个图像验证这一点，输出将是<code class="fe ni nj nk nl b">float64</code>)并且我们将创建的模型将具有权重为<code class="fe ni nj nk nl b">float32</code>；和</li><li id="0f38" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">(b)我们使用<code class="fe ni nj nk nl b">np.tranpose()</code>将每个图像的尺寸从(32 x 32 x 3)重新排序为(3 x 32 x 32 ),因为这是PyTorch模型中的图层预期的尺寸。</li></ul><p id="af48" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们选择一个小的<code class="fe ni nj nk nl b">batch_size</code>，并将<code class="fe ni nj nk nl b">shuffle</code>设为<code class="fe ni nj nk nl b">True</code>，以排除在数据收集时出现任何偏差的可能性。</p><h1 id="4021" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">定义生成器类</h1><p id="9438" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">正如在第1部分中所讨论的，生成器是一个神经网络，它试图产生(希望)逼真的图像。为此，它将一个随机噪声向量<em class="me"> z </em>作为输入(比如一个大小为100的向量；其中100的选择是任意的)，使其通过网络中的几个隐藏层，并最终输出与训练图像大小相同的RGB图像，即形状张量(3，32，32)。</p><p id="1bfc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">坦率地说，我花了一段时间来掌握应该放入我的生成器类的层(以及相关参数值)的精确排列和组合。简单地说，我们将只使用以下几层:</p><ul class=""><li id="7dba" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated">ConvTranspose2d是MVP，它将帮助您对随机噪声进行上采样，以创建图像，即从100x1x1到3x32x32。<br/>从<a class="ae lh" href="https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html" rel="noopener ugc nofollow" target="_blank">文档</a>中，可以看到它采用了以下形式:</li></ul><pre class="ks kt ku kv gt no nl np nq aw nr bi"><span id="0c39" class="ns mm it nl b gy nt nu l nv nw">ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias)</span></pre><ul class=""><li id="6e4d" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated">顾名思义，BatchNorm2d层用于对输入应用批处理规范化。根据<a class="ae lh" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html" rel="noopener ugc nofollow" target="_blank">文档</a>，它将特征的数量或<code class="fe ni nj nk nl b">num_features</code>作为输入，这可以根据前一层的输出形状很容易地计算出来。主要是，如果期望输入的大小为(N，C，H，W) ，i <strong class="lk jd"> ts值为<em class="me"> C </em>。例如，如果前一层的输出形状是<code class="fe ni nj nk nl b">batch_size, 512, 4, 4)</code>，那么就是<code class="fe ni nj nk nl b">num_features = 512</code>。</strong></li><li id="e7cc" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">ReLU或整流线性单元是发电机网络中使用的激活功能。简单来说，这一层如果是正的就直接输出输入，否则输出零。</li><li id="be12" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">Tanh是另一个激活函数，应用于发电机网络的最末端，将输入转换到[-1，1]范围。</li></ul><p id="b95e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，具有上述所有层的生成器类看起来会像这样。如果你需要一个关于如何在Pytorch中创建网络的初学者指南，请点击这里查看这篇文章。</p><blockquote class="mf mg mh"><p id="9933" class="li lj me lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">在一个非常基本的层面上，你为自己的网络模型扩展的<code class="fe ni nj nk nl b"><em class="it">Module</em></code>类应该有<code class="fe ni nj nk nl b"><em class="it">__init__</em></code>和<code class="fe ni nj nk nl b"><em class="it">forward </em></code>方法。</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/358f071256c76ae39a821c5411ac6e8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AoNhsmn6CVjznwoQhhWZ_g.png"/></div></div></figure><p id="1033" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">注意:了解该网络内部的情况非常重要，因为如果您想要处理图像，比如大小为64x64或128x128的图像，则必须更新该架构(主要是相关参数)!</em></p><p id="67ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们正在创建一个连续的<strong class="lk jd"> </strong>模型<strong class="lk jd"> </strong>，它是一个线性的层叠。也就是说，每一层的输出充当下一层的输入。</p><p id="d419" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在第一个卷积中，我们从花费100 <code class="fe ni nj nk nl b">input_channels</code>的<code class="fe ni nj nk nl b">ConvTranspose2d</code>层开始。为什么是100？你可能会问。这是因为发生器网络的输入将是类似于<code class="fe ni nj nk nl b">batch_size, 100, 1, 1</code>的东西，根据PyTorch的说法，这大致相当于100个通道的1x1图像。因此，这许多通道将进入<code class="fe ni nj nk nl b">ConvTranspose2d</code>层，因此，<code class="fe ni nj nk nl b">in_channels = 100</code>。</p><p id="5c6b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将<code class="fe ni nj nk nl b">out_channels</code>设置为512背后的逻辑完全是任意的，这是我从一开始提到的教程/博客中获得的。这个想法是在开始的时候为<code class="fe ni nj nk nl b">out_channels</code>选择一个大的数字，随后，为每个<code class="fe ni nj nk nl b">ConvTranspose2d</code>层减少这个数字(减少2倍),直到你到达可以设置<code class="fe ni nj nk nl b">out_channels = 3</code>的最后一层，这是我们生成32x32大小的RGB图像所需的精确通道数。</p><p id="1161" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，该层的输出将具有空间大小<code class="fe ni nj nk nl b">b_size, out_channels, height, width)</code>，其中高度和宽度可以根据<a class="ae lh" href="https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html" rel="noopener ugc nofollow" target="_blank">文档</a>页面上给出的公式进行计算。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/9a7206bd15ab0ba74b91d45cd60e7f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZUgPdtBEO7iH1jxFEqpTA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="9527" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的公式中代入各自的值，我们得到:</p><p id="86a2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">H _ out =(1–1)* 1–2 * 0+1 *(4–1)+0+1；和<br/>W _ out =(1–1)* 1–2 * 0+1 *(4–1)+0+1</em></p><p id="fb09" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">或者，<br/><em class="me">H _ out = 4<br/>W _ out = 4</em></p><p id="2092" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是你所看到的空间大小(如上面代码中的注释所示):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/dd495872d40cedf3812f165c112228da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HVHHUmpId4gwYvwY535l5g.png"/></div></div></figure><p id="d386" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，如果你不是一个数学奇才或觉得有点懒做上述计算，你甚至可以通过简单地创建一个虚拟生成器网络，并传递给它任何随机输入来检查层的输出。</p><p id="3f77" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，我们将创建一个只有第一个卷积层的虚拟网络:</p><pre class="ks kt ku kv gt no nl np nq aw nr bi"><span id="f5e1" class="ns mm it nl b gy nt nu l nv nw">class ExampleGenerator(Module):</span><span id="92fd" class="ns mm it nl b gy nx nu l nv nw">      def __init__(self):<br/>          # calling constructor of parent class<br/>          super().__init__()<br/>          self.gen = Sequential(<br/>              ConvTranspose2d(<br/>              in_channels = 100, <br/>              out_channels = 512 , <br/>              kernel_size = 4, <br/>              stride = 1, <br/>              padding = 0, <br/>              bias = False)<br/>              )</span><span id="763b" class="ns mm it nl b gy nx nu l nv nw">      def forward(self, input):<br/>           return self.gen(input)</span><span id="3ca8" class="ns mm it nl b gy nx nu l nv nw"><em class="me"># defining class object</em><br/>ex_gen = ExampleGenerator()</span><span id="3884" class="ns mm it nl b gy nx nu l nv nw"><br/><em class="me"># defining random input for the model: b_size = 2 here<br/>t = torch.randn(2, 100, 1, 1)</em></span><span id="1498" class="ns mm it nl b gy nx nu l nv nw"><em class="me"># checking the shape of the output from model<br/>ex_gen(t).shape</em></span><span id="57ec" class="ns mm it nl b gy nx nu l nv nw">***************** OUTPUT ***********<br/>(2, 512 , 4 , 4)</span></pre><p id="cebb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们遇到的下一层是<code class="fe ni nj nk nl b">BatchNorm2d</code>。现在，如果你已经仔细阅读了教程，你应该很清楚为什么<code class="fe ni nj nk nl b">num_features</code>被设置为512。概括地说，这是因为前一层的输出具有空间大小<code class="fe ni nj nk nl b">(b_size, 512, 4 , 4)</code>。</p><p id="a8b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们以ReLU激活来结束第一个(四个)卷积。</p><p id="4244" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其余的三个回旋或多或少遵循相同的模式。我强烈建议您手动测试这些计算，看看输入的空间大小在通过图层时是如何变化的。这将有助于您在处理不同大小的图像数据集(即32x32x3以外的图像)时，正确设置发生器和鉴别器网络中的<code class="fe ni nj nk nl b">in_channels</code>、<code class="fe ni nj nk nl b">out_channels</code>、<code class="fe ni nj nk nl b">stride</code>、<code class="fe ni nj nk nl b">kernel</code>等的值。</p><p id="aeea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里需要注意的重要一点是，<code class="fe ni nj nk nl b">BatchNorm2d</code>、<code class="fe ni nj nk nl b">ReLU</code>和<code class="fe ni nj nk nl b">Tanh</code>层不会改变输入的空间大小，这就是网络中第二个<code class="fe ni nj nk nl b">ConvTranspose2d</code>层从<code class="fe ni nj nk nl b">in_channels = 512</code>开始的原因。</p><h1 id="6d5e" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">定义鉴别器类</h1><p id="4d43" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">正如在第1部分中所讨论的，鉴别器本质上是一个二进制分类网络，它将图像作为输入，并返回输出是真实的(而不是虚假的)的标量概率。</p><p id="31ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">鉴频器网络中涉及的主要层如下:</p><ul class=""><li id="37de" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated">Conv2d:与有助于放大图像的<code class="fe ni nj nk nl b">ConvTranspose2d</code>层相反，<code class="fe ni nj nk nl b">Conv2d</code>层有助于缩小图像，例如，将图像尺寸从32x32缩小到16x16再缩小到8x8等等..一直继续下去，直到剩下1×1，即一个标量值。</li><li id="941a" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">LeakyReLU:使用<code class="fe ni nj nk nl b">LeakyReLU</code>超过<code class="fe ni nj nk nl b">ReLU</code>层的一个主要优点是它解决了<a class="ae lh" href="https://www.youtube.com/watch?v=qO_NLVjD6zE" rel="noopener ugc nofollow" target="_blank">消失渐变问题</a>。更简单地说，当输入为负时，<code class="fe ni nj nk nl b">ReLU</code>层将输出0，而<code class="fe ni nj nk nl b">LeakyReLU</code>将输出非零值。因此，当输入为负时，<code class="fe ni nj nk nl b">LeakyReLU</code>将有助于小梯度更新(而不是零梯度),因此模型可以继续学习和更新。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/cabb087812fe3b87dc03fc66c60d4432.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*6_elt7S-aa2hNHHP.jpg"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">泄漏Relu vs Relu [ <a class="ae lh" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.researchgate.net%2Ffigure%2FIllustration-of-output-of-ELU-vs-ReLU-vs-Leaky-ReLU-function-with-varying-input-values_fig8_334389306&amp;psig=AOvVaw23dYxYPclBhjtVuFBFTu_5&amp;ust=1610101312589000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCOjBhvjMie4CFQAAAAAdAAAAABAU" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><ul class=""><li id="735a" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated">Sigmoid:这是另一个激活层，我们通过它传递输入，在[0，1]范围内转换数据。</li></ul><p id="ab02" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，这是discriminator类的样子:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/c04251934e9ff800d84068e1ce9740a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qpu5xNcy2jcRRT-SrLol4w.png"/></div></div></figure><p id="b5a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它可能看起来非常类似于发电机网络，在某种意义上确实如此。也就是说，我们再次使用包含四个步长卷积的顺序网络。但是，我们现在处理的是<code class="fe ni nj nk nl b">Conv2d</code>层，而不是<code class="fe ni nj nk nl b">ConvTranspose2d</code>层。在每一个图像中，我们设置<code class="fe ni nj nk nl b">out_channels</code>最初取一个小值，然后逐渐增加2倍，直到达到我们想要的1×1图像(即<em class="me">H _ out</em>=<em class="me">W _ out</em>= 1)，此时我们设置<code class="fe ni nj nk nl b">out_channels = 1</code>。</p><p id="aae1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里需要注意的一点是，进入最后一个卷积层的输入的形状是<code class="fe ni nj nk nl b">(b_size, 256, 2, 2)</code>。因为<code class="fe ni nj nk nl b">kernel_size</code>必须始终小于输入的空间大小(在本例中为2x2)，所以我们必须为最后一层设置<code class="fe ni nj nk nl b">kernel_size = 2</code>(与前几层的<code class="fe ni nj nk nl b">kernle_size = 4</code>相反)。否则将导致运行时错误！</p><p id="549d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可能想知道，作为鉴别器输出的标量值发生了什么，而我们这里有一个形状为<code class="fe ni nj nk nl b">b_size, 1, 1, 1</code>的张量(即最后一层的输出)。好消息是，我们可以使用<code class="fe ni nj nk nl b">view(-1)</code>很容易地将其转换成包含<code class="fe ni nj nk nl b">b_size</code>值的单个向量。例如，<code class="fe ni nj nk nl b">t.view(-1)</code>将形状<code class="fe ni nj nk nl b">(2,1,1,1)</code>的四维张量<code class="fe ni nj nk nl b">t</code>重新整形为只有两个值的一维张量。我们将在后面的章节中看到它的实际应用！</p><p id="03dd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们已经为两个网络定义了类，我们可以为它们初始化一个对象。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e67de0e9cc05ff06628d875539b97aa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*HoL6cgRJsePsbxTXlWFmaw.png"/></div></figure><h1 id="d2cf" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">初始化权重和偏差</h1><p id="d0e5" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">用随机权重初始化神经网络而不是让它们都为0是很重要的。这是因为具有相同初始权重的所有神经元将在训练期间学习相同的特征，即在后续迭代期间权重将是相同的。简而言之，模型没有任何改进！</p><p id="b3b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据我看到的几篇博文，<code class="fe ni nj nk nl b">ConvTranspose2d</code>层的权重将从均值=0，标准差=0.02的正态分布中随机初始化。对于<code class="fe ni nj nk nl b">BatchNorm2d</code>层，分布的平均值和标准偏差将分别为1和0.02。这适用于发生器和鉴别器网络。</p><p id="8b81" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要同时初始化网络中的所有不同层，我们需要:</p><p id="d3cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">(a)定义将模型作为输入的函数</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/62c040df5b4401f21bf8567df329b61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UhdrYUPrr05CaInLoho_eQ.png"/></div></div></figure><p id="d54a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">(b)然后使用<code class="fe ni nj nk nl b">.apply()</code>递归初始化所有层</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c49dfcc04daf44e4aef65ec0b06b2ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Qw-rV-CvR3f7MfOxliidbA.png"/></div></figure><h1 id="64eb" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">为生成器和鉴别器设置优化器</h1><p id="b42e" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">优化器对于使用<code class="fe ni nj nk nl b">optimizer.step</code>方法在网络中执行参数更新很有用。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/261bd381428808cfe87cfa3639c59330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C6PWfHuOxrNr8uJ6Mt2gEg.png"/></div></div></figure><h1 id="8f2d" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">设置损失函数</h1><p id="3f4c" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">为了检查一幅图像的预测标签离真实标签有多远，我们将使用<code class="fe ni nj nk nl b">BCELoss</code>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/93956697168125c4fe4e282c91da69e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LwHcV70AehhiWuW5WD4Pbw.png"/></div></div></figure><h1 id="96f6" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">训练甘斯</h1><h2 id="2de1" class="ns mm it bd mn pd pe dn mr pf pg dp mv lr ph pi mx lv pj pk mz lz pl pm nb iz bi translated">伪代码</h2><p id="d6d0" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在第1部分中，我们讨论了训练GAN的主要步骤。为了刷新我们的记忆，下面是<strong class="lk jd">伪代码(</strong>使用PyTorch提供的<a class="ae lh" href="https://github.com/pytorch/examples/tree/master/dcgan" rel="noopener ugc nofollow" target="_blank">开源</a>代码生成):</p><pre class="ks kt ku kv gt no nl np nq aw nr bi"><span id="efe4" class="ns mm it nl b gy nt nu l nv nw">for each epoch:   <br/>     for each batch b of input images:<br/>       <br/>       <em class="me">######################################</em><br/>       <em class="me">## Part 1: Update Discriminator - D ##<br/>       ######################################       <br/>       # loss on real images</em><br/>       clear gradients of D<br/>       pred_labels_real = pass b through D to compute outputs<br/>       true_labels_real = [1,1,1....1]<br/>       calculate loss(pred_labels_real, true_labels_real) <br/>       calculate gradients using this loss<br/>       <br/>       <em class="me"># loss on fake images</em><br/>       generate batch of size b of fake images (b_fake) using G<br/>       pred_labels_fake = pass b_fake through D<br/>       true_labels_fake = [0,0,....0] <br/>       calculate loss(pred_labels_fake, true_labels_fake)<br/>       calculate gradients using this loss       <br/>       update weights of D<br/><em class="me">       <br/>       ######################################</em><br/>       <em class="me">#### Part 2: Update Generator - G ####<br/>       ######################################</em>       <br/>       clear gradients of G<br/>       pred_labels = pass b_fake through D<br/>       true_labels = [1,1,....1]<br/>       calculate loss(pred_labels, true_labels)<br/>       calculate gradient using this loss<br/>       update weights of G</span><span id="68a4" class="ns mm it nl b gy nx nu l nv nw"><em class="me">       ################################################</em><br/>       <em class="me">### Part 3: Plot a batch of Generator images ###<br/>       ################################################</em></span></pre><p id="eac3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在记住这一点，让我们开始一步一步地构建我们的训练函数。代码将分为三个部分——第1部分专门用于更新鉴别器，第2部分用于更新生成器，以及(可选的)第3部分用于使用我们在文章开头定义的助手函数绘制一批生成器图像。</p><h2 id="bcc2" class="ns mm it bd mn pd pe dn mr pf pg dp mv lr ph pi mx lv pj pk mz lz pl pm nb iz bi translated"><strong class="ak">第1部分:</strong> U <strong class="ak">更新鉴别器</strong></h2><p id="3708" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">该过程包括计算真实和虚假图像的损失。</p><p id="b701" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">计算真实图像损失的代码</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/e9c99752fc8f133d53884d7ac71a0994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6mx6AIgjh0rvlpz2neCf9g.png"/></div></div></figure><p id="08e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们首先使用<code class="fe ni nj nk nl b">zero_grad()</code>清除鉴别器的梯度。有必要在每次循环开始时将梯度设置为0，因为梯度是在随后的反向路径中累积的(即当<code class="fe ni nj nk nl b">loss.backward()</code>被调用时)。接下来，当输入一批真实图像(即来自我们的训练集的图像)时，我们存储来自鉴别器模型的输出。记住，<code class="fe ni nj nk nl b">b</code>的形状是(32，3，32，32)。</p><p id="4d48" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">值得注意的是，我们不是简单地将图像作为<code class="fe ni nj nk nl b">netD(b)</code>传递到网络，而是在批处理中首先使用<code class="fe ni nj nk nl b">b.to(device)</code>。这是因为我们必须把图像张量和模型放在同一个设备上。虽然在CPU上运行代码可能没什么关系，但不这样做可能会在GPU上引发运行时错误。</p><p id="13bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，如前所述，我们对模型的输出使用<code class="fe ni nj nk nl b">view(-1)</code>来将4-d张量整形为1-d张量，1-d张量包含图像是真实的可能性。</p><p id="2335" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将实像的真实标签或<code class="fe ni nj nk nl b">targets</code>定义为大小为<code class="fe ni nj nk nl b">b</code>的张量，包含所有的1。我们显式地将它设置为<code class="fe ni nj nk nl b">float32</code>，以便它与批处理<code class="fe ni nj nk nl b">b</code>中的图像类型相匹配。最后，我们确保目标标签也在与模型相同的设备上。</p><p id="05c6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，使用预测值计算<code class="fe ni nj nk nl b">BCELoss</code>，并且使用<code class="fe ni nj nk nl b">backward()</code>计算和累积目标标签和梯度。</p><p id="d73c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">计算伪图像损失的代码</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/76b8424136c8a7f576f415b3c9320594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UI7qAPcoXxQ5Ih-41VFhLg.png"/></div></div></figure><p id="4e4b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了生成一批假图像，我们首先需要一批随机噪声矢量<code class="fe ni nj nk nl b">noise</code>，将其馈送给生成器以创建<code class="fe ni nj nk nl b">fake_img</code>。接下来，我们计算鉴别器对这些假图像的输出，并将其存储在<code class="fe ni nj nk nl b">yhat</code>中。如果我们的输入(即<code class="fe ni nj nk nl b">fake_img</code>在GPU上，而模型不在GPU上，那么<code class="fe ni nj nk nl b">cuda()</code>是必不可少的，在这种情况下会抛出运行时错误。</p><p id="49b2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">需要注意的重要一点是，我们在那批假图像上使用了<code class="fe ni nj nk nl b">detach()</code>。这样做的原因是，虽然我们希望<em class="me">使用生成器的</em>服务，<strong class="lk jd">但是</strong>我们还不想<em class="me">更新</em>(一旦我们完成了对鉴别器的更新，我们就会这样做)。</p><blockquote class="mf mg mh"><p id="5503" class="li lj me lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">为什么要用<code class="fe ni nj nk nl b">detach()</code>？基本上，我们必须在我们的发电机优化器<strong class="lk jd">上跟踪步骤，只有在训练发电机时</strong>，<strong class="lk jd">不是</strong>鉴别器。</p></blockquote><p id="4762" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据第1部分的解释，这种情况下的目标标签将是一个长度为<code class="fe ni nj nk nl b">b</code>的张量，包含所有的零。其余步骤与前面的代码片段相同。</p><h2 id="af1c" class="ns mm it bd mn pd pe dn mr pf pg dp mv lr ph pi mx lv pj pk mz lz pl pm nb iz bi translated"><strong class="ak">第2部分:更新发电机</strong></h2><p id="4c81" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">步骤与鉴别器的情况大致相同。主要的区别是，现在目标标签被设置为1(而不是0)，即使它们是假图像。第1部分详细解释了我们为什么要这样做。简而言之:</p><blockquote class="mf mg mh"><p id="1fd0" class="li lj me lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">生成器希望鉴别器<strong class="lk jd">认为</strong> <em class="it"> </em>正在生成真实图像，因此它在训练期间使用真实标签1。</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pp"><img src="../Images/f8d030f34c0bbf12ed6f27147ce7a681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8TS2rH72cvPcfkhBlOncxA.png"/></div></div></figure><h2 id="cecc" class="ns mm it bd mn pd pe dn mr pf pg dp mv lr ph pi mx lv pj pk mz lz pl pm nb iz bi translated">第3部分:从生成器中绘制一批图像</h2><p id="b2d6" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">为了查看我们的生成器在每个时期的表现，我们将使用助手函数<code class="fe ni nj nk nl b">plot_images()</code>每10次迭代绘制一组图像。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pq"><img src="../Images/939a039734c70afefa21b585f4a397d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FTHRNnALcvV1AuEqk_BenA.png"/></div></div></figure><p id="bd8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在你可能会注意到在传递给绘图函数<code class="fe ni nj nk nl b">plot_images()</code>之前，<code class="fe ni nj nk nl b">fake_img</code>中的尺寸使用<code class="fe ni nj nk nl b">np.transpose()</code>重新排序。这是因为，<code class="fe ni nj nk nl b">plt.imshow()</code>方法(在<code class="fe ni nj nk nl b">plot_images()</code>中使用)要求传递给它的图像是<code class="fe ni nj nk nl b">(height, width, channels)</code>形式，然而生成器输出的图像形状是PyTorch中的标准<code class="fe ni nj nk nl b">(channels, height, width)</code>形式。为了解决这个问题，我们必须改变假图像的尺寸，这样我们就有了类似于<code class="fe ni nj nk nl b">b_size, 32, 32, 3)</code>的图像。</p><p id="12a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一件要记住的事情是，调用<code class="fe ni nj nk nl b">.detach().cpu()</code>对于在我们开始将<code class="fe ni nj nk nl b">fake_img</code>张量传递给绘图函数之前，首先将它复制到主机内存是很重要的。</p><p id="4c2e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是训练GAN的最后一个代码块——包括第1部分、第2部分和第3部分——的样子:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pr ps l"/></div></figure><p id="3fcd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们已经使用自定义图像数据集从头开始实现了一个普通的GAN！喔喔喔…</p><p id="5647" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了让您对我们的GAN生成的图像质量有一个大致的估计:</p><p id="8b5e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">纪元0(第160次迭代):很高兴看到生成器发现了人脸存在于图像中心的事实。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/dbecce323c74ac1398a8fac3aa2912ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oLw2aHnaakQyCvjmLQiMxg.png"/></div></div></figure><p id="2654" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第5个纪元(第10次迭代):与上一批相比有一些明显的改进。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/c3e9b186aaf46b87792b1083a6b3ebd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*79m5HqzxZRVOjiO0jQdplg.png"/></div></div></figure><h1 id="d3fd" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated"><strong class="ak">常见运行时错误</strong></h1><p id="1cc5" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">虽然我在Github笔记本中与您分享的代码没有错误，但我想花点时间讨论一下我在从头开始学习训练GANs时遇到的一些运行时错误。</p><ul class=""><li id="a59e" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated"><em class="me">输入类型(torch.cuda.DoubleTensor)和权重类型(torch.cuda.FloatTensor)应该相同</em></li></ul><p id="ad4b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，权重类型通常是指您的模型中的权重，如果您还记得的话，我们将其明确设置为类型<code class="fe ni nj nk nl b">float32</code>。您可能看到此错误的原因是，您正在向您的模型输入可能是<code class="fe ni nj nk nl b">float64</code>而不是<code class="fe ni nj nk nl b">float32</code>的东西，即类型不匹配问题。</p><p id="52de" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我的例子中，当我试图通过dataloader将一批图像传递给鉴别器模型<em class="me">而没有首先使用<code class="fe ni nj nk nl b">np.float32</code>显式地将它们转换为float时，我遇到了这个错误。</em></p><ul class=""><li id="7888" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated"><em class="me">输入类型(火炬。FloatTensor)和重量类型(torch.cuda.FloatTensor)应该相同</em></li></ul><p id="a040" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果仔细观察输入类型(即<em class="me"> torch ),错误本身是不言自明的。FloatTensor) </em>和重量类型(即<em class="me">torch . cuda . float tensor)</em>——其中只有一个包含单词“<em class="me"> cuda </em>”。这意味着你的模型在GPU上，而输入数据仍然在CPU上。要纠正这个错误，只需使用<code class="fe ni nj nk nl b">.to(device)</code>在GPU上发送您的输入数据张量。</p><p id="662d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我在GAN培训的第1部分遇到了这个错误，当时我正在使用<code class="fe ni nj nk nl b">yhat = netD(b).view(-1)</code>(用于计算真实图像上的鉴别器损耗)计算一批输入图像的模型输出。解决方法很简单:<code class="fe ni nj nk nl b">yhat = netD(b.to(device)).view(-1)</code>。</p></div><div class="ab cl pv pw hx px" role="separator"><span class="py bw bk pz qa qb"/><span class="py bw bk pz qa qb"/><span class="py bw bk pz qa"/></div><div class="im in io ip iq"><p id="28f5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">恭喜你走到这一步。希望本教程(以及第一部分)是一个非常有用但非常复杂的深度学习概念的热情介绍。既然您已经介绍了编码GAN，我强烈建议您尝试更多的GAN风格，如PGGAN、StyleGAN、StyleGAN2等。鉴于相关的研究论文有时会因其封装的大量算法智慧而变得有点令人不知所措，我写了一篇文章，解密了一些反复出现的<a class="ae lh" href="https://towardsdatascience.com/keywords-to-know-before-you-start-reading-papers-on-gans-8a08a665b40c" rel="noopener" target="_blank">关键词/概念，这些是你在开始阅读关于GANs </a>的论文之前应该知道的。快乐阅读！</p><p id="88fa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">直到下次:)</p><div class="qc qd gp gr qe qf"><a href="https://towardsdatascience.com/keywords-to-know-before-you-start-reading-papers-on-gans-8a08a665b40c" rel="noopener follow" target="_blank"><div class="qg ab fo"><div class="qh ab qi cl cj qj"><h2 class="bd jd gy z fp qk fr fs ql fu fw jc bi translated">在你开始阅读关于GANs的论文之前要知道的关键词</h2><div class="qm l"><h3 class="bd b gy z fp qk fr fs ql fu fw dk translated">用简单的英语解释重要的重复出现的关键词/概念。</h3></div><div class="qn l"><p class="bd b dl z fp qk fr fs ql fu fw dk translated">towardsdatascience.com</p></div></div><div class="qo l"><div class="qp l qq qr qs qo qt lb qf"/></div></div></a></div><div class="qc qd gp gr qe qf"><a href="https://medium.com/analytics-vidhya/how-to-use-autokeras-to-build-image-classification-models-using-one-line-of-code-c35b0c36e66e" rel="noopener follow" target="_blank"><div class="qg ab fo"><div class="qh ab qi cl cj qj"><h2 class="bd jd gy z fp qk fr fs ql fu fw jc bi translated">如何使用AutoKeras用一行代码建立图像分类模型？</h2><div class="qm l"><h3 class="bd b gy z fp qk fr fs ql fu fw dk translated">甚至不需要知道Conv2d、Maxpool或批处理规范化层是做什么的！</h3></div><div class="qn l"><p class="bd b dl z fp qk fr fs ql fu fw dk translated">medium.com</p></div></div><div class="qo l"><div class="qu l qq qr qs qo qt lb qf"/></div></div></a></div><div class="qc qd gp gr qe qf"><a href="https://towardsdatascience.com/interviewers-favorite-question-how-would-you-scale-your-ml-model-56e4fa40071b" rel="noopener follow" target="_blank"><div class="qg ab fo"><div class="qh ab qi cl cj qj"><h2 class="bd jd gy z fp qk fr fs ql fu fw jc bi translated">面试官最喜欢的问题-你会如何“扩展你的ML模型？”</h2><div class="qm l"><h3 class="bd b gy z fp qk fr fs ql fu fw dk translated">您正在构建一个生产就绪的ML模型吗？</h3></div><div class="qn l"><p class="bd b dl z fp qk fr fs ql fu fw dk translated">towardsdatascience.com</p></div></div><div class="qo l"><div class="qv l qq qr qs qo qt lb qf"/></div></div></a></div></div></div>    
</body>
</html>