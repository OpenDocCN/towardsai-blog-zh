<html>
<head>
<title>Principal Component Analysis in Dimensionality Reduction with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Python的主成分分析降维方法</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/principal-component-analysis-in-dimensionality-reduction-with-python-1a613006d531?source=collection_archive---------0-----------------------#2021-03-05">https://pub.towardsai.net/principal-component-analysis-in-dimensionality-reduction-with-python-1a613006d531?source=collection_archive---------0-----------------------#2021-03-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7c5d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a></h2><div class=""/><div class=""><h2 id="b233" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">将高维特征简化为低维特征</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/5fe631188901f68095dc834eed48d91c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*jhHJ7AFwQEKpas_PldeT2A.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">主成分分析投影。作者的照片</figcaption></figure><p id="adcd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在本文中，我们将讨论处理大量特征中出现的过拟合问题的特征约简方法。当高维数据适合模型时，它有时会混淆相似信息的特征。找出对目标变量影响更大的主要特征/组成部分，以及那些具有最大方差的组成部分。二维特征转换成一维特征，使得计算将更快。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/ebc70ba05191a59cad8c60884bf840a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*lPCFZTRRANcRgLjmJJkFxA.png"/></div></figure><p id="4b54" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在机器学习中，维度是数据集中特征的数量。随着数据中添加的维度越多，就会以指数方式产生更多的维度空间，这将导致更多的处理成本<strong class="lf jd"> <em class="ma">【维度诅咒】</em> </strong>。</p><blockquote class="mb mc md"><p id="fd3e" class="ld le ma lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><strong class="lf jd"> <em class="it">为什么要降低维度？</em>T9】</strong></p></blockquote><ul class=""><li id="f8d7" class="mh mi it lf b lg lh lj lk lm mj lq mk lu ml ly mm mn mo mp bi translated">我们知道训练大规模数据需要更多的计算能力和时间。</li><li id="2ac0" class="mh mi it lf b lg mq lj mr lm ms lq mt lu mu ly mm mn mo mp bi translated">对于大维度数据，可视化是不可能的。</li><li id="8b10" class="mh mi it lf b lg mq lj mr lm ms lq mt lu mu ly mm mn mo mp bi translated">更多的维度意味着更多的存储空间问题。</li></ul><div class="mv mw gp gr mx my"><a rel="noopener  ugc nofollow" target="_blank" href="/data-preprocessing-concepts-with-python-b93c63f14bb6"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jd gy z fp nd fr fs ne fu fw jc bi translated">Python中的数据预处理概念</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">一种为机器学习估值器准备数据的稳健方法</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">pub.towardsai.net</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm kx my"/></div></div></a></div><div class="mv mw gp gr mx my"><a rel="noopener  ugc nofollow" target="_blank" href="/correlation-and-its-types-in-statistics-7a723dcfd12d"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jd gy z fp nd fr fs ne fu fw jc bi translated">统计学中的相关性及其类型</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">统计学有助于理解机器学习中的行为</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">pub.towardsai.net</p></div></div><div class="nh l"><div class="nn l nj nk nl nh nm kx my"/></div></div></a></div><blockquote class="mb mc md"><p id="380d" class="ld le ma lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><strong class="lf jd"> <em class="it">我们可以使用两种技术来降低维度，如下所示:</em> </strong></p></blockquote><ol class=""><li id="d7a7" class="mh mi it lf b lg lh lj lk lm mj lq mk lu ml ly no mn mo mp bi translated">特征选择:这些是向后消除、向前选择和双向消除。</li><li id="ad5c" class="mh mi it lf b lg mq lj mr lm ms lq mt lu mu ly no mn mo mp bi translated">特征提取:这些是主成分分析(PCA)，线性判别分析(LDA)，核PCA和其他。</li></ol><p id="6363" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">主成分分析将n个特征减少到n≤p个分量特征，这些分量特征最能解释数据集的方差。</p><p id="c2d0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">主成分是数据集中原始预测值的线性组合(正交变换)。</p><p id="10b4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果有许多主成分，那么第一个PC1具有最大方差，然后是PC2、PC3…方差减少。PC1和PC2的相关性为零。</p><p id="9137" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">PCA的处理限制是，它一次获取存储器中的所有数据。因此，需要大内存，为了避免这种一次性内存使用，增量PCA开始发挥作用，该过程使用微型批处理，其结果几乎与PCA的结果相匹配。</p><p id="7779" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">python的例子:</p><p id="eb9d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">导入必要的库</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="819a" class="nu nv it nq b gy nw nx l ny nz">import numpy as np<br/>import pandas as pd</span></pre><p id="1fdb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从url读取数据集</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="52ac" class="nu nv it nq b gy nw nx l ny nz">url = "<a class="ae oa" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases<br/>/iris/iris.data</a>"</span><span id="30c4" class="nu nv it nq b gy ob nx l ny nz">names=['sepal-length','sepal-width','petal-length','petal-<br/>         width','Class']</span><span id="2103" class="nu nv it nq b gy ob nx l ny nz">df = pd.read_csv(url,names=names)</span></pre><p id="544f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">使用head方法查看前5行。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="35f3" class="nu nv it nq b gy nw nx l ny nz">df.head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/45d090cc7ce86aee6f19b6cc3b5f0f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*cnruaNnumIZnEQHKbroFtQ.png"/></div></figure><p id="f6ae" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">将数据分为独立要素和从属要素。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="4e73" class="nu nv it nq b gy nw nx l ny nz">X=df.drop('Class',1)<br/>y = df['Class']</span></pre><p id="b0c4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，在训练集和测试集中划分特征。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="f679" class="nu nv it nq b gy nw nx l ny nz">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)</span></pre><p id="e9b7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">使用标准标量来标准化数据集值。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="f74a" class="nu nv it nq b gy nw nx l ny nz">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><p id="0c3c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，这是我们这篇文章的播放器，使用来自分解类的PCA。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="6fee" class="nu nv it nq b gy nw nx l ny nz">#Apply PCA<br/>from sklearn.decomposition import PCA<br/>pca = PCA()<br/>X_train = pca.fit_transform(X_train)<br/>X_test = pca.transform(X_test)</span></pre><p id="5f51" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在此之后，我们必须知道所有主成分的方差</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="6911" class="nu nv it nq b gy nw nx l ny nz">explained_variance = pca.explained_variance_ratio_<br/>print(explained_variance)</span><span id="7bbf" class="nu nv it nq b gy ob nx l ny nz">#output:<br/>array([0.71580568, 0.24213308, 0.03690989, 0.00515135])</span></pre><p id="e288" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以使用n_components参数来使用主分量的个数。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="f319" class="nu nv it nq b gy nw nx l ny nz">#Try out with only 1 PCA<br/>#Apply PCA<br/>from sklearn.decomposition import PCA<br/>pca = PCA(n_components=1)<br/>X_train = pca.fit_transform(X_train)<br/>X_test = pca.transform(X_test)</span></pre><div class="mv mw gp gr mx my"><a rel="noopener  ugc nofollow" target="_blank" href="/fully-explained-ensemble-techniques-example-with-python-b83e50310841"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd jd gy z fp nd fr fs ne fu fw jc bi translated">用Python完整解释了整体技术示例</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">基于几种决策树的机器学习方法</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">pub.towardsai.net</p></div></div><div class="nh l"><div class="od l nj nk nl nh nm kx my"/></div></div></a></div><p id="cfb4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在我们可以在算法中使用这些x_train和x_test值。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="e4c5" class="nu nv it nq b gy nw nx l ny nz">from sklearn.ensemble import RandomForestClassifier<br/>classifier = RandomForestClassifier()<br/>classifier.fit(X_train,y_train)</span><span id="4e97" class="nu nv it nq b gy ob nx l ny nz">#output:</span><span id="7ad0" class="nu nv it nq b gy ob nx l ny nz">RandomForestClassifier(bootstrap=True, class_weight=None,<br/>            criterion='gini', max_depth=None, max_features='auto',<br/>            max_leaf_nodes=None, min_impurity_decrease=0.0,<br/>            min_impurity_split=None, min_samples_leaf=1,<br/>            min_samples_split=2, min_weight_fraction_leaf=0.0,<br/>            n_estimators=10, n_jobs=None,oob_score=False,<br/>            random_state=None, verbose=0, warm_start=False)</span></pre><p id="08b9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">现在，我们将预测我们的模型，并检查其准确性。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="68f3" class="nu nv it nq b gy nw nx l ny nz">y_pred = classifier.predict(X_test)<br/>from sklearn.metrics import accuracy_score<br/>print('Accuracy of this model is' , accuracy_score(y_test,y_pred))</span><span id="0b11" class="nu nv it nq b gy ob nx l ny nz">#output:<br/>Accuracy of this model is 0.9333333333333333</span></pre><blockquote class="mb mc md"><p id="8a7c" class="ld le ma lf b lg lh kd li lj lk kg ll me ln lo lp mf lr ls lt mg lv lw lx ly im bi translated"><strong class="lf jd"> <em class="it">结论:</em> </strong></p></blockquote><p id="34b2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">主成分分析在处理高维数据时非常有效。PCA的丰富之处在于它减少了训练时间。</p><p id="d2f5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae oa" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae oa" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="825c" class="oe nv it bd of og oh oi oj ok ol om on ki oo kj op kl oq km or ko os kp ot ou bi translated">推荐文章</h1><ol class=""><li id="f90e" class="mh mi it lf b lg ov lj ow lm ox lq oy lu oz ly no mn mo mp bi translated"><a class="ae oa" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> NLP —用Python从零到英雄</a></li></ol><p id="665d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">2.<a class="ae oa" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a></p><p id="dbe5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">3.<a class="ae oa" href="https://medium.com/towards-artificial-intelligence/python-zero-to-hero-with-examples-c7a5dedb968b?source=friends_link&amp;sk=186aff630c2241aca16522241333e3e0" rel="noopener"> Python:零到英雄带实例</a></p><p id="b9ea" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">4.<a class="ae oa" href="https://medium.com/towards-artificial-intelligence/fully-explained-svm-classification-with-python-eda124997bcd?source=friends_link&amp;sk=da300d557992d67808746ee706269b2f" rel="noopener">用Python全面讲解SVM分类</a></p><p id="7684" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">5.<a class="ae oa" href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a?source=friends_link&amp;sk=9c5c613ceb10f2d203712634f3b6fb28" rel="noopener">用Python全面解释K-means聚类</a></p><p id="3f17" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">6.<a class="ae oa" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python全面解释线性回归</a></p><p id="5df8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">7.<a class="ae oa" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python全面讲解逻辑回归</a></p><p id="f0dd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">8.<a class="ae oa" href="https://medium.com/towards-artificial-intelligence/basic-of-time-series-with-python-a2f7cb451a76?source=friends_link&amp;sk=09d77be2d6b8779973e41ab54ebcf6c5" rel="noopener">Python时间序列基础</a></p><p id="b8f7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">9.<a class="ae oa" href="https://medium.com/towards-artificial-intelligence/numpy-zero-to-hero-with-python-d135f57d6082?source=friends_link&amp;sk=45c0921423cdcca2f5772f5a5c1568f1" rel="noopener"> NumPy:用Python零到英雄</a></p><p id="7120" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">10.<a class="ae oa" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>