<html>
<head>
<title>Medical Image Segmentation: 2018 Data Science Bowl</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">医学图像分割:2018数据科学碗</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/medical-image-segmentation-2018-data-science-bowl-3093bef449a5?source=collection_archive---------0-----------------------#2021-10-25">https://pub.towardsai.net/medical-image-segmentation-2018-data-science-bowl-3093bef449a5?source=collection_archive---------0-----------------------#2021-10-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="38c6" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="d7f5" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用深度细胞神经网络(UNet，UNet++，HRNet)进行跨成像实验的细胞核分割的案例研究</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/172bb58d174587a38bbd031b485a6e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wkuZvId4aq-u39GO"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@nci?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">国立癌症研究所</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="842e" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">目录</h1><ol class=""><li id="8ef9" class="lx ly iq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated"><strong class="lz ja">摘要</strong></li><li id="9fc7" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">简介<br/> </strong> -使用的问题陈述和数据集<br/> -商业问题的ML公式化<br/> -现实世界的约束<br/> -使用的指标</li><li id="2868" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">文献综述<br/> </strong> -医学图像分割的挑战<br/> -滑动窗口法<br/> - UNet</li><li id="2317" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">探索性数据分析</strong></li><li id="7a39" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">使用的深度学习架构<br/></strong>-UNet<br/>-unet++<br/>-HRNet</li><li id="9c50" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">结果和讨论</strong></li><li id="a2d5" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">培训后量化</strong></li><li id="63cb" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">网络应用</strong></li><li id="8f68" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">总结与未来工作</strong></li><li id="56e9" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">致谢</strong></li><li id="fba0" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">参考文献</strong></li></ol><h1 id="fedd" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">摘要</h1><p id="c8bb" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">为了找到治愈任何疾病的方法，研究人员分析样本细胞对各种治疗的反应，并了解潜在的生物过程。识别细胞核是大多数分析的起点，因为它有助于识别样本中的每个单个细胞。自动化这一过程可以节省大量时间，允许更有效的药物测试和更快地解开治愈。在这个案例研究中，我建议使用深度细胞神经网络在各种条件下自动检测图像中的细胞核。我总共设计了三个由U-Net、U-Net++和HRNet衍生而来的网络，并使用平均IoU作为度量来比较它们的性能。可以看出，基于U-Net的模型表现最佳，IoU平均得分为0.861。</p><h1 id="eeac" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">介绍</h1><p id="f8fd" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Image_segmentation" rel="noopener ugc nofollow" target="_blank">图像分割</a>分割图像的特定区域，以便更好地理解和分析。例如，在医学领域，在脑部扫描中，医生可能希望突出显示特定区域(人眼不容易观察到)，以便更好地诊断。另一个应用是在自动驾驶汽车中，当您需要分割图像中不同类型的对象时。下面是视网膜图像中血管分割的一个例子。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nh"><img src="../Images/336a477e92524cdd950668f8797b35e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PwGKskl-0JThpGULZsGigA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">视网膜图像及其分割掩模(<a class="ae le" href="https://drive.grand-challenge.org/" rel="noopener ugc nofollow" target="_blank">https://drive.grand-challenge.org/</a></figcaption></figure><p id="3466" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">一般来说，有两种类型的分割:语义和实例。在语义分割中，不同类别的对象被分开分割(例如，将人从背景中分离)。然而，在实例分割中，同一类别的不同实例被分割(例如，在同一图片中将不同的人彼此分开)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/48a5b14e238fec18bf1e640ada490284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vsXei_-NBulKfQhfkxTdKw.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://learnopencv.com/human-pose-estimation-using-keypoint-rcnn-in-pytorch/" rel="noopener ugc nofollow" target="_blank">https://learnopencv . com/human-pose-estimation-using-key point-rcnn-in-py torch/</a></figcaption></figure><h2 id="3360" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">使用真实世界的问题陈述和数据集</h2><p id="ec46" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">在这个案例研究中，我试图解决一个基于语义的医学分割问题。使用的数据集来自Kaggle竞赛<a class="ae le" href="https://www.kaggle.com/c/data-science-bowl-2018/" rel="noopener ugc nofollow" target="_blank"> 2018数据科学碗</a>。这已被用作U-Net++和DoubleUNet的基准数据集。数据集由不同背景条件下的细胞核图像组成。任务是从背景中分割细胞核。下面显示了来自数据集的图像及其分割细胞核掩模的几个例子。</p><div class="kp kq kr ks gt ab cb"><figure class="nz kt oa ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/4b65645a4c5000ee59e4c8169ccb7c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*muKYQCsIrivclDXSQWgVGQ.png"/></div></figure><figure class="nz kt of ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/cac87e72b015bc80773bb84550104fc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*NTbecI2z9dJ7ZGClpZfq-Q.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk og di oh oi translated"><a class="ae le" href="https://www.kaggle.com/c/data-science-bowl-2018/" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/data-science-bowl-2018/</a></figcaption></figure></div><p id="6139" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><strong class="lz ja">现实世界的重要性:</strong>我将尝试解决的任务是自动化细胞核检测。这有助于加快研究速度，治愈几乎所有疾病，从肺癌、心脏病到罕见疾病。这是因为识别细胞核是大多数分析的起点，因为人体的30万亿个细胞中的大多数都含有一个充满DNA的细胞核，DNA是为每个细胞编程的遗传密码。识别细胞核使研究人员能够识别样本中的每个单个细胞，通过测量细胞对各种处理的反应，研究人员可以了解潜在的生物过程在起作用[2]。</p><h2 id="c740" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">问题的ML公式</h2><p id="6611" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">一般来说，图像分割也可以作为一个多类分类问题，其中图像中的每个像素都必须被分配一个类。注意，这里的输入和输出都是图像。这里，我的输入数据点将是细胞的图像，输出数据点将是其分割的细胞核掩模。输出图像由该领域的专家标记。对于我们的问题，输出图像是二值图像，背景为黑色，细胞核为白色。</p><p id="39a1" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">因此，现在给定输入图像(比如形状HxWx3)，任务是产生分割的细胞核的输出二进制图像(形状HxWx1)，即将二进制数0或1分配给输出二进制图像的每个像素。</p><h2 id="b385" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">现实世界的约束</h2><p id="0ab1" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">在现实世界中，生物医学图像的分割被认为对于诊断目的非常重要。在许多情况下，一个小小的错误就可能导致错误的诊断，这可能会给病人的健康带来巨大的风险。然而，在我们的情况下，由于分割的细胞核将更多地用于一般治疗，因此小的误差应该不是大问题。</p><p id="c435" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">就等待时间而言，细胞核的分割图像不需要在毫秒内产生。在大多数情况下，几秒钟，甚至几分钟都不会造成伤害。</p><h2 id="dcbc" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">使用的度量</h2><p id="7082" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">所用的度量是平均IoU，即并集上的平均交集。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/6abd306867ccc4b7985db1948b299d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*01cbzFH4IVWhmiPp3mXp2Q.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Jaccard_index</a></figcaption></figure><p id="5972" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">这是一个非常常用的指标以及骰子系数。两个对象的平均IOU可以定义为它们之间的交集面积除以并集面积。在图像分割中，这两个对象是待分割部分的真实区域和预测区域。</p><p id="ecd1" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">为了更好地解释这一点，假设我们要分割的对象是一个足球。这里，第一个对象是真实图像中的足球区域。第二个对象将是预测图像中的足球区域。</p><p id="79e9" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">那么“意思是”在“意思是借据”中意味着什么呢？“Mean IoU”中的“Mean”代表所有待分割类别的平均值。因此，假设在一项任务中，我们需要从背景中分割出所有的猫。所以这个任务可以解释为一个二元分类任务，两个类分别是猫和背景。因此，平均IoU的计算方法是IoU(cat) + IoU(background) / 2</p><p id="466c" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">类似地，对于多类分类，<br/>意味着IoU = (IoU(class1) + IoU(class2) …..+ IoU(classN)) / N</p><p id="557f" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">取所有类别的平均值有助于减少数据不平衡的影响。关于借据的详细解释可以在<a class="ae le" href="https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2" rel="noopener" target="_blank">这里</a>找到。</p><h1 id="ef05" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">文献评论</h1><h2 id="d2d6" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">医学图像分割面临的挑战</h2><p id="60f5" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">图像分割的任务本身对于深度学习方法来说是一个挑战，因为网络的输出也必须是图像。典型的CNN接收一幅图像作为输入，输出一个数字。这里，我们必须输出一个图像。最重要的是，在医学图像分割中，由于训练图像的数量非常少，这个问题变得更加棘手。因此，在数据稀缺的情况下，这项任务相当具有挑战性。</p><p id="345a" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">解决这一任务的传统方法包括阈值和基于聚类的方法。在过去的几年里，针对医学图像分割的深度学习领域已经取得了很多进展。</p><h2 id="9c3f" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">滑动窗口法</h2><p id="7823" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">在这种方法中，输出图像中的每个像素都是单独预测的。为每个像素提供的输入是围绕该像素的周围网格(比如64x64)的裁剪图像。因此，对于每个图像(HxWx3)，我们总共有HxW个单独的输出和一个64×64×3的输入图像面片作为每个输出的输入。这种方法在当时是非常简单的，但是有很多缺点。1)该方法在运行时非常耗时，并且由于重叠的补丁而存在大量冗余。2)当使用较大的补丁来保持全局结构时，大量的最大池被使用，这在保持局部结构方面失败，并且对于较小的补丁，尽管本地化更好，但是全局上下文丢失。</p><h2 id="e9ad" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">优信网</h2><p id="7c3d" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">这一领域的先驱论文之一为这个问题引入了一个非常独特的架构，叫做U-Net。U-Net的架构如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/872c488c6e2f3bbb84715ae691a30555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OP0mvRcF4uM8bEoU"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://arxiv.org/pdf/1505.04597v1.pdf" rel="noopener ugc nofollow" target="_blank"> U-Net:用于生物医学图像分割的卷积网络</a></figcaption></figure><p id="3f63" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">U-Net是一个编码器-解码器类型的模型，确保输入和输出都是图像。该架构的编码器部分是普通卷积+最大池，我们最终得到一个非常小的特征映射。网络的这一部分试图在小特征尺寸的地图中捕获关于图像的所有信息。现在，这个映射被馈送到解码器部分，解码器部分试图解开信息，然后得到期望的输出。这里，为了接近输出图像，需要增大特征图的大小，同时减小其深度。为此，我们使用上conv(上采样+ conv)。当使用max pool时，本地信息丢失了，up-conv没有帮助检索该信息，因此我们使用crop + copy在相同的水平级别上从以前的地图中复制该信息。最终输出包含两个地图。一个用于背景，一个用于薄膜。论文中没有使用任何填充材料。</p><p id="b9d5" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">U-Net在来自<a class="ae le" href="http://brainiac2.mit.edu/isbi_challenge/" rel="noopener ugc nofollow" target="_blank"> EM分割挑战</a>的非常少的训练图像(30)下做得非常好。它从两个方面改进了滑动窗口:跳过连接和上采样。通过上采样，我们不再需要进行逐像素分割，也不再需要牺牲全局环境(如在滑动窗口中)。跳过连接有助于在最大化池化的同时保留丢失的本地上下文。</p><p id="5237" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">自从引入U-Net以来，已经提出了许多成功的架构，它们都是以此为基础构建的。在这个案例研究中，我比较了三个模型的性能:U-Net、<a class="ae le" href="https://arxiv.org/pdf/1807.10165v1.pdf" rel="noopener ugc nofollow" target="_blank"> U-Net++和</a><a class="ae le" href="https://arxiv.org/pdf/1908.07919v2.pdf" rel="noopener ugc nofollow" target="_blank"> HRNet </a>。后两者的架构在“使用的深度学习架构”一节中解释。</p><h1 id="38b4" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">探索性数据分析</h1><p id="76be" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">数据集由不同背景中的细胞的图像和它们对应的来自背景的分割细胞核的掩模组成。比赛分两个阶段。为了训练，总共提供了670幅图像和相应的掩模。阶段1测试集包含65幅图像，阶段2测试包含3019幅图像。为阶段1测试图像而不是阶段2测试提供了掩模。因此，在本案例研究中，我使用670幅第1阶段训练图像进行训练，并使用65幅第1阶段测试图像作为验证图像。没有提供阶段2测试的掩码，因此我们只能使用它们来手动分析我们的模型如何处理看不见的数据。</p><p id="9b9c" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">总而言之，在这个例子中，我使用670张图片进行训练，65张图片进行验证。3019测试图像(无遮罩)，将仅用于一些手动分析。下面显示了一些由图像及其遮罩组成的示例。</p><div class="kp kq kr ks gt ab cb"><figure class="nz kt ol ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/17993e411c9faf15d8c835ad11191f00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*d-G52xhKtqHhFCV2eZ1Kpg.png"/></div></figure><figure class="nz kt om ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/4b086437142bc6e9323a0c7ba480fc3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*wrNfbfu_jNVvMsCGluyAwg.png"/></div></figure></div><div class="ab cb"><figure class="nz kt of ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/182376efa5d6ebe5a203a2a39c650150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*sEOElKy3k6YQWn0BC0Ul9g.png"/></div></figure><figure class="nz kt oa ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/4defc512a01573f504066ad826f01923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*VRXUKO0mLMjl1TFwueaJTg.png"/></div></figure></div><p id="e141" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">只看前两张图像，你可能会认为，这可以通过简单的阈值处理很容易地解决，但事实并非如此，因为图像包含许多不同的背景。这在接下来的两个例子中显而易见。这是比赛中最具挑战性的部分。更有趣的是，验证图像比训练图像有更多的变化。因此，必须设计一个模型，既能很好地概括，又不会过度适应训练集。</p><h2 id="a73b" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">数据集详细信息</h2><p id="1dea" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">在对一些基本数据进行分析后，发现了以下观察结果:</p><ul class=""><li id="a6cf" class="lx ly iq lz b ma ni mc nj me on mg oo mi op mk oq mm mn mo bi translated">这些图像有不同的形状，最常见的是256x256</li><li id="1d59" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oq mm mn mo bi translated">在train和Val数据中有两种类型的图像:RGB和RGBA (4通道)。最后一个通道称为alpha通道，它决定透明度(255表示不透明，0表示透明)。移除此通道会将我们的图像转换为RGB图像。</li><li id="c0d9" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oq mm mn mo bi translated">在仔细检查测试数据时，我发现只有一个图像是罕见的灰度图像。其余所有其他图像不是RGB就是RGBA。</li><li id="154b" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oq mm mn mo bi translated">图像有不同的大小。</li><li id="f972" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oq mm mn mo bi translated">所有图像的比例都是0-255。没有图像显示在0-1的范围内。</li></ul><p id="a7cc" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><strong class="lz ja">列车图片示例:</strong></p><ul class=""><li id="2922" class="lx ly iq lz b ma ni mc nj me on mg oo mi op mk oq mm mn mo bi translated">在训练数据中，大多数图像是黑白的。有些是有色的。黑白图像看起来很容易从人眼中分离出来。彩色图像通常是紫色的。火车图像没有太多的变化。</li></ul><p id="2ce0" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">下面是一些火车图片的例子，左边是图片，右边是蒙版。</p><div class="kp kq kr ks gt ab cb"><figure class="nz kt or ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/502eedc72f50de809fe26a6c0bd1a353.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*GmA6VGt-1ZFqTctZmCBvyA.png"/></div></figure><figure class="nz kt os ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/0deb5f63def78b1432d7340fa2371931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*6h_Y8Xf7NjMZKN_J57g4xQ.png"/></div></figure></div><div class="ab cb"><figure class="nz kt ot ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/6c673916c93535ed4f7fd85aa3d3fa24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Qif59cqfOKxFk6-BPy_JzQ.png"/></div></figure><figure class="nz kt ou ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/03d1b4b2475f64394dffb3815fb794c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*JyZrHSSN3FWqbLnqcGIzJw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk og di oh oi translated">列车图像示例</figcaption></figure></div><p id="6f7a" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><strong class="lz ja">验证图片示例:</strong></p><ul class=""><li id="54ca" class="lx ly iq lz b ma ni mc nj me on mg oo mi op mk oq mm mn mo bi translated">与训练图像相比，验证图像似乎更难分割。一些图像类似于来自训练数据的图像，而一些图像非常不同，如下所示。</li></ul><div class="kp kq kr ks gt ab cb"><figure class="nz kt ov ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/715ce1a71fa66ec3a7f2b841d57b0c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*d0rVh4GKwPbKJ81tJBxNDA.png"/></div></figure><figure class="nz kt ow ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/2eacbb4c836e6f81bb4e94dea28f7f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*DolkHopmMmkIv8nAhFa5Lg.png"/></div></figure></div><div class="ab cb"><figure class="nz kt ox ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/2cf4f8eece1732a5f6799247332b403e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*sQOW4wQQGMEzYpJKeiGn8g.png"/></div></figure><figure class="nz kt oy ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/b42fd041198991cb133ea56d96c09f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*Qki2BXYCz6CmLukc06u6dA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk oz di pa oi translated">验证图像示例</figcaption></figure></div><p id="7012" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><strong class="lz ja">验证图片示例:</strong></p><ul class=""><li id="2e2b" class="lx ly iq lz b ma ni mc nj me on mg oo mi op mk oq mm mn mo bi translated">另一方面，测试图像包含更多的变化。下面显示了一些测试图像的示例(无遮罩)。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/96016bc92de05740774963e31ec6379f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZ58O18aMwKPn_G3QtPrrg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">测试图像的示例</figcaption></figure><ul class=""><li id="8341" class="lx ly iq lz b ma ni mc nj me on mg oo mi op mk oq mm mn mo bi translated">因此，一般的观察是，从训练到验证到测试，图像的变化增加。因此，过度拟合的机会更高，因此模型的概括能力是模型构建的一个非常重要的部分。</li></ul><h1 id="ea84" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">使用深度学习架构</h1><p id="a5b0" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">我比较了三种型号的性能:U-Net、U-Net++和HRNet。我将图像的输入和输出形状设为256x256，因为这是最常见的形状，也是最先进的CNN网络中非常标准的输入形状。</p><h2 id="9ebb" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">优信网</h2><p id="9fb3" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">该模型的架构已经在“文献综述”部分进行了解释。与每个都有四个编码器和解码器模块的标准U-Net不同，我每个都有三个。对于上采样，我使用了Conv2DTranspose层。整个代码都是用Keras写的，有Tensorflow后端。下面给出了带有模型图像的U-Net的代码。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pc pd l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pe"><img src="../Images/504b29f8cc458bd5f48428c62e1f20fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KTcOZRr0nfqzuOcjmGlIIg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">基于形状的U-Net架构(图片由作者提供)</figcaption></figure><h2 id="9dec" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated"><strong class="ak"> U-Net++ </strong></h2><p id="a361" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">在U-Net中，两个主要思想是跳过连接和上采样。通过上采样，我们不再需要进行逐像素分割，也不再需要牺牲全局上下文(如在滑动窗口中)。而跳过连接有助于在最大化池化的同时保留丢失的本地上下文。在医学分割中，尽可能少的错误和尽可能多的细节是非常重要的。据观察，跳过连接对于恢复目标对象的细粒度细节非常有帮助；即使在复杂背景下也能生成细节清晰的分割蒙版。这是因为细粒度的低级信息存在于实际图像中，当我们由于最大池而向下和向右移动网络时，这些信息变得更粗糙。并且当解码器试图重建图像时，跳过连接试图将该信息提供回解码器部分。如果没有跳过连接，解码器实际上对图像没有直接的概念，而只有最终的编码图。通过跳过连接，解码器可以了解实际图像和细节。</p><p id="c821" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><a class="ae le" href="https://arxiv.org/pdf/1807.10165v1.pdf" rel="noopener ugc nofollow" target="_blank"> U-Net ++ </a>以此为基础，尝试提炼这些跳跃连接。U-Net将来自解码器子网络的深层、语义、粗粒度特征映射与来自编码器子网络的浅层、低级、细粒度特征映射相结合。并且它直接将它们结合起来(裁剪和复制)。U-Net2+努力在将编码器映射与解码器映射组合之前丰富它们，即在组合之前使它们在语义上(逻辑上)更加相似。</p><p id="f164" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">这个建筑的灵感来自于<a class="ae le" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803" rel="noopener" target="_blank">的密集网络</a>。密集网络的优点是1)强梯度流2)更少的参数和计算效率(网络更复杂，因此网络不需要那么深)3)多样化的特征，因为每个特征从不同的层获得特征。</p><p id="f26c" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">下面给出了U-Net++的架构。(节点大写X表示conv操作，箭头表示特征地图的移动。小x1，j表示节点x1，j的输出)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pf"><img src="../Images/f3f125916bf45521520641e4d9444aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPuxChYTiCqv-IFjiOOViQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://arxiv.org/pdf/1807.10165v1.pdf" rel="noopener ugc nofollow" target="_blank"> UNet++:一种用于医学图像分割的嵌套U-Net架构</a></figcaption></figure><p id="04c5" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">这里引入了两个新概念:密集跳跃路径和深度监督</p><p id="6a8d" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><strong class="lz ja">密集跳跃连接</strong>:设x i，j表示节点X i，j的输出，其中I索引下采样层以及编码器，j索引沿着跳跃路径的密集块的卷积层。由x i，j表示的特征地图的堆叠计算如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pg"><img src="../Images/bceb2e94b3d08492985194d68c496fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRfuaTid2jAlD4PlOLAcnA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://arxiv.org/pdf/1807.10165v1.pdf" rel="noopener ugc nofollow" target="_blank"> UNet++:一种用于医学图像分割的嵌套U-Net架构</a></figcaption></figure><p id="665b" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">其中函数H()是卷积运算，后面是激活函数，U()表示上采样层，而[ ]表示连接层。</p><p id="6cf3" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">请注意，这不仅在编码器和解码器层之间使用密集连接的方式上有所不同，而且我们使用卷积而不是简单地复制跳过连接。在最初的U-Net中，他们只是复制。除此之外，本文还使用了<strong class="lz ja">深度监督</strong>，其中对所有顶级语义层的输出进行了平均。</p><p id="f4d9" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">对于它的实现，我将编码器和解码器模块中的卷积数减少了一层。相同的代码和模型图像如下所示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pc pd l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ph"><img src="../Images/474def7e0a35a14c7c28405ccfd3fd7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h4nN0bgO9GFAAWDZMWaTzQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">基于U-Net++的形状架构(图片由作者提供)</figcaption></figure><p id="d0c8" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><strong class="lz ja"> HRNet <br/> </strong>该网络的灵感来自<a class="ae le" href="https://arxiv.org/pdf/1908.07919v2.pdf" rel="noopener ugc nofollow" target="_blank">这篇</a>论文介绍了一种更密集连接的分割架构，其想法是提取不同分辨率特征的深度表示，然后将它们组合在一起。因为代码和图像都很大，所以我没有在这里包含它们。你可以在我的GitHub页面上找到代码。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pi"><img src="../Images/1d55230daf2fb9aa9924e3da4465ea47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OQG3y7OlGr49Hl3njP_8Vw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://arxiv.org/pdf/1908.07919v2.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度高分辨率表征学习</a></figcaption></figure><h1 id="7484" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">结果和讨论</h1><p id="0ac5" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">所有模型都用二元交叉熵作为损失函数进行训练。使用Adam优化器，学习率为1e-3。我用20个纪元的耐心来监控验证损失。对于度量，我使用了Mean IoU的自定义实现。</p><p id="c32e" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">验证损失和平均IoU的图表如下所示</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pj"><img src="../Images/833d9ce6e8f0860b254c5fbb090a51e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJWKpAqPUyK3xxr-OVg0tg.jpeg"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pk"><img src="../Images/841a2f1ee9e032475b14aeb56264a017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NBC-bQ0VrasaEYsJTw2CMA.jpeg"/></div></div></figure><p id="8532" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">在训练之后，对于每个模型，选择在该时期具有最佳平均IoU的模型实例。所有型号的对比如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pl"><img src="../Images/df5408162fc4a7144727349d6d506c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2flUTBizR4hDEWq01WGFg.png"/></div></div></figure><p id="661a" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">三个型号的性能都不相上下。每个模型的训练都在第50个纪元标记附近停止，这意味着所有模型都在第30-35个纪元标记附近达到收敛。表现最好的是U-Net，验证损失为0.1098，平均IoU为0.861。U-Net++以0.8602的分数相差不远。HRNet的性能最低，平均IoU为0.85。</p><p id="12fc" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><em class="pm">除此之外，我还尝试增加U-Net和U-Net++的编码器-解码器模块的数量，但模型开始过度拟合，没有给出任何更好的结果，因此我没有在这里包括它们。</em></p><h2 id="81d7" class="no lg iq bd lh np nq dn ll nr ns dp lp me nt nu lr mg nv nw lt mi nx ny lv iw bi translated">预测掩码</h2><p id="5944" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">下面显示了所有具有平均IoU分数的模型的预测掩码。每行包含一个图像和一个真实遮罩，后面依次是UNet、UNet++和HRNet的预测遮罩。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/90fe11556f227862066f2d68b7287d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vr9zYmIMiiOk7Ij_tBkxGQ.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi po"><img src="../Images/71b8a4538b1f1de60a65d7a07eb63880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FOho9su5GjN-FwJnLYjP3Q.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/c7c35ca1e3e9977ae2ab7277a2338118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6T7LsgIu8PsWLiFtNMtRsg.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pp"><img src="../Images/47dda7e8452a017fd6ea0c6260f7b776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QVzAq2fUCE7kd-Ige_2WJQ.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/138ab3c2da4919eed4ab927cfcce672a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17qdMcUT80EJaDROi9HDAA.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/bdd9f58548d737f789d2058f435c5001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tk6HSI6Sp1BALhjK-2CAkA.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pq"><img src="../Images/3f5305993b2f5fd3e39c1972186b3899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*beRFyrVYgkcKcTsX3W-TjQ.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/88843c343cb5ff4ce96ee644d386a937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WA0xZQzmcRwLAaDVCNaNDQ.png"/></div></div></figure><p id="57ef" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">以下是对测试图像的一些预测(没有真正的面具，因此没有分数)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/94b0b222eb35dcb9076d3b0de6cbd30d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dy4guuuaG34l_jOYAFtaTg.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/8952332d4a9365459e6bfbe380746f07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RuGh8ThKx4eSvcyPdg3T8A.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/7d6f98bea38dceda334fcac7294215d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*meYuWXAaPFHUDof5ApybLQ.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pr"><img src="../Images/83c305cefc988a82e70a6685fc36ad13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VXgA52HBpEtKu-YUOcRGvA.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/a7ef6a971ce02e6734ce7d99322309e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EqfcSxckJ7bREbTXSLan3Q.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/dbd5694c71d204a23e81bc01a91a5437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UhErtnDd_MKt-rmsmLCSzQ.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/2e3c61be266a916ef8f05146c6447f49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Kvmx-U-S_gpOWjtWRTDJw.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/8e6970ed14004e6b2bb769a8bdc4b3e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UDLUp4GfOcRLn0amuUx3Q.png"/></div></div></figure><p id="2540" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">对于验证图像，模型的性能相当不错，但是测试预测并不令人满意。对于如此少的训练图像，仍然存在一般化的问题。</p><p id="64da" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">最后，我选择了最好的模型作为UNet。该模型的<strong class="lz ja">平均推理时间</strong>为<strong class="lz ja"> 0.964秒</strong>，而<strong class="lz ja">模型大小</strong>为<strong class="lz ja"> 28 MB </strong>。</p><h1 id="51d3" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">训练后量化</h1><blockquote class="ps pt pu"><p id="4e68" class="mu mv pm lz b ma ni ka mw mc nj kd mx pv nk mz na pw nl nc nd px nm nf ng mk ij bi translated">训练后量化是一种转换技术，可以减少模型大小，同时还可以改善CPU和硬件加速器延迟，而模型准确性几乎不会下降<br/> - TensorFlow官方文档</p></blockquote><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="463a" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">有不同的量化方法。对于我的模型，我使用了Float16量化，其中所有的权重都转换为16位浮点数。这将模型大小减少了一半:从28 MB减少到14 MB。</p><p id="daaa" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">一个重要的检入量化是看它是否不降低模型的性能。这正是我不能使用动态范围量化等其他方法的原因，因为我的机箱中的CPU不支持这些方法，因此获得输出需要花费大量时间。因此，不可能检查性能是否有任何下降。我可以检查Float16量化，预测性能没有下降<strong class="lz ja">(量化模型的平均IoU分数也是0.861)，所以我继续使用它。</strong></p><p id="7c3f" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">所以，最后，<strong class="lz ja">型号尺寸</strong>为<strong class="lz ja"> 14 MB </strong>。推断时间从<strong class="lz ja"> 0.964 </strong>秒增加到<strong class="lz ja"> 1.261 </strong>秒，这很好，因为模型尺寸减小了。</p><h1 id="0087" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">Web应用程序</h1><p id="2f08" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">我为这个项目构建了一个web应用程序，并通过streamlit进行了部署:<br/><a class="ae le" href="https://share.streamlit.io/kriz17/mdt_app" rel="noopener ugc nofollow" target="_blank">https://share.streamlit.io/kriz17/mdt_app</a></p><h1 id="9510" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">总结和未来工作</h1><p id="be81" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">在这个案例研究中，我设计了基于深度CNN的模型，用于在各种条件下自动检测图像中的细胞核。比较了基于U-Net、U-Net++和HRNet的总共三个模型的性能。表现最好的模型是基于U-Net的模型，IoU平均得分为0.861。此外，我使用Float16量化将模型大小从28 MB减少到14 MB。该模型的平均推理时间为每个样本0.126秒，这对于手头的任务来说是相当不错的。最后，我使用streamlit部署了这个模型。</p><p id="2574" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">所有三个模型都做得相当不错，每个模型都获得了大于0.85的平均IoU分数。它们可以用于具有标准背景的图像，如在训练数据中，但是没有很大的变化。主要的挑战是小的训练数据集和验证图像背景的高度变化。模型的传统架构导致过度拟合，因此我不得不减小模型的大小。在未来的工作中，我想尝试使用弹性变形进行数据扩充，就像最初的U-Net论文中实现的那样。我还想探索基于注意力机制的变压器模型，因为它们已经被证明对小数据集非常有效[3]。</p><h1 id="fc2e" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">感谢</h1><p id="2310" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">我要感谢整个应用人工智能课程团队，特别是Ramana Sir在整个案例研究中对我的指导。</p><h1 id="41c4" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参考</h1><p id="11c0" class="pw-post-body-paragraph mu mv iq lz b ma mb ka mw mc md kd mx me my mz na mg nb nc nd mi ne nf ng mk ij bi translated">[1]<a class="ae le" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></p><p id="9c55" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">[2]<a class="ae le" href="https://www.kaggle.com/c/data-science-bowl-2018/" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/data-science-bowl-2018/</a></p><p id="c896" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">[3]https://arxiv.org/pdf/2102.10662v2.pdf<a class="ae le" href="https://arxiv.org/pdf/2102.10662v2.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="ec16" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">[4]https://arxiv.org/pdf/2009.13120.pdf<a class="ae le" href="https://arxiv.org/pdf/2009.13120.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="3243" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">[5]<a class="ae le" href="https://medium.com/swlh/image-segmentation-using-deep-learning-a-survey-e37e0f0a1489" rel="noopener">https://medium . com/swlh/image-segmentation-using-deep-learning-a-survey-e 37 E0 f 0a 1489</a></p><p id="e47f" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><a class="ae le" href="https://paperswithcode.com/task/medical-image-segmentation" rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com/task/medical-image-segmentation</a></p><p id="7cce" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">[7]https://en.wikipedia.org/wiki/Image_segmentatio</p><p id="fad0" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><a class="ae le" href="https://arxiv.org/pdf/1505.04597v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1505.04597v1.pdf</a></p><p id="deb2" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">[9]<a class="ae le" href="https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760" rel="noopener" target="_blank">https://towards data science . com/review-u-net-biomedical-image-segmentation-d 02 BF 06 ca 760</a></p><p id="3a43" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">[10]<a class="ae le" href="https://drive.google.com/file/d/1U-SEtD5NExw86Bau7mOW9Lz9F1n7LpF3/view" rel="noopener ugc nofollow" target="_blank">https://drive . Google . com/file/d/1U-set D5 nexw 86 Bau 7 mow 9 LZ 9f 1n 7 LPF 3/view</a></p><p id="7868" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">[11]<a class="ae le" href="https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf</a></p><p id="c296" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><a class="ae le" href="https://arxiv.org/pdf/1802.02427.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1802.02427.pdf</a></p><p id="9fa7" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><a class="ae le" href="https://arxiv.org/pdf/1807.10165v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1807.10165v1.pdf</a></p><p id="43c8" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><a class="ae le" href="https://arxiv.org/ftp/arxiv/papers/2004/2004.08790.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/ftp/arxiv/papers/2004/2004.08790.pdf</a></p><p id="1c31" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">[15]<a class="ae le" href="https://sh-tsang.medium.com/reading-unet-3-a-full-scale-connected-unet-medical-image-segmentation-ebb5e7f53caa" rel="noopener">https://sh-tsang . medium . com/reading-unet-3-a-full-scale-connected-unet-medical-image-segmentation-ebb 5 e 7 f 53 CAA</a></p><p id="a259" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated"><a class="ae le" href="https://arxiv.org/pdf/1908.07919v2.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1908.07919v2.pdf</a></p></div><div class="ab cl py pz hu qa" role="separator"><span class="qb bw bk qc qd qe"/><span class="qb bw bk qc qd qe"/><span class="qb bw bk qc qd"/></div><div class="ij ik il im in"><p id="1b8e" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">完整的代码可以在我的GitHub档案中找到:</p><div class="qf qg gp gr qh qi"><a href="https://github.com/kriz17/Medical-Image-Segmentation" rel="noopener  ugc nofollow" target="_blank"><div class="qj ab fo"><div class="qk ab ql cl cj qm"><h2 class="bd ja gy z fp qn fr fs qo fu fw iz bi translated">GitHub-kriz 17/医学图像分割</h2><div class="qp l"><h3 class="bd b gy z fp qn fr fs qo fu fw dk translated">使用基于深度CNN的模型自动检测发散图像中的细胞核。请找到关于这个案例的详细博客…</h3></div><div class="qq l"><p class="bd b dl z fp qn fr fs qo fu fw dk translated">github.com</p></div></div><div class="qr l"><div class="qs l qt qu qv qr qw ky qi"/></div></div></a></div><p id="400d" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">在LinkedIn上与我联系:</p><div class="qf qg gp gr qh qi"><a href="https://www.linkedin.com/in/kriz-moses/" rel="noopener  ugc nofollow" target="_blank"><div class="qj ab fo"><div class="qk ab ql cl cj qm"><h2 class="bd ja gy z fp qn fr fs qo fu fw iz bi translated">印度中央邦克里兹·摩西-IIT·印多尔-印多尔| LinkedIn</h2><div class="qp l"><h3 class="bd b gy z fp qn fr fs qo fu fw dk translated">在全球最大的职业社区LinkedIn上查看Kriz Moses的个人资料。Kriz的教育列在他们的…</h3></div><div class="qq l"><p class="bd b dl z fp qn fr fs qo fu fw dk translated">www.linkedin.com</p></div></div><div class="qr l"><div class="qx l qt qu qv qr qw ky qi"/></div></div></a></div><p id="3b42" class="pw-post-body-paragraph mu mv iq lz b ma ni ka mw mc nj kd mx me nk mz na mg nl nc nd mi nm nf ng mk ij bi translated">PS :如果你认为他们可以改进项目/博客，请随时提供意见/批评。我一定会努力做出必要的改变。</p></div></div>    
</body>
</html>