<html>
<head>
<title>K-means Clustering from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类从零开始</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/k-means-clustering-from-scratch-efd629435987?source=collection_archive---------0-----------------------#2020-07-03">https://pub.towardsai.net/k-means-clustering-from-scratch-efd629435987?source=collection_archive---------0-----------------------#2020-07-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="38e5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="3a58" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">NumPy是你所需要的</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d74b74c6a1d4d8ae6002c9f916c3cc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WnUnIee8UxvpdaTR"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lh" href="https://unsplash.com/@heyerlein?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">h·海尔林</a>拍摄的照片</figcaption></figure><blockquote class="li"><p id="bff8" class="lj lk it bd ll lm ln lo lp lq lr ls dk translated">一个算法必须被看到才会被相信——唐纳德·克努特</p></blockquote><h1 id="964c" class="lt lu it bd lv lw lx ly lz ma mb mc md ki me kj mf kl mg km mh ko mi kp mj mk bi translated">概观</h1><p id="ca80" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ls im bi translated">机器学习科学可以大致分为两类:</p><ul class=""><li id="4d8d" class="ng nh it mn b mo ni mr nj mu nk my nl nc nm ls nn no np nq bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">监督学习</a></li><li id="007a" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls nn no np nq bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Unsupervised_learning" rel="noopener ugc nofollow" target="_blank">无监督学习</a></li></ul><p id="21d0" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">在这篇博文中，我们将实现一个流行的无监督学习算法，k-means聚类。</p><p id="8c36" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">这种流行的算法使用数值距离度量将数据划分到聚类中。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="9b1a" class="lt lu it bd lv lw og ly lz ma oh mc md ki oi kj mf kl oj km mh ko ok kp mj mk bi translated">算法</h1><p id="8981" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ls im bi translated">假设我们有一堆观察结果，我们想要将“相似的”观察结果分割在一起。我们将使用下面的算法来实现我们的目标。</p><h2 id="30c1" class="ol lu it bd lv om on dn lz oo op dp md mu oq or mf my os ot mh nc ou ov mj iz bi translated"><em class="ow"> K均值算法</em></h2><p id="cdda" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ls im bi translated"><em class="ox">输入:k(聚类数)，D(数据点)</em></p><ol class=""><li id="f04d" class="ng nh it mn b mo ni mr nj mu nk my nl nc nm ls oy no np nq bi translated"><em class="ox">选择随机k个数据点作为初始聚类均值</em></li><li id="e98d" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated"><em class="ox">将D中的每个数据点关联到最近的质心。<br/>这将把数据分成k个簇。</em></li><li id="2093" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated"><em class="ox">重新计算质心</em></li><li id="12e0" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated"><em class="ox">重复步骤2和步骤3，直到数据点的聚类成员不再有变化<br/>。</em></li></ol><p id="fb76" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">让我们更详细地看看上面的算法。</p><p id="6c27" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">我们首先将每个数据点随机分配给一个聚类。然后，我们计算每组聚类的均值。</p><p id="77d6" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">之后，我们继续计算每个点和聚类平均值之间的平方<a class="ae lh" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>。然后，我们根据数据点和每个聚类的聚类均值之间的最小平方欧几里德距离，为每个数据点分配一个聚类。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/0509313c6c3ee13839ecdb23f9d92ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/0*qrIZk5zZRBdT40v2"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">两点p和q之间的欧几里德距离(<a class="ae lh" href="https://arachnoid.com/latex/" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="5f14" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">然后重新计算聚类平均值，我们继续根据平方欧几里德距离重新分配每个数据点，直到没有数据点的聚类分配发生变化。</p><p id="fc79" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">如果你问一个统计学家，她/他可能会告诉你，我们正试图最小化<strong class="mn jd">类内平方和(WCSS)。现在让我们试着用Python实现这个算法。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/21b52e51063b865c4b72f01ae06eaa50.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/0*IOadXp6eh2f04vU_"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">类内平方和。(<a class="ae lh" href="https://arachnoid.com/latex/" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="6416" class="lt lu it bd lv lw og ly lz ma oh mc md ki oi kj mf kl oj km mh ko ok kp mj mk bi translated">履行</h1><p id="20f4" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ls im bi translated">尽管Python中有许多k-means算法的库实现，但为了提供一种有指导意义的方法，我决定只使用Numpy。Numpy是Python中用于数值计算的流行库。</p><h2 id="745f" class="ol lu it bd lv om on dn lz oo op dp md mu oq or mf my os ot mh nc ou ov mj iz bi translated">代码走查</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="c951" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">我们首先创建一个名为<code class="fe pd pe pf pg b">Kmeans</code>的类，并向它传递一个构造函数参数<code class="fe pd pe pf pg b">k</code>。这个参数是一个<strong class="mn jd">超参数</strong>。超参数是用户在训练机器学习算法之前设置的参数。在我们的例子中，这是我们希望将数据划分到的集群总数。我们还向构造函数添加了两个属性，<code class="fe pd pe pf pg b">means</code>存储聚类的平均值，<code class="fe pd pe pf pg b">_cluster_ids</code>存储聚类的id值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="d924" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">然后我们创建一个名为<code class="fe pd pe pf pg b">cluster_ids</code>的方法，作为集群id的get方法。<code class="fe pd pe pf pg b">@property</code>是一个函数装饰器。要了解更多信息，请查看这篇文章。另一个叫做<code class="fe pd pe pf pg b">_init_centroid</code>的方法被创建来<strong class="mn jd">随机分配</strong>每个数据点到一个集群。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="3c74" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated"><code class="fe pd pe pf pg b">_cluster_means</code>计算我们聚类的平均值。它接受一个包含数据的Numpy数组和另一个将集群id作为输入的Numpy数组。我们使用一个临时数组<code class="fe pd pe pf pg b">temp</code>来存储我们的特征和集群id。然后，我们计算每个集群中每个数据点的平均值，并将其作为数组返回。</p><p id="7191" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">注意，可能有一些集群可能没有任何数据(因为我们最初是随机分配集群的)。因此，如果有一个没有数据的聚类，我们随机选择一个观察值作为该聚类的一部分。</p><p id="e844" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated"><code class="fe pd pe pf pg b">_compute_cluster</code>是确定哪个聚类的均值最接近数据点的方法。<code class="fe pd pe pf pg b">np.linalg.norm()</code>方法计算<strong class="mn jd">欧几里德距离</strong>。我们对此求平方以获得<strong class="mn jd">类内平方和。</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pb pc l"/></div></figure><p id="d28d" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">最后，我们创建了协调集群过程的fit方法。</p><p id="ecfa" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated"><code class="fe pd pe pf pg b">fit()</code>方法中的步骤:</p><ol class=""><li id="f162" class="ng nh it mn b mo ni mr nj mu nk my nl nc nm ls oy no np nq bi translated">我们首先将每个观察初始化为一个集群。我们还创建了一个零数组来存储新的集群id。</li><li id="e988" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated">然后，我们使用函数<code class="fe pd pe pf pg b">itertools.count()</code>创建一个无限循环，并计算聚类平均值。</li><li id="c6d0" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated">然后，我们根据聚类平均值和每个数据点之间的平方距离分配新的聚类id。</li><li id="be74" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated">然后，我们检查是否有任何数据点改变了聚类。如果是，那么我们使用新的聚类id来重新计算聚类均值。</li><li id="3197" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated">重复步骤2到4，直到没有数据点改变聚类。</li></ol><p id="cddf" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">好了，伙计们！您已经成功创建了自己的能够对数据进行聚类的k均值聚类类。以下是几个数据集的一些结果:</p><h2 id="96df" class="ol lu it bd lv om on dn lz oo op dp md mu oq or mf my os ot mh nc ou ov mj iz bi translated">形象化</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ph pc l"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ph pc l"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ph pc l"/></div></figure></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="0edf" class="lt lu it bd lv lw og ly lz ma oh mc md ki oi kj mf kl oj km mh ko ok kp mj mk bi translated">选择k的值</h1><p id="7e21" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ls im bi translated">由于k是一个超参数，我们必须有一些方法来选择k的最佳值。一个流行的方法是<a class="ae lh" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)#:~:text=In%20cluster%20analysis%2C%20the%20elbow,number%20of%20clusters%20to%20use." rel="noopener ugc nofollow" target="_blank">肘方法</a>。</p><p id="841b" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">简而言之，肘法绘制了一条聚类数相对于解释变异百分比的曲线。通过遵循收益递减的<a class="ae lh" href="https://en.wikipedia.org/wiki/Diminishing_returns" rel="noopener ugc nofollow" target="_blank">定律，由肘方法产生的曲线被从业者用来确定最优的集群数量。</a></p><p id="ba87" class="pw-post-body-paragraph ml mm it mn b mo ni kd mq mr nj kg mt mu nw mw mx my nx na nb nc ny ne nf ls im bi translated">如果添加一个额外的聚类没有显著改善k的变化，我们选择坚持当前的聚类数。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="2374" class="lt lu it bd lv lw og ly lz ma oh mc md ki oi kj mf kl oj km mh ko ok kp mj mk bi translated">提示和优化</h1><p id="bf39" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ls im bi translated">以下是一些确保获得良好聚类的技巧:</p><ol class=""><li id="98e4" class="ng nh it mn b mo ni mr nj mu nk my nl nc nm ls oy no np nq bi translated"><strong class="mn jd">删除非数字特征</strong> : <br/>数据可能有表示为数字特征的非数字(分类)特征。这些数字并不具有一定的数量价值，它们可能被用作一个群体的标签。例如，如果我们正在处理一个人口数据集，一个名为“性别”的列可能具有代表男性和女性的值0和1。我们必须小心移除这些特征，因为它们没有任何量化值，因此会扭曲我们算法的“距离”概念。</li><li id="6907" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated"><strong class="mn jd">特征缩放:<br/> </strong>数值型数据会有不同的范围。具有巨大范围的特定特征可能对我们的聚类目标函数产生不利影响。与其他要素相比，具有较大范围值的要素将在聚类过程中占据主导地位。因此，对我们的数据进行缩放至关重要，以便每个特征的贡献与算法成比例。</li><li id="0d18" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated"><strong class="mn jd">更好的初始化:<br/> </strong>在我们的算法中，我们将初始聚类随机分配给数据。由于这种固有的随机性，我们的算法可能不总是提供好的聚类。有几种方法可以改进设置初始聚类的标准。对于这个任务，<a class="ae lh" href="https://en.wikipedia.org/wiki/K-means%2B%2B" rel="noopener ugc nofollow" target="_blank"> k-means++算法是一个流行的选择。</a></li><li id="c26b" class="ng nh it mn b mo nr mr ns mu nt my nu nc nv ls oy no np nq bi translated"><strong class="mn jd">不同的算法:<br/> </strong>有一些算法是k-means算法的变体，它们在处理诸如异常值之类的某些约束时更健壮。一个这样的算法是<a class="ae lh" href="https://en.wikipedia.org/wiki/K-medoids" rel="noopener ugc nofollow" target="_blank"> k-medoids </a>。k-medoids算法使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Taxicab_geometry" rel="noopener ugc nofollow" target="_blank"> L1距离</a>而不是L2距离(欧几里德距离)。有许多其他的聚类算法对于特定的应用是有用的，例如层次聚类、基于密度的聚类、模糊聚类等。</li></ol></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="4bcd" class="lt lu it bd lv lw og ly lz ma oh mc md ki oi kj mf kl oj km mh ko ok kp mj mk bi translated">结论</h1><p id="7e0f" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ls im bi translated">我希望你们都喜欢这篇博文。更多关于数据科学的文章，请查看我在medium上的其他帖子。请随时在<a class="ae lh" href="https://www.linkedin.com/in/sayarbanerjee/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系。这篇博文的代码在我的<a class="ae lh" href="https://github.com/Sayar1106/TowardsDataSciencecodefiles/blob/master/Kmeansfromscratch/kmeans.py" rel="noopener ugc nofollow" target="_blank"> Github </a>上。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="a68d" class="lt lu it bd lv lw og ly lz ma oh mc md ki oi kj mf kl oj km mh ko ok kp mj mk bi translated">参考</h1><div class="pi pj gp gr pk pl"><a href="https://scikit-learn.org/stable/datasets/index.html#datasets" rel="noopener  ugc nofollow" target="_blank"><div class="pm ab fo"><div class="pn ab po cl cj pp"><h2 class="bd jd gy z fp pq fr fs pr fu fw jc bi translated">7.数据集加载实用程序-sci kit-了解0.23.1文档</h2><div class="ps l"><h3 class="bd b gy z fp pq fr fs pr fu fw dk translated">sklearn.datasets包嵌入了一些小的玩具数据集，如入门部分所介绍的。这个包裹…</h3></div><div class="pt l"><p class="bd b dl z fp pq fr fs pr fu fw dk translated">scikit-learn.org</p></div></div><div class="pu l"><div class="pv l pw px py pu pz lb pl"/></div></div></a></div><div class="pi pj gp gr pk pl"><a href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener  ugc nofollow" target="_blank"><div class="pm ab fo"><div class="pn ab po cl cj pp"><h2 class="bd jd gy z fp pq fr fs pr fu fw jc bi translated">k均值聚类</h2><div class="ps l"><h3 class="bd b gy z fp pq fr fs pr fu fw dk translated">k-means聚类是一种矢量量化的方法，它起源于信号处理，目的是划分n。</h3></div><div class="pt l"><p class="bd b dl z fp pq fr fs pr fu fw dk translated">en.wikipedia.org</p></div></div><div class="pu l"><div class="qa l pw px py pu pz lb pl"/></div></div></a></div><div class="pi pj gp gr pk pl"><a href="https://www.oreilly.com/library/view/data-science-from/9781492041122/" rel="noopener  ugc nofollow" target="_blank"><div class="pm ab fo"><div class="pn ab po cl cj pp"><h2 class="bd jd gy z fp pq fr fs pr fu fw jc bi translated">从头开始的数据科学，第二版</h2><div class="ps l"><h3 class="bd b gy z fp pq fr fs pr fu fw dk translated">要真正学习数据科学，你不仅要掌握工具——数据科学库、框架、模块和…</h3></div><div class="pt l"><p class="bd b dl z fp pq fr fs pr fu fw dk translated">www.oreilly.com</p></div></div><div class="pu l"><div class="qb l pw px py pu pz lb pl"/></div></div></a></div></div></div>    
</body>
</html>