<html>
<head>
<title>Activation Function in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/activation-function-in-neural-networks-e5b95ab2e478?source=collection_archive---------2-----------------------#2022-10-14">https://pub.towardsai.net/activation-function-in-neural-networks-e5b95ab2e478?source=collection_archive---------2-----------------------#2022-10-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f16f1e157f7554599d1d9e8e62c235ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7mE1hFF7WAUF7FXttVjzxw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</figcaption></figure><p id="efc7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">激活函数决定一个神经元是否应该被激活，有时，它也被称为<strong class="ke ir">传递函数</strong>。激活函数的主要作用是将来自前几层的总加权输入转换成馈送给下一层神经元的输出。</p><p id="b7e4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们看看神经网络中神经元的架构。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi la"><img src="../Images/c7ac87f79e4565ab536d2ed803d6ea46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AsjGnS6iOgsA5RecS_ig5Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</figcaption></figure><p id="70db" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">激活函数的主要目的是给线性模型添加非线性。在转发传播过程中，它在每一层都引入了一个额外的步骤。如果我们不放置激活函数，每个神经元将充当一个线性函数，进而将整个网络转化为线性回归模型。</p><h1 id="e7c9" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">激活功能的类型</h1><p id="1f15" class="pw-post-body-paragraph kc kd iq ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">让我们回顾一下神经网络中常用的激活函数。</p><h2 id="a3a1" class="mi lg iq bd lh mj mk dn ll ml mm dp lp kn mn mo lt kr mp mq lx kv mr ms mb mt bi translated">1)二元阶跃激活函数</h2><p id="eed6" class="pw-post-body-paragraph kc kd iq ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">层中的神经元应该被激活还是不被激活取决于特定的阈值。</p><p id="1cc1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">将馈送给激活函数的输入与阈值进行比较。如果输入大于它，那么神经元被激活，否则它被去激活。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/b7ae64a75484d200af4c0308629b6f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*Z1nZJZo1J9VTS3LKVWA1CA.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/b24767f69de5656f58a08292b254b0c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*hadxGcDr284hFZkNue9ZLw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">二元阶跃函数|作者图片</figcaption></figure><p id="e581" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下面是二元阶跃函数的导数</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/9a61d4fefdea75e26e92a26ca223b1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/format:webp/1*pn1yAkyrm3c-GFxCMcJ8tw.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/df8f27921ba2f0a2c30833eaa73f090b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ZJvvC4KX00fSFFxc9WvsLA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">二元阶跃函数的导数|作者图片</figcaption></figure><p id="2b00" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以下是二元阶跃函数的一些限制:</p><ul class=""><li id="d7de" class="mx my iq ke b kf kg kj kk kn mz kr na kv nb kz nc nd ne nf bi translated">不能用于多类分类问题。</li><li id="7053" class="mx my iq ke b kf ng kj nh kn ni kr nj kv nk kz nc nd ne nf bi translated">函数的梯度为零，这在反向传播过程中造成阻碍。</li></ul><h2 id="4078" class="mi lg iq bd lh mj mk dn ll ml mm dp lp kn mn mo lt kr mp mq lx kv mr ms mb mt bi translated">2)线性激活函数</h2><p id="5c7b" class="pw-post-body-paragraph kc kd iq ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">线性激活函数，也称为<strong class="ke ir">无激活</strong>或<strong class="ke ir">恒等函数</strong>，激活与输入成比例。该函数只是简单地给出它被赋予的值。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b5fa9c984c556049dcdd0068d0ea230f.png" data-original-src="https://miro.medium.com/v2/resize:fit:184/format:webp/1*GbbYg1jD4ICOQOqkk-j0OQ.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/bdde6d91392d02200b603244503aff7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*UtjyfsT1QpJ-gwWqFM0sFw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">线性函数|作者图片</figcaption></figure><p id="5281" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下面是一个线性函数的一阶导数</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/2f5949fbaf7cba1b9f5e3eed389c62c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*sYqCfi5AIbZCbClgYhlshg.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3279286746e6394df137a2ad03c63436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dWvW2VexX-jBnEYBshCNkg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">线性函数的导数|作者图片</figcaption></figure><p id="936b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以下是线性函数的一些限制:</p><ul class=""><li id="24d3" class="mx my iq ke b kf kg kj kk kn mz kr na kv nb kz nc nd ne nf bi translated">不可能使用反向传播，因为函数的导数是常数，与输入无关。</li><li id="6426" class="mx my iq ke b kf ng kj nh kn ni kr nj kv nk kz nc nd ne nf bi translated">无论神经网络有多少层，最后一层仍将是第一层的线性函数。所以，一个线性激活函数把神经网络变成了一个单层网络。</li></ul><h2 id="0ecb" class="mi lg iq bd lh mj mk dn ll ml mm dp lp kn mn mo lt kr mp mq lx kv mr ms mb mt bi translated">3)乙状结肠激活功能</h2><p id="b03d" class="pw-post-body-paragraph kc kd iq ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">该函数将任何实数值作为输入，并输出0到1范围内的值。输入越大(越正)，输出值越接近1，而输入越小(越负)，输出越接近0。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/551c20ae52884900cdd164120e3b0512.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*OKEMZ7PzlBONQWyuFGERTw.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3c5fea018433c5b907b69e73e4ad771a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lHLKfauHpQykCHGRNpFejg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">Sigmoid激活功能|作者图片</figcaption></figure><p id="3eb2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是一个sigmoid函数的导数</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/cb4f963dc40172e2fea2f43d97fb2240.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*I9DPdfIXVXFjv0EYbTThPw.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/7070061c63ca882a1ecf14350a8a12cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*wgkzKiXye7VWPEKQ4Yg9zQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">Sigmoid激活函数的导数|图片由作者提供</figcaption></figure><ul class=""><li id="7050" class="mx my iq ke b kf kg kj kk kn mz kr na kv nb kz nc nd ne nf bi translated">它通常用于我们必须预测概率作为输出的模型。</li><li id="6a61" class="mx my iq ke b kf ng kj nh kn ni kr nj kv nk kz nc nd ne nf bi translated">该函数是可微分的，并且提供了平滑的梯度。</li></ul><p id="5ae0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以下是sigmoid函数的一些限制:</p><ul class=""><li id="a34b" class="mx my iq ke b kf kg kj kk kn mz kr na kv nb kz nc nd ne nf bi translated">梯度值仅在-3到3的范围内有意义，在其他区域图形变得更加平坦。</li><li id="3265" class="mx my iq ke b kf ng kj nh kn ni kr nj kv nk kz nc nd ne nf bi translated">对于大于3或小于-3的值，函数将具有非常小的梯度。当梯度值接近零时，网络遭受<strong class="ke ir"> <em class="np">消失梯度</em> </strong>问题。</li><li id="15da" class="mx my iq ke b kf ng kj nh kn ni kr nj kv nk kz nc nd ne nf bi translated">逻辑函数的输出不是围绕零对称的，<em class="np">，即值总是正的，</em>，这使得训练困难。</li></ul><h1 id="c2f1" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">4) Tanh激活功能</h1><p id="007c" class="pw-post-body-paragraph kc kd iq ke b kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz ij bi translated">双曲正切函数与sigmoid函数非常相似，不同之处在于输出范围为-1至1。在Tanh中，输入越大，输出值越接近1，而输入越小，输出越接近-1，而不是sigmoid中的0。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3ac2749061ab9a3e5f988b3bcace89ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*zJevfvATD1FYcYvF-xx3GA.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a47a29c6016166c41bcea440055fef05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*t47XOWMwwXSbe0PRFdwp4Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">Tanh激活功能|作者图片</figcaption></figure><p id="1263" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是双曲正切函数的导数</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1ba311edef972b741b5ec2d0312acfa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*hXzT1H9Z9LWMlumcpO3c3g.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/13864b3cb59f3af09bdde75a6576a2e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*N8DuPIemu-PrxOs_3MUByA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">Tanh激活函数的导数|图片由作者提供</figcaption></figure><ul class=""><li id="093e" class="mx my iq ke b kf kg kj kk kn mz kr na kv nb kz nc nd ne nf bi translated">tanh激活函数的输出是以零为中心的，它有助于将数据置于中心，并使下一层的学习更加容易。</li></ul><p id="0ab4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以下是双曲正切函数的一些限制:</p><ul class=""><li id="5dcc" class="mx my iq ke b kf kg kj kk kn mz kr na kv nb kz nc nd ne nf bi translated">当梯度值接近零时，网络遭受<strong class="ke ir"> <em class="np">消失梯度</em> </strong>问题。</li><li id="9aaf" class="mx my iq ke b kf ng kj nh kn ni kr nj kv nk kz nc nd ne nf bi translated">与sigmoid函数相比，该函数的梯度更陡。</li></ul></div></div>    
</body>
</html>