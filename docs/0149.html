<html>
<head>
<title>APTOS 2019 Blindness Detection — Playing around with ResNeXts and Progressive NASNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">APTOS 2019失明检测—使用ResNeXts和Progressive NASNet</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/aptos-2019-blindness-detection-playing-around-with-resnexts-and-progressive-nasnets-2ab82163daea?source=collection_archive---------1-----------------------#2019-09-03">https://pub.towardsai.net/aptos-2019-blindness-detection-playing-around-with-resnexts-and-progressive-nasnets-2ab82163daea?source=collection_archive---------1-----------------------#2019-09-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5a7f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">APTOS 2019失明检测| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向AI </a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/ae68928dc548f27d32d2b3c539b1b3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jGqZUe-koITWYjzdvCn1wQ.png"/></div></div></figure></div><div class="ab cl kh ki hu kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="ij ik il im in"><h2 id="f8d0" class="ko kp iq bd kq kr ks dn kt ku kv dp kw kx ky kz la lb lc ld le lf lg lh li iw bi translated">不久前，Kaggle宣布了挑战:APTOS 2019失明检测——检测糖尿病视网膜病变，在为时已晚之前停止失明。</h2><p id="70b6" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">竞赛的目的是预测糖尿病视网膜病变的严重程度，等级为0-4。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="fda3" class="ko kp iq mj b gy mn mo l mp mq">0 - No DR</span><span id="b046" class="ko kp iq mj b gy mr mo l mp mq">1 - Mild</span><span id="9882" class="ko kp iq mj b gy mr mo l mp mq">2 - Moderate</span><span id="31be" class="ko kp iq mj b gy mr mo l mp mq">3 - Severe</span><span id="c6d2" class="ko kp iq mj b gy mr mo l mp mq">4 - Proliferative DR</span></pre><p id="b636" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">训练集由3661个图像和4个类别组成。我们应该注意到数据集是不平衡的。嗯，我们将使用一些技巧来最大化可能的结果，包括增加，冻结和解冻层等。</p><p id="6598" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">我尝试了几个模型，如<strong class="ll ja"> DenseNet、ResNet50/101、Inception v3/v4 </strong>甚至<strong class="ll ja">ResNeXt-101–32x8d</strong>—具有<strong class="ll ja"> 88M </strong>参数的架构、<strong class="ll ja"> 82.2 </strong> <em class="mx"> top-1 </em>和<strong class="ll ja"> 96.4 </strong> <em class="mx"> top-5 </em>错误无法超过<strong class="ll ja"> 66.2</strong></p><p id="142e" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">所以，我坐下来分析了数据集和几个模型。我的想法是——不管我把它们调得多好，大模型都过拟合，小模型都过拟合。</p><p id="3ac1" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">再次搜索模型，我得到了:</p><ul class=""><li id="903b" class="my mz iq ll b lm ms lq mt kx na lb nb lf nc md nd ne nf ng bi translated"><strong class="ll ja"><em class="mx">ResNeXt 5 32x4d—{</em></strong><a class="ae nh" href="https://arxiv.org/pdf/1611.05431.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mx">参见此处论文</em> </a> <strong class="ll ja"> <em class="mx"> } </em> </strong></li><li id="dd72" class="my mz iq ll b lm ni lq nj kx nk lb nl lf nm md nd ne nf ng bi translated"><strong class="ll ja"> <em class="mx"> PNASNet 5大— { </em> </strong> <a class="ae nh" href="https://arxiv.org/pdf/1712.00559.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mx">参见此处论文</em> </a> <strong class="ll ja"> <em class="mx"> } </em> </strong></li></ul><p id="88d1" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">并且，决定尝试两种模式。</p><p id="12c3" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">我实现了<strong class="ll ja"> TensorboardX </strong>，执行后生成<em class="mx">‘run’</em>目录，这样我们就可以可视化训练过程了。</p><p id="0105" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">火炬。数据并行？不，我现在是谷歌可乐的受害者。</p><h1 id="8608" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">ResNeXt50培训的PyTorch代码:</h1><figure class="me mf mg mh gt ka"><div class="bz fp l di"><div class="oe of l"/></div></figure><h1 id="7cb2" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">对于PNASNet 5 Large:</h1><p id="e8c5" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">我最初使用了<a class="ae nh" href="https://github.com/Cadene" rel="noopener ugc nofollow" target="_blank"> Cadene的</a>实现，然后通过加载模型转换成我的代码进行微调。</p><p id="bcfb" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated"><strong class="ll ja">训练集</strong></p><p id="d6e3" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">合并数据集后，每个类的图像总数如下所示:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="2676" class="ko kp iq mj b gy mn mo l mp mq">Total - 3661 images<br/><br/><br/>0 - No DR : 1805 images</span><span id="2bf9" class="ko kp iq mj b gy mr mo l mp mq">1 - Mild : 370 images</span><span id="9daf" class="ko kp iq mj b gy mr mo l mp mq">2 - Moderate : 999 images</span><span id="e420" class="ko kp iq mj b gy mr mo l mp mq">3 - Severe : 193 images</span><span id="6e4c" class="ko kp iq mj b gy mr mo l mp mq">4 - Proliferative DR : 295 images</span></pre><p id="56bf" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">实际上，这两个阶层之间有着巨大的差异。我们将使用增强，但无论如何——增强图像可以有所帮助，但不是很多。</p><p id="a268" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">一个简单的代码可以帮助你在解压后对图片进行分类:</p><figure class="me mf mg mh gt ka"><div class="bz fp l di"><div class="oe of l"/></div></figure><h1 id="5e35" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">数据扩充</h1><p id="e299" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">数据集没有那么丰富。我们总共有3661张图片。因此，我将使用PyTorch的数据增强技术，例如:</p><blockquote class="og oh oi"><p id="6882" class="lj lk mx ll b lm ms lo lp lq mt ls lt oj mu lv lw ok mv ly lz ol mw mb mc md ij bi translated"><em class="iq">变换。RandomVerticalFlip(p=0.5)，</em></p><p id="8622" class="lj lk mx ll b lm ms lo lp lq mt ls lt oj mu lv lw ok mv ly lz ol mw mb mc md ij bi translated"><em class="iq">变换花样。RandomRotation((0，360)，center=None)，</em></p><p id="d36b" class="lj lk mx ll b lm ms lo lp lq mt ls lt oj mu lv lw ok mv ly lz ol mw mb mc md ij bi translated"><em class="iq">变换花样。随机水平翻转(p=0.5) </em></p></blockquote><p id="812e" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">以0.5的概率随机垂直翻转图像。<br/>0，359范围内随机旋转(基本给定了一整圈旋转的能力)<br/>以0.5概率随机水平翻转。</p><p id="193f" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">PyTorch数据转换技术完美地工作。到该时期结束时，该模型将在一幅图像上看到另外3个不同的放大。</p><h1 id="64ba" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated"><strong class="ak">数据调整大小</strong></h1><p id="0f8f" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated"><strong class="ll ja"> ResNeXt </strong>正在对<em class="mx"> 299x299 </em>调整大小的图像进行训练。但是，<strong class="ll ja"> PNASNet </strong>需要<em class="mx"> 331x331 </em>输入。因此，我将分别修改代码。</p><h2 id="b139" class="ko kp iq bd kq kr ks dn kt ku kv dp kw kx ky kz la lb lc ld le lf lg lh li iw bi translated"><em class="om">小UX: </em></h2><p id="7e8c" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">如果我们想让这个过程在后台运行，而不是让笔记本电脑整夜开着，我们可以使用<em class="mx"> nohup。</em></p><p id="257d" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">该命令将是:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e0b7" class="ko kp iq mj b gy mn mo l mp mq">nohup python3 train.py &amp;</span></pre><p id="65f4" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">我们可以通过以下方式查看标准输出:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="8b05" class="ko kp iq mj b gy mn mo l mp mq">cat nohup.out</span></pre><p id="e2f7" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">或者，通过以下方式跟踪他们:</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a290" class="ko kp iq mj b gy mn mo l mp mq">tail -f nohup.out</span></pre><p id="d761" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">此外，我真的不想惹ngrok。于是，我用<strong class="ll ja">子进程</strong>函数每隔一段时间下载一次<strong class="ll ja">张量板</strong>输出，刷新一次。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="cc28" class="ko kp iq mj b gy mn mo l mp mq">from time import sleep</span><span id="0513" class="ko kp iq mj b gy mr mo l mp mq">import subprocess</span><span id="1afe" class="ko kp iq mj b gy mr mo l mp mq">for i in range(10000):</span><span id="5a92" class="ko kp iq mj b gy mr mo l mp mq">subprocess.run('scp user@ip_address:~/APTOS/runs/Aug30_21-58-43/* runs/', shell=True)</span><span id="879d" class="ko kp iq mj b gy mr mo l mp mq">sleep(20)</span></pre><h1 id="ffb5" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">开始训练吧。</h1><figure class="me mf mg mh gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi on"><img src="../Images/72b585aebe8f8b98e7c1b507091efe67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XpjBNsES6BezhY-Bi-RPyg.gif"/></div></div></figure><p id="96e6" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">计划:</p><ul class=""><li id="8759" class="my mz iq ll b lm ms lq mt kx na lb nb lf nc md nd ne nf ng bi translated"><strong class="ll ja">用学习率1e-3训练ResNeXt for 3和PNASNet个历元。</strong></li><li id="54bd" class="my mz iq ll b lm ni lq nj kx nk lb nl lf nm md nd ne nf ng bi translated"><strong class="ll ja">以学习率1e-4 </strong>训练ResNeXt和PNASNet个时期</li><li id="18ac" class="my mz iq ll b lm ni lq nj kx nk lb nl lf nm md nd ne nf ng bi translated"><strong class="ll ja">ResNeXt——冻结除3，4层之外的所有层，将lr降低到1e-5，并通过将lr降低10x来逐步解冻块。</strong></li><li id="f5ac" class="my mz iq ll b lm ni lq nj kx nk lb nl lf nm md nd ne nf ng bi translated"><strong class="ll ja">PNAS net——冻结除cell_9、10、11以外的一切，继续1e-5 lr上的训练。之后，逐步解冻细胞，并将lr退火10x。</strong></li></ul><p id="5636" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">为什么？</p><p id="c197" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">在玩这两个模型时，我注意到它在lr <strong class="ll ja"> 1e-3 </strong>的前两个时期产生了有说服力的准确性，但是如果继续使用相同的参数进行训练，它会产生大约相同的准确性和损失。将lr降低10倍有助于提高精度并继续降低损耗。4个历元对于整个模型来说已经足够完美，可以了解数据集并研究足够多的特征。</p><p id="5a82" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">但是，在只留下负责更深层特征的最深层之后，使得模型更加复杂，并且加快了训练过程。换句话说:我给模型一些时间来熟悉数据集并研究它，然后将它集中在数据集最强大的组件上。</p><figure class="me mf mg mh gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi oo"><img src="../Images/cb436310c93a4a0632bb962928db3411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLgnqpdggadO44nfkxdEGw.png"/></div></div></figure><h1 id="5db4" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">第一部分</h1><p id="4146" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">学习率<strong class="ll ja"> 1e-3 </strong>，所有层，2个时期。</p><p id="4b83" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated"><strong class="ll ja"> <em class="mx"> PNASNet 5大型</em> </strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="b53e" class="ko kp iq mj b gy mn mo l mp mq">Epoch 1/50.. Train loss: 3.503.. Test loss: 5.913.. Test accuracy: 0.011 <br/>Epoch 1/50.. Train loss: 1.449.. Test loss: 1.084.. Test accuracy: 0.697 <br/>Epoch 1/50.. Train loss: 0.806.. Test loss: 0.673.. Test accuracy: 0.745 <br/>Epoch 1/50.. Train loss: 0.630.. Test loss: 0.606.. Test accuracy: 0.776 <br/>Epoch 1/50.. Train loss: 0.681.. Test loss: 0.591.. Test accuracy: 0.773 <br/>Epoch 2/50.. Train loss: 0.610.. Test loss: 0.546.. Test accuracy: 0.796 <br/>Epoch 2/50.. Train loss: 0.711.. Test loss: 0.564.. Test accuracy: 0.792 <br/>Epoch 2/50.. Train loss: 0.474.. Test loss: 0.582.. Test accuracy: 0.790 <br/>Epoch 2/50.. Train loss: 0.484.. Test loss: 0.539.. Test accuracy: 0.807 <br/>Epoch 2/50.. Train loss: 0.555.. Test loss: 0.527.. Test accuracy: 0.811 <br/>Epoch 3/50.. Train loss: 0.559.. Test loss: 0.520.. Test accuracy: 0.814 <br/>Epoch 3/50.. Train loss: 0.455.. Test loss: 0.507.. Test accuracy: 0.821 <br/>Epoch 3/50.. Train loss: 0.572.. Test loss: 0.486.. Test accuracy: 0.822 <br/>Epoch 3/50.. Train loss: 0.408.. Test loss: 0.520.. Test accuracy: 0.831 <br/>Epoch 3/50.. Train loss: 0.546.. Test loss: 0.466.. Test accuracy: 0.829</span></pre><p id="298e" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated"><strong class="ll ja"><em class="mx">ResNeXt 50 32x4d</em></strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e5d4" class="ko kp iq mj b gy mn mo l mp mq">Epoch 1/50.. Train loss: 2.787.. Test loss: 3.990.. Test accuracy: 0.254 <br/>Epoch 1/50.. Train loss: 1.460.. Test loss: 0.893.. Test accuracy: 0.684 <br/>Epoch 1/50.. Train loss: 0.722.. Test loss: 0.654.. Test accuracy: 0.738 <br/>Epoch 2/50.. Train loss: 0.817.. Test loss: 0.640.. Test accuracy: 0.771 <br/>Epoch 2/50.. Train loss: 0.590.. Test loss: 0.533.. Test accuracy: 0.807 <br/>Epoch 2/50.. Train loss: 0.517.. Test loss: 0.496.. Test accuracy: 0.820 <br/>Epoch 3/50.. Train loss: 0.579.. Test loss: 0.559.. Test accuracy: 0.772 <br/>Epoch 3/50.. Train loss: 0.470.. Test loss: 0.493.. Test accuracy: 0.809 <br/>Epoch 3/50.. Train loss: 0.512.. Test loss: 0.477.. Test accuracy: 0.836</span></pre><h1 id="fc8e" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">第二部分</h1><p id="4d96" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">所有层</p><p id="fef7" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated"><strong class="ll ja"> <em class="mx"> PNASNet 5大型</em> </strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="9fed" class="ko kp iq mj b gy mn mo l mp mq">Epoch 1/50.. Train loss: 0.472.. Test loss: 0.402.. Test accuracy: 0.848 <br/>Epoch 1/50.. Train loss: 0.452.. Test loss: 0.370.. Test accuracy: 0.857 <br/>Epoch 1/50.. Train loss: 0.474.. Test loss: 0.383.. Test accuracy: 0.840 <br/>Epoch 1/50.. Train loss: 0.402.. Test loss: 0.383.. Test accuracy: 0.855 <br/>Epoch 2/50.. Train loss: 0.552.. Test loss: 0.398.. Test accuracy: 0.848 <br/>Epoch 2/50.. Train loss: 0.453.. Test loss: 0.378.. Test accuracy: 0.867 <br/>Epoch 2/50.. Train loss: 0.431.. Test loss: 0.391.. Test accuracy: 0.846 <br/>Epoch 2/50.. Train loss: 0.307.. Test loss: 0.379.. Test accuracy: 0.857 <br/>Epoch 3/50.. Train loss: 0.453.. Test loss: 0.378.. Test accuracy: 0.855 <br/>Epoch 3/50.. Train loss: 0.372.. Test loss: 0.376.. Test accuracy: 0.851 <br/>Epoch 3/50.. Train loss: 0.429.. Test loss: 0.380.. Test accuracy: 0.862 <br/>Epoch 3/50.. Train loss: 0.408.. Test loss: 0.385.. Test accuracy: 0.855</span></pre><p id="d788" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated"><strong class="ll ja">T5】ResNeXt 50T7】</strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d66f" class="ko kp iq mj b gy mn mo l mp mq">Epoch 1/50.. Train loss: 0.230.. Test loss: 0.423.. Test accuracy: 0.837 <br/>Epoch 1/50.. Train loss: 0.454.. Test loss: 0.416.. Test accuracy: 0.846 <br/>Epoch 1/50.. Train loss: 0.462.. Test loss: 0.410.. Test accuracy: 0.841 <br/>Epoch 2/50.. Train loss: 0.458.. Test loss: 0.416.. Test accuracy: 0.837 <br/>Epoch 2/50.. Train loss: 0.437.. Test loss: 0.386.. Test accuracy: 0.854 <br/>Epoch 2/50.. Train loss: 0.407.. Test loss: 0.406.. Test accuracy: 0.846 <br/>Epoch 3/50.. Train loss: 0.425.. Test loss: 0.401.. Test accuracy: 0.845 <br/>Epoch 3/50.. Train loss: 0.424.. Test loss: 0.394.. Test accuracy: 0.836<br/>Epoch 3/50.. Train loss: 0.552.. Test loss: 0.398.. Test accuracy: 0.846</span></pre><h1 id="6722" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">冻结和解冻图层并调整lr后</h1><p id="ce91" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated"><strong class="ll ja"> <em class="mx"> PNASNet 5大型</em> </strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="a214" class="ko kp iq mj b gy mn mo l mp mq">Epoch 1/50.. Train loss: 0.220.. Test loss: 0.344.. Test accuracy: 0.884 <br/>Epoch 1/50.. Train loss: 0.477.. Test loss: 0.361.. Test accuracy: 0.873 <br/>Epoch 1/50.. Train loss: 0.428.. Test loss: 0.345.. Test accuracy: 0.881 <br/>Epoch 1/50.. Train loss: 0.410.. Test loss: 0.358.. Test accuracy: 0.872 <br/>Epoch 1/50.. Train loss: 0.403.. Test loss: 0.356.. Test accuracy: 0.874 <br/>Epoch 1/50.. Train loss: 0.414.. Test loss: 0.332.. Test accuracy: 0.884 <br/></span></pre><p id="caab" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated"><strong class="ll ja"> <em class="mx"> ResNeXt 50 </em> </strong></p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="5d05" class="ko kp iq mj b gy mn mo l mp mq">Epoch 1/50.. Train loss: 0.176.. Test loss: 0.354.. Test accuracy: 0.876 <br/>Epoch 1/50.. Train loss: 0.332.. Test loss: 0.370.. Test accuracy: 0.865 <br/>Epoch 2/50.. Train loss: 0.401.. Test loss: 0.361.. Test accuracy: 0.865 <br/>Epoch 2/50.. Train loss: 0.376.. Test loss: 0.366.. Test accuracy: 0.866 <br/>Epoch 2/50.. Train loss: 0.342.. Test loss: 0.354.. Test accuracy: 0.870 <br/>Epoch 3/50.. Train loss: 0.399.. Test loss: 0.372.. Test accuracy: 0.870 <br/>Epoch 3/50.. Train loss: 0.330.. Test loss: 0.349.. Test accuracy: 0.875</span></pre><h1 id="b3c6" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">结果</h1><p id="e792" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">我们在<strong class="ll ja"> ResNeXt 50 </strong>上的<strong class="ll ja"><em class="mx"/></strong>准确率为87.5%，在 PNASNet 5上的<strong class="ll ja"> <em class="mx">准确率为88.4%。</em></strong></p><p id="ed8c" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">基本上，我已经尝试了SOTA，平庸，最后，两个顶级架构的图像分类。而且，我们知道，这些模型在现实生活中的工作方式与它们在测试/验证程序中显示的方式并不完全相同。</p><p id="72e0" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">总之，一夜之间调整两个模型很有趣，最后—我将对实际测试(提交)集进行预测，我们将看到结果。</p><h1 id="3d20" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">预言；预测；预告</h1><p id="99f7" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">我对预测部分的推断。</p><figure class="me mf mg mh gt ka"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="8bee" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">如果我们使用<a class="ae nh" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>进行预测，我们应该注意，有时<strong class="ll ja"> tqdm </strong>并不是一个很好的选择，只要它为每个输出刷新stdout，页面就会崩溃。我总是首先尝试使用tqdm，如果效果不好，就在循环中删除它，并编写我的脚本版本来查看预测过程。我们应该确保删除*。csv文件，否则它会在其中追加新的预测。</p><h1 id="9b78" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">卡格尔</h1><figure class="me mf mg mh gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi op"><img src="../Images/f6f96c14ee0287b2d14e67364efeec37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7nSbUCdEHnjr48TsKW3NoA.png"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk translated"><a class="ae nh" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com</a></figcaption></figure><p id="db9d" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">而试图在预测部分后进行投稿时，<a class="ae nh" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ll ja"> kaggle </strong> </a>让我怒不可遏。基本上，他们在内核中有一个bug，每次我试图提交预测时都会抛出提交错误。在搜索了一段时间后，我看到了一个卡格勒的评论，它实际上帮助了我。</p><p id="7d10" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">另外，在内核中关闭GPU和互联网也很有帮助，除了在Kaggle上将Python docker image降级到1-7版本。</p><p id="3e86" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">我要借用他们的代码提交部分:</p><figure class="me mf mg mh gt ka"><div class="bz fp l di"><div class="oe of l"/></div><figcaption class="oq or gj gh gi os ot bd b be z dk translated">摘自https://www.kaggle.com/kinnachen对<a class="ae nh" href="https://www.kaggle.com/kinnachen" rel="noopener ugc nofollow" target="_blank">的评论</a></figcaption></figure><h1 id="98c7" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">最终结果:</h1><p id="bde3" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated"><strong class="ll ja"> <em class="mx"> ResNeXt 50 </em> </strong></p><figure class="me mf mg mh gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ou"><img src="../Images/0df5b52fd352cd4438035d222a4fd043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XCSSStDggg5WIA6ESVZTA.png"/></div></div></figure><p id="53b7" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated"><strong class="ll ja"> <em class="mx"> PNASNet 5 </em> </strong></p><figure class="me mf mg mh gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ov"><img src="../Images/bb2555e64c0db0628069e62488fefbb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ux51jCAY5V-W6yM0LOQ41A.png"/></div></div></figure><p id="dfb4" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">当我检查PNASNet工作如此糟糕的原因时，我注意到许多0和2，几个1作为提交的预测数字。并且绝对没有4或3 .<br/>NASNet在0和2类上过度拟合，只要他们持有大部分数据，而在其他类上做出很差的预测，或者根本没有做出预测。</p><p id="5fbd" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">此外，ResNeXt 50仅在夜间调谐时表现良好。</p><p id="f929" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">离挑战结束还有3天，我打算一有时间就尝试更多有前途的方法。</p><p id="fe90" class="pw-post-body-paragraph lj lk iq ll b lm ms lo lp lq mt ls lt kx mu lv lw lb mv ly lz lf mw mb mc md ij bi translated">希望你喜欢它！</p><h1 id="ed69" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated"><strong class="ak">更新:</strong></h1><p id="0b8e" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">通过裁剪数据集，并训练最后一层更长的时间，我取得了4.1%的改善。</p><figure class="me mf mg mh gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ow"><img src="../Images/e8ab4737e87d626eeefc485cbdb27df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qytF_9NMysNRLHM_MuJWAg.png"/></div></div></figure><h1 id="5744" class="nn kp iq bd kq no np nq kt nr ns nt kw nu nv nw la nx ny nz le oa ob oc li od bi translated">第二次更新</h1><p id="3581" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt kx lu lv lw lb lx ly lz lf ma mb mc md ij bi translated">平均颜色减法给出了2.0%的改善。</p><figure class="me mf mg mh gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ox"><img src="../Images/a9133869d076edd8e5b341e0bf4cbc0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V0qlo2HOy53gCpV811GcMQ.png"/></div></div></figure></div></div>    
</body>
</html>