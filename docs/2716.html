<html>
<head>
<title>Reinforcement Learning: Monte-Carlo Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:蒙特卡罗学习</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/reinforcement-learning-monte-carlo-learning-dc9b49aa16bd?source=collection_archive---------0-----------------------#2022-04-29">https://pub.towardsai.net/reinforcement-learning-monte-carlo-learning-dc9b49aa16bd?source=collection_archive---------0-----------------------#2022-04-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="515e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">无模型学习。</h2></div><blockquote class="kf kg kh"><p id="7b41" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在前三篇博客中，我们讨论了如何将RL问题公式化为MDP，并使用动态编程(值迭代和策略迭代)来解决它们。到目前为止，我们所看到的一切都与我们完全了解的环境有关，如转移概率、奖励函数等。一个自然的问题是，<strong class="kl ir">在我们没有这些函数和概率的环境中会怎么样？</strong>这就是被称为无模型学习的另一类强化学习的用武之地。在这篇博客中，我们将学习一种叫做<strong class="kl ir">蒙特卡罗方法</strong>的无模型算法。<em class="iq">那么，在我们开始之前，让我们先来看看我们将要</em> <strong class="kl ir"> <em class="iq"> </em> </strong>谈的<em class="iq">这个故事里有什么:</em></p></blockquote><ul class=""><li id="b2ea" class="lf lg iq kl b km kn kp kq lh li lj lk ll lm le ln lo lp lq bi translated"><em class="kk">基本</em> <strong class="kl ir"> <em class="kk">直觉</em> </strong> <em class="kk">蒙特卡罗</em></li><li id="fbb8" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated"><em class="kk">计算</em> <strong class="kl ir"> <em class="kk">意为</em> </strong> <em class="kk">即</em> <strong class="kl ir"> <em class="kk">增量意为</em> </strong></li><li id="0efa" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated">我们如何在无模型环境中评估策略？( <strong class="kl ir"> <em class="kk">政策评估</em> </strong> <em class="kk"> ) </em></li><li id="e76e" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated"><em class="kk"/><strong class="kl ir"><em class="kk">探索</em></strong><em class="kk">……</em></li><li id="4187" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated"><strong class="kl ir"> <em class="kk">政策上</em> </strong> <em class="kk">改进上</em> <strong class="kl ir"> <em class="kk">控制上</em> </strong> <em class="kk"> ) </em></li><li id="0d85" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated"><em class="kk">什么是</em><strong class="kl ir"><em class="kk">GLIE</em></strong><em class="kk">蒙特卡罗？</em></li><li id="9272" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated"><em class="kk">重要采样</em></li><li id="9ef4" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated"><strong class="kl ir"> <em class="kk">关-政策</em> </strong> <em class="kk">政策改进(</em> <strong class="kl ir"> <em class="kk">控制</em> </strong> <em class="kk"> ) </em></li></ul><p id="e4c0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">这将是一个漫长的旅程，所以给自己弄杯咖啡，坐好！☕🐱‍🏍</p><h2 id="1a5d" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated">蒙特卡罗的基本直觉</h2><p id="3a95" class="pw-post-body-paragraph ki kj iq kl b km mp jr ko kp mq ju kr lh mr ku kv lj ms ky kz ll mt lc ld le ij bi translated">蒙特卡罗(MC)方法涉及从<strong class="kl ir">经验</strong>中学习。那是什么意思？它意味着通过一系列的状态、行动和奖励来学习。假设，我们的代理处于状态s1，采取行动a1，获得奖励r1，然后转移到状态s2。这整个过程是一种体验。</p><p id="0ddb" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">现在，我们知道贝尔曼方程将一个状态的价值描述为直接回报加上下一个状态的价值。至于我们的例子，我们不知道模型动力学(转移概率)，所以我们不能计算这个。但是，如果我们看着我们对环境动力学一无所知的环境，问问我们自己，我们对环境还知道(或能知道)什么是<strong class="kl ir">回报</strong>。如果我们取这些奖励的平均值，并重复这个过程无限次，那么我们可以估计出接近实际值的状态值。<strong class="kl ir"> <em class="kk">引人注目！</em> </strong>蒙特卡洛就是从这些序列中获得的平均奖励中学习。<strong class="kl ir">所以，说起来… </strong></p><blockquote class="kf kg kh"><p id="933b" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">蒙特卡洛包括通过<strong class="kl ir">从环境中取样</strong>奖励和<strong class="kl ir">对获得的奖励进行平均</strong>来学习。每一集，我们的经纪人都会采取行动并获得奖励。当每一集(经历)结束时，我们取平均值。</p></blockquote><p id="885a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">这就产生了一个问题，<em class="kk">我们是不是在一集里每次去那个州的时候都平均了一下奖励？好的是和否</em> <strong class="kl ir"> <em class="kk">解释</em> </strong> <em class="kk">:在一集里，如果我们只对第一次访问状态</em> <strong class="kl ir"> <em class="kk"> s </em> </strong> <em class="kk">后获得的奖励进行平均，那么它就叫做</em> <strong class="kl ir"> <em class="kk">首次访问蒙特卡洛，</em> </strong> <em class="kk">如果在一集里，我们对每次状态</em> <strong class="kl ir"> <em class="kk"> s </em>获得的奖励进行平均在这个博客中，我们将主要讨论初诊MC。</strong></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2c679620c358b1c0dd50b94487d5f7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*dehxsf1VU4nhRtAcbUdD1Q.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">参考文献1</figcaption></figure><p id="9620" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">这就描述了MC的症结所在。但是它留下了许多没有回答的问题，比如我们如何确保所有的州都被访问过？或者说如何高效的计算均值？在剩下的博客中，我们将继续这个想法。让我们学习一个这样的想法，即<strong class="kl ir"> <em class="kk">增量的意思是</em> </strong>。</p><h2 id="19e6" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated">增量方法</h2><p id="ca8d" class="pw-post-body-paragraph ki kj iq kl b km mp jr ko kp mq ju kr lh mr ku kv lj ms ky kz ll mt lc ld le ij bi translated">如前所述，为了计算州的价值函数，我们对访问该州后获得的奖励进行平均。它是这样描述的:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/2dc6e85974357115279528c84ed813c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J6sSQHGegI0huc50CLaCkQ.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">价值函数计算</figcaption></figure><p id="a065" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">你可能会问，如果我们只在一集里第一次访问状态<strong class="kl ir"> s </strong>时计算奖励，那么计数器N将总是1。不，因为这个计数器在每一集都帮助我们去过的州。</p><p id="c920" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">展望未来，我们可能希望计算奖励并逐步平均，而不是一次性将所有奖励相加并进行分配。<strong class="kl ir"> <em class="kk">为什么？因为保存所有奖励的总和然后除以该状态被访问的总次数在计算上是昂贵的，并且随着遇到更多的奖励，这种需求将会增长。每次遇到新的状态/奖励时，我们都必须分配更多的内存。</em> </strong>因此，这里我们将求导定义为增量方式。</p><blockquote class="kf kg kh"><p id="fcd3" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">增量均值:也称为移动平均线，它以增量方式保持均值，即<strong class="kl ir">一旦遇到新的回报就计算均值</strong>。为了计算平均值，我们不需要等到遇到每一个奖励，然后用它除以该州被访问的总次数。我们在网上计算。</p></blockquote><p id="b11d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">让我们一步一步地推导出增量方法:</p><p id="1f78" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">▹n个时间步长的总平均值计算如下:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/4187e0728f2f3512eb406d0e32befd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*TI-NWnuTV6gbIqqtht105w.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">总均值/平均值</figcaption></figure><p id="6568" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">▹，假设，U(n)是我们要计算的新平均值。然后，这个U(N)可以被分解为之前的平均值直到<strong class="kl ir"> N-1 </strong>和在<strong class="kl ir"> N </strong>时间步长获得的值的和。可以写成:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/dfda6a2d51aec3d99ed63df45b4bc0c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*JGbhYYHxfEJKzMw98IBjrQ.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">新平均值=总平均值(N-1)+N时间步长的值</figcaption></figure><p id="6f22" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">▹现在来看求和项，它可以被定义为(<strong class="kl ir"> N-1 </strong>)乘以直到N-1所获得的平均值，即U(N-1)。将它代入上式:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/207c168dc088013306ac8ed519cfebef.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*sJ_-2UYFyhR3Bj4urb1WjQ.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">取代</figcaption></figure><p id="2b0d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">▹Rearranging这些术语:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c4a05335144a2aaeefee632d6f58d105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*SfwTp490bMz0L70OQsQjyA.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">打开支架</figcaption></figure><p id="2706" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">▹Taking N常见并进一步求解:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi np"><img src="../Images/6c27a401e09a466271a3b2c7a0bc00bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*rVy-ojJmMGH5zwZAU-wjaw.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">增量平均值的最终等式</figcaption></figure><p id="13ab" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated"><strong class="kl ir"> <em class="kk">这个等式告诉我们什么？</em> </strong></p><p id="a3b6" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">它告诉我们，新的平均值(<strong class="kl ir"> U(N) </strong>)可以被定义为平均值(<strong class="kl ir"> U(N-1) </strong>)的<strong class="kl ir">先前估计</strong>与当前<strong class="kl ir">获得的</strong>回报(<strong class="kl ir"> X(N) </strong>)和按某个步长加权的平均值(<strong class="kl ir"> U(N-1) </strong>)的<strong class="kl ir">先前估计</strong>之间的差之和当前获得的回报(<strong class="kl ir"> X(N) </strong>)和之前估计的平均值(U(N-1))之间的差异被称为<strong class="kl ir"> <em class="kk">误差项</em> </strong>，这意味着我们只是在这个误差项的方向上估计我们的新平均值。需要注意的是，我们并没有精确地计算新的平均值，而只是将新的平均值推向真实平均值的方向。</p><p id="86ae" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">这概括了增量方法。😤现在，让我们更深入地了解我们如何评估/预测MC中的策略。</p><h2 id="d72d" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated">无模型环境中的策略评估(适用于MC)</h2><p id="108f" class="pw-post-body-paragraph ki kj iq kl b km mp jr ko kp mq ju kr lh mr ku kv lj ms ky kz ll mt lc ld le ij bi translated">策略评估如我们上面讨论的那样完成。下面就来细说一下。回想一下，价值函数是通过平均每集获得的奖励来计算的。因此，下一个问题将是我们如何从这个<strong class="kl ir">价值函数</strong>中<strong class="kl ir">构建/提取</strong>一个策略。让我们快速地看一下政策是如何从状态-值函数和状态-动作值(Q-函数)构建的。</p><p id="4626" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">数学上，策略可以从状态值函数中提取:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/491ef4b0bbfc543d84bf049c4d301c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*JuOWDlFwizu-GGBPF0aYYQ.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">来自国家价值函数的政策</figcaption></figure><p id="fe8b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">我们可以看到，为了从状态值函数中提取策略，我们最大化了状态值。但是这个等式有两个问题。首先，在无模型环境中，我们不知道转移概率(<strong class="kl ir"> Pss' </strong>)，其次，我们也不知道<strong class="kl ir"> s' </strong>。</p><p id="fe19" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">现在，让我们看看状态-动作函数。该策略被提取为:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b83678422b13ebbc369b48b1722dd545.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*uMhZ9Llqevpc5I6tc-Aoxg.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">国家行为功能的政策</figcaption></figure><p id="9469" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">如我们所见，我们只需要最大化Q值来提取策略，即我们最大化动作并选择给出最佳Q值(状态-动作值)的动作。因此，对于MC，我们使用<strong class="kl ir"> <em class="kk">状态-动作值</em> </strong>进行策略评估和改进步骤。</p><p id="0fe1" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated"><strong class="kl ir"> <em class="kk">我们如何计算状态-动作值？</em> </strong>嗯，<strong class="kl ir"> <em class="kk"> </em> </strong>我们计算动作值的方法与计算状态值的方法相同，唯一不同的是，现在我们为每个状态-动作对(s，a)计算动作值<strong class="kl ir">，而不仅仅是状态s。因此，只有当状态<strong class="kl ir"> s </strong>被访问并且动作<strong class="kl ir"> a </strong>被从中取出时，才称状态-动作对(s，a)被访问。如前一节所述，我们对动作值进行平均，这就给出了该状态的状态-动作值。如果我们仅在每集第一次访问(s，a)对之后求平均，那么它被称为<strong class="kl ir">首次访问MC </strong>，如果我们在每集对它的每次访问之后求平均超过(s，a ),那么它被称为<strong class="kl ir">每次访问MC </strong>。</strong></p><p id="d0b9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">因此，对国家行动值进行平均给了我们政策评估和改进的机制。但是它也有一个缺点就是<strong class="kl ir">探索的问题</strong>。让我们也处理一下。</p><h2 id="affc" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated">探索的问题</h2><p id="97cd" class="pw-post-body-paragraph ki kj iq kl b km mp jr ko kp mq ju kr lh mr ku kv lj ms ky kz ll mt lc ld le ij bi translated">对于策略评估步骤，我们运行一系列事件，对于每个事件，我们计算遇到的每个(s，a)对的状态-动作值，然后取平均值，这就定义了(s，a)对的值。然后，当我们的政策改进步骤w.r.t .转向国家行动价值观时，我们表现得贪婪。<strong class="kl ir"> <em class="kk">一切看起来都不错但是等着吧！</em> </strong>如果我们把贪婪作为我们的政策改进步骤，那么将会有许多我们可能永远不会去拜访的国家行动对。如果我们不访问它们，那么我们就没有东西可以平均，它们的状态值将会是0。这意味着我们可能永远不会知道那对国家行动组合有多好。这是我们将在本届会议上努力解决的问题。<strong class="kl ir"> <em class="kk">什么样的政策适合MC？</em>T11】</strong></p><p id="3ee9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">嗯，我们想要一个通常为<strong class="kl ir">随机</strong> π(a|s) &gt; 0的策略，这意味着它有非零概率为所有状态选择任何行动，然后在接近最优时逐渐变为确定性策略。最初，随机的并且所有动作的π(a|s) &gt;为0将确保所有动作在确保探索的状态中有相等的机会被选择，然后逐渐转移到确定性策略，因为<strong class="kl ir">最优</strong>策略将总是确定性策略，其在一个状态中采取所有动作中可能的最佳动作。这个策略确保我们继续访问不同的状态-动作对。</p><p id="3ec9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">这可以使用两种方法来完成，即政策内方法和政策外方法。这两者都将在下面的章节中讨论。</p><h1 id="8d58" class="ns lx iq bd ly nt nu nv mb nw nx ny me jw nz jx mh jz oa ka mk kc ob kd mn oc bi translated">蒙特卡洛控制</h1><p id="8e55" class="pw-post-body-paragraph ki kj iq kl b km mp jr ko kp mq ju kr lh mr ku kv lj ms ky kz ll mt lc ld le ij bi translated">MC控制类似于我们看到的使用动态规划解决马尔可夫决策过程(这里)。我们遵循<strong class="kl ir">广义策略迭代</strong> (GPI)的相同思想，其中我们不直接尝试估计最优策略，而是保持对动作值函数和策略的估计。然后，我们不断推动行动价值函数，使其更好地逼近政策，反之亦然(见之前的博客)。这个想法可以表述为:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi od"><img src="../Images/6f8531d90c7fbe52800cd3cf99b7fdc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*obtQKHgLFxBs72pYdjIfAg.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">面向大规模定制的控制思想</figcaption></figure><p id="fd69" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">在MC中，我们从某个策略<strong class="kl ir"> π </strong>开始，用它来计算动作值。一旦计算了动作值(策略评估),则相对于这些动作值(控制)贪婪地行动，以构造新的策略π*，其优于或等于初始策略π。在这两个步骤之间摇摆最终会产生一个最优策略。</p><h2 id="f659" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated">政策控制</h2><p id="ac66" class="pw-post-body-paragraph ki kj iq kl b km mp jr ko kp mq ju kr lh mr ku kv lj ms ky kz ll mt lc ld le ij bi translated">在基于策略的方法中，我们改进了用于计算Q函数的相同策略。直观地，假设我们使用策略π评估状态s的状态动作值(Q函数),那么在策略控制中，我们将改进这个相同的策略π。在基于策略的方法中使用的策略是ε贪婪策略。那么，<strong class="kl ir"> <em class="kk">什么是ε-贪婪策略？</em>T3】</strong></p><p id="a808" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">如前一节所述，MC的理想策略是π(a|s) &gt; 0，这种策略称为<strong class="kl ir">ε-软策略</strong>。on-policy中使用的策略称为ϵ<strong class="kl ir">-贪婪策略</strong>。这种类型的策略具有以概率<strong class="kl ir"> 1- ϵ + ϵ/A(s) </strong>选择<strong class="kl ir">贪婪</strong>动作的概率，以及以概率<strong class="kl ir"> ϵ/A(s) </strong>采取<strong class="kl ir">随机动作</strong>的概率，其中A(s)是可以从该状态采取的动作总数。它可以表示为:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/008886823e2daf770a6b2af688a69f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*xGdzg8e7X61SRAPUBHjQtw.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">ε-贪婪政策。</figcaption></figure><p id="445e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">对于<strong class="kl ir"> ϵ-greedy政策</strong>，每个行动都有<strong class="kl ir"> π(a|s) &gt; ϵ/A(s) </strong>的被选中概率。ϵ-greedy策略在大多数时候选择贪婪的行为，但也有极小的概率选择完全随机的行为。这就解决了我们探索的问题。</p><p id="df81" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">现在，我们可以通过表现贪婪来改进策略，即通过采取对一个状态具有最大q值的动作。这给定为(我们之前看到过):</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b83678422b13ebbc369b48b1722dd545.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*uMhZ9Llqevpc5I6tc-Aoxg.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">政策改进步骤</figcaption></figure><p id="0f63" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">按照GPI的思想，评估和改进步骤一直进行到达到最优，如下所示。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4926df3d36535897e082e541f80f0e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*21SSDKVzIWa2fz1ZWAARCg.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">评估和改进的循环。</figcaption></figure><p id="aa9b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">但是回想一下，GPI不一定要求对一个完整的过程进行策略评估，相反，我们可以对一个事件评估一个策略，然后对它表现出贪婪的行为。它只需要将行动值和策略稍微推向最佳行动值和最佳策略，如下所示:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi og"><img src="../Images/98185888c0f5ba30748896b77bab80e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*7vepjotTtPoNtfYeUoJUfw.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">政策评估与改进的GPI理念。</figcaption></figure><p id="c5fd" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">结合我们刚刚看到的所有机制，我们可以提出一个名为<strong class="kl ir">无限探索极限中的贪婪(GLIE)的策略算法。</strong></p><h2 id="d56c" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated"><strong class="ak">贪婪于极限与无限探索(GLIE) </strong></h2><p id="42f9" class="pw-post-body-paragraph ki kj iq kl b km mp jr ko kp mq ju kr lh mr ku kv lj ms ky kz ll mt lc ld le ij bi translated">到目前为止，一切都是为了这个算法。如前所述，MC政策的理想制定是:</p><ol class=""><li id="04b7" class="lf lg iq kl b km kn kp kq lh li lj lk ll lm le oh lo lp lq bi translated">我们不断探索，这意味着每个状态-动作对都被访问了无数次。</li><li id="2f13" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le oh lo lp lq bi translated">最终，后面的步骤转向更贪婪的策略，即确定性策略。</li></ol><p id="e2d7" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">通过在ϵ-greedy政策中为ϵ设定正确的价值观来确保这一点。那么，对ϵ来说，什么是正确的价值观呢？ </p><blockquote class="kf kg kh"><p id="5451" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">初始策略通常是一些随机初始化的值，以确保我们探索更多，如果我们保持ϵ~1 的<strong class="kl ir">值，就可以确保这一点。在后面的时间步骤中，我们需要一个更加确定的策略，这可以通过设置<strong class="kl ir"> ϵ~0 </strong>来确保。</strong></p></blockquote><p id="2a54" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">【glie mc如何设置ϵ值？</p><p id="3f38" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">在GLIE MC中，ϵ值逐渐<strong class="kl ir">衰减</strong>，这意味着ε的值随着每个时间步长变得越来越小。通常，做法是用ε值= 1.0，然后慢慢地<strong class="kl ir">衰减</strong>ε值一些<strong class="kl ir">小的量</strong>。ϵ的值为0将意味着一个贪婪的政策被遵循。因此，在GLIE蒙特卡罗方法中，ϵ值随着时间的推移逐渐降低，以利于开采而不是勘探。</p><p id="c7d8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">接下来，我们研究<strong class="kl ir">重要抽样</strong>，它是不符合政策的MC的<strong class="kl ir">症结</strong>。</p><h2 id="74ff" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated">重要抽样</h2><p id="7622" class="pw-post-body-paragraph ki kj iq kl b km mp jr ko kp mq ju kr lh mr ku kv lj ms ky kz ll mt lc ld le ij bi translated">在非策略中，我们有两个策略，即<strong class="kl ir">行为策略</strong>和<strong class="kl ir">目标策略</strong>。行为策略用于探索环境。它通常遵循探索性政策。目标策略是我们想要通过学习基于行为策略的的价值函数<strong class="kl ir">来改进为最优策略的策略。因此，目标是<strong class="kl ir">通过计算从来自<strong class="kl ir">行为策略分布</strong><strong class="kl ir">【b(a/s)】</strong>的样本中导出的值函数来学习<strong class="kl ir">目标策略分布</strong><strong class="kl ir">【π(a/s)】</strong>。重要的采样<strong class="kl ir">在这方面对</strong>有所帮助。详细解释:</strong></strong></p><p id="b12c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">假设我们要估计<strong class="kl ir">随机变量x </strong>的期望值其中X是从行为策略分布<strong class="kl ir"> b [x ~b] </strong> w.r.t采样到目标策略分布π即Eπ[X]。我们知道:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/dce3c302cd1ba31cbad998eaa93383f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*cGcgEI4EY95hZYxU62YgCg.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">X的期望值= X的值乘以X的概率</figcaption></figure><p id="f118" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">除以并乘以行为分布b(x):</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c430db4d64530aa3cac3a2900b2cc33f.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*7iybAPe2Sv40sIHuEAq67A.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">等式2</figcaption></figure><p id="d961" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">重新安排条款:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/82bb5e3b2471541c8f55abc5db6581db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*-F2M6Kl17ZCOdm507I6SWQ.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">等式3。</figcaption></figure><p id="4602" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">在等式3中，<strong class="kl ir">x ~目标</strong>策略分布与<strong class="kl ir">x ~行为</strong>策略分布的比值称为<strong class="kl ir">重要抽样比</strong>，用<strong class="kl ir"> p </strong>表示。现在，π的期望值来自b。我们可以通过两种方式使用重要抽样来计算目标策略的状态值函数，即<strong class="kl ir"> <em class="kk">普通重要抽样</em> </strong>和<strong class="kl ir"> <em class="kk">加权重要抽样</em> </strong>。</p><p id="5e1a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">普通的重要采样跟踪状态被访问的时间步长，用<strong class="kl ir"> T(s) </strong>表示。Vπ是缩放从<strong class="kl ir">行为策略</strong>获得的回报，然后除以<strong class="kl ir"> T(s) </strong>的结果。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4e7045a8ce7cb6c4a7a3fcb3843b2015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*qAXsWhW8Z7ida82QK1htyg.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">情商。8.普通重要抽样</figcaption></figure><p id="904f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">另一方面，加权重要抽样被定义为<strong class="kl ir">加权平均值</strong>，如下所示:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi of"><img src="../Images/a555d4afb807c88030086ea0ca16ab78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*AyTTzOmlv_h4j3t1NFyghw.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">情商。9.加权重要抽样</figcaption></figure><p id="b77a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">普通重要抽样<strong class="kl ir">无偏</strong>，而加权重要抽样<strong class="kl ir">有偏</strong>。<strong class="kl ir">这是什么意思？</strong>好吧，假设我们只对从一个状态返回的<strong class="kl ir">单个</strong>采样，那么在<strong class="kl ir">加权</strong>重要采样中<strong class="kl ir">分子</strong>和<strong class="kl ir">分母</strong>中的比率项将会抵消。因此，估计值Vπ将是行为策略b的回报，没有任何缩放。这意味着它的期望是<strong class="kl ir">不是Vπ </strong>而是<strong class="kl ir"> Vb </strong>。另一方面，普通重要抽样的方差<strong class="kl ir">是无界的</strong>，而加权重要抽样则不是这种情况。这总结了重要的抽样。</p><h2 id="e71f" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated">非政策蒙特卡罗</h2><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi om"><img src="../Images/df242f51f970862881c60df6419f4c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*Io4AjpNyF8Xw0mBYr1z2pg.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">参考文献3</figcaption></figure><p id="8f68" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">如前所述，我们希望从行为策略中评估目标策略。像在MC中一样，我们通过平均回报来估计状态的值。我们将在这里做同样的事情。我们通过估计回报来计算价值函数，如下所示:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e2fdcbec4b10e1f66e722e87ff957d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*TriC8GaK4m7NaYTp8Yip6A.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">Eq 4。不符合政策的价值函数</figcaption></figure><p id="d655" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">其中p是重要抽样比，定义为π <strong class="kl ir">下某<strong class="kl ir">轨迹</strong>的<strong class="kl ir">概率</strong>除以b下某<strong class="kl ir">轨迹</strong>的概率。回想一下，策略π下的轨迹定义为在状态<strong class="kl ir"> s </strong>采取行动<strong class="kl ir"> a </strong>的概率乘以如果<strong class="kl ir"> a </strong>和<strong class="kl ir">访问<strong class="kl ir">s’</strong>的概率它被定义为:</strong></strong></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/5dce23296ddcef2aedf505d629fa592d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*UohycRw3B0-6W9lusql2Bg.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">轨迹概率</figcaption></figure><p id="923e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">我们可以根据重要抽样定义相同的轨迹概率如下:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi op"><img src="../Images/77cbe3069e1b4030256e472af91b145a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ec6IeFI8_As0dxbgL5uPUQ.jpeg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">Eq 6。轨迹的重要采样率</figcaption></figure><p id="e047" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">来自eq。6很明显，一个重要的采样比被定义为目标策略π下轨迹的概率除以行为策略b下轨迹的概率。我们还可以推断出<strong class="kl ir">转移</strong>概率项<strong class="kl ir">取消</strong>，留下依赖于两个<strong class="kl ir">策略</strong>的比率。</p><p id="8b71" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated"><strong class="kl ir"> <em class="kk">这个比例用在哪里？</em>T55】</strong></p><p id="2273" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">因为我们试图估算Vπ。我们使用这个比率来转换从行为策略b获得的回报，以便估计<strong class="kl ir">目标</strong>策略π的Vπ。如下所示:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d3f4aab62494c7d8e577bff10c125900.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*PKTwj4yRD_NDmBen6qQ12g.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">Eq 7。重要抽样比的使用。</figcaption></figure><p id="2d61" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">我们现在准备定义非策略控制。</p><h2 id="7ebf" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated">非策略蒙特卡罗控制</h2><p id="7b81" class="pw-post-body-paragraph ki kj iq kl b km mp jr ko kp mq ju kr lh mr ku kv lj ms ky kz ll mt lc ld le ij bi translated">回想一下，我们的目的是从行为策略获得的回报中改进目标策略。这两项政策完全<strong class="kl ir">无关</strong>。虽然目标策略可以是确定性的，即它可以根据q函数贪婪地行动，但是我们希望我们的<strong class="kl ir">行为</strong>策略是探索性的。为了确保这一点，我们需要确保从关于每个状态-动作对的行为策略中获得足够的<strong class="kl ir">返回</strong>。行为策略通常是软的，这意味着每个状态-动作对都有一个被选中的非零概率<strong class="kl ir">确保每个状态-动作被访问无限次。</strong></p><p id="a13a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">此外，回想一下，使用增量方法，我们可以按照最佳目标策略的方向更新/调整我们的价值函数，如下所示:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi or"><img src="../Images/6edbfa72bbdec82a2092721a5bd4113a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*-cVlJossrCfsjKJnPAdC1w.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk translated">不符合策略的MC更新规则</figcaption></figure><p id="594e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">情商。图10显示了第一个<strong class="kl ir"> n个</strong>返回的更新规则，该规则通过重要的采样比率进行加权。这个比率一般是<strong class="kl ir">加权</strong>的重要抽样比率。</p><p id="bca2" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated"><strong class="kl ir">那它！走到这一步真了不起！</strong>🤖🐱‍👤</p><p id="faec" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">此外，我被之前RL博客上的回复弄得不知所措。谢谢你，我希望这个博客能以同样的方式帮助你。下一次，我们将研究<strong class="kl ir">时差学习</strong>。</p><p id="73d1" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">希望这个故事能增加你对蒙特卡洛的了解。我很乐意在Instagram 上与你联系。</p></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><h2 id="a79e" class="lw lx iq bd ly lz ma dn mb mc md dp me lh mf mg mh lj mi mj mk ll ml mm mn mo bi translated">其他RL博客:</h2><ul class=""><li id="41a9" class="lf lg iq kl b km mp kp mq lh pa lj pb ll pc le ln lo lp lq bi translated"><a class="ae os" href="https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da" rel="noopener" target="_blank">强化学习:马尔可夫决策过程(第一部分)</a></li><li id="5bc2" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated"><a class="ae os" href="https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3" rel="noopener" target="_blank">强化学习:贝尔曼方程与最优性(下)</a></li><li id="9ae2" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated"><a class="ae os" href="https://towardsdatascience.com/reinforcement-learning-solving-mdps-using-dynamic-programming-part-3-b53d32341540" rel="noopener" target="_blank">强化学习:使用动态规划解决马尔可夫决策过程</a></li><li id="0344" class="lf lg iq kl b km lr kp ls lh lt lj lu ll lv le ln lo lp lq bi translated">强化学习:蒙特卡罗学习(这一个)</li></ul><p id="a4c8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated"><em class="kk">参考文献:</em></p><p id="312e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">[1] <a class="ae os" href="https://web.archive.org/web/20200929125734/http://himanshusahni.github.io/2018/02/23/reinforcement-learning-never-worked.html" rel="noopener ugc nofollow" target="_blank">奖励M-E-M-E </a></p><p id="a777" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">理查德·萨顿和安迪·巴尔托。强化学习:导论。</p><p id="b71d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated"><a class="ae os" href="https://siegel.work/blog/RLModelFree/" rel="noopener ugc nofollow" target="_blank">机器人M-E-M-E </a></p><p id="c67a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lh kt ku kv lj kx ky kz ll lb lc ld le ij bi translated">[4] <a class="ae os" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&amp;index=5" rel="noopener ugc nofollow" target="_blank">大卫·西尔弗RL讲座</a></p></div></div>    
</body>
</html>