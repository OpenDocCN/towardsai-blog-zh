<html>
<head>
<title>Are You Sure That You Can Implement Image Classification Networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你确定你能实现图像分类网络吗？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/are-you-sure-that-you-can-implement-image-classification-networks-d5f0bffb242d?source=collection_archive---------0-----------------------#2022-03-25">https://pub.towardsai.net/are-you-sure-that-you-can-implement-image-classification-networks-d5f0bffb242d?source=collection_archive---------0-----------------------#2022-03-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a82077acc4d807b93fad5300ce3a4571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U5s_DmgPQgE7Rf88.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">图片来自:<a class="ae kc" href="https://machinelearningmastery.com/applications-of-deep-learning-for-computer-vision/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/applications-of-deep-learning-for-computer-vision/</a></figcaption></figure><p id="0c60" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我最近提交论文之前，我坚信由于PyTorch或Jax等构造良好的深度学习框架，我可以轻松实现图像分类网络。尽管它们为网络提供了模块，但我意识到有太多的实现细节需要我在实践中考虑。在我的第一个实现中，这个模型在我的玩具数据集中达到了70%的准确率。为了达到预期的性能，我被迫使用许多GPU来扩大有效的批处理大小。但是用很多GPU训练花了很长时间。相比之下，在应用了我即将介绍的几种技术之后，该模型显示出比第一个模型<strong class="kf ir">高14%的准确性，而训练时间和GPU数量分别减少了四倍和四倍</strong>。这是一种令人惊叹的体验，因为尽管模型架构是相同的，但总的预期GPU时间已经减少到1/16。</p><p id="143a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很多讲座，描述深度学习往往会遗漏实际实现的细节。因此，<strong class="kf ir">我将分享我的个人经历，这可能对未来的研究有益</strong>。我参考了一篇著名的论文，名为“用卷积神经网络进行图像分类的锦囊妙计”，它分享了设计训练过程的有用技巧。</p><div class="lb lc gp gr ld le"><a href="https://arxiv.org/abs/1812.01187" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">卷积神经网络用于图像分类的技巧包</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">最近在图像分类研究中取得的许多进展可以归功于训练过程的改进…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">arxiv.org</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls jw le"/></div></div></a></div><p id="d740" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是我在项目中学到的一些东西。</p><ul class=""><li id="136d" class="lt lu iq kf b kg kh kk kl ko lv ks lw kw lx la ly lz ma mb bi translated">考虑将<strong class="kf ir">学习率</strong>预热和<strong class="kf ir">余弦退火</strong>用于学习率。</li><li id="f87e" class="lt lu iq kf b kg mc kk md ko me ks mf kw mg la ly lz ma mb bi translated">学习率应根据使用的GPU的数量<strong class="kf ir">进行不同设置。</strong></li><li id="a6a5" class="lt lu iq kf b kg mc kk md ko me ks mf kw mg la ly lz ma mb bi translated">应用<strong class="kf ir">标签平滑</strong>以获得更好的训练稳定性。</li><li id="b740" class="lt lu iq kf b kg mc kk md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kf ir">移除偏置衰减</strong>。</li><li id="debb" class="lt lu iq kf b kg mc kk md ko me ks mf kw mg la ly lz ma mb bi translated">从<strong class="kf ir"> PyTorch-lightning </strong>中获益，便于实施</li><li id="b923" class="lt lu iq kf b kg mc kk md ko me ks mf kw mg la ly lz ma mb bi translated">PyTorch中的几个小选项可以进一步提高和改进您的代码。</li></ul></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="0412" class="mo mp iq bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">线性预热+余弦退火</h1><p id="caf7" class="pw-post-body-paragraph kd ke iq kf b kg nm ki kj kk nn km kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">当我使用指数衰减学习率调度器时，我必须使用大批量来快速获得期望的性能。相应地，我应该使用多个GPU，并稍微修改网络架构以使用SyncBatchNorm，而不是普通的BatchNorm操作。要达到60%的准确率，我要训练网络50个纪元；换句话说，一天。正如论文《卷积神经网络图像分类锦囊》中所推荐的，我改变了学习率调度器。</p><p id="6bfc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所提出的学习速率调度器有两个阶段:线性预热和余弦退火。下图显示了日程安排的完整形式。</p><figure class="ns nt nu nv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/1c2c3a3006c9a49605f5ca962aac755c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hCv7uIFIpZl1TTdXHYubIg.png"/></div></div></figure><p id="f462" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在预热阶段，学习率从0线性增加到目标学习率。使用大的学习率在数值上是不稳定的，因为在训练开始时所有的参数都是随机的。因此，在我的玩具实验中，简单的学习率衰减策略(如步长衰减和指数衰减)无法训练ResNet34和ResNet50，尽管ResNet18取得了成功。除此之外，学习速度的连续性甚至提高了训练的稳定性。通常，在许多实验中，线性预热的5个时期是足够的。在更改了学习率调度器之后，我可以将有效的批处理大小从4096减少到256，而不会因为提高了训练稳定性而降低性能。</p><p id="4aa7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据一个流行的学术档案，“论文与代码”，这种学习率调度广泛用于各种任务。</p><div class="lb lc gp gr ld le"><a href="https://paperswithcode.com/method/linear-warmup-with-cosine-annealing" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">论文与代码线性热身与余弦退火解释</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">带余弦退火的线性预热是一个学习率计划，其中我们线性增加学习率为$n$…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">paperswithcode.com</p></div></div><div class="ln l"><div class="nw l lp lq lr ln ls jw le"/></div></div></a></div><h1 id="3943" class="mo mp iq bd mq mr nx mt mu mv ny mx my mz nz nb nc nd oa nf ng nh ob nj nk nl bi translated">依赖批量大小的学习速率</h1><p id="be41" class="pw-post-body-paragraph kd ke iq kf b kg nm ki kj kk nn km kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">在训练阶段控制批量大小不会改变随机梯度下降的期望，但会改变方差。当使用大批量时，我们可以提高学习率，因为它的方差比使用小批量时要小。先前的研究证实<strong class="kf ir">根据经验，随着批量的增加线性增加学习率效果更好</strong>。具体来说，我们应该在使用两倍的大批量时将学习率提高一倍，在有效批量变为一半时将学习率降低一半。</p><h1 id="2075" class="mo mp iq bd mq mr nx mt mu mv ny mx my mz nz nb nc nd oa nf ng nh ob nj nk nl bi translated">标签平滑</h1><p id="bca7" class="pw-post-body-paragraph kd ke iq kf b kg nm ki kj kk nn km kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">大多数有完全监督的分类网络都是用CELoss训练的，CELoss是CrossEntropyLoss的缩写。CELoss的正式定义是:</p><figure class="ns nt nu nv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/b3afc6d979a28c265e694fc8fd670f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tP-o0bOQ0YJGbdOBZ0b-ew.png"/></div></div></figure><p id="3ae7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们推导这个训练目标的封闭形式的解时，我们可能会得到负的无穷大值，这在最近的深度学习库上是不处理的。此外，这个极端值鼓励输出分数显著不同，潜在地导致过度拟合。因此，建议使用软版本的CrossEntropyLoss。</p><figure class="ns nt nu nv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi od"><img src="../Images/75738de8b2f53ff5edef6b4de7f616dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VS_GHK_hWAKEyCLCVwBU1Q.png"/></div></div></figure><p id="d2ce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">标签平滑不是使用二进制值0或1，而是将目标分数1分布到不同的箱中，以实现更平滑的分布。平滑CrossEntropyLoss的最优解不再是无穷大或负无穷大值。在我的例子中，我观察到使用标签平滑后性能提高了2%。有关CrossEntropyLoss的更多详细描述，请参考以下链接:</p><div class="lb lc gp gr ld le"><a href="https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e" rel="noopener follow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">交叉熵损失函数</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">在大多数分类问题中用于优化机器学习模型的损失函数…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">towardsdatascience.com</p></div></div><div class="ln l"><div class="oe l lp lq lr ln ls jw le"/></div></div></a></div><h1 id="d2f8" class="mo mp iq bd mq mr nx mt mu mv ny mx my mz nz nb nc nd oa nf ng nh ob nj nk nl bi translated">移除偏置衰减</h1><p id="a0ec" class="pw-post-body-paragraph kd ke iq kf b kg nm ki kj kk nn km kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">权重衰减是避免过度拟合训练模型的一个很好的正则化技巧。正如论文《混合精度的高可扩展深度学习训练系统:四分钟训练ImageNet》中指出的那样，<strong class="kf ir">对偏向项应用权重衰减对于更快的训练</strong>来说并不可取。因此，我移除了模型中的偏差衰减。通过消除偏置衰减，我在实验中得到的影响可以忽略不计。相比之下，在下面的论文中的实验中有显著的差异。</p><div class="lb lc gp gr ld le"><a href="https://arxiv.org/abs/1807.11205" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">具有混合精度的高度可扩展的深度学习训练系统:在四个…</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">具有数据并行性的同步随机梯度下降(SGD)优化器被广泛应用于训练大规模数据集</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">arxiv.org</p></div></div><div class="ln l"><div class="of l lp lq lr ln ls jw le"/></div></div></a></div><h1 id="015f" class="mo mp iq bd mq mr nx mt mu mv ny mx my mz nz nb nc nd oa nf ng nh ob nj nk nl bi translated">几个杂项</h1><p id="84c0" class="pw-post-body-paragraph kd ke iq kf b kg nm ki kj kk nn km kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">从现在开始，我将介绍几种工程技巧，它们可以使您的代码变得更简单、更高效。</p><h2 id="3ed5" class="og mp iq bd mq oh oi dn mu oj ok dp my ko ol om nc ks on oo ng kw op oq nk or bi translated">将数据放在本地或SSD内存中。不是硬盘！</h2><p id="323f" class="pw-post-body-paragraph kd ke iq kf b kg nm ki kj kk nn km kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">我花了很多时间处理这个问题。这个问题对我来说很糟糕，因为没有错误，但是代码很慢。分析完可执行文件后，我发现磁盘I/O操作是我的代码的瓶颈。由于我的远程服务器有一个大容量的硬盘存储器，我用它来存储我的大规模玩具数据集。然而，当我们使用大量的磁盘I/O操作时，这会极大地降低代码速度。如果你不能把所有的数据都放在运行时内存中，<strong class="kf ir">我强烈建议你把数据放在本地内存中，至少放在SSD内存中。</strong>(无硬盘广场)</p><h2 id="2a99" class="og mp iq bd mq oh oi dn mu oj ok dp my ko ol om nc ks on oo ng kw op oq nk or bi translated">使用PyTorch-Lightning进行16位(半)精确训练</h2><p id="2cb1" class="pw-post-body-paragraph kd ke iq kf b kg nm ki kj kk nn km kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">正如PyTorch-Lightning官方文档中所建议的，使用16位精度可以增加训练时间。最近基于Volta和图灵架构的GPU设备大大减少了训练时间。根据NVIDIA官方文档，在计算最密集的模型架构上观察到高达3倍的整体加速，而性能略有下降。<strong class="kf ir"> PyTorch-Lightning通过添加一个参数</strong>，支持使用半精度训练的便捷选项。另外，多GPU训练也是类似的做法。</p><figure class="ns nt nu nv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi os"><img src="../Images/d53a6d5e69127689974a54bb4f344615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQjO1XNNSmqZbICJSaR2Bw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">使用PyTorch-Lightning时，您可以通过一个参数简单地更改精度。</figcaption></figure><div class="lb lc gp gr ld le"><a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">英伟达深度学习框架</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">NVIDIA深度学习框架文档-2022年3月11日最后更新-发送反馈-准备使用Docker…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">docs.nvidia.com</p></div></div></div></a></div></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="f0ef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我分享了许多实用的建议，可以帮助您更快、更方便地实现。我很确定许多读者都熟悉分类的原则，因为许多相关的讲座已经彻底地教过他们了。我对自己的实施技能也很有信心。然而，这段经历引发了我的自我反思。我希望这篇文章能帮助您扩展您的实现技能。</p><blockquote class="ot ou ov"><p id="fb14" class="kd ke ow kf b kg kh ki kj kk kl km kn ox kp kq kr oy kt ku kv oz kx ky kz la ij bi translated">喜欢我的文章吗？更多文章请访问我的个人页面！<br/>作者:Yoonwoo Jeong <br/>所属单位:POSTECH，计算机视觉实验室<br/>邮箱:jeongyw12382@postech.ac.kr <br/>个人页面:【http://github.com/jeongyw12382】T4</p></blockquote></div></div>    
</body>
</html>