<html>
<head>
<title>GPT-3 from OpenAI is here and it’s a Monster</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自OpenAI的GPT-3在这里，它是一个怪物</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/gpt-3-from-openai-is-here-and-its-a-monster-f0ab164ea2f8?source=collection_archive---------0-----------------------#2020-06-07">https://pub.towardsai.net/gpt-3-from-openai-is-here-and-its-a-monster-f0ab164ea2f8?source=collection_archive---------0-----------------------#2020-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="456a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="c77a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">1750亿个参数，比之前最大的模型大10倍，GPT-3是迄今为止最大的训练变压器</h2></div><p id="13c7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">来自OpenAI的最新GPT-3拥有1750亿个参数，比之前最大的模型，微软的图灵-NLG的T2大10倍。从这个大型模型中出现的行为是令人兴奋的:执行特定的NLP任务(如翻译或问答)需要较少的微调。那是什么意思？</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/58229fdbad3091cb715ed0d95a1e50d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fKu41pUdz_PkJEvKbzVrow.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">语言模型是一次性学习器:GPT-3——模型越大，样本越少，在NLP任务中表现良好所需的微调就越少。图片来自OpenAI paper。</figcaption></figure><p id="063e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">假设您想要建立一个从英语到法语的翻译模型。你需要一个预先训练好的语言模型(比如BERT ),然后输入带有成对翻译的英语单词/句子数据。GPT-3无需任何额外的学习就能完成这项任务。您只需提供一个提示(询问句子或短语):</p><blockquote class="me mf mg"><p id="2b7a" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">"把英语翻译成法语:cheese = &gt; "</p></blockquote><p id="abbb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">得到</p><blockquote class="me mf mg"><p id="59e0" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">" fromage "</p></blockquote><p id="b10d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其他任务也是如此。一个问答示例是给出一个提示:</p><blockquote class="me mf mg"><p id="b3ba" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">“问:48加76是多少？</p></blockquote><p id="9d84" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你会得到</p><blockquote class="me mf mg"><p id="6431" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">“答:124。”</p></blockquote><p id="9551" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">看起来很刺激，对吧？我们可以继续这样的任务。更多例子参见<a class="ae ln" href="http://openai.com" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>的原论文名为<a class="ae ln" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">“语言模型是很少出手的学习者”</strong> </a>。少量学习在这里指的是向一个模型展示几个例子，然后要求做同样的事情。在上面的例子中，例如给出一个提示:</p><blockquote class="me mf mg"><p id="a290" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">"把英语翻译成法语:sea otter =&gt; loutre de mer，cheese = &gt; "</p></blockquote><p id="e9d2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是一个一次性学习的例子，我上面展示的是一个一次性学习的例子——模型中没有给出任何例子，只有一个任务描述“将英语翻译成法语”</p><p id="d618" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">微软的图灵-NLG(拥有175亿个参数，比GPT-3小10倍)已经能够回答诸如“二战何时开始”、“谁是英国女王”等一般性问题。，无需任何额外的微调。GPT-3走得更远，能够高精度地进行基本常识推理。例如，它可以填充以下句子中的空格:</p><blockquote class="me mf mg"><p id="4bd1" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">爱丽丝是鲍勃的朋友。爱丽丝去拜访她的朋友_____</p></blockquote><p id="ab9c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">给出的答案是“鲍勃”</p><p id="e53d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种语言理解水平以前从未在通用模型上实现过，商业应用的可能性仍有待发现。</p><p id="4a93" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2020年似乎是文本理解机器学习方法取得突破的又一年。如果你想更深入地了解GPT-3的细节，那么除了阅读原文，你可以看看GPT-3的这个<a class="ae ln" href="https://www.youtube.com/watch?v=SY5PvZrJhLE" rel="noopener ugc nofollow" target="_blank">视频解释</a>或围绕论文的这个<a class="ae ln" href="https://www.youtube.com/watch?v=7qPDwsCLbZc&amp;t=290s" rel="noopener ugc nofollow" target="_blank">视频讨论</a>。</p><h2 id="05ec" class="ml mm it bd mn mo mp dn mq mr ms dp mt la mu mv mw le mx my mz li na nb nc iz bi translated"><a class="ae ln" href="http://https//contentyze.com" rel="noopener ugc nofollow" target="_blank">附:如果你想玩类似GPT-3的模型，测试一下我的内容生成平台Contentyze。</a></h2></div></div>    
</body>
</html>