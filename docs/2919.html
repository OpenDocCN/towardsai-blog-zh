<html>
<head>
<title>How to Design a Pre-training Model (TSFormer) For Time Series?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何为时间序列设计一个预训练模型(TSFormer)？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-design-a-pre-training-model-tsformer-for-time-series-c2a177ebb51d?source=collection_archive---------1-----------------------#2022-07-08">https://pub.towardsai.net/how-to-design-a-pre-training-model-tsformer-for-time-series-c2a177ebb51d?source=collection_archive---------1-----------------------#2022-07-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="83af" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">最近，在NLP(自然语言处理)任务中有许多尝试，其中大多数都利用了预先训练的模型。NLP任务的提要大部分是由人类创造的数据，充满了丰富和优秀的信息，几乎可以被认为是一个数据单元。在时间序列预测中，我们可以感觉到缺乏这样的预训练模型。为什么不能像在NLP中那样在时间序列中利用这种优势？！本文是提出这种模型的详细说明。该模型是通过考虑两个观点开发的，从输入到输出有4个部分。此外，为了更好地理解，还添加了Python代码。</h2></div><h1 id="5b61" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">t变压器</h1><ul class=""><li id="7a42" class="kx ky iq kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">它是一个基于<strong class="kz ir"><em class="lp"/></strong><em class="lp">rans</em><strong class="kz ir"><em class="lp">原</em><em class="lp">ts原</em></strong><strong class="kz ir">的<em class="lp">无监督预训练</em>模型</strong></li><li id="acb5" class="kx ky iq kz b la lq lc lr le ls lg lt li lu lk ll lm ln lo bi translated">这个模型能够捕捉我们数据中非常长的依赖关系。</li></ul></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="381c" class="kf kg iq bd kh ki mc kk kl km md ko kp jw me jx kr jz mf ka kt kc mg kd kv kw bi translated"><em class="mh"> NLP和时间序列:</em></h1><p id="75b6" class="pw-post-body-paragraph mi mj iq kz b la lb jr mk lc ld ju ml le mm mn mo lg mp mq mr li ms mt mu lk ij bi translated">在某种程度上，NLP信息和时间序列数据是<strong class="kz ir">同</strong>。它们都是<strong class="kz ir">顺序数据</strong>和<strong class="kz ir">局部敏感数据，</strong>表示与其下一个/前一个数据点相关。顺便说一下，有一些<strong class="kz ir">差异</strong>我将在下面说:</p><p id="f538" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">正如我们为NLP任务所做的那样，在提出我们的<strong class="kz ir">预训练模型</strong>时，我们应该考虑两个事实(事实上，这些是差异):</p><ol class=""><li id="b722" class="kx ky iq kz b la mv lc mw le na lg nb li nc lk nd lm ln lo bi translated">时间序列数据中的密度比自然语言中的密度低得多</li><li id="a572" class="kx ky iq kz b la lq lc lr le ls lg lt li lu lk nd lm ln lo bi translated">我们需要一个比NLP数据更长序列长度的时间序列数据</li></ol></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="5415" class="kf kg iq bd kh ki mc kk kl km md ko kp jw me jx kr jz mf ka kt kc mg kd kv kw bi translated"><strong class="ak"><em class="mh">t从零开始转换</em> </strong></h1><p id="2a3a" class="pw-post-body-paragraph mi mj iq kz b la lb jr mk lc ld ju ml le mm mn mo lg mp mq mr li ms mt mu lk ij bi translated">它的过程和其他所有模型一样，就像一次旅行(没什么新意，但视角很好)。正如我给你讲过的<strong class="kz ir"> <em class="lp"> MAE </em> </strong>的策略，主要架构是经过一个<strong class="kz ir">编码器</strong>然后处理成一个<strong class="kz ir">解码器</strong>，最后<strong class="kz ir"> </strong>只是重构目标。</p><p id="b322" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">您可以在图1中看到它的架构:</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/1631110ba2ff270d834f9b8f3edbe3b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*GnVVyzziDwutwPMzpIbvSQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">图1 |【<a class="ae nq" href="https://ui.adsabs.harvard.edu/abs/2022arXiv220609113S/abstract" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><blockquote class="nr"><p id="e2cd" class="ns nt iq bd nu nv nw nx ny nz oa lk dk translated">就这样，伙计！！没有比这个数字更多的了。😆然而；如果您想知道它是如何工作的，我将在下一节中举例说明:</p></blockquote><p id="d494" class="pw-post-body-paragraph mi mj iq kz b la ob jr mk lc oc ju ml le od mn mo lg oe mq mr li of mt mu lk ij bi translated">流程:1。<strong class="kz ir">2<em class="lp">掩蔽</em>2</strong>。<strong class="kz ir"> <em class="lp">编码</em> </strong> 3。<strong class="kz ir"> <em class="lp">解码</em> </strong> 4。<strong class="kz ir"> <em class="lp">重建目标</em> </strong></p><h2 id="0db9" class="og kg iq bd kh oh oi dn kl oj ok dp kp le ol om kr lg on oo kt li op oq kv or bi translated">1.掩饰</h2><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/fa59294c744b5ae98fbcdc2b64619d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*2FBNV-AQhnFRe3aUrjsqxg.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">图2 | <a class="ae nq" href="https://ui.adsabs.harvard.edu/abs/2022arXiv220609113S/abstract" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="23b1" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">这是为下一步(编码器)提供输入的第一步。我们可以看到，<strong class="kz ir">输入序列</strong>(<strong class="kz ir"><em class="lp"/></strong>)已经被分配成长度为<strong class="kz ir"><em class="lp">【l】</em></strong>的<strong class="kz ir"> <em class="lp"> P </em>面片</strong>。因此，用于预测下一时间步的<strong class="kz ir">滑动窗口</strong>的长度为<strong class="kz ir"><em class="lp">P</em></strong>x<strong class="kz ir"><em class="lp">L</em></strong>。</p><p id="badc" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated"><strong class="kz ir">制作比</strong> ( <strong class="kz ir"> <em class="lp"> r </em> </strong>)是<strong class="kz ir"> 75 </strong> % ( <em class="lp">比较高吧？);</em>这仅仅是关于制作一个<strong class="kz ir">自监督任务</strong>并使<strong class="kz ir">编码器</strong>更<strong class="kz ir">高效</strong>。</p><p id="221d" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">这样做的主要原因(<em class="lp">我指的是修补输入序列</em>)是:</p><ol class=""><li id="8509" class="kx ky iq kz b la mv lc mw le na lg nb li nc lk nd lm ln lo bi translated"><strong class="kz ir">段</strong>(即补丁)比<strong class="kz ir">分离点</strong>更好<strong class="kz ir">。</strong></li><li id="6d2c" class="kx ky iq kz b la lq lc lr le ls lg lt li lu lk nd lm ln lo bi translated">利用<strong class="kz ir">下游模型</strong>使得<strong class="kz ir">更简单</strong>(stg nn将一个单元段作为输入)</li><li id="8ac3" class="kx ky iq kz b la lq lc lr le ls lg lt li lu lk nd lm ln lo bi translated">这样，我们可以<strong class="kz ir">减小编码器<strong class="kz ir">输入</strong>的<strong class="kz ir">尺寸</strong>。</strong></li></ol><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="os ot l"/></div></figure><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="c0d6" class="og kg iq bd kh oh oi dn kl oj ok dp kp le ol om kr lg on oo kt li op oq kv or bi translated">2.编码</h2><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/07001bfb50fcc1c8ca9fd1c4cd12c2a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*JNuv_Vhjze_Hz-ySpf_7yQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">图3 | [ <a class="ae nq" href="https://ui.adsabs.harvard.edu/abs/2022arXiv220609113S/abstract" rel="noopener ugc nofollow" target="_blank">来源</a> ]</figcaption></figure><p id="a56f" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">如你所见，这是一个<strong class="kz ir"> <em class="lp">输入嵌入、位置编码和变换块</em> </strong>的顺序。编码器可以只在<strong class="kz ir">未屏蔽的补丁</strong>上执行。<em class="lp">等等！！那是什么？？</em></p><p id="422e" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated"><strong class="kz ir"> <em class="lp">输入嵌入</em> </strong></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/579bdf2d025fdb3ed499d1738f6b566e.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*gWH7asq9o-OU8rJXwOuBIA.png"/></div></figure><p id="1918" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated"><strong class="kz ir"> <em class="lp"> Q) </em> </strong>上一步是关于遮罩，现在我说我们需要无遮罩的？？！！</p><p id="fa09" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated"><strong class="kz ir"> <em class="lp"> A) </em>输入嵌入</strong>。</p><p id="09af" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated"><strong class="kz ir"> <em class="lp">问)</em> </strong>如何？</p><p id="2eb1" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">a)它实际上是一个<strong class="kz ir">线性投影</strong>，将未遮掩转换为<strong class="kz ir">潜在空间</strong>。其公式如下所示:</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/bdb25b5cb3b341c96d5b231ffe635887.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*sGhzeFsOl2TXF3uZRwf2QQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">Eq 1。</figcaption></figure><p id="ed44" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated"><strong class="kz ir"> <em class="lp"> W </em> </strong>和<strong class="kz ir"> <em class="lp"> b </em> </strong>是<strong class="kz ir">可学习参数</strong>和<strong class="kz ir"> <em class="lp"> U </em> </strong>是<strong class="kz ir"> <em class="lp"> d </em>隐藏维度</strong>中的<strong class="kz ir">模型输入向量</strong>。</p><p id="7012" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated"><strong class="kz ir">位置编码</strong></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/b877ced79892cc30a6585dbe52d9cae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*XZAR_7hfPJ_y1vG_eaSp0w.png"/></div></figure><p id="6b5c" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">简单的<strong class="kz ir">位置编码层</strong>用于附加<strong class="kz ir">新的顺序信息</strong>。添加了术语“<strong class="kz ir">可学习的</strong>”，这有助于显示<strong class="kz ir">比<strong class="kz ir"> <em class="lp">正弦</em> </strong>性能更好。同样，<strong class="kz ir">可学习的位置</strong>嵌入显示了时间序列的良好<strong class="kz ir">结果</strong>。</strong></p><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="449f" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated"><strong class="kz ir">变压器块</strong></p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/282ab9f768fa4b3706868b6159f58a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*deDyJy18OwtB0q0XU9e5rQ.png"/></div></figure><p id="ca89" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">研究人员使用了<strong class="kz ir"> 4层变形金刚</strong>，仅低于计算机视觉和NLP任务中的常见量。此处所示的变压器类型是最常用的变压器架构。你可以在2017年出版的《<a class="ae nq" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>》中通读。顺便说一下，我要给你一个总结(这个简短的说明来自我以前的一篇文章，<a class="ae nq" href="https://rezayazdanfar.medium.com/informer-beyond-efficient-transformer-for-long-sequence-time-series-forecasting-4eeabb669eb" rel="noopener">Informer:Beyond Efficient Transformer for Long Sequence Time-Series Forecasting’</a>):</p><p id="50b0" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">变形金刚是新的深度学习模型，呈现的速度越来越快。他们采用了<strong class="kz ir">自我关注</strong>的机制，并在NLP和计算机视觉的挑战性任务中显示出模型性能的显著提高。变压器架构可以设想成两部分，称为<strong class="kz ir"> <em class="lp">编码器</em> </strong>和<strong class="kz ir"> <em class="lp">解码器</em></strong><em class="lp"/>，如图4所示，如下:</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/5bccb24e442e5286b0d52301c93bd5c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*9Bvv5RtEfEg2Xh3gfhyqxg.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">图4。变压器架构| [ <a class="ae nq" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">来源</a> ]</figcaption></figure><p id="1dbf" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">变形金刚的要点是独立于地域。与CNN等其他流行模式相比，《变形金刚》不受本地化的限制。此外，我们不建议在《变形金刚》中使用任何CNN架构；相反，我们在《变形金刚》中使用基于注意力的结构，这让我们能够完成更好的结果。<br/>注意力架构可以概括在图5中:</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/143432ee1e504a5cd020cf07c7791004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*6ak1v5jB16Dxp4kV4LS_MQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">图5(左)缩放的点积注意力。(右)多头注意力由几个并行运行的注意力层组成。|<a class="ae nq" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="c750" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">比例点积注意力的函数是Eq。2</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/051e89a935db7cf1b10ee9c6af972a43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*RsopGQS30PoBAY7Yv2OIZw.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">情商。2 [ <a class="ae nq" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="80a0" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">q(查询)，K(键)，V(向量)是我们注意力的输入。</p><p id="4448" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">对于变形金刚完整的基本实现，请看"<a class="ae nq" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank"><em class="lp"/></a>。"它让你对注意力和变形金刚有了很大的了解；事实上，我第一次通过这篇论文完整地理解了这个重要的模型。</p><blockquote class="nr"><p id="58c3" class="ns nt iq bd nu nv pb pc pd pe pf lk dk translated">我觉得这个总结量足够变形金刚用了</p></blockquote><figure class="pg ph pi pj pk nj"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="1fa2" class="og kg iq bd kh oh oi dn kl oj ok dp kp le ol om kr lg on oo kt li op oq kv or bi translated">3.解码</h2><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/0f3bcca7c1eef2b3ad3f38130a780e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*hhQvORwUw_iOn3b_pRrfEw.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">图6 | [ <a class="ae nq" href="https://ui.adsabs.harvard.edu/abs/2022arXiv220609113S/abstract" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="8d95" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">解码器包括一系列变换器块。它适用于所有补丁集(屏蔽令牌等)。).相比之下，<strong class="kz ir"> MAEs </strong> ( <em class="lp">屏蔽自动编码器</em>)，由于补丁已经有位置信息，所以没有位置嵌入。<strong class="kz ir">层数</strong>正好是<strong class="kz ir">一层</strong>。之后，简单的<strong class="kz ir"> MLPs </strong>(多层感知)被使用(<em class="lp">我想确定没有必要说明MLPs😉</em>)，使得<strong class="kz ir">输出长度<em class="lp">等于</em> </strong>到<strong class="kz ir">每个面片</strong>。</p><h2 id="ea2f" class="og kg iq bd kh oh oi dn kl oj ok dp kp le ol om kr lg on oo kt li op oq kv or bi translated">4.重建目标</h2><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/3713ea3574dafbae7e72628851664e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*Peu3Wicswwhw2qK0WhlDxA.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">图7 | [ <a class="ae nq" href="https://ui.adsabs.harvard.edu/abs/2022arXiv220609113S/abstract" rel="noopener ugc nofollow" target="_blank">来源</a> ]</figcaption></figure><p id="eff0" class="pw-post-body-paragraph mi mj iq kz b la mv jr mk lc mw ju ml le mx mn mo lg my mq mr li mz mt mu lk ij bi translated">并行地，<strong class="kz ir">计算</strong>超过<strong class="kz ir">被遮蔽的面片</strong>是针对<strong class="kz ir">的每个数据点</strong> ( <strong class="kz ir"> <em class="lp"> i </em> </strong>)。另外，选择<strong class="kz ir"> mae </strong> ( <strong class="kz ir"> <em class="lp">平均绝对误差</em> </strong>)作为<strong class="kz ir">主序列</strong>和<strong class="kz ir">重建序列</strong>的<strong class="kz ir">损失函数</strong>。</p><figure class="nf ng nh ni gt nj gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/eae6e8729df512dfef34e5327650502a.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*-NetRWcWgJxX5G6SLvt1xQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">Eq 3。</figcaption></figure></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><blockquote class="nr"><p id="7baf" class="ns nt iq bd nu nv pb pc pd pe pf lk dk translated">就是这样！！你做到了！不太难，是吗？？！😉😉</p><p id="8d51" class="ns nt iq bd nu nv pb pc pd pe pf lk dk translated">现在，让我们来看看架构:</p></blockquote><figure class="pg ph pi pj pk nj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/1631110ba2ff270d834f9b8f3edbe3b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*GnVVyzziDwutwPMzpIbvSQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk translated">图8 |【<a class="ae nq" href="https://ui.adsabs.harvard.edu/abs/2022arXiv220609113S/abstract" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><figure class="nf ng nh ni gt nj"><div class="bz fp l di"><div class="os ot l"/></div></figure><h1 id="a4d3" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结束了</h1><blockquote class="po pp pq"><p id="61db" class="mi mj lp kz b la mv jr mk lc mw ju ml pr mx mn mo ps my mq mr pt mz mt mu lk ij bi translated"><strong class="kz ir">代码</strong>和<strong class="kz ir">架构</strong>的来源分别是<a class="ae nq" href="https://github.com/zezhishao/STEP" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae nq" href="https://ui.adsabs.harvard.edu/abs/2022arXiv220609113S/abstract" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="e8b5" class="mi mj lp kz b la mv jr mk lc mw ju ml pr mx mn mo ps my mq mr pt mz mt mu lk ij bi translated"><em class="iq">可以在</em><strong class="kz ir"><em class="iq">Twitter</em></strong><a class="ae nq" href="https://twitter.com/reza__yazdanfar" rel="noopener ugc nofollow" target="_blank"><em class="iq">这里</em> </a> <em class="iq">或者</em><strong class="kz ir"><em class="iq">LinkedIn</em></strong><a class="ae nq" href="http://www.linkedin.com/in/rezayazdanfar" rel="noopener ugc nofollow" target="_blank"><em class="iq">这里</em> </a> <em class="iq">联系我。最后，如果你觉得这篇文章有趣又有用，可以在</em> <strong class="kz ir"> <em class="iq">中</em> </strong> <em class="iq">上</em> <strong class="kz ir"> <em class="iq">关注</em> </strong> <em class="iq">我，获取更多来自我的文章。</em></p></blockquote></div></div>    
</body>
</html>