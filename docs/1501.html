<html>
<head>
<title>Fully Explained Decision Tree Classification with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python全面解释决策树分类</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fully-explained-decision-tree-classification-with-python-d90d3bd16836?source=collection_archive---------1-----------------------#2021-02-08">https://pub.towardsai.net/fully-explained-decision-tree-classification-with-python-d90d3bd16836?source=collection_archive---------1-----------------------#2021-02-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="669b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="16f5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">分类问题决策树的深入研究</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/eb6c29ac53c4a21c0f9caf1f19b3fb19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLJWt9hD2zTS-x3gRhYuqg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">决策树。作者的照片</figcaption></figure><p id="ee1d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本文中，决策树算法是所有其他树模型的基础模型。决策树来自CART(分类和回归树)算法，它是sklearn中的优化版本。这些是非参数监督学习。非参数意味着数据是无分布的，即变量是名义变量或序数变量。</p><p id="0fe8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">决策树通过选择根节点并进一步分割成节点来做出决定。这种分割基于决策树中使用的指标。早期的文章是关于回归和分类的度量。但是在决策树的情况下，度量标准有一点不同。</p><blockquote class="md me mf"><p id="f6fe" class="lh li mg lj b lk ll kd lm ln lo kg lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><strong class="lj jd">决策树中的<em class="it">指标</em>中的</strong></p></blockquote><ol class=""><li id="b31e" class="mk ml it lj b lk ll ln lo lq mm lu mn ly mo mc mp mq mr ms bi translated"><strong class="lj jd"> <em class="mg">基尼杂质和熵</em> </strong></li><li id="3940" class="mk ml it lj b lk mt ln mu lq mv lu mw ly mx mc mp mq mr ms bi translated"><strong class="lj jd"> <em class="mg">信息增益</em> </strong></li><li id="a341" class="mk ml it lj b lk mt ln mu lq mv lu mw ly mx mc mp mq mr ms bi translated"><strong class="lj jd"> <em class="mg">方差减少</em> </strong></li></ol><blockquote class="md me mf"><p id="590f" class="lh li mg lj b lk ll kd lm ln lo kg lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><strong class="lj jd"> <em class="it">决策树有几种从数据集生成决策树的算法如下:</em> </strong></p></blockquote><ul class=""><li id="40df" class="mk ml it lj b lk ll ln lo lq mm lu mn ly mo mc my mq mr ms bi translated"><strong class="lj jd"> <em class="mg"> ID3(迭代二分法3): </em> </strong>它会生成较小的树，对连续数据没有用，因为它会导致在该属性中查找多个拆分，并且需要较长的时间。在训练集上也是如此。</li><li id="7ddb" class="mk ml it lj b lk mt ln mu lq mv lu mw ly mx mc my mq mr ms bi translated"><strong class="lj jd"> <em class="mg"> C4.5: </em> </strong>它是ID3的高级版本，也基于阈值处理连续数据。它在缺少值的数据集中也很有用。在创建树之后，它还可以做修剪工作。</li><li id="1c3b" class="mk ml it lj b lk mt ln mu lq mv lu mw ly mx mc my mq mr ms bi translated"><strong class="lj jd"> <em class="mg"> CART: </em> </strong>它是一个分类回归树，根据输出变量是分类的还是数值的来生成树。购物车检测到属性中不可能有进一步的增益，并停止拆分。</li></ul><blockquote class="md me mf"><p id="209e" class="lh li mg lj b lk ll kd lm ln lo kg lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><strong class="lj jd"> <em class="it">我们将看到不同的算法如何使用不同的度量来做树的分裂。</em> </strong></p></blockquote><h2 id="7655" class="mz na it bd nb nc nd dn ne nf ng dp nh lq ni nj nk lu nl nm nn ly no np nq iz bi translated">基尼系数和熵</h2><ul class=""><li id="a8e3" class="mk ml it lj b lk nr ln ns lq nt lu nu ly nv mc my mq mr ms bi translated">算法的CART版本中使用了Gini杂质。它用于查找错误分类观察值的概率，并且使用较少的基尼值进行更好的分割。</li><li id="9d3b" class="mk ml it lj b lk mt ln mu lq mv lu mw ly mx mc my mq mr ms bi translated">熵也用于基于错误分类的观察来分割树。由于对数计算，需要的时间稍长。</li><li id="95ca" class="mk ml it lj b lk mt ln mu lq mv lu mw ly mx mc my mq mr ms bi translated">这里的主要区别是，基尼系数的取值范围是从0到0.5。而熵的取值范围是从0到1。</li><li id="a256" class="mk ml it lj b lk mt ln mu lq mv lu mw ly mx mc my mq mr ms bi translated">算法的ID3和C4.5版本中使用了熵。</li></ul><h2 id="fa1e" class="mz na it bd nb nc nd dn ne nf ng dp nh lq ni nj nk lu nl nm nn ly no np nq iz bi translated">信息增益</h2><p id="6765" class="pw-post-body-paragraph lh li it lj b lk nr kd lm ln ns kg lp lq nw ls lt lu nx lw lx ly ny ma mb mc im bi translated">它与熵值一起使用。从先前状态到信息状态的熵值的差异。</p><p id="af16" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一般来说，分割是基于从目标变量到其他属性的最大增益或信息来完成的。</p><p id="0b1d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">增益(i) =信息增益(Y)-熵(Ai)</p><p id="3a95" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">y-因变量</p><p id="c537" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Ai —独立变量的属性(i=1，2…n)</p><h2 id="6071" class="mz na it bd nb nc nd dn ne nf ng dp nh lq ni nj nk lu nl nm nn ly no np nq iz bi translated">方差缩减</h2><p id="bd31" class="pw-post-body-paragraph lh li it lj b lk nr kd lm ln ns kg lp lq nw ls lt lu nx lw lx ly ny ma mb mc im bi translated">它在回归问题中很有用，因为当输出变量是连续数字时，节点的分割是基于方差值的。</p><div class="nz oa gp gr ob oc"><a href="https://medium.com/towards-artificial-intelligence/fully-explained-svm-classification-with-python-eda124997bcd" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jd gy z fp oh fr fs oi fu fw jc bi translated">用Python全面解释了SVM分类</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">如何用一个真实的例子解决分类问题。</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">medium.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq lb oc"/></div></div></a></div><blockquote class="md me mf"><p id="0278" class="lh li mg lj b lk ll kd lm ln lo kg lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><strong class="lj jd"> <em class="it">现在是用python做分类问题的时候了。</em>T3】</strong></p></blockquote><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="66cc" class="mz na it os b gy ow ox l oy oz"># Importing the libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span></pre><p id="2237" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">导入库后，下一步我们将读取CSV文件，并将数据分成特征和目标变量。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="6d4e" class="mz na it os b gy ow ox l oy oz"># Importing the dataset<br/>dataset = pd.read_csv('Social_Network_Ads.csv')<br/>x_set_values = dataset.iloc[:, [2, 3]].values<br/>y_set_values = dataset.iloc[:, 4].values</span></pre><p id="c7b4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在将数据分为训练数据和测试数据。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="a57b" class="mz na it os b gy ow ox l oy oz"># Splitting the dataset into the Training set and Test set<br/>from sklearn.model_selection import train_test_split</span><span id="eeac" class="mz na it os b gy pa ox l oy oz">X_train, X_test, y_train, y_test = train_test_split(x_set_values,<br/>                y_set_values, test_size = 0.25, random_state = 0)</span></pre><p id="5548" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下一步，我们可以进行标准缩放，但我认为在决策树中我们不应该这样做，因为分割应该基于真实值，而不是缩放值。但是如果比例部分如下所示:</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="f745" class="mz na it os b gy ow ox l oy oz">#Feature Scaling<br/>from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><p id="2f9e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这个算法中，我们将使用熵。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="4bbd" class="mz na it os b gy ow ox l oy oz"># Fitting the classifier classifier to the Training set<br/>from sklearn.tree import DecisionTreeClassifier<br/>classifier = DecisionTreeClassifier(criterion = 'entropy', random_state= 0)<br/>classifier.fit(X_train, y_train)</span><span id="6002" class="mz na it os b gy pa ox l oy oz">#output:</span><span id="427e" class="mz na it os b gy pa ox l oy oz">DecisionTreeClassifier(class_weight=None, criterion='entropy',<br/>            max_depth=None,<br/>            max_features=None, max_leaf_nodes=None,<br/>            min_impurity_decrease=0.0, min_impurity_split=None,<br/>            min_samples_leaf=1, min_samples_split=2,<br/>            min_weight_fraction_leaf=0.0, presort=False,<br/>            random_state=0,splitter='best')</span></pre><p id="c769" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，我们将预测数据并制作我们的模型。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="5f05" class="mz na it os b gy ow ox l oy oz"># Predicting the Test set results<br/>y_pred = classifier.predict(X_test)</span></pre><p id="34c2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，我们计算混淆矩阵。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="ba4c" class="mz na it os b gy ow ox l oy oz"># Making the Confusion Matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="4e55" class="mz na it os b gy pa ox l oy oz"><br/>#output:</span><span id="d41f" class="mz na it os b gy pa ox l oy oz">array([[66,  2],<br/>              [ 8, 24]], dtype=int64)</span></pre><p id="120f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用熵可视化训练和测试结果。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="0045" class="mz na it os b gy ow ox l oy oz"># Visualising the Training set results<br/>from matplotlib.colors import ListedColormap</span><span id="9bf5" class="mz na it os b gy pa ox l oy oz">X_set, y_set = X_train, y_train</span><span id="6841" class="mz na it os b gy pa ox l oy oz">X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop =<br/>                     X_set[:, 0].max() + 1, step = 0.01),<br/>                     np.arange(start = X_set[:, 1].min() - 1, stop = <br/>                     X_set[:, 1].max() + 1, step = 0.01))</span><span id="a8c6" class="mz na it os b gy pa ox l oy oz">plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), <br/>             X2.ravel()]).T).reshape(X1.shape),<br/>             alpha = 0.5, cmap = ListedColormap(('red', 'green')))</span><span id="ea1e" class="mz na it os b gy pa ox l oy oz">plt.xlim(X1.min(), X1.max())<br/>plt.ylim(X2.min(), X2.max())</span><span id="d188" class="mz na it os b gy pa ox l oy oz">for i, j in enumerate(np.unique(y_set)):<br/>    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],<br/>                alpha=0.5,<br/>                c = ListedColormap(('red', 'green'))(i), label = j)</span><span id="389a" class="mz na it os b gy pa ox l oy oz">plt.title('Decision Tree (Training set)')<br/>plt.xlabel('Age')<br/>plt.ylabel('Estimated Salary')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/bf6f16372eb1a7aa10611119cea552f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*miSBGZimWlutV59WCn6MRg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">训练集中的熵准则分类。作者的照片</figcaption></figure><p id="e08e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">将标准更改为“基尼”后，混淆矩阵值不会改变。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/f56fc3199f924132c53875931d969882.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*TpJ7vQPIe7ROMu2YpB5v2A.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">训练集中的基尼准则分类。作者的照片</figcaption></figure><p id="e3e8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是基于训练集的熵和基尼的树分裂分离准则。我们观察到图中有很好的分类。</p><div class="nz oa gp gr ob oc"><a href="https://medium.com/towards-artificial-intelligence/scatter-plot-and-line-chart-dashboard-in-plotly-with-python-4e886ea5f3e6" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jd gy z fp oh fr fs oi fu fw jc bi translated">使用Python在Plotly中实现散点图和折线图仪表板</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">用Plotly可视化</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">medium.com</p></div></div><div class="ol l"><div class="pd l on oo op ol oq lb oc"/></div></div></a></div><div class="nz oa gp gr ob oc"><a href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jd gy z fp oh fr fs oi fu fw jc bi translated">充分解释了使用Python进行K-means聚类</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">群体相似性机器学习中的非监督部分。</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">medium.com</p></div></div><div class="ol l"><div class="pe l on oo op ol oq lb oc"/></div></div></a></div><blockquote class="md me mf"><p id="a5aa" class="lh li mg lj b lk ll kd lm ln lo kg lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><strong class="lj jd">结论:</strong></p></blockquote><p id="07f5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">决策树在分类和回归中非常有用。标准参数给出数据集的不同树分裂。</p><p id="f919" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae pf" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae pf" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="f3cf" class="pg na it bd nb ph pi pj ne pk pl pm nh ki pn kj nk kl po km nn ko pp kp nq pq bi translated">推荐文章</h1><ol class=""><li id="60fb" class="mk ml it lj b lk nr ln ns lq nt lu nu ly nv mc mp mq mr ms bi translated"><a class="ae pf" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> NLP —用Python从零到英雄</a></li></ol><p id="e2e6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">2.<a class="ae pf" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a></p><p id="932e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">3.<a class="ae pf" href="https://medium.com/towards-artificial-intelligence/python-zero-to-hero-with-examples-c7a5dedb968b?source=friends_link&amp;sk=186aff630c2241aca16522241333e3e0" rel="noopener"> Python:零到英雄附实例</a></p><p id="3c07" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">4.<a class="ae pf" href="https://medium.com/towards-artificial-intelligence/mysql-zero-to-hero-with-syntax-of-all-topics-92e700762c7b?source=friends_link&amp;sk=35a3f8dc1cf1ebd1c4d5008a5d12d6a3" rel="noopener"> MySQL:零到英雄</a></p><p id="1143" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">5.<a class="ae pf" href="https://medium.com/towards-artificial-intelligence/basic-of-time-series-with-python-a2f7cb451a76?source=friends_link&amp;sk=09d77be2d6b8779973e41ab54ebcf6c5" rel="noopener">Python时间序列基础</a></p><p id="40ca" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">6.<a class="ae pf" href="https://medium.com/towards-artificial-intelligence/numpy-zero-to-hero-with-python-d135f57d6082?source=friends_link&amp;sk=45c0921423cdcca2f5772f5a5c1568f1" rel="noopener"> NumPy:用Python零到英雄</a></p><p id="e04e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">7.<a class="ae pf" href="https://medium.com/towards-artificial-intelligence/fundamentals-of-series-and-data-frame-in-pandas-with-python-6e0b8a168a0d?source=friends_link&amp;sk=955350bf43c7d1680be6e37b15b6628b" rel="noopener">用python实现熊猫系列和数据帧的基础</a></p><p id="01cb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">8.<a class="ae pf" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>