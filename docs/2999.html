<html>
<head>
<title>How To Make STGNNsCapable of Forecasting Long-term Multivariate Time Series Data?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使stg nns能够预测长期多元时间序列数据？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/how-to-make-stgnnscapable-of-forecasting-long-term-multivariate-time-series-data-9fe5efd77fa1?source=collection_archive---------0-----------------------#2022-07-29">https://pub.towardsai.net/how-to-make-stgnnscapable-of-forecasting-long-term-multivariate-time-series-data-9fe5efd77fa1?source=collection_archive---------0-----------------------#2022-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="9e82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从能源到医疗保健，时间序列预测(TSF)数据在所有行业中都至关重要。研究人员通过开发TFS模型取得了一些重大进展。通过彻底考虑时间序列的模式及其关系，基于数据集中的长期依赖性的分析是必须的。这篇文章是关于在另一个模型的基础上设计一个新的模型来执行长期依赖和产生段级表示。这个模型站在STEP上，STGNN(时空图神经网络)的缩写+增强+预训练模型。</p><h2 id="eafc" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">步骤:</h2><p id="f4a8" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">首先，不要混淆:</p><ul class=""><li id="5764" class="lj lk iq jp b jq jr ju jv jy ll kc lm kg ln kk lo lp lq lr bi translated"><strong class="jp ir">时空图数据=多元时间序列</strong></li></ul><p id="e45f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，使用的数据<em class="ls">(交通流量)</em>是由<em class="ls">传感器</em>记录的道路上的时间序列数据。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/40203229cf9245d034dcb2a4a04a9f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*YFjFrrAaw26KxB-le81m8A.png"/></div></figure><p id="29f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您看到上面图1中的两种模式了吗？？</p><p id="005d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">回答</strong>:重复模式有两种:1。<strong class="jp ir">日常</strong> 2。<strong class="jp ir">每周周期</strong></p><p id="fb5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一，STGNNs是“时空图神经网络”的缩写，给那些不知道/知道微薄的人。(不难，只需要<em class="ls">Google一下</em>；为不想浪费时间或分心的人提及)</p><p id="07d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> STGGNNs =序列网络+图形神经网络(GNNs) </strong></p><p id="8d89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用<em class="ls"> GNNs </em>来处理时序和时序模型之间的关系，以指示时序<em class="ls">模式</em>。通过这两个术语的结合，我们可以把握<strong class="jp ir"> <em class="ls">的杰出成果</em> </strong>。顺便说一句，正如研究人员所说，没有免费的午餐。这意味着<strong class="jp ir">强大的模型</strong>需要<strong class="jp ir">复杂的架构</strong>；因此(在大多数情况下)，<strong class="jp ir">的计算成本随着输入长度的增加而增加</strong>(<strong class="jp ir"><em class="ls"/></strong>或<strong class="jp ir"><em class="ls"/></strong>)。此外，不要忘记我们的时间序列的大小，这通常是相当可观的。<strong class="jp ir"> <em class="ls"> STGNNs </em> </strong>和其他模型一样，可以预测小窗口做出预测。这种依赖小窗口的能力使得模型不可靠。</p><p id="1e49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">问题</strong> : 1。STGNNs不能捕获长期依赖关系。</p><p id="a842" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.缺少依赖关系图。</p><p id="1c76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">解决方案</strong>:步骤(STGNN通过可扩展的时间序列预训练得到增强)</p><p id="f309" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">STGNNs的修改版本</p><p id="2fc2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">插图</strong>:</p><p id="f9ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">两项举措:</p><p id="a1ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 1 </strong>。提出<strong class="jp ir">t变压器</strong>，一个基于变压器的模块，具有自动编码器(编码器-解码器)结构，作为无监督模型。这个转换器能够捕获长依赖关系。</p><p id="9343" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2 </strong>。提出一个<strong class="jp ir">图架构学习器</strong>来学习依赖图。</p><p id="22e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在提出这两个之后，我们只需要将它们焊接成一个联合模型，这就是最终的解决方案。就是这样！！听起来很容易？！让我们尽可能使它们简单。😉</p><p id="472d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看提议的架构:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mb"><img src="../Images/f5c2da3f3ac1e9db39b733de93864462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V9uah_6Zc4qVbPUUjCyb4A.png"/></div></div></figure><p id="0130" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从图2中可以看出，该模型包括两个阶段:</p><p id="5b5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第一阶段)</strong>预培训</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/1631110ba2ff270d834f9b8f3edbe3b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*GnVVyzziDwutwPMzpIbvSQ.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">图3</figcaption></figure><p id="08af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该方案是一个屏蔽的自动编码模型，它是为依赖于变换块(TSFormer)的时间序列数据训练的。该模型能够捕获长依赖关系，并产生包含一些有价值信息的段级表示。</p><p id="9595" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第二阶段)</strong>预测</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/2d54aa0ce4994a6d4b820e217a423aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*k3Y-cS-P7w89ZPUeUYpaaw.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">图4</figcaption></figure><p id="14c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在此阶段，来自前一阶段的预训练模型(其捕获长期依赖性)用于修改下游STGNN。此外，还设计了一个离散稀疏图学习器，以防预定义图丢失。</p><p id="9088" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大体上我就做了这么多。因此，让我们更深入地了解这两个阶段的细节:</p><h1 id="40e8" class="mm km iq bd kn mn mo mp kq mq mr ms kt mt mu mv kw mw mx my kz mz na nb lc nc bi translated">1.训练前阶段</h1><p id="16b1" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">这种尝试，我的意思是使用预先训练的模型，是由于在NLP项目中应用它们的兴趣(当然，结果)的增加。虽然预训练模型在NLP(序列数据)中被广泛采用，但与时间序列有一些<strong class="jp ir">差异</strong>。你可以在我之前的文章中读到它的完整描述:“<a class="ae nd" rel="noopener ugc nofollow" target="_blank" href="/how-to-design-a-pre-training-model-tsformer-for-time-series-c2a177ebb51d"> <strong class="jp ir"> <em class="ls">如何为时间序列设计一个预训练模型(TSFormer)？</em> </strong> </a></p><h1 id="34ea" class="mm km iq bd kn mn mo mp kq mq mr ms kt mt mu mv kw mw mx my kz mz na nb lc nc bi translated">2.预测阶段</h1><p id="63e2" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">这里的输入分为<strong class="jp ir"> <em class="ls"> P </em>长度<strong class="jp ir">的不重叠面片</strong><em class="ls">L</em>T23】。我们的<strong class="jp ir"> <em class="ls"> TSFormer </em> </strong>为预测阶段的每个输入(<strong class="jp ir"> S </strong> i)产生指示。<strong class="jp ir">stg ns</strong>的一个特点是它们采用了最新的</strong>。因此，根据<strong class="jp ir"><em class="ls">t前</em> </strong>产生的指示，我们将修改<strong class="jp ir"> STGNNs </strong>。</p></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><h1 id="5dc9" class="mm km iq bd kn mn nl mp kq mq nm ms kt mt nn mv kw mw no my kz mz np nb lc nc bi translated">从零开始|流程</h1><ul class=""><li id="4e97" class="lj lk iq jp b jq le ju lf jy nq kc nr kg ns kk lo lp lq lr bi translated">这一阶段的编码器部分与TSFormer中的相同；这里不提供对它的描述，以免这篇文章太长。想了解详情可以看我完整演示过的另一篇文章(<a class="ae nd" rel="noopener ugc nofollow" target="_blank" href="/how-to-design-a-pre-training-model-tsformer-for-time-series-c2a177ebb51d"> <em class="ls">)如何为时间序列设计一个预训练模型(TSFormer)？</em> </a>)。</li></ul><h2 id="a995" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">图形学习的结构</h2><p id="9d74" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated"><strong class="jp ir">问题)</strong>大多数图依赖<strong class="jp ir">一个预定义的图</strong>就是<strong class="jp ir">不可用</strong>或者<strong class="jp ir">不够好</strong>大多数情况下——还有，<strong class="jp ir">混合</strong><strong class="jp ir">学习</strong>的<strong class="jp ir">方式(求节点之间的关系(对于<em class="ls"> ex。</em><strong class="jp ir"><em class="ls">I</em></strong><em class="ls"/>和<em class="ls"> j </em>)的时序和<strong class="jp ir"> STGNNs </strong>导致<strong class="jp ir">极大的复杂性</strong>。</strong></p><p id="23c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">解决方案)</strong>预培训的TSFormer</p><p id="fe37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">解释)</strong>提出<strong class="jp ir">一个离散稀疏图</strong>。怎么会？<strong class="jp ir"> 1。图形规则化</strong>到<em class="ls">拟合</em> <strong class="jp ir">监督信息</strong>。<strong class="jp ir"> 2。一个KNN图</strong>到<em class="ls">缰</em>的<strong class="jp ir">稀疏度</strong>。其配方总结如下:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nt"><img src="../Images/daf45f89cbcd84ca0aaf1996a83f5ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*72gx8aD7glsNQ_w22I_faA.png"/></div></div></figure><h2 id="4872" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">下游时空图神经网络</h2><p id="a02d" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated"><strong class="jp ir">问题</strong> ) <strong class="jp ir">常用补丁</strong> <strong class="jp ir">输入</strong> : <strong class="jp ir">最后补丁</strong> + <strong class="jp ir">依赖图</strong></p><p id="90b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">解</strong> ) <strong class="jp ir">步骤</strong>(将输入面片的表示添加到输入)</p><p id="2cc8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">解读</strong>)正如我们在我之前的文章《<a class="ae nd" rel="noopener ugc nofollow" target="_blank" href="/how-to-design-a-pre-training-model-tsformer-for-time-series-c2a177ebb51d"> <strong class="jp ir"> <em class="ls">如何为时间序列设计一个预训练模型(TSFormer)？</em> </strong> </a>”，<strong class="jp ir">t former</strong>捕捉<strong class="jp ir">long-dependencies</strong>；因此，它使得<strong class="jp ir"> H在<strong class="jp ir">信息</strong>方面更加丰富</strong>。此外，WaveNet被选为我们的<strong class="jp ir">后端，</strong>它有助于正确捕获多元时间序列。<em class="ls">但是如何？？</em>它将<strong class="jp ir">图形卷积</strong>与<strong class="jp ir">扩张卷积</strong>混合。因此，我们的<strong class="jp ir">预测</strong>得到了<strong class="jp ir"> WaveNet的输出潜在的、隐藏的表示</strong>的支持。<em class="ls">如何？？</em>通过使用<strong class="jp ir"> MLP </strong>。</p><p id="3b75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Q) </strong>如果你观察预测阶段架构，你会看到<strong class="jp ir">两个流</strong>进入<strong class="jp ir"> <em class="ls">时空图NN块</em> </strong>。那么，我们该如何应对呢？</p><p id="620f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> A) </strong>通过使用Eq7:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ce6b87d15e29565d86db6878e8f62d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*0e4X4D6OLYNeys-W33kkiw.png"/></div></figure><p id="450a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，MLP做出了预测:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1b1023485a98e200aeae55a3a0e5f683.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*Kh11arzdWLbrpgwcM9Ggug.png"/></div></figure><p id="d74b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下游STGNN的输出:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/bef370523e0be8ac12cf9979f4364e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*S0SKQBR-RWnLFbutKg1kiQ.png"/></div></figure></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><blockquote class="nx"><p id="dcf9" class="ny nz iq bd oa ob oc od oe of og kk dk translated">这次STGNN修改到此结束。希望你喜欢。其余的是真实世界数据集上的结果。</p></blockquote><h2 id="060e" class="kl km iq bd kn ko oh dn kq kr oi dp kt jy oj kv kw kc ok ky kz kg ol lb lc ld bi translated">结果:</h2><h2 id="c3fc" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">数据:</strong></h2><p id="3cee" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">该模型在美国三个地区的三个交通速度数据集上进行训练:</p><ol class=""><li id="f3eb" class="lj lk iq jp b jq jr ju jv jy ll kc lm kg ln kk om lp lq lr bi translated"><strong class="jp ir">METR-拉</strong></li><li id="51b2" class="lj lk iq jp b jq on ju oo jy op kc oq kg or kk om lp lq lr bi translated"><strong class="jp ir"> PEMS湾</strong></li><li id="e185" class="lj lk iq jp b jq on ju oo jy op kc oq kg or kk om lp lq lr bi translated"><strong class="jp ir"> PEMS04 </strong></li></ol><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi os"><img src="../Images/e380c7ec551e80d77445d038cf0b749c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3RvNZwBDkHH5lpm3DKr1Ug.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk translated">表1。数据集的统计</figcaption></figure><h2 id="956c" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">指标:</h2><ol class=""><li id="38b2" class="lj lk iq jp b jq le ju lf jy nq kc nr kg ns kk om lp lq lr bi translated"><strong class="jp ir"> MAE ( <em class="ls">平均绝对误差</em> ) </strong></li><li id="cb98" class="lj lk iq jp b jq on ju oo jy op kc oq kg or kk om lp lq lr bi translated"><strong class="jp ir"> RMSE ( <em class="ls">均方根绝对误差</em> ) </strong></li><li id="e92d" class="lj lk iq jp b jq on ju oo jy op kc oq kg or kk om lp lq lr bi translated"><strong class="jp ir"> MAPE ( <em class="ls">平均绝对百分比误差</em> ) </strong></li></ol><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ca"><img src="../Images/939c0c9b6b81fa072f13a4e732f6ddfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sMIU1DSI1l-b3vLm1ROO8w.png"/></div></div></figure><h1 id="5bb6" class="mm km iq bd kn mn mo mp kq mq mr ms kt mt mu mv kw mw mx my kz mz na nb lc nc bi translated">结束了</h1><blockquote class="ot ou ov"><p id="9b05" class="jn jo ls jp b jq jr js jt ju jv jw jx ow jz ka kb ox kd ke kf oy kh ki kj kk ij bi translated"><em class="iq">出处是</em> <a class="ae nd" href="https://ui.adsabs.harvard.edu/abs/2022arXiv220609113S/abstract" rel="noopener ugc nofollow" target="_blank"> <em class="iq">本</em> </a> <em class="iq">。</em></p><p id="ffa4" class="jn jo ls jp b jq jr js jt ju jv jw jx ow jz ka kb ox kd ke kf oy kh ki kj kk ij bi translated"><em class="iq">你可以</em> <strong class="jp ir"> <em class="iq">联系</em> </strong> <em class="iq">我上</em> <strong class="jp ir"> <em class="iq">推特</em> </strong> <a class="ae nd" href="https://twitter.com/reza__yazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a> <em class="iq">或者</em><strong class="jp ir"><em class="iq">LinkedIn</em></strong><a class="ae nd" href="http://www.linkedin.com/in/rezayazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a> <em class="iq">。最后，如果你觉得这篇文章有趣又有用，可以</em> <strong class="jp ir"> <em class="iq">关注</em> </strong> <em class="iq">我上</em> <strong class="jp ir"> <em class="iq">中</em> </strong> <em class="iq">获取更多来自我的文章。</em></p></blockquote></div></div>    
</body>
</html>