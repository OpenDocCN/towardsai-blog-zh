<html>
<head>
<title>Combining the Transformers Expressivity with the CNNs Efficiency for High-Resolution Image Synthesis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">结合变形金刚的表现力和细胞神经网络的高分辨率图像合成效率</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/combining-the-transformers-expressivity-with-the-cnns-efficiency-for-high-resolution-image-synthesis-31c6767547da?source=collection_archive---------2-----------------------#2021-02-02">https://pub.towardsai.net/combining-the-transformers-expressivity-with-the-cnns-efficiency-for-high-resolution-image-synthesis-31c6767547da?source=collection_archive---------2-----------------------#2021-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="bb4b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="4109" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">TL；DR:他们将GANs和卷积方法的效率与transformers的表达能力结合起来，为语义指导的高质量图像合成提供了一种强大而省时的方法。</h2></div><p id="e8bf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果标题和副标题听起来像是另一种语言，那么这篇文章就是为你而写的！</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/bfc8318885ab00a7e945d82a4fe55d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fxu6AStSNfvnWSrSgSzzlw.png"/></div></div></figure><h2 id="cf5a" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated">图像-GPT</h2><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="df22" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可能听说过最近由OpenAI发布的iGPT，或图像-GPT，我在我的频道上报道过。这是最先进的生成变压器模型。OpenAI在图像的像素表示上使用transformer架构来执行图像合成。简而言之，他们使用一幅图像的一半像素作为输入的变压器来生成另一半图像。正如你在这里看到的，它非常强大。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi gj"><img src="../Images/13dc3f7acf10de4b4545d4710d047fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NFbPrVd5eYiUiYI5bBPv6g.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated">陈等，【生成式像素预处理】，(2020)</figcaption></figure><p id="8e84" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">但是，你也知道，有4k高分辨率的图像和视频。你知道一张4k图像有多少像素吗？<br/>以百万甚至千万计。与自然语言处理应用程序的简单短语或段落相比，这是一个相当长的序列。因为转换器被设计为学习顺序数据上的远程交互，在这种情况下将顺序使用所有像素，所以它们的方法在计算上要求过高，并且不能缩放超过192 x 192图像分辨率。</p><p id="9ae1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以变形金刚不能用于图像，因为没有人想生成超低清晰度的图像，对不对？嗯，不尽然。</p><h2 id="eb32" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated"><a class="ae mx" href="https://compvis.github.io/taming-transformers/" rel="noopener ugc nofollow" target="_blank">驯服高分辨率图像合成的变压器</a></h2><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi my"><img src="../Images/f6f74176c9fa3840d75791d9bbcba884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IQ2GmQbRnwX-mQu0ldMFTw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><a class="ae mx" href="https://compvis.github.io/taming-transformers/" rel="noopener ugc nofollow" target="_blank">驯服用于高分辨率图像合成的变压器</a>，Esser等人，2020年</figcaption></figure><p id="cbdd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">德国海德堡大学的研究人员最近发表了一篇新论文，将卷积方法的效率与变形金刚的表达能力相结合，以产生高质量图像的语义指导合成。这意味着他们使用卷积神经网络来获得图像的上下文丰富的表示，然后使用该表示代替实际图像来训练转换器模型，以从其合成实际图像，从而允许比iGPT高得多的图像分辨率，同时保持最终图像的质量。但是我们一会儿会回到这个话题，并给出更好的解释。</p><p id="85cc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你不熟悉CNN或变形金刚，我强烈建议你看我做的解释它们的视频，以便更好地理解这种方法。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="08b5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这篇论文被称为“驯服高分辨率图像合成的变形金刚”，正如我所说，它使变形金刚能够从语义图像合成高分辨率图像，就像你在这里看到的一样。其中唯一需要的信息是近似的语义分割，显示在图像中的哪个位置你想要什么样的环境，并且它将输出完整的高清晰度图像，用真实的山、草、天空、日落等填充分割。</p><p id="8d60" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在的问题是，为什么这些研究人员和OpenAI使用变压器而不是我们典型的GAN架构进行图像合成？<br/>嗯，使用变形金刚进行图像生成的优势很明显:<br/> 1。它们继续在各种各样的任务上显示出最先进的结果，并且非常有前途。<br/> 2。它们不包含在CNN中发现的归纳偏差，其中二维图像和过滤器的使用导致局部交互的优先化。这种归纳偏见是CNN如此高效的原因，但它可能限制太多，无法使网络“富有表现力”，或“原创”。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mz"><img src="../Images/3f2ff8b3013da3395b9ed0fec1a55698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjFM2rklZBloeBn6gfLSLw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><a class="ae mx" href="https://compvis.github.io/taming-transformers/" rel="noopener ugc nofollow" target="_blank">驯服高分辨率图像合成的变形金刚</a>，埃塞尔等人，2020年</figcaption></figure><p id="d037" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们知道变形金刚更有“表现力”，非常强大，剩下的唯一一件事就是想办法让它更有效率。事实上，在他们的方法中，他们实现了使用这种由来自CNN的感应偏置引起的高效率以及变压器的表现力。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mz"><img src="../Images/da34828dadc7996aeb7f172931296f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WnjpckAjRSl8-2_xjpQowQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><a class="ae mx" href="https://compvis.github.io/taming-transformers/" rel="noopener ugc nofollow" target="_blank">驯服高分辨率图像合成的变压器</a>，Esser等人，2020年</figcaption></figure><p id="1c06" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如我所说，卷积神经网络架构由经典的编码器-解码器和使用鉴别器(他们称之为VQGAN)的对抗训练程序组成，用于以码本的形式生成图像的有效和丰富的表示。顾名思义，它是一种GAN架构，用于训练生成器生成高分辨率图像。如果你不熟悉GANs是如何工作的，你可以看看我制作的解释它们的视频。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mz"><img src="../Images/8b2c161f613bbd12bcb16909f615bea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SGpeSFKjcDAZvn6UT_VHRg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><a class="ae mx" href="https://compvis.github.io/taming-transformers/" rel="noopener ugc nofollow" target="_blank">驯服高分辨率图像合成的变压器</a>，埃塞尔等人，2020年</figcaption></figure><p id="2edc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一旦完成了第一次训练，它们只采用解码器，然后该解码器用于表示输入图像的编码信息，作为变换器的输入，这里称为码本。使得变换器不是直接使用图像的像素，而是使用这个包含图像表示的码本，该图像表示的形式是感觉上丰富的图像成分的组合。当然，这个密码本是由极度压缩的数据组成的，因此它可以被转换器顺序读取。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi mz"><img src="../Images/b92423a939f0bafecf45692eb9f967e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1xXdFja9WjkyQOgiHVOM-g.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk translated"><a class="ae mx" href="https://compvis.github.io/taming-transformers/" rel="noopener ugc nofollow" target="_blank">驯服高分辨率图像合成的变压器</a>，Esser等人，2020年</figcaption></figure><p id="923c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，使用此表示作为转换器的训练数据集，它学习预测此表示中可能的下一个索引的分布，就像常规的自回归模型一样。这意味着它会自动构建一个回归方程，使用以前的时间步长作为输入来预测未来时间步长的值。因此将CNN和GANs与变压器相结合来执行高分辨率图像合成。</p><h2 id="c78a" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated"><a class="ae mx" href="https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb" rel="noopener ugc nofollow" target="_blank">演示</a></h2><p id="7557" class="pw-post-body-paragraph kr ks it kt b ku na kd kw kx nb kg kz la nc lc ld le nd lg lh li ne lk ll lm im bi translated"><a class="ae mx" href="https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb" rel="noopener ugc nofollow" target="_blank">在这里</a>，你可以找到<a class="ae mx" href="https://github.com/CompVis/taming-transformers" rel="noopener ugc nofollow" target="_blank">代码</a>的<a class="ae mx" href="https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb" rel="noopener ugc nofollow" target="_blank">演示版本</a>，你现在就可以在google colab上试用，无需设置任何东西。他们已经设置好了，你只需要运行这几行。它从GitHub下载他们的代码，并自动安装所需的依赖项。然后，它加载模型并导入它的预训练版本。最后，您可以使用他们的分割图像作为测试，或者上传您自己的分割图像，再运行几行来编码分割。我提醒你，这是转换器创建与你的图像相关联的特定码本的必要步骤。而你终于可以制作出这种超高质量的图像了！</p><h2 id="0c5f" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated">观看更多结果！</h2><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="mr ms l"/></div></figure><h2 id="70ce" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated">结论</h2><p id="a585" class="pw-post-body-paragraph kr ks it kt b ku na kd kw kx nb kg kz la nc lc ld le nd lg lh li ne lk ll lm im bi translated">当然，这只是这篇新论文的概述。我强烈建议阅读它以获得更好的技术理解。还有，我前面提到过，他们的代码在GitHub上有预训练好的模型，你可以自己尝试，甚至改进！所有的链接都在下面的参考文献中。</p><p id="6284" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你喜欢我的工作，并想了解最新的人工智能技术，你绝对应该在我的社交媒体频道上关注我。</p><ul class=""><li id="ad2c" class="nf ng it kt b ku kv kx ky la nh le ni li nj lm nk nl nm nn bi translated">订阅我的<a class="ae mx" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> YouTube频道</strong> </a>。</li><li id="0837" class="nf ng it kt b ku no kx np la nq le nr li ns lm nk nl nm nn bi translated">关注我的项目上<a class="ae mx" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> LinkedIn </strong> </a> <strong class="kt jd"> </strong>和这里上<strong class="kt jd"> </strong> <a class="ae mx" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="kt jd">中</strong> </a> <strong class="kt jd">。</strong></li><li id="9c4e" class="nf ng it kt b ku no kx np la nq le nr li ns lm nk nl nm nn bi translated">一起学习AI，加入我们的<a class="ae mx" href="https://discord.gg/learnaitogether" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">不和谐社区</strong> </a>，<em class="nt">分享你的项目、论文、最佳课程，寻找Kaggle队友，等等！</em></li></ul></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h2 id="1a18" class="lz ma it bd mb mc md dn me mf mg dp mh la mi mj mk le ml mm mn li mo mp mq iz bi translated">参考</h2><p id="2c15" class="pw-post-body-paragraph kr ks it kt b ku na kd kw kx nb kg kz la nc lc ld le nd lg lh li ne lk ll lm im bi translated">驯服高分辨率图像合成的变压器，Esser等人，2020年</p><p id="be00" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">项目与论文和结果的链接</strong>:<a class="ae mx" href="https://compvis.github.io/taming-transformers/" rel="noopener ugc nofollow" target="_blank">https://compvis.github.io/taming-transformers/</a><br/><strong class="kt jd">代码</strong>:<a class="ae mx" href="https://github.com/CompVis/taming-transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/CompVis/taming-transformers</a><br/><strong class="kt jd">Colab演示立即开始采样</strong>:<a class="ae mx" href="https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb" rel="noopener ugc nofollow" target="_blank">https://Colab . research . Google . com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers . ipynb</a></p></div></div>    
</body>
</html>