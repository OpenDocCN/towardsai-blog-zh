<html>
<head>
<title>An Intuitive Introduction of Word2Vec by Building a Word2Vec From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始构建Word2Vec，直观地介绍Word2Vec</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/an-intuitive-introduction-of-word2vec-by-building-a-word2vec-from-scratch-a1647e1c266c?source=collection_archive---------1-----------------------#2019-06-24">https://pub.towardsai.net/an-intuitive-introduction-of-word2vec-by-building-a-word2vec-from-scratch-a1647e1c266c?source=collection_archive---------1-----------------------#2019-06-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/eadb1f62b38360b25ab20338e0d9fffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*d0JWmF36SUey7aS8bvA-dw.jpeg"/></div></div></figure><h2 id="1e9b" class="iz ja jb bd b dl jc jd je jf jg jh dk ji translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="8ca6" class="pw-subtitle-paragraph kh jk jb bd b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky dk translated">了解Word2Vec及其优势</h2></div><h1 id="f262" class="kz la jb bd lb lc ld le lf lg lh li lj kq lk kr ll kt lm ku ln kw lo kx lp lq bi translated">介绍</h1><p id="92ae" class="pw-post-body-paragraph lr ls jb lt b lu lv kl lw lx ly ko lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">在本文中，我将尝试解释<strong class="lt jl"> Word2Vec </strong>向量表示，这是一种从原始文本中学习单词嵌入的无监督模型，我还将尝试提供经典方法<strong class="lt jl">一键编码</strong>和<strong class="lt jl"> Word2Vec </strong>之间的比较。</p><h1 id="aecf" class="kz la jb bd lb lc ld le lf lg lh li lj kq lk kr ll kt lm ku ln kw lo kx lp lq bi translated">独热编码矢量表示</h1><p id="6841" class="pw-post-body-paragraph lr ls jb lt b lu lv kl lw lx ly ko lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">解决文本相关问题的经典方法是<strong class="lt jl">一键编码</strong>单词。这种方法有许多缺点。</p><ul class=""><li id="8d9b" class="mn mo jb lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">如果数据集有一万个唯一的单词(<strong class="lt jl">词汇</strong>)，那么<strong class="lt jl">单热编码</strong>向量表示将有一万个维度。</li><li id="9e68" class="mn mo jb lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">在<strong class="lt jl"> one-hot-encoded </strong>矢量表示中，矢量的大小将是<strong class="lt jl">词汇表</strong>的大小。<strong class="lt jl">词汇表</strong>中的大部分单词不会出现在每个文档中。所以<strong class="lt jl">一个热编码的</strong>矢量表示是一个大部分为空(零)的矢量。</li><li id="00c5" class="mn mo jb lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">它在计算上是低效的。</li><li id="57db" class="mn mo jb lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">在<strong class="lt jl">一个热编码的</strong>矢量表示中，相似的单词将不具有相似的矢量。</li></ul><p id="132c" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">我们来考虑下面两句话。</p><p id="8fd4" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">我喜欢看电影。</p><p id="3b9f" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">我喜欢看电影。</p><p id="9b9f" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">在<strong class="lt jl">一键编码</strong>中，单词的矢量表示如下</p><figure class="nh ni nj nk gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/97305c1dc9b4e812331fb5c70ad86a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*3aCGcAhnxSFg3FUNf-ECdA.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk translated">一次性编码矢量表示</figcaption></figure><p id="a143" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">直觉上我们知道<strong class="lt jl">享受</strong>和<strong class="lt jl">喜欢</strong>是一种相似的词。<strong class="lt jl">电影</strong>和<strong class="lt jl">享受</strong>之间的欧氏距离与<strong class="lt jl">享受<strong class="lt jl">和<strong class="lt jl">喜欢之间的欧氏距离相同。这是一个主要的缺点。</strong></strong></strong></p><h1 id="c35c" class="kz la jb bd lb lc ld le lf lg lh li lj kq lk kr ll kt lm ku ln kw lo kx lp lq bi translated">Word2vec矢量表示</h1><p id="e441" class="pw-post-body-paragraph lr ls jb lt b lu lv kl lw lx ly ko lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated"><strong class="lt jl"> Word2Vec </strong>是一种帮助我们实现相似单词的相似向量的方法。彼此相关的单词被映射到高维空间中彼此更接近的点。<strong class="lt jl"> Word2Vec </strong>方法有以下优点。</p><ul class=""><li id="0485" class="mn mo jb lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">Word2Vec 基于这样一个事实，即共享相似上下文的单词也共享语义。</li><li id="614a" class="mn mo jb lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt jl"> Word2vec </strong>模型通过使用其邻居来预测一个单词，通过学习称为嵌入的密集向量。</li><li id="f7de" class="mn mo jb lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt jl"> Word2vec </strong>计算效率也很高。</li><li id="255f" class="mn mo jb lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt jl"> Word2vec </strong>是一个无监督的模型，从原始文本中学习单词嵌入。</li><li id="0dd3" class="mn mo jb lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">Word2vec 有两种型号:T2 CBOW型号和T4 skip-gram型号。</li><li id="f67c" class="mn mo jb lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt jl"> Skip-gram: </strong>当使用输入(目标)单词预测周围单词(也称为上下文单词)时。</li><li id="9126" class="mn mo jb lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt jl"> CBOW(连续词袋):</strong>利用周围词(上下文词)预测目标词时。</li></ul><p id="dd94" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated"><strong class="lt jl"> Word2vec </strong>是一个三层神经网络，其中第一层为输入层，最后几层为输出层。中间层构建了一个潜在的表示，因此输入单词被转换成输出向量表示。</p><p id="99e8" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">在<strong class="lt jl"> Word2vec </strong>单词的向量表示中，我们可以发现单词向量之间有趣的数学关系。</p><p id="3159" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">国王——男人=王后——女人</p><p id="2503" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">在我们前面两个句子的例子中。</p><p id="b7f3" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">如果我们遵循<strong class="lt jl"> CBOW </strong>方法，将周围(上下文)单词作为输入，并尝试预测目标单词，那么输出将如下。</p><figure class="nh ni nj nk gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/90a2d13a07f031e37d9a7dd389631593.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*JJtVEyRbbq6zSnBzdIbqpw.png"/></div></figure><p id="36bb" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">输入和输出的向量形式如下所示。</p><figure class="nh ni nj nk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/259578c7d2d216e3a0afa84db45e06a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KLSaL2nZ-3pbiBYVKDmmA.png"/></div></div></figure><p id="2e84" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">在我们当前示例的神经网络中，在隐藏层中将有三个神经元，并且输出将有五个具有softmax函数的神经元，从而它将给出单词的概率。</p><figure class="nh ni nj nk gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/8fce7504e2d253642014957904163719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cwb7HUP8lm-XviIyB2nhNA.png"/></div></div></figure><p id="30a1" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated"><strong class="lt jl">伴随jupyter本帖的笔记本可以在</strong><a class="ae ns" href="https://github.com/nitwmanish/An-Intuitive-Introduction-Of-Word2Vec-By-Building-A-Word2Vec-From-Scratch" rel="noopener ugc nofollow" target="_blank"><strong class="lt jl">Github</strong></a><strong class="lt jl">上找到。</strong></p><h1 id="4572" class="kz la jb bd lb lc ld le lf lg lh li lj kq lk kr ll kt lm ku ln kw lo kx lp lq bi translated">结论</h1><p id="75ad" class="pw-post-body-paragraph lr ls jb lt b lu lv kl lw lx ly ko lz ma mb mc md me mf mg mh mi mj mk ml mm ij bi translated">经典方法的缺点是，它没有考虑单词在句子中出现的顺序，上下文丢失了。它假设文档中的单词相互独立。这种经典方法中的矢量表示导致数据稀疏。这个缺点可以通过<strong class="lt jl"> Word2vec </strong>矢量表示来克服。</p><p id="b1a4" class="pw-post-body-paragraph lr ls jb lt b lu mp kl lw lx mq ko lz ma nd mc md me ne mg mh mi nf mk ml mm ij bi translated">我希望这篇文章能帮助你理解 Word2vec，以及为什么我们更喜欢Word2vec而不是one-hot-encoding <em class="nt">。它至少提供了一个很好的解释，并在</em> one-hot-encoding和Word2vec矢量表示之间提供了一个高层次的比较<em class="nt">。</em></p></div></div>    
</body>
</html>