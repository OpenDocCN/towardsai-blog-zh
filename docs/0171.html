<html>
<head>
<title>Review: DUNet — Deformable U-Net for Retinal Vessels Segmentation (Biomedical Image Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:DUNet——用于视网膜血管分割的可变形U网(生物医学图像分割)</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/review-dunet-deformable-u-net-for-retinal-vessels-segmentation-biomedical-image-segmentation-fc0c2f72bb19?source=collection_archive---------0-----------------------#2019-10-04">https://pub.towardsai.net/review-dunet-deformable-u-net-for-retinal-vessels-segmentation-biomedical-image-segmentation-fc0c2f72bb19?source=collection_archive---------0-----------------------#2019-10-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0d69" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">DUNet | <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">对AI </a>的技术回顾</h2><div class=""/><div class=""><h2 id="0819" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><a class="ae ko" href="https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------" rel="noopener" target="_blank">优网</a> + <a class="ae ko" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN </a>，胜过<a class="ae ko" href="https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------" rel="noopener" target="_blank">优网</a> &amp; <a class="ae ko" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN </a></h2></div><p id="35ec" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi ll translated"><span class="l lm ln lo bm lp lq lr ls lt di">在这个故事中，<strong class="kr ja"> DUNet </strong>，由天津大学、林雪平大学和。</span></p><p id="da76" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">可变形U形网:</p><ul class=""><li id="4a9f" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">利用视网膜血管的<strong class="kr ja">局部特征</strong>的U形架构，用上采样算子提取<strong class="kr ja">上下文信息</strong>。</li><li id="b230" class="lu lv iq kr b ks md kv me ky mf lc mg lg mh lk lz ma mb mc bi translated">将低层特征地图与高层特征地图相结合，实现<strong class="kr ja">精确定位</strong>。</li><li id="bef2" class="lu lv iq kr b ks md kv me ky mf lc mg lg mh lk lz ma mb mc bi translated">使用可变形卷积网络(<a class="ae ko" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN </a>)根据血管的尺度和形状通过<strong class="kr ja">自适应地调整感受野</strong>来捕捉各种形状和尺度的视网膜血管。</li></ul><p id="4340" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">有了DUNet，就有可能对疾病进行早期诊断。发表于<strong class="kr ja"> 2019 JKNOSYS </strong>(当前影响因子:5.101)。(<a class="mi mj ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----fc0c2f72bb19--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h1 id="0b64" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">概述</h1><ol class=""><li id="ec46" class="lu lv iq kr b ks nj kv nk ky nl lc nm lg nn lk no ma mb mc bi translated"><strong class="kr ja"> DUNet架构</strong></li><li id="ae32" class="lu lv iq kr b ks md kv me ky mf lc mg lg mh lk no ma mb mc bi translated"><strong class="kr ja">实验结果</strong></li></ol></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h1 id="45cd" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">1.<strong class="ak"> DUNet </strong>架构</h1><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi np"><img src="../Images/54fb3eb425e1567ba3b2938d46ac0dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PgOnQyVI4wVkU9mf5Yjaew.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk translated"><strong class="bd mt"> DUNet架构</strong></figcaption></figure><ul class=""><li id="a352" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">DUNet网络架构如上图所示。</li><li id="8ab0" class="lu lv iq kr b ks md kv me ky mf lc mg lg mh lk lz ma mb mc bi translated">该架构由<a class="ae ko" href="https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------" rel="noopener" target="_blank"> U-Net </a>框架中的<strong class="kr ja">卷积编码器(左侧)</strong>和<strong class="kr ja">解码器(右侧)</strong>组成。</li><li id="cf80" class="lu lv iq kr b ks md kv me ky mf lc mg lg mh lk lz ma mb mc bi translated">在每个编码和解码阶段，<strong class="kr ja">可变形卷积块</strong>用于<strong class="kr ja">通过学习局部、密集和自适应感受野</strong>对各种形状和尺度的视网膜血管建模。</li></ul><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi of"><img src="../Images/3eee3928b4e227bfee28974413c73e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*lA4ZL9Y151lc5EzPXNYsnA.png"/></div><figcaption class="ob oc gj gh gi od oe bd b be z dk translated"><strong class="bd mt">可变形卷积网络</strong></figcaption></figure><ul class=""><li id="074e" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">每个可变形卷积块由卷积偏移层、卷积层、批量归一化层和激活层组成，卷积偏移层是可变形卷积的核心概念。</li></ul><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi og"><img src="../Images/e2652481089b6add86a0a033d222c668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*sauss-gzYDqcPpXNIXselg.png"/></div></figure><ul class=""><li id="10c7" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">没有<a class="ae ko" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN </a>，对于正常卷积中的每个网格点(绿色)，感受野是固定的:</li></ul><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7012b065285e5a4ce4ca99c4536eafa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*qbm2QTR_jGXxoYrQSAI7jg.png"/></div></figure><ul class=""><li id="432c" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">用<a class="ae ko" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN </a>，对于正常卷积中的每个格点(蓝色)，δ<em class="oi">mi</em>(δ<em class="oi">Xi</em>和δ<em class="oi">易)</em>被学习到具有自适应感受野:</li></ul><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/594dfb3a50aa010b0dd2ebd389ea9f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*NRkPT0CIS4GaCkf0oinFYw.png"/></div></figure><ul class=""><li id="ac49" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">(如果有兴趣，请阅读我对<a class="ae ko" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN </a>的评论。)</li><li id="d42a" class="lu lv iq kr b ks md kv me ky mf lc mg lg mh lk lz ma mb mc bi translated">在DUNet的底部，我们使用正常的卷积层而不是可变形块，因为大量的参数将被引入而没有实质的性能改进。</li></ul></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h1 id="7540" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">2.实验结果</h1><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi ok"><img src="../Images/e3d69394feddbf8718a5d473b98133c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I1i2Xav5Awr3CETNaCHlSg.png"/></div></div></figure><ul class=""><li id="4ff6" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">在上述3个数据集上，DUNet的表现优于<a class="ae ko" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN </a>和<a class="ae ko" href="https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------" rel="noopener" target="_blank">优网</a>。</li></ul><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi ol"><img src="../Images/a3e365608557bb41eed3d04c447d0605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OyRgTfMxKDuCXaZi_5Kymw.png"/></div></div></figure><ul class=""><li id="d10a" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">DUNet只需要0.88M个参数，比<a class="ae ko" href="https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------" rel="noopener" target="_blank"> U-Net </a>少。</li></ul><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi om"><img src="../Images/117018a47fe29328c76b73fe439ab18c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vWF8WBm-jIFkTvC58CBKAg.png"/></div></div></figure><ul class=""><li id="16aa" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">与其他方法相比，DUNet获得了最好的或相当的性能。</li></ul><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi on"><img src="../Images/0619ae39ca357c4198f91125bc451efa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ct8xeePKGoc_OCBQx4QXoQ.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk translated"><strong class="bd mt">第一列:眼底图像。第二栏:地面真相。第三列:DUNet生成的分割结果。</strong></figcaption></figure><ul class=""><li id="8f60" class="lu lv iq kr b ks kt kv kw ky lw lc lx lg ly lk lz ma mb mc bi translated">高分辨率HRF数据集也如上所述进行测试。</li><li id="846f" class="lu lv iq kr b ks md kv me ky mf lc mg lg mh lk lz ma mb mc bi translated">DUNet可能无法分割一些粗血管(黄色圆圈)，这些结果可能与用于训练的低分辨率补片有关。</li><li id="31a4" class="lu lv iq kr b ks md kv me ky mf lc mg lg mh lk lz ma mb mc bi translated">相反，DUNet成功地分割了微血管(蓝色圆圈)。</li></ul></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><p id="3527" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">通过结合DCN和T2的U-Net，组成了DUNet。(文中显示了很多结果。请放心自己看。谢谢:)</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h1 id="27ee" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">参考</h1><p id="fbb9" class="pw-post-body-paragraph kp kq iq kr b ks nj ka ku kv nk kd kx ky oo la lb lc op le lf lg oq li lj lk ij bi translated">【2019 JKNOSYS】【DUNet】<br/><a class="ae ko" href="https://www.sciencedirect.com/science/article/pii/S0950705119301984#aep-article-footnote-id1" rel="noopener ugc nofollow" target="_blank">DUNet:用于视网膜血管分割的可变形网络</a></p><h1 id="1d53" class="mr ms iq bd mt mu or mw mx my os na nb kf ot kg nd ki ou kj nf kl ov km nh ni bi translated">我以前的评论</h1><p id="670f" class="pw-post-body-paragraph kp kq iq kr b ks nj ka ku kv nk kd kx ky oo la lb lc op le lf lg oq li lj lk ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(但)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(想)(要)(让)(这)(些)(人)(都)(有)(这)(些)(情)(况)(,)(我)(们)(还)(不)(想)(要)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(就)(是)(这)(些)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(了)(,)(我)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。</p><p id="036e" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">物体检测</strong> [ <a class="ae ko" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------" rel="noopener">过食</a> ] [ <a class="ae ko" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a> ] [ <a class="ae ko" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------" rel="noopener">快R-CNN </a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快R-CNN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a>][<a class="ae ko" href="https://towardsdatascience.com/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=post_page---------------------------" rel="noopener" target="_blank">CRAFT</a>][<a class="ae ko" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank">R-FCN</a>][<a class="ae ko" href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------" rel="noopener" target="_blank">离子</a><a class="ae ko" href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------" rel="noopener" target="_blank"> [</a><a class="ae ko" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank">G-RMI</a>][<a class="ae ko" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------" rel="noopener">TDM</a>][<a class="ae ko" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae ko" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>][<a class="ae ko" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolo v1</a>][<a class="ae ko" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolo v2/yolo 9000</a>][<a class="ae ko" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolo v3</a>][<a class="ae ko" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank">FPN</a>[<a class="ae ko" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank">retina net</a>[<a class="ae ko" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank">DCN</a></p><p id="6080" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">)(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。 [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]</p><p id="4644" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">)(他)(们)(都)(不)(是)(真)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)(实)( )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="1233" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">实例分割</strong> [ <a class="ae ko" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b?source=post_page---------------------------" rel="noopener"> SDS </a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-hypercolumn-instance-segmentation-367180495979?source=post_page---------------------------" rel="noopener" target="_blank">超列</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339?source=post_page---------------------------" rel="noopener" target="_blank">深度掩码</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61?source=post_page---------------------------" rel="noopener" target="_blank">清晰度掩码</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------" rel="noopener" target="_blank">多路径网络</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34?source=post_page---------------------------" rel="noopener" target="_blank"> MNC </a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92?source=post_page---------------------------" rel="noopener" target="_blank">实例中心</a> ] [ <a class="ae ko" href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=post_page---------------------------" rel="noopener" target="_blank"> FCIS </a></p><p id="03fc" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="ef79" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><a class="ae ko" href="https://towardsdatascience.com/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------" rel="noopener" target="_blank"> </a><a class="ae ko" href="https://towardsdatascience.com/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=post_page---------------------------" rel="noopener" target="_blank"> 汤普森 NIPS'14 </a> 汤普森 CVPR'15  汤普森 CVPR'15  汤普森 CVPR'15 <a class="ae ko" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener"> CPM </a></p><p id="c37b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">编解码后期处理</strong> [ <a class="ae ko" href="https://towardsdatascience.com/review-arcnn-deblocking-denoising-a098deeb792" rel="noopener" target="_blank"> ARCNN </a> ] [ <a class="ae ko" href="https://medium.com/@sh.tsang/review-cnn-for-h-264-hevc-compressed-image-deblocking-codec-post-processing-361a84e65b94" rel="noopener">林DCC'16 </a> ] [ <a class="ae ko" href="https://medium.com/@sh.tsang/review-ifcnn-in-loop-filtering-using-convolutional-neural-network-codec-post-processing-1b89c8ddf417" rel="noopener"> IFCNN </a> ] [ <a class="ae ko" href="https://medium.com/@sh.tsang/review-cnn-for-compressed-image-deblocking-deblocking-44508bf99bdc" rel="noopener">李ICME ' 17</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-vrcnn-variable-filter-size-residue-learning-cnn-codec-post-processing-4a8a337ea73c" rel="noopener">VR CNN</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-dcad-deep-cnn-based-auto-decoder-codec-post-processing-e05a8f15f0c6" rel="noopener">DCAD</a>][<a class="ae ko" href="https://medium.com/@sh.tsang/review-ds-cnn-decode-side-scalable-cnn-codec-post-processing-4bd85a4cfcfd" rel="noopener">DS-CNN</a>]</p><p id="c582" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">生成对抗网络</strong><a class="ae ko" href="https://medium.com/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75" rel="noopener">甘</a></p></div></div>    
</body>
</html>