<html>
<head>
<title>Back Translation in Text Augmentation by nlpaug</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">nlpaug文本增强中的反向翻译</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/back-translation-in-text-augmentation-by-nlpaug-d65518dd092f?source=collection_archive---------1-----------------------#2020-08-29">https://pub.towardsai.net/back-translation-in-text-augmentation-by-nlpaug-d65518dd092f?source=collection_archive---------1-----------------------#2020-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="e20a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="dfd6" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">NLP的数据扩充——由反向翻译用4行代码生成合成数据</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2c34b2f18955b5af869ae5706fae5f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X9BKMxcAKflJQBp_"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae le" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><blockquote class="lf lg lh"><p id="aafd" class="li lj lk ll b lm ln ka lo lp lq kd lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">英语是具有大量翻译训练数据的语言之一，而某些语言可能没有足够的数据来训练机器翻译模型。Sennrich等人使用反向翻译方法来生成更多的训练数据，以提高翻译模型的性能。</p><p id="05bc" class="li lj lk ll b lm ln ka lo lp lq kd lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">假设我们要训练一个翻译英语(源语言)→粤语(目标语言)的模型，而粤语没有足够的训练数据。回译是将目标语言翻译成源语言，并混合源语句和回译语句来训练模型。因此可以增加从源语言到目标语言的训练数据的数量。</p></blockquote><p id="fa29" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mf lt lu lv mg lx ly lz mh mb mc md me ij bi translated">在之前的<a class="ae le" href="https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28" rel="noopener" target="_blank">故事</a>中，提到了反向翻译方法为NLP任务生成合成数据。这样，我们可以进行更多的模型训练，特别是针对低资源NLP任务和语言的训练。</p><p id="3307" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mf lt lu lv mg lx ly lz mh mb mc md me ij bi translated">这个故事将涵盖脸书人工智能研究(<a class="ae le" href="https://ai.facebook.com/research/" rel="noopener ugc nofollow" target="_blank"> FAIR </a>)团队如何训练翻译模型，以及我们如何利用预训练模型为您的模型生成更多训练数据。通过利用<a class="ae le" href="https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46" rel="noopener">子字模型</a>、大型<a class="ae le" href="https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Fdata-augmentation-in-nlp-2801a34dfc28" rel="noopener">反向翻译</a>和模型组装，ng等人(2019)获得WMT 19奖。他们研究了两对语言和四个语言方向，分别是英语← →德语(EN ← → DE)和英语← →俄语(EN ← →RU)。他们演示了如何使用反向翻译来提高模型性能。在那之后，我将展示如何通过使用回译来编写几行代码来生成合成数据。以下是关于数据处理、数据扩充和翻译模型的一些细节。</p><h1 id="6ca6" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">数据处理</h1><h2 id="9f93" class="na mj iq bd mk nb nc dn mo nd ne dp ms mf nf ng mu mg nh ni mw mh nj nk my iw bi translated">子词</h2><p id="6459" class="pw-post-body-paragraph li lj iq ll b lm nl ka lo lp nm kd lr mf nn lu lv mg no ly lz mh np mc md me ij bi translated">在NLP的前期，<a class="ae le" href="https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a" rel="noopener" target="_blank">单词级</a>和<a class="ae le" href="https://towardsdatascience.com/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10" rel="noopener" target="_blank">字符级</a>令牌用于训练一个模型。在最先进的NLP模型中，子词(在词和字符级别之间)是标记化阶段的标准方式。例如，由于出现频率，它使用“trans”和“lation”来表示“translation”。你可以在这里看看<a class="ae le" href="https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46" rel="noopener">的3种不同的子词算法。Ng等人选择bye对编码(BPE ),分别对EN←→DE和EN← →RU标记化进行32K和24次拆分操作。</a></p><h2 id="3dda" class="na mj iq bd mk nb nc dn mo nd ne dp ms mf nf ng mu mg nh ni mw mh nj nk my iw bi translated">数据过滤</h2><p id="dea1" class="pw-post-body-paragraph li lj iq ll b lm nl ka lo lp nm kd lr mf nn lu lv mg no ly lz mh np mc md me ij bi translated">为了确保只有正确语言的句子对，Ng等人使用<a class="ae le" href="https://github.com/saffsd/langid.py" rel="noopener ugc nofollow" target="_blank"> langid </a> (Lui et al .，2012)过滤掉无效数据。langid是一个语言识别工具，它告诉你文本属于哪种语言。</p><p id="caf9" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mf lt lu lv mg lx ly lz mh mb mc md me ij bi translated">如果句子包含超过250个标记或者源和目标之间的长度比超过1.5，那么它将被排除在模型训练之外。我怀疑它可能会向模型中引入过多的噪声信息。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ccbac1368907ac9ff6370af9418b2128.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*w5Bt44rigJsAdLXcf-tKCw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">不同过滤方法的数据大小(ng等人，2019年)</figcaption></figure><p id="73c6" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mf lt lu lv mg lx ly lz mh mb mc md me ij bi translated">第三种过滤方式是针对单语数据。为了保持高质量的单语数据，Ng等人采用了<a class="ae le" href="https://www.aclweb.org/anthology/P10-2041.pdf" rel="noopener ugc nofollow" target="_blank"> Moore-Lewis方法</a> (2010)从较大的语料库中去除噪声数据。简而言之，Moore和Lewis通过源数据语言模型和更大的语料库语言模型的差异对文本进行评分。在挑选了高质量的语料库之后，它将使用反向翻译模型来为翻译模型生成一对训练数据。</p><h1 id="456b" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">数据扩充</h1><h2 id="7bb5" class="na mj iq bd mk nb nc dn mo nd ne dp ms mf nf ng mu mg nh ni mw mh nj nk my iw bi translated">回译</h2><p id="97be" class="pw-post-body-paragraph li lj iq ll b lm nl ka lo lp nm kd lr mf nn lu lv mg no ly lz mh np mc md me ij bi translated">在从更大的单语语料库中过滤低质量的数据之后，它准备好训练中间的目标-源模型。从实验来看，基于回译数据训练的集成模型比单个模型更好。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/7a7a9e68eea992daf963655760aff03e.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*ytDubhrRXffxdJJH5n4crg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">单个模型和集合在英语到俄语的SacreBLEU上的比较(Ng等人，2019年)</figcaption></figure><h1 id="361f" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">翻译模型</h1><p id="f22b" class="pw-post-body-paragraph li lj iq ll b lm nl ka lo lp nm kd lr mf nn lu lv mg no ly lz mh np mc md me ij bi translated">和往常一样，FAIRSEQ采用了<a class="ae le" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">变压器架构</a> (Vaswani et al .，2017)。转换器利用多个注意力网络来计算表示。关于Transformer架构的更多信息，您可以访问这篇<a class="ae le" href="https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b" rel="noopener" target="_blank">文章</a>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b55095223b9eb3d8aab7714fc3deb29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/0*9IHhdWezPfYng76g.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk translated">变压器架构(拉德福德等人，2018年)</figcaption></figure><p id="ff3d" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mf lt lu lv mg lx ly lz mh mb mc md me ij bi translated">另一项技能是利用多个训练模型来形成用于预测的集成模型。</p><h2 id="b3fd" class="na mj iq bd mk nb nc dn mo nd ne dp ms mf nf ng mu mg nh ni mw mh nj nk my iw bi translated">微调</h2><p id="805a" class="pw-post-body-paragraph li lj iq ll b lm nl ka lo lp nm kd lr mf nn lu lv mg no ly lz mh np mc md me ij bi translated">在对过滤和回译数据进行培训后，ng等人通过使用前一年的数据集(如newstest2012和newstest2013)来利用该模型。</p><h1 id="de6e" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">由反向翻译生成合成数据</h1><p id="c69a" class="pw-post-body-paragraph li lj iq ll b lm nl ka lo lp nm kd lr mf nn lu lv mg no ly lz mh np mc md me ij bi translated"><a class="ae le" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank"> nlpaug </a>提供了一种简单的方法，通过4行代码生成合成数据。</p><p id="80a3" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mf lt lu lv mg lx ly lz mh mb mc md me ij bi translated">在幕后，<a class="ae le" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank"> nlpaug </a>利用来自<a class="ae le" href="https://github.com/pytorch/fairseq" rel="noopener ugc nofollow" target="_blank"> fairseq </a>(由脸书AI Research发布)的预训练模型执行2次翻译。以下面的例子为例，它将源输入(英语)翻译成德语。之后，向第二个模型提供翻译后的文本(德语，第一个模型的输出)，它将输出翻译后的文本(英语)。代码如下:</p><pre class="kp kq kr ks gt nt nu nv nw aw nx bi"><span id="cac4" class="na mj iq nu b gy ny nz l oa ob"><strong class="nu ja">import</strong> <strong class="nu ja">nlpaug.augmenter.word</strong> <strong class="nu ja">as</strong> <strong class="nu ja">naw</strong><br/><br/>text = 'The <strong class="nu ja"><em class="lk">quick</em></strong> brown fox jumped over the lazy dog'<br/>back_translation_aug = naw.BackTranslationAug(<br/>    from_model_name='transformer.wmt19.en-de',<br/>    to_model_name='transformer.wmt19.de-en')<br/>back_translation_aug.augment(text)</span></pre><p id="c6bb" class="pw-post-body-paragraph li lj iq ll b lm ln ka lo lp lq kd lr mf lt lu lv mg lx ly lz mh mb mc md me ij bi translated">举上面的例子，可以把“敏捷的<strong class="ll ja"><em class="lk"/></strong>棕狐狸跳过懒狗”改成“敏捷的<strong class="ll ja"><em class="lk"/></strong>棕狐狸跳过懒狗”</p><h1 id="9339" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">延伸阅读</h1><ul class=""><li id="3e9b" class="oc od iq ll b lm nl lp nm mf oe mg of mh og me oh oi oj ok bi translated">文本增强库(<a class="ae le" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank"> nlpaug </a>)</li><li id="1fb4" class="oc od iq ll b lm ol lp om mf on mg oo mh op me oh oi oj ok bi translated">预培训翻译库(<a class="ae le" href="https://github.com/pytorch/fairseq" rel="noopener ugc nofollow" target="_blank"> fairseq </a></li><li id="0d69" class="oc od iq ll b lm ol lp om mf on mg oo mh op me oh oi oj ok bi translated"><a class="ae le" href="https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28" rel="noopener" target="_blank">自然语言处理中的数据扩充</a></li><li id="6529" class="oc od iq ll b lm ol lp om mf on mg oo mh op me oh oi oj ok bi translated"><a class="ae le" href="https://towardsdatascience.com/data-augmentation-library-for-text-9661736b13ff" rel="noopener" target="_blank">文本数据扩充库</a></li></ul><h1 id="46a9" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">关于我</h1><p id="d2d7" class="pw-post-body-paragraph li lj iq ll b lm nl ka lo lp nm kd lr mf nn lu lv mg no ly lz mh np mc md me ij bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新发展。你可以通过<a class="ae le" href="https://medium.com/@makcedward/" rel="noopener"> Medium、</a> <a class="ae le" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae le" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="c326" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">参考</h1><ul class=""><li id="ee06" class="oc od iq ll b lm nl lp nm mf oe mg of mh og me oh oi oj ok bi translated">米（meter的缩写））路易和鲍德温。<a class="ae le" href="https://www.aclweb.org/anthology/P12-3005.pdf" rel="noopener ugc nofollow" target="_blank"> langid.py:一个现成的语言识别工具</a>。2012</li><li id="d6d3" class="oc od iq ll b lm ol lp om mf on mg oo mh op me oh oi oj ok bi translated">瓦斯瓦尼、沙泽尔、帕尔马、乌兹科雷特、琼斯、戈麦斯和凯泽..注意是你所需要的一切。2017</li><li id="6adf" class="oc od iq ll b lm ol lp om mf on mg oo mh op me oh oi oj ok bi translated">南Edunov，M Ott，M Auli和D. Grangierv。<a class="ae le" href="http://Understanding Back-Translation at Scale" rel="noopener ugc nofollow" target="_blank">理解尺度上的回译</a>。2018</li><li id="a10a" class="oc od iq ll b lm ol lp om mf on mg oo mh op me oh oi oj ok bi translated">名词（noun的缩写）Ng、K. Yee、A. Baevski、M. Ott、M. Auli和S Edunov。<a class="ae le" href="https://arxiv.org/pdf/1907.06616.pdf" rel="noopener ugc nofollow" target="_blank">脸书博览会WMT19新闻翻译任务提交</a>。2019</li></ul></div></div>    
</body>
</html>