<html>
<head>
<title>GAN Training Breakthrough for Limited Data Applications &amp; New NVIDIA Program! NVIDIA Research</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有限数据应用的GAN培训突破&amp;新NVIDIA计划！英伟达研究</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/gan-training-breakthrough-for-limited-data-applications-new-nvidia-program-nvidia-research-3652c4c172e6?source=collection_archive---------2-----------------------#2020-12-08">https://pub.towardsai.net/gan-training-breakthrough-for-limited-data-applications-new-nvidia-program-nvidia-research-3652c4c172e6?source=collection_archive---------2-----------------------#2020-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="82ab" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>，<a class="ae ep" href="https://towardsai.net/p/category/news" rel="noopener ugc nofollow" target="_blank">新闻</a>，<a class="ae ep" href="https://towardsai.net/p/category/technology" rel="noopener ugc nofollow" target="_blank">科技</a></h2><div class=""/><div class=""><h2 id="0e7e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过NVIDIA开发的这种新的训练方法，你可以用十分之一的图像训练出一个强大的生成模型！使得许多无法访问如此多图像的应用成为可能！</h2></div><blockquote class="kr ks kt"><p id="168b" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="it">原载于</em><a class="ae lr" href="https://www.louisbouchard.ai/nvidia-ada/" rel="noopener ugc nofollow" target="_blank"><em class="it">louisbouchard . ai</em></a><em class="it">，前两天看了我的博客</em><a class="ae lr" href="https://www.louisbouchard.ai/tag/artificial-intelligence/" rel="noopener ugc nofollow" target="_blank"><em class="it"/></a><em class="it">！</em></p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/570a3edd4020a6e4c7a451909f311fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nIO9oMz9uXd3HG-ZQIg4xA.png"/></div></div></figure><p id="241e" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这篇新论文介绍了一种训练GAN架构的技术。它们被用在许多与计算机视觉相关的应用程序中，在这些应用程序中，我们希望按照特定的风格生成图像的逼真变换。如果你不熟悉GANs是如何工作的，我强烈建议你在继续之前看一下我做的解释它的视频。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mh mi l"/></div></figure><p id="74db" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">如你所知，甘斯的建筑是以对抗的方式训练的。这意味着同时存在两个网络训练，一个训练用于从输入、生成器生成变换图像，另一个训练用于将生成的图像与训练图像的基本事实区分开。这些训练图像的基础事实正是我们希望对每个输入图像实现的变换结果。然后，我们试图同时优化这两个网络，从而使生成器越来越好地生成看起来真实的图像。但是，为了产生这些伟大而现实的结果，我们需要两样东西。由成千上万的图像组成的训练数据集，并在过度拟合之前停止训练。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mj"><img src="../Images/e268461230fd57af601a49588bab42a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z9DaJHOEICJJQ_nSspbsow.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">图片via:<a class="ae lr" href="https://arxiv.org/abs/2006.06676" rel="noopener ugc nofollow" target="_blank">NVIDIA</a>用有限数据训练生成性对抗网络</figcaption></figure><p id="81af" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">GAN训练期间的过度拟合将意味着我们的鉴别器的反馈将变得毫无意义，并且生成的图像只会变得更差。当你训练你的网络超过你的数据量时，就会发生这种情况，质量只会变得更差，正如你在黑点之后看到的。</p><p id="c026" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这些是NVIDIA通过这篇论文解决的问题。他们意识到这基本上是同一个问题，可以通过一个解决方案来解决。他们提出了一种他们称之为适应性鉴别器增强的方法。他们的方法在理论上非常简单，您可以将其应用于您已经拥有的任何GAN架构，而无需进行任何更改。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mo"><img src="../Images/7908834490c0af9fa61a4493fb3ed073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ypNa-fgMaPiqq3jzsaDUA.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">图片via:<a class="ae lr" href="https://arxiv.org/abs/2006.06676" rel="noopener ugc nofollow" target="_blank">NVIDIA用有限的数据训练生成性对抗网络</a></figcaption></figure><p id="5e99" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">正如你可能知道的，在深度学习的大多数领域，我们执行我们所谓的数据增强来对抗过度拟合。在计算机视觉中，通常在训练阶段对图像进行变换，以增加训练数据的数量。这些变换可以是应用旋转、添加噪声、改变颜色等等。来修改我们的输入图像并创建一个独特的版本。让我们的网络在更加多样化的数据集上训练，而不必创建或找到更多的图像。不幸的是，这不能容易地应用于GAN架构，因为生成器将学习按照这些相同的增强来生成图像。这就是NVIDIA的团队所做的。他们找到了一种方法来使用这些增强来防止模型过度拟合，同时确保这些增强不会泄露到生成的图像上。他们基本上将这组图像增强应用于显示给鉴别器的所有图像，其中每个变换随机发生的概率是选定的，并且使用这些修改的图像来评估鉴别器的性能。这种大量的随机应用的变换使得鉴别器不太可能看到甚至一个未改变的图像。当然，生成器经过训练和指导，只生成干净的图像，而不进行任何变换。他们的结论是，只有当每个变换的发生概率低于80%时，这种通过向鉴别器显示增强数据来训练GAN架构的方法才有效。它越高，应用的扩充就越多，因此您将拥有更多样化的训练数据集。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mp"><img src="../Images/e48dab619e12f53ee85cb03a47da10fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JDCc3ETzkdKtdj4IGheGwQ.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">图片via:<a class="ae lr" href="https://arxiv.org/abs/2006.06676" rel="noopener ugc nofollow" target="_blank">NVIDIA用有限的数据训练生成性对抗网络</a></figcaption></figure><p id="4095" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">他们发现，虽然这解决了训练图像数量有限的问题，但根据初始数据集的大小，仍然存在在不同时间出现的过度拟合问题。这就是为什么他们想出了一种适应性的方法来做这种增强。不是用另一个超参数来决定出现的理想增强概率，而是在训练期间控制增强强度。从0开始，然后根据训练集和验证集之间的差异迭代调整其值。指示过度拟合是否正在发生。该验证集只是相同类型图像的不同集，网络没有被训练。验证集只需要由鉴别器以前没有见过的图像组成。它用于测量我们结果的质量，量化我们网络的分歧程度，同时量化过度拟合。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mq"><img src="../Images/7f4f6309c43ddc53f0ac4fc12f963561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bwGXMB96EQRZQ4f6j2baTQ.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">图片via:<a class="ae lr" href="https://arxiv.org/abs/2006.06676" rel="noopener ugc nofollow" target="_blank">NVIDIA用有限的数据训练生成性对抗网络</a></figcaption></figure><p id="5ebd" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">在这里，您可以在FFHQ数据集上看到这种针对多个训练集大小的适应性鉴别器增强的结果。在这里，我们使用FID测量，你可以看到随着时间的推移越来越好，永远不会达到这种过度拟合问题，只会变得更糟。FID或弗雷歇初始距离基本上是生成图像和真实图像的分布之间的距离的度量。它测量生成的图像样本的质量。它越低，我们的结果越好。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mr"><img src="../Images/977e247eabb7a6865a65f399a5528bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zRv3ePPLGaxTn5OXiggrmA.png"/></div></div></figure><p id="e817" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">这个FFHQ数据集包含70 000张来自Flickr的高质量人脸。它是作为生成性对抗网络的基准而创建的。事实上，他们成功地匹配了StyleGAN2的结果，使用了数量级更少的图像，正如你在这里看到的。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ms"><img src="../Images/fe9a4b463115c8fe05abc92a425e3acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ALiL6Hhff2MOpcTo4RL_6A.png"/></div></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk translated">图片via:<a class="ae lr" href="https://arxiv.org/abs/2006.06676" rel="noopener ugc nofollow" target="_blank">NVIDIA用有限的数据训练生成性对抗网络</a></figcaption></figure><p id="320d" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">其中对1 000到140 000个训练示例绘制结果，再次使用FFHQ数据集上的相同FID测量。</p><h2 id="7feb" class="mt mu it bd mv mw mx dn my mz na dp nb me nc nd ne mf nf ng nh mg ni nj nk iz bi translated">观看视频，了解这种新训练方法的更多示例:</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mh mi l"/></div></figure><h2 id="be70" class="mt mu it bd mv mw mx dn my mz na dp nb me nc nd ne mf nf ng nh mg ni nj nk iz bi translated">结论</h2><p id="8cc1" class="pw-post-body-paragraph ku kv it kx b ky nl kd la lb nm kg ld me nn lg lh mf no lk ll mg np lo lp lq im bi translated">当然，代码也是完全可用的，并且很容易使用TensorFlow实现到您的GAN架构中。如果您想在自己的代码中实现这一点，或者想通过阅读本文对这一技术有更深入的理解，下面的参考资料中链接了代码和论文。这篇论文刚刚发表在NeurIPS 2020上，以及NVIDIA的另一份公告。他们宣布了一个名为应用研究加速器计划的新项目。他们的目标是支持研究项目，通过部署到商业和政府组织采用的GPU加速应用程序中来产生实际影响。向学生提供硬件、资金、技术指导、支持等。如果这符合你目前的需求，你一定要看一看，我在视频的描述中也链接了它！</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="8117" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld me lf lg lh mf lj lk ll mg ln lo lp lq im bi translated">如果你喜欢我的工作并想支持我，我会非常感谢你在我的社交媒体频道上关注我:</p><ul class=""><li id="fcbb" class="nx ny it kx b ky kz lb lc me nz mf oa mg ob lq oc od oe of bi translated">支持我的最好方式就是跟随我上<a class="ae lr" href="https://medium.com/@whats_ai" rel="noopener"><strong class="kx jd"/></a>。</li><li id="f965" class="nx ny it kx b ky og lb oh me oi mf oj mg ok lq oc od oe of bi translated">订阅我的<a class="ae lr" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank">T5】YouTube频道T7】。</a></li><li id="7061" class="nx ny it kx b ky og lb oh me oi mf oj mg ok lq oc od oe of bi translated">在<a class="ae lr" href="https://www.linkedin.com/company/what-is-artificial-intelligence" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> LinkedIn </strong> </a>上关注我的项目</li><li id="fefd" class="nx ny it kx b ky og lb oh me oi mf oj mg ok lq oc od oe of bi translated">一起学习AI，加入我们的<a class="ae lr" href="https://discord.gg/SVse4Sr" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> Discord社区</strong> </a>，<em class="kw">分享你的项目、论文、最佳课程，寻找Kaggle队友，等等！</em></li></ul></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h2 id="2e63" class="mt mu it bd mv mw mx dn my mz na dp nb me nc nd ne mf nf ng nh mg ni nj nk iz bi translated">参考</h2><p id="6108" class="pw-post-body-paragraph ku kv it kx b ky nl kd la lb nm kg ld me nn lg lh mf no lk ll mg np lo lp lq im bi translated"><strong class="kx jd">NVIDIA利用有限数据训练生成性对抗网络</strong>。发表在NeurIPS 2020会议上。<a class="ae lr" href="https://arxiv.org/abs/2006.06676" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2006.06676</a><br/>T23】ADA—GitHub同代码。<a class="ae lr" href="https://github.com/NVlabs/stylegan2-ada" rel="noopener ugc nofollow" target="_blank">https://github.com/NVlabs/stylegan2-ada</a><br/><strong class="kx jd">英伟达的应用研究计划。</strong><a class="ae lr" href="https://www.nvidia.com/accelerateresearch/" rel="noopener ugc nofollow" target="_blank">https://www.nvidia.com/accelerateresearch/</a></p></div></div>    
</body>
</html>