<html>
<head>
<title>Which NLP Task Does NOT Benefit From Pre-trained Language Models?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">哪项自然语言处理任务没有受益于预先训练的语言模型？</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/which-nlp-task-does-not-benefit-from-pre-trained-language-models-90430ed1207e?source=collection_archive---------0-----------------------#2022-08-18">https://pub.towardsai.net/which-nlp-task-does-not-benefit-from-pre-trained-language-models-90430ed1207e?source=collection_archive---------0-----------------------#2022-08-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="33e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">预训练的通用语言表示模型有着如此悠久的历史，产生了巨大的影响，以至于我们理所当然地认为它们是所有NLP任务完全100%必要的基础。有两个独立的阶跃函数创新推动了所有NLP任务的准确性:(1)像Word2Vec和GloVe这样的统计语言模型，以及最近的(2)像<a class="ae kl" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a>、<a class="ae kl" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank"> ELMo </a>和最近的<a class="ae kl" href="https://huggingface.co/bigscience/bloom" rel="noopener ugc nofollow" target="_blank"> BLOOM </a>这样的神经语言模型。在建模工作流的开始插入预先训练好的神经语言模型<strong class="jp ir"> <em class="km">几乎</em> </strong>保证能提高性能，但至少有一种情况不能。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kn"><img src="../Images/c303e4f99cfe66e7c24c26c16eacb631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*_ERKOXAe1WKiKvY8SBGQ9Q.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">侧边栏:为什么是芝麻街主题？！</figcaption></figure><h1 id="a7a2" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">命名实体识别(NER)</h1><p id="3ec1" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">只需看看最初的BERT论文，标题为“<a class="ae kl" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a>”，即可在第5节中看到预训练BERT嵌入如何提高NER性能的详细分析。下面的BERT图显示了一个典型的机器学习工作流，它利用任何语言模型来完成一般的NLP任务。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/b71c4c7d82827a9d5afef29b093d01fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*n2ckqoZ-PXgtd8VcZODncg.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">来源:<a class="ae kl" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.04805.pdf</a>—BERT整体预培训和微调流程</figcaption></figure><p id="0e69" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">论文还显示了在问答(QA)和自然语言理解(NLU)的大杂烩任务<a class="ae kl" href="https://openreview.net/pdf?id=rJ4km2R5t7" rel="noopener ugc nofollow" target="_blank"> GLUE </a>上的显著进步。</p><h1 id="1294" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">实体消歧</h1><p id="2635" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">全球ED任务还使用BERT在多个数据集上取得了新的最先进的结果。请参见本“<a class="ae kl" href="https://arxiv.org/pdf/1909.00426.pdf" rel="noopener ugc nofollow" target="_blank">使用BERT </a>进行全局实体消歧”的相关工作部分，了解将BERT用作ed预处理步骤的各种工作流程的概要。</p><h1 id="614f" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">摘录摘要</h1><p id="b91a" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">在“<a class="ae kl" href="https://arxiv.org/pdf/1903.10318.pdf" rel="noopener ugc nofollow" target="_blank">微调用于提取摘要的BERT</a>”中可以找到一个简单的BERT变体，它在几个ES数据集上再次实现了最先进的性能。</p><h1 id="22cd" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">情感分析</h1><p id="8b74" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">在最近的论文“<a class="ae kl" href="https://arxiv.org/abs/2201.03382" rel="noopener ugc nofollow" target="_blank">用于情感分析的BERT:预训练和微调的备选方案</a>”中，情感分析同样受到BERT语言模型的存在的青睐。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="10c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我可以继续…但是我不会。预训练语言模型的荣耀是显而易见的。我们只需要站在巨人的肩膀上，他们花了无数个小时来准备大量的数据，部署昂贵的GPU来为我们预先训练这些模型。然而，这些模型并不是灵丹妙药。</p><p id="6c9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">《芝麻街》和《老友记》中未能显示出一致性能提升的主要自然语言任务是<strong class="jp ir"> <em class="km">【神经机器翻译】【NMT】</em></strong>。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="bafa" class="ld le iq bd lf lg mo li lj lk mp lm ln lo mq lq lr ls mr lu lv lw ms ly lz ma bi translated">NMT通常不会从预先训练的语言模型中受益</h1><p id="1910" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">很难找到讨论它为什么不起作用的论文，很容易想象为什么。写关于什么不起作用的论文不是很受欢迎…也不太可能获得认可或被频繁引用。唉，糟糕——那么我为什么又要写这篇文章呢？</p><p id="4f47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我找到一篇关于这个主题的论文:“<a class="ae kl" href="https://arxiv.org/pdf/1804.06323.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">预训练单词嵌入何时以及为什么对神经机器翻译有用？</strong></a><strong class="jp ir"/>读起来很有意思。他们将NMT分为两类任务:</p><ol class=""><li id="4c00" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">低资源语言的NMT</li><li id="0385" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated">高资源语言的NMT</li></ol><p id="001f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">他们所说的<strong class="jp ir"> <em class="km">低/高</em> </strong>资源语言是指可以获得的平行语料库的大小。对于世界上最流行的语言，可以很容易地在网上找到开源的大型平行语料库。最大的此类知识库是<a class="ae kl" href="https://opus.nlpl.eu/" rel="noopener ugc nofollow" target="_blank"> OPUS </a>，这是一个开放的并行语料库，对于任何希望训练NMT模型的机器学习工程师来说，这都是一个惊人的资源。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi nh"><img src="../Images/c5a96b4f6b3db0e3380f4ed49e831d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r-KehZVUzzfh2xypQeQz7Q.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">来源:<a class="ae kl" href="https://opus.nlpl.eu/" rel="noopener ugc nofollow" target="_blank"> OPUS </a> -英汉高资源平行语料库</figcaption></figure><p id="2764" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图显示，开放的英汉平行语料库拥有1.03亿个平行句或172K个平行文档。但是如果你想训练一个NMT模型把波斯语翻译成中文呢？在这种情况下，您只有来自517个文档的600万个平行句子可以使用。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi ni"><img src="../Images/ad7bcf4c2400ef5725c1fedc5dde56f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5XxAnMEChA3kB9wjqBcv7Q.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">来源:<a class="ae kl" href="https://opus.nlpl.eu/" rel="noopener ugc nofollow" target="_blank"> OPUS </a> -波斯语(fa)和汉语(zh)低资源平行语料库</figcaption></figure><p id="701b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如您所料，低资源语言受益于预先训练的语言模型，并能够在通过NMT网络反向传播错误的同时，在微调嵌入时获得更好的性能。然而，令人惊讶的是，对于高资源语言，在NMT模型训练之前使用预训练语言模型作为预处理步骤的效果并没有导致性能增益。</p><p id="7e89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">必须指出的是，语言模型只有在源语言和目标语言(例如，第一个例子中的中文和英文)上都经过训练的情况下，才适用于机器翻译。这些通常被称为多语言嵌入模型或语言不可知嵌入。他们能够获得有趣的结果，即多种语言中的单词在嵌入空间中获得相似的向量表示。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/394b6765527d6a2656a53c08697c617d.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*ioG0y5gTgOrQzOJF_OzIBw.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">来源:<a class="ae kl" href="https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html#:~:text=A%20multilingual%20embedding%20model%20is,semantic%20information%20for%20language%20understanding." rel="noopener ugc nofollow" target="_blank"> AI Googleblog </a></figcaption></figure><p id="e063" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是多语言语言模型是如何训练的呢？事实证明，他们是在与NMT完全相同的数据上接受训练的:一个源语言和目标语言之间的大规模平行语料库。那么，语言模型是否有一个根本性的缺点，使它们无法有效地完成NLP任务？不，语言模型使用与NMT模型相同的数据，并且它们都是由同一个动力单元构建的:转换器。</p><p id="d795" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回顾一下，语言模型和NMT是在相同的数据上训练的，使用非常相似的基础架构。当你考虑相似性时，语言模型并没有带来什么新的东西，所以BERT、ELMo、ERNIE和我们的其他芝麻街朋友没有出现在NMT的报纸上吹捧模型性能的巨大突破也就不足为奇了。</p><p id="1ca9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">持怀疑态度的读者可能会在这一解释中找出漏洞。当然，有一些可设计的用例是在大型并行语料库上训练逻辑模型，但在小得多的语料库上训练伯特+ NMT工作流会直观地提高性能。但我认为，一个严肃的深度学习工程师不太可能在没有所有可用数据的情况下试图建立一个NMT模型……除了纯粹的学术好奇心。</p><p id="d6eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我小心翼翼地提到了一些令人毛骨悚然的细节，所以如果你感兴趣，我推荐你阅读<a class="ae kl" href="https://arxiv.org/pdf/1804.06323.pdf" rel="noopener ugc nofollow" target="_blank">的原文</a>！</p><p id="1dca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望你喜欢这篇简短的探索，探究是什么让NLP算法成功的背后的直觉。更多深度学习知识请点赞、分享、关注。</p></div></div>    
</body>
</html>