<html>
<head>
<title>Gradient Descent for Machine Learning (ML) 101 with Python Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python教程的机器学习(ML) 101的梯度下降</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/gradient-descent-algorithm-for-machine-learning-python-tutorial-ml-9ded189ec556?source=collection_archive---------0-----------------------#2020-12-28">https://pub.towardsai.net/gradient-descent-algorithm-for-machine-learning-python-tutorial-ml-9ded189ec556?source=collection_archive---------0-----------------------#2020-12-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e2a7647ef05fb5b66e38c508ec20a49e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IeqK49_2wRXe4_K5DQp-SQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">非标准化sin(x)函数的三维线框图。来源:维基媒体知识共享[ <a class="ae jg" href="https://commons.wikimedia.org/wiki/File:MATLAB_mesh_sinc3D.svg" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]</figcaption></figure><h2 id="0067" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a>，<a class="ae ep" href="https://towardsai.net/p/category/editorial" rel="noopener ugc nofollow" target="_blank">编辑</a>，<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><div class=""><h2 id="7b94" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">使用Python深入研究机器学习(ML)的梯度下降算法的教程</h2></div><p id="75d3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后更新，2021年1月7日</p><p id="3d0d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">作者:</strong>萨妮娅·帕维斯，<a class="ae jg" href="https://mktg.best/vguzs" rel="noopener ugc nofollow" target="_blank">罗伯托·伊里翁多</a></p><p id="40f3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">本教程的代码可在</strong><a class="ae jg" href="https://github.com/towardsai/tutorials/tree/master/gradient_descent_tutorial" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Github</strong></a><strong class="lj jt">上获得，其完整实现也可在</strong><a class="ae jg" href="https://colab.research.google.com/drive/1bSHQVqbVD7ZqDHfyDy03dSWCdYmrUYmF?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Google Colab</strong></a><strong class="lj jt">上获得。</strong></p><blockquote class="md me mf"><p id="181f" class="lh li mg lj b lk ll kt lm ln lo kw lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><em class="jj">🤖走向AI是一个讨论人工智能、数据科学、数据可视化、深度学习、机器学习、NLP、计算机视觉、相关新闻、机器人、自动驾驶汽车、编程、技术等的社区！</em> <a class="ae jg" href="https://towardsai.net/backers" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> <em class="jj">加入我们</em> </strong> </a> <em class="jj">🤖</em></p></blockquote><h1 id="eb51" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">目录</h1><ul class=""><li id="176a" class="nc nd jj lj b lk ne ln nf lq ng lu nh ly ni mc nj nk nl nm bi translated"><a class="ae jg" href="#dbf9" rel="noopener ugc nofollow">什么是梯度下降？</a></li><li id="069d" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#d0c8" rel="noopener ugc nofollow">成本函数</a></li><li id="f035" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#7fd3" rel="noopener ugc nofollow">渐变</a></li><li id="ca50" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#32ea" rel="noopener ugc nofollow"> Python实现</a></li><li id="edcf" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#b7a5" rel="noopener ugc nofollow">学习率</a></li><li id="10f5" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#183b" rel="noopener ugc nofollow">收敛</a></li><li id="79aa" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#1c83" rel="noopener ugc nofollow">凸函数</a></li><li id="3a60" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#2f3c" rel="noopener ugc nofollow">批量梯度下降</a></li><li id="2061" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="http://30cb" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a></li><li id="fe46" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#340b" rel="noopener ugc nofollow">小批量梯度下降</a></li><li id="3064" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#340b" rel="noopener ugc nofollow">结论</a></li><li id="d9f5" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#a6ee" rel="noopener ugc nofollow">资源</a></li><li id="8310" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><a class="ae jg" href="#93c5" rel="noopener ugc nofollow">参考文献</a></li></ul><blockquote class="ns"><p id="e9a2" class="nt nu jj bd nv nw nx ny nz oa ob mc dk translated">📚查看我们的<a class="ae jg" href="https://towardsai.net/p/deep-learning/convolutional-neural-networks-cnns-tutorial-with-python-417c29f0403f" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a>教程。📚</p></blockquote></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="dbf9" class="mk ml jj bd mm mn oj mp mq mr ok mt mu ky ol kz mw lb om lc my le on lf na nb bi translated">什么是梯度下降？</h1><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/396deb5630e1e57de393cb4d8c5e085d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*-t2fw32J8jcvZuVm.gif"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:渐变下降图|来源:维基媒体知识共享[ <a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Gradient_descent.gif" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]</figcaption></figure><p id="6b7b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">梯度下降</strong>是神经网络[ <a class="ae jg" href="https://www.researchgate.net/publication/221653420_Large-scale_matrix_factorization_with_distributed_stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank"> 7 </a> ]、数据科学、优化和机器学习任务中最常用的机器学习算法之一。梯度下降算法及其变体几乎可以在每个机器学习模型中找到。</p><p id="6a51" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">梯度下降是一种在机器学习模型中调整参数的流行优化方法。它的目标是应用优化来找到最小或最小的误差值。它主要用于更新模型的参数，在这种情况下，参数是指回归中的系数和神经网络中的权重。</p><p id="8fe4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">梯度</strong>是一个向量值函数，它描述了一个函数图形的切线斜率，指向该函数最显著的增长率的方向。它是一个导数，表示一个成本函数[ <a class="ae jg" href="http://sites.science.oregonstate.edu/math/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/grad/grad.html" rel="noopener ugc nofollow" target="_blank"> 6 </a> ]的倾斜度或斜率。</p><p id="7e22" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:</p><blockquote class="md me mf"><p id="0379" class="lh li mg lj b lk ll kt lm ln lo kw lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated">假设约翰在山顶上，他的目标是到达底田，但约翰是盲人，看不到底线。他将如何解决这个问题？</p></blockquote><p id="95f3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了解决这个问题，他将采取小步骤，并向更高的倾斜方向移动，通过一次移动一步来反复应用这些步骤，直到他最终到达山脚。</p><p id="bb5d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">梯度下降的执行方式与示例中提到的方式相同。它的主要目的是到达山的最低点。</p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/c09a2d60b7e336420c12860eeff66625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QhPZ4yGFGdXklywNlAk0Wg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:梯度下降可视化。[ <a class="ae jg" href="https://ithelp.ithome.com.tw/articles/10218912" rel="noopener ugc nofollow" target="_blank"> 2 </a></figcaption></figure><p id="5b3f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了深入了解梯度下降，必须详细了解以下主题:</p><ul class=""><li id="86ff" class="nc nd jj lj b lk ll ln lo lq ou lu ov ly ow mc nj nk nl nm bi translated">价值函数</li><li id="d8cf" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">成本函数的最小化</li><li id="334e" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">最小值和最大值</li><li id="7809" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">凸函数</li><li id="35cb" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">梯度</li><li id="4428" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">停止条件</li><li id="ee40" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">学习率</li></ul><h1 id="d0c8" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">价值函数</h1><p id="5b48" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">成本函数衡量机器学习算法对分配数据的性能。它量化了预测值和期望值之间的误差，并以单个实数的形式模拟它。从根本上说，它衡量的是由算法造成的预测误差。它显示了给定数据集的预测值和实际值之间的差异。如果成本函数具有较低的值，则模型可以具有更好的预测能力。成本函数的一个优秀值是零— <strong class="lj jt">我们使用梯度下降算法来最小化成本函数</strong>。</p><p id="8724" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些是机器学习中使用的重要成本函数:</p><ul class=""><li id="bed4" class="nc nd jj lj b lk ll ln lo lq ou lu ov ly ow mc nj nk nl nm bi translated">均方误差</li><li id="883f" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">对数损失或交叉熵</li><li id="ee28" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">KL-散度</li></ul><p id="2a6d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">均方差方程:</strong></p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/88e81dc91d348cbb50289336015c4c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YDa5_ZzeoQWSHmah.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3:均方误差方程</figcaption></figure><p id="6986" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中:</p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/598bad802de6beca591696903e6df727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*j4vIpfQG96rQN67U.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:图4:均方误差方程的条件。</figcaption></figure><p id="2b84" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">均方误差用于机器学习中的回归算法。</p><p id="767f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Python代码示例:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="dea5" class="pg ml jj pc b gy ph pi l pj pk">def sum_of_squares(v):<br/>    val = sum(item ** 2 for item in v)</span></pre><p id="5278" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">对数损失或交叉熵方程:</strong></p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/b34639410bd4e95a7e1a72a8ac11a5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YHdkTa1BjSxXU3mE.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图5:对数损失或交叉熵的等式。</figcaption></figure><p id="76d4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在分类问题中使用了代价函数log loss或交叉熵。</p><h2 id="508b" class="pg ml jj bd mm pl pm dn mq pn po dp mu lq pp pq mw lu pr ps my ly pt pu na jp bi translated">成本函数与梯度下降的关系</h2><p id="159a" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">成本函数是最小化的情况。成本函数可以是训练集的误差平方和。梯度下降法是一种求多元函数最小值的方法[ <a class="ae jg" href="https://towardsdatascience.com/minimizing-the-cost-function-gradient-descent-a5dd6b5350e1" rel="noopener" target="_blank"> 8 </a> ]，可以用来最小化代价函数。综上所述，如果<strong class="lj jt">成本函数</strong>是<strong class="lj jt"> K </strong>变量的函数，那么<strong class="lj jt">梯度</strong>就是代表成本增长最快方向的<strong class="lj jt">长度-K </strong>向量。</p><p id="b5a3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如:</p><p id="e7fe" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设存在回归模型将被应用于二元分类的情况。总是期望以高精度和最小的误差或损失来执行模型。</p><p id="ee23" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所以，线性方程→ <strong class="lj jt"> y= mx + c </strong>中，一般都是计算参数值<strong class="lj jt"> m </strong>和<strong class="lj jt"> c </strong>。期望最小的误差或损失，这就是所谓的成本函数或损失函数。</p><p id="3032" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">寻找损失最小的参数值<strong class="lj jt"> m </strong>和<strong class="lj jt"> c </strong>总是具有挑战性。在这种情况下，<strong class="lj jt">梯度下降</strong>算法进入画面。本质上，它有助于模型找到损失最小的点。这就是为什么它与成本函数有密切的关系。</p><h1 id="7fd3" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">梯度</h1><p id="8c57" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">直线的坡度表示直线有多陡[ <a class="ae jg" href="https://www.mathsisfun.com/gradient.html" rel="noopener ugc nofollow" target="_blank"> 17 </a> ]。</p><p id="e5f1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">梯度方程:</p><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/dd53954c5e5e79f344c89df6e0452d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/0*zuD7Kelis7lI3G_l.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图6:梯度的方程式。</figcaption></figure><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/495e2174f59483acdcb0b74041aeb9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/0*H5oP_dsvkHQjU6A5.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7:渐变图。</figcaption></figure><p id="a3af" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与梯度相关的几点:</p><ul class=""><li id="a9be" class="nc nd jj lj b lk ll ln lo lq ou lu ov ly ow mc nj nk nl nm bi translated">向上为正，向下为负</li><li id="8897" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">从左边开始，穿过右边是积极的。</li></ul><p id="538d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">设e1，e2，.。。，ed 2 Rd是一组特定的单位向量，使得ei = (0，0，.。。, 0, 1, 0, .。。，0)其中对于ei，1在第I个坐标中。</p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi px"><img src="../Images/01d75f9a322fe47011fc1c273f1d65a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ju2CwLQFm2SAtILH.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:梯度的方程式。</figcaption></figure><p id="2d8f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">梯度下降是机器学习中用于估计模型参数的常用优化算法。根据微积分，这是一个纯偏导数，给出了函数增加最快的输入方向。</p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/22eabacbfc0d64dc0c3e4e8b824328f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PeDGCkVwAy9qw_YQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:使用梯度下降寻找最小值。</figcaption></figure><p id="5884" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基本上，为了最大化一个函数，算法选择一个随机的起点，测量梯度，在梯度的方向上迈出一小步，然后用新的起点重复。类似地，通过在相反的方向上采取小的步骤，功能被最小化。我们基于其初始值来计算成本函数，并且参数估计在各个步骤上被细化，使得成本函数最终暗示最小值。</p><p id="2028" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">梯度下降算法</strong>:</p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pz"><img src="../Images/7b5d03ac666f08dc160f108fe528534a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vAHnjF_fIJ4yunSU.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:梯度下降算法。</figcaption></figure><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/90aeaf108bdd4d96240c5c5f575aaaac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/0*mr0YwvYPkiZ0xUWT.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:渐变步骤。</figcaption></figure><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qb"><img src="../Images/1b16e47ff544cc40ec7489fb1b0ca5c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bFoZqU5P_MnnYpON.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12:成本函数最小化过程。[ <a class="ae jg" href="https://www.mygreatlearning.com/blog/understanding-learning-rate-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"> 9 </a></figcaption></figure><p id="7ea1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">梯度下降方程:</p><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/15bb10bc18e3d1390b1c0b0e8c166895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/0*e_KJwlUOyMOg6P4y.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图13:梯度下降算法的等式。</figcaption></figure><p id="c3df" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，这个公式告诉我们它需要去的下一个位置，也就是最陡的下降方向。</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="f9c2" class="pg ml jj pc b gy ph pi l pj pk">#Pseudocode<br/>train(θ) = (1/2m) Σ( hθ(x(i)) - y^(i))^2</span><span id="22d2" class="pg ml jj pc b gy qd pi l pj pk">Repeat {<br/> θj = θj – (learning-rate/m) * Σ( hθ(x^(i)) - y^(i))xj^(i)<br/>    For every j = 0 … n <br/>}</span></pre><p id="322b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这种情况下:</p><p id="e346" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> xj^(i) </strong>是<strong class="lj jt"> i^th </strong>训练示例的第j个特征。接下来，我们重复直到它达到收敛:</p><ol class=""><li id="b8fe" class="nc nd jj lj b lk ll ln lo lq ou lu ov ly ow mc qe nk nl nm bi translated">给定梯度，计算参数随学习率的变化。</li><li id="5cbd" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc qe nk nl nm bi translated">用新的参数值重新计算新的梯度。</li><li id="6788" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc qe nk nl nm bi translated">重复步骤1。</li></ol><p id="588f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有三种<strong class="lj jt">常见的梯度下降类型</strong>:</p><ul class=""><li id="39e5" class="nc nd jj lj b lk ll ln lo lq ou lu ov ly ow mc nj nk nl nm bi translated"><strong class="lj jt">批量梯度下降</strong></li><li id="89fa" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><strong class="lj jt">随机梯度下降</strong></li><li id="13f0" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated"><strong class="lj jt">小批量梯度下降</strong></li></ul><h1 id="32ea" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">用Numpy实现梯度下降的Python实现:</h1><p id="9f36" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">导入所有必需的包:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="6b09" class="pg ml jj pc b gy ph pi l pj pk">import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline</span></pre><p id="d186" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从CSV读取数据:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="f16f" class="pg ml jj pc b gy ph pi l pj pk">column_names = ['Population', 'Profit']df = pd.read_csv('/content/data.txt', header=None, names=column_names)df.head()</span></pre><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/93b7df1d4a0fd210f76a036daeb430a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/0*wFsyg3Dhj_dx6ob0.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图14:数据输出。</figcaption></figure><p id="c248" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">获取X和Y的值:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="c17c" class="pg ml jj pc b gy ph pi l pj pk">df.insert(0, 'Theta0', 1)cols = df.shape[1]X = df.iloc[:,0:cols-1]Y = df.iloc[:,cols-1:cols]theta = np.matrix(np.array([0]*X.shape[1]))X = np.matrix(X.values)Y = np.matrix(Y.values)</span></pre><p id="f540" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">绘制数据:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="9237" class="pg ml jj pc b gy ph pi l pj pk">df.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))</span></pre><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qg"><img src="../Images/9cc501e2cdd40b311afd1404a8fa202c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gh5zDapZzltTNIqG.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图15:数据的绘制。</figcaption></figure><p id="77f8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">计算RSS的方法:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="2f3b" class="pg ml jj pc b gy ph pi l pj pk">def calculate_RSS(X, y, theta):  <br/>    inner = np.power(((X * theta.T) - y), 2)<br/>    return np.sum(inner) / (2 * len(X))</span></pre><p id="d19d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">计算梯度下降的方法:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="5eed" class="pg ml jj pc b gy ph pi l pj pk">def gradientDescent(X, Y, theta, alpha, iters):  <br/>    t = np.matrix(np.zeros(theta.shape))<br/>    parameters = int(theta.ravel().shape[1])<br/>    cost = np.zeros(iters)<br/>    <br/>    for i in range(iters):<br/>        error = (X * theta.T) - Y    for j in range(parameters):<br/>         term = np.multiply(error, X[:,j])<br/>         t[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))     theta = t<br/>     cost[i] = calculate_RSS(X, Y, theta)     return theta, cost</span></pre><p id="4ef1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">不应用梯度下降计算误差:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="74fe" class="pg ml jj pc b gy ph pi l pj pk">error = calculate_RSS(X, Y, theta)error</span></pre><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/e07fc679bf5b636f22dd4c89e06f191b.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*GoUSsle68Xaxs6Le.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图16:没有应用梯度下降的错误。</figcaption></figure><p id="1b7f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过应用梯度下降计算误差:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="0a97" class="pg ml jj pc b gy ph pi l pj pk">g, cost = gradientDescent(X, Y, theta, 0.01, 1000)g</span></pre><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/9b58d45581ac8de216792e46949affb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/0*AVJ9F1XRUr1LeKrE.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图17:误差矩阵。</figcaption></figure><p id="3ed4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">应用梯度下降后计算误差:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="8d7c" class="pg ml jj pc b gy ph pi l pj pk">error = calculate_RSS(X, Y, g)error</span></pre><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/ac1f12ba7e489946cabd5e8dc8f2b713.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/0*IfmsHgZ_uV7dW_Mz.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图18:应用梯度下降后的错误。</figcaption></figure><p id="2069" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数据绘图:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="6c76" class="pg ml jj pc b gy ph pi l pj pk">x = np.linspace(df.Population.min(), df.Population.max(), 100)f = g[0, 0] + (g[0, 1] * x)fig, ax = plt.subplots(figsize=(12,8))ax.plot(x, f, 'r', label='Prediction')ax.scatter(df.Population, df.Profit, label='Traning Data')ax.legend(loc=2)ax.set_xlabel('Population')ax.set_ylabel('Profit')ax.set_title('Predicted Profit vs. Population Size')</span></pre><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qk"><img src="../Images/b2ca18848d992495bb2a0b7dd0f45f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Nkayis9KEfL1QLZH.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图19:数据的绘制。</figcaption></figure><h1 id="b7a5" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">学习率</h1><p id="d024" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">学习速率是一个超参数，用于控制算法更新参数估计值或学习参数值[ <a class="ae jg" href="https://www.mygreatlearning.com/blog/understanding-learning-rate-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"> 9 </a> ]的速率。它主要是优化算法中的一个调整参数，并在向最小损失函数移动时确定每次迭代的步长。它用于缩放梯度下降过程中参数更新的幅度。它将梯度乘以一个数字(学习率或步长)来确定下一个点。</p><p id="a248" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">示例:</p><p id="7e6b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有一个<strong class="lj jt">大小</strong>为<strong class="lj jt"> 4.2 </strong>和<strong class="lj jt">学习速率</strong>为<strong class="lj jt"> 0.01的梯度，然后</strong>梯度下降算法将选取下一个点，距离上一个点<strong class="lj jt"> 0.042 </strong>。</p><p id="e5bc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">理解<strong class="lj jt">局部最大值</strong>和<strong class="lj jt">局部最小值概念</strong>对于详细理解学习率概念至关重要。</p><h1 id="3618" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">局部极大</h1><p id="53a3" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">一个函数<strong class="lj jt"> f(x) </strong>在<strong class="lj jt"> x = a，</strong>处有一个局部最大值，如果<strong class="lj jt"> f(a) </strong>的值比<strong class="lj jt"> f(x) </strong>在<strong class="lj jt"> x=a </strong>的一个小邻域内的所有值都重要。因此，在下面所示的数学等式中:</p><blockquote class="md me mf"><p id="f715" class="lh li mg lj b lk ll kt lm ln lo kw lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><strong class="lj jt"><em class="jj">f(a)&gt;f(a-h)</em></strong><em class="jj">和</em><strong class="lj jt"><em class="jj">f(a)&gt;f(a+h)</em></strong><em class="jj">。其中，</em><strong class="lj jt"><em class="jj">h&gt;0</em></strong><em class="jj">，则</em><strong class="lj jt"><em class="jj"/></strong><em class="jj">称为局部最大值的点。</em></p></blockquote><p id="066f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从根本上说，局部最大值是景观中大于其每个相邻状态的峰值位置。</p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ql"><img src="../Images/4ecb266329c4ce639fdaa9bdf7954a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fIIwB3a9HWN17wL1.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图20:局部最大值。</figcaption></figure><h1 id="15d3" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">局部极小</h1><p id="a547" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">如果在<strong class="lj jt"> x = a </strong>处的函数值小于在<strong class="lj jt"> x = a </strong>的相邻点处的函数值，则函数<strong class="lj jt"> f(x) </strong>在<strong class="lj jt">x = a</strong>处具有局部最小值。因此，在下面所示的数学等式中:</p><blockquote class="md me mf"><p id="b456" class="lh li mg lj b lk ll kt lm ln lo kw lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><strong class="lj jt"><em class="jj">f(a)&lt;f(a-h)</em></strong><em class="jj">和</em> <strong class="lj jt"> <em class="jj"> f (a) &lt; f (a + h)。</em> </strong> <em class="jj">其中</em><strong class="lj jt"><em class="jj">h&gt;0</em></strong><em class="jj">，则a称为局部最小值的点。</em></p></blockquote><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qm"><img src="../Images/e8932a41c71e773e0a4edac1e19f6ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Xytdi9f_YEwkx1r4.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图21:局部最大值和局部最小值|来源:维基百科[ <a class="ae jg" href="https://en.wikipedia.org/wiki/Maxima_and_minima#/media/File:Extrema_example_original.svg" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]，许可证<a class="ae jg" href="http://www.gnu.org/licenses/old-licenses/fdl-1.2.html" rel="noopener ugc nofollow" target="_blank"> GFDPL 1.2 </a></figcaption></figure><h1 id="e705" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">学习速度的重要性</h1><p id="637f" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">为了在梯度下降中达到局部最小值，将学习率设置为既不太低也不太高的适当值是至关重要的。学习率是它的一个组成部分，因为如果它采取的步骤太大，它可能达不到<strong class="lj jt">本地最小值。</strong>毕竟<strong class="lj jt">，</strong>它在梯度下降的凸函数之间来回弹跳。</p><p id="f92c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果学习率设置为最小值，那么梯度下降将最终达到局部最小值，然而，这可能需要一段时间[ <a class="ae jg" href="https://builtin.com/data-science/gradient-descent" rel="noopener ugc nofollow" target="_blank"> 16 </a> ]。</p><p id="4a6d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">学习率可能出现两种不同的情况:</p><ul class=""><li id="5a47" class="nc nd jj lj b lk ll ln lo lq ou lu ov ly ow mc nj nk nl nm bi translated">大学习率</li><li id="fd5c" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">小学习率</li></ul><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qn"><img src="../Images/0faf3b48afbefc433c29412653b8df6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ARiY_3OpxNoArNw2"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图22:不同的学习率。</figcaption></figure><p id="cd58" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，根据上面显示的不同场景，学习率不应该太高或太低。如果学习率太大，振荡就会发散。</p><p id="dc6a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">显示学习率:</p><pre class="op oq or os gt pb pc pd pe aw pf bi"><span id="aaa5" class="pg ml jj pc b gy ph pi l pj pk">def step_gradient(b_current, m_current, points, learning_rate):<br/>    b_gradient = 0<br/>    m_gradient = 0<br/>    n = float(len(points))<br/>    <br/>    for i in range(0, len(points)):<br/>        x = points[i, 0]<br/>        y = points[i, 1]<br/>        b_gradient += -(2/n) * (y - ((m_current * x) + b_current))<br/>        m_gradient += -(2/n) * x * (y - ((m_current * x) + b_current))<br/>        <br/>    latest_b = b_current - (learning_rate * b_gradient)<br/>    latest_m = m_current - (learning_rate * m_gradient)<br/>    return [latest_b, latest_m]</span></pre><h1 id="183b" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">趋同；聚集</h1><p id="dedb" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">收敛是指损失函数没有明显改善的位置，我们停留在最小值附近的一个点上。如果成本函数在每次迭代后都下降，那么就说梯度下降是正确的。如果梯度下降不再降低成本函数，并且或多或少地保持在同一水平，则它收敛。</p><p id="999c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当梯度下降开始足够接近最小值时，它收敛到局部最小值。如果有多个局部极小值，其收敛性取决于迭代开始的点。收敛到全局最小值是一个挑战。</p><p id="a2a9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">收敛性总是取决于优化所使用的函数类型。如果目标函数是凸的，梯度下降总是收敛到同一个局部极小值，它也是全局极小值。</p><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/53c02b65d6326d9cd2ef649d33f626e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/0*BfLHiaNj_dmHSy8i.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图23:梯度下降中的收敛。</figcaption></figure><h1 id="1c83" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">凸函数</h1><p id="fd6a" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">如果函数<strong class="lj jt"> </strong>的图上任意两点之间的线段位于图<a class="ae jg" href="https://towardsdatascience.com/gradient-descent-unraveled-3274c895d12d" rel="noopener" target="_blank"> 15 </a>之上或之上，则为凸函数。</p><p id="b800" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数学上，凸函数可以定义为:</p><blockquote class="md me mf"><p id="dbe2" class="lh li mg lj b lk ll kt lm ln lo kw lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><em class="jj">凸函数是指在其定义域内每个区间中点的值不超过其在区间两端的值的算术平均值的函数[ </em> <a class="ae jg" href="https://artofproblemsolving.com/wiki/index.php/Convex_function" rel="noopener ugc nofollow" target="_blank"> <em class="jj"> 14 </em> </a> <em class="jj"> ]。</em></p></blockquote><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qp"><img src="../Images/25bfef76b2884bca562bdaf2f633fc23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dKUJSnQSZnueAgB1.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图24:凸函数图。</figcaption></figure><p id="d3dd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从而进入数学方程式:</p><p id="f2b8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有一个区间【T8【a，b】<strong class="lj jt">f(x)</strong>是一个函数<strong class="lj jt"> x1 </strong>和<strong class="lj jt"> x2 </strong>是区间【T16【a，b】和任意<strong class="lj jt"> λ </strong>中的两点其中<strong class="lj jt"> 0 &lt; λ &lt; 1 </strong>。所以，</p><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qq"><img src="../Images/750aeedc7784c9570af35ac0235916ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2igIzyKFrnVzuzi_.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图25:凸的方程式。</figcaption></figure><figure class="op oq or os gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/5402f1d5ab51ec1080f01dfee5725b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iupLPYz1HHYGqxm1.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图26:区间上的凸函数|来源:维基百科[ <a class="ae jg" href="https://en.wikipedia.org/wiki/Convex_function#/media/File:ConvexFunction.svg" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]，许可证<a class="ae jg" href="https://creativecommons.org/licenses/by-sa/3.0" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 3.0 </a></figcaption></figure><h1 id="abe9" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">梯度下降的主要类型</h1><h2 id="2f3c" class="pg ml jj bd mm pl pm dn mq pn po dp mu lq pp pq mw lu pr ps my ly pt pu na jp bi translated">批量梯度下降(BGD)</h2><p id="7b3c" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">批量梯度下降为训练数据集中的每个示例计算误差，但仅在评估了所有训练示例后更新模型[ <a class="ae jg" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" rel="noopener ugc nofollow" target="_blank"> 13 </a> ]。这对于凸的或相对光滑的误差流形是非常好的。这种方法计算量很大，但随着特征数量的增加，它的规模也很大。以下内容对于批量梯度下降至关重要:</p><ul class=""><li id="2f79" class="nc nd jj lj b lk ll ln lo lq ou lu ov ly ow mc nj nk nl nm bi translated">这是非常缓慢和计算昂贵的。</li><li id="a9bf" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">从不建议使用庞大的训练数据集。</li><li id="af4c" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">在这里，收敛是缓慢的。</li><li id="7f9f" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">它使用整个训练数据集计算梯度。</li></ul><h2 id="30cb" class="pg ml jj bd mm pl pm dn mq pn po dp mu lq pp pq mw lu pr ps my ly pt pu na jp bi translated">随机梯度下降</h2><p id="f282" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">批量梯度下降中有一种情况，如下所述:</p><blockquote class="md me mf"><p id="948b" class="lh li mg lj b lk ll kt lm ln lo kw lp mh lr ls lt mi lv lw lx mj lz ma mb mc im bi translated"><em class="jj">假设有一个数据集，有500万个例子。所以，走一步，模型将计算所有500万个例子的梯度。这种情况发生在批量梯度下降中。</em></p></blockquote><p id="f67e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上述情况是低效的，所以随机梯度下降是在图片中处理这个问题。本质上，它会计算错误，并更新训练资料集中每个范例的模型。</p><p id="6ea1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">随机梯度下降有以下几点:</p><ul class=""><li id="570b" class="nc nd jj lj b lk ll ln lo lq ou lu ov ly ow mc nj nk nl nm bi translated">它使用单个训练样本计算梯度。</li><li id="c8cd" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">建议使用大型训练样本。</li><li id="db52" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">与批量梯度下降相比，它速度更快，计算成本更低。</li><li id="cc48" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">它更快地达到收敛。</li><li id="fb57" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">它能更容易地浅化局部极小值。</li></ul><h2 id="340b" class="pg ml jj bd mm pl pm dn mq pn po dp mu lq pp pq mw lu pr ps my ly pt pu na jp bi translated">小批量梯度下降(MBGD)</h2><p id="403e" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">小批量梯度下降将训练数据集分成小批量，这些批量用于计算模型误差和更新模型系数。它可以用于更平滑的曲线。</p><p id="7f64" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">小批量梯度下降具有以下要点:</p><ul class=""><li id="7765" class="nc nd jj lj b lk ll ln lo lq ou lu ov ly ow mc nj nk nl nm bi translated">当数据集很大时，可以使用它。</li><li id="b5ab" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">它直接收敛到最小值。</li><li id="2648" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">对于较大的数据集，速度更快。</li><li id="f6e8" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">由于在SGD中，同时使用的只有一个例子，所以无法实现矢量化实现。</li><li id="8a41" class="nc nd jj lj b lk nn ln no lq np lu nq ly nr mc nj nk nl nm bi translated">它可以降低计算速度——为了解决这个问题，混合使用了批量梯度下降和SGD。</li></ul><figure class="op oq or os gt iv gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/2da16a91d1584d4efc567943d48a1945.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/0*nfl2mc7LO2PO2p3P.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图27:最小批量梯度下降。</figcaption></figure><h1 id="28f6" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">结论</h1><p id="2c0a" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">梯度下降是一种一阶迭代优化算法[ <a class="ae jg" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank"> 10 </a> ]，用于获得可微函数的局部最小值。它基于一个凸函数，并反复调整其参数，使给定的函数最小化到其局部最小值。</p><p id="72f6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">就梯度下降的类型而言，批量梯度下降(BGD)是机器学习中最常用的梯度下降形式[ <a class="ae jg" href="https://machinelearningmastery.com/master-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank"> 12 </a> ]。使用梯度下降法优化系数的一些日常算法是线性回归和逻辑回归[ <a class="ae jg" href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener ugc nofollow" target="_blank"> 11 </a> ]。</p><p id="01e5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">损失函数</strong>解释了模型在当前参数集(权重和偏差)下的表现，而<strong class="lj jt">梯度下降</strong>用于寻找最佳参数集。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="c232" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">免责声明:</strong>本文表达的观点仅代表作者个人观点，不代表卡内基梅隆大学或其他(直接或间接)与作者相关的公司的观点。这些文章并不打算成为最终产品，而是当前思想的反映，同时也是讨论和改进的催化剂。</p><p id="14da" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">除非另有说明，所有图片均来自作者。</strong></p><p id="1be4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过<a class="ae jg" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向AI </a>发布</p><h1 id="a6ee" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">资源</h1><p id="c3a3" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated"><a class="ae jg" href="https://github.com/towardsai/tutorials/tree/master/gradient_descent_tutorial" rel="noopener ugc nofollow" target="_blank"> Github库</a>。</p><p id="744b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://colab.research.google.com/drive/1bSHQVqbVD7ZqDHfyDy03dSWCdYmrUYmF?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google colab实现</a>。</p><h1 id="93c5" class="mk ml jj bd mm mn mo mp mq mr ms mt mu ky mv kz mw lb mx lc my le mz lf na nb bi translated">参考</h1><p id="49e9" class="pw-post-body-paragraph lh li jj lj b lk ne kt lm ln nf kw lp lq ox ls lt lu oy lw lx ly oz ma mb mc im bi translated">[1] MATLAB mesh sic3D.svg，Wikimedia Commons，<a class="ae jg" href="https://commons.wikimedia.org/wiki/File:MATLAB_mesh_sinc3D.svg" rel="noopener ugc nofollow" target="_blank">https://Commons . Wikimedia . org/wiki/File:MATLAB _ mesh _ sinc3d . SVG</a></p><p id="9b93" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi">[2] 從 Gradient Descent to Optimizer, by ITHome, <a class="ae jg" href="https://ithelp.ithome.com.tw/articles/10218912" rel="noopener ugc nofollow" target="_blank">https://ithelp.ithome.com.tw/articles/10218912</a></p><p id="7b25" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3]梯度下降，维基共享资源，<a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Gradient_descent.gif" rel="noopener ugc nofollow" target="_blank">https://Commons . Wikimedia . org/wiki/File:Gradient _ descent . gif</a></p><p id="08d3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4]本地和全球最大值和最小值，维基百科，许可GFDL 1.2，<a class="ae jg" href="https://en.wikipedia.org/wiki/Maxima_and_minima#/media/File:Extrema_example_original.svg" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Maxima _ and _ Minima #/media/File:Extrema _ example _ original . SVG</a></p><p id="f6fd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]区间上的凸函数，维基百科，License CC BY-SA 3.0，<a class="ae jg" href="https://en.wikipedia.org/wiki/Convex_function#/media/File:ConvexFunction.svg" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Convex _ Function #/media/File:Convex Function . SVG</a></p><p id="c717" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6]《梯度与方向导数》，俄勒冈州立大学，<a class="ae jg" href="http://sites.science.oregonstate.edu/math/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/grad/grad.html" rel="noopener ugc nofollow" target="_blank">http://sites . science . oregonstate . edu/math/home/programs/underbrad/CalculusQuestStudyGuides/vcalc/grad/grad . html</a></p><p id="dac7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[7]杰穆拉、莱纳&amp;尼坎普、埃里克&amp;哈斯、彼得&amp;西斯马尼斯、亚尼斯。(2011).分布式随机梯度下降的大规模矩阵分解。ACM SIGKDD知识发现和数据挖掘国际会议录。69–77.10.1145/2020408.2020426.</p><p id="9124" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[8]最小化成本函数:梯度下降，XuanKhanh Nguyen，走向数据科学，<a class="ae jg" href="https://towardsdatascience.com/minimizing-the-cost-function-gradient-descent-a5dd6b5350e1" rel="noopener" target="_blank">https://towardsdatascience . com/Minimizing-the-cost-function-Gradient-descent-a5dd 6b 5350 e 1</a></p><p id="c459" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[9]理解机器学习中的学习率，伟大的学习团队，<a class="ae jg" href="https://www.mygreatlearning.com/blog/understanding-learning-rate-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://www . mygreatlearning . com/blog/Understanding-Learning-Rate-in-Machine-Learning/</a></p><p id="1782" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[10]梯度下降，维基百科，<a class="ae jg" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gradient_descent</a></p><p id="ed51" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[11]用于机器学习的梯度下降，机器学习掌握，<a class="ae jg" href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://Machine Learning Mastery . com/Gradient-Descent-for-Machine-Learning/</a></p><p id="9212" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[12]Master Machine Learning Algorithms，Jason Brownlee博士，<a class="ae jg" href="https://machinelearningmastery.com/master-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">https://Machine Learning mastery . com/Master-Machine-Learning-Algorithms/</a></p><p id="3008" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[13]关于小批量梯度下降和如何配置批量大小的温和介绍，机器学习掌握，<a class="ae jg" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" rel="noopener ugc nofollow" target="_blank">https://machinelementmastery . com/Gentle-Introduction-Mini-Batch-Gradient-Descent-Configure-Batch-Size/</a></p><p id="c990" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[14]凸函数，解题的艺术，<a class="ae jg" href="https://artofproblemsolving.com/wiki/index.php/Convex_function" rel="noopener ugc nofollow" target="_blank">https://artofproblemsolution . com/wiki/index . PHP/Convex _ Function</a></p><p id="e145" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[15] Gradient Descent Unraveled，Manpreet Singh Minhas，<a class="ae jg" href="https://towardsdatascience.com/gradient-descent-unraveled-3274c895d12d" rel="noopener" target="_blank">https://towardsdatascience . com/Gradient-Descent-Unraveled-3274 c 895d 12d</a></p><p id="c0f9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[16]梯度下降:机器学习最流行的算法之一介绍，尼克拉斯·东格斯，<a class="ae jg" href="https://builtin.com/data-science/gradient-descent" rel="noopener ugc nofollow" target="_blank">https://builtin.com/data-science/gradient-descent</a></p><p id="5b4d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【17】直线的坡度(斜率)，数学很好玩，<a class="ae jg" href="https://www.mathsisfun.com/gradient.html" rel="noopener ugc nofollow" target="_blank">https://www.mathsisfun.com/gradient.html</a></p></div></div>    
</body>
</html>