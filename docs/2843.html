<html>
<head>
<title>The Most Important Paper in ML Interpretability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML可解释性中最重要的论文</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/the-most-important-paper-in-ml-interpretability-9e2dc2f10672?source=collection_archive---------1-----------------------#2022-06-14">https://pub.towardsai.net/the-most-important-paper-in-ml-interpretability-9e2dc2f10672?source=collection_archive---------1-----------------------#2022-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3120" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">出版四年后,《可解释性的构建模块》仍然是ML互操作性领域的开创性论文。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9f989647ef666317db8ad9dda19f1fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FmbfVAj5ElIFjxpK.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:Neurohive</figcaption></figure><blockquote class="ky kz la"><p id="aa48" class="lb lc ld le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我最近创办了一份专注于人工智能的教育时事通讯，已经有超过125，000名订户。《序列》是一份无废话(意思是没有炒作，没有新闻等)的ML导向时事通讯，需要5分钟阅读。目标是让你与机器学习项目、研究论文和概念保持同步。请通过订阅以下内容来尝试一下:</p></blockquote><div class="ly lz gp gr ma mb"><a href="https://thesequence.substack.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mc ab fo"><div class="md ab me cl cj mf"><h2 class="bd iu gy z fp mg fr fs mh fu fw is bi translated">序列</h2><div class="mi l"><h3 class="bd b gy z fp mg fr fs mh fu fw dk translated">与机器学习、人工智能和数据发展保持同步的最佳资源…</h3></div><div class="mj l"><p class="bd b dl z fp mg fr fs mh fu fw dk translated">thesequence.substack.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp ks mb"/></div></div></a></div><p id="3ce6" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">ML的未来是可解释的ML！这个原则是大多数ML专家都同意的，尽管他们对如何达到这些可解释性水平有不同的看法。</p><p id="a018" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">任何深度学习解决方案的一个挑战性要素是理解深度神经网络所做的知识和决策。虽然解释神经网络做出的决定一直很困难，但随着深度学习的兴起和使用多维数据集操作的大规模神经网络的激增，这个问题已经成为一个噩梦。毫不奇怪，神经网络的解释已经成为深度学习生态系统中最活跃的研究领域之一。</p><p id="37b7" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">试着想象一个大型神经网络，它有数亿个神经元，正在执行图像识别等深度学习任务。通常，您希望了解网络是如何做出具体决策的。目前的大多数研究都集中在检测网络中哪些神经元被激活。知道neuron-12345触发五次是相关的，但在整个网络的规模中不是非常有用。关于神经网络中决策理解的研究主要集中在三个方面:特征可视化、属性和降维。特别是谷歌，在特征可视化领域做了大量工作，发布了一些<a class="ae mt" href="https://distill.pub/2017/feature-visualization/" rel="noopener ugc nofollow" target="_blank">卓越的研究和工具</a>。2018年，谷歌研究人员发表了一篇题为<a class="ae mt" href="https://distill.pub/2018/building-blocks/" rel="noopener ugc nofollow" target="_blank">“可解释性的构建模块”</a>的论文，成为机器学习可解释性领域的开创性论文。该论文提出了一些新的观点来理解深度神经网络如何做出决策。</p><p id="27eb" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">谷歌研究的主要见解是，不要孤立地看待不同的可解释性技术，而是将其视为帮助理解神经网络行为的更大模型的可组合构件。例如，特征可视化是理解单个神经元处理的信息的非常有效的技术，但是不能将这种洞察力与神经网络做出的整体决策相关联。归因是一种更可靠的技术，可以解释不同神经元之间的关系，但在理解单个神经元所做的决定时就不那么重要了。结合这些构建模块，谷歌创建了一个可解释性模型，它不仅解释了神经网络检测到的内容，而且回答了T2网络如何组合这些个体片段以做出后来的决定，以及T4为什么做出这些决定。</p><p id="d586" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">谷歌新的可解释性模型具体是如何工作的？嗯，在我看来，主要的创新在于，它分析了神经网络不同组件在不同层次上做出的决定:单个神经元、相连的神经元组和完整的层。谷歌还使用一种称为矩阵分解的新研究技术来分析任意组神经元对最终决策的影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c3744ebaac77e01e15055113edd0137e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b7SccoHpcFmIRLAh.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mt" href="https://distill.pub/2018/building-blocks/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2018/building-blocks/</a></figcaption></figure><p id="9655" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">考虑谷歌的可解释性块的一个好方法是作为一个模型，该模型在从基本计算图到最终决策的不同抽象级别上检测关于神经网络决策的见解。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/4e3c1f904f6385f1d75cd8f855607ebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MaQSWl3LCa2SpEvB.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae mt" href="https://distill.pub/2018/building-blocks/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2018/building-blocks/</a></figcaption></figure><p id="25bb" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mq lm ln lo mr lq lr ls ms lu lv lw lx im bi translated">谷歌对深度神经网络可解释性的研究不仅仅是一次理论练习。该研究小组随论文发布了<a class="ae mt" href="https://github.com/tensorflow/lucid" rel="noopener ugc nofollow" target="_blank"> Lucid </a>，这是一个神经网络可视化库，允许开发人员进行排序Lucid功能可视化，以说明神经网络各个部分所做的决策。谷歌也发布了<a class="ae mt" href="https://github.com/tensorflow/lucid#notebooks" rel="noopener ugc nofollow" target="_blank"> colab笔记本</a>。这些笔记本使得使用Lucid在交互式环境中创建清晰的可视化变得极其容易</p></div></div>    
</body>
</html>