<html>
<head>
<title>Knowing Linear Regression better!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更懂线性回归！！</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/linear-regression-fffa6c970075?source=collection_archive---------1-----------------------#2020-11-07">https://pub.towardsai.net/linear-regression-fffa6c970075?source=collection_archive---------1-----------------------#2020-11-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="011d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/statistics" rel="noopener ugc nofollow" target="_blank">统计数据</a></h2><div class=""/><div class=""><h2 id="f2cc" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">OLS和梯度下降</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/8b6c925654dc1e8759d78c499cdd46d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*Ug3nqhhggPB7Pdgk9KcaRg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated"><a class="ae la" href="https://i1.wp.com/cmdlinetips.com/wp-content/uploads/2020/03/Linear_Regression_fit_with_Matrix_Multiplication.jpg?w=594&amp;ssl=1" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="fd53" class="lb lc iq bd ld le lf dn lg lh li dp lj lk ll lm ln lo lp lq lr ls lt lu lv iw bi translated">回归简介</h2><p id="b174" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me lk mf mg mh lo mi mj mk ls ml mm mn mo ij bi translated">回归是监督学习模型的一种算法。当输出或从属特征是连续的并被标记时，我们应用回归算法。回归用于找出自变量和输出变量之间的关系或方程。例如，给定如下，我们有变量x₁，x₂，…,xₙ，其对变量y的输出有贡献。我们必须找到x变量和因变量y之间的关系。因此该方程定义如下，例如-</p><p id="66ce" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">y = F( x₁，x₂，…,xₙ)</p><p id="b923" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">y = 5 x₁+8x₂+….+12xₙ</p><p id="4051" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">为了更好地找到独立特征和从属特征之间的关系，我们将简化独立特征以找到用于理解目的的等式。</p><h2 id="ad69" class="lb lc iq bd ld le lf dn lg lh li dp lj lk ll lm ln lo lp lq lr ls lt lu lv iw bi translated">线性回归</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/e96ce1b3a8bb24a2bb0ea89121154215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMN_lh-2EekXN8zQvIyLiQ.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="6ca8" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">假设我们要根据输入变量Experience计算工资。因此，薪水成为我们的<strong class="ly ja">独立特征x、</strong>，经验成为我们的<strong class="ly ja">非独立特征y、</strong></p><p id="cd27" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">考虑上图中的图(1)，其中y值随着x值的增加而线性增加。可以得出结论，相关性是线性比例的。所以得到的图是一条直线，x和y的关系给定为y=2x+4。</p><p id="80a4" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">考虑上图中的图(2 ),其中y的值与x的增加值不成比例。从图(2)中可以看出，y的值相对于y是不规则的。因此这种类型的回归称为非线性回归。</p><p id="3e98" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">通过以上讨论，我们必须明白生活不会总是一帆风顺的。考虑下图中的图(1 ),图中的点分布在整个平面上。有可能找到最佳拟合线和通过平面上所有点的方程吗？是的，实际上，这是可能的，从下图的图(2)中可以看出。但是可行吗？不，从图(2)中可以清楚地看到，我们过度拟合了数据，这将损害模型在准确性方面的性能。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mz"><img src="../Images/bbb7abc151ea603768968d67cbf50ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MSIUBAg_rs_ark6WhdgNWQ.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="75cc" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">如何避免过拟合，同时又能找到一条穿过所有点的线？我们选择通过所有点的最佳拟合线，使得所有给定点到该线的距离最小，并且我们可以通过最小化误差来找到输出和输入要素之间的关系。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi na"><img src="../Images/9825cee5f407c4963f85e6cb63d0a3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*5jJD5sgcrnWdId1CbOaxTQ.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="ff1e" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">我们绘制了所有的点(x1，y1)，(x2，y2)，…..,(xₙ,yₙ)并计算已被选择以获得最佳拟合线的各条线之间的误差。例如，在下图中，我们有一个点(x₁,y₁)，它使用公式(y₁-ῡ)计算与直线(1)的距离。类似地，我们使用公式(y2-ῡ)计算第二个点(x₂,y₂)到线(1)的距离。这样，我们从(1)中获得每个点的实际结果和预测结果的误差差，然后对误差(1)求和。类似地，我们计算来自线(2)、线(3)的所有点的误差值的总和，因为我们的主要目标是最小化误差，所以我们集中于选择最佳拟合线，其具有位于最小距离的点。</p><p id="9168" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">一旦得到这条线，我们就有了与这条线相关的方程，记为<strong class="ly ja"> Y = θ₀ + θ₁*X </strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/6346b447efafe91be7c69b3ec014fa18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*3tO0EOaxqTnoFdCUJ7Midw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="7351" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">从上图中我们可以看出，主要目标是最小化因变量的实际值和预测值之间的误差总和。这个方程被称为损失函数。该过程将依次优化<strong class="ly ja"> θ₀ </strong>和<strong class="ly ja">θ₁</strong>的值，这将导致定义与空间中所有给定点具有最小距离的最佳拟合线。</p><h2 id="2216" class="lb lc iq bd ld le lf dn lg lh li dp lj lk ll lm ln lo lp lq lr ls lt lu lv iw bi translated">OLS方法</h2><p id="6755" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me lk mf mg mh lo mi mj mk ls ml mm mn mo ij bi translated">应用OLS(普通最小二乘法)来寻找线性回归模型的参数值。我们已经推导出上面的等式<strong class="ly ja"> Y = θ₀ + θ₁*X. </strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/15aefb7dbe9903e95147e2715f5aa652.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*EUCUVIZ0QNiKOM8Knr3HGg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><h2 id="f73b" class="lb lc iq bd ld le lf dn lg lh li dp lj lk ll lm ln lo lp lq lr ls lt lu lv iw bi translated">最大值和最小值的修正</h2><p id="e047" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me lk mf mg mh lo mi mj mk ls ml mm mn mo ij bi translated">在我们继续之前，让我们了解一下最大值和最小值的工程数学概念。点1、2和3指示坡度改变方向的位置，向上或向下。如果你观察，从下图可以看出，如果我们在点1、2和3处画一条切线，它看起来平行于X轴。所以这些点的斜率是0。斜率变为零的这些点称为驻点。此外，下图中斜率值突然开始增加的点1称为全局最小值，反之亦然，点2称为全局最大值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nd"><img src="../Images/ca2bbcbf42b7fdc3bd86a52ade319d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*en_viOTSUdu4i02-9nAUOg.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="7645" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">从上面的概念，我们可以得出结论，我们可以找到任何给定方程的驻点，使其导数等于零，即在该点的斜率等于零。通过求解一阶导数，我们找到了方程的驻点。但是我们需要确定驻点是全局最大值还是全局最小值。</p><p id="b154" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">在求二阶导数时，如果值大于零，则代表全局最小值，如果值小于零，则代表全局最大值。这样，我们就能算出全局最大值和最小值。</p><p id="fd7d" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">我们以下面的例子来理解上面关于最大值和最小值的规则。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/78d49132129f6d3947d3fbc94fc42fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*FUKmI1I7MlqiOQVV87nzfw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="ec98" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">从以上所有讨论中，我们可以观察到，当图达到凸曲线时，它达到最小值。因此，我们可以得出结论，我们讨论的重点必须是达到损失函数的凸性。</p><h2 id="dbb5" class="lb lc iq bd ld le lf dn lg lh li dp lj lk ll lm ln lo lp lq lr ls lt lu lv iw bi translated">优化<strong class="ak"> θ₀和θ₁ </strong></h2><p id="d00e" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me lk mf mg mh lo mi mj mk ls ml mm mn mo ij bi translated">因此，我们手中有一个问题陈述，优化θ₀和θ₁的值，使得损失函数达到凸性(最小值)。正如我们从最大值和最小值概念中学到的，我们集中于获得给定损失函数的驻点，如下所示。使用它，我们得到两个等式，如下图所示。查看这两个方程，我们知道∑x ₙ，∑ yₙ，∑xₙyₙ和∑xₙ可以从给定的数据中获得，这些数据将有一个自变量薪金(x)和因变量(y)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/02ab19a94422f95008e4a79507fb9683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*Vp7YEO0F_PDuTl2txejwKg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/204121afaf69670b781b92b644a87d19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*k6ObV_oW5TyV4xf3LkLjbA.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="e1c3" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">为了更进一步，我们可以对θ₀和θ₁进行二阶导数，形成下图所示的海森矩阵。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c7d346601df2c2f90d2be9f60832592d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*cfOUEzN6ljaoQo7Ty9XbQg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/de2cc6f852a5dcfa3ecaeab529fed80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*NeiFQl_emv0HR_xqOxzJng.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ni"><img src="../Images/ec3adeaebeddb4ab7dff58c72eb117b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*hSDqarLmj3DcnGfh_60Dnw.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><h2 id="2994" class="lb lc iq bd ld le lf dn lg lh li dp lj lk ll lm ln lo lp lq lr ls lt lu lv iw bi translated">利用矩阵求凸性的方法</h2><p id="6824" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me lk mf mg mh lo mi mj mk ls ml mm mn mo ij bi translated">虽然我们找到了θ₀和θ₁在平稳点的值，我们现在需要确认平稳点是凸的还是凹的。为了确认矩阵的凸性，我们有下面给出的方法:</p><p id="e67b" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated"><strong class="ly ja">领先主辅修</strong></p><p id="4b12" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">如果对应于一个主子式的矩阵是较大矩阵的<strong class="ly ja">二次左上部分的行列式(即，它由从1到k的行和列中的矩阵元素组成)，则该主子式被称为<strong class="ly ja">前导主子式</strong>(k阶)。</strong></p><p id="3885" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">第k阶前导主子式是通过删除最后n k行和列形成的第k阶主子式的行列式。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/3db8d348c42c40614673a3be0bae5e21.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*RcnrJZbDOyAdgFEbjMsnXg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="9011" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">A的主前导子矩阵是由A的前r行和r列形成的子矩阵，其中r=1，2，…..，n。</p><p id="72c8" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">这些子矩阵是</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/cfcf5ced949583ba2c6d28027c242c3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*aUfQNCeHY_0FteceO6Ii8g.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="2e3e" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated"><strong class="ly ja">定理:</strong></p><p id="219d" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated"><strong class="ly ja"> <em class="nl">对称矩阵A是正定的当且仅当每个主副行列式都是正的。</em>T13】</strong></p><p id="41e1" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated"><strong class="ly ja">举例理解主次</strong></p><p id="026e" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">给定下面的矩阵A，我们从中形成主要的子矩阵。一旦完成，我们取行列式的值。</p><p id="b8cf" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">如果所有主子式的行列式值都大于零，则称为正定(如2，3，1)。</p><p id="f9c6" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">如果所有主子式的行列式值都大于或等于零，则称之为半正定(如2，0，1)。</p><p id="b5a6" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">如果所有主子式的行列式值都小于零，则称之为负定(如-2，-3，-1)。</p><p id="184a" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">如果所有主子式的行列式值都小于或等于零，则称之为负半定(如-2，0，-1)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nm"><img src="../Images/365e51853c3cc77cb27f5ee217d31d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPWC9BlmHEjUhT70_m73sw.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="08d7" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">从上图可以看出，主副行列式的值大于零，所以它是正的。</p><p id="5c1a" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated"><strong class="ly ja">函数的凸性</strong></p><p id="f300" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">对于给定的矩阵A，</p><p id="da1e" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">a是凸⇔，a是半正定的。</p><p id="bf79" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">a是凹⇔ A是负半定的。</p><p id="222f" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">a是严格凸⇔ A是正定的。</p><p id="6903" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">a是严格凹⇔ A是负定的。</p><h2 id="3132" class="lb lc iq bd ld le lf dn lg lh li dp lj lk ll lm ln lo lp lq lr ls lt lu lv iw bi translated">OLS的局限性</h2><ul class=""><li id="9459" class="nn no iq ly b lz ma mc md lk np lo nq ls nr mo ns nt nu nv bi translated">如果我们观察OLS方法，那么有很高的机会获得假阳性凸点，即，在同一个方程中可能有两个极小值。其中一个可能是局部最小值，而另一个是全局最小值。</li><li id="48f0" class="nn no iq ly b lz nw mc nx lk ny lo nz ls oa mo ns nt nu nv bi translated">此外，没有任何微调参数可用于控制模型的性能。我们无法控制模型的准确性。因为我们的目标是建立一个优化的系统，所以我们必须专注于控制模型效率。</li></ul><h2 id="8890" class="lb lc iq bd ld le lf dn lg lh li dp lj lk ll lm ln lo lp lq lr ls lt lu lv iw bi translated">梯度下降法</h2><p id="1ed7" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me lk mf mg mh lo mi mj mk ls ml mm mn mo ij bi translated">为了克服OLS方法的局限性，我们引入了一种新的模型来优化<strong class="ly ja"> θ₀ </strong>和<strong class="ly ja">θ₁</strong>的值，称为梯度下降法。这里，我们将θ₀ 和θ₁<strong class="ly ja">的值初始化为一些随机值，并计算模型输出的总误差。如果误差不在允许范围内，我们使用学习参数<strong class="ly ja"> α减少<strong class="ly ja"> θ₀ </strong>和<strong class="ly ja"> θ₁ </strong>。</strong></strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/6ad49c12c15b2a12f1fc40a221939bbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*zEjaKspaDMWvX6i6Q5P-bg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="6b7a" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">如果与前一次迭代相比，我们在当前迭代中得到的模型输出的误差值更高，那么我们可能已经在前一次迭代中实现了收敛。或者我们有另一个选项来更新值<strong class="ly ja"> α </strong>并验证输出误差。这里我们对误差函数j(<strong class="ly ja">θ₀,θ₁</strong>)w . r . t<strong class="ly ja">θ₀</strong>和<strong class="ly ja"> θ₁ </strong>求导，并在下一次迭代中使用这些值来获得<strong class="ly ja"> θ₀ </strong>和<strong class="ly ja"> θ₁ </strong>的新值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d3225aba56cdf6827dc79b08dcd4cdcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*Q6UsrejB4q9EnOsxHVZXSg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/763adb32589b2b4f975ccb968db2842e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*uAfACsxEXjYgoWUMq98pIg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</figcaption></figure><p id="e963" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">这都是关于线性回归背后的数学直觉，这有助于我们更好地了解这个概念。人们可以很容易地从Python包中获得执行线性回归时要使用的方法。</p><p id="d309" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">感谢您阅读这篇文章！！</p><p id="0f3d" class="pw-post-body-paragraph lw lx iq ly b lz mp ka mb mc mq kd me lk mr mg mh lo ms mj mk ls mt mm mn mo ij bi translated">我希望这能帮助你更好地理解这个话题。</p></div></div>    
</body>
</html>