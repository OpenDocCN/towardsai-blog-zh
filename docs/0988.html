<html>
<head>
<title>ELECTRA: Pre-Training Text Encoders as Discriminators rather than Generators</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ELECTRA:预训练文本编码器作为鉴别器而不是生成器</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/electra-pre-training-text-encoders-as-discriminators-rather-than-generators-c5661f7ea0d5?source=collection_archive---------0-----------------------#2020-09-30">https://pub.towardsai.net/electra-pre-training-text-encoders-as-discriminators-rather-than-generators-c5661f7ea0d5?source=collection_archive---------0-----------------------#2020-09-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8cd4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></h2><div class=""/><div class=""><h2 id="3b53" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">伊莱克特拉和伯特有什么区别？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/a67285cf104952348737d1f801c1766c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/0*zfiiqyw1cxj2iZvr"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated"><a class="ae ld" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马志威</a>在<a class="ae ld" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="cb60" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><a class="ae ld" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank"> BERT </a> (Devlin et al .，2018)是最近NLP任务的基线。基于BERT架构发布的新模型有<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6" rel="noopener"> RoBERTA </a>(刘等2019)<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/a-lite-bert-for-reducing-inference-time-bed8d990daac" rel="noopener">ALBERT</a>(兰等2019)。Clark等人发布了ELECTRA (Clark等人，2020)，其目标是在保持高质量性能的同时减少计算时间和资源。诀窍是引入用于掩蔽语言模型(MLM)预测的生成器，并将生成器结果转发给鉴别器</p><p id="e043" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">。MLM是BERT中的培训目标之一(Devlin et al .，2018)。然而，由于训练阶段和微调阶段之间的错位，它受到了批评。简而言之，MLM面具令牌由[面具]和模型将预测现实世界，以学习单词的代表性。另一方面，ELECTRA (Clark et al .，2020)包含两个模型，即生成器和鉴别器。屏蔽的令牌将被发送到发生器，并为鉴别器生成替代输入(即ELECTRA模型)。在训练阶段之后，生成器将被丢弃，而我们只保留鉴别器用于微调和推理。</p><p id="5e9a" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">Clark等人将这种方法命名为<strong class="lg jd">替代令牌检测</strong>。在下面的章节中，我们将介绍ELECTRA (Clark et al .，2020)是如何工作的。</p><h1 id="aebb" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">输入数据</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/b3d33c93f3110d576bae41d616bcbe31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fpDsC3n6oWAMz0EUf9U4zg.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">ELECTRA培训流程概述(Clark等人，2020年)</figcaption></figure><p id="d378" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">如前所述，有2个模型处于培训阶段。不是将屏蔽令牌(例如[MASK])馈送给目标模型(即鉴别器/ ELECTRA)，而是训练小MLM来预测屏蔽令牌。不包括任何屏蔽令牌的发生器的输出成为鉴别器的输入。</p><p id="02a6" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">生成器有可能预测到相同的令牌(即上图中的“the”)。它将继续跟踪以生成鉴别器的真实标签。以上图为例，只有“ate”会被标记为“replaced”，而其余的(包括“the”)都是“original”标签。</p><p id="e83a" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">你可以想象生成器是一个小型的屏蔽语言模型(例如BERT)。生成器的目标是为鉴别器和学习单词表示(也称为令牌嵌入)生成训练数据。实际上，生成器的想法类似于在<a class="ae ld" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank"> nlpaug </a>中NLP的数据扩充方法。</p><h1 id="e79a" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">模型设置</h1><p id="e919" class="pw-post-body-paragraph le lf it lg b lh mx kd lj lk my kg lm ln mz lp lq lr na lt lu lv nb lx ly lz im bi translated">为了提高预训练的效率，Clark等人发现在生成器和鉴别器之间共享权重可能不是一个好方法。事实上，它们只在两个模型之间共享令牌和位置嵌入。下图显示了替换标记检测方法优于屏蔽语言模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/96775ab18f9fba993d065af67b62bb7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*DE6H3FpXTWg_BiDPNXlUDg.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">替换标记检测和屏蔽语言模型之间的性能比较(Clark et al .，2020)</figcaption></figure><p id="f431" class="pw-post-body-paragraph le lf it lg b lh li kd lj lk ll kg lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated">其次，较小尺寸的发生器提供了更好的结果。小尺寸的发生器不仅导致更好的结果，而且减少了总的训练时间。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/f22af00b88e4d03c3a24ffa3041f1f0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*2T9leqyGekt4g2iZ8LF2HQ.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">不同发电机尺寸和鉴别器尺寸的性能(Clark等人，2020年)</figcaption></figure><h1 id="91f6" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">调谐超参数</h1><p id="d080" class="pw-post-body-paragraph le lf it lg b lh mx kd lj lk my kg lm ln mz lp lq lr na lt lu lv nb lx ly lz im bi translated">克拉克等人在微调超参数方面做了很多。它包括模型的隐藏大小、学习速率和批量大小。以下是不同尺寸的ELECTRA模型的最佳超参数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ne"><img src="../Images/20e83f507747639356cf460b9a0ae558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5BrksvaMvtKU6swk4Qbvkg.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk translated">预训练超参数(克拉克等人，2020年)</figcaption></figure><h1 id="65f8" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">拿走</h1><ul class=""><li id="b2bb" class="nf ng it lg b lh mx lk my ln nh lr ni lv nj lz nk nl nm nn bi translated"><em class="no">生成对抗网络(GAN) </em>:这种方法类似于生成虚假数据来欺骗或攻击模型的GAN(要了解对抗攻击的更多信息，您可以查看这里的<a class="ae ld" href="https://medium.com/hackernoon/does-your-nlp-model-able-to-prevent-adversarial-attack-45b5ab75129c" rel="noopener"/>和这里的<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/adversarial-attacks-in-textual-deep-neural-networks-245dc90029df" rel="noopener"/>)。然而，发电机从培训ELECTRA是不同的。首先，由生成器生成的正确令牌被认为是“真实的”而不是“伪造的”。此外，生成器被训练为最大似然，而不是欺骗鉴别器。</li><li id="ef41" class="nf ng it lg b lh np lk nq ln nr lr ns lv nt lz nk nl nm nn bi translated">在生产中采用BERT的主要挑战是资源分配。1 G内存几乎是生产中BERT机型的最低要求。可以预见，有越来越多的新的NLP模型专注于减少模型的规模和推理时间。</li></ul><h1 id="b003" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">关于我</h1><p id="e2b9" class="pw-post-body-paragraph le lf it lg b lh mx kd lj lk my kg lm ln mz lp lq lr na lt lu lv nb lx ly lz im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是NLP和平台相关领域的最新工作。欢迎在<a class="ae ld" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与<a class="ae ld" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>联系，或者在<a class="ae ld" href="http://medium.com/@makcedward/" rel="noopener"> Medium </a>或<a class="ae ld" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="6ecf" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">延伸阅读</h1><ul class=""><li id="acaf" class="nf ng it lg b lh mx lk my ln nh lr ni lv nj lz nk nl nm nn bi translated"><a class="ae ld" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">伯特</a>、<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/a-robustly-optimized-bert-pretraining-approach-f6b6e537e6a6" rel="noopener">罗伯塔</a>和<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/a-lite-bert-for-reducing-inference-time-bed8d990daac" rel="noopener">艾伯特</a>简介</li><li id="5684" class="nf ng it lg b lh np lk nq ln nr lr ns lv nt lz nk nl nm nn bi translated">NLP的数据扩充(<a class="ae ld" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank"> nlpaug </a>)</li><li id="6365" class="nf ng it lg b lh np lk nq ln nr lr ns lv nt lz nk nl nm nn bi translated">自然语言处理中的对抗性攻击(<a class="ae ld" href="https://medium.com/hackernoon/does-your-nlp-model-able-to-prevent-adversarial-attack-45b5ab75129c" rel="noopener"> 1 </a>，<a class="ae ld" href="https://medium.com/towards-artificial-intelligence/adversarial-attacks-in-textual-deep-neural-networks-245dc90029df" rel="noopener"> 2 </a>)</li></ul><h1 id="87f9" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">参考</h1><ul class=""><li id="9ce7" class="nf ng it lg b lh mx lk my ln nh lr ni lv nj lz nk nl nm nn bi translated">J.Devlin，M. W. Chang，K. Lee和K. Toutanova。<a class="ae ld" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特:语言理解深度双向转换器的预训练</a>。2018</li><li id="2529" class="nf ng it lg b lh np lk nq ln nr lr ns lv nt lz nk nl nm nn bi translated">Y.刘、m .奥特、n .戈亚尔、j .杜、m .乔希、d .陈、o .利维、m .刘易斯、L. Zettlemoyer和V. Stoyanov。<a class="ae ld" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank"> RoBERTa:一种稳健优化的BERT预训练方法</a>。2019.</li><li id="41be" class="nf ng it lg b lh np lk nq ln nr lr ns lv nt lz nk nl nm nn bi translated">Z.兰、陈、古德曼、金佩尔、夏尔马和索里科特。ALBERT:一个用于自我监督语言表达学习的Lite BERT。2019</li><li id="aa23" class="nf ng it lg b lh np lk nq ln nr lr ns lv nt lz nk nl nm nn bi translated">K.Clark，M. Luong，Q. V. Le，C. D. Manning .<a class="ae ld" href="https://arxiv.org/pdf/2003.10555.pdf" rel="noopener ugc nofollow" target="_blank"> ELECTRA:将文本编码器预先训练成鉴别器而不是生成器</a>。2020</li></ul></div></div>    
</body>
</html>