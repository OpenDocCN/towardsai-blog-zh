<html>
<head>
<title>K-means Clustering in a Nutshell</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简而言之，k-均值聚类</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/k-means-clustering-in-a-nutshell-26b0ca9b9ef1?source=collection_archive---------1-----------------------#2020-07-06">https://pub.towardsai.net/k-means-clustering-in-a-nutshell-26b0ca9b9ef1?source=collection_archive---------1-----------------------#2020-07-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/b85ed3052cf1a4a78837a4b725ac05a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0nkLwwW5NRdxq3x0"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Raj shri bharat KS在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="c0df" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="df05" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">最简单的聚类算法</h2></div><p id="400c" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">K-means是一个<strong class="lg jq"> <em class="ma">无监督聚类</em> </strong>机器学习模型。<br/>在<em class="ma">无监督学习</em>中，数据集不包含用于训练数据的<em class="ma">目标</em>值。<em class="ma">聚类</em>是一种将相似数据点分组的技术。因此，当我们没有目标变量时，以及当机器问题是分组相似的数据时，使用K-means。</p><p id="7904" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这可以通过聚类来实现，使得同一聚类中的点之间的距离，称为<strong class="lg jq">聚类内距离</strong>最小<em class="ma"/>，而不同聚类中的点之间的距离，称为<strong class="lg jq">聚类间距离</strong>最大<em class="ma"/>。</p><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/96b81325df92b2e21053dba6fcdd77c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*E4_ZfFiWG04GOpUBgfpoWA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">集群内距离和集群间距离</figcaption></figure><p id="bc17" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">在本文中，我们将深入了解K-means及其变体，并通过一个例子来展示它。</strong></p><p id="8747" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">文章的流程如下</p><ol class=""><li id="bafd" class="mg mh jg lg b lh li lk ll ln mi lr mj lv mk lz ml mm mn mo bi translated">K-means的目标</li><li id="af63" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz ml mm mn mo bi translated">K-means背后的算法</li><li id="0b4d" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz ml mm mn mo bi translated">K均值的局限性</li><li id="85b3" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz ml mm mn mo bi translated">k-表示++的意思</li><li id="4cf9" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz ml mm mn mo bi translated">k-水母类</li><li id="23bb" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz ml mm mn mo bi translated">履行</li></ol><p id="8e89" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们开始吧！</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="83e2" class="nb nc jg bd nd ne nf ng nh ni nj nk nl kv nm kw nn ky no kz np lb nq lc nr ns bi translated">K-means的目标</h1><p id="a9d2" class="pw-post-body-paragraph le lf jg lg b lh nt kq lj lk nu kt lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">K-means算法将一组<em class="ma"> X </em>的<em class="ma"> N </em>个样本分成<em class="ma"> K </em>个不相交的聚类<em class="ma"> C </em>，每个聚类由样本的<em class="ma">均值μⱼ </em>来描述。这些平均值通常被称为簇“<em class="ma">质心</em>”；注意，一般来说，它们不是来自<em class="ma"> X </em>的点，尽管它们生活在同一个空间。</p><p id="f7d5" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">K-means算法旨在选择最小化<strong class="lg jq">惯性</strong>或<strong class="lg jq">组内平方和标准</strong>的质心:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7eb52e9aa1e1b003ec9b70af4ff2578a.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*3PAb-3GUWm51PJBiBKEnwg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">K-均值的目标函数</figcaption></figure></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="e47c" class="nb nc jg bd nd ne nf ng nh ni nj nk nl kv nm kw nn ky no kz np lb nq lc nr ns bi translated">K-means背后的算法</h1><p id="a2f0" class="pw-post-body-paragraph le lf jg lg b lh nt kq lj lk nu kt lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">使用劳埃德算法解决k-means问题。它包括以下步骤</p><p id="264a" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">步骤1:初始化</strong> <br/>我们从给定的数据集中随机选择K个点作为K个质心</p><p id="8867" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">步骤2:赋值</strong> <br/>基于步骤1中的质心，我们将数据点分组为簇。对于数据集中的每个点，我们将该点分组到聚类中，聚类的质心与数据点的距离最小</p><p id="8f4b" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">步骤3:更新</strong> <br/>根据步骤2中获得的聚类数据点，我们计算质心并用这个新值更新质心</p><p id="9711" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">第四步:迭代和终止<br/> </strong>迭代第二步和第三步，直到质心收敛，即在一次迭代中质心不变时终止</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="9023" class="nb nc jg bd nd ne nf ng nh ni nj nk nl kv nm kw nn ky no kz np lb nq lc nr ns bi translated"><em class="nz">K均值的局限性:</em></h1><ul class=""><li id="ca39" class="mg mh jg lg b lh nt lk nu ln oa lr ob lv oc lz od mm mn mo bi translated">K-means在异构集群(即<em class="ma">不同大小和密度</em>的集群)方面存在问题。并且还具有<em class="ma">非球形</em>(非凸形)结构</li></ul><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/237b206dcc2fad5c8e88e43ecb362265.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*1NvLpIFDHAAVHpi86FNOYg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">非球状结构</figcaption></figure><ul class=""><li id="ab2a" class="mg mh jg lg b lh li lk ll ln mi lr mj lv mk lz od mm mn mo bi translated">k均值<em class="ma">受到异常值</em>的严重影响</li><li id="eedc" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz od mm mn mo bi translated">质心聚类<em class="ma">不可解释</em>。为了克服这个问题，我们使用了K-means的一个小变体，称为<strong class="lg jq"> K-Medoids </strong></li><li id="b1cb" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz od mm mn mo bi translated">K-means对初始化敏感。当我们遇到如下图所示的聚类问题时。我们期望该算法将所有绿色数据点聚类成单个聚类，所有蓝色数据点聚类成单个聚类，所有红色数据点聚类成单个聚类。</li></ul><div class="mc md me mf gt ab cb"><figure class="of is og oh oi oj ok paragraph-image"><img src="../Images/2946e864178cfe9fe0ea869393523979.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*-iPzAaqNT5orRzavrF40Yg.png"/></figure><figure class="of is ol oh oi oj ok paragraph-image"><img src="../Images/0898a7286da544e4d6bf33b4608645e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*m28tmuFNZMr0vfw_HuohbQ.png"/></figure><figure class="of is om oh oi oj ok paragraph-image"><img src="../Images/b1adade1ec3e8481776a9cf1b24f028e.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*toKOuzUp2tfGSqAqk7-qeQ.png"/><figcaption class="iz ja gj gh gi jb jc bd b be z dk on di oo op translated">理想聚类、最优聚类、次优聚类</figcaption></figure></div><p id="b34d" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">尽管我们使用相同的超参数K来执行K-means，但是我们有相等的概率得到完全不同的中心和最右边的结果</p><p id="1afa" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了避免这样的聚类结果，我们使用K-means++算法。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="2796" class="nb nc jg bd nd ne nf ng nh ni nj nk nl kv nm kw nn ky no kz np lb nq lc nr ns bi translated"><strong class="ak"> K-means++ </strong></h1><p id="f954" class="pw-post-body-paragraph le lf jg lg b lh nt kq lj lk nu kt lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">K-means算法和K-means++算法的区别在于，我们聪明地选择质心，而不是随机初始化质心。</p><p id="8d50" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">第一步:初始化</strong> <br/>我们从给定的数据集中随机选择一个数据点作为质心。然后，对于每个数据点，我们计算最近的质心(第一次迭代中的唯一质心)之间的距离，比方说“d”，并以与该距离d成比例的概率选取更多的质心，迭代直到我们得到K个质心。</p><p id="ff81" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">步骤2:赋值</strong> <br/>基于步骤1中的质心，我们将数据点分组为簇。对于数据集中的每个点，我们将该点分组到聚类中，聚类的质心与数据点的距离最小。</p><p id="9de0" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">步骤3:更新</strong> <br/>根据步骤2中获得的聚类数据点，我们计算质心并用这个新值更新质心。</p><p id="853b" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">第四步:迭代和终止<br/> </strong>迭代第二步和第三步，直到质心收敛，即在一次迭代中质心不变时终止。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="f8b9" class="nb nc jg bd nd ne nf ng nh ni nj nk nl kv nm kw nn ky no kz np lb nq lc nr ns bi translated">k-水母类</h1><p id="2625" class="pw-post-body-paragraph le lf jg lg b lh nt kq lj lk nu kt lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">K-means算法和K-medoids算法之间的区别在于，我们没有使用来自空间中所有点的集合的质心，而是添加了质心必须属于数据集的条件。因此，通过观察质心，即数据集中的数据点，我们可以解释结果。实施K-Medoids涉及的步骤如下:</p><p id="50f5" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">第一步:初始化</strong> <br/>我们从给定的数据集中随机选取一个数据点作为质心。然后，对于每个数据点，我们计算最近的质心(第一次迭代中的唯一质心)之间的距离，比方说“d”，并以与该距离d成比例的概率选取更多的质心，迭代直到我们得到K个质心。</p><p id="8bc4" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">第二步:赋值</strong> <br/>基于第一步中的质心，我们将数据点分组。对于数据集中的每个点，我们将该点分组到聚类中，该聚类的质心距离数据点的距离最小(这些质心称为<strong class="lg jq">质心</strong>)</p><p id="44d5" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">步骤3:更新<br/> </strong> <em class="ma">用非medoid数据点交换每个簇的</em> medoid。计算损失，如果损失增加，取消互换。</p><p id="bdd1" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">步骤4:迭代和终止<br/> </strong>对所有聚类中的所有数据点重复步骤3。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="c21e" class="nb nc jg bd nd ne nf ng nh ni nj nk nl kv nm kw nn ky no kz np lb nq lc nr ns bi translated">K-means的实现</h1><p id="fa9c" class="pw-post-body-paragraph le lf jg lg b lh nt kq lj lk nu kt lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">让我们通过可视化来理解结果。创建具有四个聚类的数据集。</p><pre class="mc md me mf gt oq or os ot aw ou bi"><span id="2d7d" class="ov nc jg or b gy ow ox l oy oz">from sklearn.datasets.samples_generator import make_blobs<br/>X, y_true = make_blobs(n_samples=300, centers=4,<br/>                       cluster_std=0.60, random_state=0)<br/>plt.scatter(X[:, 0], X[:, 1], s=50)</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/68f16168b6325a5d91ae5b995067a3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*k4x1s4dgSyIQHnrigcrPvA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">原始资料</figcaption></figure><p id="2ab3" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在任何数据集上应用K-means只是一行代码。感谢Sklearn！</p><pre class="mc md me mf gt oq or os ot aw ou bi"><span id="5d94" class="ov nc jg or b gy ow ox l oy oz">from sklearn.cluster import KMeans<br/>kmeans = KMeans(n_clusters=4)<br/>kmeans.fit(X)<br/>y_kmeans = kmeans.predict(X)</span></pre><p id="3513" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们绘制应用K-均值后的结果。</p><pre class="mc md me mf gt oq or os ot aw ou bi"><span id="7943" class="ov nc jg or b gy ow ox l oy oz">plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')</span><span id="628e" class="ov nc jg or b gy pb ox l oy oz">centers = kmeans.cluster_centers_<br/>plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/3047704c44b3234253113ed266c59694.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*fiHsKwN-RY5IrOt3FUZ6QQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">聚类数据集</figcaption></figure><p id="a2e6" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在上面的图中，黄色、绿色、蓝色、紫色中的每个点都被分成相应的组，每个组中的深色大点是该组的质心。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="d402" class="nb nc jg bd nd ne nf ng nh ni nj nk nl kv nm kw nn ky no kz np lb nq lc nr ns bi translated">优势:</h1><ol class=""><li id="241c" class="mg mh jg lg b lh nt lk nu ln oa lr ob lv oc lz ml mm mn mo bi translated">平均复杂度由O(k n T)给出，其中n是样本数，T是迭代次数。实际上，k-means算法非常快(目前最快的聚类算法之一)</li><li id="2e53" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz ml mm mn mo bi translated">该算法易于理解</li><li id="4da6" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz ml mm mn mo bi translated">它可以很好地扩展到大量的样本</li></ol><h1 id="ff72" class="nb nc jg bd nd ne pd ng nh ni pe nk nl kv pf kw nn ky pg kz np lb ph lc nr ns bi translated">应用:</h1><ul class=""><li id="98de" class="mg mh jg lg b lh nt lk nu ln oa lr ob lv oc lz od mm mn mo bi translated">图象分割法</li><li id="8989" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz od mm mn mo bi translated">通过聚类相似的数据，手动标记这些数据点所需的时间和精力可以大大减少</li><li id="7632" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz od mm mn mo bi translated">大型数据集概述</li><li id="424a" class="mg mh jg lg b lh mp lk mq ln mr lr ms lv mt lz od mm mn mo bi translated">对相关问题进行分组，例如对相似的客户进行分组，对相关的文档进行分组，对具有相似波动的股票进行分组</li></ul></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><p id="b35a" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">谢谢你的阅读。以后我会写更多初学者友好的帖子。请在<a class="ae jd" href="https://medium.com/@ramyavidiyala" rel="noopener">媒体</a>上关注我，以便了解他们。我欢迎反馈，可以通过Twitter <a class="ae jd" href="https://twitter.com/ramya_vidiyala" rel="noopener ugc nofollow" target="_blank"> ramya_vidiyala </a>和LinkedIn <a class="ae jd" href="https://www.linkedin.com/in/ramya-vidiyala-308ba6139/" rel="noopener ugc nofollow" target="_blank"> RamyaVidiyala </a>联系我。快乐学习！</p></div></div>    
</body>
</html>