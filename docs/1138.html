<html>
<head>
<title>Decision Trees in Machine Learning (ML) with Python Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python的机器学习(ML)中的决策树教程</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/decision-trees-in-machine-learning-ml-with-python-tutorial-3bfb457bce67?source=collection_archive---------0-----------------------#2020-11-14">https://pub.towardsai.net/decision-trees-in-machine-learning-ml-with-python-tutorial-3bfb457bce67?source=collection_archive---------0-----------------------#2020-11-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/cf00574916ce8df9b46b58f9743ae719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hAkoqDGyzSNtdpYSQUKh6w.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">来源:图片来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=683437" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a></figcaption></figure><h2 id="f7ee" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>、<a class="ae ep" href="https://towardsai.net/p/category/editorial" rel="noopener ugc nofollow" target="_blank">编辑</a>、<a class="ae ep" href="https://towardsai.net/p/category/programming" rel="noopener ugc nofollow" target="_blank">编程</a></h2><div class=""/><div class=""><h2 id="8f04" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">用Python研究机器学习(ML)中的决策树</h2></div><p id="fdbd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后更新，2021年1月8日</p><p id="e525" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">作者:</strong>萨妮娅·帕维斯，<a class="ae jg" href="https://mktg.best/vguzs" rel="noopener ugc nofollow" target="_blank">罗伯托·伊里翁多</a></p><div class="is it gp gr iu md"><a href="https://members.towardsai.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd jt gy z fp mi fr fs mj fu fw js bi translated">加入我们吧↓ |面向人工智能成员|数据驱动的社区</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">加入人工智能，成为会员，你将不仅支持人工智能，但你将有机会…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">members.towardsai.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ja md"/></div></div></a></div><p id="7e57" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">本教程的代码可在</strong><a class="ae jg" href="https://github.com/towardsai/tutorials/tree/master/decision_tree_learning" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Github</strong></a><strong class="lj jt">上获得，其完整实现也可在</strong><a class="ae jg" href="https://colab.research.google.com/drive/1y2kKG8Blu9WLjjHbSIYqN4h1erI7geia?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">Google Colab</strong></a><strong class="lj jt">上获得。</strong></p><h2 id="fc52" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">目录</h2><ol class=""><li id="5e32" class="nk nl jj lj b lk nm ln nn lq no lu np ly nq mc nr ns nt nu bi translated">什么是决策树？</li><li id="1cd7" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#ed5b" rel="noopener ugc nofollow">决策树示例</a></li><li id="79b7" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#f707" rel="noopener ugc nofollow">构建决策树</a></li><li id="74a9" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#6216" rel="noopener ugc nofollow">节点杂质</a></li><li id="e2a8" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#1289" rel="noopener ugc nofollow">熵</a></li><li id="ab15" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#7bdd" rel="noopener ugc nofollow">基尼</a></li><li id="2b2b" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#84b4" rel="noopener ugc nofollow">决策树学习中的过拟合</a></li><li id="e46c" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#8de5" rel="noopener ugc nofollow">修剪</a></li><li id="24a3" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#1cb8" rel="noopener ugc nofollow">基于决策树分类的优缺点</a></li><li id="45bd" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#f6f6" rel="noopener ugc nofollow">代码实现</a></li><li id="45ef" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#dbc8" rel="noopener ugc nofollow">高级决策树</a></li><li id="c75a" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#3317" rel="noopener ugc nofollow">结论</a></li><li id="6663" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#029f" rel="noopener ugc nofollow">资源</a></li><li id="817f" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc nr ns nt nu bi translated"><a class="ae jg" href="#1f0f" rel="noopener ugc nofollow">参考文献</a></li></ol></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><blockquote class="oh"><p id="a714" class="oi oj jj bd ok ol om on oo op oq mc dk translated">📚查看我们的<a class="ae jg" href="https://towardsai.net/p/data-science/gradient-descent-algorithm-for-machine-learning-python-tutorial-ml-9ded189ec556" rel="noopener ugc nofollow" target="_blank">梯度下降教程</a>。📚</p></blockquote><h1 id="8c20" class="or mt jj bd mu os ot ou mx ov ow ox na ky oy kz nd lb oz lc ng le pa lf nj pb bi translated">什么是决策树？</h1><p id="5dca" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">决策树是机器学习、统计学、数据挖掘和机器学习中分类和预测问题的重要和流行的工具[ <a class="ae jg" href="https://en.wikipedia.org/wiki/Decision_tree" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]。它描述了可以被人类解释并应用于知识系统(如数据库)的规则。基本上，决策树<em class="pf"> T </em>以树结构的形式对<em class="pf"> d </em>(分类器或回归函数)进行编码，该树结构呈现以下属性:</p><ul class=""><li id="9700" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated"><strong class="lj jt">决策节点</strong>:定义对单个属性的测试。</li><li id="e73a" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated"><strong class="lj jt">叶节点</strong>:显示目标属性的值。</li><li id="eea7" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated"><strong class="lj jt">边缘</strong>:是一个属性的拆分。</li><li id="f75e" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated"><strong class="lj jt">路径</strong>:做最后决定的是一个析取测试。</li></ul><p id="21ee" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些是决策树的其他名称:</p><ul class=""><li id="043c" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">树形分类器。</li><li id="0556" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">分而治之策略。</li><li id="66dc" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">分级分类器。</li><li id="9d23" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">多级分类器。</li></ul><p id="fe79" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">它通过从树根开始并穿过树根到达叶子节点来对案例进行分类。</p><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/684034af06dc43e2c2d52c60458acf73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4c_F6pbeBkzAP_P2ADQbPw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图1:决策树。</figcaption></figure><p id="e57e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">决策树使用节点和树叶来做出决策。</p><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/81437d9975c0e8c225c55e5b3c931e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1eJ39MafBcKg0_l0.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图2:向量分类。</figcaption></figure><p id="1a93" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以决策树的形式表示上述分类:</p><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/508524c4d49c991bf294b1004a37d457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8jpnVzRqABg_dZNl.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图3:用于分类的决策树。</figcaption></figure><h1 id="ed5b" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">决策树示例</h1><h2 id="793b" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated"><strong class="ak">问题陈述</strong></h2><p id="e2db" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">预测两个不同队(第一队和第二队)之间的篮球比赛结果。</p><h2 id="040f" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated"><strong class="ak">游戏相关的可用知识或属性列表</strong></h2><ul class=""><li id="440f" class="nk nl jj lj b lk nm ln nn lq no lu np ly nq mc pj ns nt nu bi translated">彼得打中锋还是前锋？</li><li id="7bd7" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">比赛的地点是哪里——主场还是客场？</li><li id="d2e1" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">比赛的开始时间是什么时候？</li><li id="81e1" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">对方中锋高不高？</li></ul><h2 id="e731" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated"><strong class="ak">历史数据</strong></h2><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pu"><img src="../Images/5e21201ec939789147229dc20cea13a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z5sgZTGyfUk7GYIk.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图4:一场篮球比赛的历史数据。</figcaption></figure><h2 id="086e" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated"><strong class="ak">预测数据</strong></h2><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pv"><img src="../Images/bfef2076548b58c0330ea4e5745dd2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CiF9y1Oz6BDweQji.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图5:预测篮球比赛的数据。</figcaption></figure><p id="df70" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，根据图5，历史和预测数据:</p><ul class=""><li id="915d" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">将学到的规则推广到新数据中。</li><li id="a467" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">这是一个分类问题。</li></ul><h1 id="f707" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">构建决策树</h1><p id="8d4b" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">决策树学习包括学习一系列<strong class="lj jt"> if/else </strong>查询，这些查询可以让我们几乎立即找到“正确”的答案。这些问题也被称为测试。它搜索所有可能的测试，并找到对目标变量最有指导意义的一个。</p><h2 id="1d17" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">如何生长决策树？</h2><ul class=""><li id="7f01" class="nk nl jj lj b lk nm ln nn lq no lu np ly nq mc pj ns nt nu bi translated">顶层节点也称为根节点，它代表整个数据集。</li><li id="1f30" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">决策树逐渐将训练集分成越来越小的子集。</li><li id="7f74" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如果测试为“真”，则为左侧节点分配一个点；否则，它被分配给正确的节点。</li><li id="384d" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">包含数据点并且共享相同目标值的树叶被称为<strong class="lj jt">纯</strong> [ <a class="ae jg" href="https://towardsai.net/p/data-science/best-data-science-books-free-and-paid-data-science-book-recommendations-b519046dcca5#5ed1" rel="noopener ugc nofollow" target="_blank"> 1 </a>。</li><li id="e35e" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">对新数据点的预测是通过检查该点位于特征空间的分区的哪个区域，然后预测该区域中的多数目标来进行的[ <a class="ae jg" href="https://towardsai.net/p/data-science/best-data-science-books-free-and-paid-data-science-book-recommendations-b519046dcca5#5ed1" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]。</li></ul><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="75f4" class="ms mt jj px b gy qb qc l qd qe">from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_splitcancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/a2e95ff752c3307f3597bedbd37bbdad.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/0*-xN7fbuEj1axXd5J.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图6:训练和测试数据集的准确性。</figcaption></figure><ul class=""><li id="c539" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">如果我们不限制决策树的深度，它可以变得任意深和复杂。</li><li id="4771" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如图6所示，训练集上的准确率是100% —因为叶子是纯的，所以树已经长得足够深，可以毫无疑问地记住训练数据上的所有标签。</li><li id="07c0" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">树可以长得很大，这种树很难理解。较大的树通常不如较小的树精确。</li></ul><p id="5ef1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">创建决策树叶子的代码实现(叶子是基于ml _ tasks回归或分类创建的):</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="68fe" class="ms mt jj px b gy qb qc l qd qe">def create_leaf(data, ml_task):<br/>    label_column = data[:, -1]<br/>    <br/>    if ml_task == "regression":<br/>        leaf = np.mean(label_column)<br/>    else:<br/>        unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)<br/>        i = counts_unique_classes.argmax()<br/>        leaf = unique_classes[i]<br/>    <br/>    return leaf</span></pre><p id="30b7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">决策树的主要目的是选择合适的特征来将树分割成子部分。然后，我们在分割期间在后台应用ID3算法。</p><h1 id="6216" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">节点杂质</h1><p id="78d6" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">节点杂质是节点内的同质性。如果事例有多个响应值，则节点是不纯的。如果所有实例都具有相同的响应值或目标变量或杂质值= 0，则节点是纯的。</p><p id="5896" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是两种最常用的测量节点杂质的方法:</p><ul class=""><li id="a7d3" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">熵。</li><li id="a80f" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">基尼。</li></ul><p id="0043" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基于子节点的不纯程度来选择最佳分割。当节点处的所有模式属于同一类别时，节点杂质为0。当节点N上的所有类别都有相同的可能性时，杂质达到最大值。</p><p id="aefb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">检查纯度的代码片段:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="818b" class="ms mt jj px b gy qb qc l qd qe">def check_purity(data):<br/>    label_column = data[:, -1]<br/>    unique_classes = np.unique(label_column)<br/>    <br/>    if len(unique_classes) == 1:<br/>        return True<br/>    else:<br/>        return False</span></pre><h1 id="1289" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">熵</h1><p id="0f0c" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">在决策树中，熵是一种无序或不确定性。它是对一堆数据中的杂质、无序或不确定性的度量。这是一种控制由决策树决定的数据分割的方法。它影响决策树如何形成它的边界。我们使用熵来衡量数据集的不纯度或随机性。</p><p id="2276" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">给定如下所示的熵方程:</p><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qg"><img src="../Images/833db44b27d312a7010d984dbd7f8abf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fFyouSC1wvbANKSa.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图7:熵的等式。</figcaption></figure><p id="5505" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">计算熵的代码片段:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="e3e3" class="ms mt jj px b gy qb qc l qd qe">def get_entropy(data):<br/>    label_col = data[:, -1]<br/>    a, counts = np.unique(label_col, return_counts=True)<br/>    prob = counts / counts.sum()<br/>    entropy = sum(probabilities * -np.log2(probabilities))<br/>    <br/>    return entropy</span></pre><p id="3412" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">熵的一个简单例子:</p><p id="7e84" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设有一款包描述了两种不同的场景:</p><ul class=""><li id="8c19" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated"><strong class="lj jt">袋子A </strong>有100个绿色的球。彼得想从这个包里选一个绿色的球。这里，袋A的熵为0，因为它意味着0杂质或总纯度。</li><li id="0f93" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">我们用红球替换A袋中的40个绿球，同样，我们用黑球替换10个绿球。现在，约翰想从这个包里选择一个绿色的球。在这种情况下，由于袋子杂质的增加，抽取绿色球的概率将从1.0下降到0.5。</li></ul><p id="b2a0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">香农熵</strong>模型使用以2为底的对数函数<strong class="lj jt"> (log2(P(x)) </strong>来度量熵。</p><h2 id="3c66" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">香农信息论</h2><p id="50df" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">只有两类——是，不是。</p><p id="1bd8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们的例子中，<em class="pf"> t </em>是发送给接收者的一组消息，接收者必须猜测它们的类别，因此:</p><ul class=""><li id="4ad3" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">如果<strong class="lj jt"> p(Yes | t) = 1 (resp。，p(No | t) = 1) </strong>，那么接收者猜测一个新的例子为是。没有消息需要发送。</li><li id="012b" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如果<strong class="lj jt"> p(Yes | t) = p (No | t) = 0.5 </strong>，那么接收者无法猜测，我们必须告诉他们一个新例子的类，发送一个一位的消息。</li><li id="c5be" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如果<strong class="lj jt"> 0 &lt; p(Yes | t) &lt; 1 </strong>，那么接收器平均需要少于一个比特来知道一个新例子的类别。</li></ul><h2 id="0e45" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">信息增益</h2><p id="3221" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">信息增益衡量一个特征提供了多少关于类的信息。</p><p id="cc7d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于以下几点，信息增益在决策树中非常重要:</p><ul class=""><li id="6123" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">它是决策树算法建立决策树所接受的主键。</li><li id="6eb3" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">决策树将永远试图最大化信息增益。</li><li id="c618" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">具有最高信息增益的属性将首先被测试或分割。</li></ul><p id="6563" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图显示了信息增益的等式:</p><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qh"><img src="../Images/223eb2067b206595b06e4f97306454cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a-kFSUngaDNDCiEi.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图8:信息增益的等式。</figcaption></figure><h2 id="7f84" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">熵计算示例</h2><p id="17bf" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">有一条供车辆行驶的道路，这条道路具有多种特征，如坡度、颠簸、限速等。</p><p id="0890" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是数据集:</p><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qi"><img src="../Images/8e11e81b7799eeebb331042918c7d7ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T3BP4QxvP7PCv0Sk.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图9:道路数据集。</figcaption></figure><p id="f15e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">功能:</strong></p><ul class=""><li id="15e3" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">情况</li><li id="62b9" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">崎岖不平</li><li id="261e" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">速度限制</li></ul><p id="5f9c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">标签:</strong></p><ul class=""><li id="e59a" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">速度</li></ul><p id="8dba" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">观察总数:4</p><p id="8853" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">计算等级特征的熵:</strong></p><p id="1064" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以标签为父节点像<strong class="lj jt"> SSFF </strong> →慢慢快快。</p><p id="6d6a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">SSFF的熵:</p><p id="6150" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">p(慢)= 2/4 = 0.5</p><p id="fe6b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">p(快速)= 2/4 = 0.5</p><p id="47db" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所以，SSFF的熵:</p><p id="349d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">母体的熵= { 0.5 log2(0.5)+0.5 log2(0.5)} =-{-0.5+(-0.5)}</p><p id="5b6b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，父代的熵= 1</p><p id="c8f0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，为了找到grade特性的信息增益，按照grade特性分割父节点，如图10所示。</p><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qj"><img src="../Images/6d82cb3f09078912241158bdec18312f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7ifhzAhTpDDPqjwT.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图10:等级特性对父节点的分割。</figcaption></figure><p id="9259" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">分别计算两个(左和右)子节点SSF和F的熵。</p><p id="200d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">节点F </strong>的熵= 0(注意:0，因为都来自同一个类)</p><p id="15dc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">节点SSF的熵:</strong></p><p id="5354" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">p(慢)= 2/3 = 0.334</p><p id="f74f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">p(快速)= 1/3 = 0.334</p><p id="d21e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所以，<strong class="lj jt">节点SSF </strong>的熵=-{ 0.667 log2(0.667)+0.334 log2(0.334)}</p><p id="25d9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi">= -{-0.38 + (-0.52)} = 0.9</p><figure class="pl pm pn po gt iv gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/cb1b71924052d2f48dfe98fe1dbdef3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/0*1-E95G5yUluMN0t-.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图11:父节点和子节点中的节点数量。</figcaption></figure><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ql"><img src="../Images/80daf9df674eb3703a66b6b251e7f5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JG2UZN7ODUGALijP.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图12:计算子节点熵的等式。</figcaption></figure><figure class="pl pm pn po gt iv gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/4268b2a4d51e698ee8c1d8da2b0c5249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*g_qIYFj3CS8hwMq6.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图13:带有加权平均值的子节点的熵。</figcaption></figure><p id="cbd7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从等式中得到的等级信息增益= 1–0.675 = 0.325</p><p id="00ed" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">熵的范围在<strong class="lj jt"> 0到1 </strong>之间。</p><h1 id="7bdd" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">基尼</h1><p id="1fd6" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">像熵一样，基尼指数也是决策树中用于计算信息增益的一种标准。决策树使用信息增益来分割节点。基尼衡量的是一个节点的杂质。</p><p id="a04c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基尼系数的范围在0到0.5之间。与熵相比，基尼系数更适合用来选择最佳特征[ <a class="ae jg" href="https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]。</p><p id="5a41" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">衡量杂质的基尼系数公式:</p><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qn"><img src="../Images/82c7d527117f27ff0df913971c3ac508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KxkOUhhv8ZGVUpea.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图14:基尼系数公式。</figcaption></figure><p id="609c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基尼指数的分割标准:</p><ul class=""><li id="d202" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">它假设每个属性存在几个可能的分割值。</li><li id="dab9" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">所有属性都被假定为连续值</li><li id="d15d" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">可以针对分类属性对其进行修改。</li></ul><h1 id="84b4" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">决策树学习中的过拟合</h1><p id="229d" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">过拟合是机器学习中的一个严重问题，它会导致模型中最差的性能问题。类似地，由于以下问题，决策树也可能面临过度拟合的问题:</p><ul class=""><li id="19c4" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">如果决策树长得太长。</li><li id="8df5" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如果决策树中的实例数量随着树的构建而变小。</li></ul><p id="4256" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们使用修剪来避免决策树中的过度拟合。</p><h1 id="8de5" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">修剪</h1><p id="ed11" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">修剪是调整决策树以最小化误分类错误的过程。这是分裂的逆过程。</p><p id="faf6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有两种方法可以执行修剪:</p><ul class=""><li id="593c" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">预修剪。</li><li id="aa2b" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">后期修剪。</li></ul><p id="28f6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">它识别并移除反映噪声或异常值的分支。</p><h2 id="11eb" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">完全树</h2><p id="4a1b" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">完整的树表示树的停止模式。它遵循以下步骤:</p><ul class=""><li id="1b34" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">如果所有记录都属于同一个类，那么停止展开一个节点。</li><li id="05f8" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如果所有记录都有相同的属性值，那么停止展开一个节点。</li></ul><h2 id="245b" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">预修剪</h2><p id="ed28" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">在预修剪方法中，树不会完全生长。它遵循早期停止规则。它遵循以下步骤:</p><ul class=""><li id="25e6" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">在算法变成完全成长的树之前停止算法。</li><li id="b01c" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如果所有实例都属于同一个类，则停止。</li><li id="26e0" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如果所有属性值都相同，则停止。</li></ul><h2 id="cb77" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">后期修剪</h2><p id="afe8" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">后剪枝是决策树中避免过度拟合的最流行的方法。它通过以下步骤从根本上解决了过度拟合的问题:</p><ul class=""><li id="51d8" class="nk nl jj lj b lk ll ln lo lq pg lu ph ly pi mc pj ns nt nu bi translated">完全生长决策树。</li><li id="d5b4" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">遵循自下而上的方法来修剪决策树的节点。</li><li id="e92b" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如果在修剪节点后泛化的误差有所改善，则用叶节点替换子树。</li><li id="caae" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">在子树中，叶节点的类标签由多数类确定。</li></ul><h1 id="1cb8" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">基于决策树的分类的优缺点</h1><p id="902c" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">以下是基于决策树的分类的优点和缺点:</p><h2 id="5eb3" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">优势</h2><ul class=""><li id="21e4" class="nk nl jj lj b lk nm ln nn lq no lu np ly nq mc pj ns nt nu bi translated">建造它非常便宜。</li><li id="d181" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">它提供了出色的准确性。</li><li id="4acd" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">在“未知”记录的分类上非常快。</li><li id="a686" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">对于小尺寸的树，解释起来很简单。</li><li id="84b9" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">它可以处理连续属性和符号属性。</li><li id="6cd1" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">它对噪声数据具有可接受的性能。</li></ul><h2 id="3202" class="ms mt jj bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj jp bi translated">不足之处</h2><ul class=""><li id="a0cc" class="nk nl jj lj b lk nm ln nn lq no lu np ly nq mc pj ns nt nu bi translated">可能存在系统内存问题，因为数据需要放入内存中。</li><li id="a7f2" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">如果有新数据，需要重新训练。</li><li id="e29e" class="nk nl jj lj b lk nv ln nw lq nx lu ny ly nz mc pj ns nt nu bi translated">它有轴平行决策边界的问题。</li></ul><h1 id="f6f6" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">代码实现</h1><p id="3c4c" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">对于这个例子，我们将使用虹膜数据集。这里我们一步一步地展示决策树的代码实现:</p><p id="bdda" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">导入必要的库:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="d373" class="ms mt jj px b gy qb qc l qd qe">import numpy as np <br/>import pandas as pd <br/>import matplotlib.pyplot as plt<br/>import seaborn as snsfrom sklearn import tree%matplotlib inline</span></pre><p id="6d11" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">读取虹膜数据集:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="910a" class="ms mt jj px b gy qb qc l qd qe">data = pd.read_csv('Iris.csv')<br/>data</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qo"><img src="../Images/e9987ca6e5f720b7d0aa4e79038f03dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1amYHmzfJg3eTMei.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图15:虹膜数据集。</figcaption></figure><p id="dcc2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">虹膜数据的形状:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="ee33" class="ms mt jj px b gy qb qc l qd qe">data.shape</span></pre><p id="2218" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Iris数据集的列名:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="90cb" class="ms mt jj px b gy qb qc l qd qe">col_names = ['id', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']data.columns = col_namescol_names</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qp"><img src="../Images/4cd8664d75fb04b9f8b10c12f98d5236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gfFBGtSXYVZeAAqd.png"/></div></div></figure><p id="bea3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Iris数据集的列</p><p id="29ea" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从数据集中删除“id”列:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="4598" class="ms mt jj px b gy qb qc l qd qe">data = data.drop(['id'], axis=1)</span></pre><p id="1ea9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">检查虹膜数据集的头部:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="1cf6" class="ms mt jj px b gy qb qc l qd qe">data.head()</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/412759848c75d8998043a0719fba65ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*Q3R7m3fZCVGtay2E.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图16:虹膜数据集。</figcaption></figure><p id="d9fe" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">获取虹膜数据集信息:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="d44f" class="ms mt jj px b gy qb qc l qd qe">data.info()</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/2a9bf3b700e003b4653917bf4b7c5cb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/0*Eyoas7jCz4dsN1wF.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图17:虹膜数据集信息。</figcaption></figure><p id="320f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">获取计数:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="cb2a" class="ms mt jj px b gy qb qc l qd qe">data['species'].value_counts()</span></pre><p id="df37" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">获取目标列:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="01b4" class="ms mt jj px b gy qb qc l qd qe">target_col = ['species']</span></pre><p id="4dd5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">获取X和y的值:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="62da" class="ms mt jj px b gy qb qc l qd qe">X = data.drop(['species'], axis=1)y = data['species']</span></pre><p id="5f77" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数据集的拆分:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="0339" class="ms mt jj px b gy qb qc l qd qe">from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)</span></pre><p id="afd4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">决策树分类器:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="3329" class="ms mt jj px b gy qb qc l qd qe">from sklearn.tree import DecisionTreeClassifier</span></pre><p id="37ee" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">应用带有基尼指数的决策树分类:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="8f44" class="ms mt jj px b gy qb qc l qd qe">clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)</span></pre><p id="3532" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">适合模型:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="d2f7" class="ms mt jj px b gy qb qc l qd qe">clf_gini.fit(X_train, y_train)</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/ec46d631120a0dec2cecadb9dd5faf2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/0*u47g-a3tyd3EtVPJ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图18:决策树分类器。</figcaption></figure><p id="9ac8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">预测:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="15d0" class="ms mt jj px b gy qb qc l qd qe">y_pred_gini = clf_gini.predict(X_test)<br/>y_pred_gini</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qt"><img src="../Images/1a3dc214a273b1d6f6c48177a6b681dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1oyFik6pDvO0Qm1s.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图19:预测。</figcaption></figure><p id="84cd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用标准基尼指数获得准确性:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="f305" class="ms mt jj px b gy qb qc l qd qe">from sklearn.metrics import accuracy_scoreprint('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred_gini)))# y_pred_gini are the predicted class labels in the test-set.</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div class="gh gi qu"><img src="../Images/3139806facefa2cc2da799282d363421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/0*Ke0TEnOFbcZjURW_.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图20:模型准确性。</figcaption></figure><p id="ad53" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">检查是否过度配合和配合不足:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="9878" class="ms mt jj px b gy qb qc l qd qe">print('Training set score: {:.4f}'.format(clf_gini.score(X_train, y_train)))print('Test set score: {:.4f}'.format(clf_gini.score(X_test, y_test)))</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/72b507b0daff1d8d941e1bbfcd7a9f46.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*zqtlQVk3pf39RwQg.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图21:训练和测试数据集得分。</figcaption></figure><p id="e475" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">绘制决策树:</p><pre class="pl pm pn po gt pw px py pz aw qa bi"><span id="eef1" class="ms mt jj px b gy qb qc l qd qe">plt.figure(figsize=(12,8))<br/>tree.plot_tree(clf_gini.fit(X_train, y_train))</span></pre><figure class="pl pm pn po gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi qw"><img src="../Images/6c5d2d38163772d4555bde2bd2ca18c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5dxGIL9CzxzANkSP.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk translated">图Iris数据集的决策树图。</figcaption></figure><h1 id="dbc8" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">高级决策树</h1><p id="4e04" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">决策树还有另一个重要的改进，那就是<strong class="lj jt">随机森林</strong>。这类似于决策树，但需要多个树，因此命名为森林。<strong class="lj jt">随机森林</strong>集成一种用于机器学习的监督学习技术。在单个决策树中有更多的机会过度拟合，但是在多个树在一起的情况下，随着树越来越深，训练误差是最小的。</p><h1 id="3317" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated">结论</h1><p id="06f6" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">决策树被广泛使用，并且是预测建模中最常用的方法之一。它们有助于预测未来，非常容易理解。它们处理离散属性的效率更高，但是这些树很有可能遭受错误传播。</p><p id="980f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">决策树对异常值也不敏感，因为分割是基于分割范围内样本的比例而不是绝对值进行的[ <a class="ae jg" href="https://www.elsevier.com/books/T/A/9780444520753" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]。它们非常直观，很容易向非技术用户解释。</p><p id="f2b1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">决策树的另一个关键实践是参数之间的非线性关系不会影响树的性能。因此，在决策树中，对高维数据的预测更快。</p></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><p id="0e6f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">免责声明:</strong>本文表达的观点仅代表作者个人观点，不代表卡内基梅隆大学或其他(直接或间接)与作者相关的公司的观点。这些文章并不打算成为最终产品，而是当前思想的反映，同时也是讨论和改进的催化剂。</p><p id="70f2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">除非另有说明，所有图片均来自作者。</strong></p><p id="c641" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过<a class="ae jg" href="https://towardsai.net/" rel="noopener ugc nofollow" target="_blank">向艾</a>发布</p><h1 id="029f" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated"><strong class="ak">资源</strong></h1><p id="d191" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated"><a class="ae jg" href="https://github.com/towardsai/tutorials/tree/master/decision_tree_learning" rel="noopener ugc nofollow" target="_blank"> Github库</a>。</p><p id="753d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" href="https://colab.research.google.com/drive/1y2kKG8Blu9WLjjHbSIYqN4h1erI7geia?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google colab实现</a>。</p><h1 id="1f0f" class="or mt jj bd mu os ot ou mx ov ow ox na ky pr kz nd lb ps lc ng le pt lf nj pb bi translated"><strong class="ak">参考文献</strong></h1><p id="7d01" class="pw-post-body-paragraph lh li jj lj b lk nm kt lm ln nn kw lp lq pc ls lt lu pd lw lx ly pe ma mb mc im bi translated">[1]Python机器学习入门:数据科学家指南第1版，Andreas C.Muller，Sarah Guido，<a class="ae jg" href="https://towardsai.net/p/data-science/best-data-science-books-free-and-paid-data-science-book-recommendations-b519046dcca5#5ed1" rel="noopener ugc nofollow" target="_blank">https://toward sai . net/p/Data-science/best-Data-science-books-free-and-payed-Data-science-book-recommendations-b 519046 DCCA 5 # 5ed 1</a></p><p id="1498" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]决策树算法，Comp328教程1，张开，幻灯片分享，<a class="ae jg" href="https://www.slideshare.net/Ami_Surati/decision-tree-51573521" rel="noopener ugc nofollow" target="_blank">https://www . Slide Share . net/Ami _ Surati/Decision-Tree-51573521</a></p><p id="d809" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3]决策树中的基尼杂质和熵— ML，极客为极客，<a class="ae jg" href="https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/" rel="noopener ugc nofollow" target="_blank">https://www . geeksforgeeks . org/Gini-inexture-and-Entropy-in-Decision-Tree-ML/</a></p><p id="e3c4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4]决策树，维基百科，<a class="ae jg" href="https://en.wikipedia.org/wiki/Decision_tree" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Decision_tree</a></p><p id="f6a9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]决策树，现代信息处理，第1版。纸质书和电子书，伯纳黛特·布琼-梅尼尔·朱利安尼拉·科莱蒂罗纳德·r·雅戈，国际标准书号9780444520753，9780080461694，<a class="ae jg" href="https://www.elsevier.com/books/T/A/9780444520753" rel="noopener ugc nofollow" target="_blank">https://www.elsevier.com/books/T/A/9780444520753</a></p></div></div>    
</body>
</html>