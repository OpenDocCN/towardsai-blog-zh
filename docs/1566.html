<html>
<head>
<title>ShaRF: Take a Picture From a Real-Life Object, and Create a 3D Model of It</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ShaRF:从现实生活中的物体上拍一张照片，然后创建一个它的3D模型</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/sharf-take-a-picture-from-a-real-life-object-and-create-a-3d-model-of-it-c6809806b32?source=collection_archive---------2-----------------------#2021-02-22">https://pub.towardsai.net/sharf-take-a-picture-from-a-real-life-object-and-create-a-3d-model-of-it-c6809806b32?source=collection_archive---------2-----------------------#2021-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="38b6" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/computer-vision" rel="noopener ugc nofollow" target="_blank">计算机视觉</a></h2><div class=""/><div class=""><h2 id="4efc" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">想象一下，拍一张物体的照片，然后以3D形式插入到你正在创作的电影或视频游戏中，或者插入到3D场景中作为插图，那该有多酷。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/075eaf1e83edbe507e291d1008431b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GCMDXpOaQgAQFa2QNwsuqQ.png"/></div></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ld le l"/></div></figure><p id="741d" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">单幅图像的神经场景表示是一个非常复杂的问题。“最终目标”是能够从现实生活中的物体拍摄照片，并将这张照片转换为3D场景。这意味着模型理解整个三维场景，或现实生活中的场景，使用单一图片的信息。即使对人类来说，这有时也很困难，因为图像中的颜色或阴影会欺骗我们的眼睛。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mb"><img src="../Images/3faa0eef18a01a69052ca434adad9e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3QXwM9KfSZEXvaAy"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">由<a class="ae mg" href="https://unsplash.com/@lazycreekimages?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">迈克尔·泽兹奇</a>在<a class="ae mg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/4c807cf8c87ff6c2509eed33549d67b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*XZgGKXsRZ1LvoPRUQEV6fA.png"/></div></figure><p id="a2e9" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">哦，它不仅需要理解图像的深度，这已经是一项具有挑战性的任务，而且它还需要用正确的材料和纹理重建物体，使它们看起来真实。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/eb8cf9201f425000782b579b01679ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*g8CwPQrczIkp_T2PxeXkTA.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">更多结果… [1]</figcaption></figure><p id="5b14" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">想象一下，拍一张物体的照片，然后以3D形式插入到你正在创作的电影或视频游戏中，或者插入到3D场景中作为插图，那该有多酷。</p><p id="e3c0" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">好吧，我不是唯一一个考虑这种类型的模型可能创造的所有可能性的人，因为谷歌的研究人员正在他们的新论文“ShaRF:来自单个视图的形状调节辐射场”中研究这个问题。</p><div class="ks kt ku kv gt ab cb"><figure class="mi kw mj mk ml mm mn paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/aaba21b571e322d2cf5970a22d59f378.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*XQN6trPZebaO16JEIuzXmA.png"/></div></figure><figure class="mi kw mo mk ml mm mn paragraph-image"><img src="../Images/bc5188ca7b2debd582a80cadb543fa5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*uYIjSxYH9urYp7Eavw8x2Q.gif"/><figcaption class="mc md gj gh gi me mf bd b be z dk mp di mq mr translated">更多结果… [1]</figcaption></figure></div><p id="ee7e" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">请注意，对于您看到的每个结果，他们只使用了一张从任何角度拍摄的照片。然后，它被发送到模型，以产生这些结果，当你想到任务的复杂性和所有可能的参数时，这对我来说是不可思议的。例如照明、分辨率、尺寸、角度或视点、图像中物体的位置等。！如果你和我一样，你可能想知道他们是怎么做到的。</p><p id="7532" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">好吧，我撒了一点小谎，他们不仅把图像作为网络的输入，而且还把相机参数作为过程的辅助。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ms"><img src="../Images/12bf16f878029469c741a62cbaeebc91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yzwUGvYfCnWvyz67mUSCGQ.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">图片来自[1]</figcaption></figure><p id="5f6e" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">他们的算法学习了一种功能，可以将这些3D点和3D视点转换为RGB颜色以及每个点的密度值。提供足够的信息，以便稍后从任何视点渲染场景。这称为辐射场，将位置及其观察方向作为输入，输出每个点的颜色和体积密度值。</p><p id="0b3a" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">这与NeRF的做法非常相似。我已经完成了一篇论文。基本上，在NeRF情况下，辐射场函数是使用在图像和预期输出上训练的神经网络来完成的。这意味着他们需要每个场景的大量图像，以及为每个场景训练不同的网络。使得这个过程非常昂贵和低效。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ld le l"/></div></figure><p id="a650" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">因此，我们的目标是找到一种更好的方法来获得所需的辐射场，由RGB和密度值组成，然后在新的视图中以3D形式呈现对象。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mt"><img src="../Images/6a54110a0d424b9f55592463e4bb4e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMgBunuXbWLEsqwH1vXIdQ.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">模型[1]</figcaption></figure><p id="1fe8" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">为了获得创建这种辐射场所需的信息，他们使用了他们所谓的形状网络，将图像的潜在代码映射到由体素组成的3D形状。体素就像像素一样，但是在三维空间中，并且所讨论的潜在代码基本上是图像中物体形状的所有有用信息。这种压缩的形状信息是使用由完全连接的层和随后的卷积组成的神经网络找到的，卷积是计算机视觉应用的强大架构，因为卷积具有两个主要属性:它们对于平移是不变的，并且使用图像的局部属性。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mu"><img src="../Images/c3c414c40ae487aada40bfc1cbb9f4bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJpIKNS1nyl3_5AgFHiEIg.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">形状网络G [1]</figcaption></figure><p id="5105" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">当然，这个网络是在多个图像上训练的，并且能够找到一个好的函数来将形状信息映射成我们所说的潜在代码。然后，利用这个潜在代码来产生这个第一3D形状估计。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi gj"><img src="../Images/3d7142df34876bebb640f6cbaaa28808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLcGqWQKHRzDBmq6JIWe7w.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk translated">模型[1]</figcaption></figure><p id="e3a6" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">你可能会认为我们已经完成了，但事实并非如此。这只是第一步，然后，正如我们所讨论的，我们需要这个表示的辐射场，在这里使用一个外观网络。这里，它再次使用类似的潜在代码，但是对于外观，以及作为输入的3D形状，使用另一个网络(这里称为f)产生该辐射场。然后，该辐射场最终可以与相机参数信息一起使用，以产生该对象的最终渲染。</p><p id="ee49" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">当然，这只是这篇新论文的概述。我强烈推荐阅读下面链接的论文。不幸的是，代码现在还不可用，但是我联系了其中一个作者，他说将在几周内可用，所以请继续关注！</p><p id="177e" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如果你喜欢我的工作，并想了解最新的人工智能技术，你绝对应该在我的社交媒体频道上关注我。</p><ul class=""><li id="fed2" class="mv mw it lh b li lj ll lm lo mx ls my lw mz ma na nb nc nd bi translated">订阅我的<a class="ae mg" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"> <strong class="lh jd"> YouTube频道</strong> </a>。</li><li id="a7e7" class="mv mw it lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">在<a class="ae mg" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"><strong class="lh jd">LinkedIn</strong></a><strong class="lh jd"/>上关注我的项目，在<strong class="lh jd"> </strong> <a class="ae mg" href="https://whats-ai.medium.com/" rel="noopener"> <strong class="lh jd">上关注中型</strong> </a> <strong class="lh jd">。</strong></li><li id="4beb" class="mv mw it lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">一起学习AI，加入我们的<a class="ae mg" href="https://discord.gg/learnaitogether" rel="noopener ugc nofollow" target="_blank"> <strong class="lh jd"> Discord社区</strong> </a>，<em class="nj">分享你的项目、论文、最佳课程，寻找Kaggle队友，等等！</em></li></ul></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><h2 id="75d5" class="nr ns it bd nt nu nv dn nw nx ny dp nz lo oa ob oc ls od oe of lw og oh oi iz bi translated">参考</h2><p id="6ffb" class="pw-post-body-paragraph lf lg it lh b li oj kd lk ll ok kg ln lo ol lq lr ls om lu lv lw on ly lz ma im bi translated">[1] Rematas，k .，Martin-Brualla，r .，和Ferrari，v .，“ShaRF:来自单一视图的形状调节辐射场”，(2021)，<a class="ae mg" href="https://arxiv.org/abs/2102.08860" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2102.08860</a></p><p id="0499" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">ShaRF的项目网站和代码链接:<a class="ae mg" href="http://www.krematas.com/sharf/index.html" rel="noopener ugc nofollow" target="_blank">http://www.krematas.com/sharf/index.html</a></p><p id="4efa" class="pw-post-body-paragraph lf lg it lh b li lj kd lk ll lm kg ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">[3]米尔登霍尔等，NeRF:将场景表示为用于视图合成的神经辐射场，(2020)，<a class="ae mg" href="https://www.matthewtancik.com/nerf" rel="noopener ugc nofollow" target="_blank">https://www.matthewtancik.com/nerf</a></p></div></div>    
</body>
</html>