# 人工智能中的偏见、公平和可解释性

> 原文：<https://pub.towardsai.net/bias-vs-fairness-vs-explainability-in-ai-5e0f37ffb22?source=collection_archive---------0----------------------->

## [机器学习](https://towardsai.net/p/category/machine-learning)

![](img/7a75e8ccf5221d6440667f24c0135a30.png)

[卢卡斯](https://unsplash.com/@hauntedeyes?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

在过去的几年里，有一个明显的重点是建立机器学习系统，在某种程度上，是负责任的和道德的。术语“偏见”、“公平”和“可解释性”到处出现，但它们的定义通常非常模糊，而且它们被广泛误解为指同一件事。这篇博客旨在澄清这一点…

# 偏见

在我们看偏见如何出现在机器学习中之前，让我们从这个词的字典定义开始:

> 对某人或某团体的倾向或偏见，特别是以一种被认为不公平的方式

看啊！偏见的定义包括“不公平”一词。显而易见，为什么“偏见”和“公平”这两个术语经常被混淆。

偏见几乎在每个阶段都会影响机器学习系统。这里有一个例子可以说明我们周围世界的历史偏见是如何渗透到你的数据中的:

假设您正在构建一个模型来预测文本序列中的下一个单词。为了确保你得到了大量的训练数据，你把过去 50 年里写的每本书都给了它。然后你让它预测这个句子中的下一个单词:

> “首席执行官的名字是 ____”。

然后，你会注意到，也许并不奇怪，你的模型更有可能预测到 CEO 的男性名字，而不是女性。所发生的是，你无意中把我们社会中存在的历史刻板印象融入到你的模型中。

然而，偏差不仅仅出现在数据中，它也可能出现在模型中。如果用来测试一个模型的数据不能准确地代表真实世界，你最终会得到所谓的评估偏差。

一个很好的例子就是训练一个面部识别系统，然后使用 Instagram 上的照片来测试它。你的模型在测试集上可能有很高的准确性，但在现实世界中可能表现不佳，因为 Instagram 的大多数用户年龄在 18 至 35 岁之间。你的模型现在偏向于那个年龄段，在老年人或年轻人的脸上表现会更差。

在机器学习中实际上有许多不同类型的偏见，我将在一个单独的博客中涵盖所有这些。

偏见这个词几乎总是带有负面含义，但重要的是要注意，在机器学习中并不总是如此。预先了解您要解决的问题可以帮助您在建模过程中选择相关的特征。这引入了人为偏见，但通常可以加速或改进建模过程。

![](img/54ae99dac440ab63075835fb6f7a0475.png)

艾米丽·莫特在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

# 可解释性

有时被称为可解释性，可解释性试图解释**一个机器学习模型如何**做出预测。它是关于询问一个模型，收集关于为什么做出一个特定预测(或一系列预测)的信息，然后以一种可理解的方式将这些信息呈现给人类。

当试图解释模型如何工作时，通常会遇到两种情况:

*   **黑盒** —你没有访问权，也没有关于底层模型的信息。模型的输入和输出都可以用来生成解释。
*   **白盒** —您可以访问底层模型，因此更容易提供关于为什么做出某个预测的信息。

总的来说，“白盒”模型往往在设计上更简单，有时是故意的，这样就可以很容易地产生解释。缺点是，使用更简单、更易解释的模型可能无法捕捉数据中关系的复杂性，这意味着您可能面临可解释性和模型性能之间的权衡。

当进行可解释性时，我们通常对两件事情感兴趣:

*   **模型视图** —总的来说，哪些特性对模型来说比其他特性更重要？
*   **实例视图** —对于一个特定的预测，哪些因素起了作用？

用于可解释性的技术取决于您的模型是黑盒还是白盒，您是对模型视图还是实例视图感兴趣，还取决于您正在探索的数据类型。[开源库 Alibi](https://docs.seldon.io/projects/alibi/en/latest/overview/algorithms.html) 很好地进一步详细解释了这些技术。

就个人而言，我喜欢将白盒模型视为“可解释性”(因为对可解释模型的要求)，将黑盒模型视为“可解释性”(因为我们试图解释未知)。然而，可悲的是，没有官方定义，这些词经常被互换使用。

![](img/d5623313816388c4da811d8e750d28af.png)

照片由 [Piret Ilver](https://unsplash.com/@saltsup?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

# 公平

公平是三个术语中最主观的一个。正如我们对偏见所做的那样，在看它如何应用于机器学习之前，让我们先看一下它的日常定义:

> "没有偏袒或歧视的公正的待遇或行为."

把这个应用到机器学习的环境中，我喜欢用的定义是:

> “如果算法基于敏感特征做出不偏袒或歧视某些个人或群体的预测，那么它就是公平的。”

你将看到的大多数定义(包括我上面的)倾向于将范围缩小到影响人类的机器学习。通常这就是人工智能会产生灾难性后果的地方，所以公平是非常重要的。像抵押贷款批准或医疗诊断这样的事情是如此改变生活的事件，以至于我们以公平和负责任的方式处理预测是至关重要的。

你可能会问自己“什么是‘敏感特征’？”这是一个非常好的问题。对定义的解释在很大程度上取决于你将什么归类为敏感。一些明显的例子往往是种族、性别、性取向、残疾等…

一种方法是在构建模型时删除所有“敏感”属性。起初，这似乎是一件明智的事情，但实际上存在多个问题:

*   **敏感特性实际上可能对模型**至关重要。想象一下，你正试图预测一个孩子完全长大后的身高。删除年龄和性别等敏感属性会让你的预测变得毫无用处。
*   **公平不一定是不可知论者。**有时，为了照顾那些可能在其他方面受到歧视的人，包含敏感特征是很重要的。这方面的一个例子是大学招生，单纯的分数可能不是找到最聪明学生的最佳途径。那些获得资源较少或教育质量较低的人可能会有更好的成绩。
*   **敏感特征可能隐藏在其他属性中。**通常可以使用非敏感特性的组合来确定敏感特性的值。例如，申请人的全名可能允许机器学习模型推断他们的种族、国籍或性别。

现实是，人工智能公平是一个非常困难的领域。它要求决策者为每个用例定义“公平”是什么样子，这有时可能非常主观。通常在群体公平和个体公平之间也有一个权衡。以之前的大学招生为例，让你的算法对一个没有同等教育资源的弱势群体更公平(群体公平)，是以那些拥有良好教育背景、现在成绩不再足够好的人(个人公平)为代价的。

# 摘要

总之，偏见、可解释性和公平性不是一回事。当试图解释机器学习模型的全部或部分时，你可能会发现该模型包含偏见。这种偏见的存在甚至意味着你的模型是不公平的。然而，这并不意味着可解释性、偏见和公平是一回事。

# TL；速度三角形定位法(dead reckoning)

**偏见**是对特定群体、个人或特征的偏好或偏见，有多种形式。

**可解释性**是解释模型如何或为什么做出预测的能力

公平是指不偏不倚或不加区别地使用人工智能的主观实践，尤其是对人类而言