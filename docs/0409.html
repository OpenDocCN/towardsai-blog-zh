<html>
<head>
<title>Interview Questions: Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面试问题:物体检测</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/interview-questions-object-detection-9430d7dee763?source=collection_archive---------0-----------------------#2020-04-19">https://pub.towardsai.net/interview-questions-object-detection-9430d7dee763?source=collection_archive---------0-----------------------#2020-04-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/04c9bad89986c5fdcab3db91beb43136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5I6zpwsBeRxezNMSrh3Uag.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated">在<a class="ae kc" href="https://unsplash.com/s/photos/confused?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae kc" href="https://unsplash.com/@pisauikan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">pisuikan</a>拍摄的照片</figcaption></figure><p id="880b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我目前正在找一份计算机视觉工程师的工作。在这篇文章中，我试图分享我所学到的东西。我要感谢<a class="ae kc" href="https://medium.com/@jonathan_hui" rel="noopener">乔纳森</a>带来了这个令人敬畏的<a class="ae kc" href="https://medium.com/@jonathan_hui/object-detection-series-24d03a12f904" rel="noopener">物体探测系列</a>。</p><p id="eade" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">“这是我个人参考。如果您发现任何错误，请评论我将纠正它们。”</strong></p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="0487" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"><em class="li">YOLO的损失函数是什么？【</em></strong><a class="ae kc" href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" rel="noopener"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">】</em></strong></p><p id="381f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡YOLO使用预测和地面实况之间的误差平方和来计算损失。损失函数包括:</p><ul class=""><li id="5298" class="lj lk iq kf b kg kh kk kl ko ll ks lm kw ln la lo lp lq lr bi translated"><strong class="kf ir">分类损失</strong>。</li><li id="59c3" class="lj lk iq kf b kg ls kk lt ko lu ks lv kw lw la lo lp lq lr bi translated"><strong class="kf ir">定位损失</strong>(预测边界框与地面真实值之间的误差)。</li><li id="dfa9" class="lj lk iq kf b kg ls kk lt ko lu ks lv kw lw la lo lp lq lr bi translated"><strong class="kf ir">信心丧失</strong>(箱子的对象性)。</li></ul><blockquote class="lx ly lz"><p id="fc33" class="kd ke li kf b kg kh ki kj kk kl km kn ma kp kq kr mb kt ku kv mc kx ky kz la ij bi translated">损失函数=分类损失+定位损失+置信度损失</p></blockquote><p id="c14a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"> <em class="li">两阶段法的优势是什么？【</em></strong><a class="ae kc" href="https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de" rel="noopener" target="_blank"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">】</em></strong></p><p id="c566" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡在像R-CNN这样的两阶段方法中，他们首先预测<em class="li">几个</em>候选物体位置，然后使用卷积神经网络将这些候选物体位置中的每一个分类为类别之一或背景。</p><p id="fb74" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"> <em class="li">单镜头方法面临的主要问题是什么？[</em></strong><a class="ae kc" href="https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de" rel="noopener" target="_blank"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">][</em></strong><a class="ae kc" href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" rel="noopener"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">]</em></strong></p><p id="2547" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡像SSD这样的单次拍摄方法会受到类<em class="li">不平衡的严重影响。</em> SSD在训练期间重新采样对象类别和背景类别的比率，因此它不会被图像背景淹没。</p><p id="c993" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"> <em class="li">视网膜局灶性丢失是什么？【</em></strong><a class="ae kc" href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" rel="noopener"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">】</em></strong></p><p id="8f7e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡焦点丢失有助于处理阶级不平衡。焦点损失(FL)采用一种方法来减少训练有素的班级的损失。所以只要模型擅长检测背景，就会减少其损失，重新强调对对象类的训练。</p><p id="f6bb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"><em class="li">SSD中的损失函数是什么？【</em></strong><a class="ae kc" href="https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab" rel="noopener" target="_blank"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">】</em></strong></p><p id="ed35" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡SSD的损失函数是两个关键组件的组合:</p><ul class=""><li id="b83a" class="lj lk iq kf b kg kh kk kl ko ll ks lm kw ln la lo lp lq lr bi translated"><strong class="kf ir">置信度损失</strong>:测量网络对计算边界框的<em class="li">对象</em>的置信度。分类<a class="ae kc" href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/#cross-entropy" rel="noopener ugc nofollow" target="_blank">交叉熵</a>用于计算这种损失。</li><li id="60bf" class="lj lk iq kf b kg ls kk lt ko lu ks lv kw lw la lo lp lq lr bi translated"><strong class="kf ir">位置损失:</strong>这衡量<em class="li">距离</em>网络的预测边界框与训练集中的地面真实边界框有多远。<a class="ae kc" href="https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/" rel="noopener ugc nofollow" target="_blank">此处使用L2规范</a>。</li></ul><p id="44e8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"><em class="li">SSD _ loss = confidence _ loss+alpha * location _ loss</em></strong></p><p id="2a4e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="li">α</em>项帮助我们平衡位置损失的贡献。</p><p id="c6d7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"> <em class="li">什么是FPN？【</em></strong><a class="ae kc" href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" rel="noopener"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">】</em></strong></p><p id="99c5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡特征金字塔网络(<strong class="kf ir"> FPN </strong>)是一个用特征金字塔概念设计的特征提取器，用来提高精度和速度。图像首先通过CNN路径，产生语义丰富的最终层。然后，为了重新获得更好的分辨率，它通过对该特征图进行上采样来创建自上而下的路径。虽然自上而下的路径有助于检测不同大小的对象，但空间位置可能会有偏差。在原始特征图和相应的重建层之间添加横向连接，以改善对象定位。它目前提供了一种在多个尺度上检测物体的领先方法，YOLOv3，更快的R-CNN就是用这种技术建立起来的。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi md"><img src="../Images/eb84efa1402d0dbeeca0a6083cdf637b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pxfq-xksGQq73aid2zOgag.png"/></div></div></figure><p id="16ac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"> <em class="li">我们为什么要使用数据增强？【</em></strong><a class="ae kc" href="https://github.com/andrewekhalel/MLQuestions" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">】</em></strong></p><p id="d1a7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡数据扩充是一种通过修改现有数据来合成新数据的技术，其方式是不改变目标或以已知方式改变目标。数据扩充对于提高准确性非常重要。增强数据技术，如翻转、裁剪、添加噪声和颜色失真。</p><p id="4a70" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据增强有助于提高SSD300的性能:</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/1913d22d5360dd3fdac054b170d355fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*9ewf7dq2QjedStXo8QB72g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk translated"><a class="ae kc" href="http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="9789" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"><em class="li">SDD比更快的R-CNN有什么优势？【</em></strong><a class="ae kc" href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06" rel="noopener"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li"/></strong></p><p id="e8c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡SSD通过消除在更快的R-CNN中使用的区域提议网络(RPN)的需要来加速该过程。</p><p id="8f88" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"> <em class="li">物体检测用的度量是什么？【</em></strong><a class="ae kc" href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li"/></strong></p><p id="59ed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡mAP (mean Average precision)是测量物体检测器精度的常用度量。平均精度计算0到1之间的召回值的平均精度值。</p><p id="da5e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir">T47】什么是NMS？【</strong><a class="ae kc" href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" rel="noopener"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">】</em></strong></p><p id="2ee7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡非最大值抑制(NMS)是许多计算机视觉对象检测算法中使用的技术。为单个类从许多重叠的包围盒中选择一个包围盒是一类算法。</p><p id="94f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">NMS实施:</p><ol class=""><li id="690c" class="lj lk iq kf b kg kh kk kl ko ll ks lm kw ln la mj lp lq lr bi translated">按降序对预测可信度分数进行排序。</li><li id="4e8c" class="lj lk iq kf b kg ls kk lt ko lu ks lv kw lw la mj lp lq lr bi translated">从最高分数开始，如果我们发现任何先前的预测与当前预测具有相同的类别和IoU &gt;阈值(通常我们使用0.5)，则忽略任何当前预测。</li><li id="f833" class="lj lk iq kf b kg ls kk lt ko lu ks lv kw lw la mj lp lq lr bi translated">重复上述步骤，直到检查完所有预测。</li></ol><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/1d5a32c697bf9de3a94d2cd8532b5e57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emXsX6e0K81bgyDvhsTYmg.png"/></div></div></figure><p id="90ae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"> <em class="li">什么是欠条？【</em></strong><a class="ae kc" href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06" rel="noopener"><strong class="kf ir"><em class="li">src</em></strong></a><strong class="kf ir"><em class="li">】</em></strong></p><p id="c80f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡NMS使用了并集交集(IoU)的概念。IoU计算两个边界框(地面真实的边界框和预测的边界框)的并集上的交集。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/f89dab7d1bd094140374bf5b2d0d8f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08dXQnWIht6sA_vNbhQjeQ.png"/></div></div></figure><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mm"><img src="../Images/8a3c72120e3a5806e4a6cb5faa6ae999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i_mg1zO-t1exOV9JmM9K1Q.png"/></div></div></figure><p id="66ea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌<strong class="kf ir"> <em class="li">什么时候你说一个物体检测方法是高效的？</em> </strong></p><p id="15cc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡检测的性能效率用每秒浮点运算次数(FLOPS)来衡量。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/9d09d45c23e07abfda79894998b6733e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*DtInmi_A-W6HjhjreB5Kyw.png"/></div></figure><p id="f570" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上图中，你可以看到EfficientDet和YOLOv3的FLOPS更少，所以我们可以说它们是高效的。</p><p id="fa90" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">📌自定义对象检测中的一些实践问题</p><p id="035c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">💡试用Monk对象检测库</p><div class="mo mp gp gr mq mr"><a href="https://github.com/Tessellate-Imaging/Monk_Object_Detection/tree/master/application_model_zoo" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd ir gy z fp mw fr fs mx fu fw ip bi translated">镶嵌成像/Monk_Object_Detection</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">低代码、易于安装的对象检测管道的一站式存储库。…</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">github.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf jw mr"/></div></div></a></div></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="8d6d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我对计算机视觉和深度学习充满热情。我是<a class="ae kc" href="https://github.com/Tessellate-Imaging/Monk_Object_Detection" rel="noopener ugc nofollow" target="_blank"> Monk </a>库的开源贡献者。如果你喜欢蒙克，给我们GitHub回购⭐️。</p><p id="e9c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你也可以在以下网址看到我的其他作品:</p><div class="mo mp gp gr mq mr"><a href="https://medium.com/@akulahemanth" rel="noopener follow" target="_blank"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd ir gy z fp mw fr fs mx fu fw ip bi translated">阿库拉·赫曼思·库马尔培养基</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">阅读阿库拉·赫曼思·库马尔在媒介上的作品。💟计算机视觉| Linkedin…</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">medium.com</p></div></div><div class="na l"><div class="ng l nc nd ne na nf jw mr"/></div></div></a></div></div></div>    
</body>
</html>