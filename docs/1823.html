<html>
<head>
<title>Neural Networks: The Rise of Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:递归神经网络的兴起</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88?source=collection_archive---------2-----------------------#2021-05-06">https://pub.towardsai.net/neural-networks-the-rise-of-recurrent-neural-networks-df740252da88?source=collection_archive---------2-----------------------#2021-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f12e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="f8b8" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">深度学习中的渐进一代</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/926d296a0b82a76107177000d8c3f6e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2DFfuW4msTv5G6jsLmbkgw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">pic的来源是<a class="ae lh" href="https://medium.com/deeplearningbrasilia/deep-learning-recurrent-neural-networks-f9482a24d010" rel="noopener">中的</a></figcaption></figure><p id="c704" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，我们将讨论深度学习中的递归神经网络的基础。递归神经网络的兴起是为了克服我们在神经网络和卷积神经网络中看到的局限性。</p><h2 id="e75c" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">涵盖的主题:</h2><pre class="ks kt ku kv gt mw mx my mz aw na bi"><span id="6017" class="me mf it mx b gy nb nc l nd ne"><strong class="mx jd">1. Limitation of neural network and convolutional neural network<br/>2. Introduction about the architecture of RNN<br/>3. Working of RNN<br/>4. Limitation of RNN</strong></span></pre><blockquote class="nf ng nh"><p id="54be" class="li lj ni lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">限制</em> </strong></p></blockquote><h2 id="d9b7" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">人工神经网络</h2><ul class=""><li id="2aad" class="nm nn it lk b ll no lo np lr nq lv nr lz ns md nt nu nv nw bi translated">设计合适的神经网络没有特定的规则，而是采用试凑法。</li><li id="51e9" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">模特的训练时间比较多。</li><li id="2d03" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">在基于矩阵的大数据的情况下，ram的存储器大小需要更多。然后数据需要分成许多批，因此任务计算复杂。</li><li id="34c7" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">图像的输入和输出大小是固定的。</li><li id="20ed" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">没有到操作前一状态的记忆存储链接。</li></ul><h2 id="b4d5" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">卷积神经网络</h2><ul class=""><li id="a7cf" class="nm nn it lk b ll no lo np lr nq lv nr lz ns md nt nu nv nw bi translated">图像的倾斜和旋转会给下一个神经元带来更少的信息。</li><li id="394d" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">与无噪声图像相比，识别有噪声图像的难度。</li><li id="25c1" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">CNN中输入和输出图像的固定大小也是一样的。</li><li id="b828" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">CNN也没有到前一状态的记忆存储链接来识别预测。</li></ul><p id="c730" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，递归神经网络克服了图像的长期依赖性和固定维数的问题。</p><p id="00dc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">长期依赖性意味着，由于RNN处理的记忆存储能力的这种依赖性，未来预测与前一阶段或第一阶段神经元相关联。</p><p id="8d74" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">递归神经网络捕捉输入数据中存在的顺序信息，即在进行预测时两个数据点之间的相关性。</p><div class="oc od gp gr oe of"><a rel="noopener  ugc nofollow" target="_blank" href="/understand-tensorflow-basic-with-python-87281e737db9"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd jd gy z fp ok fr fs ol fu fw jc bi translated">使用Python了解TensorFlow Basic</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">张量流中使用的基本术语</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">pub.towardsai.net</p></div></div><div class="oo l"><div class="op l oq or os oo ot lb of"/></div></div></a></div><blockquote class="nf ng nh"><p id="d987" class="li lj ni lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated"><strong class="lk jd"> <em class="it">关于RNN建筑的介绍</em> </strong></p></blockquote><p id="d6ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">RNN建筑的视图如下所示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/742347d41b7e932823dcb6aff611df4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*L6ZF7fwGJeX-z_eEc_QSwQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">作者的照片</figcaption></figure><p id="9199" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上图显示了人工神经网络和递归神经网络之间的差异。RNN中的隐藏层可以再次将隐藏层的输出馈送到输入。多个神经元的输入和输出是输入和输出向量。</p><pre class="ks kt ku kv gt mw mx my mz aw na bi"><span id="0a32" class="me mf it mx b gy nb nc l nd ne">X = [x1, x2,----x(t-1)]</span><span id="2b32" class="me mf it mx b gy ov nc l nd ne">Y = [y1, y2,----y(t-1)]</span></pre><h2 id="931f" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">RNN的类型</h2><ul class=""><li id="24b0" class="nm nn it lk b ll no lo np lr nq lv nr lz ns md nt nu nv nw bi translated"><strong class="lk jd">一对一:</strong>在这种类型的RNN中，有一个输入和一个输出。它也类似于一个简单的神经网络。这种RNN在二元分类中的实际应用。</li><li id="6dc3" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated"><strong class="lk jd">一对多:</strong>这种类型的RNN有一个输入和多个输出。输入也可以是图像形式，输出是该图像中的多个特征。</li><li id="a2b6" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">多对一:在这种类型的RNN中，有许多输入和一个输出。这种类型的一个例子是股票预测，其中输入是多个时间序列数据，输出是特定时间的预测输出。</li><li id="7761" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated"><strong class="lk jd">多对多:</strong>在这种类型的RNN中有许多输入和许多输出。这种类型的一个例子是语言转换，其中输入语言具有许多特征，而转换后的输出语言也具有多个特征。</li></ul><h2 id="4ba0" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">与RNN建筑相关的术语</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/34c7127359ccab7903068de026c8bedb.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*1OyPbGeuyLkX0zRizPU5fQ.png"/></div></figure><ul class=""><li id="c758" class="nm nn it lk b ll lm lo lp lr ox lv oy lz oz md nt nu nv nw bi translated"><strong class="lk jd">输入大小:</strong>输入的大小是一个(Kx1)的向量，对于所有的输入，大小只是一个向量形式。输入可以是图像或文字，但采用向量形式。</li><li id="16bd" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated"><strong class="lk jd">输出大小:</strong>输出的大小也是Kx1的向量。</li><li id="b819" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated"><strong class="lk jd">权重(U): </strong>是输入和隐藏层之间的一个权重。</li><li id="414e" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated"><strong class="lk jd">权重(V): </strong>是隐藏层和输出层之间的一个权重。</li><li id="91ad" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated"><strong class="lk jd">权重(W): </strong>它是一个自我之间隐含层之间的权重。</li></ul><h2 id="28d9" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">RNN方程式</h2><p id="b6f8" class="pw-post-body-paragraph li lj it lk b ll no kd ln lo np kg lq lr pa lt lu lv pb lx ly lz pc mb mc md im bi translated">RNN体系结构中有几种类型的等式，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/e756dd2151da017ab4338cd705c94478.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*ETngd_zSD-Hnzo5NAvZU4A.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">RNN方程式。作者的照片</figcaption></figure><p id="4846" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">等式<strong class="lk jd"> H(t) </strong>具有水平方向的信息流，即从左到右。第二个等式具有垂直方向的信息流，即从底部到顶部。</p><p id="7006" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> H(t) </strong>是<strong class="lk jd"> h(t-1) </strong>和<strong class="lk jd"> x(t) </strong>的函数，其中<strong class="lk jd"> H(t) </strong>是当前隐藏状态，<strong class="lk jd"> h(t-1) </strong>是前一隐藏状态，<strong class="lk jd"> x(t) </strong>是当前输入。</p><p id="d459" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> U、W和V </strong>是权重矩阵。</p><div class="oc od gp gr oe of"><a rel="noopener  ugc nofollow" target="_blank" href="/step-by-step-basic-understanding-of-neural-networks-with-keras-in-python-94f4afd026e5"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd jd gy z fp ok fr fs ol fu fw jc bi translated">使用Python中的Keras逐步基本了解神经网络</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">具有定义的神经网络的学习</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">pub.towardsai.net</p></div></div><div class="oo l"><div class="pe l oq or os oo ot lb of"/></div></div></a></div><div class="oc od gp gr oe of"><a rel="noopener  ugc nofollow" target="_blank" href="/understand-time-series-components-with-python-4bc3e2ba1189"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd jd gy z fp ok fr fs ol fu fw jc bi translated">用Python理解时间序列组件</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">机器学习中预测模型的基本概念及实例</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">pub.towardsai.net</p></div></div><div class="oo l"><div class="pf l oq or os oo ot lb of"/></div></div></a></div><blockquote class="nf ng nh"><p id="9cad" class="li lj ni lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated"><strong class="lk jd"><em class="it">RNN的作品</em> </strong></p></blockquote><p id="e28e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">RNN的基本功能是记住以前的状态，以便预测的输出更加准确。</p><p id="2467" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将讨论如下所示的两种工作状态:</p><ul class=""><li id="0feb" class="nm nn it lk b ll lm lo lp lr ox lv oy lz oz md nt nu nv nw bi translated">正向传播</li><li id="8fad" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">反向传播</li></ul><h2 id="5ccb" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">正向传播</h2><p id="9132" class="pw-post-body-paragraph li lj it lk b ll no kd ln lo np kg lq lr pa lt lu lv pb lx ly lz pc mb mc md im bi translated">我们上面讨论的两个方程向前传播，因此流量信息将水平和垂直传播。我们可以看到一个简单的正向传播图，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/429a1c675251851d20ea50df9d114c8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*lqxXlJ4g13-eCyD9jzpeEA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">在RNN向前传播。作者的照片</figcaption></figure><p id="04ef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正向传播中还有两个等式，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/79c64ad2e54f399d155df81bf262f690.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*HJwJyb3C1MVNLnnj-eeRjg.png"/></div></figure><p id="64ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">等式1st是分类问题情况下的多类熵损失。熵损失用于寻找概率以提高分类性能，因此，多类用于多类预测。</p><pre class="ks kt ku kv gt mw mx my mz aw na bi"><span id="6836" class="me mf it mx b gy nb nc l nd ne">y(t) is the actual label<br/>y^(t) is the predicted label</span></pre><p id="dcca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">等式2nd是通过从i=0到时间(T)的单独输出预测的RNN中的总损失。</p><p id="f5e2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这两个方程用于量化预测与实际标签的接近程度。</p><h2 id="32bf" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">通过时间的反向传播或反向传播(BPTT)</h2><p id="f32f" class="pw-post-body-paragraph li lj it lk b ll no kd ln lo np kg lq lr pa lt lu lv pb lx ly lz pc mb mc md im bi translated">这是我们训练网络的模型的一部分。此部分的功能用于更新RNN的可训练参数，参数为U、V和W，即RNN中的重量。</p><p id="251e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们使用梯度下降方程来更新权重，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/3b58b9e9b4481f67eaf46d72721853a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*EuHYbh0VGwqHNaQP51yvFg.png"/></div></figure><p id="4e62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">等式3是W权重的更新等式。方程4是V权值的更新方程。方程5是U权值的更新方程。L是完全损失，α是学习参数。</p><blockquote class="nf ng nh"><p id="838a" class="li lj ni lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated"><strong class="lk jd"><em class="it">RNN的局限</em> </strong></p></blockquote><p id="0ee0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们讨论了一个模型来克服神经网络和CNN模型，但仍然有一个问题与消失梯度问题。为了解决这个问题，我们采用了以下方法:</p><ol class=""><li id="9050" class="nm nn it lk b ll lm lo lp lr ox lv oy lz oz md pj nu nv nw bi translated"><strong class="lk jd">随时间截断反向传播:</strong>这是一种我们用来标记时间之间的部分以避免过度反向传播的方法。</li><li id="2f36" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md pj nu nv nw bi translated"><strong class="lk jd">裁剪方法:</strong>对特定范围的权重的导数进行裁剪。这个方法在TensorFlow中很容易实现，这样梯度下降就不会爆炸和消失。</li><li id="3df3" class="nm nn it lk b ll nx lo ny lr nz lv oa lz ob md pj nu nv nw bi translated">使用先进的架构，如LSTM和格鲁</li></ol><blockquote class="nf ng nh"><p id="228b" class="li lj ni lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated"><strong class="lk jd">T9】结论:T11】</strong></p></blockquote><p id="e787" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文给出了递归神经网络的基本理解。逐渐消失的梯度问题促使人们寻找更先进的建筑，如LSTM和GRU，这将在以后的文章中讨论。</p><p id="bacb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae lh" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="0a4e" class="pk mf it bd mg pl pm pn mj po pp pq mm ki pr kj mp kl ps km ms ko pt kp mv pu bi translated">推荐文章</h1><p id="2b59" class="pw-post-body-paragraph li lj it lk b ll no kd ln lo np kg lq lr pa lt lu lv pb lx ly lz pc mb mc md im bi translated"><a class="ae lh" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> 1。NLP —零到英雄与Python </a> <br/> 2。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a>T5】3 .<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/exception-handling-concepts-in-python-4d5116decac3?source=friends_link&amp;sk=a0ed49d9fdeaa67925eac34ecb55ea30">Python中的异常处理概念</a> <br/> 4。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/principal-component-analysis-in-dimensionality-reduction-with-python-1a613006d531?source=friends_link&amp;sk=3ed0671fdc04ba395dd36478bcea8a55">用Python进行主成分分析降维</a> <br/> 5。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a?source=friends_link&amp;sk=9c5c613ceb10f2d203712634f3b6fb28" rel="noopener">用Python全面讲解K-means聚类</a> <br/> 6。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-linear-regression-with-python-fe2b313f32f3?source=friends_link&amp;sk=53c91a2a51347ec2d93f8222c0e06402" rel="noopener">用Python </a> <br/> 7全面讲解了线性回归。<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd?source=friends_link&amp;sk=528181f15a44e48ea38fdd9579241a78" rel="noopener">用Python </a> <br/>充分解释了Logistic回归8。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/nengo-a-new-neural-network-building-and-deployment-tool-66677c65fa19?source=friends_link&amp;sk=6f6d2495d486dda2acb28f2d2bf7dd77"> Nengo:一种新的神经网络构建和部署工具</a> <br/> 9。<a class="ae lh" rel="noopener ugc nofollow" target="_blank" href="/data-wrangling-with-python-part-1-969e3cc81d69?source=friends_link&amp;sk=9c3649cf20f31a5c9ead51c50c89ba0b">与Python的数据角力—第一部分</a> <br/> 10。<a class="ae lh" href="https://medium.com/analytics-vidhya/confusion-matrix-in-machine-learning-91b6e2b3f9af?source=friends_link&amp;sk=11c6531da0bab7b504d518d02746d4cc" rel="noopener">机器学习中的混淆矩阵</a></p></div></div>    
</body>
</html>