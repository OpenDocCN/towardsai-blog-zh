<html>
<head>
<title>Supermasks: A Simple Introduction and Implementation in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">super masks:py torch中的简单介绍和实现</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/supermasks-a-simple-introduction-and-implementation-in-pytorch-a80cd9f1f0a6?source=collection_archive---------0-----------------------#2020-09-07">https://pub.towardsai.net/supermasks-a-simple-introduction-and-implementation-in-pytorch-a80cd9f1f0a6?source=collection_archive---------0-----------------------#2020-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2c4c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning/deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><p id="8231" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">对神经网络的一般理解是，需要计算来调整神经网络的权重，以便它可以在给定的数据集上执行特定的任务。然而，这似乎并不完全正确。显然，给定一个随机初始化的密集神经网络:</p><ol class=""><li id="6279" class="kx ky it kb b kc kd kg kh kk kz ko la ks lb kw lc ld le lf bi translated">存在这样的子网络，当被训练时，可以在训练后实现与原始网络一样的性能。</li><li id="fa74" class="kx ky it kb b kc lg kg lh kk li ko lj ks lk kw lc ld le lf bi translated">更令人惊讶的是，存在这样的子网络，它们无需任何训练，就能在某项任务上比随机初始化表现得更好。例如，在MNIST数据集上，它可以达到86%的准确率，在CIFAR-10数据集上，可以达到41%的准确率！</li></ol><p id="50af" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">第一种情况下的子网被称为<strong class="kb jd">彩票</strong>，因为它们赢得了初始化彩票。在第二种情况下，<strong class="kb jd"> Supermasks </strong>是我们可以用来表示子网权重的方法。在这篇文章中，我解释了什么是超级掩码，以及我们如何通过反向传播找到超级掩码。</p><h1 id="5aa1" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">为什么我们要费心去寻找这样子网络呢？</h1><p id="d46c" class="pw-post-body-paragraph jz ka it kb b kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw im bi translated">神经网络的权重是随机生成的。然而，在现实中，随机数并没有那么随机。我的意思是，使用相同的种子号，我们可以确定性地生成相同的<strong class="kb jd">随机数序列！</strong></p><p id="1bb3" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">同样，子网络可以用二进制序列表示。因此，一个庞大的神经网络的全部参数可以用一个种子数和几个二进制掩码来表示。在实践中，哪些是神经网络权重的压缩表示？</p><h1 id="5c7d" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">什么是超级任务？</h1><p id="07e4" class="pw-post-body-paragraph jz ka it kb b kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw im bi translated">在动手编写代码之前，我们先来讨论一下什么是Supermask。超掩码是一个二元矩阵，其形状类似于线性层的权重矩阵。只要超掩码的元素为1，权重矩阵上的相应元素将保持不变。否则，它们将被设置为零。Supermask和权重矩阵的简单元素乘积将完成这项工作。</p><p id="749a" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><em class="mo">“一图胜千言。”</em>对吧？</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mp"><img src="../Images/67347fa78760076b87ca4ab4531b0761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BywD95k-qMUJbzeaBjmYjA.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">这就是二进制矩阵如何屏蔽实值矩阵。(图片由作者提供)</figcaption></figure><p id="aebe" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">通过使用掩码将权重矩阵的值设置为零，我们实际上是在选择全连接网络的子网络。就像下图一样:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/8c63a2b6ee83a33ed2145e26bc45c6bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*zTCiVn2Ir7gzA-UU3QKLqg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk translated">完全连接的网络(左)。及其子网络(右)。(图片由作者提供)</figcaption></figure><h1 id="137a" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">如何找到Supermask？</h1><p id="41ea" class="pw-post-body-paragraph jz ka it kb b kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw im bi translated">通过本文<a class="ae ng" href="https://arxiv.org/abs/1905.01067" rel="noopener ugc nofollow" target="_blank">中描述的方法，可以通过反向传播找到超级掩码。</a>设<em class="mo"> Wf </em>表示线性层的权重矩阵，该权重矩阵被随机初始化，之后保持不变。并且让<em class="mo"> Wm </em>表示另一个权重矩阵，它也是随机初始化的，并且具有与<em class="mo"> Wf </em>相同的形状。与<em class="mo"> Wf </em>不同，矩阵<em class="mo"> Wm </em>将通过反向传播进行更新。<em class="mo"> Wm </em>通过一个基于元素的sigmoid函数来生成伯努利函数的概率:</p><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="9681" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">然后，逐元素的伯努利函数通过将<em class="mo"> Wb </em>的每个元素视为伯努利分布的概率来生成二进制掩码:</p><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="ddaf" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">矩阵m1是一个二进制掩码。然后，它用于屏蔽全连接权重矩阵(<em class="mo"> Wf) </em>的值，以便计算<em class="mo"> Ws </em>，即有效权重矩阵(即子网络的权重):</p><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="229a" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">最后，子网络的权重可以应用于输入，然后通过激活函数，以便计算预测输出:</p><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="3658" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">正如我们将在实现中看到的，这也可以扩展到多层架构。在反向传播过程中，计算损耗梯度w.r.t <em class="mo"> Wm </em>。因此，<em class="mo"> Wm </em>被迭代更新。</p><p id="6b8d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在下一节中，我将讨论这个过程在PyTorch中的实现。</p><h1 id="7943" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">给我看看代码！</h1><p id="eed1" class="pw-post-body-paragraph jz ka it kb b kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw im bi translated">现在我们有了一个总体的想法和数学基础，实现它是简单的。首先，我应该提到PyTorch中的Bernouli函数，它不跟踪梯度。所以，我定义了我自己版本的支持梯度的Bernouli函数:</p><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="nj ni l"/></div></figure><p id="308a" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">下一步是创建一个自定义线性图层，支持内部掩膜。我称之为<strong class="kb jd">屏蔽线性。尽管这些代码是不言自明的(感谢脸书为PyTorch :D公司所做的工作)，我还是提供了一些注释以使其更加清晰:</strong></p><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="nj ni l"/></div></figure><p id="55d9" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">现在，我们可以将这些层放在一起，形成一个完全连接的神经网络:</p><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="nj ni l"/></div></figure><p id="98cd" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">最后，可以使用PyTorch的常规内置反向传播来训练该模型:</p><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="nj ni l"/></div></figure><p id="e663" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我已经在Github上提供了该项目的完整源代码。这是我自己可以得到的。并且可以直接在<a class="ae ng" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank"> google colab </a>上运行。</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><h1 id="5fe4" class="ll lm it bd ln lo nr lq lr ls ns lu lv lw nt ly lz ma nu mc md me nv mg mh mi bi translated">结束讨论！</h1><p id="f03b" class="pw-post-body-paragraph jz ka it kb b kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw im bi translated">我发现彩票和超级任务的概念如此迷人。关于这些主题已经有了广泛的持续研究。这里实现的算法是所有算法中最简单的一个。然而，我鼓励读者查看我写这篇文章的主要参考资料。另外，我很感谢你花时间阅读这篇文章。请让我知道这篇文章中可能存在的任何科学和/或技术错误。并且分享这篇文章如果你觉得它有趣和有用！</p><h1 id="0581" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">参考资料:</h1><p id="35da" class="pw-post-body-paragraph jz ka it kb b kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks mn ku kv kw im bi translated">[1]周、哈蒂、贾尼斯·兰、罗莎妮·刘和杰森·约辛斯基。<a class="ae ng" href="https://arxiv.org/abs/1905.01067" rel="noopener ugc nofollow" target="_blank">解构彩票:零、符号和超级谜题</a>。(2020年3月3日)。神经信息处理系统进展32 (NIPS 2019)</p><p id="3191" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[2]https://eng.uber.com/deconstructing-lottery-tickets/<a class="ae ng" href="https://eng.uber.com/deconstructing-lottery-tickets/" rel="noopener ugc nofollow" target="_blank"/></p><p id="26a4" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[3]弗兰克、乔纳森和迈克尔·卡宾。<a class="ae ng" href="http://arxiv.org/abs/1803.03635" rel="noopener ugc nofollow" target="_blank">彩票假说:寻找稀疏的、可训练的神经网络。</a>(2019年3月4日)</p><p id="e451" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[4] Ramanujan、Vivek、Mitchell Wortsman、Aniruddha Kembhavi、阿里·法尔哈迪和Mohammad Rastegari。<a class="ae ng" href="https://arxiv.org/abs/1911.13299" rel="noopener ugc nofollow" target="_blank">随机加权的神经网络中隐藏着什么？</a>(2020年3月30日)。CVPR 2020</p></div></div>    
</body>
</html>