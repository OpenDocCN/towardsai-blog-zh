<html>
<head>
<title>General Video Understanding with AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能的一般视频理解</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/general-video-understanding-with-ai-e88956333848?source=collection_archive---------2-----------------------#2022-09-08">https://pub.towardsai.net/general-video-understanding-with-ai-e88956333848?source=collection_archive---------2-----------------------#2022-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4589" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当这样的模型看到这样的图片或者更复杂的视频时，它能理解什么？</h2></div><blockquote class="ki kj kk"><p id="a168" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">最初发表于<a class="ae li" href="https://www.louisbouchard.ai/general-video-recognition/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae li" href="https://www.louisbouchard.ai/general-video-recognition/" rel="noopener ugc nofollow" target="_blank">我的博客上读到的！</a></p></blockquote><h2 id="2b46" class="lj lk it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">观看视频</h2><figure class="mf mg mh mi gt mj"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="10b8" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">我们已经看到人工智能生成文本，然后<a class="ae li" href="https://youtu.be/qOxde_JV0vI" rel="noopener ugc nofollow" target="_blank">生成图像</a>，最近甚至生成短视频，尽管它们仍然需要工作。当你想到没有人真正参与这些作品的创作过程，并且它只需要被训练一次，然后就像stable diffusion一样被成千上万的人使用，结果是令人难以置信的。尽管如此，这些模型真的明白他们在做什么吗？他们知道他们刚刚制作的图片或视频真正代表了什么吗？当这样的模型看到这样的图片或者更复杂的视频时，它能理解什么？</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/90f7179c2881217eec7e34d355bf411c.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/1*Y5gtoWbSuum5qQl8IQnuxg.gif"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">论文中的结果实例。</figcaption></figure><p id="a0e1" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">让我们专注于两者中更具挑战性的一个，并深入研究人工智能如何通过一项名为一般视频识别的任务来理解视频，其中的目标是让模型将视频作为输入，并使用文本来描述短视频中发生的事情。</p><p id="2781" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">普通视频识别是理解视频中最具挑战性的任务之一。然而，这可能是衡量一个模型了解正在发生的事情的能力的最佳方式。这也是许多应用程序背后的基础，这些应用程序依赖于对视频的良好理解，如体育分析或自动驾驶。但是是什么使得这项任务如此复杂呢？嗯，有两件事:</p><ol class=""><li id="3566" class="mt mu it ko b kp kq ks kt ls mv lw mw ma mx lh my mz na nb bi translated">我们需要理解显示的内容，即特定视频的每一帧或每一幅图像。</li><li id="4660" class="mt mu it ko b kp nc ks nd ls ne lw nf ma ng lh my mz na nb bi translated">我们需要能够以人类理解的方式说出我们理解的东西，这意味着单词。</li></ol><p id="cf2b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">幸运的是，第二个挑战已经被语言社区解决了无数次，我们可以接管他们的工作。更准确地说，我们可以借鉴语言-图像领域的人们对剪辑甚至稳定扩散等模型所做的工作，在这些模型中，你有一个文本编码器和图像编码器，它们学习将两种类型的输入编码成同一种表示。这样，通过用数百万个图像标题示例训练该架构，您可以将相似的场景与相似的文本提示进行比较。将文本和图像编码在一个相似的空间是非常强大的，因为它需要更少的空间来执行计算，并且它允许我们很容易地将文本与图像进行比较。意思是模型仍然不能理解一幅图像或者甚至一个简单的句子，但是它可以理解两者是否相似。我们离智能还很远，但这对大多数情况来说是非常有用和足够好的。</p><p id="31bd" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">现在带来了最大的挑战:视频。为此，我们将使用Bolin Ni及其同事在他们最近的论文“扩展通用视频识别的语言图像预训练模型”中的方法。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/335beccc62bb0852f8671d9eec5a90d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sjbUh4eA1vxvhXprBnPUEw.png"/></div></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">模型概述。图片来自报纸。</figcaption></figure><p id="b835" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">由于时间信息，视频比图像复杂得多，这意味着多个帧，并且每个帧都通过连贯的运动和动作与下一个和前一个帧相链接。模型需要看到每一帧之前、之中和之后发生了什么，以便对场景有一个正确的理解。</p><p id="7933" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">就像在YouTube上一样。在短视频中，你不能真的向前跳过5秒，因为你会错过有价值的信息。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi nm"><img src="../Images/0388330f4e54f1d738649af1910bc9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RcbKVxbOzdQyb5xU.png"/></div></a></figure><p id="b0d9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">在这种情况下，他们将每一帧发送到我们刚刚讨论过的同一个图像编码器，使用基于视觉转换器的架构，利用注意力将它们处理到一个压缩空间中。</p><p id="60ba" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">如果你不熟悉视觉变形金刚，或者注意力机制，我将邀请你<a class="ae li" href="https://youtu.be/QcCJJOLCeJQ" rel="noopener ugc nofollow" target="_blank">观看我制作的介绍它们的视频</a>。</p><p id="92d1" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">一旦您有了每一帧的表示，您可以使用类似的基于注意力的过程让每一帧一起通信，并允许您的模型在帧之间交换信息，并为视频创建最终表示。这种使用注意力的帧之间的信息交换将作为模型的某种记忆，以便将视频作为一个整体来理解，而不是将几个随机图像放在一起。</p><p id="354e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">最后，我们使用另一个注意力模块将我们拥有的帧的文本编码与我们的压缩视频表示合并。</p><figure class="mf mg mh mi gt mj gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/5a6c0b62c97bba5ed3552e49b97aa2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/1*rWLQ4n2O7P0bs1vLNy4MjA.gif"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk translated">论文中的结果实例。</figcaption></figure><p id="6b6c" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">瞧！</p><p id="a8d8" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">这是人工智能理解视频的一种方式。当然，这只是微软研究院作为视频识别介绍的这篇伟大论文的概述，我邀请您阅读这篇论文以更好地理解他们的方法。</p><p id="bc40" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">感谢您通读整篇文章，下周我将带着另一篇精彩的论文与您见面！</p><p id="96b9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku ls kw kx ky lw la lb lc ma le lf lg lh im bi translated">路易斯（号外乐团成员）</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="370b" class="lj lk it bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">参考</h2><p id="dbf2" class="pw-post-body-paragraph kl km it ko b kp nu ju kr ks nv jx ku ls nw kx ky lw nx lb lc ma ny lf lg lh im bi translated">阅读全文:【https://www.louisbouchard.ai/general-video-recognition/】<br/>倪，b，彭，h，陈，m，张，s，孟，g，傅，j，向，s，凌，h，2022。通用视频识别的扩展语言图像预处理模型。arXiv预印本arXiv:2208.02816。代码:<a class="ae li" href="https://github.com/microsoft/VideoX/tree/master/X-CLIP" rel="noopener ugc nofollow" target="_blank">https://github.com/microsoft/VideoX/tree/master/X-CLIP</a><br/>【我的时事通讯(一个新的人工智能应用每周向你的电子邮件解释！):<a class="ae li" href="https://www.louisbouchard.ai/newsletter/" rel="noopener ugc nofollow" target="_blank">https://www.louisbouchard.ai/newsletter/</a></p></div></div>    
</body>
</html>