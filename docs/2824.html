<html>
<head>
<title>Hugging Face vs Flair: Comparing “Default” Sentiment Analysis Pipelines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">拥抱脸与天赋:比较“默认”情感分析管道</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/hugging-face-vs-flair-comparing-default-sentiment-analysis-pipelines-28d4d10e127f?source=collection_archive---------0-----------------------#2022-06-07">https://pub.towardsai.net/hugging-face-vs-flair-comparing-default-sentiment-analysis-pipelines-28d4d10e127f?source=collection_archive---------0-----------------------#2022-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eb6c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Transformer的拥抱脸和天赋从零开始构建情感分析，以确定积极、消极和中立</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/60607a91976e93bfca712995d1efc390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W44hbMF_4miWQhUIN0VoeA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">由Unsplash的<a class="ae ky" href="https://unsplash.com/@possessedphotography" rel="noopener ugc nofollow" target="_blank">附身摄影</a>拍摄</figcaption></figure><h1 id="4eb0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">变形金刚的拥抱脸</h1><p id="7885" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">目前世界上最受欢迎的自然语言处理(NLP)库之一，基于注意力机制(<a class="ae ky" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">免除递归和卷积</a>)，它提供了对大量强大模型的访问，以允许为JAX、PyTorch和Tensorflow构建NLP管道和最先进的机器学习。</p><p id="0904" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">专注于质量，同时更具并行性，需要更少的培训时间，它提供了无数(实际上是数千个)预训练模型来执行实施和任务，以支持您在各种形式上的用例，如视觉和音频，而不仅仅是文本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/b34a87aa6c3cdcc49f1ec02e52451572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KmphGLKndofVSl0tVPSyKA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">转换器遵循这种整体架构，对编码器和解码器使用堆叠自关注和逐点全连接层，分别如图1的左半部分和右半部分所示。【https://bit.ly/3tinEw0 T4】</figcaption></figure><p id="7b14" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一些应用包括:<br/> —聊天机器人(intents)；<br/> —垃圾邮件过滤；<br/> —仇恨言论检测；还有<br/>——社会科学</p><p id="d50b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Transformer架构</a> [1]于2017年6月推出，主要关注对需要翻译的用例的原始研究。随后，新款上市，例如:</p><p id="9841" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">—2018年6月:<a class="ae ky" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">第一个预训练的变压器模型GPT </a>被用于在各种NLP任务上进行微调，并获得了最先进的结果；<br/>—2018年10月:<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>，一个大型预训练模型，旨在产生更好的句子摘要；<br/>—2019年2月:<a class="ae ky" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>，GPT的改进型(也更大)；<br/>—2019年10月:<a class="ae ky" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank"> DistilBERT </a>，速度快60%，内存轻40%，仍保留97%的BERT性能的蒸馏版；<br/>—2019年10月:<a class="ae ky" href="https://arxiv.org/abs/1910.13461" rel="noopener ugc nofollow" target="_blank"> BART </a>和<a class="ae ky" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> T5 </a>，两个使用与原变压器模型相同架构的大型预训练模型(第一个这么做的…)；<br/>—2020年5月，<a class="ae ky" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>，GPT-2的更大版本，无需微调(称为<em class="mt">零炮学习</em>)</p><h1 id="680f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">天资</h1><p id="2624" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">受解决神经语言建模和命名实体识别(NER)中的用例需求的启发，您可以将最先进的自然语言处理(NLP)模型应用于您的反馈(自由文本响应或评论)、NER、词性标注(PoS)、它们的“Flair嵌入”、对各种嵌入的有效访问(例如ELMo、GloVe)，同时Flair继续扩展到多种语言。</p><p id="45d9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">关于解决“再现性”的论文，(1)注意到在识别模型和只报告结果(但没有代码)的论文中再现性的一般无能，(2)注意到承诺发布但从未发布的代码，以及(3)注意到发布但“不可能”运行的代码，这是他们对再现性的“迫切需要”，以实现以下目标:</p><p id="1140" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">—快速设置(框架、数据集、模型)；<br/> —模块化(易于理解什么做什么，易于切换组件，如用手套嵌入件替换BERT嵌入件)；<br/> —适用于新的数据集/任务；和<br/> —易于构建。</p><p id="6ab0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">命名实体识别是一项任务，旨在定位非结构化文本中的命名实体并将其分类到预定义的类别中[2]，包括(但不限于取决于模型及其全面的深入研究)人名、位置或医疗代码。一般来说，NER被视为单层序列标记问题[3]，其中每个标记都用一个标记来标记。</p><p id="c79c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">底线:它的基础反映了它的前提:( 1)易于复制的研究和(2)易于应用的研究。这些启发了开源Flair的开发，它是PyTorch生态系统的一部分。</p><h1 id="21b1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第1部分:从头开始的实现管道</h1><p id="587e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你可以在Github上找到我所有的代码:<a class="ae ky" href="https://bit.ly/3tfPiKj" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3tfPiKj</a></p><h2 id="f474" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">1.导入组类型。</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">完整的代码可以在这里找到:【https://bit.ly/3tfPiKj】T2</figcaption></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h2 id="1942" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">2.从Kaggle导入测试数据，并隔离十行进行实验。</h2><p id="140b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">2a。读取熊猫数据帧并检索信息()</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">完整的代码可以在这里找到:【https://bit.ly/3tfPiKj】T4</figcaption></figure><p id="6a35" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2b。隔离十行</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">完整的代码可以在这里找到:<a class="ae ky" href="https://bit.ly/3tfPiKj" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3tfPiKj</a></figcaption></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h2 id="c488" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">3.实施Flair并打印结果。</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">完整的代码可以在这里找到:<a class="ae ky" href="https://bit.ly/3tfPiKj" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3tfPiKj</a></figcaption></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h2 id="37a2" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">4.实现拥抱脸和打印结果。</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">完整的代码可以在这里找到:<a class="ae ky" href="https://bit.ly/3tfPiKj" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3tfPiKj</a></figcaption></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h2 id="c289" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">5.重新组织列，依次堆叠天赋和拥抱脸预测，以便观察。</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk translated">完整的代码可以在这里找到:<a class="ae ky" href="https://bit.ly/3tfPiKj" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3tfPiKj</a></figcaption></figure><h1 id="b699" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">第2部分:一些结果和后续步骤</strong></h1><p id="18a4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">正如有成千上万的模型供您使用一样，这是事情变得有趣的地方，以便您最终决定使用什么模型。</p><p id="e077" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有趣的是，即使对于这样一个简单的数据集，我们也注意到上面的<strong class="lt iu"><em class="mt">Id = 28</em></strong>with Flair预测它为正面，而拥抱脸为负面。请注意置信度得分的差异。</p><p id="f803" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">ID # 28的语句如下:</p><blockquote class="np nq nr"><p id="d986" class="lr ls mt lt b lu mn ju lw lx mo jx lz ns mp mc md nt mq mg mh nu mr mk ml mm im bi translated">“我很高兴亚马逊有这些电池。我很难在其他地方找到它们，因为它们是如此独特的尺寸。我的车库开门器需要它们。物超所值。”</p><p id="ad34" class="lr ls mt lt b lu mn ju lw lx mo jx lz ns mp mc md nt mq mg mh nu mr mk ml mm im bi translated">Flair的预测(默认管道):<strong class="lt iu">正</strong> <br/>抱紧脸预测(默认管道):<strong class="lt iu">负</strong></p></blockquote><p id="79e9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">重要的是要声明，我按照默认模型的代码激活了默认管道。例如，上面代码中激活的拥抱脸的默认管道模型是“<strong class="lt iu"><em class="mt">”distil bert-base-un cased-fine tuned-SST-2-English，“</em> </strong>一个更小更快版本的BERT，具有来自斯坦福情感树银行的英语数据集。由于这是默认的模型参数，我们不需要在代码中显式调用它。</p><p id="25d6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从这里开始，接下来的步骤有很多方法，我们可以通过这些方法构建一个具体的模型和具体的记号化器，并替换这个默认参数。自动化、简单性和最少的编码量肯定有很大的好处，这样我们可以快速激活模型，进行分析，并获得评估结果，从而得出结论性的结果。</p><p id="d853" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当超越这种自动化或快速输出管道实现时，机会变得无穷无尽，特别是像Hugging Face以编程方式倡导的那样，它允许在NLP管道内开箱即用地轻松替换和自定义激活各种模型，以获得自定义的、更精确的、策划的结果。</p><p id="a451" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">已经有非常成功的定向情绪确定的实现，例如，您可以激活Flair或拥抱脸来确定该情绪(简单地是积极的、消极的或中性的)，引入TextBlob(例如)来激活关于<strong class="lt iu"> <em class="mt">该情绪有多积极或有多消极</em> </strong>的预测(由Flair和/或拥抱脸确定)，并通过情绪的大小进行排序。我创作了一个有效的用例，在员工体验洞察中被成功激活；该出版物可在此处找到:</p><div class="nv nw gp gr nx ny"><a href="https://medium.com/@AnilTilbe/actually-you-need-all-3-for-1-outcome-nlp-sentiment-analysis-with-vader-textblob-and-flair-51226744a05e" rel="noopener follow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">事实上，你需要所有3对1的结果:NLP情绪分析与VADER，文本和天赋</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">整合VADER、文本斑点和天赋来测量情绪和他们的感伤程度，以更准确地量化…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">medium.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om ks ny"/></div></div></a></div><p id="725d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在一个特定的用例中，我觉得我必须使用基于我对各种模型的情感评估的天赋。为了让你了解这种情况，我需要分析80，000条自由文本评论，确定它们的情绪(天赋)，并通过TextBlob对它们的量化程度进行排序。我还使用VADER作为TextBlob的参考来评估TextBlob在“中性”范围内的表现。对于大小为80k的数据，使用特定Flair管道的设备，需要近4个小时来完成情绪管道确定(2015年，双核pc，可以说是通知此类工作最低效的笔记本电脑)。TextBlob本身耗时约180秒(同一台PC)。同样，成本效益、时间限制和我们工作的绝对性。</p><p id="f548" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">关于TextBlob，特别是我之前在数据科学中如何引用它，下面是我撰写的一篇解决该问题的出版物:</p><div class="nv nw gp gr nx ny"><a href="https://medium.com/mlearning-ai/how-to-quantify-sentiment-to-measure-magnitude-nlp-with-textblob-27861559f7a5" rel="noopener follow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">如何量化情感以测量量级:使用TextBlob的NLP</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">作为确定某事是积极还是消极的一部分，构建一个分析实现可能会很方便…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">medium.com</p></div></div><div class="oh l"><div class="on l oj ok ol oh om ks ny"/></div></div></a></div><p id="ea36" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我之所以特别提到TextBlob，是因为除了准确性之外，我们还使用各种技术设备来进行预期的分析(比如源自云的GPU，而不仅仅是您实际拥有的东西)。随着时间的限制，当我们没有几周或几个月的时间来确定最有效的模型来实现我们所寻求的特定类型的精度和/或准确性时，我发现非常有益的是，我可以求助于VADER和TextBlob，同时与其他管道并行工作，在其允许的定制中进行短期和长期管道激活。</p><p id="a5f1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[1]:莫日德·盖尼、任翔和乔纳森·梅。2021.你所需要的就是交叉注意力:为机器翻译调整预训练的变压器。在<em class="mt">2021年自然语言处理经验方法会议记录</em>，1754-1765页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。<br/>【2】:瓦斯瓦尼、阿希什&amp;沙泽尔、诺姆&amp;帕尔马、尼基&amp;乌斯科雷特、雅各布&amp;琼斯、利翁&amp;戈麦斯、艾丹&amp;凯泽、卢卡什&amp;波洛舒欣、伊利亚、<a class="ae ky" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">《注意力就是你需要的一切》</a>，2017。艾伦·阿克比克、塔尼亚·博格曼、邓肯·布莱思、卡希夫·拉苏尔、斯特凡·施韦特和罗兰·沃尔格拉夫。2019.FLAIR:一个易于使用的框架，用于最先进的NLP。在<em class="mt">计算语言学协会北美分会2019年会议记录(示范)</em>中，第54-59页，明尼苏达州明尼阿波利斯。计算语言学协会。</p><p id="b8a3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">flair Github:<a class="ae ky" href="https://github.com/flairNLP/flair" rel="noopener ugc nofollow" target="_blank">https://github.com/flairNLP/flair</a><br/>抱脸Github:<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers</a><br/>我的Github此代码发布的地方:<a class="ae ky" href="https://bit.ly/3tfPiKj" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3tfPiKj</a></p></div></div>    
</body>
</html>