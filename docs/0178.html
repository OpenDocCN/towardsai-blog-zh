<html>
<head>
<title>QWeb: Solving Web Navigation Problems using DQN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">QWeb:使用DQN解决网页导航问题</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/qweb-solvingweb-navigation-problems-using-dqn-8d40ac6a7a5e?source=collection_archive---------0-----------------------#2019-10-11">https://pub.towardsai.net/qweb-solvingweb-navigation-problems-using-dqn-8d40ac6a7a5e?source=collection_archive---------0-----------------------#2019-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a6fe" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">学习导航网络| <a class="ae ep" href="https://towardsai.net" rel="noopener ugc nofollow" target="_blank">走向人工智能</a></h2><div class=""/><div class=""><h2 id="effc" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">学习准确浏览网页</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/10154e7747f3d022e1b3cb3c45229d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9xv8X9df2wojULtL4Gx9sg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/s/photos/web-navigation?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae lh" href="https://unsplash.com/@emilep?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">émile Perron</a>拍摄</figcaption></figure><h1 id="76c2" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="6b99" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">模型强化学习算法在很多现实游戏中都取得了惊人的成绩，比如<a class="ae lh" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noopener ugc nofollow" target="_blank"> Alpha Go </a>和<a class="ae lh" href="https://openai.com/five/" rel="noopener ugc nofollow" target="_blank"> OpenAI Five </a>。在本文中，我们讨论了强化学习的一个更贴近生活的应用，称为网络导航问题，其中一个代理按照一些指令学习导航网络。具体来说，我们讨论了一种由Gur等人在2019年ICLR会议上提出的名为QWeb的算法，该算法利用DQN来解决网络导航问题。</p><p id="6f7f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">尽管这里没有进行数学推理，但我仍然放了许多数学表达式来说明底层的过程。为了更好的可读性，你可能想参考<a class="ae lh" href="https://xlnwel.github.io/blog/reinforcement%20learning/QWeb/" rel="noopener ugc nofollow" target="_blank">我的个人博客</a>。</p><h1 id="2859" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">预赛</h1><h2 id="1136" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">网页导航问题的简单介绍</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/871647209c0c9cbd1c3d7ec170c5e762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gf6M8SBQtjzBWoQc67kgcw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">web导航问题中的DOM树和指令示例</figcaption></figure><p id="7656" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们首先介绍网络导航问题中的两个术语:</p><ul class=""><li id="517d" class="nn no it mc b md mw mg mx mj np mn nq mr nr mv ns nt nu nv bi translated">DOM是网页的树形表示，其元素被表示为一列命名属性。</li><li id="dd1a" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv ns nt nu nv bi translated"><strong class="mc jd">指令</strong>是由键值对表示的字段列表。</li></ul><p id="7abd" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">代理的目标是导航网页(即，修改DOM树)以使其符合指令。下图演示了几个MiniWoB web任务:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/b36ea4e045eef9278949a6646b70db24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVQwO-ZLTWAHfVNFSHHYEA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">资料来源:石等《比特的世界:一个开放域的网络代理平台》</figcaption></figure><h2 id="6cb8" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">问题设置</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/172db437f9e4bbfe3ca430007ce4fa64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*Z-dbXOI8kVzqH7J1E4a9Aw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">QWeb的状态和动作空间</figcaption></figure><p id="4383" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们首先为我们的问题制定MDP，<em class="od"> M= &lt; S，𝜌，a，p，r，g，𝛾 &gt; </em>，其中𝜌表示初始状态分布，转移函数<em class="od"> P </em>是确定性的，目标<em class="od"> G </em>由指令指定，𝛾是贴现因子。其余符号定义如下:</p><ul class=""><li id="6e7a" class="nn no it mc b md mw mg mx mj np mn nq mr nr mv ns nt nu nv bi translated"><strong class="mc jd">状态空间、</strong> <em class="od"> S、</em>由指令和DOM树组成。</li><li id="f328" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv ns nt nu nv bi translated"><strong class="mc jd">动作空间，</strong> <em class="od"> A，</em>包含两个复合动作<em class="od"> Click(e) </em>和<em class="od"> Type(e，y) </em>，其中<em class="od"> e </em>是一个叶子DOM元素，<em class="od"> y </em>是一个指令中字段的值。我们进一步引入三个原子动作来表达这些复合动作:<em class="od"> a^D </em>选择一个DOM元素<em class="od"> e </em>，<em class="od"> a^C </em>指定一个点击或键入动作，<em class="od"> a^T </em>生成一个类型序列。现在我们可以用<em class="od">a^d=e</em><em class="od">a^c='click'</em>和<em class="od">类型(e，y)</em>a^d=e<em class="od">a^c='type'</em>来表示<em class="od">点击(e) </em>。</li><li id="79aa" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv ns nt nu nv bi translated"><strong class="mc jd">奖励，</strong> <em class="od"> R，</em>是一集最终状态和最终目标状态的函数。如果这些状态相同，则为<em class="od"> +1 </em>，否则为<em class="od"> -1 </em>。不给予中间奖励。</li></ul><h1 id="9c42" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">QWeb</h1><p id="1380" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">QWeb使用深度Q网络(DQN)来为每个状态和每个原子动作生成Q值，从而解决了上述问题。在奖励增加和一些课程学习方法的帮助下，培训过程几乎与传统的DQN相同，我们将在后面讨论。但是现在让我们首先关注QWeb的架构，它本质上是这个算法最有成果的部分。</p><h2 id="56ca" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">体系结构</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/5b25d5a56783f8b744610844b78215b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hwr57GZghIDBAE2N1j_YBA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:Gur等人,《学会浏览网页》</figcaption></figure><p id="15e9" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们详细讨论了上图中的每个部分</p><p id="1a25" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">编码用户指令</strong>:正如我们在预备知识中看到的，一条用户指令由一系列字段组成，即键值对<em class="od"> &lt; K，V &gt; </em>。为了产生一个表示，我们首先对关键字中的单词进行编码，对于第<em class="od"> i </em>对的关键字中的第<em class="od"> j </em>个单词，给出我们<em class="od"> f(i，j) </em>。然后我们将一个关键字表示为这些嵌入的平均值，即<em class="od">f _ K(I)= { 1 \ over N } \ sum _ { j } f _ { K }(I，j) </em>，其中<em class="od"> N </em>是关键字的字数。我们也遵循相同的过程来编码值，具有嵌入<em class="od"> f_V(i，j) </em>和<em class="od"> f_V(i) </em>。然后通过全连接层进一步编码键和值嵌入的串联产生指令字段，即<em class="od"> f(i)=FC([f_K(i)，f_V(i)]) </em>，其中<em class="od">【。】</em>表示矢量拼接。整个指令现在可以表示为<em class="od"> f=Stack(f(i)) </em>，一个矩阵，它的行是指令字段。</p><p id="629e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">编码DOM-指令交集</strong>:我们首先通过平均每个序列和每个属性上的单词嵌入来编码DOM元素<em class="od"> j </em>，这给了我们<em class="od"> D(j) </em>。然后对于每个指令字段<em class="od"> f(i) </em>和每个嵌入<em class="od"> D(j) </em>的元素，我们通过编码器<em class="od"> E(f(i)，D(j)) </em>进行编码。<em class="od"> D(j) </em>的条件嵌入可以表示为这些嵌入的加权平均，即<em class="od">E _ { cond }(j)= \ sum _ { I } p _ { I } E(f(I)，D(j)) </em>，其中概率<em class="od"> p_i=softmax(u*f(i)) </em>，其中<em class="od"> u </em>为可训练向量。人们可以把<em class="od"> E_C </em>作为一个自我关注模块，其中<em class="od"> Q=u，K=f，</em>和<em class="od"> V=E(f，D) </em>。(如果你想了解更多关于自我关注的信息，请参考<a class="ae lh" href="https://medium.com/towards-artificial-intelligence/attention-is-all-you-need-transformer-4c34aa78308f" rel="noopener">这篇文章</a></p><p id="8e03" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">编码DOM树</strong>:我们将条件DOM元素嵌入<em class="od"> E_{cond}(j) </em>与DOM元素嵌入<em class="od"> D(j) </em>连接起来，为每个DOM元素生成一个向量，<em class="od"> E_{conc}(j)=[E_{cond}(j)，D(j)】</em>。然后，我们在DOM元素列表的顶部运行一个双向LSTM(biLSTM)网络来编码DOM树。biLSTM的每个输出向量然后通过具有tanh的全连接层进行变换，以生成DOM元素表示，即<em class="od">E _ { tree }(j)= tanh(FC(bil STM(E _ { conc }(j)))</em>。</p><p id="bf68" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">生成<em class="od"> Q </em>值</strong>:我们计算每个字段和每个DOM元素之间的成对相似性，以生成一个上下文矩阵<em class="od"> M=fE_{tree}^T </em>，其中<em class="od"> M </em>的行和列分别显示当前状态下每个字段和每个DOM元素的后验值——注意<em class="od"> M[i][j] </em>是<em class="od"> f(i) </em>和<em class="od"> E_{tree}(j)的点积我们现在使用<em class="od"> M </em>的行作为每个指令字段的<em class="od"> Q </em>值，即<em class="od"> Q(s，a^t=i)=m[i】</em>。我们计算每个DOM元素的<em class="od"> Q </em>值，通过全连接层变换<em class="od"> M </em>并对行求和，即<em class="od"> Q(s，a^D)=sum(FC(M^T)，1) </em>，其中<em class="od"> M^T </em>是<em class="od"> M </em>的转置。最后，通过使用另一个全连接层，即<em class="od">【q(s,a^c)=fc(m^t】</em>，将<em class="od"> M </em>的行转换成<em class="od"> 2 </em>维向量，生成DOM元素上的点击和键入动作的<em class="od"> Q </em>值。</em></p><p id="0efc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">结合浅层编码</strong>:在回报稀少且输入词汇较多的情况下，例如在有数百个机场的机票预订环境中，仅使用单词嵌入很难学习到良好的语义相似性。Gur等人提出用浅层指令和DOM树编码来扩充深层<em class="od"> Q </em>网络，以缓解这个问题。对于浅层编码，我们首先将指令字段和DOM元素的嵌入矩阵定义为基于单词的相似性，例如Jaccard相似性、诸如子集或超集的二进制指示符。我们举一个具体的例子，看看这个嵌入矩阵是什么样子的。假设指令字段的语料库由<em class="od"> ["loc "，" date "，" name"] </em>组成，DOM元素的语料库由<em class="od"> ["name "，" loc"] </em>组成，对应的浅层编码矩阵如下所示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0ef6d14f6422f95c94e4fb4c76351735.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*vU8h19Qv-eh_jaruvEBQtw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">Jaccard相似性示例</figcaption></figure><p id="c3fd" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通过分别对浅编码矩阵的列或行求和，生成DOM元素或指令字段的浅输入向量。举上面的例子；如果我们有一个指令字段<em class="od"> ["loc" "name"] </em>，那么得到的输入向量就是<em class="od"> [0，1] +[1，0]=[1，1] </em>。然后，使用带有tanh的全连接层对这些浅层输入向量进行转换，并通过可训练变量进行缩放，以生成相应的浅层<em class="od"> Q </em>值。最终值是深值和浅值的组合</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/005278f4fcf0fb48af4f65af4b370a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sgLi3Z305h4nCphD3gy53g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">DOM和type action的最终Q值，其中u和v是训练期间学习的标量变量</figcaption></figure><h2 id="e207" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">奖励增加</h2><p id="8341" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">关于架构就说这么多，现在让我们把注意力转移到奖励函数上。预备中定义的环境奖励函数极其稀疏——代理仅在情节结束时获得奖励，更糟糕的是，成功状态是总状态空间的一小部分，这使得成功奖励更加稀疏。因此，作者介绍了潜在的补救奖励。具体来说，它们定义了一个势函数<em class="od"> Potential(s，g) </em>，该函数计算当前状态<em class="od"> s </em>和目标状态<em class="od"> g </em>之间匹配DOM元素的数量。然后，潜在回报函数被计算为下一状态和当前状态的两个潜在值之间的比例差:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/28537b0ba3b1de5a1a1486bc358aa9b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7N7lQT8grRNvZqgz7cXuwg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">潜在回报</figcaption></figure><h1 id="40e9" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">课程学习</h1><p id="f276" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">作者还提出了两种加速学习过程的课程学习策略:</p><ul class=""><li id="c4db" class="nn no it mc b md mw mg mx mj np mn nq mr nr mv ns nt nu nv bi translated">热启动:为了加快学习过程，我们通过选择接近目标状态的初始状态来热启动一集。这些初始状态可以通过随机选择DOM元素的子集并让oracle代理执行正确的操作来获得。热启动的初始状态和目标状态之间的距离随着训练的进行而增加——这可以通过缩小所选子集来容易地实现。</li><li id="b2f1" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv ns nt nu nv bi translated"><strong class="mc jd">目标模拟</strong>:我们也像<a class="ae lh" href="https://arxiv.org/abs/1707.01495" rel="noopener ugc nofollow" target="_blank"> HER </a>所做的那样，将初始状态附近的状态重新标记为子目标，但是这里这些状态是由oracle代理生成的。和以前一样，子目标和初始状态之间的距离随着训练的进行而增加。</li></ul><h2 id="2d2c" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">算法</h2><p id="7c54" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">算法现在应该很清楚了:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/47efad8ed0707411a36d623656b2fc59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fdfWS07aYdPtTe4p3LYMpA.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/56e8a0e32be9ec55405829cf049df75f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KU8b2nUPH7S9DHu6B_A5XA.png"/></div></div></figure><h1 id="58ef" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">MetaQWeb</h1><p id="f3ca" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在上面的方法中，我们求助于oracle代理来加速稀疏回报问题中的学习过程，然而，有时拥有oracle代理是一种奢侈。Gur等人建议将任意导航策略视为某个隐藏指令的专家指令遵循策略。如果我们能够恢复潜在的指令，我们就可以自主地生成新的专家演示，并使用它们来进行课程学习。直观地说，从策略生成指令比遵循指令更容易，因为我们不需要导航器与动态网页交互并采取复杂的动作。在这一节中，我们首先看到如何创建一个任意的导航策略，然后给出一种方法来推断给定策略的指令。为了完整起见，我们将把它们放在一起。</p><h1 id="b1f0" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">基于规则的随机策略(RBRP)</h1><p id="e551" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">基于规则的随机化策略(RBRP，出于某种隐晦的原因，作者将其命名为RRND，为了便于理解，我们采用这个缩写)迭代地访问当前状态下的每个DOM元素并采取行动。它在访问完所有DOM元素后停止，并返回最终状态作为目标状态以及所有中间状态-动作对。我们把这些状态-动作对看作是由某种最优策略产生的。</p><h1 id="3aa9" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">指令生成环境</h1><p id="8125" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">为了训练从目标状态推断指令的代理，我们定义了具有以下属性的指令生成环境:</p><ul class=""><li id="d04d" class="nn no it mc b md mw mg mx mj np mn nq mr nr mv ns nt nu nv bi translated">我们为每个环境预定义了一组可能的指令密钥</li><li id="28ad" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv ns nt nu nv bi translated">状态空间由采样的目标和指令中的单个密钥组成，采样时不替换预定义密钥集中的密钥</li><li id="bc46" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv ns nt nu nv bi translated">指令动作是由1) <em class="od"> a^D </em>和2)<em class="od">a^a</em>组成的复合动作，前者选择一个DOM元素，后者生成一个对应于当前键的值。</li><li id="066d" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv ns nt nu nv bi translated">在每个动作之后，如果相应键的生成值是正确的，代理将收到一个正奖励(+ <em class="od"> 1 </em>),否则将收到一个负奖励(<em class="od"> -1 </em>)。初始状态和目标状态采用QWeb等课程学习策略进行采样。</li></ul><h1 id="a250" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">INET</h1><p id="7ad5" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">INET将指令键和目标DOM树作为输入，并输出实现目标的完整指令。这是通过顺序填充由指令生成环境预定义的每个键的值来完成的。值由一个复合动作生成:找到一个DOM元素；从元素中选择一个DOM属性作为值。接下来，我们介绍INET架构。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0c4fdffbaed1ae74d2f68af21c988bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*KmSOG1TpysLpBDBeYVrk-Q.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">来源:Gur等人,《学会浏览网页》</figcaption></figure><p id="221c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">INET使用与QWeb类似的结构来编码指令键和DOM树，导致分别为键和树嵌入<em class="od"> f_K(i) </em>和<em class="od"> E_{tree} </em>。然后我们计算出<em class="od"> a^D </em>的<em class="od"> Q </em>值为<em class="od">q^i(s_t,a^d)=f_k(i)e_{tree}^t</em>，a^A的<em class="od"> Q </em>值为<em class="od"> Q^I(a,a^A，a^d=j)=fc([e_{tree}[j],f_k(i】)</em>其中<em class="od">。】</em>表示矢量拼接。</p><h2 id="1581" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">算法</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/e8ed89188ac09622dc97fa027ae22725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b63y6vMOlMCc-QiqVubeTg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">MetaQWeb算法</figcaption></figure><p id="26e1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在MetaQWeb中，RBRP随机生成一个目标和一个从当前DOM状态实现该目标的“最优”策略，然后INET试图找到将代理引向该目标的底层指令。然后这些数据被输入到QWeb来训练我们的网络导航算法。</p><p id="4ca0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果我们把国家-行动对看作由某种最优政策产生的最优轨迹，那么密集报酬可能是某种示范损失。然而，作者似乎采取了基于潜力的奖励，如前一节所述。</p><h1 id="cbd5" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">实验结果</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/1ae76a6c92a9e666aa53adaa1911ec6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KnpNecM7uyKDXzqWvHgtA.png"/></div></div></figure><p id="b820" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">正如我们所看到的，QWeb能够在基于DQN的实验中表现出出色的性能。</p><h1 id="e9a8" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">参考</h1><p id="cf4f" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">Izzeddin Gur，Ulrich Rueckert，Aleksandra Faust，Dilek Hakkani-Tur。学习在2019年的ICLR浏览网页</p></div></div>    
</body>
</html>