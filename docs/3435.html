<html>
<head>
<title>Training a Language Model To Give (Non) Legal Advice</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练语言模型以给出(非)法律建议</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/training-a-large-language-model-to-give-non-legal-advice-b9f6d7d11016?source=collection_archive---------0-----------------------#2022-12-23">https://pub.towardsai.net/training-a-large-language-model-to-give-non-legal-advice-b9f6d7d11016?source=collection_archive---------0-----------------------#2022-12-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="fe15" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">在本文中，我将介绍在法律文本数据集上微调BLOOM等大型语言模型的基础知识。你可以在HuggingFace Spaces上试试！</p></blockquote><p id="e4f7" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">我目前正在探索学习使用大型语言模型(LLM)的诀窍。这包括学习如何微调预训练模型，如OPT和BLOOM。</p><p id="14aa" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">为了继续这个探索，我决定尝试建立一个可以回答法律问题的模型。因此，我在此向您介绍:<strong class="jw iu"> BetterCallBloom，</strong>您友好的人工智能法律助理！</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kw"><img src="../Images/ef0ed7c6b46110e9d27ccb6283f84ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JliDVAl4g8t6D4v49iedGA.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk translated"><a class="ae ks" href="https://huggingface.co/spaces/tomrb/bettercallbloom" rel="noopener ugc nofollow" target="_blank"> BettercallBLOOM空间</a>界面截图</figcaption></figure><p id="9663" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在本文中，我将介绍使用Hugging Face Transformers库微调大型语言模型所需的步骤，以及使用简单的前端界面部署训练好的模型所需的步骤。</p></div><div class="ab cl lm ln hx lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="im in io ip iq"><h1 id="0ff6" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">布鲁姆模型</h1><p id="28c6" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke kt mt kh ki ku mu kl km kv mv kp kq kr im bi translated">这个项目背后的明星是来自<a class="ae ks" href="https://huggingface.co/bigscience" rel="noopener ugc nofollow" target="_blank"> BigScience initiative背后令人敬畏的人们的开源BLOOM模型。</a>为了促进由私人研究和未发布模型主导的领域中的研究包容性，BLOOM initiative产生了176B参数的完全开源大型语言模型(与其私人竞争对手OpenAI的GPT3规模相同)。在<a class="ae ks" href="https://huggingface.co/bigscience" rel="noopener ugc nofollow" target="_blank">拥抱面</a>上有多个检查点。</p><p id="4656" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在一个稍微技术性的层面上，BLOOM是一个自回归语言模型，它是用一个“只有解码器”的转换器架构为文本生成任务构建的。它是在ROOTS语料库上进行预训练的，该语料库由超过1.6TB的文本数据构成，包括46种自然语言和13种编程语言。</p></div><div class="ab cl lm ln hx lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="im in io ip iq"><h1 id="df5a" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">法律数据集堆</h1><p id="820f" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke kt mt kh ki ku mu kl km kv mv kp kq kr im bi translated">我在<em class="jv">堆法律</em>数据集上对模型进行了微调，这是一个256 GB的法律文本语料库，范围从美国各州代码到律师考试大纲。法律堆<em class="jv">的一个子集</em>来自于<a class="ae ks" href="https://www.reddit.com/r/legaladvice/" rel="noopener ugc nofollow" target="_blank"> r/legal_advice </a>子编辑，用户可以在那里询问简单的法律问题。这个子集对于我们的用例来说很有趣，因为它已经是问答的格式了。</p><p id="be06" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">让我们看一个来自r/legal_advice子集的原始样本:</p><pre class="kx ky kz la gt mw mx my bn mz na bi"><span id="53ca" class="nb lu it mx b be nc nd l ne nf">{'text': 'Title: Landlord broke lease agreement, what are my rights? (Chicago, IL)\nQuestion:Our landlord has been promising us a washer/dryer unit since we moved in (July 2015). When we resigned the lease August 2016, we wrote into the lease that an in-unit washer and dryer would be installed by September 30th 2016.\n\nSince September 30th, there have been continuous delays in getting the W/D installed. Since it has now been almost a month past the date the W/D was supposed to be installed, I am wondering what types of rights as a tenant I have? \n\nThanks ahead of time for any and all advice given.\nAnswer #1: You can let your landlord know in writing that he is in default under the current lease agreement and give him a reasonable timeframe to cure his default.  \n\nIf he fails to correct the default, you can likely end your lease and move.',<br/> 'created_timestamp': '10-25-2016',<br/> 'downloaded_timestamp': '11-09-2021',<br/> 'url': 'https://www.reddit.com/r/legaladvice/comments/59cv5x/landlord_broke_lease_agreement_what_are_my_rights/'}</span></pre><p id="2d03" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">更具体地看“文本”参数，我们可以看到我们有Reddit帖子的“标题”,问题，然后是一组N个顶级答案。这对我们来说太完美了！我们有一大堆用简单易懂的英语写的问答，我们可以据此微调我们的模型。</p><p id="f795" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">数据集托管在Huggingface数据集上，我们可以用一行程序下载它:</p><pre class="kx ky kz la gt mw mx my bn mz na bi"><span id="a0e9" class="nb lu it mx b be nc nd l ne nf">from datasets import load_dataset<br/><br/>dataset = load_dataset("pile-of-law/pile-of-law",'r_legaladvice')</span></pre></div><div class="ab cl lm ln hx lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="im in io ip iq"><h1 id="d0b6" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">微调布鲁姆-3B</h1><p id="3b44" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke kt mt kh ki ku mu kl km kv mv kp kq kr im bi translated">因为我没有沉溺于无限计算，所以我将自己限制在3B参数BLOOM检查点，这可以使用HuggingFace的<em class="jv"> transformers </em>库轻松下载:</p><pre class="kx ky kz la gt mw mx my bn mz na bi"><span id="64a0" class="nb lu it mx b be nc nd l ne nf">from transformers import BloomTokenizerFast, BloomForCausalLM<br/>tokenizer = BloomTokenizerFast.from_pretrained("bigscience/bloom-3b")<br/>model = BloomForCausalLM.from_pretrained("bigscience/bloom-3b")</span></pre><p id="73cb" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">我们将在因果语言建模任务中对BLOOM进行微调，在该任务中，模型必须根据过去的标记来预测句子中的下一个标记。为此，我们需要准备数据集，以便创建对应于单个序列的输入文本的“块”。</p><p id="735c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">我们将基于HuggingFace团队提供的优秀示例笔记本，对<a class="ae ks" href="https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上可用的语言模型进行微调。</p><p id="bd78" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">我们首先从每个样本中移除URL和时间戳，并标记文本字段。我们可以使用<em class="jv"> dataset.map </em>函数高效地将函数应用于多线程数据集:</p><pre class="kx ky kz la gt mw mx my bn mz na bi"><span id="597f" class="nb lu it mx b be nc nd l ne nf">def tokenize_function(examples):<br/>    return tokenizer(examples["text"])<br/><br/>tokenized_dataset = dataset.map(tokenize_function, <br/>                                batched=True, <br/>                                num_proc=8, <br/>                                remove_columns=["text","created_timestamp","downloaded_timestamp","url"])</span></pre><p id="7aa1" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">接下来，我们创建文本“块”,这些文本将代表我们独特的令牌序列。块大小可以根据GPU上可用的VRAM进行调整。</p><pre class="kx ky kz la gt mw mx my bn mz na bi"><span id="a629" class="nb lu it mx b be nc nd l ne nf">block_size = 128<br/>def group_texts(examples):<br/>    # Concatenate all texts.<br/>    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}<br/>    total_length = len(concatenated_examples[list(examples.keys())[0]])<br/>    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can<br/>        # customize this part to your needs.<br/>    total_length = (total_length // block_size) * block_size<br/>    # Split by chunks of max_len.<br/>    result = {<br/> k: [t[i : i + block_size] for i in range(0, total_length, block_size)]<br/>        for k, t in concatenated_examples.items()<br/>    }<br/>    result["labels"] = result["input_ids"].copy()<br/>    return result<br/><br/>lm_datasets = tokenized_dataset.map(<br/>    group_texts,<br/>    batched=True,<br/>    batch_size=1000,<br/>    num_proc=8,<br/>)</span></pre><p id="7099" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">数据集预处理到此结束！像hugging face<em class="jv">transformers&amp;datasets</em>这样的现代库提供了惊人的抽象程度。</p><p id="72a1" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">你猜怎么着？</p><p id="4195" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">培训更加简单:</p><pre class="kx ky kz la gt mw mx my bn mz na bi"><span id="53a5" class="nb lu it mx b be nc nd l ne nf">trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    train_dataset=lm_datasets["train"],<br/>    eval_dataset=lm_datasets["validation"],<br/>)</span></pre><p id="425b" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">我将为您保存一些在<em class="jv"> training_args </em>中定义的训练参数(详见GitHub中的代码)，但就是这么简单。训练器API将在r/legal device的训练数据集上运行3个时期的训练。</p><p id="67a2" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">该模型在具有40GB VRAM的A-100上训练，这允许我对200个样本和16个批量使用block_size。使用这些设置，在单个A100上进行3个时期的微调大约需要26个小时。</p><p id="bc47" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在<a class="ae ks" href="https://huggingface.co/tomrb/bettercallbloom-3b" rel="noopener ugc nofollow" target="_blank">支撑面</a>上可以获得训练后的重量。</p><h1 id="905b" class="lt lu it bd lv lw ng ly lz ma nh mc md me ni mg mh mi nj mk ml mm nk mo mp mq bi translated">使用Huggingface空间和Gradio部署模型</h1><p id="1ee4" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke kt mt kh ki ku mu kl km kv mv kp kq kr im bi translated">为了构建一个与模型交互的快速界面，我使用了Gradio，并使用廉价的CPU升级将模型托管在HuggingFace Spaces上，以便模型适合RAM内存。</p><p id="0af6" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">为了让模型正常运行，我们需要做一点提示，以便输入与Reddit数据样本的格式相同:</p><pre class="kx ky kz la gt mw mx my bn mz na bi"><span id="d039" class="nb lu it mx b be nc nd l ne nf">tokenizer = BloomTokenizerFast.from_pretrained("tomrb/bettercallbloom-3b")<br/>model = BloomForCausalLM.from_pretrained("tomrb/bettercallbloom-3b",low_cpu_mem_usage=True)<br/><br/>generator = pipeline('text-generation', model=model, tokenizer=tokenizer,do_sample=False)<br/><br/><br/>def preprocess(text):<br/>    #We add 'Question :' and 'Answer #1:' at the start and end of the prompt<br/>    return "\nQuestion: " + text + "\nAnswer #1:"<br/><br/><br/>def generate(text):<br/>    <br/>    preprocessed_text = preprocess(text)<br/>    result = generator(preprocessed_text, max_length=128)<br/>    output = re.split(r'\nQuestion:|Answer #1:|Answer #|Title:',result[0]['generated_text'])[2]<br/>    <br/>    return output</span></pre><p id="36de" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在加载我们的模型和标记器之后，预处理函数将在输入问题的前面添加一个“\nQuestion:”并在末尾添加一个“\ n答案#1:”。这可以防止模型试图继续提问，并提示它立即生成答案。</p><p id="c7b1" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">然后，generate函数简单地将预处理后的输入传递给模型，然后应用regex split函数提取模型生成的第一个答案。</p><p id="793b" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">我用下面的代码片段构建了一个非常简单的Gradio前端:</p><pre class="kx ky kz la gt mw mx my bn mz na bi"><span id="a038" class="nb lu it mx b be nc nd l ne nf">with gr.Blocks() as demo:<br/><br/>  input_text = gr.Textbox(label="Input", lines=6)  <br/>  buton = gr.Button("Submit ")  <br/>  output_text = gr.Textbox(lines=6, label="Output")<br/>  buton.click(generate, inputs=[input_text], outputs=output_text)  <br/><br/>demo.launch(enable_queue=True, debug=True)</span></pre><p id="5414" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">瞧！我们现在在PileOfLaw的r/legal_advice子数据集上有了一个微调版的布鲁姆-3B，它由HuggingFace spaces上的公共前端托管。相当整洁！</p><p id="4311" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">让我们尝试几个样本，看看模型的表现如何。</p><h1 id="581e" class="lt lu it bd lv lw ng ly lz ma nh mc md me ni mg mh mi nj mk ml mm nk mo mp mq bi translated">微调模型的定性性能</h1><p id="0d25" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke kt mt kh ki ku mu kl km kv mv kp kq kr im bi translated">我们将比较BetterCallBloom-3b和Vanilla Bloom-3b(基本Bloom-3b)的答案。</p><blockquote class="jq jr js"><p id="989d" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">问题1: <br/>我和一个朋友正在创业。我们需要填写什么类型的法律文件来澄清企业的所有权？</p></blockquote><p id="2102" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">贝特尔·布鲁姆-3B:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nl"><img src="../Images/069e5856c9e805f101dac0bb751a9351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_PEO7-iPhHLHbF7H8QxMZQ.png"/></div></div></figure><p id="3fc8" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">香草绽放-3B:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nm"><img src="../Images/e20d8906dd426039fd0233e8345ff630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5GDgNdmX6dDjr07zZiuxQ.png"/></div></div></figure><blockquote class="jq jr js"><p id="0a13" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">问题2: <br/>由于定价错误，一家公司要求额外付款或在零件发运后要求退回零件是否合法，您是否需要遵守这一要求？</p></blockquote><p id="c317" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">贝特卡尔布鲁姆-3B</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nn"><img src="../Images/20ee71b8e32846ef15d0529572166cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pT7F8VOkIfLY1vrM8jatrw.png"/></div></div></figure><p id="6fc9" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">香草绽放-3b:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi no"><img src="../Images/7db79a7c2f389b766877adee50588c82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nIYH6JWF3U_1QuP1ZtZB9w.png"/></div></div></figure><blockquote class="jq jr js"><p id="3bf5" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">问题3: <br/>鉴于邻居拒绝让树木移除公司进入其房产并阻止移除过程，我有什么办法移除悬挂在邻居房产和房屋上的危险树木？</p></blockquote><p id="7ce3" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">贝特卡尔布鲁姆-3B</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi np"><img src="../Images/91e115c73d6de4792a5378e52feefdda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SPVfYw33pPBQSHmpEnhq2Q.png"/></div></div></figure><p id="fd9e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">香草绽放-3b:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nq"><img src="../Images/a42af0885bf6150364e8b55e111b516c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JS_7V5RlA2oHtjR6rIlo3A.png"/></div></div></figure><p id="d8be" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">现在我不知道这些答案在法律上是否站得住脚，但我们仍然可以提出两点有趣的看法。香草布卢姆-3B有重复自己的趋势，而它的微调版本没有。其次，BetterCallBloom的回答总体来说非常简短，这可能是r/legal_advice top answers的一个特点。</p><p id="c6d6" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在看到ChatGPT等大型模型的惊人能力后，答案的整体质量很差，不是预期的那样。以下是ChatGPT对问题3的回答示例:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nr"><img src="../Images/957e0989feb947d33ef0627549d8afac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y8h4igdDCssZXqf0EM-f9w.png"/></div></div></figure><p id="1b74" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">ChatGPT的零炮回答质量之高令人瞠目结舌！</p><p id="60d4" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">然而，我们需要记住，来自OpenAI的GPT家族超过175B个参数(比BetterCallBloom-3B大50倍)，并且指令根据人类反馈进行微调，以确保更好的对齐和更高质量的结果。</p><p id="26ac" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在第二篇后续文章中，我将尝试用更大的模型扩展BetterCallBloom，并研究开源指令微调模型，如谷歌的<a class="ae ks" href="https://huggingface.co/google/flan-t5-base" rel="noopener ugc nofollow" target="_blank"> Flan-T5 </a>。为了支持和帮助我发布cool open finetuned模型和应用程序，请按这里的按钮给我买杯咖啡吧！</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><a href="https://www.buymeacoffee.com/thomasrb"><div class="gh gi ns"><img src="../Images/4bc5de35955c00939383a18fb66b41d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*9vg3-OY14aZN1UpKwIxxZg.png"/></div></a></figure><p id="aed6" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">与此同时，您可以在<a class="ae ks" href="https://github.com/ThomasRochefortB/bettercallbloom" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上随意查看微调和演示前端接口的代码。你也可以在<a class="ae ks" href="https://huggingface.co/spaces/tomrb/bettercallbloom" rel="noopener ugc nofollow" target="_blank">拥抱脸空间</a>或者<a class="ae ks" href="https://www.kaggle.com/thomasrochefort/bettercallbloom-3b" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>上试试。另外，你可以在推特上关注我，了解我最新的项目。</p></div><div class="ab cl lm ln hx lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="im in io ip iq"><h1 id="3b37" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">参考资料:</h1><p id="c17f" class="pw-post-body-paragraph jt ju it jw b jx mr jz ka kb ms kd ke kt mt kh ki ku mu kl km kv mv kp kq kr im bi translated">[1] Scao，Teven Le等，“BLOOM:一个176B参数的开放存取多语言语言模型”arXiv预印本arXiv:2211.05100  (2022)。</p><p id="40ea" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">[2] Henderson，Peter等人，“法律堆:从法律和256GB开源法律数据集学习负责任的数据过滤”arXiv预印本arXiv:2207.00220  (2022)。</p></div></div>    
</body>
</html>