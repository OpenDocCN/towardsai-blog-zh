<html>
<head>
<title>26 Words About Machine Learning, Every AI-Savvy Leader Must Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于机器学习的26个词，每个精通人工智能的领导者都必须知道</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/ai-learning-2eaea82ee6d?source=collection_archive---------3-----------------------#2020-05-15">https://pub.towardsai.net/ai-learning-2eaea82ee6d?source=collection_archive---------3-----------------------#2020-05-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3afa" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">人工智能</h2><div class=""/><div class=""><h2 id="74c8" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">你觉得你能解释这些吗？检验你的知识！</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/758dfb9e3c748b62446433b11d10c245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YeUT_Ph2s1lnNlw1EJkSsg.jpeg"/></div></div></figure><p id="b0f0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="lw">【这是</em> <strong class="lc ja"> <em class="lw">系列</em> </strong> <em class="lw">的第5部。在继续之前，请确保您阅读了关于</em> <a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-search-e0cb610237f6" rel="noopener"> <em class="lw">搜索</em></a><a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-knowledge-1020a00eb45d" rel="noopener"><em class="lw">知识</em></a><a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-uncertainty-4ac6810899ac" rel="noopener"><em class="lw">不确定性</em> </a>和<a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-optimization-b8735dc09448" rel="noopener"> <em class="lw">优化</em> </a> <em class="lw">的内容。接下来的题目是</em> <a class="ae lx" href="https://medium.com/towards-artificial-intelligence/26-words-about-neural-networks-every-ai-neural-networks-1085bd972fd5" rel="noopener"> <em class="lw">神经网络</em> </a> <em class="lw">和</em> <a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-language-1d266caa72c6" rel="noopener"> <em class="lw">语言</em> </a> <em class="lw">。】</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ly"><img src="../Images/74d8dd867c5eabacce6893440ced572d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c18F7ACZHyhhIzc4z1EOog.png"/></div></div></figure><p id="6640" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在过去的五年中，全球对“<em class="lw">机器学习</em>”的搜索查询激增。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lz"><img src="../Images/afb695daaa38e73f4ba5a3d399097d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DhExkk3RMFZ6tx689VXrgA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated"><a class="ae lx" href="https://trends.google.com/trends/explore?hl=en-US&amp;tz=-120&amp;date=all&amp;hl=en-US&amp;q=machine+learning&amp;sni=3" rel="noopener ugc nofollow" target="_blank">谷歌搜索趋势</a>，对“机器学习”的兴趣超时</figcaption></figure><p id="f655" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">或许，我们可以用这个术语缺乏明确统一的定义和边界来解释我们的好奇心。目前，最接近一致接受的版本可能听起来像“<em class="lw">计算机无需明确编程就能学习的能力。</em></p><p id="3fd1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是还有更多</p><p id="5b6e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们的新闻供稿、商业期刊和知名专家用诸如<em class="lw">监督学习、无监督学习</em> g、r<em class="lw">e enforcement learning</em>以及最近最流行的<em class="lw">深度学习等时髦词汇轰炸我们。</em></p><p id="0bbc" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">你如何区分所有这些字段？<br/>而机器如何自我学习？</p><p id="8110" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">为了帮助你回答这些问题，本文围绕<strong class="lc ja"><em class="lw"/></strong>学习领域，对<strong class="lc ja"> <em class="lw">主要概念和术语</em> </strong>进行了简要的定义和解释。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="ee2b" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">监督学习:</h1><p id="ebd2" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated"><strong class="lc ja">监督学习:</strong>一个机器学习任务；基于示例输入-输出对学习将输入映射到输出的函数</p><p id="36c1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">分类:</strong>监督学习任务；学习将输入点映射到离散类别的函数</p><p id="51a8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">最近邻分类:</strong>一种算法，在给定输入的情况下，选择离该输入最近的数据点的分类</p><p id="a7ca" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">K-最近邻分类法:</strong>一种算法，在给定输入的情况下，从K个最接近该输入的数据点中选择最常见的类别</p><p id="7a45" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">权重因子:</strong>赋予数据点的权重，以赋予其在组中较轻或较重的重要性</p><p id="d75e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">感知器学习规则:</strong>一种方法，给定数据点<em class="lw"> (x，y) </em>，根据以下内容更新每个权重:</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="64f9" class="nn mm iq nj b gy no np l nq nr">w[i] = w[i] + α(actual value - estimate) × x[i]<br/>w[i] = w[i] + α (y — h[w](x)) × x[i]</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/25de1a5584ea8608d048108f9d4af3d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppWMMP8YGOJeb7nh_q4M7g.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">硬阈值与软阈值</figcaption></figure><p id="394a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">支持向量机:</strong>(或SVM)一种流行的监督式<strong class="lc ja"> </strong>机器学习算法，分析数据并将其分为两类，用于分类和回归分析</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/1bf36921df894b9476034b7d7047bf96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fuVHLbvevNGL8CiMAN9XIg.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">支持向量机</figcaption></figure><p id="c2cb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">最大边距分隔符:</strong>使任何数据点之间的距离最大化的边界</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/b51cff53f619fa1b8fab4dc28da8f46c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IOOQ5TrlCcSmPWNCQHkfLg.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">最大边距分隔符</figcaption></figure><p id="6147" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">回归:</strong>有监督的学习任务；学习将输入点映射到连续值的函数，从而能够预测实数输出</p><h2 id="d9d1" class="nn mm iq bd mn nv nw dn mr nx ny dp mv lj nz oa mx ln ob oc mz lr od oe nb iw bi translated">评估假设:</h2><p id="2c26" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated"><strong class="lc ja">损失函数:</strong>表示我们的假设表现有多差的函数</p><p id="5429" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">0–1损失函数:</strong>一个简单的指标函数，提供关于预测准确性的信息；当目标和输出相等时返回0，否则返回1:</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="764f" class="nn mm iq nj b gy no np l nq nr">L(actual, predicted) =<br/>   0 if actual = predicted, <br/>   1 otherwise</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/105486c90f8f4f013a0c44a826f55a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSmjs4wrrTE7tHOr1CaOrQ.jpeg"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">0-1损失函数</figcaption></figure><p id="cb20" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> L1损失函数:</strong>一个用于最小化误差的损失函数，通过对真实值和预测值之间的所有<em class="lw">绝对</em>差异求和:</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="6def" class="nn mm iq nj b gy no np l nq nr">L(actual, predicted) = | actual - predicted |</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/f8d72c2aee802026b88f13af31832459.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U6EGBlxW2edTr9dQASSqeA.jpeg"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">L1损失函数</figcaption></figure><p id="e4d6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> L2损失函数:</strong>用于减少误差的损失函数，通过对真实值和预测值之间的所有<em class="lw">平方</em>差求和，从而惩罚单个高变化:</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="02d5" class="nn mm iq nj b gy no np l nq nr">L(actual, predicted) = (actual — predicted)^2</span></pre><p id="f8c5" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">过度拟合:</strong>与特定数据集过于拟合的模型，因此可能无法推广到未来数据</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/88e5225d95c35908cfeaa99743548fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0T4MTxz6pVamQmd7AKx7Sw.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">过度拟合</figcaption></figure><p id="9114" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">正则化:</strong>惩罚更复杂的假设，支持更简单、更一般的假设</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="3e09" class="nn mm iq nj b gy no np l nq nr">cost(h) = loss(h) + λcomplexity(h)</span></pre><p id="c315" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">维持交叉验证:</strong>将数据拆分为训练集和测试集，以便在训练集和测试集上进行学习</p><p id="536d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> K倍交叉验证:</strong>将数据拆分成<em class="lw"> k </em>个集合，实验<em class="lw"> k </em>次，每个集合作为一次测试集，剩余数据作为训练集</p><h1 id="4822" class="ml mm iq bd mn mo oi mq mr ms oj mu mv kf ok kg mx ki ol kj mz kl om km nb nc bi translated">强化学习:</h1><p id="96ba" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated"><strong class="lc ja">强化学习:</strong>给定一套奖励或惩罚，学习将来采取什么行动</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/7ae2b2ab2550c7ac2edcb90b2205ff29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A6QklBw2Ahe4byO82hrroA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">强化学习</figcaption></figure><p id="d7aa" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">马尔可夫链:</strong>描述一系列可能事件的随机模型，其中每个事件的概率仅取决于前一事件达到的状态</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/35f7163f63acd5ef3a36c19303fcad6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NoYp54Y1Xzq_AJBxxvoIMQ.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">马尔可夫链</figcaption></figure><p id="b1a2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">马尔可夫决策过程:</strong>决策模型，代表状态、行动和奖励</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/147460da708f80adb762fe1686e9d658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FJ0XsOLF1xYjPXq3ZF3tEQ.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">马尔可夫决策过程</figcaption></figure><p id="30e4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> Q-learning: </strong>学习一个函数的方法；在状态<em class="lw"> s </em>下执行动作<em class="lw"> a </em>的值的估计</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="a82a" class="nn mm iq nj b gy no np l nq nr">Q(s, a)</span></pre><p id="50a0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">贪婪决策:</strong>当处于状态<em class="lw"> s </em>时，选择动作<em class="lw">a</em>max。问(美、俄)</p><p id="7e06" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> ε-greedy: </strong>(或ε-greedy)一种简单的机器学习算法，当在<em class="lw">探索性</em>和<em class="lw">开发性</em>选项之间做出决定时，该算法会考虑随机性:</p><pre class="kp kq kr ks gt ni nj nk nl aw nm bi"><span id="221f" class="nn mm iq nj b gy no np l nq nr">1-ε   (exploitative, choose estimated best move)<br/>ε     (explorative, choose a random move)</span></pre><p id="e852" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="lw">【如果你对探索vs .剥削的争论很好奇，可以看看</em> <a class="ae lx" href="https://towardsdatascience.com/exploration-in-reinforcement-learning-e59ec7eeaa75" rel="noopener" target="_blank"> <em class="lw">这篇文章</em> </a> <em class="lw">作者</em><a class="op oq ep" href="https://medium.com/u/1f2b933522e2?source=post_page-----2eaea82ee6d--------------------------------" rel="noopener" target="_blank"><em class="lw">Ziad sall Oum</em></a><em class="lw">。】</em></p><p id="a9f3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">函数逼近:</strong>逼近<em class="lw"> Q(s，a) </em>，通常通过组合各种特征的函数，而不是为每个状态-动作对存储一个值</p><h1 id="7f1e" class="ml mm iq bd mn mo oi mq mr ms oj mu mv kf ok kg mx ki ol kj mz kl om km nb nc bi translated">无监督学习:</h1><p id="4998" class="pw-post-body-paragraph la lb iq lc b ld nd ka lf lg ne kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated"><strong class="lc ja">无监督学习:</strong>给定输入数据而没有任何额外的反馈和学习模式</p><p id="43f3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">聚类:</strong>将一组对象组织成组，使得相似的对象倾向于在同一组中</p><p id="2b5d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> K-means聚类:</strong>一种基于重复向聚类分配点并更新这些聚类的中心来对数据进行聚类的算法</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi or"><img src="../Images/8f1ec0d08057916082126014a18a809c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HwMi4FWMO51khHOFSKKuFw.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk translated">k均值聚类</figcaption></figure></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="851e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在，你已经准备好继续成为一名成熟的人工智能领导者，你应该能够解释机器学习的关键术语和概念。这个领域将持续存在，并有望成为这十年中创造价值的主要来源之一。</p><blockquote class="os"><p id="b28c" class="ot ou iq bd ov ow ox oy oz pa pb lv dk translated">机器学习的一个突破值十个微软。</p><p id="e3e5" class="ot ou iq bd ov ow ox oy oz pa pb lv dk translated">比尔·盖茨</p></blockquote><p id="4d50" class="pw-post-body-paragraph la lb iq lc b ld pc ka lf lg pd kd li lj pe ll lm ln pf lp lq lr pg lt lu lv ij bi translated">探索类似<strong class="lc ja"> <em class="lw">相关话题</em> </strong>，包括<a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-search-e0cb610237f6" rel="noopener"> <em class="lw">搜索</em></a><a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-knowledge-1020a00eb45d" rel="noopener"><em class="lw">知识</em></a><a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-uncertainty-4ac6810899ac" rel="noopener"><em class="lw">不确定性</em></a><em class="lw"/><a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-optimization-b8735dc09448" rel="noopener"><em class="lw">优化</em></a><a class="ae lx" href="https://medium.com/towards-artificial-intelligence/26-words-about-neural-networks-every-ai-neural-networks-1085bd972fd5" rel="noopener"><em class="lw">神经网络</em></a><a class="ae lx" href="https://medium.com/towards-artificial-intelligence/ai-language-1d266caa72c6" rel="noopener"><em class="lw">语言</em> </a>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ly"><img src="../Images/74d8dd867c5eabacce6893440ced572d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c18F7ACZHyhhIzc4z1EOog.png"/></div></div></figure><p id="a55e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">喜欢读什么？</em> </strong> <em class="lw"> </em> <strong class="lc ja"> <em class="lw">渴望了解更多？</em> </strong> <em class="lw"> <br/>跟我上</em> <a class="ae lx" href="https://medium.com/@yannique" rel="noopener"> <em class="lw">中</em> </a> <em class="lw">或</em><a class="ae lx" href="https://www.linkedin.com/in/yannique/" rel="noopener ugc nofollow" target="_blank"><em class="lw">LinkedIn</em></a><em class="lw">。</em></p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="ce81" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja"> <em class="lw">关于作者:<br/> </em> </strong> Yannique Hecht作品在结合策略、客户洞察、数据、创新等领域。虽然他的职业生涯一直在航空、旅游、金融和技术行业，但他对管理充满热情。Yannique专门开发AI &amp;机器学习产品商业化的策略。</p></div></div>    
</body>
</html>