<html>
<head>
<title>A Quick Introduction to Semantic Clustering for Large Texts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大文本语义聚类快速介绍</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/a-quick-introduction-to-semantic-clustering-for-large-texts-3660a77b9611?source=collection_archive---------1-----------------------#2021-02-04">https://pub.towardsai.net/a-quick-introduction-to-semantic-clustering-for-large-texts-3660a77b9611?source=collection_archive---------1-----------------------#2021-02-04</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="1878" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a></h2><div class=""/><div class=""><h2 id="588a" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">在大型语料库中轻松地将具有相似含义的文档分组在一起。</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/7fd95ed445321f0aa21d94984f9d1164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hDX_1Z7z7r3Xmmo_"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">照片由<a class="ae li" href="https://unsplash.com/@melissaaskew?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">梅丽莎·艾斯丘</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="ce42" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">很多时候，一个简单的自然语言处理任务，比如将大量的句子组合在一起，看起来会令人望而生畏。这些数据可能会遇到多种障碍和多种技术障碍。</p><blockquote class="mf mg mh"><p id="35b6" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated">我们本地机器的GPU可能无法满足我们运行传统聚类算法所需的处理能力。类似地，我们可能会在对大量数据(例如100k+句子)进行编码时耗尽内存。</p><p id="15c8" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated">目前也有许多先进的模型，当用于新任务的迁移学习而不是通过从头开始训练建立新的模型时，这些模型表现得更好。</p></blockquote><p id="5cbe" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在这篇文章中，我记录了一个免费的、开源的NLP库是如何帮助我以最小的努力完成所有这些任务的，以及它是如何在内存使用和性能方面如此高效，并产生最先进的结果。</p><p id="4444" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">让我们开始吧！</p><h2 id="091b" class="mm mn iu bd mo mp mq dn mr ms mt dp mu ls mv mw mx lw my mz na ma nb nc nd ja bi translated">了解数据！</h2><p id="5ba4" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">之前，在本教程的<a class="ae li" href="https://medium.com/this-code/how-to-make-a-youtube-video-comments-dataset-with-the-googles-python-api-34cf32a14d16" rel="noopener">中，我已经谈过用你最喜欢的创作者的YouTube视频的评论来制作你自己的文本数据集。我使用YouTube Python数据API构建了一个脚本来收集评论。很有趣！</a></p><p id="ff81" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">在本文中，我们将在收集的评论中寻找一些<em class="mi">最常被谈论的话题</em>。</strong></p><p id="4d48" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果你还没有亲自经历和实现这个过程，我强烈建议你这么做，因为在我看来，建立你自己的数据集是一次很好的学习经历。我知道这一点，因为我已经建立了两个。我怎么强调都不为过。</p><p id="1fc3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">不管怎样，如果你想跳过这一部分——下面是我从评论的<em class="mi"> CSV文件</em>中收集的数据:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj nj"><img src="../Images/df75fbf3971bd33e8b8ea422addefa38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ilscOlpFKkS4Ia9bX4eIPw.png"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated">评论数据预览</figcaption></figure><p id="f25c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">有四列:</strong></p><ol class=""><li id="fa23" class="nk nl iu ll b lm ln lp lq ls nm lw nn ma no me np nq nr ns bi translated">注释的文本</li><li id="e1a7" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me np nq nr ns bi translated">评论的作者</li><li id="1fa6" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me np nq nr ns bi translated">comment ID(在聚类算法中不使用它，但无论如何都要保留它，因为它可以用于以后的一些应用程序)最后，</li><li id="419e" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me np nq nr ns bi translated">每个评论对应的赞。</li></ol><p id="ff96" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我收集的数据包括从YouTube上一个创作者的不同视频中收集的37k条评论。你可以继续自己的实验，通过摆弄API和脚本，从而尝试构建一个包含更多不同视频的综合数据集。</p><blockquote class="ny"><p id="e46c" class="nz oa iu bd ob oc od oe of og oh me dk translated">这完全取决于您稍后将对该数据执行的任务类型，所以请记住这一点。</p></blockquote><p id="9495" class="pw-post-body-paragraph lj lk iu ll b lm oi ke lo lp oj kh lr ls ok lu lv lw ol ly lz ma om mc md me in bi translated">现在我们知道了我们在这里处理的是什么数据，让我们继续，看看我们在语义聚类中到底要做什么。</p><h2 id="530a" class="mm mn iu bd mo mp mq dn mr ms mt dp mu ls mv mw mx lw my mz na ma nb nc nd ja bi translated">群集任务的用例</h2><p id="ff5c" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">根据收集到的意见，我们希望首先确定两件关键的事情:</p><ul class=""><li id="9c4e" class="nk nl iu ll b lm ln lp lq ls nm lw nn ma no me on nq nr ns bi translated">我们认为什么样的话题是相同的，以及</li><li id="80ee" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me on nq nr ns bi translated">在这些话题应该如何分组的问题上，<em class="mi">的喜欢数</em>有发言权吗</li></ul><p id="c851" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">下一步是安装<a class="ae li" href="https://sbert.net" rel="noopener ugc nofollow" target="_blank">句子Bert变形金刚库</a>。这是他们的官方文档，你可以用它来更熟悉这个库本身。不过，为了继续，让我们先构建一个新的虚拟环境。</p><blockquote class="mf mg mh"><p id="9dc4" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated">建立一个干净的环境对于任何Python项目来说都是非常重要的。我还在本文末尾提供了一个参考资料，供您了解更多信息。</p></blockquote><p id="b9d1" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在终端中，<strong class="ll je">在您的项目目录中创建</strong>和<strong class="ll je"> cd </strong>，并输入:</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="0b95" class="mm mn iu op b gz ot ou l ov ow">pipenv shell</span></pre><p id="7887" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这将创建并激活环境。让我们也安装jupyter笔记本。如果您已经这样做了，请跳过这一步。</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="adf0" class="mm mn iu op b gz ot ou l ov ow">pipenv install jupyter</span></pre><p id="a576" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">最后，你可以运行它，</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="bda2" class="mm mn iu op b gz ot ou l ov ow">pipenv run jupyter notebook</span></pre><p id="0ab7" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这将启动您的笔记本环境。在新的单元格中，继续运行这行代码来安装<strong class="ll je"> sbert </strong>:</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="557c" class="mm mn iu op b gz ot ou l ov ow">!pipenv install sentence-transformers</span></pre><p id="b92f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">完成后，我们将开始将数据导入笔记本。</p><blockquote class="mf mg mh"><p id="e3a3" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated"><strong class="ll je"> Pro提示:</strong>确保注释掉那行代码，以便将来使用，因为您不想多次安装该库！</p></blockquote><h2 id="a0ca" class="mm mn iu bd mo mp mq dn mr ms mt dp mu ls mv mw mx lw my mz na ma nb nc nd ja bi translated">建立评论列表</h2><p id="df53" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">首先导入<strong class="ll je">熊猫</strong>并读取csv文件。</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="2e44" class="mm mn iu op b gz ot ou l ov ow">import pandas as pd</span><span id="4a63" class="mm mn iu op b gz ox ou l ov ow">df1 = pd.read_csv('data/MAKE YOUR OWN CAMERA TRANSITIONS!!.csv', usecols = [1, 4]).sort_values(by = ['Like Count'], ascending = False).reset_index(drop = True)</span><span id="d039" class="mm mn iu op b gz ox ou l ov ow">df1.head()</span></pre><p id="08e5" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我已经把我的数据放入<strong class="ll je">数据</strong>文件夹，根据你的需要更改它。我还做了另外三件事:</p><ol class=""><li id="4fa4" class="nk nl iu ll b lm ln lp lq ls nm lw nn ma no me np nq nr ns bi translated">评论将按喜欢数降序排列，因此我们将获得整个视频中最有影响力的评论(如果我们认为喜欢决定了评论获得的参与度)。您也可以定义另一个度量来决定这一点。</li><li id="4d40" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me np nq nr ns bi translated">我已经重置了索引，并确保不包含任何索引列。</li><li id="9fa5" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me np nq nr ns bi translated">我已经确保只包括文本和类似的计数在这个项目中进一步使用。<strong class="ll je">评论ID </strong>和<strong class="ll je">作者</strong>列也有它们的用处，但是在这里，为了基于主题对评论进行聚类，我们并不真的需要它们。</li></ol><p id="2c13" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，我们得到这样的数据帧:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oy"><img src="../Images/3f3e7d96a6f86463c8c56333fae746ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j3xi_5-FAa4YaCqgAi9D1w.png"/></div></div><figcaption class="le lf gk gi gj lg lh bd b be z dk translated"><strong class="bd mo"> df1 </strong>数据帧的前5行</figcaption></figure><p id="71ba" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在对于句子转换器，我们需要给它一个注释列表，以便它从中计算它们的嵌入。在本教程的最后，如果你想了解更多，我有一个很好的资源供你参考。</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="fb6e" class="mm mn iu op b gz ot ou l ov ow">comments_list_1 = df1.Comment.tolist()</span></pre><p id="0b00" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">一旦我们从所有文件中得到最终的注释列表，我们就可以开始对它们进行编码了！</p><h2 id="1fe2" class="mm mn iu bd mo mp mq dn mr ms mt dp mu ls mv mw mx lw my mz na ma nb nc nd ja bi translated">试用编码器</h2><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="41ee" class="mm mn iu op b gz ot ou l ov ow">from sentence_transformers import SentenceTransformer, util<br/>import numpy as np</span><span id="42ce" class="mm mn iu op b gz ox ou l ov ow">model = SentenceTransformer('distilbert-base-nli-stsb-quora-ranking')</span></pre><p id="6b18" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">distilbert变压器只是你可以使用的模型之一，还有许多其他的你可以尝试。</p><p id="1e48" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">编码只需一个简单的步骤:</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="fbd7" class="mm mn iu op b gz ot ou l ov ow">embeddings = model.encode(comments_list, show_progress_bar=True, convert_to_numpy=True)</span></pre><p id="92a0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">其中，<strong class="ll je">评论_列表</strong>是我们根据所有评论制作的列表。</p><blockquote class="mf mg mh"><p id="a0cf" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated">这是需要很多时间才能完成的步骤。因此，我建议你在你的本地机器上<strong class="ll je">腌制</strong>，这样你就只需要计算<em class="iu">一次</em>。</p></blockquote><p id="3167" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在我们已经完成了，剩下的唯一一步就是执行集群。</p><h2 id="8379" class="mm mn iu bd mo mp mq dn mr ms mt dp mu ls mv mw mx lw my mz na ma nb nc nd ja bi translated">制作聚类函数</h2><p id="1ae1" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">这是我们项目的主要部分，也是我们一直热切期待的部分！</p><p id="b9a6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为工作定义一个新的功能。</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="516b" class="mm mn iu op b gz ot ou l ov ow">def detect_clusters(embeddings, threshold=0.90, min_community_size=20):</span></pre><p id="a3a1" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这里有两个主要参数需要理解，所以让我们来看一下:</p><ul class=""><li id="0eab" class="nk nl iu ll b lm ln lp lq ls nm lw nn ma no me on nq nr ns bi translated"><strong class="ll je">阈值</strong> —每个评论相对于其他评论的语义相似度的余弦值。</li><li id="5348" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me on nq nr ns bi translated"><strong class="ll je"> min_community_size </strong> —这是为了将一个集群定义为单独的集群而必须堆积到该集群中的最小评论数量。</li></ul><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="be62" class="mm mn iu op b gz ot ou l ov ow"># Compute cosine similarity scores<br/>cos_scores = util.pytorch_cos_sim(embeddings, embeddings)</span></pre><p id="6a20" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">接下来，我们确保根据我们之前指定的最小社区规模过滤这些分数。</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="efa6" class="mm mn iu op b gz ot ou l ov ow"># Minimum size for a community<br/>    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=True)</span></pre><p id="1649" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后，我们过滤我们的阈值。您肯定需要调整这两个值，以便为您正在使用的数据集获得所需的适当聚类。</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="612c" class="mm mn iu op b gz ot ou l ov ow"># Filter for rows &gt;= min_threshold<br/>    extracted_communities = []<br/>    for i in range(len(top_k_values)):<br/>        if top_k_values[i][-1] &gt;= threshold:<br/>            new_cluster = []</span></pre><p id="27a0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">最后，我们继续用提取的社区构建我们的集群。只有获得的<strong class="ll je">前k个最相似分数</strong>将被用于附加到聚类中。</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="6370" class="mm mn iu op b gz ot ou l ov ow"># Only check top k most similar entries<br/>            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=True)<br/>            top_idx_large = top_idx_large.tolist()<br/>            top_val_large = top_val_large.tolist()</span><span id="8a13" class="mm mn iu op b gz ox ou l ov ow">if top_val_large[-1] &lt; threshold:<br/>            for idx, val in zip(top_idx_large, top_val_large):<br/>                  if val &lt; threshold:<br/>                       break</span><span id="43f8" class="mm mn iu op b gz ox ou l ov ow">new_cluster.append(idx)<br/>         else:<br/>             # Iterate over all entries (slow)<br/>             for idx, val in enumerate(cos_scores[i].tolist()):<br/>                  if val &gt;= threshold:<br/>                       new_cluster.append(idx)</span><span id="cb1a" class="mm mn iu op b gz ox ou l ov ow">extracted_communities.append(new_cluster)</span></pre><p id="6715" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">最后，我们不希望集群中有任何共同的、重复的社区。我们也把它们去掉吧。</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="88f3" class="mm mn iu op b gz ot ou l ov ow">unique_communities = []<br/>extracted_ids = set()</span><span id="ab15" class="mm mn iu op b gz ox ou l ov ow">for community in extracted_communities:<br/>        add_cluster = True<br/>        for idx in community:<br/>            if idx in extracted_ids:<br/>                add_cluster = False<br/>                break</span><span id="baa8" class="mm mn iu op b gz ox ou l ov ow">        if add_cluster:<br/>            unique_communities.append(community)<br/>            for idx in community:<br/>                extracted_ids.add(idx)</span><span id="bac7" class="mm mn iu op b gz ox ou l ov ow">return unique_communities</span></pre><p id="09c0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">瞧，我们的功能完成了。</p><h2 id="3a7e" class="mm mn iu bd mo mp mq dn mr ms mt dp mu ls mv mw mx lw my mz na ma nb nc nd ja bi translated">获得结果！</h2><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="052a" class="mm mn iu op b gz ot ou l ov ow">clusters = detect_clusters(embeddings, min_community_size=15, threshold=0.95)</span></pre><p id="0304" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">让我们看看我们的函数产生的一些结果！</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="7afe" class="mm mn iu op b gz ot ou l ov ow">Cluster 8, #22 Elements <br/>	 Your video was really helpful, I will try these tips for  my new contents. Thank you.<br/>	 Very, very useful tool.  Thanks your best Video.<br/>	 Very good video.  Can be an inspiration for me.  thank you<br/>	 I really love your video. Hoping see you in real life. Respcect!!<br/>	 Man your videos be awesome this is a awesome channel I learned a lot from you just...</span></pre><p id="0946" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">这个集群有人感谢创作者非常有用的视频。</strong></p><p id="24dc" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">看一个不同的集群:这里<strong class="ll je">人们在谈论视频的伟大thumnail </strong></p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="e6c8" class="mm mn iu op b gz ot ou l ov ow">Cluster 4, #45 Elements<br/>         The thumbnail though. Great hacks!<br/>	 That thumbnail though.<br/>	 Wow that thumbnail 🔥<br/>	 The thumbnail scared me...<br/>	 That thumbnail is the scariest thing I've seen all day<br/>	 this is the best thumbnail ive ever seen</span></pre><p id="c9ca" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">或者在<strong class="ll je">中带有评论的集群，人们只说一件特定的事情。</strong></p><p id="a23c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">例如:</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="a88f" class="mm mn iu op b gz ot ou l ov ow">Cluster 17, #29 Elements</span><span id="8203" class="mm mn iu op b gz ox ou l ov ow">         Such a great great work sir you nailed it ❤️ love form india<br/>	 mindblowing bro,love from India<br/>	 Dude!!You are inspiration!! Great work Peter!! Love from India!! ❤️❤️<br/>	 Man u r such an inspiring <br/>         Love and respect from India<br/>	 Love from India!<br/>	 learnt so much from you today. love from India.</span></pre><p id="fc19" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">或者甚至:</p><pre class="kt ku kv kw gu oo op oq or aw os bi"><span id="5b22" class="mm mn iu op b gz ot ou l ov ow">Cluster 19, #15 Elements</span><span id="08c8" class="mm mn iu op b gz ox ou l ov ow">         now i'm craving coffee..i don't even like coffee.<br/>	 I don't even like coffee.<br/>	 You are right lol I dont like coffee at all.<br/>	 Yep, I don't drink coffee anymore, and I never really liked it too much. Still enjoy the coffee parts.<br/>	 Don’t like coffee?! Ha! 😂<br/>	 I don't even like coffee, but it's so aesthetically pleasing!</span></pre><p id="fbf3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">诸如此类！这只是一个例子，说明了这种算法有多么强大，它可以在您选择构建的任何现实世界项目中提供多么棒的应用程序！</p><p id="fc47" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在我的下一篇文章中，我将探索更多分析这些聚类的方法，以及利用我们的另一个专栏的一些技术——也就是<strong class="ll je">喜欢计数</strong>。</p></div><div class="ab cl oz pa hy pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="in io ip iq ir"><p id="ca1a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">结束…以下是我谈到的资源:</strong></p><ol class=""><li id="2370" class="nk nl iu ll b lm ln lp lq ls nm lw nn ma no me np nq nr ns bi translated"><a class="ae li" href="https://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe" rel="noopener"> NLP嵌入深度探讨</a> —从这篇精彩的文章中了解如何从文本中生成嵌入。</li><li id="ff18" class="nk nl iu ll b lm nt lp nu ls nv lw nw ma nx me np nq nr ns bi translated"><a class="ae li" href="https://medium.com/this-code/5-step-guide-to-setting-up-a-new-python-environment-for-data-science-fcae3c3951c7?source=your_stories_page-------------------------------------" rel="noopener">为数据科学/深度学习建立一个全新的Python环境。</a> —帮助您快速启动并运行的指南。</li></ol><p id="f4fa" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">快乐学习！😁</p></div><div class="ab cl oz pa hy pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="in io ip iq ir"><p id="b551" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">感谢您的阅读！</p><p id="f1c9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">另外，请点击此处查看我所有数据科学文章的代码库:</p><div class="pg ph gq gs pi pj"><a href="https://github.com/yashprakash13/data-another-day" rel="noopener  ugc nofollow" target="_blank"><div class="pk ab fp"><div class="pl ab pm cl cj pn"><h2 class="bd je gz z fq po fs ft pp fv fx jd bi translated">yashprakash 13/data-另一天</h2><div class="pq l"><h3 class="bd b gz z fq po fs ft pp fv fx dk translated">我在我的…上学习和撰写的所有数据科学项目、概念、工具和资源的主存储库</h3></div><div class="pr l"><p class="bd b dl z fq po fs ft pp fv fx dk translated">github.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px lc pj"/></div></div></a></div><p id="e5d9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">联系我的<a class="ae li" href="https://www.linkedin.com/in/yashprakash13/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae li" href="https://twitter.com/csandyash" rel="noopener ugc nofollow" target="_blank"> Twitter </a>！</p></div></div>    
</body>
</html>