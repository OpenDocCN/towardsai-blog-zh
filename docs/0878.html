<html>
<head>
<title>How to Improve Data Labeling Efficiency with Auto-Labeling, Uncertainty Estimates, and Active Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何通过自动标注、不确定性估计和主动学习来提高数据标注效率</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/improving-data-labeling-efficiency-with-auto-labeling-uncertainty-estimates-and-active-learning-5848272365be?source=collection_archive---------0-----------------------#2020-09-02">https://pub.towardsai.net/improving-data-labeling-efficiency-with-auto-labeling-uncertainty-estimates-and-active-learning-5848272365be?source=collection_archive---------0-----------------------#2020-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7fc7" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/2041aea239dad89ad6fb63f6f80d4d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GfR8F5BsHPP-Vgjmd165mA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk translated">如何判断这个被标记的图像对训练模型是否“有用”？多读点了解一下。来源:知识共享的衍生，照片由<a class="ae ko" href="https://sv.m.wikipedia.org/wiki/Fil:Sixth_Avenue_and_Central_Park_South_(Unsplash).jpg" rel="noopener ugc nofollow" target="_blank">弗兰克·科恩托普</a>拍摄</figcaption></figure><p id="5168" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这篇文章中，我们将深入研究机器学习理论和技术，这些理论和技术是为了评估我们在<a class="ae ko" href="http://www.superb-ai.com" rel="noopener ugc nofollow" target="_blank"> Superb AI </a>的自动标记AI而开发的。更具体地说，我们的数据平台如何估计自动标注的不确定性，并将其应用于主动学习。</p><p id="99ce" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在开始之前，最好先了解一下最流行的方法可以被归类到哪些类别。根据我们的经验，深度学习不确定性估计的大多数工作分为两类。第一种属于<em class="ln">蒙特卡罗抽样</em>的范畴，对每个原始数据进行多种模型推断，并使用这些推断之间的差异来估计不确定性。第二种方法<em class="ln">通过让神经网络学习分布参数来模拟模型输出</em>的概率分布。这里的主要目的是给我们探索的技术种类以广度，并希望提供一些关于我们如何和为什么在这个主题上达到我们独特的位置的清晰性。我们还希望有效地证明我们的特定不确定性估计方法的可扩展性。</p><h1 id="311c" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">1.自动标注功效的快速回顾</h1><p id="a1e6" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">在我们深入研究评估自动标记性能的各种方法之前，有一点需要注意。自动标记人工智能虽然非常强大，但并不总是100%准确。因此，我们需要测量和评估在使用自动标记时，我们可以在多大程度上信任输出。一旦我们可以做到这一点，那么使用自动标注的最有效的方法就是让人类用户根据这一度量来区分查看和编辑哪个自动标注的注释的优先级。</p><p id="a62f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">测量模型输出的“可信度”是一种流行的方法。然而，这种方法的一个众所周知的缺点是，如果模型过度拟合给定的训练数据，即使预测结果是错误的，置信水平也可能会错误地高。因此，置信度不能用来衡量我们可以“信任”多少自动标注的注释。</p><p id="d759" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">相比之下，估计模型输出的“不确定性”是一种更有根据的方法，因为这种方法从统计上衡量了我们对模型输出的信任程度。利用这一点，我们可以获得与模型预测误差的概率成比例的不确定性度量，而不管模型置信度得分和模型过度拟合。这就是为什么我们认为一个有效的自动标记技术需要与一个健壮的方法相结合来估计预测的不确定性。</p><h1 id="3320" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">2.方法1:蒙特卡罗抽样</h1><p id="d4f9" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">研究团体提出的一种可能的不确定性估计方法是获得每个输入数据(即图像)的多个模型输出，并使用这些输出计算不确定性。这种方法可以看作是一种基于蒙特卡罗抽样的方法。</p><p id="5865" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们看看下面的一个3类分类输出的例子。</p><blockquote class="mr ms mt"><p id="84ca" class="kp kq ln kr b ks kt ku kv kw kx ky kz mu lb lc ld mv lf lg lh mw lj lk ll lm im bi translated">y1 = [0.9，0.1，0]</p><p id="6445" class="kp kq ln kr b ks kt ku kv kw kx ky kz mu lb lc ld mv lf lg lh mw lj lk ll lm im bi translated">y2 = [0.01，0.99，0]</p><p id="f730" class="kp kq ln kr b ks kt ku kv kw kx ky kz mu lb lc ld mv lf lg lh mw lj lk ll lm im bi translated">y3 = [0，0，1]</p><p id="afc8" class="kp kq ln kr b ks kt ku kv kw kx ky kz mu lb lc ld mv lf lg lh mw lj lk ll lm im bi translated">y4 = [0，0，1]</p></blockquote><p id="a121" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里，<strong class="kr jd"> <em class="ln"> y1 ~ y4 </em> </strong>中的每一个是来自四个不同模型对相同输入数据的模型输出(即，第一个模型给予类#1最高的概率，等等。).最天真的方法是使用四个不同的模型来获得这四个输出，但使用贝叶斯深度学习或辍学层可以为单个模型提供随机性，并允许我们从单个模型获得多个输出。</p><p id="1a99" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一种特殊类型的基于蒙特卡罗的方法被称为贝叶斯主动学习(BALD) [1，2]。BALD对不确定性的定义如下:</p><blockquote class="mr ms mt"><p id="de17" class="kp kq ln kr b ks kt ku kv kw kx ky kz mu lb lc ld mv lf lg lh mw lj lk ll lm im bi translated">不确定性(x) =熵(Avg[y1，…，yn])-Avg[熵(y1)，…，熵(yn)]</p></blockquote><p id="9877" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">就像我们上面的示例输出一样，BALD假设我们可以为单个输入数据(x)获得多个输出(<strong class="kr jd"> <em class="ln"> y1 ~ yn </em> </strong>)。例如，x可以是图像，每个<strong class="kr jd"> <em class="ln"> y </em> </strong>可以是多类softmax输出向量。</p><p id="7b3d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">根据BALD公式，只有当多个模型输出将高概率分配给同一类别时，模型预测才具有低不确定性。例如，如果一个模型输出以高概率预测了“人”类，而另一个模型输出以高概率预测了“车”类，则组合不确定性将非常高。类似地，如果两个模型对“汽车”类和“人”类赋予近似相同的概率，则不确定性会很高。似乎很合理。</p><p id="7e6f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，使用像BALD这样的蒙特卡罗采样方法来估计自动标签不确定性的一个缺点是，需要为每个输入数据创建多个模型输出，从而导致更多的推理计算和更长的推理时间。</p><p id="2026" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在下一节中，我们将看看解决这个问题的另一种方法。</p><h1 id="55df" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">3.方法2:分布建模</h1><p id="e7e3" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">让我们假设模型预测遵循特定的概率分布函数(PDF)。我们可以让模型直接学习预测PDF，而不是让模型优化预测精度。</p><p id="8c04" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们这样做，神经网络输出将不是对输入数据的单一预测，而是定义或确定概率分布形状的参数。一旦我们获得了这个分布，我们就可以通过从这些参数描述的概率分布中进行采样来容易地获得最终输出<strong class="kr jd"><em class="ln">【y】</em></strong>(即softmax输出)。当然，我们可以从该分布中进行蒙特卡罗采样，并像我们在上一节中所做的那样计算模型不确定性，但这将超出不确定性分布建模的目的。</p><p id="2465" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过使用分布建模方法，我们可以使用众所周知的分布均值和方差(或其他类似的随机性度量)的标准公式来直接计算输出方差。</p><p id="da3a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因为我们只需要对每个数据进行一次模型预测来计算自动标记的不确定性，所以在推断时间内，这种方法在计算上比蒙特卡罗<em class="ln">有效得多。然而，分布建模有一个警告。当模型直接学习概率分布时，类得分<strong class="kr jd"> <em class="ln"> y </em> </strong>是概率定义的，因此模型必须在“预期损失”上进行优化。通常这使得精确的计算变得不可能，或者封闭形式的方程可能极其复杂，这意味着在训练时间</em>期间它可能是低效的<em class="ln">。</em></p><p id="6f14" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">幸运的是，这在实际意义上不是一个问题，因为对于神经网络来说，计算损失及其梯度的计算时间，无论是简单的损失还是复杂的损失公式，与总的反向计算时间相比都是微不足道的。</p><p id="4d8e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">有一项关于将这种不确定性分布建模方法应用于多类图像分类任务的现有工作，称为“证据深度学习”[3]。我们将在下一节更深入地研究这项工作。</p><h1 id="5c82" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">4.用于多类分类的证据深度学习</h1><p id="f1a0" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">证据深度学习(EDL)是一种不确定性估计方法，它使用上面解释的不确定性分布建模方法。具体来说，EDL假设模型预测概率分布遵循狄利克雷分布。</p><p id="bcd8" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">出于几个原因，狄利克雷分布是用于此目的的明智选择:</p><p id="c51d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">1.)狄利克雷分布用于对总和为1的非负向量进行随机采样，因此适用于对softmax输出进行建模。</p><p id="450b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2.)计算狄利克雷分布的期望损失及其梯度的公式是精确和简单的。</p><p id="f3ce" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">(关于计算狄利克雷分布的期望损失的更多细节，看一看文献[3]的等式3、4和5。每个方程分别对应于多项式分布、交叉熵损失和平方和损失。)</p><p id="7660" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">3.)最重要的是，狄利克雷分布不仅有一个计算方差测度的公式，还有一个计算落在0和1之间的理论不确定性测度的公式。</p><p id="e8c4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">(这是因为我们可以将证据的Dempster-Shafer理论映射到Dirichlet分布。如果你很好奇，可以在这里【4】阅读更多关于这个<a class="ae ko" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.545.917&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">的内容。)</a></p><p id="97b8" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们更进一步讨论最后一点，第三点。我们可以使用以下公式计算狄利克雷分布的不确定性:</p><blockquote class="mr ms mt"><p id="47d0" class="kp kq ln kr b ks kt ku kv kw kx ky kz mu lb lc ld mv lf lg lh mw lj lk ll lm im bi translated">不确定性(x) =总和[<strong class="kr jd">1</strong>/总和[ <strong class="kr jd"> 1 </strong> + <strong class="kr jd"> z </strong> ]</p></blockquote><p id="e08b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在哪里，</p><ul class=""><li id="007c" class="mx my it kr b ks kt kw kx la mz le na li nb lm nc nd ne nf bi translated"><strong class="kr jd"> 1 </strong>和<strong class="kr jd"> z </strong>是长度为N的向量，其中N是类的数量(因此，Sum[ <strong class="kr jd"> 1 </strong> ] = N)。</li><li id="9f37" class="mx my it kr b ks ng kw nh la ni le nj li nk lm nc nd ne nf bi translated"><strong class="kr jd"> z </strong>是定义狄利克雷分布的参数，是一个非负向量(不一定总和为1)。</li><li id="5eb3" class="mx my it kr b ks ng kw nh la ni le nj li nk lm nc nd ne nf bi translated">因为Sum[<strong class="kr jd">1</strong>+<strong class="kr jd">z</strong>]≥Sum[<strong class="kr jd">1</strong>](等式为当<strong class="kr jd"> z </strong>的所有元素都为0时)，所以不确定度总是在0和1之间。</li></ul><p id="108c" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">神经网络输出<strong class="kr jd"> z </strong>等于狄利克雷分布参数，并且在语义上，向量<strong class="kr jd"> z </strong>的每个元素的大小对应于每个类别的确定性的水平。</p><p id="b141" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们想象一下两种情况下的狄利克雷分布。</p><figure class="nm nn no np gt kd gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8903b7de1e2c4ab75403a03008d07724.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/0*r2fbuJs8r9RircUY"/></div></figure><p id="cdb9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随着Sum[ <strong class="kr jd"> z </strong> ]变大，分布变窄，以平均值为中心。</p><figure class="nm nn no np gt kd gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a5e2a2defeded33c043d7b6c4826145c.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/0*WVE_35xrQY-xS94g"/></div></figure><p id="7a0e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随着Sum[ <strong class="kr jd"> z </strong>变小，分布变宽变平，当<strong class="kr jd"> z </strong> = <strong class="kr jd"> 0 </strong>时成为均匀分布。</p><p id="0a1f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">例如，当给定输入<strong class="kr jd"> <em class="ln"> x </em> </strong>接近决策边界时(一个“硬例子”)，有更高的可能性:</p><ol class=""><li id="0267" class="mx my it kr b ks kt kw kx la mz le na li nb lm nq nd ne nf bi translated">模型预测将是不正确的，</li><li id="7ec2" class="mx my it kr b ks ng kw nh la ni le nj li nk lm nq nd ne nf bi translated">该模型将获得高损失值，并且</li><li id="f1f2" class="mx my it kr b ks ng kw nh la ni le nj li nk lm nq nd ne nf bi translated">预期损失增加。</li></ol><p id="dde2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了对预期损失进行优化，模型在训练期间学习减少<strong class="kr jd"> z </strong>值，从而使预测概率分布变平并减少预期损失。</p><p id="f6a3" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">另一方面，当输入x远离决策边界时(一个“简单的例子”)，模型学习增加<strong class="kr jd"> z </strong>来缩小分布，以便它接近指向预测类的独热向量。</p><h1 id="bc80" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">5.高超的人工智能的不确定性估计</h1><p id="3861" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">我们上面讨论的两种类型的不确定性估计，蒙特卡罗方法(如BALD)和不确定性分布建模方法(如EDL ),都是为图像分类问题而设计的。我们看到这两种方法各有缺点:蒙特卡罗方法需要从每个输入得到多个输出，在推理时效率很低；不确定性分布建模要求预测概率分布遵循某种已知类型的分布。</p><p id="9e20" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将这两种方法结合起来，发明了一种专利混合方法，并应用于我们的自动标签引擎。下面是使用我们的自动标记功能及其不确定性估计的两个最佳实践，用于<strong class="kr jd">主动学习</strong>。</p><h2 id="50a0" class="nr lp it bd lq ns nt dn lu nu nv dp ly la nw nx mc le ny nz mg li oa ob mk iz bi translated"><strong class="ak"> Ex。1) </strong>高效的数据标注和质量保证</h2><p id="96c8" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">使用不确定性估计的最有效方法之一是标注训练数据。</p><p id="5695" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">用户可以首先运行Superb AI的自动标记，以获得自动标记的注释以及估计的不确定性。不确定性测量以两种方式显示:</p><p id="2095" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">1) <strong class="kr jd">图像级难度</strong> —我们的自动标记引擎将每张图像分为容易、中等或困难。基于这种难度度量，人类审查者可以集中于更难的图像，并且更有效地对数据进行采样以供审查。</p><figure class="nm nn no np gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oc"><img src="../Images/9d70f757763ca3bfc2a61939b1023445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DZYRewNLlq-ctEu4"/></div></div></figure><p id="9296" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2) <strong class="kr jd">注释级不确定性</strong> —自动标注引擎还对每个注释(边界框、多边形)的不确定性进行评分，并要求您对低于阈值的注释进行审核。在下面的例子中，大多数车辆都被自动标记，你可以看到我们的人工智能要求用户查看场景中较小或较远的注释(用黄色标记)。</p><figure class="nm nn no np gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi od"><img src="../Images/a1fa33dc1d31244ad30a1bc814acedf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OCGM3vHXEKd2KrIZFAKYmQ.png"/></div></div></figure><h2 id="854d" class="nr lp it bd lq ns nt dn lu nu nv dp ly la nw nx mc le ny nz mg li oa ob mk iz bi translated">《出埃及记》2)通过挖掘硬例子进行有效的模型训练</h2><p id="7f41" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">使用不确定性估计的另一种方法是有效地提高生产级模型的性能。</p><p id="16dd" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">事实上，训练数据集中的大多数样本都是“简单的例子”，对提高模型的性能没有多大帮助。具有高度不确定性的罕见数据点或“硬例子”才是有价值的。因此，如果可以从干草堆中找到“硬例子”，模型性能可以更快地提高。</p><p id="0ce2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了说明这一点，我们在<a class="ae ko" href="https://cocodataset.org/#download" rel="noopener ugc nofollow" target="_blank"> COCO 2017验证集</a>标签上应用了不确定性估计技术。这是结果。</p><figure class="nm nn no np gt kd gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/1b3c6c15e749c52202019156c4334414.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/0*3A83wdKaCkvD8My5"/></div></figure><p id="b1d7" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，我们绘制了图像级不确定性的直方图。如您所见，大多数图像的不确定性非常低，低于0.1，不确定性水平呈长尾分布，最高约为4.0。绿色、黄色和红色表示它们在我们的套件平台上将显示为简单、中等和困难。</p><figure class="nm nn no np gt kd gh gi paragraph-image"><div class="gh gi of"><img src="../Images/32a4f6da951031f50e5168c46be68e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*jo7lAYdAekwbN97x"/></div></figure><p id="78e4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面是图像级不确定性差异的一个示例。随着不确定性度量的增加，您可以看到图像变得更加复杂——更多的对象、更小的对象以及它们之间更多的遮挡。</p><figure class="nm nn no np gt kd gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7ae9591eb09304d0799883300cf76d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/0*iweA2LXwHgLzShC-"/></div></figure><p id="9441" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，我们绘制了注释(边界框)不确定性的直方图。同样，大多数边界框注释具有非常低的不确定性(低于0.05)，有趣的是，对于这种情况，直方图对于0.1和0.3之间的不确定性度量或多或少是一致的。</p><figure class="nm nn no np gt kd gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/84d58dda63ad643d99ce2a2f3e45aace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/0*uwiUHqr-ck86az3c"/></div></figure><p id="a24e" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">同样，为了说明注释不确定性的差异，上面有四个不同不确定性级别的“人类”边界框。随着不确定性的增加，您可以看到这个人被遮挡、模糊和变小(在场景的更后面)。</p><p id="ee2b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">总之，即使是像COCO这样使用最广泛的数据集，也是以简单的例子为中心的。您的数据集可能也是如此。然而，正如本文所述，能够整合不确定性评估框架以快速识别决策边界附近的硬示例无疑将有助于整体优先化和主动学习。</p><h1 id="3164" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">即将推出</h1><p id="1b58" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">在我们技术系列的下一部分中，我们将讨论<strong class="kr jd">“通过类别不可知的细化提高自动标记的准确性”</strong>，然后是<strong class="kr jd">“使自动标记AI适应具有少量数据的新任务(纠正冷启动问题)”。</strong>要想在《走向AI》发布后第一时间得到通知，请在这里订阅<strong class="kr jd"/><a class="ae ko" href="https://subscribe.superb-ai.com/mail" rel="noopener ugc nofollow" target="_blank"><strong class="kr jd"><em class="ln"/></strong></a><strong class="kr jd"><em class="ln">。</em>T15】</strong></p><h1 id="a76e" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">关于Superb AI</h1><p id="b355" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated"><a class="ae ko" href="http://www.superb-ai.com?from=tai" rel="noopener ugc nofollow" target="_blank"> SUPERB AI </a>的套件是一个强大的训练数据平台，使ML团队能够创建可互操作和可扩展的训练数据管道。无论您是在为自动驾驶、癌症预测或威胁检测构建感知系统，Superb AI都提供了一种从训练数据管理开始构建AI的更快方法。如果你想试平台出来，<a class="ae ko" href="http://suite.superb-ai.com/auth/create" rel="noopener ugc nofollow" target="_blank"> <strong class="kr jd"> <em class="ln">今天就免费报名</em> </strong> </a>！</p><h1 id="84eb" class="lo lp it bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">参考</h1><p id="a597" class="pw-post-body-paragraph kp kq it kr b ks mm ku kv kw mn ky kz la mo lc ld le mp lg lh li mq lk ll lm im bi translated">[1] <a class="ae ko" href="https://arxiv.org/abs/1112.5745" rel="noopener ugc nofollow" target="_blank"> Houlsby，Neil等.“用于分类和偏好学习的贝叶斯主动学习”ArXiv abs/1112.5745 (2011年)。</a></p><p id="5cab" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">【2】<a class="ae ko" href="https://arxiv.org/abs/1703.02910" rel="noopener ugc nofollow" target="_blank">Gal，Yarin等，“利用图像数据的深度贝叶斯主动学习”ICML (2017年)。</a></p><p id="fb67" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[3] <a class="ae ko" href="https://arxiv.org/abs/1806.01768" rel="noopener ugc nofollow" target="_blank"> Sensoy，Murat等，“量化分类不确定性的证据深度学习”<em class="ln"> NeurIPS </em> (2018)。</a></p><p id="4957" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[4]<a class="ae ko" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.545.917&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">j sang，Audun等人，“将信念函数解释为狄利克雷分布”不确定性推理的符号和定量方法(2007)。</a></p></div></div>    
</body>
</html>