<html>
<head>
<title>Predicting Heart Failure Survival with Machine Learning Models — Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习模型预测心力衰竭存活率——第二部分</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/predicting-heart-failure-survival-with-machine-learning-models-part-ii-15f92db2ce1f?source=collection_archive---------1-----------------------#2020-08-28">https://pub.towardsai.net/predicting-heart-failure-survival-with-machine-learning-models-part-ii-15f92db2ce1f?source=collection_archive---------1-----------------------#2020-08-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/29aa2fe206aa4a9641f89d011cb0a7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Kvjw04sJJwGrx0wj"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Jair Lázaro 在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><div class=""><h2 id="f968" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">第二部分分步走查来分析和预测心力衰竭患者的生存率。</h2></div><h1 id="d37c" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">前言</h1><p id="d87b" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在之前的<a class="ae jd" href="https://medium.com/towards-artificial-intelligence/predicting-heart-failure-survival-with-machine-learning-models-part-i-7ff1ab58cff8" rel="noopener">帖子</a>中，我们查看了299名患者的心力衰竭数据集，其中包括一些生活方式和临床特征。那篇文章致力于探索性的数据分析，而这篇文章则致力于构建预测模型。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="477c" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">动机</h1><p id="b1db" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">激励人心的问题是——<em class="mv">‘心力衰竭患者存活的机会有多大？’。通过这次演练，我试图回答这个问题，同时给出一些关于处理不平衡数据集的见解。</em></p><p id="a375" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated"><em class="mv">这个项目的代码可以在我的</em><a class="ae jd" href="https://github.com/ani-rudra-chan/Heart-Failure-Survival-Project.git" rel="noopener ugc nofollow" target="_blank"><em class="mv">GitHub</em></a><em class="mv">资源库中找到。</em></p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="1a74" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">快速回顾</h1><p id="7472" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在之前的帖子中，我们看到—</p><ul class=""><li id="7f00" class="nb nc jg lp b lq mw lt mx lw nd ma ne me nf mi ng nh ni nj bi translated">年龄与血清肌酐略有正相关，而血清钠与血清肌酐略有负相关。</li><li id="e651" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">大多数死亡的患者没有并发症，或者至多患有贫血或糖尿病。</li><li id="00ac" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">死亡患者的射血分数似乎低于存活患者。</li><li id="323a" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">死亡患者的肌酐磷酸激酶水平似乎高于存活患者。</li></ul><p id="e518" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated"><em class="mv">(查看之前的</em> <a class="ae jd" href="https://medium.com/towards-artificial-intelligence/predicting-heart-failure-survival-with-machine-learning-models-part-i-7ff1ab58cff8" rel="noopener"> <em class="mv">帖子</em> </a> <em class="mv">以获得所用术语的入门知识)</em></p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="314f" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">概述</h1><ol class=""><li id="5b16" class="nb nc jg lp b lq lr lt lu lw np ma nq me nr mi ns nh ni nj bi translated">处理阶级不平衡</li><li id="afe8" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated">选择机器学习模型</li><li id="0277" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated">绩效衡量</li><li id="1d03" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated">数据准备</li><li id="c303" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated">分层k倍交叉验证</li><li id="fcba" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated">模型结构</li><li id="f80a" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated">巩固成果</li></ol></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="1e2d" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">1.处理阶级不平衡</h1><figure class="nu nv nw nx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/644bb469051dddc16e2b6cadee17b799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AOYMgkNxYeEAkuau"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Christophe Hautier 在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="985c" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">在戴上我们的安全帽之前，让我们快速地看一下目标职业的平衡。我们查看原始数据集中死者和幸存者的比例。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="5e5f" class="od kw jg nz b gy oe of l og oh">print('% of heart failure patients who died = {}'.format(df.death.value_counts(normalize=True)[1]))<br/>print('% of heart failure patients who survived = {}'.format(df.death.value_counts(normalize=True)[0]))</span><span id="f09d" class="od kw jg nz b gy oi of l og oh">% of heart failure patients who died = 0.3210702341137124<br/>% of heart failure patients who survived = 0.6789297658862876</span></pre><p id="f1f0" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">我们看到32%的患者死亡，而68%的患者存活。这显然是一个不平衡的数据集！。在这种情况下，无论我们选择哪种模式，都必须考虑这种不平衡。</p><p id="2bbf" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">处理不平衡数据在现实世界中很常见，这些由<a class="oj ok ep" href="https://medium.com/u/188db6757ee?source=post_page-----15f92db2ce1f--------------------------------" rel="noopener" target="_blank">德国Lahera </a>和<a class="ae jd" href="https://www.datacamp.com/community/tutorials/diving-deep-imbalanced-data?utm_source=adwords_ppc&amp;utm_campaignid=1455363063&amp;utm_adgroupid=65083631748&amp;utm_device=c&amp;utm_keyword=&amp;utm_matchtype=b&amp;utm_network=g&amp;utm_adpostion=&amp;utm_creative=332602034364&amp;utm_targetid=dsa-429603003980&amp;utm_loc_interest_ms=&amp;utm_loc_physical_ms=9300858&amp;gclid=Cj0KCQjw7ZL6BRCmARIsAH6XFDLT64HZpoAKxUUiFfYmcTOZIPQrFWafF_WdPCsxtg7wy6LN2S3lC3UaAn8QEALw_wcB" rel="noopener ugc nofollow" target="_blank"> DataCamp </a>撰写的<a class="ae jd" href="https://medium.com/strands-tech-corner/unbalanced-datasets-what-to-do-144e0552d9cd" rel="noopener">文章</a>是了解它们的好地方。</p><p id="915f" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">解决这个问题的技术概述是这样的— <strong class="lp jh">您可以为少数类(比例较小的类)的错误分类分配一个惩罚，通过这样做，允许算法学习这个惩罚。另一种方法是使用采样技术:要么对多数类进行下采样，要么对少数类进行过采样，或者两者都进行</strong> [1]。</p><p id="51be" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">在我们的练习中，我们将尝试通过以下方式来处理这种不平衡—</p><ol class=""><li id="4daa" class="nb nc jg lp b lq mw lt mx lw nd ma ne me nf mi ns nh ni nj bi translated">使用<strong class="lp jh">分层k-fold交叉验证</strong>技术来确保我们的模型的总指标不会过于乐观(意思是:好得难以置信！)并反映训练和测试数据中固有的不平衡；</li><li id="9890" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated">使用带有简单加权方案的<strong class="lp jh">惩罚模型</strong>(而不是SMOTE之类的采样技术)，该简单加权方案是类频率的倒数。</li></ol><p id="716e" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">通过遵循这些步骤，我们将观察不平衡对模型预测的影响，并尝试得出一些见解！</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="8ff3" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">2.选择机器学习模型</h1><figure class="nu nv nw nx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/bc0dd6894924eba2a1862ac0edb063e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*95Vt_4bIxKRfhkac"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">照片由<a class="ae jd" href="https://unsplash.com/@danidums?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">丹妮卡·坦尤科</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="6c9a" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">在这篇文章中，我们将考虑手头的问题是一个<strong class="lp jh">监督分类问题</strong>并且看两个基本的线性模型——</p><ol class=""><li id="2f44" class="nb nc jg lp b lq mw lt mx lw nd ma ne me nf mi ns nh ni nj bi translated"><strong class="lp jh">逻辑回归(LogReg) </strong></li><li id="3c3e" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh">支持向量机(SVM) </strong></li></ol><p id="7132" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">我们坚持使用这些工具，因为它们有一些巧妙的技巧来处理不平衡的目标标签，并且易于理解。随意尝试其他算法，如<em class="mv">随机森林、</em><em class="mv"/><em class="mv">【神经网络】</em>等。在监督模型和<em class="mv">k-最近邻</em>、<em class="mv"> DBSCAN、</em>等中。，在无监督的模型中。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="4356" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">3.绩效衡量</h1><figure class="nu nv nw nx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/375e86f43028dd88d3d655b5a432a1a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kXO5iAgxSh6s12rN"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">查尔斯·德鲁维奥在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="d69e" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">任何预测模型都必须通过某些预测指标来评估其性能。在此之前，让我们先定义我们的案例类型—</p><ol class=""><li id="9ac6" class="nb nc jg lp b lq mw lt mx lw nd ma ne me nf mi ns nh ni nj bi translated"><strong class="lp jh">真阳性(TP) </strong>:模型预测死亡，患者死亡时；</li><li id="c2a6" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh">真阴性(TN) </strong>:模型预测存活且患者存活时；</li><li id="fc70" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh">假阳性(FP) </strong>:模型预测死亡但患者存活；</li><li id="582c" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh">假阴性(FN) </strong>:模型预测存活，但患者死亡。</li></ol><p id="5d91" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">使用这些案例类型，我们定义了以下5个预测指标—</p><ol class=""><li id="ae2f" class="nb nc jg lp b lq mw lt mx lw nd ma ne me nf mi ns nh ni nj bi translated"><strong class="lp jh">回忆</strong>:这也称为<em class="mv">真阳性率</em>或模型对真阳性的<em class="mv">敏感度</em>。它被计算为<em class="mv"> TP/(TP + FN)。</em></li><li id="f462" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh"> Precision </strong>:这是模型预测的真阳性有多精确的度量。它被计算为<em class="mv"> TP/(TP+FP)。</em></li><li id="ad7f" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh">准确性:</strong>这是模型整体性能的综合度量，计算方式为<em class="mv"> (TP+TN)/(TP+TN+FP+FN)。</em></li><li id="93ee" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh">平衡精度</strong>:这是模型对每个类别进行分类的能力的综合度量。它是敏感性(TPR)和特异性(TNR)的平均值，给出为(TPR + TNR)/2。</li><li id="7974" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh"> ROC AUC </strong>:这是由不同预测阈值的真阳性率和假阳性率生成的受试者工作特征曲线(ROC)下的面积。对于随机预测值，这个值是0.5，我们的模型必须比这个值更好。</li></ol><p id="9119" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">定义了这些指标之后，重要的是概述我们期望我们的模型具有的性能。我们期望预测模型有—</p><ol class=""><li id="661e" class="nb nc jg lp b lq mw lt mx lw nd ma ne me nf mi ns nh ni nj bi translated"><strong class="lp jh">高召回</strong> —模型必须能够预测尽可能多的死亡；</li><li id="06f9" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh">高精度</strong> —模型预测的死亡人数必须精确，即尽可能与观察到的死亡人数相匹配；</li><li id="2158" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh">高平衡精度</strong> —该模型必须能够同样好地预测死亡和存活，即该模型必须对尽可能多的死亡敏感，同时对死亡和存活的预测要具体；</li><li id="8745" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated"><strong class="lp jh">高精度</strong> —模型必须具有很高的整体精度；</li><li id="a27a" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ns nh ni nj bi translated">【ROC AUC高-模型在曲线下的总面积必须大于任何随机预测值0.5。</li></ol></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="92d0" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">4.数据准备</h1><figure class="nu nv nw nx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/f70034f87e1339124bcec4bd1ec1ceb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vfRgjpmwyUiRc6e4"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Bonnie Kittle 在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="6717" class="od kw jg bd kx oo op dn lb oq or dp lf lw os ot lh ma ou ov lj me ow ox ll oy bi translated">缩放数据</h2><p id="63fe" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我们的主要数据准备将是特征缩放。我们用数字特征来做这件事，因为它们是在不同的尺度上测量的。我们在<code class="fe oz pa pb nz b">sklearn.preprocessing</code>中使用<code class="fe oz pa pb nz b">StandardScaler()</code>方法，并调整数值，使其平均值为0，方差为1。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="4764" class="od kw jg nz b gy oe of l og oh">cat_feat = df[['sex', 'smk', 'dia', 'hbp', 'anm']]<br/>num_feat = df[['age', 'plt', 'ejf', 'cpk', 'scr', 'sna']]</span><span id="b760" class="od kw jg nz b gy oi of l og oh">predictors = pd.concat([cat_feat, num_feat],axis=1)<br/>target = df['death']</span><span id="b978" class="od kw jg nz b gy oi of l og oh">from sklearn.preprocessing import StandardScaler</span><span id="0949" class="od kw jg nz b gy oi of l og oh">scaler = StandardScaler()<br/>scaled_feat = pd.DataFrame(scaler.fit_transform(num_feat.values),<br/>                           columns = num_feat.columns)<br/>scaled_predictors = pd.concat([cat_feat, scaled_feat], axis=1)</span></pre><p id="b5f9" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated"><em class="mv">(我们在当前分析中去掉了</em> <code class="fe oz pa pb nz b">time</code> <em class="mv">特征)</em></p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="e323" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">5.分层k倍交叉验证</h1><figure class="nu nv nw nx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/dc763820b2e388dad53861a75e0d20a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hTvSxSY92jxSpo3Z"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">Sergi Viladesau 在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="7222" class="od kw jg bd kx oo op dn lb oq or dp lf lw os ot lh ma ou ov lj me ow ox ll oy bi translated">快速入门</h2><p id="0423" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">现在，我们选择的这些模型有一定程度的随机性，尤其是在求解系数的时候。这意味着每次运行模型时，我们的结果都会有一些变化。为了确保我们最小化这种随机性，防止欠拟合或过拟合，我们多次运行模型，并计算我们选择的指标的平均值。</p><p id="7cb5" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated"><strong class="lp jh"> k倍交叉验证</strong>是一种众所周知的迭代验证方法，尤其是对于可能无法完美代表被研究人群的小数据集。数据集被分成k个子集，模型在前k-1个子集上被训练，并在最后第k个子集上被测试。这个过程重复k次，并计算性能测量的平均值[2]。</p><p id="be55" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated"><strong class="lp jh">分层k倍交叉验证</strong>在目标标签不平衡时提供帮助。由于通常对不平衡目标的k-fold交叉验证可能导致一些训练集只有一个目标标签进行训练，因此进行了分层。换句话说，先前的过程被重复，但是这一次，确保目标标签的比例在每个训练集中被保持[3][4]。</p><p id="2634" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">我们使用来自<code class="fe oz pa pb nz b">sklearn.model_selection</code>的<code class="fe oz pa pb nz b">StratifiedKFold</code>和<code class="fe oz pa pb nz b">cross_validate</code>进行10重交叉验证，之后我们统计列出的指标。</p><p id="4024" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">(<em class="mv">我发现</em> <a class="oj ok ep" href="https://medium.com/u/f374d0159316?source=post_page-----15f92db2ce1f--------------------------------" rel="noopener" target="_blank"> <em class="mv">杰森·布朗利</em></a><em class="mv"/><a class="ae jd" href="https://machinelearningmastery.com/" rel="noopener ugc nofollow" target="_blank"><em class="mv">machinelearningmastery.com</em></a><em class="mv">是一个非常有用的资源，可以了解更多关于这个</em></p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="a48a" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">6.模型结构</h1><figure class="nu nv nw nx gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pd"><img src="../Images/0baa750797334fa17ce05e885f1e1c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KXU7oi0CjTzXN5Zi"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk translated">由<a class="ae jd" href="https://unsplash.com/@pawel_czerwinski?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">paweczerwi324ski</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><h2 id="3286" class="od kw jg bd kx oo op dn lb oq or dp lf lw os ot lh ma ou ov lj me ow ox ll oy bi translated">逻辑回归</h2><p id="ccb1" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><em class="mv">逻辑回归是一类线性回归模型，通常适用于预测二元结果。它给出线性输入的非线性输出。其核心是逻辑函数(sigmoid函数),并且在回归系数的适当更新之后，基于该函数分配类别概率。</em></p><p id="b819" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">为了强调来自不平衡目标类别的偏差的影响，我们运行了带有和不带有惩罚的逻辑回归模型。在实例化逻辑回归模型时，可以通过<code class="fe oz pa pb nz b"> class_weight=’balanced’ </code>简单地启用惩罚。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="d6f3" class="od kw jg nz b gy oe of l og oh">#Stratified 8 fold cross validation<br/>strat_kfold = StratifiedKFold(n_splits=10, shuffle=True)</span><span id="b19a" class="od kw jg nz b gy oi of l og oh">#Instantiating the logistic regressor<br/>logreg_clf = LogisticRegression() </span><span id="0631" class="od kw jg nz b gy oi of l og oh">#To enable penalization, assign 'balanced' to the class_weight parameter</span><span id="9a66" class="od kw jg nz b gy oi of l og oh">x = scaled_predictors.values<br/>y = target.values</span><span id="c24b" class="od kw jg nz b gy oi of l og oh">#Running the model and tallying results of stratified 10-fold cross validation<br/>result = cross_validate(logreg_clf, x, y, cv=strat_kfold, scoring=['accuracy','balanced_accuracy', 'precision', 'recall', 'roc_auc'])                                                             <br/></span></pre><p id="03ae" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">我们来看看非惩罚和惩罚逻辑回归模型的预测结果。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="76b3" class="od kw jg nz b gy oe of l og oh">pd.concat([pd.DataFrame(result1).mean(),<br/>           pd.DataFrame(result2).mean()],axis=1).rename(columns={0:'Non-Penalized LogReg',1:'Penalized LogReg'})</span></pre><figure class="nu nv nw nx gt is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d322540b539808ecacbed846ae65b0ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*cTkAwM4-WdcsHYd_Ar-OGg.png"/></div></figure><p id="2eba" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">一些有趣的观察—</p><ul class=""><li id="6986" class="nb nc jg lp b lq mw lt mx lw nd ma ne me nf mi ng nh ni nj bi translated">这两个模型的总体精度几乎相同，都在72%左右，相当不错。</li><li id="c8a8" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">但是当我们观察平衡精度时，我们看到了一个主要的不同。惩罚LogReg对两类都敏感(71%)，而非惩罚LogReg则不太敏感(66%)。</li><li id="0400" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">与非惩罚LogReg (67%)相比，惩罚LogReg的<strong class="lp jh">精度</strong>较低，数值不够高。</li><li id="566b" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">对死亡的最大跳跃不敏感性或<strong class="lp jh">回忆</strong>出现在惩罚LogReg (72%)中，而非惩罚LogReg (44%)。</li><li id="54aa" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">0.76–0.77的<strong class="lp jh"> ROC AUC </strong>仍然优于随机分类器。</li></ul></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h2 id="1ddb" class="od kw jg bd kx oo op dn lb oq or dp lf lw os ot lh ma ou ov lj me ow ox ll oy bi translated">支持向量分类器</h2><p id="ae9d" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">SVC是非参数分类器，其使用特征空间中的超平面来尝试将数据点分成彼此接近的类别。在Youtube上观看StatQuest的这个 <a class="ae jd" href="https://www.youtube.com/watch?v=efR1C6CvhmE&amp;t=3s" rel="noopener ugc nofollow" target="_blank"> <em class="mv">视频</em> </a> <em class="mv">可以获得清晰的解释！</em></p><p id="1572" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">从上一篇文章中的EDA中，我们看到在散点图的外围发现了相当多被分类为死亡的数据点。我们或许可以假设线性核不能够充分分离这些数据点，而是采用<strong class="lp jh">径向基函数</strong>核。</p><p id="2b87" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">我们以与之前相同的方式实例化了一个惩罚SVC和一个非惩罚SVC，并评估了在预测不平衡类时出现的偏差。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="4265" class="od kw jg nz b gy oe of l og oh">#Stratified 10 fold cross validation<br/>strat_kfold = StratifiedKFold(n_splits=10, shuffle=True)</span><span id="e9dc" class="od kw jg nz b gy oi of l og oh">#Instantiating the SVC <br/>svc_clf = SVC(kernel='rbf')<br/>x = scaled_predictors.values<br/>y = target.values</span><span id="4958" class="od kw jg nz b gy oi of l og oh">#Running the model and tallying results of stratified 10-fold cross validation<br/>result3 = cross_validate(svc_clf, x, y, cv=strat_kfold, scoring=['accuracy','balanced_accuracy','precision','recall','roc_auc'])                                                                  </span></pre><p id="d9d8" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">我们比较了SVC模型的两个变体的预测结果。</p><pre class="nu nv nw nx gt ny nz oa ob aw oc bi"><span id="f288" class="od kw jg nz b gy oe of l og oh">pd.concat([pd.DataFrame(result3).mean(),<br/>           pd.DataFrame(result4).mean()],axis=1).rename(columns={0:'Non-Penalized SVC',1:'Penalized SVC'})</span></pre><figure class="nu nv nw nx gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/d0d5194063e80b6226f85c9983a969c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*bahuqN4OJfOpNllgaMUGCQ.png"/></div></figure><p id="efd0" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">一些有趣的观察—</p><ul class=""><li id="0058" class="nb nc jg lp b lq mw lt mx lw nd ma ne me nf mi ng nh ni nj bi translated">被罚SVC的总体<strong class="lp jh">精度</strong> (74%)和<strong class="lp jh">平衡精度</strong> (74%) <strong class="lp jh"> </strong>大于未罚SVC。</li><li id="fd91" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">与LogReg模型不同，<strong class="lp jh">精度</strong>对于SVC的两种变化都较低。</li><li id="cf5b" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">受惩罚的SVC (75%)比不受惩罚的SVC (43%)对死亡的敏感度或回忆(T19)有最大的提高。</li><li id="bb52" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">0.77-0.80的ROC AUC 仍然比随机分类器要好。</li></ul></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="7904" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">7.巩固成果</h1><p id="d698" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在这项工作结束时，重要的是，我们要总结迄今为止取得的成果，并对这一过程中获得的见解有所了解。</p><ul class=""><li id="7357" class="nb nc jg lp b lq mw lt mx lw nd ma ne me nf mi ng nh ni nj bi translated">在这个299名心力衰竭患者的数据集中，68%存活，而32%没有存活；</li><li id="d9e7" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">5个生活方式特征和5个临床特征表征了该数据集，并被用作生存的潜在预测因子；</li><li id="0abf" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">与存活者相比，大多数死亡患者没有并发症，射血分数较低，肌酐磷酸激酶水平较高；</li><li id="77bc" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">当传统的线性分类模型如逻辑回归和支持向量机用于预测生存率时，数据集中的不平衡会影响性能；</li><li id="a26f" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">10重交叉验证和逆频率惩罚方案提高了这些模型的预测性能；</li><li id="9246" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">在预测该数据集的死亡时，罚SVC比罚LogReg稍好；</li><li id="22d3" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">使用所提供的10个特征，这两个模型具有很好的能力(&gt; 70%)来区分可能存活的人和可能死亡的人。</li><li id="cf3a" class="nb nc jg lp b lq nk lt nl lw nm ma nn me no mi ng nh ni nj bi translated">给定心力衰竭患者的病史(5种生活方式和5种临床史)，这两个模型在预测患者存活方面具有至少70%的准确性。</li></ul></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><p id="aa0c" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">可以提升该项目的价值的一些有趣的方面是PCA和CATPCA以消除高度相关的特征、超参数测试、尝试无监督的机器学习模型等。</p><p id="e467" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated"><em class="mv">这个项目到此结束，我希望这两个帖子对你有用。非常欢迎反馈！</em></p><p id="11ae" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated"><em class="mv">再见！</em></p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="5ad2" class="kv kw jg bd kx ky mq la lb lc mr le lf km ms kn lh kp mt kq lj ks mu kt ll lm bi translated">参考</h1><p id="512f" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">[1]<a class="ae jd" href="https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf" rel="noopener ugc nofollow" target="_blank">https://statistics . Berkeley . edu/sites/default/files/tech-reports/666 . pdf</a></p><p id="ed49" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated"><a class="ae jd" href="https://machinelearningmastery.com/k-fold-cross-validation/" rel="noopener ugc nofollow" target="_blank">https://machinelearningmastery.com/k-fold-cross-validation/</a></p><p id="320d" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated">[3]<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . model _ selection。StratifiedKFold.html</a></p><p id="34ab" class="pw-post-body-paragraph ln lo jg lp b lq mw kh ls lt mx kk lv lw my ly lz ma mz mc md me na mg mh mi ij bi translated"><a class="ae jd" href="https://machinelearningmastery.com/k-fold-cross-validation/" rel="noopener ugc nofollow" target="_blank">https://machinelearningmastery.com/k-fold-cross-validation/</a></p></div></div>    
</body>
</html>