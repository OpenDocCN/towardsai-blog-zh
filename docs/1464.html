<html>
<head>
<title>Fully Explained SVM Classification with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python全面解释了SVM分类</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/fully-explained-svm-classification-with-python-eda124997bcd?source=collection_archive---------0-----------------------#2021-02-02">https://pub.towardsai.net/fully-explained-svm-classification-with-python-eda124997bcd?source=collection_archive---------0-----------------------#2021-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="596a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/data-science" rel="noopener ugc nofollow" target="_blank">数据科学</a></h2><div class=""/><div class=""><h2 id="32cf" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何用一个真实的例子解决分类问题。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3fc99b9777158d9b4825d0d6dcd9f4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ip8s18tMkZzM0pzsGuUN0Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">SVM分类。作者的照片</figcaption></figure><p id="2373" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本文中，我们将讨论分类问题中最常用的机器学习算法。支持向量机(SVM)算法用于<strong class="lj jd"><em class="md"/></strong><strong class="lj jd"><em class="md">【分类】</em> </strong>，也用于<strong class="lj jd"> <em class="md">异常值检测</em> </strong>。</p><p id="6143" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">超线或超平面由<strong class="lj jd"> <em class="md">决策点</em> </strong>或支持向量分开。<strong class="lj jd"> <em class="md">支持向量</em> </strong>是在最近的不同类点之间提供最大间隔的样本点。这个分离平面叫做<strong class="lj jd"> <em class="md">边缘</em> </strong>。误差越大，误差越小，误分类率也越低。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/d7dea39420d40f01b0e83b09d1c95946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*Fr1oUMklHrxb2GfXMvQ91A.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">SVM的边缘。作者的照片</figcaption></figure><p id="f593" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面的照片显示了划分不同类别的线性超平面。但是，我们可以通过选择不同的核函数参数来选择不同的标准来划分类别，这些核函数参数在SVM的分类类别中针对决策点给出。不同的核分别是<strong class="lj jd"><em class="md"/></strong><strong class="lj jd"><em class="md">RBF</em></strong><strong class="lj jd"><em class="md">多项式</em> </strong>和<strong class="lj jd"><em class="md">sigmoid(tanh)</em></strong>。</p><p id="64fe" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">支持向量机在高维数据集上非常有用。这里，高维度意味着数据集中有更多的要素列。</p><p id="2d1e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有时，如果模型过度拟合，那么为了避免这种情况，我们应该使用正则化并选择不同的核。SVM使用五重交叉验证，而不是直接概率估计。</p><p id="6adf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">作为输入，支持向量机使用密集数组和稀疏向量。这里，稀疏向量是来自大的行和列矩阵的行和列的子集。</p><p id="f6fb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">sklearn提供的SVM类中有不同的函数类用于分类。这些类是<strong class="lj jd"> <em class="md"> SVC </em> </strong>，<strong class="lj jd"> <em class="md"> NuSVC </em> </strong>，<strong class="lj jd"> <em class="md"> LinearSVC </em> </strong>，<strong class="lj jd"> <em class="md"> OneClassSVM。</em>T57】</strong></p><p id="ec3e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">SVM回归的类有<strong class="lj jd"><em class="md"/></strong><strong class="lj jd"><em class="md">NuSVR</em></strong><strong class="lj jd"><em class="md">linear SVR</em></strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/3c624cb2bd47944ad215da033744c9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*SR89K7h9IM4axtcwNExN9w.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">SVM不同的阶级功能。作者的照片</figcaption></figure><p id="ea94" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">SVC和NuSVC几乎相同，但使用了不同的参数。在线性SVC的情况下，没有内核参数，因为它的名字是线性的。OneClassSVm还用于基于密度的数据集的异常值检测。</p><div class="mg mh gp gr mi mj"><a href="https://medium.com/towards-artificial-intelligence/become-a-data-scientist-in-2021-with-these-following-steps-5bf70a0fe0a1" rel="noopener follow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jd gy z fp mo fr fs mp fu fw jc bi translated">按照以下步骤，在2021年成为一名数据科学家</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">走上数据科学家之路需要具备的基本点</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">medium.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx lb mj"/></div></div></a></div><blockquote class="my mz na"><p id="18d2" class="lh li md lj b lk ll kd lm ln lo kg lp nb lr ls lt nc lv lw lx nd lz ma mb mc im bi translated"><strong class="lj jd"><em class="it">SVM实用的一些小技巧</em> </strong></p></blockquote><ul class=""><li id="23d2" class="ne nf it lj b lk ll ln lo lq ng lu nh ly ni mc nj nk nl nm bi translated">当我们拥有大量数据时，我们应该将默认缓存大小<code class="fe nn no np nq b">SVC</code>、<code class="fe nn no np nq b">SVR</code>、<code class="fe nn no np nq b">NuSVC</code>、<code class="fe nn no np nq b">NuSVR</code>更改为500 MB。</li><li id="68f3" class="ne nf it lj b lk nr ln ns lq nt lu nu ly nv mc nj nk nl nm bi translated">当将数据中的噪声点的C值设置为较低的值时，我们还应该专注于正则化。</li><li id="9c23" class="ne nf it lj b lk nr ln ns lq nt lu nu ly nv mc nj nk nl nm bi translated">在建模或标准化之前，必须对数据进行缩放，以表示“0”和“1”的差异。</li><li id="b27b" class="ne nf it lj b lk nr ln ns lq nt lu nu ly nv mc nj nk nl nm bi translated">如果数据集不平衡，那么我们应该使用<code class="fe nn no np nq b">fit</code>方法将<code class="fe nn no np nq b">class_weight</code>参数作为平衡参数。</li></ul><blockquote class="my mz na"><p id="a198" class="lh li md lj b lk ll kd lm ln lo kg lp nb lr ls lt nc lv lw lx nd lz ma mb mc im bi translated"><strong class="lj jd"><em class="it">SVM仁</em> </strong></p></blockquote><p id="ee8a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">核是SVM中的一个数学函数，它将数据作为输入，并进行一些处理以在数据点之间进行分类分离。</p><ul class=""><li id="a8cd" class="ne nf it lj b lk ll ln lo lq ng lu nh ly ni mc nj nk nl nm bi translated"><strong class="lj jd">线性:</strong>当我们看到不同的类是线性可分的时候，就使用这个核。</li><li id="02dc" class="ne nf it lj b lk nr ln ns lq nt lu nu ly nv mc nj nk nl nm bi translated"><strong class="lj jd">径向基函数(rbf): </strong>这个核在我们不知道数据分离的情况下用作通用目的。</li></ul><p id="f02a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当我们使用<code class="fe nn no np nq b">rbf</code>内核时，我们应该始终考虑两个参数，即<code class="fe nn no np nq b">gamma</code>和<code class="fe nn no np nq b">C</code>。这两个参数是调谐参数之一。</p><ul class=""><li id="2428" class="ne nf it lj b lk ll ln lo lq ng lu nh ly ni mc nj nk nl nm bi translated"><strong class="lj jd"> Sigmoid: </strong>这种核多用于神经网络或逻辑回归进行二元分类。</li><li id="3940" class="ne nf it lj b lk nr ln ns lq nt lu nu ly nv mc nj nk nl nm bi translated"><strong class="lj jd">多项式:</strong>用于图像处理。多项式的次数大于2即<code class="fe nn no np nq b">d = 2</code>。</li></ul><div class="mg mh gp gr mi mj"><a href="https://medium.com/towards-artificial-intelligence/fully-explained-logistic-regression-with-python-f4a16413ddcd" rel="noopener follow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jd gy z fp mo fr fs mp fu fw jc bi translated">用Python全面解释逻辑回归</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">机器学习算法中的统计非线性方法</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">medium.com</p></div></div><div class="ms l"><div class="nw l mu mv mw ms mx lb mj"/></div></div></a></div><blockquote class="my mz na"><p id="4bb3" class="lh li md lj b lk ll kd lm ln lo kg lp nb lr ls lt nc lv lw lx nd lz ma mb mc im bi translated"><strong class="lj jd"> <em class="it">机器学习中的正规化</em> </strong></p></blockquote><p id="b9dc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">机器学习中的正则化被称为<code class="fe nn no np nq b">C</code>，它在模型建模中出现过拟合时使用。它也被称为惩罚参数。该参数的主要作用是避免在训练期间错误识别数据。</p><p id="b000" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们每次都在机器学习中进行优化，正则化是过拟合问题中的优化技术之一。</p><p id="80ff" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">应当以避免错误分类方式选择<code class="fe nn no np nq b">C</code>。<code class="fe nn no np nq b">C</code>值越小，分离裕度越好。当我们看到有时不同类别的数据点如此接近，以至于超平面要做一个艰难的决定来分离具有较大<code class="fe nn no np nq b">C</code>值的类别。</p><blockquote class="my mz na"><p id="12cb" class="lh li md lj b lk ll kd lm ln lo kg lp nb lr ls lt nc lv lw lx nd lz ma mb mc im bi translated"><strong class="lj jd"> <em class="it">伽玛</em> </strong></p></blockquote><p id="6326" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过检查从超平面到数据点的距离，伽玛值被认为是一个平滑的超平面。gamma的<em class="md">低值</em>表示检查近数据点距离，gamma的<em class="md">大值</em>表示测量远数据点距离超平面。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/324fc29a05154e2e85a164f5b01468ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xdiAqj2HYBBdshzJ1OtHKA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">伽玛和正则化示例。作者的照片</figcaption></figure><p id="fec2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">虽然我们不知道如何手动选择正确的优化值。在这种情况下，我们应该使用<code class="fe nn no np nq b">GridSearchCV</code>来调整参数，我们将获得参数值。</p><p id="33ae" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我知道消化所有这些要点需要时间，但是相信我，通过练习你会做得很好。</p><div class="mg mh gp gr mi mj"><a href="https://medium.com/towards-artificial-intelligence/fully-explained-k-means-clustering-with-python-e7caa573176a" rel="noopener follow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jd gy z fp mo fr fs mp fu fw jc bi translated">充分解释了使用Python进行K-means聚类</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">群体相似性机器学习中的非监督部分。</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">medium.com</p></div></div><div class="ms l"><div class="ny l mu mv mw ms mx lb mj"/></div></div></a></div><p id="90f7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们将做一个实际的SVM来解决分类问题。</p><pre class="ks kt ku kv gt nz nq oa ob aw oc bi"><span id="2b6b" class="od oe it nq b gy of og l oh oi"># Importing the libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span></pre><p id="dfc1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">导入库后，现在我们将读取CSV文件，并将特征分成独立变量和因变量。</p><pre class="ks kt ku kv gt nz nq oa ob aw oc bi"><span id="99a2" class="od oe it nq b gy of og l oh oi"># Importing the dataset<br/>dataset = pd.read_csv('Social_Network_Ads.csv')<br/>X = dataset.iloc[:, [2, 3]].values<br/>y = dataset.iloc[:, 4].values</span></pre><p id="d254" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在将数据分为训练数据和测试数据。</p><pre class="ks kt ku kv gt nz nq oa ob aw oc bi"><span id="adbd" class="od oe it nq b gy of og l oh oi"># Splitting the dataset into the Training set and Test set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)</span></pre><p id="d563" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在对机器学习算法建模之前，我们应该总是做标准的缩放。</p><pre class="ks kt ku kv gt nz nq oa ob aw oc bi"><span id="a7ae" class="od oe it nq b gy of og l oh oi"># Feature Scaling<br/>from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><p id="2ee5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有时，人们会混淆训练用的<code class="fe nn no np nq b">fit_transform</code>和测试用的<code class="fe nn no np nq b">transform</code>。这两者之间的区别是<strong class="lj jd"> <em class="md">拟合变换</em> </strong>我们在训练中进行缩放，以便我们在标准化后得到的平均值和方差对于测试数据也应该是相同的。如果我们对训练和测试数据都进行拟合变换，那么我们会得到两个数据不同的平均值和方差。所以，这对我们的模型不好。</p><p id="1d38" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这个算法中，我们将使用线性核。</p><pre class="ks kt ku kv gt nz nq oa ob aw oc bi"><span id="1607" class="od oe it nq b gy of og l oh oi"># Fitting the classifier classifier to the Training set<br/>from sklearn.svm import SVC<br/>classifier = SVC(kernel = 'linear', random_state= 0)<br/>classifier.fit(X_train, y_train)</span><span id="c4de" class="od oe it nq b gy oj og l oh oi">#output:</span><span id="6287" class="od oe it nq b gy oj og l oh oi">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,<br/>  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',<br/>  kernel='linear', max_iter=-1, probability=False, random_state=0,<br/>  shrinking=True, tol=0.001, verbose=False)</span></pre><p id="d69e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在上面的输出中，我们看到了调整参数，如<code class="fe nn no np nq b">C</code>、<code class="fe nn no np nq b">kernel</code>、<code class="fe nn no np nq b">gamma</code>、<code class="fe nn no np nq b">shrinking</code>、<code class="fe nn no np nq b">cache_size</code>等。</p><p id="c0ba" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果存在大量迭代，收缩参数用于缩短训练时间中的迭代次数。</p><p id="01ef" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，我们将预测数据并制作我们的模型。</p><pre class="ks kt ku kv gt nz nq oa ob aw oc bi"><span id="6110" class="od oe it nq b gy of og l oh oi"># Predicting the Test set results<br/>y_pred = classifier.predict(X_test)</span></pre><p id="e268" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，我们计算混淆矩阵。</p><pre class="ks kt ku kv gt nz nq oa ob aw oc bi"><span id="9b33" class="od oe it nq b gy of og l oh oi"># Making the Confusion Matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="8350" class="od oe it nq b gy oj og l oh oi">#output:</span><span id="f9ba" class="od oe it nq b gy oj og l oh oi">array([[66,  2],<br/>       [ 8, 24]], dtype=int64)</span></pre><p id="6cab" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用线性核可视化训练和测试结果。</p><pre class="ks kt ku kv gt nz nq oa ob aw oc bi"><span id="03c0" class="od oe it nq b gy of og l oh oi"># Visualising the Training set results<br/>from matplotlib.colors import ListedColormap<br/>X_set, y_set = X_train, y_train<br/>X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),<br/>                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))<br/>plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),<br/>             alpha = 0.5, cmap = ListedColormap(('red', 'green')))<br/>plt.xlim(X1.min(), X1.max())<br/>plt.ylim(X2.min(), X2.max())<br/>for i, j in enumerate(np.unique(y_set)):<br/>    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], alpha=0.5,<br/>                c = ListedColormap(('red', 'green'))(i), label = j)<br/>plt.title('SVM (Training set)')<br/>plt.xlabel('Age')<br/>plt.ylabel('Estimated Salary')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/a4574d7b8aaac1a654e99dbe93419f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*ZYD4D84Uv-3tJroL_P6rzQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">训练集中的线性核分类。作者的照片</figcaption></figure><p id="d20c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，为了可视化测试集。</p><pre class="ks kt ku kv gt nz nq oa ob aw oc bi"><span id="48a9" class="od oe it nq b gy of og l oh oi"># Visualising the Test set results<br/>from matplotlib.colors import ListedColormap<br/>X_set, y_set = X_test, y_test<br/>X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() — 1, stop = X_set[:, 0].max() + 1, step = 0.01),<br/> np.arange(start = X_set[:, 1].min() — 1, stop = X_set[:, 1].max() + 1, step = 0.01))<br/>plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),<br/> alpha = 0.5, cmap = ListedColormap((‘red’, ‘green’)))<br/>plt.xlim(X1.min(), X1.max())<br/>plt.ylim(X2.min(), X2.max())<br/>for i, j in enumerate(np.unique(y_set)):<br/> plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], alpha=0.9,<br/> c = ListedColormap((‘red’, ‘green’))(i), label = j)<br/>plt.title(‘SVM (Test set)’)<br/>plt.xlabel(‘Age’)<br/>plt.ylabel(‘Estimated Salary’)<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/13de1b5a7c583d4c27a8a76874ece1bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*4G9wGdTyMDr8c8UKtpG-Bw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk translated">测试集中的线性核分类。作者的照片</figcaption></figure><p id="9139" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些是训练集和测试集上的线性分离核。我们观察到图中有很好的分类。</p><blockquote class="my mz na"><p id="1f90" class="lh li md lj b lk ll kd lm ln lo kg lp nb lr ls lt nc lv lw lx nd lz ma mb mc im bi translated"><strong class="lj jd"> <em class="it">结论:</em> </strong></p></blockquote><p id="b2a2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">SVM分类在分类和回归中非常有用。核参数给出了不同的超平面分离，以处理任何类型的数据集。</p><p id="258a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我希望你喜欢这篇文章。通过我的<a class="ae om" href="https://www.linkedin.com/in/data-scientist-95040a1ab/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae om" href="https://twitter.com/amitprius" rel="noopener ugc nofollow" target="_blank"> twitter </a>联系我。</p><h1 id="0ec6" class="on oe it bd oo op oq or os ot ou ov ow ki ox kj oy kl oz km pa ko pb kp pc pd bi translated">推荐文章</h1><ol class=""><li id="b53d" class="ne nf it lj b lk pe ln pf lq pg lu ph ly pi mc pj nk nl nm bi translated"><a class="ae om" href="https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e?sk=2231d868766e96b13d1e9d7db6064df1" rel="noopener"> NLP —用Python从零到英雄</a></li></ol><p id="60c6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">2.<a class="ae om" href="https://medium.com/towards-artificial-intelligence/python-data-structures-data-types-and-objects-244d0a86c3cf?sk=42f4b462499f3fc3a160b21e2c94dba6" rel="noopener"> Python数据结构数据类型和对象</a></p><p id="1333" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">3.<a class="ae om" href="https://medium.com/towards-artificial-intelligence/mysql-zero-to-hero-with-syntax-of-all-topics-92e700762c7b?source=friends_link&amp;sk=35a3f8dc1cf1ebd1c4d5008a5d12d6a3" rel="noopener"> MySQL:零到英雄</a></p><p id="85b5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">4.<a class="ae om" href="https://medium.com/towards-artificial-intelligence/basic-of-time-series-with-python-a2f7cb451a76?source=friends_link&amp;sk=09d77be2d6b8779973e41ab54ebcf6c5" rel="noopener">Python时间序列基础</a></p><p id="9e1a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">5.<a class="ae om" href="https://medium.com/towards-artificial-intelligence/numpy-zero-to-hero-with-python-d135f57d6082?source=friends_link&amp;sk=45c0921423cdcca2f5772f5a5c1568f1" rel="noopener"> NumPy:用Python零到英雄</a></p><p id="3bd6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">6.<a class="ae om" href="https://medium.com/towards-artificial-intelligence/fundamentals-of-series-and-data-frame-in-pandas-with-python-6e0b8a168a0d?source=friends_link&amp;sk=955350bf43c7d1680be6e37b15b6628b" rel="noopener">用python实现熊猫系列和数据帧的基础</a></p></div></div>    
</body>
</html>