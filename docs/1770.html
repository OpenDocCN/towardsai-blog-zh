<html>
<head>
<title>Create 3D Models from Images! AI and Game Development, Design…</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从图像创建三维模型！人工智能和游戏开发，设计…</h1>
<blockquote>原文：<a href="https://pub.towardsai.net/create-3d-models-from-images-ai-and-game-development-design-1835785b8563?source=collection_archive---------0-----------------------#2021-04-18">https://pub.towardsai.net/create-3d-models-from-images-ai-and-game-development-design-1835785b8563?source=collection_archive---------0-----------------------#2021-04-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b271" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsai.net/p/category/artificial-intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>，<a class="ae ep" href="https://towardsai.net/p/category/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a></h2><div class=""/><div class=""><h2 id="7f9c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">这个被称为GANverse3D的有前途的模型只需要一个图像就可以创建一个可以定制和动画的3D人物！</h2></div><blockquote class="kr ks kt"><p id="3732" class="ku kv kw kx b ky kz kd la lb lc kg ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最初发表于<a class="ae lr" href="https://www.louisbouchard.ai/ganverse3d/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae lr" href="https://www.louisbouchard.ai/tag/artificial-intelligence/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上看到的！</p></blockquote><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/c4bf8f05ad85e6032ac07961261d381e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OVlC4P5rHw6xvfHs28vOcA.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated"><a class="ae lr" href="https://youtu.be/yf1guvkMznc" rel="noopener ugc nofollow" target="_blank"> GANverse3D:英伟达用AI重新打造的霹雳游侠KITT</a>。经允许重新发布。</figcaption></figure><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mi mj l"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">听听这个故事！</figcaption></figure><p id="c3e0" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">你在下面看到的是有人在小心翼翼地为电子游戏创造场景。仅仅是像这样的一件物品就需要专业人员花费数小时的工作。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/c582b88aff4be1c2714635d42ba12c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*D_mCVQ7pnZFrpP_k-FbuPA.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">3D场景的创建。<a class="ae lr" href="https://youtu.be/H5Fk1w_JEfU" rel="noopener ugc nofollow" target="_blank">1ne ManShow——用Blender-Timelapse制作了这个3d场景和环境</a></figcaption></figure><p id="9eba" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">在互联网上拍一张物体的照片，比如一辆汽车，然后在不到一秒钟的时间内自动将3D物体插入到你的游戏中，这多酷啊？这很酷，对吧？想象一下，在几秒钟内，你甚至可以让这辆车动起来，让轮子转动，让灯闪烁，等等。如果我告诉你人工智能已经可以做到这一点，你会相信吗？如果视频游戏还不够，这个新的应用程序适用于你正在处理的任何3D场景，插图，电影，建筑，设计等等！去除专业设计人员数百甚至数千小时的长时间反复测试，让小型企业能够以更低的成本进行快速模拟！</p><p id="8956" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">当你喝一口咖啡的时候，这个模型将已经处理了一个汽车的图像，并生成了一个完整的3D动画版本，带有逼真的前灯、尾灯和闪光灯！此外，你甚至可以在Omniverse这样的虚拟环境平台中驾驶它，正如你在这里看到的。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mo mj l"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">NVIDIA Omniverse</figcaption></figure><p id="a8cf" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">为了介绍在最近的GTC活动中展示的这一新工具，Omniverse是为依赖虚拟环境来测试新想法和在创建最终产品之前可视化原型的创作者而设计的。您可以使用该工具通过实时光线跟踪来模拟复杂的虚拟世界。因为这篇文章不是关于Omniverse的，omni verse本身就很棒，所以我不会深入这个新平台的细节。我在下面的参考资料中链接了更多关于它的资源。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/3b69c7a2d4c9f0039eae06385ec5a2f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*JcU_W4nMedDEeI6H7vUeOw.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">GANverse3D。<a class="ae lr" href="https://youtu.be/yf1guvkMznc" rel="noopener ugc nofollow" target="_blank"> GANverse3D:英伟达用AI重新打造的霹雳游侠KITT</a>。经允许重新发布。</figcaption></figure><p id="e7c3" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">在这里，我想重点介绍NVIDIA在ICLR和CVPR 2021年发布的3D模型生成技术背后的算法。事实上，这个名为GANverse3D的有前途的模型只需要一个图像就可以创建一个可以定制和动画的3D人物！仅仅从它的名字来看，我认为如果我说它使用GAN来实现这一点，您不会感到惊讶。在这里，我不会进入GANs是如何工作的，因为我在<a class="ae lr" href="https://medium.com/@whats-ai" rel="noopener">以前的文章</a>中已经多次提到过。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/1a029df4040903fe1d6c8bfad87372e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*cknyaxfJbd1cJB4hzYlniQ.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">甘训练与潜在空间表征。图片由作者提供。</figcaption></figure><p id="a856" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">生成网络在从2D图像生成3D模型中是相对较新的，也称为“逆图形”,因为需要使用物体的多个视点来理解深度、纹理和照明以生成这种精确的3D模型的任务是复杂的。好吧，研究人员发现，生成敌对网络在训练中隐含地获得了这种知识。这意味着关于物体的形状、光照和纹理的信息已经被编码在GAN模型的潜在代码中。这种潜在代码是GAN架构的编码器部分的输出，通常被发送到解码器以生成控制特定属性的新图像。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><a href="http://eepurl.com/huGLT5"><div class="gh gi mp"><img src="../Images/76e3b32bdc5e9fe271eaa029481512bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wSHLIym4FpCpAZjdJMPMQw.png"/></div></a></figure><p id="0d97" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">正如在以前的研究中观察到的，我们知道不同的层控制图像中的不同属性，这就是为什么在过去的一年中你会看到这么多不同的和酷的应用程序使用GANs，其中一些可以控制面部的风格来生成卡通图像。相比之下，其他人可以让你的头动起来，而这一切都来自于你自己的一张照片。</p><div class="lt lu lv lw gt ab cb"><figure class="mq lx mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/649b4a99c0f6a030249876e9b3a8c0bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CuVFz2m7mvaPehSe68XAhw.png"/></div></figure><figure class="mq lx mw ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/b9f5ab3148b7d51c240f0619678877a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*NKAb1yI1H-mDBTlJ3glUNQ.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk mx di my mz translated">(左)<a class="ae lr" href="https://youtu.be/zB_jQ8SUjKE" rel="noopener ugc nofollow" target="_blank">动画模型</a>，以及(右)<a class="ae lr" href="https://www.myheritage.com/deep-nostalgia" rel="noopener ugc nofollow" target="_blank"> MyHeritage深度怀旧</a>动画工具。作者图片。</figcaption></figure></div><p id="37a4" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">在这种情况下，他们使用了众所周知的StyleGAN架构，这是一个强大的生成器，用于你在互联网和我的频道上看到的许多不同的buzz应用程序。研究人员通过实验发现，前四层可以通过固定其余层来控制相机视点。因此，通过操纵StyleGAN架构的这一特性，他们可以使用前四层来自动生成这种新颖的视点，以便仅从一张图片中进行渲染！同样，正如您在下图的前两行中看到的，反过来固定前四层，它们可以生成具有相同视点的不同对象的图像。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi na"><img src="../Images/b82779885065afe5b75f3613a84374e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mj2lIGupBdA0MrdJ-7W_Mg.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">多视图生成。<a class="ae lr" href="https://arxiv.org/pdf/2010.09125.pdf" rel="noopener ugc nofollow" target="_blank">张等，英伟达，(2020)，GANverse3D </a>。经允许重新发布。</figcaption></figure><p id="e310" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">这个特性，加上不同的损失函数，不仅可以控制图像的形状和视点，还可以控制纹理和背景！这一发现非常具有创新性，因为大多数关于反向图形的工作在其渲染网络的训练期间使用3D标签或至少同一对象的多视图图像。这种类型的数据通常很难获得，因此非常有限。由于缺乏训练数据，这些方法在训练、合成图像和这些真实图像之间存在领域差距，因此难以处理真实照片。正如您所看到的，只需要一张图片就可以生成这些看起来像真的一样的惊人的转换，将数据注释的需求减少了10，000倍以上。当然，这种产生如此重要的新颖观点的GAN架构也需要在大量数据上进行训练，才能实现这一点。幸运的是，它的成本要低得多，因为它只需要物体本身的许多例子，并且不需要同一张图片的多个视点，但是这仍然限制了我们使用这种技术可以对什么物体建模。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nb"><img src="../Images/33cbaa936a5aaac95a425b7f6d5b4fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NjHfwSa_giZ64z-qRYI2sg.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">GANverse3D架构概述。【张等，英伟达，(2020)，GANverse3D 。经允许重新发布。</figcaption></figure><p id="3261" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">正如您在这里看到的，StyleGAN被用作多视图生成器来构建缺失的数据，以训练渲染架构。</p><p id="271b" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">在进入渲染器之前，让我们往回跳一点，了解整个过程。你可以看到这个架构不是从一个常规的图像开始的，而是从一个潜在的代码开始的。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/ab66e868acb4092e7445ea7bf87ac05b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*g9G7gX9Mp4M0OnICmWvRGg.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">使用StyleGAN的3D神经渲染器。【张等，英伟达，(2020)，GANverse3D 。经允许重新发布。</figcaption></figure><p id="10dc" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">这种潜在的代码基本上就是他们在训练中学到的。你在这里看到的CNN和MLP网络只是基本的卷积神经网络和多层感知器，用于创建一个代码，解开图像的形状、纹理和背景。这意味着该代码将独立地包含将在呈现模型中使用的所有这些特征。在训练过程中，这段代码被更新，通过使用不同的StyleGAN层来控制这些功能，正如我们刚才看到的。</p><p id="5afe" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">当你使用这个模型并发送一个图像时，它将通过StyleGAN编码器并创建包含我们需要的所有信息的潜在代码。然后，这些信息将被提取出来，使用我们刚刚谈到的解缠模块来提取相机视点、3D网格、纹理和图像背景。这些特征被单独发送给产生最终模型的渲染器。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nb"><img src="../Images/33cbaa936a5aaac95a425b7f6d5b4fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NjHfwSa_giZ64z-qRYI2sg.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">GANverse3D架构概述。<a class="ae lr" href="https://arxiv.org/pdf/2010.09125.pdf" rel="noopener ugc nofollow" target="_blank">张等，英伟达，(2020)，GANverse3D </a>。经允许重新发布。</figcaption></figure><p id="fd6a" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">在这种架构中，渲染器是一种最先进的可区分渲染器，称为DIB-R，这里称为DIFFGraphicsRenderer。它被称为可微分渲染器，因为这项技术也是由NVIDIA开发的，就像StyleGAN和这篇论文一样，是首批允许在整个图像上分析计算梯度的技术之一，使训练神经网络生成3D形状成为可能。你可以看到，他们主要为每个单独的任务使用最先进的模型，因为整体架构比这些模型本身更重要和创新，这些模型本身已经非常强大。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/16480193bfc6537e658d3ae9f14cf783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*4xK_0exiA3nfIttGJ648_g.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk translated">GANverse3D。<a class="ae lr" href="https://youtu.be/yf1guvkMznc" rel="noopener ugc nofollow" target="_blank"> GANverse3D:英伟达</a>用AI重新创造的霹雳游侠KITT。经允许重新发布。</figcaption></figure><p id="456b" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">这就是这篇新论文与NVIDIA新的3D平台Omniverse相结合的方式，它将允许世界各地的建筑师、创作者、游戏开发者和设计师轻松地将新的动画对象添加到他们的模型中，而不需要任何3D建模专业知识或在渲染上花费大量预算。</p><p id="714a" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">请注意，这个应用程序目前只适用于汽车、马匹和鸟类，因为GANs需要大量的数据才能很好地运行，但这是非常有前途的。我只想一年后再回来，看看它会变得多么强大。10年或20年前，谁会想到在你的电脑屏幕上创建一个可控的、逼真的汽车动画版本只需要不到一秒钟的时间？要做到这一点，它只需要在你的口袋里放一个闪亮的小玩意，拍下照片并上传。这太疯狂了。我迫不及待地想看看研究人员在未来10-20年内会拿出什么！</p><h2 id="3bfa" class="nc nd it bd ne nf ng dn nh ni nj dp nk mk nl nm nn ml no np nq mm nr ns nt iz bi translated">观看视频中的更多示例</h2><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="mo mj l"/></div></figure><p id="bf94" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">来我们的<a class="ae lr" href="https://discord.gg/learnaitogether" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> Discord社区与我们聊天:</strong> <strong class="kx jd">一起学习AI</strong></a>和<em class="kw">分享你的项目、论文、最佳课程、寻找Kaggle队友，以及更多！</em></p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="1bd0" class="pw-post-body-paragraph ku kv it kx b ky kz kd la lb lc kg ld mk lf lg lh ml lj lk ll mm ln lo lp lq im bi translated">如果你喜欢我的工作，并想与人工智能保持同步，你绝对应该关注我的其他社交媒体账户(<a class="ae lr" href="https://www.linkedin.com/in/whats-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae lr" href="https://twitter.com/Whats_AI" rel="noopener ugc nofollow" target="_blank"> Twitter </a>)并订阅我的每周人工智能<a class="ae lr" href="http://eepurl.com/huGLT5" rel="noopener ugc nofollow" target="_blank">T5】简讯 </a>！</p><h2 id="cda9" class="nc nd it bd ne nf ng dn nh ni nj dp nk mk nl nm nn ml no np nq mm nr ns nt iz bi translated">支持我:</h2><ul class=""><li id="4d01" class="ob oc it kx b ky od lb oe mk of ml og mm oh lq oi oj ok ol bi translated">支持我的最好方式是在<a class="ae lr" href="https://medium.com/@whats-ai" rel="noopener"><strong class="kx jd">Medium</strong></a><strong class="kx jd"/>上关注我，或者如果你喜欢视频格式，在<a class="ae lr" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank"><strong class="kx jd">YouTube</strong></a><strong class="kx jd"/>上订阅我的频道<strong class="kx jd"> </strong>。</li><li id="f325" class="ob oc it kx b ky om lb on mk oo ml op mm oq lq oi oj ok ol bi translated">支持我在<a class="ae lr" href="https://www.patreon.com/whatsai" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd">上的工作</strong></a></li><li id="4a26" class="ob oc it kx b ky om lb on mk oo ml op mm oq lq oi oj ok ol bi translated">加入我们的<a class="ae lr" href="https://discord.gg/learnaitogether" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jd"> Discord社区:</strong> <strong class="kx jd">一起学AI</strong></a>和<em class="kw">分享你的项目、论文、最佳课程、寻找Kaggle队友等等！</em></li></ul></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="a718" class="or nd it bd ne os ot ou nh ov ow ox nk ki oy kj nn kl oz km nq ko pa kp nt pb bi translated">参考</h1><ul class=""><li id="2372" class="ob oc it kx b ky od lb oe mk of ml og mm oh lq oi oj ok ol bi translated">视频演示:<a class="ae lr" href="https://youtu.be/dvjwRBZ3Hnw" rel="noopener ugc nofollow" target="_blank">https://youtu.be/dvjwRBZ3Hnw</a></li><li id="69fa" class="ob oc it kx b ky om lb on mk oo ml op mm oq lq oi oj ok ol bi translated">卡拉斯等人，(2019)，“斯泰勒根”:<a class="ae lr" href="https://arxiv.org/pdf/1812.04948.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1812.04948.pdf</a></li><li id="840e" class="ob oc it kx b ky om lb on mk oo ml op mm oq lq oi oj ok ol bi translated">陈等，(2019)，" DIB-R": <a class="ae lr" href="https://arxiv.org/pdf/1908.01210.pdf" rel="noopener ugc nofollow" target="_blank">，</a></li><li id="4519" class="ob oc it kx b ky om lb on mk oo ml op mm oq lq oi oj ok ol bi translated">英伟达的omniverse(2021):<a class="ae lr" href="https://www.nvidia.com/en-us/omniverse/" rel="noopener ugc nofollow" target="_blank">https://www.nvidia.com/en-us/omniverse/</a></li><li id="7b44" class="ob oc it kx b ky om lb on mk oo ml op mm oq lq oi oj ok ol bi translated">张等，(2020)，“图像满足逆图形的可微绘制和可解释的三维神经绘制”:【https://arxiv.org/pdf/2010.09125.pdf】</li><li id="55b3" class="ob oc it kx b ky om lb on mk oo ml op mm oq lq oi oj ok ol bi translated">ganverse 3d NVIDIA官方视频:<a class="ae lr" href="https://youtu.be/0PQnrnUIBlU" rel="noopener ugc nofollow" target="_blank">https://youtu.be/0PQnrnUIBlU</a></li><li id="1871" class="ob oc it kx b ky om lb on mk oo ml op mm oq lq oi oj ok ol bi translated">NVIDIA的GANverse 3D博客文章:<a class="ae lr" href="https://blogs.nvidia.com/blog/2021/04/16/gan-research-knight-rider-ai-omniverse/" rel="noopener ugc nofollow" target="_blank">https://blogs . NVIDIA . com/blog/2021/04/16/gan-research-knight-rider-ai-omni verse/</a></li></ul></div></div>    
</body>
</html>